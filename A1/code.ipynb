{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 520556528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# wye test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85672763,  1.89363852, -0.25110738, -1.09727424],\n",
       "       [-0.23227008, -0.11473912, -1.69045798,  1.31190671],\n",
       "       [ 1.44085915,  0.37027676, -0.38280027,  0.4089585 ]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.random.randn(3, 4)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Mon Mar 18 15:47:46 2024\n",
      "End time:  Mon Mar 18 15:47:47 2024\n",
      "test_fun executed in 1.0050 seconds\n"
     ]
    }
   ],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timer\n",
    "def test_fun(x):\n",
    "    time.sleep(x)\n",
    "\n",
    "test_fun(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py.\n",
    "\n",
    "Modify tensor to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45850532,  0.58140505,  0.29454185,  0.1586542 ,  0.71893752,\n",
       "        -0.0909816 ],\n",
       "       [ 0.13477889,  0.27962191, -0.11743695, -0.07979469,  0.12256677,\n",
       "        -0.55592582],\n",
       "       [-0.290316  , -0.08481382, -0.53662092,  0.0805716 ,  0.21629346,\n",
       "        -0.8638151 ],\n",
       "       [ 1.12419121,  1.25536453,  0.52773539,  0.26890837, -0.49224248,\n",
       "         0.62645091],\n",
       "       [ 0.11095123, -0.70820161,  0.66312001,  0.43883652, -0.61102968,\n",
       "         0.24015082]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    SELU              :math:`\\frac{3}{4}`\n",
    "    ================= ====================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)\n",
    "    \n",
    "\n",
    "kaiming_normal_(np.array([0] * 30).reshape(5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    preds = output.argmax(axis=-1, keepdims=True)\n",
    "    return np.mean(preds == target) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name \n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.89363852, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.31190671],\n",
       "       [1.44085915, 0.37027676, 0.        , 0.4089585 ]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class relu(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_output[self.input <= 0] = 0\n",
    "        return grad_output\n",
    "    \n",
    "\n",
    "test_relu = relu('test_relu')\n",
    "_ = test_relu.forward(test_array)\n",
    "test_relu.backward(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.y = 1. / (1. + np.exp(-input))   # save sigmoid for more convenient grad computation\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return self.y * (1 - self.y) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10369682, 0.68890954, 0.10369682, 0.10369682],\n",
       "       [0.14895921, 0.14895921, 0.14895921, 0.55312236],\n",
       "       [0.5165657 , 0.17708327, 0.12228365, 0.18406737]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class softmax(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "            input.shape = [batch size, num_class]\n",
    "        \"\"\"\n",
    "        x_max = input.max(axis=-1, keepdims=True)       # to avoid overflow\n",
    "        x_exp = np.exp(input - x_max)\n",
    "        return x_exp / x_exp.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # packaged in CrossEntropyLoss\n",
    "        return grad_output\n",
    "\n",
    "softmax('test_softmax').forward(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: more activation, tanh, gelu, leaky_relu ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(Layer):\n",
    "    def __init__(self, name, in_num, out_num):\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.in_num = in_num\n",
    "        self.out_num = out_num\n",
    "\n",
    "        W = kaiming_normal_(np.array([0] * in_num * out_num).reshape(in_num, out_num), a=math.sqrt(5))     # Kaiming Init\n",
    "        self.W = Parameter(W, self.requires_grad)\n",
    "        self.b = Parameter(np.zeros(out_num), self.requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return input @ self.W.data + self.b.data      # [batch size, in_num] @ [in_num, out_num] + [out_num] => [batch size, out_num]\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "            grad_output: [batch size, out_num]\n",
    "        \"\"\"\n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.W.grad = self.input.T @ grad_output / batch_size\n",
    "        self.b.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return grad_output @ self.W.data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchnorm(Layer):\n",
    "    def __init__(self, name, shape, requires_grad=True):\n",
    "        super().__init__(name)\n",
    "        self.gamma = Parameter(np.random.uniform(0.9, 1.1, shape), requires_grad, skip_decay=True)\n",
    "        self.beta = Parameter(np.random.uniform(-0.1, 0.1, shape), requires_grad, skip_decay=True)\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        self.running_mean = Parameter(np.zeros(shape), False)\n",
    "        self.running_var = Parameter(np.zeros(shape), False)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            batch_mean = input.mean(axis=0)\n",
    "            batch_var = input.var(axis=0)\n",
    "            batch_std = np.sqrt(batch_var + 1e-8)    # To avoid divided by 0\n",
    "\n",
    "            momentum = 0.9\n",
    "            self.running_mean.data = momentum * self.running_mean.data + (1 - momentum) * batch_mean\n",
    "            self.running_var.data = momentum * self.running_var.data + (1 - momentum) * batch_var\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            batch_mean = self.running_mean.data\n",
    "            batch_std = np.sqrt(self.running_var.data)\n",
    "\n",
    "        self.norm = (input - batch_mean) / batch_std\n",
    "        self.gamma_norm = self.gamma.data / batch_std\n",
    "\n",
    "        return self.gamma.data * self.norm + self.beta.data\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):        \n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.gamma.grad = (grad_output * self.norm).sum(axis=0) / batch_size\n",
    "        self.beta.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return self.gamma_norm * (grad_output - self.norm * self.gamma.grad - self.beta.grad)       # TODO: 推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropout(Layer):\n",
    "    def __init__(self, name, drop_rate, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, input.shape) > self.drop_rate\n",
    "            return input * self.mask * self.fix_value\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = softmax('softmax')\n",
    "\n",
    "    def __call__(self, input, ground_truth):\n",
    "        self.bacth_size = input.shape[0]\n",
    "        self.class_num = input.shape[1]\n",
    "\n",
    "        preds = self.softmax.forward(input)\n",
    "        ground_truth = self.one_hot_encoding(ground_truth)\n",
    "\n",
    "        self.grad = preds - ground_truth    #TODO: 推导要写在report上不？\n",
    "\n",
    "        loss = -1 * (ground_truth * np.log(preds + 1e-8)).sum() / self.bacth_size\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.bacth_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.size), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.num_layers = 0\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if layer.requires_grad:\n",
    "            if hasattr(layer, 'W'):\n",
    "                self.params.append(layer.W)\n",
    "            if hasattr(layer, 'b'):\n",
    "                self.params.append(layer.b)\n",
    "            if hasattr(layer, 'gamma'):\n",
    "                self.params.append(layer.gamma)\n",
    "            if hasattr(layer, 'beta'):\n",
    "                self.params.append(layer.beta)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x):\n",
    "        for layer in self.layers[::-1]:\n",
    "            x = layer.backward(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = True\n",
    "    \n",
    "    def test(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, momentum, lr, weight_decay):\n",
    "        self.parameters = parameters\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        for i, (v, p) in enumerate(zip(self.v, self.parameters)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            v = self.momentum * v + self.lr * p.grad\n",
    "            self.v[i] = v\n",
    "            p.data -= self.v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLR(object):\n",
    "    def __init__(self, optimizer, T_max):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.n = -1\n",
    "        self.base_lr = optimizer.lr\n",
    "        self.step()\n",
    "\n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        lr = self.get_lr()\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        cos = np.cos(np.pi * self.n / self.T_max)\n",
    "        return self.base_lr * (1 + cos) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, config, model=None, train_loader=None, val_loader=None):\n",
    "        self.config = config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr = self.config['lr']\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = SGD(self.model.params, self.config['momentum'], self.lr, self.config['weight_decay'])\n",
    "        self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "\n",
    "    def train(self):\n",
    "        best_acc1 = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            self.train_scheduler.step()\n",
    "\n",
    "            # evaluate on validation set\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "\n",
    "    \n",
    "    def train_per_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        for i, (input, target) in enumerate(self.train_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            self.model.backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch, i, len(self.train_loader), batch_time=batch_time,\n",
    "                        data_time=data_time, loss=losses, top1=top1))\n",
    "                \n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.test()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        i, len(self.val_loader), batch_time=batch_time, loss=losses,\n",
    "                        top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} '.format(epoch=epoch + 1 , flag='val', top1=top1))\n",
    "        print(output)\n",
    "\n",
    "        return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "        \n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Assignment1-Dataset/'\n",
    "\n",
    "train_X = np.load(file_path + 'train_data.npy')\n",
    "train_y = np.load(file_path + 'train_label.npy')\n",
    "test_X = np.load(file_path + 'test_data.npy')\n",
    "test_y = np.load(file_path + 'test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([train_y[i][0] for i in range(train_y.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/49]\tTime 0.264 (0.264)\tData 0.003 (0.003)\tLoss 2.7978 (2.7978)\tPrec@1 9.375 (9.375)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/10]\tTime 0.010 (0.010)\tLoss 1.6939 (1.6939)\tPrec@1 40.918 (40.918)\n",
      "EPOCH: 1 val Results: Prec@1 40.900 \n",
      "Best Prec@1: 40.900\n",
      "\n",
      "current lr 9.75528e-02\n",
      "Epoch: [1][0/49]\tTime 0.083 (0.083)\tData 0.001 (0.001)\tLoss 1.6806 (1.6806)\tPrec@1 40.430 (40.430)\n",
      "Test: [0/10]\tTime 0.008 (0.008)\tLoss 1.5910 (1.5910)\tPrec@1 43.750 (43.750)\n",
      "EPOCH: 2 val Results: Prec@1 44.500 \n",
      "Best Prec@1: 44.500\n",
      "\n",
      "current lr 9.04508e-02\n",
      "Epoch: [2][0/49]\tTime 0.044 (0.044)\tData 0.002 (0.002)\tLoss 1.6431 (1.6431)\tPrec@1 43.750 (43.750)\n",
      "Test: [0/10]\tTime 0.012 (0.012)\tLoss 1.5409 (1.5409)\tPrec@1 45.898 (45.898)\n",
      "EPOCH: 3 val Results: Prec@1 45.870 \n",
      "Best Prec@1: 45.870\n",
      "\n",
      "current lr 7.93893e-02\n",
      "Epoch: [3][0/49]\tTime 0.161 (0.161)\tData 0.002 (0.002)\tLoss 1.5679 (1.5679)\tPrec@1 46.094 (46.094)\n",
      "Test: [0/10]\tTime 0.010 (0.010)\tLoss 1.5089 (1.5089)\tPrec@1 46.875 (46.875)\n",
      "EPOCH: 4 val Results: Prec@1 46.650 \n",
      "Best Prec@1: 46.650\n",
      "\n",
      "current lr 6.54508e-02\n",
      "Epoch: [4][0/49]\tTime 0.072 (0.072)\tData 0.001 (0.001)\tLoss 1.5413 (1.5413)\tPrec@1 44.238 (44.238)\n",
      "Test: [0/10]\tTime 0.020 (0.020)\tLoss 1.4752 (1.4752)\tPrec@1 48.145 (48.145)\n",
      "EPOCH: 5 val Results: Prec@1 47.870 \n",
      "Best Prec@1: 47.870\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [5][0/49]\tTime 0.093 (0.093)\tData 0.002 (0.002)\tLoss 1.4884 (1.4884)\tPrec@1 46.387 (46.387)\n",
      "Test: [0/10]\tTime 0.008 (0.008)\tLoss 1.4460 (1.4460)\tPrec@1 48.730 (48.730)\n",
      "EPOCH: 6 val Results: Prec@1 48.440 \n",
      "Best Prec@1: 48.440\n",
      "\n",
      "current lr 3.45492e-02\n",
      "Epoch: [6][0/49]\tTime 0.078 (0.078)\tData 0.001 (0.001)\tLoss 1.5001 (1.5001)\tPrec@1 47.363 (47.363)\n",
      "Test: [0/10]\tTime 0.011 (0.011)\tLoss 1.4403 (1.4403)\tPrec@1 49.023 (49.023)\n",
      "EPOCH: 7 val Results: Prec@1 48.810 \n",
      "Best Prec@1: 48.810\n",
      "\n",
      "current lr 2.06107e-02\n",
      "Epoch: [7][0/49]\tTime 0.075 (0.075)\tData 0.001 (0.001)\tLoss 1.4795 (1.4795)\tPrec@1 50.391 (50.391)\n",
      "Test: [0/10]\tTime 0.008 (0.008)\tLoss 1.4295 (1.4295)\tPrec@1 49.902 (49.902)\n",
      "EPOCH: 8 val Results: Prec@1 49.160 \n",
      "Best Prec@1: 49.160\n",
      "\n",
      "current lr 9.54915e-03\n",
      "Epoch: [8][0/49]\tTime 0.038 (0.038)\tData 0.002 (0.002)\tLoss 1.4852 (1.4852)\tPrec@1 47.559 (47.559)\n",
      "Test: [0/10]\tTime 0.013 (0.013)\tLoss 1.4257 (1.4257)\tPrec@1 50.098 (50.098)\n",
      "EPOCH: 9 val Results: Prec@1 49.400 \n",
      "Best Prec@1: 49.400\n",
      "\n",
      "current lr 2.44717e-03\n",
      "Epoch: [9][0/49]\tTime 0.050 (0.050)\tData 0.002 (0.002)\tLoss 1.4117 (1.4117)\tPrec@1 50.586 (50.586)\n",
      "Test: [0/10]\tTime 0.008 (0.008)\tLoss 1.4282 (1.4282)\tPrec@1 50.293 (50.293)\n",
      "EPOCH: 10 val Results: Prec@1 49.440 \n",
      "Best Prec@1: 49.440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'sigmoid': sigmoid, \n",
    "        'softmax': softmax,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout,\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "\n",
    "    return model\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 64}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 64}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu1'}}, \n",
    "    # {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    # {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 64, 'out_num': 10}},\n",
    "]\n",
    "lr = 0.1\n",
    "bs = 1024\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4     # 2e-4, 1e-4\n",
    "seed = 0\n",
    "epoch = 10\n",
    "\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': lr, \n",
    "    'bs': bs,\n",
    "    'momentum': momentum,\n",
    "    'weight_decay': weight_decay,\n",
    "    'seed': seed,\n",
    "    'epoch': epoch,\n",
    "    \n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
