{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 520556528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# wye test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.18590096, -1.50606023, -0.03233924, -0.32722782],\n",
       "       [-1.54116619, -0.52125553,  0.16971559, -0.99620885],\n",
       "       [ 0.81831748,  0.96090466,  1.35616532, -0.70111426]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.random.randn(3, 4)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Fri Mar 29 01:36:20 2024\n",
      "End time:  Fri Mar 29 01:36:21 2024\n",
      "test_fun executed in 1.0050 seconds\n"
     ]
    }
   ],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timer\n",
    "def test_fun(x):\n",
    "    time.sleep(x)\n",
    "\n",
    "test_fun(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py.\n",
    "\n",
    "Modify tensor to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38053532,  0.1735732 , -1.06077562, -0.10529021,  0.75652059,\n",
       "        -0.71530825],\n",
       "       [ 0.58908577,  0.65507079, -0.20940775, -1.11856796, -0.02528333,\n",
       "        -0.10241898],\n",
       "       [-0.90975463, -0.04098603, -0.60648683,  0.32438372,  0.76327211,\n",
       "         0.27928326],\n",
       "       [-0.69533035, -0.13487909,  0.49598378, -0.79924988, -0.01481499,\n",
       "        -0.49228997],\n",
       "       [-0.25353233, -0.45903369, -0.16702028,  0.00479639, -0.37243647,\n",
       "        -0.23640761]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    SELU              :math:`\\frac{3}{4}`\n",
    "    ================= ====================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)\n",
    "\n",
    "kaiming_normal_(np.array([0] * 30).reshape(5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    preds = output.argmax(axis=-1, keepdims=True)\n",
    "    return np.mean(preds == target) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train_X, test_X, mode=None):\n",
    "    if mode == 'min-max':\n",
    "        min_each_feature = np.min(train_X, axis=0)\n",
    "        max_each_feature = np.max(train_X, axis=0)\n",
    "        scale = max_each_feature - min_each_feature\n",
    "        scale[scale == 0] = 1   # To avoid divided by 0\n",
    "        scaled_train = (train_X - min_each_feature) / scale\n",
    "        scaled_test = (test_X - min_each_feature) / scale\n",
    "        return scaled_train, scaled_test\n",
    "\n",
    "    if mode == 'norm':\n",
    "        std_each_feature = np.std(train_X, axis=0)\n",
    "        mean_each_feature = np.mean(train_X, axis=0)\n",
    "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
    "        norm_train = (train_X - mean_each_feature) / std_each_feature\n",
    "        norm_test = (test_X - mean_each_feature) / std_each_feature\n",
    "        return norm_train, norm_test\n",
    "\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name \n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.16971559, 0.        ],\n",
       "       [0.81831748, 0.96090466, 1.35616532, 0.        ]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class relu(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_output[self.input <= 0] = 0\n",
    "        return grad_output\n",
    "    \n",
    "\n",
    "test_relu = relu('test_relu')\n",
    "_ = test_relu.forward(test_array)\n",
    "test_relu.backward(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaky_relu(Layer):\n",
    "    def __init__(self, name, alpha, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.where(input > 0, input, self.alpha * input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        x = np.ones_like(grad_output)\n",
    "        x[grad_output < 0] *= self.alpha\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gelu(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        vec_erf = np.vectorize(math.erf)\n",
    "        return 0.5 * input * (1 + vec_erf(input / np.sqrt(2)))\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        vec_erf = np.vectorize(math.erf)\n",
    "        return 0.5 + 0.5 * vec_erf(grad_output / np.sqrt(2)) + \\\n",
    "            ((0.5 * grad_output * ((2 / np.sqrt(np.pi)) * np.exp(-(grad_output ** 2)))) / np.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.y = 1. / (1. + np.exp(-input))   # save sigmoid for more convenient grad computation\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return self.y * (1 - self.y) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return 1 - np.tanh(grad_output) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 0.25      , 0.25      , 0.25      ],\n",
       "       [0.23895047, 0.23895047, 0.2831486 , 0.23895047],\n",
       "       [0.23219394, 0.2677785 , 0.39758979, 0.10243777]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class softmax(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "            input.shape = [batch size, num_class]\n",
    "        \"\"\"\n",
    "        x_max = input.max(axis=-1, keepdims=True)       # to avoid overflow\n",
    "        x_exp = np.exp(input - x_max)\n",
    "        return x_exp / x_exp.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # packaged in CrossEntropyLoss\n",
    "        return grad_output\n",
    "\n",
    "softmax('test_softmax').forward(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(Layer):\n",
    "    def __init__(self, name, in_num, out_num):\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.in_num = in_num\n",
    "        self.out_num = out_num\n",
    "\n",
    "        W = kaiming_normal_(np.array([0] * in_num * out_num).reshape(in_num, out_num), a=math.sqrt(5))     # Kaiming Init\n",
    "        self.W = Parameter(W, self.requires_grad)\n",
    "        self.b = Parameter(np.zeros(out_num), self.requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return input @ self.W.data + self.b.data      # [batch size, in_num] @ [in_num, out_num] + [out_num] => [batch size, out_num]\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "            grad_output: [batch size, out_num]\n",
    "        \"\"\"\n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.W.grad = self.input.T @ grad_output / batch_size\n",
    "        self.b.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return grad_output @ self.W.data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchnorm(Layer):\n",
    "    def __init__(self, name, shape, requires_grad=True):\n",
    "        super().__init__(name)\n",
    "        self.gamma = Parameter(np.random.uniform(0.9, 1.1, shape), requires_grad, skip_decay=True)\n",
    "        self.beta = Parameter(np.random.uniform(-0.1, 0.1, shape), requires_grad, skip_decay=True)\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        self.running_mean = Parameter(np.zeros(shape), False)\n",
    "        self.running_var = Parameter(np.zeros(shape), False)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            batch_mean = input.mean(axis=0)\n",
    "            batch_var = input.var(axis=0)\n",
    "            batch_std = np.sqrt(batch_var + 1e-8)    # To avoid divided by 0\n",
    "\n",
    "            momentum = 0.9\n",
    "            self.running_mean.data = momentum * self.running_mean.data + (1 - momentum) * batch_mean\n",
    "            self.running_var.data = momentum * self.running_var.data + (1 - momentum) * batch_var\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            batch_mean = self.running_mean.data\n",
    "            batch_std = np.sqrt(self.running_var.data)\n",
    "\n",
    "        self.norm = (input - batch_mean) / batch_std\n",
    "        self.gamma_norm = self.gamma.data / batch_std\n",
    "\n",
    "        return self.gamma.data * self.norm + self.beta.data\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):        \n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.gamma.grad = (grad_output * self.norm).sum(axis=0) / batch_size\n",
    "        self.beta.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return self.gamma_norm * (grad_output - self.norm * self.gamma.grad - self.beta.grad)       # TODO: 推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropout(Layer):\n",
    "    def __init__(self, name, drop_rate, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, input.shape) > self.drop_rate\n",
    "            return input * self.mask * self.fix_value\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = softmax('softmax')\n",
    "\n",
    "    def __call__(self, input, ground_truth):\n",
    "        self.bacth_size = input.shape[0]\n",
    "        self.class_num = input.shape[1]\n",
    "\n",
    "        preds = self.softmax.forward(input)\n",
    "        ground_truth = self.one_hot_encoding(ground_truth)\n",
    "\n",
    "        self.grad = preds - ground_truth    #TODO: 推导要写在report上不？\n",
    "\n",
    "        loss = -1 * (ground_truth * np.log(preds + 1e-8)).sum() / self.bacth_size\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.bacth_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.size), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.num_layers = 0\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if layer.requires_grad:\n",
    "            if hasattr(layer, 'W'):\n",
    "                self.params.append(layer.W)\n",
    "            if hasattr(layer, 'b'):\n",
    "                self.params.append(layer.b)\n",
    "            if hasattr(layer, 'gamma'):\n",
    "                self.params.append(layer.gamma)\n",
    "            if hasattr(layer, 'beta'):\n",
    "                self.params.append(layer.beta)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x):\n",
    "        for layer in self.layers[::-1]:\n",
    "            x = layer.backward(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = True\n",
    "    \n",
    "    def test(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, momentum, lr, weight_decay):\n",
    "        self.parameters = parameters\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        for i, (v, p) in enumerate(zip(self.v, self.parameters)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            v = self.momentum * v + self.lr * p.grad\n",
    "            self.v[i] = v\n",
    "            p.data -= self.v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    def __init__(self, parameters, lr, weight_decay=0, beta=(0.9, 0.999), eps=1e-8):\n",
    "        self.beta1 = beta[0]\n",
    "        self.beta2 = beta[1]\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.parameters = parameters\n",
    "        self.m = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.iterations += 1\n",
    "        for i, (p, m, v) in enumerate(zip(self.parameters, self.m, self.v)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            m = self.beta1 * m + (1 - self.beta1) * p.grad\n",
    "            v = self.beta2 * v + (1 - self.beta2) * np.power(p.grad, 2)\n",
    "\n",
    "            self.m[i] = m\n",
    "            self.v[i] = v\n",
    "            \n",
    "            # bias correction\n",
    "            m = m / (1 - np.power(self.beta1, self.iterations))\n",
    "            v = v / (1 - np.power(self.beta2, self.iterations))\n",
    "\n",
    "            p.data -= self.lr * m / (np.sqrt(v + self.eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLR(object):\n",
    "    def __init__(self, optimizer, T_max):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.n = -1\n",
    "        self.base_lr = optimizer.lr\n",
    "        self.step()\n",
    "\n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        lr = self.get_lr()\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        cos = np.cos(np.pi * self.n / self.T_max)\n",
    "        return self.base_lr * (1 + cos) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, config, model=None, train_loader=None, val_loader=None):\n",
    "        self.config = config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr = self.config['lr']\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.print_freq = self.config['print_freq']\n",
    "\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = SGD(self.model.params, self.config['momentum'], self.lr, self.config['weight_decay'])\n",
    "        elif self.config['optimizer'] == 'adam':\n",
    "            self.optimizer = Adam(self.model.params, self.lr, self.config['weight_decay'])\n",
    "        self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "\n",
    "    def train(self):\n",
    "        best_acc1 = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            self.train_scheduler.step()\n",
    "\n",
    "            # evaluate on validation set\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "            # time.sleep(1)\n",
    "\n",
    "    \n",
    "    def train_per_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        for i, (input, target) in enumerate(self.train_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            self.model.backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.train_loader) - 1):\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch + 1, i, len(self.train_loader) - 1, batch_time=batch_time,\n",
    "                        loss=losses, top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} '.format(epoch=epoch + 1 , flag='train', top1=top1))\n",
    "        print(output)\n",
    "                \n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.test()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.val_loader) - 1):\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        i, len(self.val_loader) - 1, batch_time=batch_time, loss=losses,\n",
    "                        top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} '.format(epoch=epoch + 1 , flag='val', top1=top1))\n",
    "        print(output)\n",
    "\n",
    "        return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "        \n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Assignment1-Dataset/'\n",
    "\n",
    "train_X = np.load(file_path + 'train_data.npy')\n",
    "train_y = np.load(file_path + 'train_label.npy')\n",
    "test_X = np.load(file_path + 'test_data.npy')\n",
    "test_y = np.load(file_path + 'test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([train_y[i][0] for i in range(train_y.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Fri Mar 29 01:36:21 2024\n",
      "current lr 1.00000e-03\n",
      "Epoch: [1][0/48]\tTime 0.019 (0.019)\tLoss 4.4425 (4.4425)\tPrec@1 10.742 (10.742)\n",
      "Epoch: [1][9/48]\tTime 0.065 (0.027)\tLoss 4.2057 (4.2830)\tPrec@1 9.766 (10.400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][18/48]\tTime 0.036 (0.025)\tLoss 3.7002 (4.1060)\tPrec@1 14.062 (10.855)\n",
      "Epoch: [1][27/48]\tTime 0.021 (0.026)\tLoss 3.6169 (3.9681)\tPrec@1 10.742 (11.133)\n",
      "Epoch: [1][36/48]\tTime 0.003 (0.021)\tLoss 3.4748 (3.8420)\tPrec@1 12.305 (11.407)\n",
      "Epoch: [1][45/48]\tTime 0.003 (0.018)\tLoss 3.1282 (3.7278)\tPrec@1 13.574 (11.702)\n",
      "Epoch: [1][48/48]\tTime 0.004 (0.017)\tLoss 3.0823 (3.6922)\tPrec@1 13.915 (11.874)\n",
      "EPOCH: 1 train Results: Prec@1 11.874 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 2.8707 (2.8707)\tPrec@1 15.332 (15.332)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 2.9027 (2.9146)\tPrec@1 13.903 (14.680)\n",
      "EPOCH: 1 val Results: Prec@1 14.680 \n",
      "Best Prec@1: 14.680\n",
      "\n",
      "current lr 9.99753e-04\n",
      "Epoch: [2][0/48]\tTime 0.014 (0.014)\tLoss 3.0145 (3.0145)\tPrec@1 15.625 (15.625)\n",
      "Epoch: [2][9/48]\tTime 0.003 (0.006)\tLoss 2.9625 (2.9890)\tPrec@1 16.016 (15.098)\n",
      "Epoch: [2][18/48]\tTime 0.009 (0.006)\tLoss 2.8462 (2.9251)\tPrec@1 16.406 (15.568)\n",
      "Epoch: [2][27/48]\tTime 0.006 (0.006)\tLoss 2.6859 (2.8784)\tPrec@1 18.555 (15.859)\n",
      "Epoch: [2][36/48]\tTime 0.003 (0.007)\tLoss 2.6399 (2.8261)\tPrec@1 18.945 (16.287)\n",
      "Epoch: [2][45/48]\tTime 0.012 (0.007)\tLoss 2.5282 (2.7805)\tPrec@1 19.238 (16.737)\n",
      "Epoch: [2][48/48]\tTime 0.005 (0.007)\tLoss 2.5671 (2.7666)\tPrec@1 17.335 (16.946)\n",
      "EPOCH: 2 train Results: Prec@1 16.946 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 2.3689 (2.3689)\tPrec@1 21.191 (21.191)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 2.3932 (2.3890)\tPrec@1 21.556 (21.140)\n",
      "EPOCH: 2 val Results: Prec@1 21.140 \n",
      "Best Prec@1: 21.140\n",
      "\n",
      "current lr 9.99013e-04\n",
      "Epoch: [3][0/48]\tTime 0.003 (0.003)\tLoss 2.5045 (2.5045)\tPrec@1 20.898 (20.898)\n",
      "Epoch: [3][9/48]\tTime 0.002 (0.004)\tLoss 2.4784 (2.4875)\tPrec@1 19.629 (20.518)\n",
      "Epoch: [3][18/48]\tTime 0.003 (0.005)\tLoss 2.4075 (2.4551)\tPrec@1 20.508 (20.857)\n",
      "Epoch: [3][27/48]\tTime 0.003 (0.005)\tLoss 2.3602 (2.4293)\tPrec@1 20.996 (21.296)\n",
      "Epoch: [3][36/48]\tTime 0.008 (0.006)\tLoss 2.3840 (2.4081)\tPrec@1 21.680 (21.730)\n",
      "Epoch: [3][45/48]\tTime 0.010 (0.007)\tLoss 2.2207 (2.3894)\tPrec@1 25.781 (22.079)\n",
      "Epoch: [3][48/48]\tTime 0.002 (0.007)\tLoss 2.2592 (2.3834)\tPrec@1 25.000 (22.204)\n",
      "EPOCH: 3 train Results: Prec@1 22.204 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 2.1486 (2.1486)\tPrec@1 26.660 (26.660)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 2.1691 (2.1511)\tPrec@1 25.510 (26.600)\n",
      "EPOCH: 3 val Results: Prec@1 26.600 \n",
      "Best Prec@1: 26.600\n",
      "\n",
      "current lr 9.97781e-04\n",
      "Epoch: [4][0/48]\tTime 0.004 (0.004)\tLoss 2.2339 (2.2339)\tPrec@1 25.098 (25.098)\n",
      "Epoch: [4][9/48]\tTime 0.003 (0.007)\tLoss 2.2874 (2.2427)\tPrec@1 25.781 (25.293)\n",
      "Epoch: [4][18/48]\tTime 0.004 (0.007)\tLoss 2.2230 (2.2223)\tPrec@1 24.902 (25.863)\n",
      "Epoch: [4][27/48]\tTime 0.004 (0.007)\tLoss 2.1056 (2.2023)\tPrec@1 30.469 (26.318)\n",
      "Epoch: [4][36/48]\tTime 0.013 (0.007)\tLoss 2.1571 (2.1879)\tPrec@1 27.539 (26.639)\n",
      "Epoch: [4][45/48]\tTime 0.006 (0.007)\tLoss 2.1450 (2.1767)\tPrec@1 26.758 (26.855)\n",
      "Epoch: [4][48/48]\tTime 0.004 (0.007)\tLoss 2.1370 (2.1736)\tPrec@1 28.892 (26.898)\n",
      "EPOCH: 4 train Results: Prec@1 26.898 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 2.0172 (2.0172)\tPrec@1 30.469 (30.469)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 2.0361 (2.0120)\tPrec@1 29.082 (30.900)\n",
      "EPOCH: 4 val Results: Prec@1 30.900 \n",
      "Best Prec@1: 30.900\n",
      "\n",
      "current lr 9.96057e-04\n",
      "Epoch: [5][0/48]\tTime 0.013 (0.013)\tLoss 2.0762 (2.0762)\tPrec@1 27.539 (27.539)\n",
      "Epoch: [5][9/48]\tTime 0.010 (0.008)\tLoss 2.1053 (2.0627)\tPrec@1 27.930 (29.912)\n",
      "Epoch: [5][18/48]\tTime 0.004 (0.007)\tLoss 2.0193 (2.0660)\tPrec@1 30.469 (29.811)\n",
      "Epoch: [5][27/48]\tTime 0.012 (0.007)\tLoss 2.0072 (2.0536)\tPrec@1 30.371 (29.973)\n",
      "Epoch: [5][36/48]\tTime 0.015 (0.008)\tLoss 2.0389 (2.0440)\tPrec@1 30.957 (30.279)\n",
      "Epoch: [5][45/48]\tTime 0.004 (0.007)\tLoss 2.0559 (2.0357)\tPrec@1 29.688 (30.492)\n",
      "Epoch: [5][48/48]\tTime 0.008 (0.007)\tLoss 2.0107 (2.0364)\tPrec@1 29.717 (30.534)\n",
      "EPOCH: 5 train Results: Prec@1 30.534 \n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.9285 (1.9285)\tPrec@1 34.961 (34.961)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.9451 (1.9173)\tPrec@1 32.908 (34.370)\n",
      "EPOCH: 5 val Results: Prec@1 34.370 \n",
      "Best Prec@1: 34.370\n",
      "\n",
      "current lr 9.93844e-04\n",
      "Epoch: [6][0/48]\tTime 0.007 (0.007)\tLoss 1.9994 (1.9994)\tPrec@1 31.152 (31.152)\n",
      "Epoch: [6][9/48]\tTime 0.008 (0.009)\tLoss 1.9101 (1.9571)\tPrec@1 34.766 (32.861)\n",
      "Epoch: [6][18/48]\tTime 0.003 (0.007)\tLoss 1.9434 (1.9583)\tPrec@1 33.496 (32.730)\n",
      "Epoch: [6][27/48]\tTime 0.003 (0.007)\tLoss 1.9310 (1.9590)\tPrec@1 33.496 (32.656)\n",
      "Epoch: [6][36/48]\tTime 0.002 (0.006)\tLoss 1.9639 (1.9540)\tPrec@1 31.543 (32.662)\n",
      "Epoch: [6][45/48]\tTime 0.009 (0.006)\tLoss 1.9261 (1.9474)\tPrec@1 34.180 (32.948)\n",
      "Epoch: [6][48/48]\tTime 0.017 (0.007)\tLoss 1.9000 (1.9446)\tPrec@1 34.080 (33.036)\n",
      "EPOCH: 6 train Results: Prec@1 33.036 \n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.8619 (1.8619)\tPrec@1 36.914 (36.914)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.8747 (1.8482)\tPrec@1 34.311 (36.330)\n",
      "EPOCH: 6 val Results: Prec@1 36.330 \n",
      "Best Prec@1: 36.330\n",
      "\n",
      "current lr 9.91144e-04\n",
      "Epoch: [7][0/48]\tTime 0.004 (0.004)\tLoss 1.9079 (1.9079)\tPrec@1 35.742 (35.742)\n",
      "Epoch: [7][9/48]\tTime 0.029 (0.014)\tLoss 1.8613 (1.9016)\tPrec@1 35.938 (34.404)\n",
      "Epoch: [7][18/48]\tTime 0.002 (0.010)\tLoss 1.8764 (1.8864)\tPrec@1 37.012 (34.992)\n",
      "Epoch: [7][27/48]\tTime 0.009 (0.009)\tLoss 1.8707 (1.8833)\tPrec@1 35.742 (35.132)\n",
      "Epoch: [7][36/48]\tTime 0.003 (0.008)\tLoss 1.8549 (1.8784)\tPrec@1 35.938 (35.235)\n",
      "Epoch: [7][45/48]\tTime 0.003 (0.007)\tLoss 1.8020 (1.8714)\tPrec@1 38.281 (35.449)\n",
      "Epoch: [7][48/48]\tTime 0.005 (0.007)\tLoss 1.8502 (1.8710)\tPrec@1 36.792 (35.454)\n",
      "EPOCH: 7 train Results: Prec@1 35.454 \n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.8101 (1.8101)\tPrec@1 38.770 (38.770)\n",
      "Test: [9/9]\tTime 0.001 (0.003)\tLoss 1.8228 (1.7954)\tPrec@1 35.714 (37.860)\n",
      "EPOCH: 7 val Results: Prec@1 37.860 \n",
      "Best Prec@1: 37.860\n",
      "\n",
      "current lr 9.87958e-04\n",
      "Epoch: [8][0/48]\tTime 0.004 (0.004)\tLoss 1.8764 (1.8764)\tPrec@1 34.570 (34.570)\n",
      "Epoch: [8][9/48]\tTime 0.003 (0.006)\tLoss 1.8286 (1.8370)\tPrec@1 37.305 (36.074)\n",
      "Epoch: [8][18/48]\tTime 0.013 (0.009)\tLoss 1.8154 (1.8349)\tPrec@1 37.012 (36.189)\n",
      "Epoch: [8][27/48]\tTime 0.005 (0.009)\tLoss 1.8508 (1.8247)\tPrec@1 34.766 (36.471)\n",
      "Epoch: [8][36/48]\tTime 0.007 (0.008)\tLoss 1.7840 (1.8169)\tPrec@1 39.062 (36.743)\n",
      "Epoch: [8][45/48]\tTime 0.020 (0.008)\tLoss 1.8221 (1.8153)\tPrec@1 36.523 (36.795)\n",
      "Epoch: [8][48/48]\tTime 0.003 (0.008)\tLoss 1.7551 (1.8138)\tPrec@1 39.151 (36.788)\n",
      "EPOCH: 8 train Results: Prec@1 36.788 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.7699 (1.7699)\tPrec@1 40.430 (40.430)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.7766 (1.7544)\tPrec@1 36.990 (39.100)\n",
      "EPOCH: 8 val Results: Prec@1 39.100 \n",
      "Best Prec@1: 39.100\n",
      "\n",
      "current lr 9.84292e-04\n",
      "Epoch: [9][0/48]\tTime 0.004 (0.004)\tLoss 1.8114 (1.8114)\tPrec@1 38.867 (38.867)\n",
      "Epoch: [9][9/48]\tTime 0.015 (0.007)\tLoss 1.7674 (1.7808)\tPrec@1 37.793 (38.311)\n",
      "Epoch: [9][18/48]\tTime 0.004 (0.007)\tLoss 1.7677 (1.7794)\tPrec@1 36.133 (37.999)\n",
      "Epoch: [9][27/48]\tTime 0.015 (0.007)\tLoss 1.7504 (1.7759)\tPrec@1 37.109 (38.173)\n",
      "Epoch: [9][36/48]\tTime 0.003 (0.007)\tLoss 1.7748 (1.7708)\tPrec@1 38.574 (38.263)\n",
      "Epoch: [9][45/48]\tTime 0.007 (0.007)\tLoss 1.7703 (1.7696)\tPrec@1 37.891 (38.239)\n",
      "Epoch: [9][48/48]\tTime 0.004 (0.007)\tLoss 1.8166 (1.7695)\tPrec@1 36.910 (38.246)\n",
      "EPOCH: 9 train Results: Prec@1 38.246 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.7361 (1.7361)\tPrec@1 41.113 (41.113)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.7442 (1.7215)\tPrec@1 37.117 (39.710)\n",
      "EPOCH: 9 val Results: Prec@1 39.710 \n",
      "Best Prec@1: 39.710\n",
      "\n",
      "current lr 9.80147e-04\n",
      "Epoch: [10][0/48]\tTime 0.013 (0.013)\tLoss 1.6950 (1.6950)\tPrec@1 40.723 (40.723)\n",
      "Epoch: [10][9/48]\tTime 0.003 (0.009)\tLoss 1.7250 (1.7155)\tPrec@1 39.355 (40.264)\n",
      "Epoch: [10][18/48]\tTime 0.009 (0.008)\tLoss 1.7220 (1.7318)\tPrec@1 40.332 (39.833)\n",
      "Epoch: [10][27/48]\tTime 0.003 (0.008)\tLoss 1.8141 (1.7333)\tPrec@1 34.277 (39.694)\n",
      "Epoch: [10][36/48]\tTime 0.003 (0.007)\tLoss 1.7441 (1.7331)\tPrec@1 40.625 (39.675)\n",
      "Epoch: [10][45/48]\tTime 0.003 (0.008)\tLoss 1.7578 (1.7325)\tPrec@1 39.941 (39.784)\n",
      "Epoch: [10][48/48]\tTime 0.003 (0.008)\tLoss 1.7378 (1.7331)\tPrec@1 40.566 (39.802)\n",
      "EPOCH: 10 train Results: Prec@1 39.802 \n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.7102 (1.7102)\tPrec@1 41.895 (41.895)\n",
      "Test: [9/9]\tTime 0.002 (0.001)\tLoss 1.7151 (1.6946)\tPrec@1 38.393 (40.650)\n",
      "EPOCH: 10 val Results: Prec@1 40.650 \n",
      "Best Prec@1: 40.650\n",
      "\n",
      "current lr 9.75528e-04\n",
      "Epoch: [11][0/48]\tTime 0.011 (0.011)\tLoss 1.6974 (1.6974)\tPrec@1 39.551 (39.551)\n",
      "Epoch: [11][9/48]\tTime 0.004 (0.007)\tLoss 1.6899 (1.7059)\tPrec@1 39.551 (40.244)\n",
      "Epoch: [11][18/48]\tTime 0.004 (0.007)\tLoss 1.6874 (1.7025)\tPrec@1 41.406 (40.368)\n",
      "Epoch: [11][27/48]\tTime 0.011 (0.008)\tLoss 1.7162 (1.7041)\tPrec@1 41.992 (40.479)\n",
      "Epoch: [11][36/48]\tTime 0.013 (0.008)\tLoss 1.7041 (1.7024)\tPrec@1 42.285 (40.506)\n",
      "Epoch: [11][45/48]\tTime 0.008 (0.008)\tLoss 1.6853 (1.6980)\tPrec@1 43.066 (40.653)\n",
      "Epoch: [11][48/48]\tTime 0.005 (0.008)\tLoss 1.6755 (1.6982)\tPrec@1 39.151 (40.650)\n",
      "EPOCH: 11 train Results: Prec@1 40.650 \n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.6904 (1.6904)\tPrec@1 41.992 (41.992)\n",
      "Test: [9/9]\tTime 0.004 (0.002)\tLoss 1.6928 (1.6718)\tPrec@1 38.265 (41.470)\n",
      "EPOCH: 11 val Results: Prec@1 41.470 \n",
      "Best Prec@1: 41.470\n",
      "\n",
      "current lr 9.70440e-04\n",
      "Epoch: [12][0/48]\tTime 0.008 (0.008)\tLoss 1.7397 (1.7397)\tPrec@1 38.574 (38.574)\n",
      "Epoch: [12][9/48]\tTime 0.015 (0.010)\tLoss 1.6345 (1.6843)\tPrec@1 41.699 (41.084)\n",
      "Epoch: [12][18/48]\tTime 0.010 (0.009)\tLoss 1.6709 (1.6702)\tPrec@1 42.188 (41.853)\n",
      "Epoch: [12][27/48]\tTime 0.006 (0.008)\tLoss 1.6340 (1.6718)\tPrec@1 43.555 (41.560)\n",
      "Epoch: [12][36/48]\tTime 0.005 (0.007)\tLoss 1.6404 (1.6722)\tPrec@1 42.383 (41.541)\n",
      "Epoch: [12][45/48]\tTime 0.007 (0.007)\tLoss 1.7011 (1.6703)\tPrec@1 40.527 (41.561)\n",
      "Epoch: [12][48/48]\tTime 0.002 (0.007)\tLoss 1.6196 (1.6715)\tPrec@1 42.099 (41.556)\n",
      "EPOCH: 12 train Results: Prec@1 41.556 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.6690 (1.6690)\tPrec@1 43.652 (43.652)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.6714 (1.6520)\tPrec@1 39.158 (42.080)\n",
      "EPOCH: 12 val Results: Prec@1 42.080 \n",
      "Best Prec@1: 42.080\n",
      "\n",
      "current lr 9.64888e-04\n",
      "Epoch: [13][0/48]\tTime 0.009 (0.009)\tLoss 1.6972 (1.6972)\tPrec@1 40.430 (40.430)\n",
      "Epoch: [13][9/48]\tTime 0.008 (0.008)\tLoss 1.6157 (1.6544)\tPrec@1 42.188 (41.689)\n",
      "Epoch: [13][18/48]\tTime 0.004 (0.008)\tLoss 1.6553 (1.6485)\tPrec@1 41.895 (42.275)\n",
      "Epoch: [13][27/48]\tTime 0.003 (0.008)\tLoss 1.6795 (1.6496)\tPrec@1 40.137 (42.285)\n",
      "Epoch: [13][36/48]\tTime 0.012 (0.008)\tLoss 1.6739 (1.6505)\tPrec@1 40.527 (42.235)\n",
      "Epoch: [13][45/48]\tTime 0.014 (0.008)\tLoss 1.6938 (1.6500)\tPrec@1 42.676 (42.302)\n",
      "Epoch: [13][48/48]\tTime 0.011 (0.008)\tLoss 1.6345 (1.6496)\tPrec@1 43.396 (42.258)\n",
      "EPOCH: 13 train Results: Prec@1 42.258 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.6510 (1.6510)\tPrec@1 43.359 (43.359)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.6515 (1.6345)\tPrec@1 41.837 (42.610)\n",
      "EPOCH: 13 val Results: Prec@1 42.610 \n",
      "Best Prec@1: 42.610\n",
      "\n",
      "current lr 9.58877e-04\n",
      "Epoch: [14][0/48]\tTime 0.004 (0.004)\tLoss 1.6275 (1.6275)\tPrec@1 44.824 (44.824)\n",
      "Epoch: [14][9/48]\tTime 0.007 (0.010)\tLoss 1.6061 (1.6267)\tPrec@1 43.555 (43.652)\n",
      "Epoch: [14][18/48]\tTime 0.004 (0.008)\tLoss 1.6236 (1.6250)\tPrec@1 42.578 (43.164)\n",
      "Epoch: [14][27/48]\tTime 0.014 (0.008)\tLoss 1.6037 (1.6306)\tPrec@1 43.555 (43.052)\n",
      "Epoch: [14][36/48]\tTime 0.005 (0.008)\tLoss 1.6492 (1.6297)\tPrec@1 42.285 (43.183)\n",
      "Epoch: [14][45/48]\tTime 0.004 (0.008)\tLoss 1.6739 (1.6297)\tPrec@1 39.062 (43.141)\n",
      "Epoch: [14][48/48]\tTime 0.005 (0.008)\tLoss 1.6238 (1.6278)\tPrec@1 43.160 (43.158)\n",
      "EPOCH: 14 train Results: Prec@1 43.158 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.6360 (1.6360)\tPrec@1 43.945 (43.945)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.6355 (1.6194)\tPrec@1 42.092 (43.270)\n",
      "EPOCH: 14 val Results: Prec@1 43.270 \n",
      "Best Prec@1: 43.270\n",
      "\n",
      "current lr 9.52414e-04\n",
      "Epoch: [15][0/48]\tTime 0.012 (0.012)\tLoss 1.5743 (1.5743)\tPrec@1 45.410 (45.410)\n",
      "Epoch: [15][9/48]\tTime 0.002 (0.006)\tLoss 1.6116 (1.6117)\tPrec@1 44.727 (44.014)\n",
      "Epoch: [15][18/48]\tTime 0.005 (0.006)\tLoss 1.6223 (1.6171)\tPrec@1 40.723 (43.981)\n",
      "Epoch: [15][27/48]\tTime 0.004 (0.005)\tLoss 1.6240 (1.6115)\tPrec@1 43.262 (44.001)\n",
      "Epoch: [15][36/48]\tTime 0.008 (0.006)\tLoss 1.5907 (1.6123)\tPrec@1 44.238 (43.869)\n",
      "Epoch: [15][45/48]\tTime 0.006 (0.006)\tLoss 1.5812 (1.6091)\tPrec@1 43.457 (43.956)\n",
      "Epoch: [15][48/48]\tTime 0.002 (0.006)\tLoss 1.6201 (1.6089)\tPrec@1 43.042 (43.950)\n",
      "EPOCH: 15 train Results: Prec@1 43.950 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.6203 (1.6203)\tPrec@1 44.336 (44.336)\n",
      "Test: [9/9]\tTime 0.002 (0.001)\tLoss 1.6208 (1.6045)\tPrec@1 43.750 (44.060)\n",
      "EPOCH: 15 val Results: Prec@1 44.060 \n",
      "Best Prec@1: 44.060\n",
      "\n",
      "current lr 9.45503e-04\n",
      "Epoch: [16][0/48]\tTime 0.006 (0.006)\tLoss 1.6223 (1.6223)\tPrec@1 43.652 (43.652)\n",
      "Epoch: [16][9/48]\tTime 0.003 (0.005)\tLoss 1.5779 (1.5826)\tPrec@1 43.652 (44.834)\n",
      "Epoch: [16][18/48]\tTime 0.008 (0.007)\tLoss 1.6133 (1.5896)\tPrec@1 43.359 (44.351)\n",
      "Epoch: [16][27/48]\tTime 0.005 (0.007)\tLoss 1.5761 (1.5905)\tPrec@1 42.676 (44.207)\n",
      "Epoch: [16][36/48]\tTime 0.004 (0.007)\tLoss 1.5724 (1.5944)\tPrec@1 45.410 (44.146)\n",
      "Epoch: [16][45/48]\tTime 0.003 (0.007)\tLoss 1.6076 (1.5931)\tPrec@1 44.434 (44.196)\n",
      "Epoch: [16][48/48]\tTime 0.007 (0.007)\tLoss 1.5506 (1.5927)\tPrec@1 45.637 (44.210)\n",
      "EPOCH: 16 train Results: Prec@1 44.210 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.6074 (1.6074)\tPrec@1 44.238 (44.238)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.6074 (1.5912)\tPrec@1 43.240 (44.510)\n",
      "EPOCH: 16 val Results: Prec@1 44.510 \n",
      "Best Prec@1: 44.510\n",
      "\n",
      "current lr 9.38153e-04\n",
      "Epoch: [17][0/48]\tTime 0.007 (0.007)\tLoss 1.5536 (1.5536)\tPrec@1 46.582 (46.582)\n",
      "Epoch: [17][9/48]\tTime 0.003 (0.005)\tLoss 1.5359 (1.5810)\tPrec@1 46.289 (44.297)\n",
      "Epoch: [17][18/48]\tTime 0.002 (0.006)\tLoss 1.5827 (1.5829)\tPrec@1 44.727 (44.346)\n",
      "Epoch: [17][27/48]\tTime 0.009 (0.006)\tLoss 1.5897 (1.5844)\tPrec@1 43.359 (44.318)\n",
      "Epoch: [17][36/48]\tTime 0.006 (0.007)\tLoss 1.6015 (1.5800)\tPrec@1 43.457 (44.378)\n",
      "Epoch: [17][45/48]\tTime 0.003 (0.007)\tLoss 1.6068 (1.5773)\tPrec@1 43.652 (44.654)\n",
      "Epoch: [17][48/48]\tTime 0.002 (0.007)\tLoss 1.5632 (1.5770)\tPrec@1 44.458 (44.674)\n",
      "EPOCH: 17 train Results: Prec@1 44.674 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.5938 (1.5938)\tPrec@1 44.238 (44.238)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.5948 (1.5785)\tPrec@1 44.388 (44.940)\n",
      "EPOCH: 17 val Results: Prec@1 44.940 \n",
      "Best Prec@1: 44.940\n",
      "\n",
      "current lr 9.30371e-04\n",
      "Epoch: [18][0/48]\tTime 0.004 (0.004)\tLoss 1.5431 (1.5431)\tPrec@1 46.191 (46.191)\n",
      "Epoch: [18][9/48]\tTime 0.004 (0.006)\tLoss 1.6001 (1.5764)\tPrec@1 43.750 (45.049)\n",
      "Epoch: [18][18/48]\tTime 0.003 (0.005)\tLoss 1.6320 (1.5768)\tPrec@1 42.676 (44.737)\n",
      "Epoch: [18][27/48]\tTime 0.009 (0.008)\tLoss 1.5808 (1.5749)\tPrec@1 45.117 (44.810)\n",
      "Epoch: [18][36/48]\tTime 0.003 (0.007)\tLoss 1.4954 (1.5667)\tPrec@1 47.656 (45.022)\n",
      "Epoch: [18][45/48]\tTime 0.003 (0.007)\tLoss 1.4938 (1.5635)\tPrec@1 46.680 (45.162)\n",
      "Epoch: [18][48/48]\tTime 0.002 (0.007)\tLoss 1.5815 (1.5628)\tPrec@1 45.047 (45.198)\n",
      "EPOCH: 18 train Results: Prec@1 45.198 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.5804 (1.5804)\tPrec@1 45.215 (45.215)\n",
      "Test: [9/9]\tTime 0.003 (0.001)\tLoss 1.5814 (1.5671)\tPrec@1 44.133 (45.380)\n",
      "EPOCH: 18 val Results: Prec@1 45.380 \n",
      "Best Prec@1: 45.380\n",
      "\n",
      "current lr 9.22164e-04\n",
      "Epoch: [19][0/48]\tTime 0.007 (0.007)\tLoss 1.5419 (1.5419)\tPrec@1 45.801 (45.801)\n",
      "Epoch: [19][9/48]\tTime 0.006 (0.005)\tLoss 1.5730 (1.5314)\tPrec@1 45.508 (46.787)\n",
      "Epoch: [19][18/48]\tTime 0.010 (0.005)\tLoss 1.5672 (1.5346)\tPrec@1 45.020 (46.299)\n",
      "Epoch: [19][27/48]\tTime 0.005 (0.006)\tLoss 1.5390 (1.5404)\tPrec@1 45.508 (46.090)\n",
      "Epoch: [19][36/48]\tTime 0.014 (0.007)\tLoss 1.5646 (1.5475)\tPrec@1 45.312 (45.753)\n",
      "Epoch: [19][45/48]\tTime 0.005 (0.007)\tLoss 1.5383 (1.5474)\tPrec@1 46.191 (45.765)\n",
      "Epoch: [19][48/48]\tTime 0.019 (0.007)\tLoss 1.5809 (1.5462)\tPrec@1 45.637 (45.842)\n",
      "EPOCH: 19 train Results: Prec@1 45.842 \n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.5682 (1.5682)\tPrec@1 45.020 (45.020)\n",
      "Test: [9/9]\tTime 0.002 (0.002)\tLoss 1.5710 (1.5553)\tPrec@1 44.643 (45.790)\n",
      "EPOCH: 19 val Results: Prec@1 45.790 \n",
      "Best Prec@1: 45.790\n",
      "\n",
      "current lr 9.13540e-04\n",
      "Epoch: [20][0/48]\tTime 0.018 (0.018)\tLoss 1.5345 (1.5345)\tPrec@1 45.801 (45.801)\n",
      "Epoch: [20][9/48]\tTime 0.003 (0.007)\tLoss 1.4817 (1.5423)\tPrec@1 48.145 (45.781)\n",
      "Epoch: [20][18/48]\tTime 0.010 (0.006)\tLoss 1.4854 (1.5388)\tPrec@1 49.023 (45.724)\n",
      "Epoch: [20][27/48]\tTime 0.003 (0.006)\tLoss 1.5249 (1.5396)\tPrec@1 46.289 (45.689)\n",
      "Epoch: [20][36/48]\tTime 0.003 (0.006)\tLoss 1.5110 (1.5373)\tPrec@1 48.340 (45.859)\n",
      "Epoch: [20][45/48]\tTime 0.004 (0.006)\tLoss 1.5294 (1.5355)\tPrec@1 45.312 (46.026)\n",
      "Epoch: [20][48/48]\tTime 0.003 (0.006)\tLoss 1.5335 (1.5358)\tPrec@1 46.108 (46.074)\n",
      "EPOCH: 20 train Results: Prec@1 46.074 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.5549 (1.5549)\tPrec@1 45.312 (45.312)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.5601 (1.5447)\tPrec@1 45.026 (46.220)\n",
      "EPOCH: 20 val Results: Prec@1 46.220 \n",
      "Best Prec@1: 46.220\n",
      "\n",
      "current lr 9.04508e-04\n",
      "Epoch: [21][0/48]\tTime 0.011 (0.011)\tLoss 1.5529 (1.5529)\tPrec@1 44.727 (44.727)\n",
      "Epoch: [21][9/48]\tTime 0.006 (0.005)\tLoss 1.5478 (1.5275)\tPrec@1 46.094 (46.064)\n",
      "Epoch: [21][18/48]\tTime 0.005 (0.006)\tLoss 1.5188 (1.5227)\tPrec@1 45.898 (46.299)\n",
      "Epoch: [21][27/48]\tTime 0.003 (0.006)\tLoss 1.5195 (1.5238)\tPrec@1 49.805 (46.596)\n",
      "Epoch: [21][36/48]\tTime 0.004 (0.006)\tLoss 1.5455 (1.5249)\tPrec@1 45.996 (46.640)\n",
      "Epoch: [21][45/48]\tTime 0.002 (0.006)\tLoss 1.5526 (1.5244)\tPrec@1 45.605 (46.712)\n",
      "Epoch: [21][48/48]\tTime 0.004 (0.006)\tLoss 1.5348 (1.5237)\tPrec@1 47.052 (46.802)\n",
      "EPOCH: 21 train Results: Prec@1 46.802 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.5438 (1.5438)\tPrec@1 45.605 (45.605)\n",
      "Test: [9/9]\tTime 0.002 (0.002)\tLoss 1.5492 (1.5351)\tPrec@1 44.260 (46.280)\n",
      "EPOCH: 21 val Results: Prec@1 46.280 \n",
      "Best Prec@1: 46.280\n",
      "\n",
      "current lr 8.95078e-04\n",
      "Epoch: [22][0/48]\tTime 0.003 (0.003)\tLoss 1.5252 (1.5252)\tPrec@1 47.656 (47.656)\n",
      "Epoch: [22][9/48]\tTime 0.011 (0.006)\tLoss 1.4878 (1.5224)\tPrec@1 47.168 (46.641)\n",
      "Epoch: [22][18/48]\tTime 0.005 (0.006)\tLoss 1.5270 (1.5135)\tPrec@1 46.973 (47.399)\n",
      "Epoch: [22][27/48]\tTime 0.002 (0.006)\tLoss 1.5677 (1.5159)\tPrec@1 44.824 (46.987)\n",
      "Epoch: [22][36/48]\tTime 0.014 (0.006)\tLoss 1.5168 (1.5143)\tPrec@1 46.289 (46.946)\n",
      "Epoch: [22][45/48]\tTime 0.003 (0.006)\tLoss 1.5509 (1.5132)\tPrec@1 43.164 (46.958)\n",
      "Epoch: [22][48/48]\tTime 0.017 (0.006)\tLoss 1.5659 (1.5141)\tPrec@1 41.863 (46.864)\n",
      "EPOCH: 22 train Results: Prec@1 46.864 \n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.5330 (1.5330)\tPrec@1 46.289 (46.289)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.5392 (1.5253)\tPrec@1 45.536 (46.760)\n",
      "EPOCH: 22 val Results: Prec@1 46.760 \n",
      "Best Prec@1: 46.760\n",
      "\n",
      "current lr 8.85257e-04\n",
      "Epoch: [23][0/48]\tTime 0.012 (0.012)\tLoss 1.4564 (1.4564)\tPrec@1 48.047 (48.047)\n",
      "Epoch: [23][9/48]\tTime 0.005 (0.005)\tLoss 1.4968 (1.4998)\tPrec@1 49.121 (48.037)\n",
      "Epoch: [23][18/48]\tTime 0.006 (0.005)\tLoss 1.4863 (1.5015)\tPrec@1 47.363 (47.810)\n",
      "Epoch: [23][27/48]\tTime 0.010 (0.005)\tLoss 1.4379 (1.4978)\tPrec@1 51.953 (48.057)\n",
      "Epoch: [23][36/48]\tTime 0.003 (0.006)\tLoss 1.4619 (1.4985)\tPrec@1 51.660 (47.936)\n",
      "Epoch: [23][45/48]\tTime 0.003 (0.006)\tLoss 1.4802 (1.4983)\tPrec@1 46.875 (47.830)\n",
      "Epoch: [23][48/48]\tTime 0.004 (0.006)\tLoss 1.5244 (1.4993)\tPrec@1 45.165 (47.736)\n",
      "EPOCH: 23 train Results: Prec@1 47.736 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.5223 (1.5223)\tPrec@1 45.898 (45.898)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.5312 (1.5161)\tPrec@1 45.408 (47.150)\n",
      "EPOCH: 23 val Results: Prec@1 47.150 \n",
      "Best Prec@1: 47.150\n",
      "\n",
      "current lr 8.75056e-04\n",
      "Epoch: [24][0/48]\tTime 0.003 (0.003)\tLoss 1.4745 (1.4745)\tPrec@1 47.949 (47.949)\n",
      "Epoch: [24][9/48]\tTime 0.003 (0.005)\tLoss 1.5013 (1.4773)\tPrec@1 47.559 (48.320)\n",
      "Epoch: [24][18/48]\tTime 0.006 (0.005)\tLoss 1.4846 (1.4850)\tPrec@1 48.340 (47.995)\n",
      "Epoch: [24][27/48]\tTime 0.003 (0.005)\tLoss 1.5316 (1.4928)\tPrec@1 47.656 (47.775)\n",
      "Epoch: [24][36/48]\tTime 0.006 (0.005)\tLoss 1.4944 (1.4913)\tPrec@1 45.996 (47.801)\n",
      "Epoch: [24][45/48]\tTime 0.003 (0.005)\tLoss 1.4670 (1.4878)\tPrec@1 47.852 (47.960)\n",
      "Epoch: [24][48/48]\tTime 0.002 (0.005)\tLoss 1.4824 (1.4893)\tPrec@1 47.406 (47.908)\n",
      "EPOCH: 24 train Results: Prec@1 47.908 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.5149 (1.5149)\tPrec@1 45.898 (45.898)\n",
      "Test: [9/9]\tTime 0.002 (0.001)\tLoss 1.5230 (1.5072)\tPrec@1 44.898 (47.250)\n",
      "EPOCH: 24 val Results: Prec@1 47.250 \n",
      "Best Prec@1: 47.250\n",
      "\n",
      "current lr 8.64484e-04\n",
      "Epoch: [25][0/48]\tTime 0.006 (0.006)\tLoss 1.4623 (1.4623)\tPrec@1 48.535 (48.535)\n",
      "Epoch: [25][9/48]\tTime 0.003 (0.004)\tLoss 1.4250 (1.4762)\tPrec@1 48.730 (48.174)\n",
      "Epoch: [25][18/48]\tTime 0.004 (0.004)\tLoss 1.4263 (1.4812)\tPrec@1 50.293 (48.206)\n",
      "Epoch: [25][27/48]\tTime 0.007 (0.005)\tLoss 1.4709 (1.4806)\tPrec@1 48.145 (48.113)\n",
      "Epoch: [25][36/48]\tTime 0.003 (0.005)\tLoss 1.4697 (1.4841)\tPrec@1 48.730 (48.086)\n",
      "Epoch: [25][45/48]\tTime 0.004 (0.005)\tLoss 1.4635 (1.4818)\tPrec@1 48.535 (48.253)\n",
      "Epoch: [25][48/48]\tTime 0.003 (0.005)\tLoss 1.4994 (1.4825)\tPrec@1 47.288 (48.224)\n",
      "EPOCH: 25 train Results: Prec@1 48.224 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.5030 (1.5030)\tPrec@1 47.168 (47.168)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.5152 (1.4994)\tPrec@1 44.770 (47.380)\n",
      "EPOCH: 25 val Results: Prec@1 47.380 \n",
      "Best Prec@1: 47.380\n",
      "\n",
      "current lr 8.53553e-04\n",
      "Epoch: [26][0/48]\tTime 0.006 (0.006)\tLoss 1.4602 (1.4602)\tPrec@1 48.730 (48.730)\n",
      "Epoch: [26][9/48]\tTime 0.005 (0.005)\tLoss 1.4562 (1.4683)\tPrec@1 48.047 (48.799)\n",
      "Epoch: [26][18/48]\tTime 0.005 (0.006)\tLoss 1.4529 (1.4744)\tPrec@1 50.488 (48.787)\n",
      "Epoch: [26][27/48]\tTime 0.008 (0.006)\tLoss 1.4396 (1.4721)\tPrec@1 49.023 (48.567)\n",
      "Epoch: [26][36/48]\tTime 0.003 (0.006)\tLoss 1.4085 (1.4729)\tPrec@1 52.148 (48.630)\n",
      "Epoch: [26][45/48]\tTime 0.003 (0.005)\tLoss 1.4531 (1.4719)\tPrec@1 51.172 (48.730)\n",
      "Epoch: [26][48/48]\tTime 0.003 (0.005)\tLoss 1.5000 (1.4721)\tPrec@1 46.698 (48.692)\n",
      "EPOCH: 26 train Results: Prec@1 48.692 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4944 (1.4944)\tPrec@1 46.875 (46.875)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.5072 (1.4907)\tPrec@1 45.663 (47.770)\n",
      "EPOCH: 26 val Results: Prec@1 47.770 \n",
      "Best Prec@1: 47.770\n",
      "\n",
      "current lr 8.42274e-04\n",
      "Epoch: [27][0/48]\tTime 0.013 (0.013)\tLoss 1.4802 (1.4802)\tPrec@1 47.070 (47.070)\n",
      "Epoch: [27][9/48]\tTime 0.006 (0.006)\tLoss 1.4487 (1.4471)\tPrec@1 49.609 (49.688)\n",
      "Epoch: [27][18/48]\tTime 0.005 (0.006)\tLoss 1.4913 (1.4519)\tPrec@1 47.266 (49.429)\n",
      "Epoch: [27][27/48]\tTime 0.005 (0.006)\tLoss 1.4624 (1.4532)\tPrec@1 48.730 (49.299)\n",
      "Epoch: [27][36/48]\tTime 0.004 (0.005)\tLoss 1.4828 (1.4617)\tPrec@1 48.926 (48.870)\n",
      "Epoch: [27][45/48]\tTime 0.002 (0.005)\tLoss 1.4852 (1.4623)\tPrec@1 48.438 (48.960)\n",
      "Epoch: [27][48/48]\tTime 0.011 (0.006)\tLoss 1.4811 (1.4637)\tPrec@1 46.934 (48.934)\n",
      "EPOCH: 27 train Results: Prec@1 48.934 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4836 (1.4836)\tPrec@1 46.680 (46.680)\n",
      "Test: [9/9]\tTime 0.006 (0.002)\tLoss 1.5005 (1.4833)\tPrec@1 45.281 (47.870)\n",
      "EPOCH: 27 val Results: Prec@1 47.870 \n",
      "Best Prec@1: 47.870\n",
      "\n",
      "current lr 8.30656e-04\n",
      "Epoch: [28][0/48]\tTime 0.004 (0.004)\tLoss 1.4105 (1.4105)\tPrec@1 48.535 (48.535)\n",
      "Epoch: [28][9/48]\tTime 0.002 (0.004)\tLoss 1.4091 (1.4560)\tPrec@1 49.707 (49.229)\n",
      "Epoch: [28][18/48]\tTime 0.005 (0.004)\tLoss 1.4609 (1.4544)\tPrec@1 47.754 (49.265)\n",
      "Epoch: [28][27/48]\tTime 0.003 (0.004)\tLoss 1.4369 (1.4591)\tPrec@1 50.391 (49.198)\n",
      "Epoch: [28][36/48]\tTime 0.003 (0.004)\tLoss 1.5152 (1.4573)\tPrec@1 46.484 (49.308)\n",
      "Epoch: [28][45/48]\tTime 0.011 (0.005)\tLoss 1.4485 (1.4576)\tPrec@1 49.121 (49.236)\n",
      "Epoch: [28][48/48]\tTime 0.008 (0.005)\tLoss 1.5014 (1.4576)\tPrec@1 47.406 (49.192)\n",
      "EPOCH: 28 train Results: Prec@1 49.192 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4765 (1.4765)\tPrec@1 47.656 (47.656)\n",
      "Test: [9/9]\tTime 0.005 (0.001)\tLoss 1.4959 (1.4769)\tPrec@1 45.536 (48.220)\n",
      "EPOCH: 28 val Results: Prec@1 48.220 \n",
      "Best Prec@1: 48.220\n",
      "\n",
      "current lr 8.18712e-04\n",
      "Epoch: [29][0/48]\tTime 0.010 (0.010)\tLoss 1.4135 (1.4135)\tPrec@1 50.586 (50.586)\n",
      "Epoch: [29][9/48]\tTime 0.004 (0.005)\tLoss 1.4059 (1.4497)\tPrec@1 50.684 (49.248)\n",
      "Epoch: [29][18/48]\tTime 0.005 (0.005)\tLoss 1.4471 (1.4491)\tPrec@1 49.707 (49.280)\n",
      "Epoch: [29][27/48]\tTime 0.003 (0.005)\tLoss 1.4426 (1.4492)\tPrec@1 49.121 (49.379)\n",
      "Epoch: [29][36/48]\tTime 0.005 (0.005)\tLoss 1.4741 (1.4523)\tPrec@1 46.973 (49.240)\n",
      "Epoch: [29][45/48]\tTime 0.005 (0.005)\tLoss 1.4220 (1.4497)\tPrec@1 52.734 (49.408)\n",
      "Epoch: [29][48/48]\tTime 0.002 (0.005)\tLoss 1.4735 (1.4505)\tPrec@1 48.349 (49.330)\n",
      "EPOCH: 29 train Results: Prec@1 49.330 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4666 (1.4666)\tPrec@1 48.145 (48.145)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4910 (1.4703)\tPrec@1 46.173 (48.580)\n",
      "EPOCH: 29 val Results: Prec@1 48.580 \n",
      "Best Prec@1: 48.580\n",
      "\n",
      "current lr 8.06454e-04\n",
      "Epoch: [30][0/48]\tTime 0.003 (0.003)\tLoss 1.4695 (1.4695)\tPrec@1 47.168 (47.168)\n",
      "Epoch: [30][9/48]\tTime 0.003 (0.006)\tLoss 1.4647 (1.4424)\tPrec@1 49.121 (49.199)\n",
      "Epoch: [30][18/48]\tTime 0.005 (0.005)\tLoss 1.4141 (1.4417)\tPrec@1 50.781 (49.522)\n",
      "Epoch: [30][27/48]\tTime 0.021 (0.006)\tLoss 1.4178 (1.4388)\tPrec@1 48.828 (49.655)\n",
      "Epoch: [30][36/48]\tTime 0.003 (0.006)\tLoss 1.4379 (1.4392)\tPrec@1 49.023 (49.580)\n",
      "Epoch: [30][45/48]\tTime 0.009 (0.005)\tLoss 1.4368 (1.4410)\tPrec@1 48.730 (49.501)\n",
      "Epoch: [30][48/48]\tTime 0.007 (0.005)\tLoss 1.4429 (1.4420)\tPrec@1 49.882 (49.432)\n",
      "EPOCH: 30 train Results: Prec@1 49.432 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4600 (1.4600)\tPrec@1 48.145 (48.145)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4848 (1.4631)\tPrec@1 47.194 (48.790)\n",
      "EPOCH: 30 val Results: Prec@1 48.790 \n",
      "Best Prec@1: 48.790\n",
      "\n",
      "current lr 7.93893e-04\n",
      "Epoch: [31][0/48]\tTime 0.007 (0.007)\tLoss 1.3799 (1.3799)\tPrec@1 52.246 (52.246)\n",
      "Epoch: [31][9/48]\tTime 0.003 (0.007)\tLoss 1.4306 (1.4318)\tPrec@1 50.781 (50.459)\n",
      "Epoch: [31][18/48]\tTime 0.009 (0.007)\tLoss 1.3995 (1.4283)\tPrec@1 50.684 (50.272)\n",
      "Epoch: [31][27/48]\tTime 0.003 (0.006)\tLoss 1.4311 (1.4294)\tPrec@1 51.074 (50.349)\n",
      "Epoch: [31][36/48]\tTime 0.002 (0.006)\tLoss 1.3862 (1.4296)\tPrec@1 51.270 (50.203)\n",
      "Epoch: [31][45/48]\tTime 0.003 (0.005)\tLoss 1.4885 (1.4340)\tPrec@1 46.875 (49.911)\n",
      "Epoch: [31][48/48]\tTime 0.006 (0.005)\tLoss 1.4813 (1.4351)\tPrec@1 46.698 (49.836)\n",
      "EPOCH: 31 train Results: Prec@1 49.836 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4535 (1.4535)\tPrec@1 48.242 (48.242)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4778 (1.4575)\tPrec@1 46.939 (48.980)\n",
      "EPOCH: 31 val Results: Prec@1 48.980 \n",
      "Best Prec@1: 48.980\n",
      "\n",
      "current lr 7.81042e-04\n",
      "Epoch: [32][0/48]\tTime 0.012 (0.012)\tLoss 1.4314 (1.4314)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [32][9/48]\tTime 0.004 (0.005)\tLoss 1.4172 (1.4333)\tPrec@1 49.609 (50.049)\n",
      "Epoch: [32][18/48]\tTime 0.009 (0.006)\tLoss 1.3784 (1.4260)\tPrec@1 50.586 (49.995)\n",
      "Epoch: [32][27/48]\tTime 0.007 (0.006)\tLoss 1.4288 (1.4269)\tPrec@1 50.293 (50.070)\n",
      "Epoch: [32][36/48]\tTime 0.009 (0.005)\tLoss 1.3859 (1.4248)\tPrec@1 54.102 (50.058)\n",
      "Epoch: [32][45/48]\tTime 0.003 (0.005)\tLoss 1.4282 (1.4274)\tPrec@1 49.805 (50.104)\n",
      "Epoch: [32][48/48]\tTime 0.002 (0.005)\tLoss 1.4364 (1.4278)\tPrec@1 50.943 (50.106)\n",
      "EPOCH: 32 train Results: Prec@1 50.106 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4464 (1.4464)\tPrec@1 48.535 (48.535)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4744 (1.4522)\tPrec@1 47.449 (49.020)\n",
      "EPOCH: 32 val Results: Prec@1 49.020 \n",
      "Best Prec@1: 49.020\n",
      "\n",
      "current lr 7.67913e-04\n",
      "Epoch: [33][0/48]\tTime 0.003 (0.003)\tLoss 1.4533 (1.4533)\tPrec@1 47.168 (47.168)\n",
      "Epoch: [33][9/48]\tTime 0.003 (0.005)\tLoss 1.4816 (1.4222)\tPrec@1 48.926 (50.508)\n",
      "Epoch: [33][18/48]\tTime 0.003 (0.007)\tLoss 1.4099 (1.4186)\tPrec@1 50.781 (50.745)\n",
      "Epoch: [33][27/48]\tTime 0.010 (0.007)\tLoss 1.3735 (1.4152)\tPrec@1 53.418 (50.614)\n",
      "Epoch: [33][36/48]\tTime 0.004 (0.006)\tLoss 1.4310 (1.4210)\tPrec@1 49.902 (50.346)\n",
      "Epoch: [33][45/48]\tTime 0.004 (0.006)\tLoss 1.4724 (1.4213)\tPrec@1 49.219 (50.367)\n",
      "Epoch: [33][48/48]\tTime 0.002 (0.006)\tLoss 1.4378 (1.4218)\tPrec@1 47.759 (50.288)\n",
      "EPOCH: 33 train Results: Prec@1 50.288 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4408 (1.4408)\tPrec@1 48.828 (48.828)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4714 (1.4472)\tPrec@1 47.066 (49.240)\n",
      "EPOCH: 33 val Results: Prec@1 49.240 \n",
      "Best Prec@1: 49.240\n",
      "\n",
      "current lr 7.54521e-04\n",
      "Epoch: [34][0/48]\tTime 0.008 (0.008)\tLoss 1.4358 (1.4358)\tPrec@1 51.172 (51.172)\n",
      "Epoch: [34][9/48]\tTime 0.006 (0.007)\tLoss 1.3933 (1.4067)\tPrec@1 50.391 (51.201)\n",
      "Epoch: [34][18/48]\tTime 0.003 (0.008)\tLoss 1.4632 (1.4082)\tPrec@1 48.535 (51.069)\n",
      "Epoch: [34][27/48]\tTime 0.004 (0.006)\tLoss 1.4271 (1.4121)\tPrec@1 50.293 (50.698)\n",
      "Epoch: [34][36/48]\tTime 0.003 (0.006)\tLoss 1.4406 (1.4154)\tPrec@1 51.074 (50.707)\n",
      "Epoch: [34][45/48]\tTime 0.002 (0.005)\tLoss 1.3855 (1.4183)\tPrec@1 52.637 (50.505)\n",
      "Epoch: [34][48/48]\tTime 0.011 (0.006)\tLoss 1.4575 (1.4184)\tPrec@1 48.349 (50.510)\n",
      "EPOCH: 34 train Results: Prec@1 50.510 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4348 (1.4348)\tPrec@1 48.047 (48.047)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4643 (1.4414)\tPrec@1 47.704 (49.530)\n",
      "EPOCH: 34 val Results: Prec@1 49.530 \n",
      "Best Prec@1: 49.530\n",
      "\n",
      "current lr 7.40877e-04\n",
      "Epoch: [35][0/48]\tTime 0.007 (0.007)\tLoss 1.3797 (1.3797)\tPrec@1 54.102 (54.102)\n",
      "Epoch: [35][9/48]\tTime 0.008 (0.007)\tLoss 1.3981 (1.4101)\tPrec@1 50.488 (50.938)\n",
      "Epoch: [35][18/48]\tTime 0.004 (0.006)\tLoss 1.3778 (1.4035)\tPrec@1 50.586 (50.848)\n",
      "Epoch: [35][27/48]\tTime 0.008 (0.006)\tLoss 1.4064 (1.4056)\tPrec@1 50.586 (50.795)\n",
      "Epoch: [35][36/48]\tTime 0.019 (0.006)\tLoss 1.4760 (1.4082)\tPrec@1 49.219 (50.583)\n",
      "Epoch: [35][45/48]\tTime 0.004 (0.006)\tLoss 1.4835 (1.4121)\tPrec@1 47.949 (50.620)\n",
      "Epoch: [35][48/48]\tTime 0.002 (0.006)\tLoss 1.3495 (1.4121)\tPrec@1 52.830 (50.674)\n",
      "EPOCH: 35 train Results: Prec@1 50.674 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4284 (1.4284)\tPrec@1 48.438 (48.438)\n",
      "Test: [9/9]\tTime 0.003 (0.001)\tLoss 1.4577 (1.4354)\tPrec@1 47.321 (49.760)\n",
      "EPOCH: 35 val Results: Prec@1 49.760 \n",
      "Best Prec@1: 49.760\n",
      "\n",
      "current lr 7.26995e-04\n",
      "Epoch: [36][0/48]\tTime 0.007 (0.007)\tLoss 1.4224 (1.4224)\tPrec@1 50.391 (50.391)\n",
      "Epoch: [36][9/48]\tTime 0.003 (0.006)\tLoss 1.3864 (1.4017)\tPrec@1 50.684 (50.762)\n",
      "Epoch: [36][18/48]\tTime 0.012 (0.007)\tLoss 1.3996 (1.4027)\tPrec@1 52.441 (50.961)\n",
      "Epoch: [36][27/48]\tTime 0.003 (0.007)\tLoss 1.3799 (1.3987)\tPrec@1 51.953 (51.270)\n",
      "Epoch: [36][36/48]\tTime 0.006 (0.007)\tLoss 1.4504 (1.4003)\tPrec@1 51.074 (51.188)\n",
      "Epoch: [36][45/48]\tTime 0.010 (0.007)\tLoss 1.4287 (1.4038)\tPrec@1 50.586 (51.036)\n",
      "Epoch: [36][48/48]\tTime 0.010 (0.007)\tLoss 1.4591 (1.4045)\tPrec@1 48.349 (50.974)\n",
      "EPOCH: 36 train Results: Prec@1 50.974 \n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.4227 (1.4227)\tPrec@1 49.121 (49.121)\n",
      "Test: [9/9]\tTime 0.001 (0.004)\tLoss 1.4547 (1.4315)\tPrec@1 47.577 (49.930)\n",
      "EPOCH: 36 val Results: Prec@1 49.930 \n",
      "Best Prec@1: 49.930\n",
      "\n",
      "current lr 7.12890e-04\n",
      "Epoch: [37][0/48]\tTime 0.011 (0.011)\tLoss 1.3266 (1.3266)\tPrec@1 54.199 (54.199)\n",
      "Epoch: [37][9/48]\tTime 0.007 (0.006)\tLoss 1.3705 (1.3914)\tPrec@1 50.684 (51.719)\n",
      "Epoch: [37][18/48]\tTime 0.004 (0.006)\tLoss 1.4258 (1.3991)\tPrec@1 50.195 (51.306)\n",
      "Epoch: [37][27/48]\tTime 0.006 (0.006)\tLoss 1.4287 (1.3972)\tPrec@1 50.977 (51.371)\n",
      "Epoch: [37][36/48]\tTime 0.004 (0.006)\tLoss 1.4125 (1.4062)\tPrec@1 50.098 (50.921)\n",
      "Epoch: [37][45/48]\tTime 0.011 (0.006)\tLoss 1.3777 (1.4044)\tPrec@1 52.246 (50.972)\n",
      "Epoch: [37][48/48]\tTime 0.002 (0.006)\tLoss 1.4035 (1.4031)\tPrec@1 52.241 (51.060)\n",
      "EPOCH: 37 train Results: Prec@1 51.060 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4195 (1.4195)\tPrec@1 49.121 (49.121)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4488 (1.4267)\tPrec@1 47.577 (50.280)\n",
      "EPOCH: 37 val Results: Prec@1 50.280 \n",
      "Best Prec@1: 50.280\n",
      "\n",
      "current lr 6.98574e-04\n",
      "Epoch: [38][0/48]\tTime 0.003 (0.003)\tLoss 1.3815 (1.3815)\tPrec@1 51.758 (51.758)\n",
      "Epoch: [38][9/48]\tTime 0.003 (0.004)\tLoss 1.3959 (1.4052)\tPrec@1 49.707 (50.439)\n",
      "Epoch: [38][18/48]\tTime 0.008 (0.005)\tLoss 1.3359 (1.4005)\tPrec@1 53.223 (50.971)\n",
      "Epoch: [38][27/48]\tTime 0.003 (0.006)\tLoss 1.3445 (1.3969)\tPrec@1 52.734 (51.332)\n",
      "Epoch: [38][36/48]\tTime 0.003 (0.006)\tLoss 1.3949 (1.3925)\tPrec@1 51.270 (51.489)\n",
      "Epoch: [38][45/48]\tTime 0.006 (0.006)\tLoss 1.4537 (1.3969)\tPrec@1 49.609 (51.178)\n",
      "Epoch: [38][48/48]\tTime 0.002 (0.006)\tLoss 1.4350 (1.3979)\tPrec@1 52.123 (51.180)\n",
      "EPOCH: 38 train Results: Prec@1 51.180 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4093 (1.4093)\tPrec@1 50.293 (50.293)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4432 (1.4223)\tPrec@1 47.704 (50.340)\n",
      "EPOCH: 38 val Results: Prec@1 50.340 \n",
      "Best Prec@1: 50.340\n",
      "\n",
      "current lr 6.84062e-04\n",
      "Epoch: [39][0/48]\tTime 0.008 (0.008)\tLoss 1.4024 (1.4024)\tPrec@1 50.488 (50.488)\n",
      "Epoch: [39][9/48]\tTime 0.007 (0.005)\tLoss 1.3873 (1.3823)\tPrec@1 52.051 (51.562)\n",
      "Epoch: [39][18/48]\tTime 0.003 (0.005)\tLoss 1.4049 (1.3917)\tPrec@1 50.684 (51.306)\n",
      "Epoch: [39][27/48]\tTime 0.009 (0.006)\tLoss 1.3993 (1.3933)\tPrec@1 50.195 (51.273)\n",
      "Epoch: [39][36/48]\tTime 0.003 (0.008)\tLoss 1.3690 (1.3891)\tPrec@1 52.637 (51.549)\n",
      "Epoch: [39][45/48]\tTime 0.003 (0.008)\tLoss 1.3955 (1.3914)\tPrec@1 49.121 (51.433)\n",
      "Epoch: [39][48/48]\tTime 0.008 (0.008)\tLoss 1.4386 (1.3938)\tPrec@1 51.179 (51.418)\n",
      "EPOCH: 39 train Results: Prec@1 51.418 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4069 (1.4069)\tPrec@1 49.707 (49.707)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4403 (1.4188)\tPrec@1 47.577 (50.110)\n",
      "EPOCH: 39 val Results: Prec@1 50.110 \n",
      "Best Prec@1: 50.340\n",
      "\n",
      "current lr 6.69369e-04\n",
      "Epoch: [40][0/48]\tTime 0.006 (0.006)\tLoss 1.3934 (1.3934)\tPrec@1 50.391 (50.391)\n",
      "Epoch: [40][9/48]\tTime 0.002 (0.004)\tLoss 1.3415 (1.3810)\tPrec@1 52.441 (51.924)\n",
      "Epoch: [40][18/48]\tTime 0.003 (0.005)\tLoss 1.3748 (1.3750)\tPrec@1 51.758 (52.138)\n",
      "Epoch: [40][27/48]\tTime 0.015 (0.006)\tLoss 1.3964 (1.3780)\tPrec@1 51.758 (52.079)\n",
      "Epoch: [40][36/48]\tTime 0.005 (0.006)\tLoss 1.3669 (1.3837)\tPrec@1 51.367 (51.702)\n",
      "Epoch: [40][45/48]\tTime 0.003 (0.006)\tLoss 1.3729 (1.3887)\tPrec@1 50.684 (51.414)\n",
      "Epoch: [40][48/48]\tTime 0.007 (0.006)\tLoss 1.3981 (1.3888)\tPrec@1 50.943 (51.418)\n",
      "EPOCH: 40 train Results: Prec@1 51.418 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4012 (1.4012)\tPrec@1 50.488 (50.488)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4379 (1.4149)\tPrec@1 47.577 (50.530)\n",
      "EPOCH: 40 val Results: Prec@1 50.530 \n",
      "Best Prec@1: 50.530\n",
      "\n",
      "current lr 6.54508e-04\n",
      "Epoch: [41][0/48]\tTime 0.007 (0.007)\tLoss 1.4063 (1.4063)\tPrec@1 48.730 (48.730)\n",
      "Epoch: [41][9/48]\tTime 0.003 (0.004)\tLoss 1.3573 (1.3741)\tPrec@1 54.883 (51.699)\n",
      "Epoch: [41][18/48]\tTime 0.007 (0.005)\tLoss 1.4286 (1.3771)\tPrec@1 50.977 (51.866)\n",
      "Epoch: [41][27/48]\tTime 0.003 (0.006)\tLoss 1.3243 (1.3785)\tPrec@1 54.883 (51.950)\n",
      "Epoch: [41][36/48]\tTime 0.005 (0.006)\tLoss 1.3628 (1.3831)\tPrec@1 51.855 (51.708)\n",
      "Epoch: [41][45/48]\tTime 0.003 (0.005)\tLoss 1.3960 (1.3843)\tPrec@1 50.000 (51.626)\n",
      "Epoch: [41][48/48]\tTime 0.010 (0.006)\tLoss 1.3932 (1.3869)\tPrec@1 50.708 (51.526)\n",
      "EPOCH: 41 train Results: Prec@1 51.526 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3967 (1.3967)\tPrec@1 50.293 (50.293)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4367 (1.4130)\tPrec@1 47.959 (50.580)\n",
      "EPOCH: 41 val Results: Prec@1 50.580 \n",
      "Best Prec@1: 50.580\n",
      "\n",
      "current lr 6.39496e-04\n",
      "Epoch: [42][0/48]\tTime 0.008 (0.008)\tLoss 1.3638 (1.3638)\tPrec@1 52.246 (52.246)\n",
      "Epoch: [42][9/48]\tTime 0.003 (0.005)\tLoss 1.4193 (1.3774)\tPrec@1 49.316 (52.080)\n",
      "Epoch: [42][18/48]\tTime 0.008 (0.006)\tLoss 1.3770 (1.3763)\tPrec@1 52.148 (52.210)\n",
      "Epoch: [42][27/48]\tTime 0.007 (0.006)\tLoss 1.3675 (1.3802)\tPrec@1 52.930 (52.093)\n",
      "Epoch: [42][36/48]\tTime 0.011 (0.006)\tLoss 1.3993 (1.3790)\tPrec@1 49.609 (51.969)\n",
      "Epoch: [42][45/48]\tTime 0.003 (0.005)\tLoss 1.3700 (1.3828)\tPrec@1 51.270 (51.722)\n",
      "Epoch: [42][48/48]\tTime 0.004 (0.005)\tLoss 1.3550 (1.3826)\tPrec@1 53.656 (51.710)\n",
      "EPOCH: 42 train Results: Prec@1 51.710 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3957 (1.3957)\tPrec@1 50.684 (50.684)\n",
      "Test: [9/9]\tTime 0.000 (0.002)\tLoss 1.4324 (1.4098)\tPrec@1 47.704 (50.720)\n",
      "EPOCH: 42 val Results: Prec@1 50.720 \n",
      "Best Prec@1: 50.720\n",
      "\n",
      "current lr 6.24345e-04\n",
      "Epoch: [43][0/48]\tTime 0.003 (0.003)\tLoss 1.3591 (1.3591)\tPrec@1 53.613 (53.613)\n",
      "Epoch: [43][9/48]\tTime 0.006 (0.005)\tLoss 1.3778 (1.3782)\tPrec@1 51.074 (51.768)\n",
      "Epoch: [43][18/48]\tTime 0.008 (0.006)\tLoss 1.3434 (1.3864)\tPrec@1 52.441 (51.465)\n",
      "Epoch: [43][27/48]\tTime 0.006 (0.007)\tLoss 1.3477 (1.3811)\tPrec@1 54.688 (51.779)\n",
      "Epoch: [43][36/48]\tTime 0.002 (0.007)\tLoss 1.4077 (1.3826)\tPrec@1 50.977 (51.737)\n",
      "Epoch: [43][45/48]\tTime 0.003 (0.006)\tLoss 1.4402 (1.3843)\tPrec@1 49.219 (51.607)\n",
      "Epoch: [43][48/48]\tTime 0.002 (0.006)\tLoss 1.3294 (1.3829)\tPrec@1 54.009 (51.686)\n",
      "EPOCH: 43 train Results: Prec@1 51.686 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3908 (1.3908)\tPrec@1 51.562 (51.562)\n",
      "Test: [9/9]\tTime 0.002 (0.001)\tLoss 1.4307 (1.4066)\tPrec@1 47.704 (51.080)\n",
      "EPOCH: 43 val Results: Prec@1 51.080 \n",
      "Best Prec@1: 51.080\n",
      "\n",
      "current lr 6.09072e-04\n",
      "Epoch: [44][0/48]\tTime 0.010 (0.010)\tLoss 1.3787 (1.3787)\tPrec@1 52.637 (52.637)\n",
      "Epoch: [44][9/48]\tTime 0.003 (0.005)\tLoss 1.3083 (1.3672)\tPrec@1 54.883 (52.529)\n",
      "Epoch: [44][18/48]\tTime 0.003 (0.005)\tLoss 1.3962 (1.3661)\tPrec@1 52.051 (52.421)\n",
      "Epoch: [44][27/48]\tTime 0.004 (0.005)\tLoss 1.3727 (1.3758)\tPrec@1 51.562 (52.134)\n",
      "Epoch: [44][36/48]\tTime 0.012 (0.005)\tLoss 1.3304 (1.3762)\tPrec@1 54.688 (52.011)\n",
      "Epoch: [44][45/48]\tTime 0.003 (0.005)\tLoss 1.3983 (1.3779)\tPrec@1 50.293 (51.983)\n",
      "Epoch: [44][48/48]\tTime 0.002 (0.005)\tLoss 1.3439 (1.3765)\tPrec@1 51.769 (52.012)\n",
      "EPOCH: 44 train Results: Prec@1 52.012 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3907 (1.3907)\tPrec@1 50.781 (50.781)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4298 (1.4051)\tPrec@1 48.087 (50.870)\n",
      "EPOCH: 44 val Results: Prec@1 50.870 \n",
      "Best Prec@1: 51.080\n",
      "\n",
      "current lr 5.93691e-04\n",
      "Epoch: [45][0/48]\tTime 0.003 (0.003)\tLoss 1.3352 (1.3352)\tPrec@1 54.590 (54.590)\n",
      "Epoch: [45][9/48]\tTime 0.003 (0.004)\tLoss 1.3788 (1.3684)\tPrec@1 51.074 (52.188)\n",
      "Epoch: [45][18/48]\tTime 0.007 (0.005)\tLoss 1.3914 (1.3671)\tPrec@1 50.293 (52.159)\n",
      "Epoch: [45][27/48]\tTime 0.003 (0.005)\tLoss 1.3430 (1.3718)\tPrec@1 51.758 (52.277)\n",
      "Epoch: [45][36/48]\tTime 0.002 (0.005)\tLoss 1.3992 (1.3741)\tPrec@1 52.148 (52.090)\n",
      "Epoch: [45][45/48]\tTime 0.003 (0.005)\tLoss 1.3967 (1.3747)\tPrec@1 49.219 (52.038)\n",
      "Epoch: [45][48/48]\tTime 0.005 (0.005)\tLoss 1.3640 (1.3749)\tPrec@1 54.009 (52.058)\n",
      "EPOCH: 45 train Results: Prec@1 52.058 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.3869 (1.3869)\tPrec@1 51.270 (51.270)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4277 (1.4014)\tPrec@1 47.321 (51.100)\n",
      "EPOCH: 45 val Results: Prec@1 51.100 \n",
      "Best Prec@1: 51.100\n",
      "\n",
      "current lr 5.78217e-04\n",
      "Epoch: [46][0/48]\tTime 0.005 (0.005)\tLoss 1.3823 (1.3823)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [46][9/48]\tTime 0.003 (0.003)\tLoss 1.4242 (1.3769)\tPrec@1 50.098 (52.344)\n",
      "Epoch: [46][18/48]\tTime 0.006 (0.004)\tLoss 1.4114 (1.3703)\tPrec@1 49.414 (52.148)\n",
      "Epoch: [46][27/48]\tTime 0.002 (0.004)\tLoss 1.3546 (1.3694)\tPrec@1 50.977 (51.922)\n",
      "Epoch: [46][36/48]\tTime 0.003 (0.004)\tLoss 1.3743 (1.3718)\tPrec@1 52.539 (51.877)\n",
      "Epoch: [46][45/48]\tTime 0.003 (0.005)\tLoss 1.3447 (1.3682)\tPrec@1 54.980 (52.136)\n",
      "Epoch: [46][48/48]\tTime 0.003 (0.005)\tLoss 1.3557 (1.3697)\tPrec@1 52.948 (52.064)\n",
      "EPOCH: 46 train Results: Prec@1 52.064 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3844 (1.3844)\tPrec@1 51.172 (51.172)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4246 (1.3995)\tPrec@1 48.342 (51.120)\n",
      "EPOCH: 46 val Results: Prec@1 51.120 \n",
      "Best Prec@1: 51.120\n",
      "\n",
      "current lr 5.62667e-04\n",
      "Epoch: [47][0/48]\tTime 0.009 (0.009)\tLoss 1.3856 (1.3856)\tPrec@1 50.488 (50.488)\n",
      "Epoch: [47][9/48]\tTime 0.003 (0.005)\tLoss 1.3467 (1.3546)\tPrec@1 51.953 (52.490)\n",
      "Epoch: [47][18/48]\tTime 0.003 (0.004)\tLoss 1.3132 (1.3558)\tPrec@1 53.223 (52.554)\n",
      "Epoch: [47][27/48]\tTime 0.005 (0.004)\tLoss 1.3635 (1.3613)\tPrec@1 51.758 (52.518)\n",
      "Epoch: [47][36/48]\tTime 0.004 (0.004)\tLoss 1.3899 (1.3627)\tPrec@1 49.805 (52.410)\n",
      "Epoch: [47][45/48]\tTime 0.003 (0.004)\tLoss 1.3688 (1.3677)\tPrec@1 52.051 (52.197)\n",
      "Epoch: [47][48/48]\tTime 0.003 (0.004)\tLoss 1.3668 (1.3683)\tPrec@1 51.533 (52.172)\n",
      "EPOCH: 47 train Results: Prec@1 52.172 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3845 (1.3845)\tPrec@1 51.953 (51.953)\n",
      "Test: [9/9]\tTime 0.002 (0.002)\tLoss 1.4228 (1.3984)\tPrec@1 47.832 (51.150)\n",
      "EPOCH: 47 val Results: Prec@1 51.150 \n",
      "Best Prec@1: 51.150\n",
      "\n",
      "current lr 5.47054e-04\n",
      "Epoch: [48][0/48]\tTime 0.004 (0.004)\tLoss 1.3723 (1.3723)\tPrec@1 51.270 (51.270)\n",
      "Epoch: [48][9/48]\tTime 0.004 (0.006)\tLoss 1.3594 (1.3737)\tPrec@1 52.051 (52.314)\n",
      "Epoch: [48][18/48]\tTime 0.006 (0.005)\tLoss 1.3305 (1.3634)\tPrec@1 54.395 (52.688)\n",
      "Epoch: [48][27/48]\tTime 0.003 (0.004)\tLoss 1.3529 (1.3600)\tPrec@1 53.027 (52.891)\n",
      "Epoch: [48][36/48]\tTime 0.005 (0.004)\tLoss 1.3615 (1.3638)\tPrec@1 53.320 (52.618)\n",
      "Epoch: [48][45/48]\tTime 0.004 (0.004)\tLoss 1.4107 (1.3658)\tPrec@1 49.414 (52.552)\n",
      "Epoch: [48][48/48]\tTime 0.004 (0.005)\tLoss 1.4089 (1.3669)\tPrec@1 51.179 (52.478)\n",
      "EPOCH: 48 train Results: Prec@1 52.478 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3793 (1.3793)\tPrec@1 51.562 (51.562)\n",
      "Test: [9/9]\tTime 0.002 (0.001)\tLoss 1.4211 (1.3942)\tPrec@1 48.214 (51.340)\n",
      "EPOCH: 48 val Results: Prec@1 51.340 \n",
      "Best Prec@1: 51.340\n",
      "\n",
      "current lr 5.31395e-04\n",
      "Epoch: [49][0/48]\tTime 0.006 (0.006)\tLoss 1.3649 (1.3649)\tPrec@1 52.930 (52.930)\n",
      "Epoch: [49][9/48]\tTime 0.002 (0.006)\tLoss 1.3422 (1.3544)\tPrec@1 54.590 (52.910)\n",
      "Epoch: [49][18/48]\tTime 0.003 (0.005)\tLoss 1.3801 (1.3587)\tPrec@1 51.855 (52.868)\n",
      "Epoch: [49][27/48]\tTime 0.003 (0.005)\tLoss 1.3678 (1.3623)\tPrec@1 52.832 (52.836)\n",
      "Epoch: [49][36/48]\tTime 0.079 (0.007)\tLoss 1.3692 (1.3623)\tPrec@1 53.320 (52.861)\n",
      "Epoch: [49][45/48]\tTime 0.006 (0.007)\tLoss 1.4270 (1.3635)\tPrec@1 48.926 (52.628)\n",
      "Epoch: [49][48/48]\tTime 0.002 (0.007)\tLoss 1.3816 (1.3634)\tPrec@1 50.590 (52.566)\n",
      "EPOCH: 49 train Results: Prec@1 52.566 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3799 (1.3799)\tPrec@1 51.953 (51.953)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4232 (1.3943)\tPrec@1 47.832 (51.340)\n",
      "EPOCH: 49 val Results: Prec@1 51.340 \n",
      "Best Prec@1: 51.340\n",
      "\n",
      "current lr 5.15705e-04\n",
      "Epoch: [50][0/48]\tTime 0.006 (0.006)\tLoss 1.3546 (1.3546)\tPrec@1 54.590 (54.590)\n",
      "Epoch: [50][9/48]\tTime 0.009 (0.004)\tLoss 1.3733 (1.3543)\tPrec@1 52.637 (53.682)\n",
      "Epoch: [50][18/48]\tTime 0.004 (0.004)\tLoss 1.3457 (1.3535)\tPrec@1 54.199 (53.289)\n",
      "Epoch: [50][27/48]\tTime 0.003 (0.004)\tLoss 1.3183 (1.3549)\tPrec@1 54.199 (52.902)\n",
      "Epoch: [50][36/48]\tTime 0.010 (0.005)\tLoss 1.3700 (1.3581)\tPrec@1 52.832 (52.732)\n",
      "Epoch: [50][45/48]\tTime 0.005 (0.005)\tLoss 1.3490 (1.3578)\tPrec@1 52.832 (52.654)\n",
      "Epoch: [50][48/48]\tTime 0.002 (0.005)\tLoss 1.4393 (1.3606)\tPrec@1 52.241 (52.568)\n",
      "EPOCH: 50 train Results: Prec@1 52.568 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3780 (1.3780)\tPrec@1 51.855 (51.855)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4216 (1.3923)\tPrec@1 48.980 (51.380)\n",
      "EPOCH: 50 val Results: Prec@1 51.380 \n",
      "Best Prec@1: 51.380\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [51][0/48]\tTime 0.003 (0.003)\tLoss 1.3821 (1.3821)\tPrec@1 48.828 (48.828)\n",
      "Epoch: [51][9/48]\tTime 0.007 (0.004)\tLoss 1.3484 (1.3551)\tPrec@1 52.051 (52.393)\n",
      "Epoch: [51][18/48]\tTime 0.003 (0.004)\tLoss 1.3227 (1.3559)\tPrec@1 53.125 (52.452)\n",
      "Epoch: [51][27/48]\tTime 0.002 (0.004)\tLoss 1.3764 (1.3517)\tPrec@1 51.953 (52.776)\n",
      "Epoch: [51][36/48]\tTime 0.009 (0.004)\tLoss 1.3515 (1.3539)\tPrec@1 53.027 (52.708)\n",
      "Epoch: [51][45/48]\tTime 0.006 (0.005)\tLoss 1.3776 (1.3572)\tPrec@1 52.734 (52.560)\n",
      "Epoch: [51][48/48]\tTime 0.009 (0.005)\tLoss 1.3638 (1.3565)\tPrec@1 50.236 (52.540)\n",
      "EPOCH: 51 train Results: Prec@1 52.540 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3734 (1.3734)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.002 (0.002)\tLoss 1.4219 (1.3908)\tPrec@1 48.980 (51.520)\n",
      "EPOCH: 51 val Results: Prec@1 51.520 \n",
      "Best Prec@1: 51.520\n",
      "\n",
      "current lr 4.84295e-04\n",
      "Epoch: [52][0/48]\tTime 0.004 (0.004)\tLoss 1.3501 (1.3501)\tPrec@1 54.590 (54.590)\n",
      "Epoch: [52][9/48]\tTime 0.002 (0.005)\tLoss 1.3738 (1.3592)\tPrec@1 51.562 (53.516)\n",
      "Epoch: [52][18/48]\tTime 0.003 (0.006)\tLoss 1.3237 (1.3543)\tPrec@1 55.371 (53.433)\n",
      "Epoch: [52][27/48]\tTime 0.005 (0.007)\tLoss 1.3690 (1.3568)\tPrec@1 51.562 (53.097)\n",
      "Epoch: [52][36/48]\tTime 0.016 (0.007)\tLoss 1.3289 (1.3536)\tPrec@1 52.930 (53.157)\n",
      "Epoch: [52][45/48]\tTime 0.011 (0.006)\tLoss 1.4083 (1.3595)\tPrec@1 50.293 (52.743)\n",
      "Epoch: [52][48/48]\tTime 0.003 (0.006)\tLoss 1.4091 (1.3604)\tPrec@1 50.354 (52.678)\n",
      "EPOCH: 52 train Results: Prec@1 52.678 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3744 (1.3744)\tPrec@1 52.148 (52.148)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4138 (1.3890)\tPrec@1 48.852 (51.610)\n",
      "EPOCH: 52 val Results: Prec@1 51.610 \n",
      "Best Prec@1: 51.610\n",
      "\n",
      "current lr 4.68605e-04\n",
      "Epoch: [53][0/48]\tTime 0.006 (0.006)\tLoss 1.3327 (1.3327)\tPrec@1 54.297 (54.297)\n",
      "Epoch: [53][9/48]\tTime 0.003 (0.005)\tLoss 1.3499 (1.3420)\tPrec@1 53.418 (53.047)\n",
      "Epoch: [53][18/48]\tTime 0.002 (0.005)\tLoss 1.3462 (1.3466)\tPrec@1 55.371 (53.038)\n",
      "Epoch: [53][27/48]\tTime 0.004 (0.004)\tLoss 1.4192 (1.3518)\tPrec@1 49.219 (52.703)\n",
      "Epoch: [53][36/48]\tTime 0.008 (0.004)\tLoss 1.3600 (1.3520)\tPrec@1 51.855 (52.851)\n",
      "Epoch: [53][45/48]\tTime 0.004 (0.005)\tLoss 1.3403 (1.3550)\tPrec@1 51.465 (52.828)\n",
      "Epoch: [53][48/48]\tTime 0.005 (0.005)\tLoss 1.3953 (1.3565)\tPrec@1 53.892 (52.798)\n",
      "EPOCH: 53 train Results: Prec@1 52.798 \n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.3714 (1.3714)\tPrec@1 51.270 (51.270)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4178 (1.3889)\tPrec@1 48.597 (51.530)\n",
      "EPOCH: 53 val Results: Prec@1 51.530 \n",
      "Best Prec@1: 51.610\n",
      "\n",
      "current lr 4.52946e-04\n",
      "Epoch: [54][0/48]\tTime 0.010 (0.010)\tLoss 1.3379 (1.3379)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [54][9/48]\tTime 0.003 (0.007)\tLoss 1.3358 (1.3547)\tPrec@1 55.078 (52.471)\n",
      "Epoch: [54][18/48]\tTime 0.005 (0.006)\tLoss 1.3763 (1.3561)\tPrec@1 51.758 (52.539)\n",
      "Epoch: [54][27/48]\tTime 0.002 (0.006)\tLoss 1.3032 (1.3541)\tPrec@1 55.957 (52.863)\n",
      "Epoch: [54][36/48]\tTime 0.003 (0.005)\tLoss 1.2999 (1.3523)\tPrec@1 56.250 (53.059)\n",
      "Epoch: [54][45/48]\tTime 0.003 (0.005)\tLoss 1.3671 (1.3562)\tPrec@1 52.051 (52.866)\n",
      "Epoch: [54][48/48]\tTime 0.009 (0.005)\tLoss 1.3984 (1.3557)\tPrec@1 48.585 (52.822)\n",
      "EPOCH: 54 train Results: Prec@1 52.822 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3725 (1.3725)\tPrec@1 51.953 (51.953)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4134 (1.3872)\tPrec@1 48.342 (51.570)\n",
      "EPOCH: 54 val Results: Prec@1 51.570 \n",
      "Best Prec@1: 51.610\n",
      "\n",
      "current lr 4.37333e-04\n",
      "Epoch: [55][0/48]\tTime 0.004 (0.004)\tLoss 1.3448 (1.3448)\tPrec@1 53.223 (53.223)\n",
      "Epoch: [55][9/48]\tTime 0.006 (0.005)\tLoss 1.3165 (1.3393)\tPrec@1 54.688 (53.682)\n",
      "Epoch: [55][18/48]\tTime 0.005 (0.006)\tLoss 1.3359 (1.3485)\tPrec@1 52.832 (52.997)\n",
      "Epoch: [55][27/48]\tTime 0.002 (0.006)\tLoss 1.3668 (1.3535)\tPrec@1 53.125 (52.905)\n",
      "Epoch: [55][36/48]\tTime 0.007 (0.005)\tLoss 1.3003 (1.3517)\tPrec@1 55.371 (52.890)\n",
      "Epoch: [55][45/48]\tTime 0.005 (0.005)\tLoss 1.3474 (1.3521)\tPrec@1 53.613 (52.883)\n",
      "Epoch: [55][48/48]\tTime 0.002 (0.005)\tLoss 1.3752 (1.3522)\tPrec@1 52.712 (52.854)\n",
      "EPOCH: 55 train Results: Prec@1 52.854 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3709 (1.3709)\tPrec@1 52.734 (52.734)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4140 (1.3870)\tPrec@1 48.214 (51.630)\n",
      "EPOCH: 55 val Results: Prec@1 51.630 \n",
      "Best Prec@1: 51.630\n",
      "\n",
      "current lr 4.21783e-04\n",
      "Epoch: [56][0/48]\tTime 0.004 (0.004)\tLoss 1.3814 (1.3814)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [56][9/48]\tTime 0.007 (0.008)\tLoss 1.3318 (1.3596)\tPrec@1 54.785 (52.607)\n",
      "Epoch: [56][18/48]\tTime 0.005 (0.006)\tLoss 1.3252 (1.3489)\tPrec@1 54.395 (53.228)\n",
      "Epoch: [56][27/48]\tTime 0.009 (0.006)\tLoss 1.3506 (1.3469)\tPrec@1 53.613 (53.275)\n",
      "Epoch: [56][36/48]\tTime 0.002 (0.005)\tLoss 1.3486 (1.3493)\tPrec@1 53.027 (53.143)\n",
      "Epoch: [56][45/48]\tTime 0.007 (0.005)\tLoss 1.4046 (1.3522)\tPrec@1 52.539 (53.184)\n",
      "Epoch: [56][48/48]\tTime 0.002 (0.005)\tLoss 1.3547 (1.3521)\tPrec@1 54.127 (53.160)\n",
      "EPOCH: 56 train Results: Prec@1 53.160 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3702 (1.3702)\tPrec@1 51.953 (51.953)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4131 (1.3860)\tPrec@1 48.597 (51.540)\n",
      "EPOCH: 56 val Results: Prec@1 51.540 \n",
      "Best Prec@1: 51.630\n",
      "\n",
      "current lr 4.06309e-04\n",
      "Epoch: [57][0/48]\tTime 0.008 (0.008)\tLoss 1.3495 (1.3495)\tPrec@1 51.758 (51.758)\n",
      "Epoch: [57][9/48]\tTime 0.006 (0.005)\tLoss 1.2907 (1.3363)\tPrec@1 55.859 (53.379)\n",
      "Epoch: [57][18/48]\tTime 0.004 (0.006)\tLoss 1.3993 (1.3488)\tPrec@1 52.051 (53.300)\n",
      "Epoch: [57][27/48]\tTime 0.004 (0.005)\tLoss 1.3238 (1.3487)\tPrec@1 53.027 (53.153)\n",
      "Epoch: [57][36/48]\tTime 0.006 (0.005)\tLoss 1.3507 (1.3482)\tPrec@1 52.148 (53.009)\n",
      "Epoch: [57][45/48]\tTime 0.009 (0.005)\tLoss 1.3657 (1.3537)\tPrec@1 51.562 (52.756)\n",
      "Epoch: [57][48/48]\tTime 0.003 (0.005)\tLoss 1.3860 (1.3549)\tPrec@1 50.354 (52.702)\n",
      "EPOCH: 57 train Results: Prec@1 52.702 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3720 (1.3720)\tPrec@1 51.953 (51.953)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4098 (1.3863)\tPrec@1 48.597 (51.720)\n",
      "EPOCH: 57 val Results: Prec@1 51.720 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 3.90928e-04\n",
      "Epoch: [58][0/48]\tTime 0.008 (0.008)\tLoss 1.3340 (1.3340)\tPrec@1 53.223 (53.223)\n",
      "Epoch: [58][9/48]\tTime 0.008 (0.006)\tLoss 1.3353 (1.3371)\tPrec@1 55.078 (53.184)\n",
      "Epoch: [58][18/48]\tTime 0.007 (0.005)\tLoss 1.3437 (1.3442)\tPrec@1 53.809 (53.089)\n",
      "Epoch: [58][27/48]\tTime 0.008 (0.006)\tLoss 1.3786 (1.3468)\tPrec@1 52.051 (53.205)\n",
      "Epoch: [58][36/48]\tTime 0.003 (0.005)\tLoss 1.3516 (1.3481)\tPrec@1 51.465 (53.114)\n",
      "Epoch: [58][45/48]\tTime 0.013 (0.006)\tLoss 1.3954 (1.3498)\tPrec@1 51.953 (53.117)\n",
      "Epoch: [58][48/48]\tTime 0.005 (0.006)\tLoss 1.3112 (1.3493)\tPrec@1 53.892 (53.176)\n",
      "EPOCH: 58 train Results: Prec@1 53.176 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3701 (1.3701)\tPrec@1 51.855 (51.855)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4136 (1.3859)\tPrec@1 48.342 (51.520)\n",
      "EPOCH: 58 val Results: Prec@1 51.520 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 3.75655e-04\n",
      "Epoch: [59][0/48]\tTime 0.008 (0.008)\tLoss 1.4050 (1.4050)\tPrec@1 51.367 (51.367)\n",
      "Epoch: [59][9/48]\tTime 0.008 (0.006)\tLoss 1.3410 (1.3415)\tPrec@1 52.148 (53.232)\n",
      "Epoch: [59][18/48]\tTime 0.005 (0.006)\tLoss 1.3824 (1.3383)\tPrec@1 52.734 (53.418)\n",
      "Epoch: [59][27/48]\tTime 0.009 (0.006)\tLoss 1.3237 (1.3393)\tPrec@1 55.176 (53.439)\n",
      "Epoch: [59][36/48]\tTime 0.003 (0.006)\tLoss 1.3150 (1.3427)\tPrec@1 54.102 (53.373)\n",
      "Epoch: [59][45/48]\tTime 0.003 (0.006)\tLoss 1.3330 (1.3459)\tPrec@1 53.223 (53.172)\n",
      "Epoch: [59][48/48]\tTime 0.004 (0.006)\tLoss 1.3628 (1.3465)\tPrec@1 54.009 (53.244)\n",
      "EPOCH: 59 train Results: Prec@1 53.244 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3705 (1.3705)\tPrec@1 51.855 (51.855)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4113 (1.3848)\tPrec@1 47.704 (51.600)\n",
      "EPOCH: 59 val Results: Prec@1 51.600 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 3.60504e-04\n",
      "Epoch: [60][0/48]\tTime 0.008 (0.008)\tLoss 1.3210 (1.3210)\tPrec@1 52.930 (52.930)\n",
      "Epoch: [60][9/48]\tTime 0.002 (0.005)\tLoss 1.3517 (1.3504)\tPrec@1 52.441 (52.939)\n",
      "Epoch: [60][18/48]\tTime 0.011 (0.009)\tLoss 1.3616 (1.3499)\tPrec@1 53.711 (53.058)\n",
      "Epoch: [60][27/48]\tTime 0.005 (0.007)\tLoss 1.3567 (1.3478)\tPrec@1 52.832 (53.212)\n",
      "Epoch: [60][36/48]\tTime 0.007 (0.007)\tLoss 1.3496 (1.3507)\tPrec@1 53.027 (52.967)\n",
      "Epoch: [60][45/48]\tTime 0.002 (0.006)\tLoss 1.3050 (1.3494)\tPrec@1 54.492 (53.055)\n",
      "Epoch: [60][48/48]\tTime 0.004 (0.006)\tLoss 1.3583 (1.3483)\tPrec@1 51.297 (53.056)\n",
      "EPOCH: 60 train Results: Prec@1 53.056 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3689 (1.3689)\tPrec@1 52.832 (52.832)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4130 (1.3847)\tPrec@1 48.980 (51.630)\n",
      "EPOCH: 60 val Results: Prec@1 51.630 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 3.45492e-04\n",
      "Epoch: [61][0/48]\tTime 0.008 (0.008)\tLoss 1.3126 (1.3126)\tPrec@1 54.785 (54.785)\n",
      "Epoch: [61][9/48]\tTime 0.003 (0.004)\tLoss 1.3735 (1.3438)\tPrec@1 51.172 (53.408)\n",
      "Epoch: [61][18/48]\tTime 0.018 (0.005)\tLoss 1.3201 (1.3393)\tPrec@1 55.273 (53.726)\n",
      "Epoch: [61][27/48]\tTime 0.003 (0.005)\tLoss 1.2998 (1.3462)\tPrec@1 56.152 (53.512)\n",
      "Epoch: [61][36/48]\tTime 0.003 (0.005)\tLoss 1.2801 (1.3458)\tPrec@1 55.957 (53.487)\n",
      "Epoch: [61][45/48]\tTime 0.009 (0.005)\tLoss 1.3852 (1.3464)\tPrec@1 52.539 (53.384)\n",
      "Epoch: [61][48/48]\tTime 0.002 (0.004)\tLoss 1.3614 (1.3464)\tPrec@1 50.943 (53.358)\n",
      "EPOCH: 61 train Results: Prec@1 53.358 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.3731 (1.3731)\tPrec@1 52.051 (52.051)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4127 (1.3856)\tPrec@1 49.107 (51.590)\n",
      "EPOCH: 61 val Results: Prec@1 51.590 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 3.30631e-04\n",
      "Epoch: [62][0/48]\tTime 0.004 (0.004)\tLoss 1.3397 (1.3397)\tPrec@1 55.273 (55.273)\n",
      "Epoch: [62][9/48]\tTime 0.004 (0.005)\tLoss 1.3540 (1.3431)\tPrec@1 52.539 (53.682)\n",
      "Epoch: [62][18/48]\tTime 0.007 (0.006)\tLoss 1.3811 (1.3460)\tPrec@1 50.000 (53.274)\n",
      "Epoch: [62][27/48]\tTime 0.004 (0.005)\tLoss 1.3389 (1.3448)\tPrec@1 53.320 (53.289)\n",
      "Epoch: [62][36/48]\tTime 0.010 (0.005)\tLoss 1.2952 (1.3421)\tPrec@1 53.516 (53.402)\n",
      "Epoch: [62][45/48]\tTime 0.003 (0.005)\tLoss 1.3517 (1.3461)\tPrec@1 52.734 (53.225)\n",
      "Epoch: [62][48/48]\tTime 0.004 (0.005)\tLoss 1.3608 (1.3467)\tPrec@1 53.892 (53.224)\n",
      "EPOCH: 62 train Results: Prec@1 53.224 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3711 (1.3711)\tPrec@1 52.148 (52.148)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4122 (1.3843)\tPrec@1 48.980 (51.670)\n",
      "EPOCH: 62 val Results: Prec@1 51.670 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 3.15938e-04\n",
      "Epoch: [63][0/48]\tTime 0.008 (0.008)\tLoss 1.3497 (1.3497)\tPrec@1 54.297 (54.297)\n",
      "Epoch: [63][9/48]\tTime 0.005 (0.005)\tLoss 1.3599 (1.3340)\tPrec@1 53.516 (54.043)\n",
      "Epoch: [63][18/48]\tTime 0.003 (0.005)\tLoss 1.3913 (1.3404)\tPrec@1 51.660 (53.639)\n",
      "Epoch: [63][27/48]\tTime 0.007 (0.005)\tLoss 1.3585 (1.3403)\tPrec@1 52.441 (53.561)\n",
      "Epoch: [63][36/48]\tTime 0.003 (0.005)\tLoss 1.3496 (1.3437)\tPrec@1 53.125 (53.389)\n",
      "Epoch: [63][45/48]\tTime 0.003 (0.005)\tLoss 1.3478 (1.3446)\tPrec@1 53.027 (53.352)\n",
      "Epoch: [63][48/48]\tTime 0.004 (0.005)\tLoss 1.3974 (1.3452)\tPrec@1 50.236 (53.294)\n",
      "EPOCH: 63 train Results: Prec@1 53.294 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3693 (1.3693)\tPrec@1 52.734 (52.734)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4105 (1.3838)\tPrec@1 48.469 (51.550)\n",
      "EPOCH: 63 val Results: Prec@1 51.550 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 3.01426e-04\n",
      "Epoch: [64][0/48]\tTime 0.005 (0.005)\tLoss 1.3189 (1.3189)\tPrec@1 54.785 (54.785)\n",
      "Epoch: [64][9/48]\tTime 0.004 (0.005)\tLoss 1.3607 (1.3309)\tPrec@1 53.223 (53.906)\n",
      "Epoch: [64][18/48]\tTime 0.012 (0.006)\tLoss 1.3897 (1.3383)\tPrec@1 51.953 (53.726)\n",
      "Epoch: [64][27/48]\tTime 0.003 (0.006)\tLoss 1.2858 (1.3390)\tPrec@1 55.762 (53.826)\n",
      "Epoch: [64][36/48]\tTime 0.003 (0.005)\tLoss 1.4002 (1.3418)\tPrec@1 50.488 (53.547)\n",
      "Epoch: [64][45/48]\tTime 0.003 (0.005)\tLoss 1.3047 (1.3427)\tPrec@1 53.223 (53.380)\n",
      "Epoch: [64][48/48]\tTime 0.002 (0.005)\tLoss 1.3550 (1.3445)\tPrec@1 52.594 (53.360)\n",
      "EPOCH: 64 train Results: Prec@1 53.360 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3723 (1.3723)\tPrec@1 52.441 (52.441)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4092 (1.3839)\tPrec@1 48.852 (51.690)\n",
      "EPOCH: 64 val Results: Prec@1 51.690 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 2.87110e-04\n",
      "Epoch: [65][0/48]\tTime 0.003 (0.003)\tLoss 1.3509 (1.3509)\tPrec@1 52.637 (52.637)\n",
      "Epoch: [65][9/48]\tTime 0.004 (0.004)\tLoss 1.3332 (1.3317)\tPrec@1 52.930 (53.320)\n",
      "Epoch: [65][18/48]\tTime 0.011 (0.004)\tLoss 1.3351 (1.3346)\tPrec@1 53.809 (53.839)\n",
      "Epoch: [65][27/48]\tTime 0.004 (0.005)\tLoss 1.3629 (1.3397)\tPrec@1 52.246 (53.669)\n",
      "Epoch: [65][36/48]\tTime 0.004 (0.005)\tLoss 1.3558 (1.3449)\tPrec@1 53.613 (53.429)\n",
      "Epoch: [65][45/48]\tTime 0.005 (0.005)\tLoss 1.3358 (1.3426)\tPrec@1 52.637 (53.499)\n",
      "Epoch: [65][48/48]\tTime 0.002 (0.005)\tLoss 1.3675 (1.3441)\tPrec@1 52.594 (53.456)\n",
      "EPOCH: 65 train Results: Prec@1 53.456 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3718 (1.3718)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4114 (1.3842)\tPrec@1 48.342 (51.620)\n",
      "EPOCH: 65 val Results: Prec@1 51.620 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 2.73005e-04\n",
      "Epoch: [66][0/48]\tTime 0.005 (0.005)\tLoss 1.3204 (1.3204)\tPrec@1 52.539 (52.539)\n",
      "Epoch: [66][9/48]\tTime 0.003 (0.004)\tLoss 1.3226 (1.3371)\tPrec@1 55.273 (53.633)\n",
      "Epoch: [66][18/48]\tTime 0.003 (0.004)\tLoss 1.3497 (1.3380)\tPrec@1 54.199 (53.778)\n",
      "Epoch: [66][27/48]\tTime 0.003 (0.004)\tLoss 1.3347 (1.3409)\tPrec@1 52.148 (53.519)\n",
      "Epoch: [66][36/48]\tTime 0.004 (0.005)\tLoss 1.3512 (1.3441)\tPrec@1 52.734 (53.312)\n",
      "Epoch: [66][45/48]\tTime 0.007 (0.004)\tLoss 1.3455 (1.3457)\tPrec@1 53.027 (53.310)\n",
      "Epoch: [66][48/48]\tTime 0.002 (0.004)\tLoss 1.3568 (1.3467)\tPrec@1 51.769 (53.214)\n",
      "EPOCH: 66 train Results: Prec@1 53.214 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.3738 (1.3738)\tPrec@1 52.051 (52.051)\n",
      "Test: [9/9]\tTime 0.005 (0.002)\tLoss 1.4095 (1.3849)\tPrec@1 49.107 (51.650)\n",
      "EPOCH: 66 val Results: Prec@1 51.650 \n",
      "Best Prec@1: 51.720\n",
      "\n",
      "current lr 2.59123e-04\n",
      "Epoch: [67][0/48]\tTime 0.003 (0.003)\tLoss 1.2828 (1.2828)\tPrec@1 55.957 (55.957)\n",
      "Epoch: [67][9/48]\tTime 0.007 (0.005)\tLoss 1.3857 (1.3380)\tPrec@1 51.270 (53.311)\n",
      "Epoch: [67][18/48]\tTime 0.003 (0.005)\tLoss 1.3472 (1.3394)\tPrec@1 52.734 (53.295)\n",
      "Epoch: [67][27/48]\tTime 0.005 (0.005)\tLoss 1.3707 (1.3437)\tPrec@1 52.539 (53.303)\n",
      "Epoch: [67][36/48]\tTime 0.006 (0.005)\tLoss 1.3542 (1.3450)\tPrec@1 54.004 (53.257)\n",
      "Epoch: [67][45/48]\tTime 0.003 (0.005)\tLoss 1.3696 (1.3451)\tPrec@1 54.297 (53.233)\n",
      "Epoch: [67][48/48]\tTime 0.002 (0.005)\tLoss 1.3253 (1.3454)\tPrec@1 53.892 (53.206)\n",
      "EPOCH: 67 train Results: Prec@1 53.206 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3725 (1.3725)\tPrec@1 52.637 (52.637)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4116 (1.3853)\tPrec@1 49.362 (51.730)\n",
      "EPOCH: 67 val Results: Prec@1 51.730 \n",
      "Best Prec@1: 51.730\n",
      "\n",
      "current lr 2.45479e-04\n",
      "Epoch: [68][0/48]\tTime 0.003 (0.003)\tLoss 1.2957 (1.2957)\tPrec@1 56.152 (56.152)\n",
      "Epoch: [68][9/48]\tTime 0.011 (0.007)\tLoss 1.2781 (1.3303)\tPrec@1 55.957 (53.984)\n",
      "Epoch: [68][18/48]\tTime 0.003 (0.006)\tLoss 1.3632 (1.3409)\tPrec@1 52.734 (53.228)\n",
      "Epoch: [68][27/48]\tTime 0.003 (0.005)\tLoss 1.3416 (1.3435)\tPrec@1 53.223 (53.177)\n",
      "Epoch: [68][36/48]\tTime 0.005 (0.005)\tLoss 1.3590 (1.3454)\tPrec@1 52.441 (53.344)\n",
      "Epoch: [68][45/48]\tTime 0.005 (0.005)\tLoss 1.3604 (1.3457)\tPrec@1 52.734 (53.395)\n",
      "Epoch: [68][48/48]\tTime 0.002 (0.005)\tLoss 1.3812 (1.3457)\tPrec@1 54.127 (53.412)\n",
      "EPOCH: 68 train Results: Prec@1 53.412 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3729 (1.3729)\tPrec@1 52.441 (52.441)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4109 (1.3861)\tPrec@1 49.745 (51.890)\n",
      "EPOCH: 68 val Results: Prec@1 51.890 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 2.32087e-04\n",
      "Epoch: [69][0/48]\tTime 0.003 (0.003)\tLoss 1.3125 (1.3125)\tPrec@1 56.934 (56.934)\n",
      "Epoch: [69][9/48]\tTime 0.007 (0.005)\tLoss 1.3690 (1.3364)\tPrec@1 53.516 (54.434)\n",
      "Epoch: [69][18/48]\tTime 0.005 (0.005)\tLoss 1.3704 (1.3364)\tPrec@1 52.246 (54.215)\n",
      "Epoch: [69][27/48]\tTime 0.003 (0.005)\tLoss 1.3541 (1.3421)\tPrec@1 53.809 (53.889)\n",
      "Epoch: [69][36/48]\tTime 0.003 (0.005)\tLoss 1.3601 (1.3449)\tPrec@1 52.051 (53.642)\n",
      "Epoch: [69][45/48]\tTime 0.002 (0.005)\tLoss 1.3450 (1.3464)\tPrec@1 54.199 (53.486)\n",
      "Epoch: [69][48/48]\tTime 0.002 (0.005)\tLoss 1.3607 (1.3472)\tPrec@1 50.472 (53.428)\n",
      "EPOCH: 69 train Results: Prec@1 53.428 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3746 (1.3746)\tPrec@1 52.734 (52.734)\n",
      "Test: [9/9]\tTime 0.002 (0.001)\tLoss 1.4126 (1.3869)\tPrec@1 48.724 (51.820)\n",
      "EPOCH: 69 val Results: Prec@1 51.820 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 2.18958e-04\n",
      "Epoch: [70][0/48]\tTime 0.009 (0.009)\tLoss 1.3236 (1.3236)\tPrec@1 53.223 (53.223)\n",
      "Epoch: [70][9/48]\tTime 0.003 (0.006)\tLoss 1.3715 (1.3483)\tPrec@1 51.172 (53.320)\n",
      "Epoch: [70][18/48]\tTime 0.008 (0.006)\tLoss 1.3399 (1.3447)\tPrec@1 52.832 (53.469)\n",
      "Epoch: [70][27/48]\tTime 0.011 (0.006)\tLoss 1.3917 (1.3432)\tPrec@1 50.098 (53.488)\n",
      "Epoch: [70][36/48]\tTime 0.005 (0.005)\tLoss 1.3547 (1.3454)\tPrec@1 52.441 (53.299)\n",
      "Epoch: [70][45/48]\tTime 0.006 (0.005)\tLoss 1.3533 (1.3455)\tPrec@1 53.711 (53.335)\n",
      "Epoch: [70][48/48]\tTime 0.002 (0.005)\tLoss 1.3308 (1.3464)\tPrec@1 53.892 (53.266)\n",
      "EPOCH: 70 train Results: Prec@1 53.266 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.3758 (1.3758)\tPrec@1 52.148 (52.148)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4139 (1.3883)\tPrec@1 49.235 (51.800)\n",
      "EPOCH: 70 val Results: Prec@1 51.800 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 2.06107e-04\n",
      "Epoch: [71][0/48]\tTime 0.009 (0.009)\tLoss 1.3455 (1.3455)\tPrec@1 52.930 (52.930)\n",
      "Epoch: [71][9/48]\tTime 0.006 (0.007)\tLoss 1.3322 (1.3419)\tPrec@1 55.273 (53.262)\n",
      "Epoch: [71][18/48]\tTime 0.003 (0.005)\tLoss 1.3510 (1.3449)\tPrec@1 50.879 (53.238)\n",
      "Epoch: [71][27/48]\tTime 0.005 (0.005)\tLoss 1.3449 (1.3477)\tPrec@1 53.418 (53.421)\n",
      "Epoch: [71][36/48]\tTime 0.004 (0.005)\tLoss 1.3140 (1.3455)\tPrec@1 57.129 (53.571)\n",
      "Epoch: [71][45/48]\tTime 0.012 (0.005)\tLoss 1.3599 (1.3443)\tPrec@1 53.516 (53.539)\n",
      "Epoch: [71][48/48]\tTime 0.005 (0.005)\tLoss 1.4227 (1.3464)\tPrec@1 49.057 (53.432)\n",
      "EPOCH: 71 train Results: Prec@1 53.432 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.3766 (1.3766)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4151 (1.3894)\tPrec@1 49.745 (51.780)\n",
      "EPOCH: 71 val Results: Prec@1 51.780 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 1.93546e-04\n",
      "Epoch: [72][0/48]\tTime 0.006 (0.006)\tLoss 1.3097 (1.3097)\tPrec@1 55.762 (55.762)\n",
      "Epoch: [72][9/48]\tTime 0.008 (0.007)\tLoss 1.3919 (1.3434)\tPrec@1 52.051 (54.043)\n",
      "Epoch: [72][18/48]\tTime 0.004 (0.006)\tLoss 1.2975 (1.3421)\tPrec@1 55.469 (53.552)\n",
      "Epoch: [72][27/48]\tTime 0.006 (0.006)\tLoss 1.3790 (1.3487)\tPrec@1 53.418 (53.394)\n",
      "Epoch: [72][36/48]\tTime 0.006 (0.006)\tLoss 1.3365 (1.3517)\tPrec@1 53.125 (53.297)\n",
      "Epoch: [72][45/48]\tTime 0.005 (0.006)\tLoss 1.2958 (1.3492)\tPrec@1 56.641 (53.399)\n",
      "Epoch: [72][48/48]\tTime 0.003 (0.006)\tLoss 1.3373 (1.3494)\tPrec@1 52.594 (53.368)\n",
      "EPOCH: 72 train Results: Prec@1 53.368 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.3784 (1.3784)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4139 (1.3901)\tPrec@1 49.617 (51.750)\n",
      "EPOCH: 72 val Results: Prec@1 51.750 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 1.81288e-04\n",
      "Epoch: [73][0/48]\tTime 0.003 (0.003)\tLoss 1.3568 (1.3568)\tPrec@1 52.832 (52.832)\n",
      "Epoch: [73][9/48]\tTime 0.007 (0.004)\tLoss 1.2926 (1.3463)\tPrec@1 52.441 (53.662)\n",
      "Epoch: [73][18/48]\tTime 0.003 (0.006)\tLoss 1.3482 (1.3433)\tPrec@1 52.246 (53.593)\n",
      "Epoch: [73][27/48]\tTime 0.005 (0.005)\tLoss 1.3487 (1.3459)\tPrec@1 53.809 (53.582)\n",
      "Epoch: [73][36/48]\tTime 0.003 (0.005)\tLoss 1.3686 (1.3520)\tPrec@1 53.125 (53.405)\n",
      "Epoch: [73][45/48]\tTime 0.008 (0.005)\tLoss 1.4122 (1.3499)\tPrec@1 52.637 (53.441)\n",
      "Epoch: [73][48/48]\tTime 0.004 (0.005)\tLoss 1.3365 (1.3500)\tPrec@1 53.184 (53.352)\n",
      "EPOCH: 73 train Results: Prec@1 53.352 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3796 (1.3796)\tPrec@1 52.832 (52.832)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4142 (1.3904)\tPrec@1 49.745 (51.870)\n",
      "EPOCH: 73 val Results: Prec@1 51.870 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 1.69344e-04\n",
      "Epoch: [74][0/48]\tTime 0.004 (0.004)\tLoss 1.3642 (1.3642)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [74][9/48]\tTime 0.005 (0.005)\tLoss 1.3239 (1.3554)\tPrec@1 57.715 (52.910)\n",
      "Epoch: [74][18/48]\tTime 0.003 (0.005)\tLoss 1.3447 (1.3527)\tPrec@1 52.539 (53.233)\n",
      "Epoch: [74][27/48]\tTime 0.003 (0.005)\tLoss 1.3890 (1.3521)\tPrec@1 53.516 (53.271)\n",
      "Epoch: [74][36/48]\tTime 0.003 (0.005)\tLoss 1.3863 (1.3502)\tPrec@1 50.684 (53.381)\n",
      "Epoch: [74][45/48]\tTime 0.003 (0.005)\tLoss 1.3036 (1.3484)\tPrec@1 54.980 (53.401)\n",
      "Epoch: [74][48/48]\tTime 0.008 (0.005)\tLoss 1.3846 (1.3507)\tPrec@1 50.708 (53.302)\n",
      "EPOCH: 74 train Results: Prec@1 53.302 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3819 (1.3819)\tPrec@1 52.930 (52.930)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4142 (1.3918)\tPrec@1 49.617 (51.890)\n",
      "EPOCH: 74 val Results: Prec@1 51.890 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 1.57726e-04\n",
      "Epoch: [75][0/48]\tTime 0.004 (0.004)\tLoss 1.2952 (1.2952)\tPrec@1 55.176 (55.176)\n",
      "Epoch: [75][9/48]\tTime 0.003 (0.003)\tLoss 1.3731 (1.3382)\tPrec@1 54.199 (53.857)\n",
      "Epoch: [75][18/48]\tTime 0.008 (0.004)\tLoss 1.3199 (1.3471)\tPrec@1 56.445 (53.696)\n",
      "Epoch: [75][27/48]\tTime 0.002 (0.004)\tLoss 1.3530 (1.3484)\tPrec@1 54.102 (53.627)\n",
      "Epoch: [75][36/48]\tTime 0.003 (0.004)\tLoss 1.3200 (1.3482)\tPrec@1 56.543 (53.740)\n",
      "Epoch: [75][45/48]\tTime 0.003 (0.004)\tLoss 1.3609 (1.3495)\tPrec@1 54.785 (53.651)\n",
      "Epoch: [75][48/48]\tTime 0.002 (0.004)\tLoss 1.3123 (1.3505)\tPrec@1 54.953 (53.644)\n",
      "EPOCH: 75 train Results: Prec@1 53.644 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3828 (1.3828)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4159 (1.3929)\tPrec@1 49.745 (51.820)\n",
      "EPOCH: 75 val Results: Prec@1 51.820 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 1.46447e-04\n",
      "Epoch: [76][0/48]\tTime 0.007 (0.007)\tLoss 1.4034 (1.4034)\tPrec@1 52.051 (52.051)\n",
      "Epoch: [76][9/48]\tTime 0.003 (0.005)\tLoss 1.4064 (1.3645)\tPrec@1 50.098 (53.037)\n",
      "Epoch: [76][18/48]\tTime 0.005 (0.005)\tLoss 1.3391 (1.3561)\tPrec@1 52.637 (53.022)\n",
      "Epoch: [76][27/48]\tTime 0.005 (0.005)\tLoss 1.3793 (1.3559)\tPrec@1 51.367 (53.069)\n",
      "Epoch: [76][36/48]\tTime 0.003 (0.005)\tLoss 1.3596 (1.3527)\tPrec@1 52.539 (53.257)\n",
      "Epoch: [76][45/48]\tTime 0.002 (0.005)\tLoss 1.3283 (1.3525)\tPrec@1 53.418 (53.310)\n",
      "Epoch: [76][48/48]\tTime 0.005 (0.005)\tLoss 1.3029 (1.3515)\tPrec@1 56.132 (53.358)\n",
      "EPOCH: 76 train Results: Prec@1 53.358 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3856 (1.3856)\tPrec@1 52.637 (52.637)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4168 (1.3946)\tPrec@1 50.255 (51.750)\n",
      "EPOCH: 76 val Results: Prec@1 51.750 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 1.35516e-04\n",
      "Epoch: [77][0/48]\tTime 0.007 (0.007)\tLoss 1.3447 (1.3447)\tPrec@1 53.516 (53.516)\n",
      "Epoch: [77][9/48]\tTime 0.003 (0.004)\tLoss 1.3269 (1.3438)\tPrec@1 56.055 (54.199)\n",
      "Epoch: [77][18/48]\tTime 0.011 (0.005)\tLoss 1.3354 (1.3486)\tPrec@1 55.859 (53.778)\n",
      "Epoch: [77][27/48]\tTime 0.003 (0.005)\tLoss 1.4025 (1.3493)\tPrec@1 50.293 (53.770)\n",
      "Epoch: [77][36/48]\tTime 0.006 (0.005)\tLoss 1.3265 (1.3498)\tPrec@1 56.934 (53.795)\n",
      "Epoch: [77][45/48]\tTime 0.003 (0.005)\tLoss 1.3102 (1.3538)\tPrec@1 56.250 (53.662)\n",
      "Epoch: [77][48/48]\tTime 0.006 (0.005)\tLoss 1.3584 (1.3540)\tPrec@1 51.415 (53.606)\n",
      "EPOCH: 77 train Results: Prec@1 53.606 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3876 (1.3876)\tPrec@1 52.734 (52.734)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4191 (1.3963)\tPrec@1 49.872 (51.840)\n",
      "EPOCH: 77 val Results: Prec@1 51.840 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 1.24944e-04\n",
      "Epoch: [78][0/48]\tTime 0.004 (0.004)\tLoss 1.3811 (1.3811)\tPrec@1 50.293 (50.293)\n",
      "Epoch: [78][9/48]\tTime 0.007 (0.004)\tLoss 1.3139 (1.3360)\tPrec@1 56.055 (54.297)\n",
      "Epoch: [78][18/48]\tTime 0.012 (0.005)\tLoss 1.3732 (1.3472)\tPrec@1 52.734 (53.839)\n",
      "Epoch: [78][27/48]\tTime 0.003 (0.005)\tLoss 1.3456 (1.3533)\tPrec@1 53.516 (53.707)\n",
      "Epoch: [78][36/48]\tTime 0.006 (0.005)\tLoss 1.3766 (1.3571)\tPrec@1 52.344 (53.463)\n",
      "Epoch: [78][45/48]\tTime 0.005 (0.005)\tLoss 1.3783 (1.3566)\tPrec@1 52.930 (53.416)\n",
      "Epoch: [78][48/48]\tTime 0.002 (0.005)\tLoss 1.3829 (1.3559)\tPrec@1 51.651 (53.398)\n",
      "EPOCH: 78 train Results: Prec@1 53.398 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3906 (1.3906)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4207 (1.3985)\tPrec@1 50.000 (51.720)\n",
      "EPOCH: 78 val Results: Prec@1 51.720 \n",
      "Best Prec@1: 51.890\n",
      "\n",
      "current lr 1.14743e-04\n",
      "Epoch: [79][0/48]\tTime 0.007 (0.007)\tLoss 1.3261 (1.3261)\tPrec@1 56.055 (56.055)\n",
      "Epoch: [79][9/48]\tTime 0.003 (0.004)\tLoss 1.3312 (1.3363)\tPrec@1 54.883 (54.170)\n",
      "Epoch: [79][18/48]\tTime 0.002 (0.005)\tLoss 1.3696 (1.3437)\tPrec@1 51.953 (53.973)\n",
      "Epoch: [79][27/48]\tTime 0.004 (0.005)\tLoss 1.3609 (1.3458)\tPrec@1 52.734 (53.826)\n",
      "Epoch: [79][36/48]\tTime 0.005 (0.005)\tLoss 1.3303 (1.3509)\tPrec@1 54.199 (53.719)\n",
      "Epoch: [79][45/48]\tTime 0.003 (0.005)\tLoss 1.3646 (1.3536)\tPrec@1 54.102 (53.679)\n",
      "Epoch: [79][48/48]\tTime 0.004 (0.005)\tLoss 1.3947 (1.3559)\tPrec@1 52.830 (53.582)\n",
      "EPOCH: 79 train Results: Prec@1 53.582 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3926 (1.3926)\tPrec@1 52.148 (52.148)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4215 (1.4004)\tPrec@1 50.128 (51.910)\n",
      "EPOCH: 79 val Results: Prec@1 51.910 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 1.04922e-04\n",
      "Epoch: [80][0/48]\tTime 0.004 (0.004)\tLoss 1.3772 (1.3772)\tPrec@1 52.930 (52.930)\n",
      "Epoch: [80][9/48]\tTime 0.005 (0.004)\tLoss 1.3613 (1.3580)\tPrec@1 52.637 (54.014)\n",
      "Epoch: [80][18/48]\tTime 0.006 (0.006)\tLoss 1.3369 (1.3532)\tPrec@1 53.613 (53.819)\n",
      "Epoch: [80][27/48]\tTime 0.007 (0.005)\tLoss 1.3833 (1.3595)\tPrec@1 54.102 (53.505)\n",
      "Epoch: [80][36/48]\tTime 0.011 (0.006)\tLoss 1.3710 (1.3604)\tPrec@1 51.953 (53.341)\n",
      "Epoch: [80][45/48]\tTime 0.003 (0.006)\tLoss 1.3395 (1.3584)\tPrec@1 56.543 (53.530)\n",
      "Epoch: [80][48/48]\tTime 0.009 (0.006)\tLoss 1.3641 (1.3589)\tPrec@1 52.830 (53.512)\n",
      "EPOCH: 80 train Results: Prec@1 53.512 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.3942 (1.3942)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4234 (1.4025)\tPrec@1 50.128 (51.880)\n",
      "EPOCH: 80 val Results: Prec@1 51.880 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 9.54915e-05\n",
      "Epoch: [81][0/48]\tTime 0.017 (0.017)\tLoss 1.3768 (1.3768)\tPrec@1 51.172 (51.172)\n",
      "Epoch: [81][9/48]\tTime 0.008 (0.007)\tLoss 1.3412 (1.3570)\tPrec@1 55.078 (53.232)\n",
      "Epoch: [81][18/48]\tTime 0.003 (0.006)\tLoss 1.3200 (1.3589)\tPrec@1 53.418 (53.413)\n",
      "Epoch: [81][27/48]\tTime 0.003 (0.005)\tLoss 1.3670 (1.3592)\tPrec@1 52.148 (53.401)\n",
      "Epoch: [81][36/48]\tTime 0.004 (0.005)\tLoss 1.3328 (1.3589)\tPrec@1 53.711 (53.545)\n",
      "Epoch: [81][45/48]\tTime 0.009 (0.007)\tLoss 1.3736 (1.3610)\tPrec@1 52.734 (53.492)\n",
      "Epoch: [81][48/48]\tTime 0.002 (0.007)\tLoss 1.3933 (1.3616)\tPrec@1 52.830 (53.448)\n",
      "EPOCH: 81 train Results: Prec@1 53.448 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.3960 (1.3960)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4256 (1.4045)\tPrec@1 49.745 (51.750)\n",
      "EPOCH: 81 val Results: Prec@1 51.750 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 8.64597e-05\n",
      "Epoch: [82][0/48]\tTime 0.003 (0.003)\tLoss 1.3504 (1.3504)\tPrec@1 53.613 (53.613)\n",
      "Epoch: [82][9/48]\tTime 0.006 (0.005)\tLoss 1.3683 (1.3554)\tPrec@1 52.637 (53.223)\n",
      "Epoch: [82][18/48]\tTime 0.007 (0.010)\tLoss 1.3821 (1.3559)\tPrec@1 52.441 (53.356)\n",
      "Epoch: [82][27/48]\tTime 0.007 (0.008)\tLoss 1.3855 (1.3576)\tPrec@1 52.734 (53.414)\n",
      "Epoch: [82][36/48]\tTime 0.015 (0.008)\tLoss 1.3352 (1.3586)\tPrec@1 53.906 (53.444)\n",
      "Epoch: [82][45/48]\tTime 0.012 (0.009)\tLoss 1.3764 (1.3600)\tPrec@1 52.832 (53.395)\n",
      "Epoch: [82][48/48]\tTime 0.004 (0.009)\tLoss 1.4189 (1.3624)\tPrec@1 53.656 (53.382)\n",
      "EPOCH: 82 train Results: Prec@1 53.382 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.3995 (1.3995)\tPrec@1 52.441 (52.441)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.4285 (1.4067)\tPrec@1 49.745 (51.750)\n",
      "EPOCH: 82 val Results: Prec@1 51.750 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 7.78360e-05\n",
      "Epoch: [83][0/48]\tTime 0.005 (0.005)\tLoss 1.3793 (1.3793)\tPrec@1 52.246 (52.246)\n",
      "Epoch: [83][9/48]\tTime 0.012 (0.008)\tLoss 1.3990 (1.3624)\tPrec@1 52.148 (53.223)\n",
      "Epoch: [83][18/48]\tTime 0.006 (0.007)\tLoss 1.3744 (1.3703)\tPrec@1 52.930 (53.151)\n",
      "Epoch: [83][27/48]\tTime 0.011 (0.007)\tLoss 1.3568 (1.3651)\tPrec@1 54.395 (53.380)\n",
      "Epoch: [83][36/48]\tTime 0.002 (0.007)\tLoss 1.3537 (1.3673)\tPrec@1 54.004 (53.365)\n",
      "Epoch: [83][45/48]\tTime 0.003 (0.006)\tLoss 1.2863 (1.3651)\tPrec@1 58.691 (53.448)\n",
      "Epoch: [83][48/48]\tTime 0.007 (0.006)\tLoss 1.3419 (1.3660)\tPrec@1 55.307 (53.428)\n",
      "EPOCH: 83 train Results: Prec@1 53.428 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4037 (1.4037)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.002 (0.001)\tLoss 1.4316 (1.4105)\tPrec@1 49.490 (51.720)\n",
      "EPOCH: 83 val Results: Prec@1 51.720 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 6.96290e-05\n",
      "Epoch: [84][0/48]\tTime 0.012 (0.012)\tLoss 1.3128 (1.3128)\tPrec@1 55.371 (55.371)\n",
      "Epoch: [84][9/48]\tTime 0.003 (0.005)\tLoss 1.3983 (1.3700)\tPrec@1 51.758 (52.734)\n",
      "Epoch: [84][18/48]\tTime 0.007 (0.005)\tLoss 1.3555 (1.3692)\tPrec@1 53.223 (53.120)\n",
      "Epoch: [84][27/48]\tTime 0.002 (0.005)\tLoss 1.4298 (1.3733)\tPrec@1 51.562 (53.111)\n",
      "Epoch: [84][36/48]\tTime 0.010 (0.006)\tLoss 1.3156 (1.3707)\tPrec@1 57.227 (53.149)\n",
      "Epoch: [84][45/48]\tTime 0.003 (0.007)\tLoss 1.3593 (1.3669)\tPrec@1 53.613 (53.305)\n",
      "Epoch: [84][48/48]\tTime 0.004 (0.007)\tLoss 1.4231 (1.3682)\tPrec@1 49.646 (53.306)\n",
      "EPOCH: 84 train Results: Prec@1 53.306 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4058 (1.4058)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4335 (1.4126)\tPrec@1 49.745 (51.570)\n",
      "EPOCH: 84 val Results: Prec@1 51.570 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 6.18467e-05\n",
      "Epoch: [85][0/48]\tTime 0.003 (0.003)\tLoss 1.3688 (1.3688)\tPrec@1 53.711 (53.711)\n",
      "Epoch: [85][9/48]\tTime 0.003 (0.006)\tLoss 1.3738 (1.3679)\tPrec@1 54.102 (53.691)\n",
      "Epoch: [85][18/48]\tTime 0.008 (0.006)\tLoss 1.3468 (1.3683)\tPrec@1 55.469 (53.778)\n",
      "Epoch: [85][27/48]\tTime 0.004 (0.005)\tLoss 1.3616 (1.3664)\tPrec@1 52.539 (53.498)\n",
      "Epoch: [85][36/48]\tTime 0.004 (0.005)\tLoss 1.4425 (1.3715)\tPrec@1 51.270 (53.273)\n",
      "Epoch: [85][45/48]\tTime 0.003 (0.005)\tLoss 1.3558 (1.3708)\tPrec@1 54.688 (53.329)\n",
      "Epoch: [85][48/48]\tTime 0.006 (0.005)\tLoss 1.3886 (1.3723)\tPrec@1 53.066 (53.274)\n",
      "EPOCH: 85 train Results: Prec@1 53.274 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4097 (1.4097)\tPrec@1 52.051 (52.051)\n",
      "Test: [9/9]\tTime 0.004 (0.002)\tLoss 1.4354 (1.4156)\tPrec@1 49.745 (51.580)\n",
      "EPOCH: 85 val Results: Prec@1 51.580 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 5.44967e-05\n",
      "Epoch: [86][0/48]\tTime 0.007 (0.007)\tLoss 1.3857 (1.3857)\tPrec@1 54.590 (54.590)\n",
      "Epoch: [86][9/48]\tTime 0.003 (0.006)\tLoss 1.3526 (1.3856)\tPrec@1 54.785 (53.389)\n",
      "Epoch: [86][18/48]\tTime 0.004 (0.006)\tLoss 1.3749 (1.3731)\tPrec@1 50.977 (53.639)\n",
      "Epoch: [86][27/48]\tTime 0.010 (0.005)\tLoss 1.3934 (1.3723)\tPrec@1 53.516 (53.826)\n",
      "Epoch: [86][36/48]\tTime 0.006 (0.005)\tLoss 1.3298 (1.3753)\tPrec@1 54.102 (53.568)\n",
      "Epoch: [86][45/48]\tTime 0.009 (0.005)\tLoss 1.3929 (1.3754)\tPrec@1 51.953 (53.475)\n",
      "Epoch: [86][48/48]\tTime 0.005 (0.005)\tLoss 1.4361 (1.3759)\tPrec@1 50.708 (53.406)\n",
      "EPOCH: 86 train Results: Prec@1 53.406 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4127 (1.4127)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4386 (1.4187)\tPrec@1 49.107 (51.630)\n",
      "EPOCH: 86 val Results: Prec@1 51.630 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 4.75865e-05\n",
      "Epoch: [87][0/48]\tTime 0.005 (0.005)\tLoss 1.3595 (1.3595)\tPrec@1 53.027 (53.027)\n",
      "Epoch: [87][9/48]\tTime 0.008 (0.005)\tLoss 1.3920 (1.3726)\tPrec@1 52.051 (53.447)\n",
      "Epoch: [87][18/48]\tTime 0.006 (0.005)\tLoss 1.3383 (1.3765)\tPrec@1 54.980 (53.454)\n",
      "Epoch: [87][27/48]\tTime 0.005 (0.005)\tLoss 1.3842 (1.3765)\tPrec@1 53.516 (53.369)\n",
      "Epoch: [87][36/48]\tTime 0.010 (0.005)\tLoss 1.3902 (1.3772)\tPrec@1 54.590 (53.444)\n",
      "Epoch: [87][45/48]\tTime 0.005 (0.006)\tLoss 1.4186 (1.3806)\tPrec@1 51.367 (53.354)\n",
      "Epoch: [87][48/48]\tTime 0.003 (0.006)\tLoss 1.3790 (1.3801)\tPrec@1 51.651 (53.272)\n",
      "EPOCH: 87 train Results: Prec@1 53.272 \n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.4172 (1.4172)\tPrec@1 52.734 (52.734)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4419 (1.4226)\tPrec@1 49.617 (51.630)\n",
      "EPOCH: 87 val Results: Prec@1 51.630 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 4.11227e-05\n",
      "Epoch: [88][0/48]\tTime 0.004 (0.004)\tLoss 1.3163 (1.3163)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [88][9/48]\tTime 0.009 (0.005)\tLoss 1.3647 (1.3728)\tPrec@1 54.980 (53.604)\n",
      "Epoch: [88][18/48]\tTime 0.005 (0.005)\tLoss 1.3924 (1.3774)\tPrec@1 52.734 (53.439)\n",
      "Epoch: [88][27/48]\tTime 0.014 (0.005)\tLoss 1.3557 (1.3769)\tPrec@1 55.762 (53.669)\n",
      "Epoch: [88][36/48]\tTime 0.004 (0.005)\tLoss 1.4617 (1.3821)\tPrec@1 52.441 (53.455)\n",
      "Epoch: [88][45/48]\tTime 0.004 (0.005)\tLoss 1.3902 (1.3825)\tPrec@1 54.004 (53.390)\n",
      "Epoch: [88][48/48]\tTime 0.004 (0.005)\tLoss 1.3969 (1.3831)\tPrec@1 50.472 (53.346)\n",
      "EPOCH: 88 train Results: Prec@1 53.346 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4218 (1.4218)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4455 (1.4269)\tPrec@1 49.362 (51.580)\n",
      "EPOCH: 88 val Results: Prec@1 51.580 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 3.51118e-05\n",
      "Epoch: [89][0/48]\tTime 0.008 (0.008)\tLoss 1.3812 (1.3812)\tPrec@1 51.758 (51.758)\n",
      "Epoch: [89][9/48]\tTime 0.003 (0.005)\tLoss 1.3526 (1.3803)\tPrec@1 55.664 (53.389)\n",
      "Epoch: [89][18/48]\tTime 0.003 (0.005)\tLoss 1.4104 (1.3794)\tPrec@1 50.977 (53.115)\n",
      "Epoch: [89][27/48]\tTime 0.003 (0.008)\tLoss 1.3945 (1.3839)\tPrec@1 54.102 (53.247)\n",
      "Epoch: [89][36/48]\tTime 0.007 (0.007)\tLoss 1.4027 (1.3857)\tPrec@1 53.125 (53.302)\n",
      "Epoch: [89][45/48]\tTime 0.003 (0.007)\tLoss 1.4301 (1.3888)\tPrec@1 53.418 (53.235)\n",
      "Epoch: [89][48/48]\tTime 0.004 (0.007)\tLoss 1.3579 (1.3881)\tPrec@1 56.840 (53.314)\n",
      "EPOCH: 89 train Results: Prec@1 53.314 \n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.4264 (1.4264)\tPrec@1 52.148 (52.148)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4489 (1.4309)\tPrec@1 49.362 (51.590)\n",
      "EPOCH: 89 val Results: Prec@1 51.590 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 2.95596e-05\n",
      "Epoch: [90][0/48]\tTime 0.004 (0.004)\tLoss 1.3790 (1.3790)\tPrec@1 53.223 (53.223)\n",
      "Epoch: [90][9/48]\tTime 0.005 (0.004)\tLoss 1.3973 (1.3869)\tPrec@1 54.102 (53.145)\n",
      "Epoch: [90][18/48]\tTime 0.003 (0.004)\tLoss 1.4136 (1.3895)\tPrec@1 53.418 (53.130)\n",
      "Epoch: [90][27/48]\tTime 0.004 (0.004)\tLoss 1.4089 (1.3934)\tPrec@1 51.660 (53.041)\n",
      "Epoch: [90][36/48]\tTime 0.008 (0.006)\tLoss 1.3923 (1.3924)\tPrec@1 52.832 (53.101)\n",
      "Epoch: [90][45/48]\tTime 0.011 (0.006)\tLoss 1.3981 (1.3917)\tPrec@1 51.465 (53.108)\n",
      "Epoch: [90][48/48]\tTime 0.002 (0.006)\tLoss 1.4102 (1.3929)\tPrec@1 51.887 (53.078)\n",
      "EPOCH: 90 train Results: Prec@1 53.078 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4325 (1.4325)\tPrec@1 52.539 (52.539)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4537 (1.4363)\tPrec@1 49.490 (51.530)\n",
      "EPOCH: 90 val Results: Prec@1 51.530 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 2.44717e-05\n",
      "Epoch: [91][0/48]\tTime 0.016 (0.016)\tLoss 1.3937 (1.3937)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [91][9/48]\tTime 0.005 (0.007)\tLoss 1.3758 (1.3975)\tPrec@1 51.270 (53.213)\n",
      "Epoch: [91][18/48]\tTime 0.008 (0.007)\tLoss 1.3714 (1.3877)\tPrec@1 54.102 (53.737)\n",
      "Epoch: [91][27/48]\tTime 0.003 (0.006)\tLoss 1.4354 (1.3910)\tPrec@1 49.805 (53.446)\n",
      "Epoch: [91][36/48]\tTime 0.003 (0.006)\tLoss 1.3774 (1.3927)\tPrec@1 53.613 (53.260)\n",
      "Epoch: [91][45/48]\tTime 0.008 (0.006)\tLoss 1.4472 (1.3966)\tPrec@1 51.172 (53.108)\n",
      "Epoch: [91][48/48]\tTime 0.004 (0.006)\tLoss 1.3998 (1.3968)\tPrec@1 53.302 (53.116)\n",
      "EPOCH: 91 train Results: Prec@1 53.116 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4362 (1.4362)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4568 (1.4399)\tPrec@1 49.490 (51.480)\n",
      "EPOCH: 91 val Results: Prec@1 51.480 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 1.98532e-05\n",
      "Epoch: [92][0/48]\tTime 0.008 (0.008)\tLoss 1.4362 (1.4362)\tPrec@1 50.684 (50.684)\n",
      "Epoch: [92][9/48]\tTime 0.004 (0.008)\tLoss 1.4536 (1.4072)\tPrec@1 50.000 (52.559)\n",
      "Epoch: [92][18/48]\tTime 0.004 (0.007)\tLoss 1.4036 (1.4070)\tPrec@1 52.832 (52.914)\n",
      "Epoch: [92][27/48]\tTime 0.011 (0.006)\tLoss 1.3639 (1.4048)\tPrec@1 54.297 (53.059)\n",
      "Epoch: [92][36/48]\tTime 0.003 (0.006)\tLoss 1.3733 (1.4007)\tPrec@1 54.980 (53.194)\n",
      "Epoch: [92][45/48]\tTime 0.007 (0.006)\tLoss 1.3834 (1.4014)\tPrec@1 55.176 (53.150)\n",
      "Epoch: [92][48/48]\tTime 0.011 (0.006)\tLoss 1.4186 (1.4014)\tPrec@1 53.656 (53.176)\n",
      "EPOCH: 92 train Results: Prec@1 53.176 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4422 (1.4422)\tPrec@1 52.539 (52.539)\n",
      "Test: [9/9]\tTime 0.005 (0.002)\tLoss 1.4613 (1.4453)\tPrec@1 49.235 (51.410)\n",
      "EPOCH: 92 val Results: Prec@1 51.410 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 1.57084e-05\n",
      "Epoch: [93][0/48]\tTime 0.009 (0.009)\tLoss 1.3893 (1.3893)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [93][9/48]\tTime 0.006 (0.006)\tLoss 1.3649 (1.3884)\tPrec@1 54.883 (54.229)\n",
      "Epoch: [93][18/48]\tTime 0.005 (0.006)\tLoss 1.4133 (1.3977)\tPrec@1 54.785 (54.066)\n",
      "Epoch: [93][27/48]\tTime 0.013 (0.005)\tLoss 1.4297 (1.4028)\tPrec@1 51.758 (53.631)\n",
      "Epoch: [93][36/48]\tTime 0.003 (0.005)\tLoss 1.3848 (1.4032)\tPrec@1 53.613 (53.505)\n",
      "Epoch: [93][45/48]\tTime 0.014 (0.005)\tLoss 1.3863 (1.4080)\tPrec@1 56.152 (53.233)\n",
      "Epoch: [93][48/48]\tTime 0.003 (0.005)\tLoss 1.4345 (1.4087)\tPrec@1 51.651 (53.222)\n",
      "EPOCH: 93 train Results: Prec@1 53.222 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4481 (1.4481)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4663 (1.4513)\tPrec@1 49.107 (51.280)\n",
      "EPOCH: 93 val Results: Prec@1 51.280 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 1.20416e-05\n",
      "Epoch: [94][0/48]\tTime 0.003 (0.003)\tLoss 1.4190 (1.4190)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [94][9/48]\tTime 0.003 (0.005)\tLoss 1.3951 (1.4005)\tPrec@1 54.395 (53.945)\n",
      "Epoch: [94][18/48]\tTime 0.003 (0.005)\tLoss 1.4073 (1.4099)\tPrec@1 53.223 (53.264)\n",
      "Epoch: [94][27/48]\tTime 0.006 (0.005)\tLoss 1.4055 (1.4125)\tPrec@1 52.246 (53.024)\n",
      "Epoch: [94][36/48]\tTime 0.008 (0.006)\tLoss 1.4217 (1.4150)\tPrec@1 52.539 (52.922)\n",
      "Epoch: [94][45/48]\tTime 0.004 (0.005)\tLoss 1.3860 (1.4140)\tPrec@1 54.199 (53.083)\n",
      "Epoch: [94][48/48]\tTime 0.002 (0.005)\tLoss 1.4571 (1.4145)\tPrec@1 52.241 (53.100)\n",
      "EPOCH: 94 train Results: Prec@1 53.100 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4554 (1.4554)\tPrec@1 52.148 (52.148)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4719 (1.4576)\tPrec@1 48.980 (51.290)\n",
      "EPOCH: 94 val Results: Prec@1 51.290 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 8.85637e-06\n",
      "Epoch: [95][0/48]\tTime 0.004 (0.004)\tLoss 1.4437 (1.4437)\tPrec@1 53.223 (53.223)\n",
      "Epoch: [95][9/48]\tTime 0.010 (0.006)\tLoss 1.4211 (1.4123)\tPrec@1 51.367 (53.535)\n",
      "Epoch: [95][18/48]\tTime 0.002 (0.005)\tLoss 1.4185 (1.4144)\tPrec@1 52.734 (53.608)\n",
      "Epoch: [95][27/48]\tTime 0.003 (0.005)\tLoss 1.4173 (1.4158)\tPrec@1 53.418 (53.509)\n",
      "Epoch: [95][36/48]\tTime 0.007 (0.006)\tLoss 1.4230 (1.4192)\tPrec@1 52.148 (53.215)\n",
      "Epoch: [95][45/48]\tTime 0.005 (0.005)\tLoss 1.4735 (1.4214)\tPrec@1 49.414 (53.184)\n",
      "Epoch: [95][48/48]\tTime 0.006 (0.005)\tLoss 1.4004 (1.4199)\tPrec@1 55.778 (53.260)\n",
      "EPOCH: 95 train Results: Prec@1 53.260 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.4614 (1.4614)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4770 (1.4634)\tPrec@1 49.107 (51.180)\n",
      "EPOCH: 95 val Results: Prec@1 51.180 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 6.15583e-06\n",
      "Epoch: [96][0/48]\tTime 0.004 (0.004)\tLoss 1.4382 (1.4382)\tPrec@1 50.879 (50.879)\n",
      "Epoch: [96][9/48]\tTime 0.003 (0.003)\tLoss 1.4274 (1.4294)\tPrec@1 53.906 (53.086)\n",
      "Epoch: [96][18/48]\tTime 0.003 (0.004)\tLoss 1.4536 (1.4293)\tPrec@1 50.586 (52.858)\n",
      "Epoch: [96][27/48]\tTime 0.007 (0.004)\tLoss 1.4430 (1.4331)\tPrec@1 51.562 (52.801)\n",
      "Epoch: [96][36/48]\tTime 0.004 (0.005)\tLoss 1.4185 (1.4296)\tPrec@1 55.176 (53.096)\n",
      "Epoch: [96][45/48]\tTime 0.003 (0.005)\tLoss 1.4436 (1.4278)\tPrec@1 54.980 (53.231)\n",
      "Epoch: [96][48/48]\tTime 0.004 (0.005)\tLoss 1.4294 (1.4286)\tPrec@1 52.594 (53.176)\n",
      "EPOCH: 96 train Results: Prec@1 53.176 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4685 (1.4685)\tPrec@1 51.758 (51.758)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.4836 (1.4705)\tPrec@1 49.362 (51.190)\n",
      "EPOCH: 96 val Results: Prec@1 51.190 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 3.94265e-06\n",
      "Epoch: [97][0/48]\tTime 0.008 (0.008)\tLoss 1.4640 (1.4640)\tPrec@1 52.148 (52.148)\n",
      "Epoch: [97][9/48]\tTime 0.004 (0.006)\tLoss 1.4434 (1.4284)\tPrec@1 50.977 (53.115)\n",
      "Epoch: [97][18/48]\tTime 0.013 (0.006)\tLoss 1.4293 (1.4307)\tPrec@1 52.344 (53.187)\n",
      "Epoch: [97][27/48]\tTime 0.003 (0.006)\tLoss 1.4280 (1.4330)\tPrec@1 53.027 (53.268)\n",
      "Epoch: [97][36/48]\tTime 0.009 (0.006)\tLoss 1.4392 (1.4327)\tPrec@1 51.855 (53.231)\n",
      "Epoch: [97][45/48]\tTime 0.006 (0.005)\tLoss 1.4054 (1.4349)\tPrec@1 55.273 (53.087)\n",
      "Epoch: [97][48/48]\tTime 0.006 (0.005)\tLoss 1.4447 (1.4358)\tPrec@1 52.123 (53.014)\n",
      "EPOCH: 97 train Results: Prec@1 53.014 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4765 (1.4765)\tPrec@1 52.051 (52.051)\n",
      "Test: [9/9]\tTime 0.001 (0.001)\tLoss 1.4902 (1.4778)\tPrec@1 49.107 (51.190)\n",
      "EPOCH: 97 val Results: Prec@1 51.190 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 2.21902e-06\n",
      "Epoch: [98][0/48]\tTime 0.006 (0.006)\tLoss 1.4722 (1.4722)\tPrec@1 51.953 (51.953)\n",
      "Epoch: [98][9/48]\tTime 0.004 (0.005)\tLoss 1.4029 (1.4459)\tPrec@1 55.664 (52.881)\n",
      "Epoch: [98][18/48]\tTime 0.004 (0.009)\tLoss 1.4315 (1.4439)\tPrec@1 52.441 (52.873)\n",
      "Epoch: [98][27/48]\tTime 0.009 (0.008)\tLoss 1.4671 (1.4426)\tPrec@1 50.195 (52.884)\n",
      "Epoch: [98][36/48]\tTime 0.006 (0.008)\tLoss 1.4570 (1.4426)\tPrec@1 54.590 (53.083)\n",
      "Epoch: [98][45/48]\tTime 0.004 (0.007)\tLoss 1.4108 (1.4423)\tPrec@1 55.078 (53.153)\n",
      "Epoch: [98][48/48]\tTime 0.004 (0.007)\tLoss 1.4487 (1.4429)\tPrec@1 52.005 (53.134)\n",
      "EPOCH: 98 train Results: Prec@1 53.134 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4846 (1.4846)\tPrec@1 51.855 (51.855)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.4967 (1.4853)\tPrec@1 48.852 (51.120)\n",
      "EPOCH: 98 val Results: Prec@1 51.120 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 9.86636e-07\n",
      "Epoch: [99][0/48]\tTime 0.003 (0.003)\tLoss 1.4227 (1.4227)\tPrec@1 54.199 (54.199)\n",
      "Epoch: [99][9/48]\tTime 0.003 (0.005)\tLoss 1.4425 (1.4505)\tPrec@1 52.734 (52.979)\n",
      "Epoch: [99][18/48]\tTime 0.005 (0.005)\tLoss 1.4259 (1.4546)\tPrec@1 54.980 (52.863)\n",
      "Epoch: [99][27/48]\tTime 0.003 (0.005)\tLoss 1.4442 (1.4499)\tPrec@1 54.004 (53.031)\n",
      "Epoch: [99][36/48]\tTime 0.006 (0.005)\tLoss 1.4502 (1.4503)\tPrec@1 54.004 (53.030)\n",
      "Epoch: [99][45/48]\tTime 0.004 (0.005)\tLoss 1.4687 (1.4503)\tPrec@1 51.855 (53.083)\n",
      "Epoch: [99][48/48]\tTime 0.005 (0.005)\tLoss 1.4350 (1.4496)\tPrec@1 54.363 (53.152)\n",
      "EPOCH: 99 train Results: Prec@1 53.152 \n",
      "Test: [0/9]\tTime 0.002 (0.002)\tLoss 1.4924 (1.4924)\tPrec@1 51.855 (51.855)\n",
      "Test: [9/9]\tTime 0.001 (0.002)\tLoss 1.5041 (1.4931)\tPrec@1 49.107 (51.140)\n",
      "EPOCH: 99 val Results: Prec@1 51.140 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "current lr 2.46720e-07\n",
      "Epoch: [100][0/48]\tTime 0.004 (0.004)\tLoss 1.4855 (1.4855)\tPrec@1 53.223 (53.223)\n",
      "Epoch: [100][9/48]\tTime 0.003 (0.007)\tLoss 1.4457 (1.4576)\tPrec@1 53.223 (53.496)\n",
      "Epoch: [100][18/48]\tTime 0.010 (0.009)\tLoss 1.4647 (1.4574)\tPrec@1 52.832 (53.331)\n",
      "Epoch: [100][27/48]\tTime 0.003 (0.008)\tLoss 1.4416 (1.4602)\tPrec@1 53.320 (53.090)\n",
      "Epoch: [100][36/48]\tTime 0.005 (0.008)\tLoss 1.4729 (1.4618)\tPrec@1 51.270 (52.985)\n",
      "Epoch: [100][45/48]\tTime 0.003 (0.007)\tLoss 1.4126 (1.4599)\tPrec@1 55.371 (53.144)\n",
      "Epoch: [100][48/48]\tTime 0.010 (0.007)\tLoss 1.4735 (1.4598)\tPrec@1 52.712 (53.146)\n",
      "EPOCH: 100 train Results: Prec@1 53.146 \n",
      "Test: [0/9]\tTime 0.001 (0.001)\tLoss 1.5009 (1.5009)\tPrec@1 52.344 (52.344)\n",
      "Test: [9/9]\tTime 0.000 (0.001)\tLoss 1.5112 (1.5010)\tPrec@1 48.980 (51.230)\n",
      "EPOCH: 100 val Results: Prec@1 51.230 \n",
      "Best Prec@1: 51.910\n",
      "\n",
      "End time:  Fri Mar 29 01:36:51 2024\n",
      "main executed in 30.5878 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'gelu': gelu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "@timer\n",
    "def main():\n",
    "    file_path = './Assignment1-Dataset/'\n",
    "\n",
    "    train_X = np.load(file_path + 'train_data.npy')\n",
    "    train_y = np.load(file_path + 'train_label.npy')\n",
    "    test_X = np.load(file_path + 'test_data.npy')\n",
    "    test_y = np.load(file_path + 'test_label.npy')\n",
    "\n",
    "    layers = [\n",
    "        {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 64}},\n",
    "        {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 64}}, \n",
    "        {'type': 'dropout', 'params': {'name': 'dropout', 'drop_rate': 0.1}},\n",
    "        # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "        # {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "        {'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "        #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "        #{'type': 'gelu', 'params': {'name': 'gelu1'}},  \n",
    "        # {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "        # {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "        {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 64, 'out_num': 10}},\n",
    "    ]\n",
    "  \n",
    "    bs = 1024\n",
    "    config = {\n",
    "        'layers': layers,\n",
    "        'lr': 0.001, \n",
    "        'bs': bs,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "        'seed': 0,\n",
    "        'epoch': 100,\n",
    "        'optimizer': 'adam',  # adam, sgd\n",
    "        'pre-process': 'norm',      # min-max, norm, None\n",
    "        'print_freq': 50000 // bs // 5\n",
    "    }\n",
    "    np.random.seed(config['seed'])\n",
    "\n",
    "    # pre process\n",
    "    train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "    train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "    test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "    model = get_model(config['layers'])\n",
    "    trainer = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "    trainer.train()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
