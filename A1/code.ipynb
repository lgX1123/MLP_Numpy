{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 520556528 + 530101303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Assignment1-Dataset/'\n",
    "\n",
    "train_X = np.load(file_path + 'train_data.npy')\n",
    "train_y = np.load(file_path + 'train_label.npy')\n",
    "test_X = np.load(file_path + 'test_data.npy')\n",
    "test_y = np.load(file_path + 'test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples of train set: 50000.\n",
      "Number of features: 128.\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of samples of train set: {train_X.shape[0]}.')\n",
    "print(f'Number of features: {train_X.shape[1]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class: 10.\n",
      "All classes: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of class: {np.unique(train_y).shape[0]}.')\n",
    "print(f'All classes: {np.unique(train_y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decorator for recording training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Sun Apr  7 21:30:38 2024\n",
      "End time:  Sun Apr  7 21:30:39 2024\n",
      "test_fun executed in 1.0051 seconds\n"
     ]
    }
   ],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timer\n",
    "def test_fun(x):\n",
    "    time.sleep(x)\n",
    "\n",
    "test_fun(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py.\n",
    "\n",
    "Modify tensor to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    SELU              :math:`\\frac{3}{4}`\n",
    "    ================= ====================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    \"\"\"Parameter class for saving data and gradients\"\"\"\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    \"\"\"\n",
    "        output: [batch_size, num_of_class]\n",
    "        target: [batch_size, 1]\n",
    "    \"\"\"\n",
    "    preds = output.argmax(axis=-1, keepdims=True)\n",
    "    return np.mean(preds == target) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-max normalization:\n",
    "\n",
    "$$x_{min-max} = {{x-min(x)}\\over{max(x)-min(x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization:\n",
    "\n",
    "$$x_{norm} = {{x-\\mu}\\over{\\sigma}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train_X, test_X, mode=None):\n",
    "    if mode == 'min-max':\n",
    "        print('Pre-process: min-max normalization')\n",
    "        min_each_feature = np.min(train_X, axis=0)\n",
    "        max_each_feature = np.max(train_X, axis=0)\n",
    "        scale = max_each_feature - min_each_feature\n",
    "        scale[scale == 0] = 1   # To avoid divided by 0\n",
    "        scaled_train = (train_X - min_each_feature) / scale\n",
    "        scaled_test = (test_X - min_each_feature) / scale\n",
    "        return scaled_train, scaled_test\n",
    "\n",
    "    if mode == 'standardization':  \n",
    "        print('Pre-process: standardization')\n",
    "        std_each_feature = np.std(train_X, axis=0)\n",
    "        mean_each_feature = np.mean(train_X, axis=0)\n",
    "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
    "        norm_train = (train_X - mean_each_feature) / std_each_feature\n",
    "        norm_test = (test_X - mean_each_feature) / std_each_feature\n",
    "        return norm_train, norm_test\n",
    "    \n",
    "    print('No pre-process')\n",
    "\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name \n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward:\n",
    "$$f(x) = \\max(0, x)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward:\n",
    "\n",
    "$$f'(x) = \\begin{cases} \n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0 \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_output[self.input <= 0] = 0\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward:\n",
    "\n",
    "$$f(x) = \\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha x & \\text{if } x \\leq 0 \n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward:\n",
    "\n",
    "$$f'(x) = \\begin{cases} \n",
    "1 & \\text{if } x > 0 \\\\\n",
    "\\alpha & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaky_relu(Layer):\n",
    "    def __init__(self, name, alpha, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.where(input > 0, input, self.alpha * input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        tmp = np.where(self.input > 0, 1, self.alpha)\n",
    "        return tmp * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward:\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.y = 1. / (1. + np.exp(-input))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return self.y * (1 - self.y) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward:\n",
    "\n",
    "$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward:\n",
    "\n",
    "$$\\tanh'(x) = 1 - \\tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.y = np.tanh(input)\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return (1 - self.y ** 2) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward:\n",
    "\n",
    "$$softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x_max = input.max(axis=-1, keepdims=True)       # to avoid overflow\n",
    "        x_exp = np.exp(input - x_max)\n",
    "        return x_exp / x_exp.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # packaged in CrossEntropyLoss for more convenient grad computation\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{xW} + \\mathbf{b}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of W:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of b:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{y}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(Layer):\n",
    "    def __init__(self, name, in_num, out_num):\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.in_num = in_num\n",
    "        self.out_num = out_num\n",
    "\n",
    "        W = kaiming_normal_(np.array([0] * in_num * out_num).reshape(in_num, out_num), a=math.sqrt(5))\n",
    "        self.W = Parameter(W, self.requires_grad)\n",
    "        self.b = Parameter(np.zeros(out_num), self.requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "            input: [batch size, in_num]\n",
    "            W: [in_num, out_num]\n",
    "            b: [out_num]\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        return input @ self.W.data + self.b.data      # [batch size, in_num] @ [in_num, out_num] + [out_num] => [batch size, out_num]\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "            grad_output: [batch size, out_num]\n",
    "        \"\"\"\n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.W.grad = self.input.T @ grad_output / batch_size   # [in_num, batch size] @ [batch size, out_num] => [in_num, out_num], \n",
    "        self.b.grad = grad_output.sum(axis=0) / batch_size      # here divided by batch size to compute avg of gradient\n",
    "        return grad_output @ self.W.data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:\n",
    "\n",
    "$$CrossEntropy = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of softmax:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = \\sum_{i}^{d} \\left( \\frac{\\partial L}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_k} \\right)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}_i} = - \\frac{y_i}{\\hat{y}_i}, \\qquad \\frac{\\partial \\hat{y}_i}{\\partial z_k} = \\begin{cases} \n",
    "\\hat{y}_i(1 - \\hat{y}_i) & \\text{if } i = k \\\\\n",
    "-\\hat{y}_k\\hat{y}_i & \\text{if } i \\neq k \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z_k} = - \\left( (y_k(1 - \\hat{y}_k)) - \\sum_{i \\neq k}^{d} y_i \\hat{y}_k \\right) = -(y_k - \\hat{y}_k \\sum_{i}^{d} y_i) = \\hat{y}_k - y_k\n",
    "$$\n",
    "\n",
    "$$=> \\frac{\\partial L}{\\partial z} = \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = softmax('softmax')\n",
    "\n",
    "    def __call__(self, input, ground_truth):\n",
    "        self.bacth_size = input.shape[0]\n",
    "        self.class_num = input.shape[1]\n",
    "\n",
    "        preds = self.softmax.forward(input)\n",
    "        ground_truth = self.one_hot_encoding(ground_truth)\n",
    "\n",
    "        self.grad = preds - ground_truth\n",
    "\n",
    "        loss = -1 * (ground_truth * np.log(preds + 1e-8)).sum() / self.bacth_size   # to avoid divided by 0\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.bacth_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.size), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchnorm(Layer):\n",
    "    def __init__(self, name, shape, requires_grad=True):\n",
    "        super().__init__(name)\n",
    "        self.gamma = Parameter(np.random.uniform(0.9, 1.1, shape), requires_grad, skip_decay=True)\n",
    "        self.beta = Parameter(np.random.uniform(-0.1, 0.1, shape), requires_grad, skip_decay=True)\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        self.running_mean = Parameter(np.zeros(shape), False)\n",
    "        self.running_var = Parameter(np.zeros(shape), False)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            batch_mean = input.mean(axis=0)\n",
    "            batch_var = input.var(axis=0)\n",
    "            batch_std = np.sqrt(batch_var + 1e-8)    # To avoid divided by 0\n",
    "\n",
    "            momentum = 0.9\n",
    "            self.running_mean.data = momentum * self.running_mean.data + (1 - momentum) * batch_mean\n",
    "            self.running_var.data = momentum * self.running_var.data + (1 - momentum) * batch_var\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            batch_mean = self.running_mean.data\n",
    "            batch_std = np.sqrt(self.running_var.data)\n",
    "\n",
    "        self.norm = (input - batch_mean) / batch_std\n",
    "        self.gamma_norm = self.gamma.data / batch_std\n",
    "\n",
    "        return self.gamma.data * self.norm + self.beta.data\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):        \n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.gamma.grad = (grad_output * self.norm).sum(axis=0) / batch_size\n",
    "        self.beta.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return self.gamma_norm * (grad_output - self.norm * self.gamma.grad - self.beta.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropout(Layer):\n",
    "    def __init__(self, name, drop_rate, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, input.shape) > self.drop_rate\n",
    "            return input * self.mask * self.fix_value\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.num_layers = 0\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if layer.requires_grad:\n",
    "            if hasattr(layer, 'W'):\n",
    "                self.params.append(layer.W)\n",
    "            if hasattr(layer, 'b'):\n",
    "                self.params.append(layer.b)\n",
    "            if hasattr(layer, 'gamma'):\n",
    "                self.params.append(layer.gamma)\n",
    "            if hasattr(layer, 'beta'):\n",
    "                self.params.append(layer.beta)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x):\n",
    "        for layer in self.layers[::-1]:\n",
    "            x = layer.backward(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = True\n",
    "    \n",
    "    def test(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:\n",
    "\n",
    "$$v_t = \\beta v_{t-1} + \\alpha g_t$$\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - v_t$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, $\\beta$ is the momentum term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, momentum, lr, weight_decay):\n",
    "        self.parameters = parameters\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        for i, (v, p) in enumerate(zip(self.v, self.parameters)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            v = self.momentum * v + self.lr * p.grad\n",
    "            self.v[i] = v\n",
    "            p.data -= self.v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
    "\n",
    "bias correction: $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$, $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    def __init__(self, parameters, lr, weight_decay=0, beta=(0.9, 0.999), eps=1e-8):\n",
    "        self.beta1 = beta[0]\n",
    "        self.beta2 = beta[1]\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.parameters = parameters\n",
    "        self.m = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.iterations += 1\n",
    "        for i, (p, m, v) in enumerate(zip(self.parameters, self.m, self.v)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            m = self.beta1 * m + (1 - self.beta1) * p.grad\n",
    "            v = self.beta2 * v + (1 - self.beta2) * np.power(p.grad, 2)\n",
    "\n",
    "            self.m[i] = m\n",
    "            self.v[i] = v\n",
    "            \n",
    "            # bias correction\n",
    "            m = m / (1 - np.power(self.beta1, self.iterations))\n",
    "            v = v / (1 - np.power(self.beta2, self.iterations))\n",
    "\n",
    "            p.data -= self.lr * m / (np.sqrt(v + self.eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Annealing Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula:\n",
    "\n",
    "$$\\eta_t = \\frac{1}{2}\\eta_{\\text{base}} (1 + \\cos(\\frac{T_{\\text{cur}}}{T_{\\max}}\\pi))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLR(object):\n",
    "    def __init__(self, optimizer, T_max):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.n = -1\n",
    "        self.base_lr = optimizer.lr\n",
    "        self.step()\n",
    "\n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        lr = self.get_lr()\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        cos = np.cos(np.pi * self.n / self.T_max)\n",
    "        return self.base_lr * (1 + cos) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, config, model=None, train_loader=None, val_loader=None):\n",
    "        self.config = config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr = self.config['lr']\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.print_freq = self.config['print_freq']\n",
    "        self.scheduler = self.config['scheduler']\n",
    "        self.train_precs = []\n",
    "        self.test_precs = []\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = SGD(self.model.params, self.config['momentum'], self.lr, self.config['weight_decay'])\n",
    "        elif self.config['optimizer'] == 'adam':\n",
    "            self.optimizer = Adam(self.model.params, self.lr, self.config['weight_decay'])\n",
    "        if self.scheduler == 'cos':\n",
    "            self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "\n",
    "    @timer\n",
    "    def train(self):\n",
    "        best_acc1 = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            if self.scheduler == 'cos':\n",
    "                self.train_scheduler.step()\n",
    "\n",
    "            # evaluate on validation set\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "\n",
    "    \n",
    "    def train_per_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        for i, (input, target) in enumerate(self.train_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # compute gradient and step\n",
    "            self.model.backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.train_loader) - 1):\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch + 1, i, len(self.train_loader) - 1, batch_time=batch_time,\n",
    "                        loss=losses, top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='train', top1=top1, losses=losses))\n",
    "        print(output)\n",
    "        self.train_losses.append(losses.avg)\n",
    "        self.train_precs.append(top1.avg)\n",
    "                \n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.test()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.val_loader) - 1):\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        i, len(self.val_loader) - 1, batch_time=batch_time, loss=losses,\n",
    "                        top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='val', top1=top1, losses=losses))\n",
    "        print(output)\n",
    "        self.test_losses.append(losses.avg)\n",
    "        self.test_precs.append(top1.avg)\n",
    "\n",
    "        return top1.avg\n",
    "    \n",
    "    def plot_cm(self, save_path):\n",
    "        self.model.test()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            output = np.argmax(output, axis=1)\n",
    "            y_pred += list(output)\n",
    "            y_true += list(target.flatten())\n",
    "            \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"Ground Truth\")\n",
    "        plt.xlabel(\"Prediction\")\n",
    "        plt.savefig(save_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "        \n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configs of Baseline vs Best Model\n",
    "\n",
    "| Modules                | Baseline          | Best Model        |\n",
    "| ---------------------- | ----------------- | ----------------- |\n",
    "| Batch size             | 128               | 1024              |\n",
    "| Learning rate          | 0.1               | 0.01              |\n",
    "| Scheduler              | CosineAnnealingLR | None              |\n",
    "| Epoch                  | 100               | 200               |\n",
    "| Pre-processing         | Standardization   | Standardization   |\n",
    "| Number of Hidden layer | 2                 | 2                 |\n",
    "| Hidden units           | [64, 32]          | [256, 128]        |\n",
    "| Activations            | [Relu, Relu]      | [Relu, Relu]      |\n",
    "| Weight initialisation  | Kaiming           | Kaiming           |\n",
    "| Weight decay           | 5e-4              | 5e-4              |\n",
    "| Optimizer              | SGD with Momentum | SGD with Momentum |\n",
    "| Momentum               | 0.9               | 0.9               |\n",
    "| Batch Normalisation    | Yes               | Yes               |\n",
    "| Dropout rate           | 0.1               | 0.3               |\n",
    "| Accuracy               | 53.03%            | 58.71%            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-process: standardization\n",
      "Start time:  Sun Apr  7 21:38:34 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/48]\tTime 0.069 (0.069)\tLoss 6.3604 (6.3604)\tPrec@1 10.059 (10.059)\n",
      "Epoch: [1][9/48]\tTime 0.019 (0.029)\tLoss 5.7673 (6.0833)\tPrec@1 10.254 (10.771)\n",
      "Epoch: [1][18/48]\tTime 0.049 (0.032)\tLoss 4.9681 (5.7364)\tPrec@1 13.867 (11.570)\n",
      "Epoch: [1][27/48]\tTime 0.031 (0.030)\tLoss 4.5470 (5.4580)\tPrec@1 15.723 (12.266)\n",
      "Epoch: [1][36/48]\tTime 0.024 (0.030)\tLoss 4.0316 (5.2018)\tPrec@1 16.016 (12.962)\n",
      "Epoch: [1][45/48]\tTime 0.024 (0.028)\tLoss 3.9700 (4.9673)\tPrec@1 17.578 (13.827)\n",
      "Epoch: [1][48/48]\tTime 0.027 (0.029)\tLoss 3.9148 (4.9051)\tPrec@1 18.986 (14.072)\n",
      "EPOCH: 1 train Results: Prec@1 14.072 Loss: 4.9051\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 2.5207 (2.5207)\tPrec@1 23.438 (23.438)\n",
      "Test: [9/9]\tTime 0.012 (0.012)\tLoss 2.4671 (2.4695)\tPrec@1 24.235 (24.160)\n",
      "EPOCH: 1 val Results: Prec@1 24.160 Loss: 2.4695\n",
      "Best Prec@1: 24.160\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/48]\tTime 0.034 (0.034)\tLoss 3.6886 (3.6886)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [2][9/48]\tTime 0.032 (0.022)\tLoss 3.5065 (3.6394)\tPrec@1 18.945 (18.467)\n",
      "Epoch: [2][18/48]\tTime 0.020 (0.022)\tLoss 3.3405 (3.5276)\tPrec@1 21.387 (18.930)\n",
      "Epoch: [2][27/48]\tTime 0.020 (0.021)\tLoss 3.1686 (3.4438)\tPrec@1 21.484 (19.413)\n",
      "Epoch: [2][36/48]\tTime 0.040 (0.022)\tLoss 3.1179 (3.3656)\tPrec@1 20.996 (19.811)\n",
      "Epoch: [2][45/48]\tTime 0.019 (0.022)\tLoss 2.9688 (3.2950)\tPrec@1 21.484 (20.119)\n",
      "Epoch: [2][48/48]\tTime 0.017 (0.022)\tLoss 2.9902 (3.2750)\tPrec@1 23.821 (20.266)\n",
      "EPOCH: 2 train Results: Prec@1 20.266 Loss: 3.2750\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 2.1158 (2.1158)\tPrec@1 29.492 (29.492)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 2.0517 (2.0555)\tPrec@1 30.740 (30.110)\n",
      "EPOCH: 2 val Results: Prec@1 30.110 Loss: 2.0555\n",
      "Best Prec@1: 30.110\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/48]\tTime 0.025 (0.025)\tLoss 2.7495 (2.7495)\tPrec@1 24.023 (24.023)\n",
      "Epoch: [3][9/48]\tTime 0.049 (0.027)\tLoss 2.8650 (2.8634)\tPrec@1 22.656 (23.057)\n",
      "Epoch: [3][18/48]\tTime 0.028 (0.030)\tLoss 2.7034 (2.8145)\tPrec@1 24.121 (23.047)\n",
      "Epoch: [3][27/48]\tTime 0.040 (0.036)\tLoss 2.7192 (2.7798)\tPrec@1 22.363 (23.200)\n",
      "Epoch: [3][36/48]\tTime 0.021 (0.033)\tLoss 2.6361 (2.7456)\tPrec@1 24.219 (23.321)\n",
      "Epoch: [3][45/48]\tTime 0.022 (0.030)\tLoss 2.5153 (2.7097)\tPrec@1 24.805 (23.654)\n",
      "Epoch: [3][48/48]\tTime 0.030 (0.031)\tLoss 2.5097 (2.6996)\tPrec@1 25.590 (23.714)\n",
      "EPOCH: 3 train Results: Prec@1 23.714 Loss: 2.6996\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.9692 (1.9692)\tPrec@1 33.008 (33.008)\n",
      "Test: [9/9]\tTime 0.013 (0.011)\tLoss 1.9056 (1.9110)\tPrec@1 33.036 (32.960)\n",
      "EPOCH: 3 val Results: Prec@1 32.960 Loss: 1.9110\n",
      "Best Prec@1: 32.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/48]\tTime 0.018 (0.018)\tLoss 2.4991 (2.4991)\tPrec@1 25.195 (25.195)\n",
      "Epoch: [4][9/48]\tTime 0.019 (0.030)\tLoss 2.4951 (2.4789)\tPrec@1 25.586 (25.283)\n",
      "Epoch: [4][18/48]\tTime 0.019 (0.037)\tLoss 2.4547 (2.4551)\tPrec@1 25.195 (25.601)\n",
      "Epoch: [4][27/48]\tTime 0.021 (0.032)\tLoss 2.2913 (2.4244)\tPrec@1 26.953 (25.820)\n",
      "Epoch: [4][36/48]\tTime 0.027 (0.029)\tLoss 2.3667 (2.4048)\tPrec@1 24.707 (25.757)\n",
      "Epoch: [4][45/48]\tTime 0.031 (0.028)\tLoss 2.3214 (2.3866)\tPrec@1 27.051 (25.860)\n",
      "Epoch: [4][48/48]\tTime 0.017 (0.028)\tLoss 2.1949 (2.3794)\tPrec@1 27.712 (25.922)\n",
      "EPOCH: 4 train Results: Prec@1 25.922 Loss: 2.3794\n",
      "Test: [0/9]\tTime 0.021 (0.021)\tLoss 1.8910 (1.8910)\tPrec@1 34.277 (34.277)\n",
      "Test: [9/9]\tTime 0.002 (0.008)\tLoss 1.8292 (1.8369)\tPrec@1 35.459 (35.020)\n",
      "EPOCH: 4 val Results: Prec@1 35.020 Loss: 1.8369\n",
      "Best Prec@1: 35.020\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/48]\tTime 0.027 (0.027)\tLoss 2.3123 (2.3123)\tPrec@1 24.023 (24.023)\n",
      "Epoch: [5][9/48]\tTime 0.026 (0.031)\tLoss 2.2414 (2.2464)\tPrec@1 27.344 (26.855)\n",
      "Epoch: [5][18/48]\tTime 0.021 (0.027)\tLoss 2.2352 (2.2446)\tPrec@1 25.293 (26.886)\n",
      "Epoch: [5][27/48]\tTime 0.021 (0.025)\tLoss 2.2065 (2.2325)\tPrec@1 25.977 (27.138)\n",
      "Epoch: [5][36/48]\tTime 0.015 (0.025)\tLoss 2.1471 (2.2232)\tPrec@1 26.758 (27.122)\n",
      "Epoch: [5][45/48]\tTime 0.014 (0.024)\tLoss 2.2061 (2.2101)\tPrec@1 26.758 (27.312)\n",
      "Epoch: [5][48/48]\tTime 0.021 (0.023)\tLoss 2.1916 (2.2068)\tPrec@1 29.835 (27.422)\n",
      "EPOCH: 5 train Results: Prec@1 27.422 Loss: 2.2068\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.8424 (1.8424)\tPrec@1 35.156 (35.156)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.7869 (1.7938)\tPrec@1 36.352 (36.330)\n",
      "EPOCH: 5 val Results: Prec@1 36.330 Loss: 1.7938\n",
      "Best Prec@1: 36.330\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/48]\tTime 0.014 (0.014)\tLoss 2.1191 (2.1191)\tPrec@1 29.883 (29.883)\n",
      "Epoch: [6][9/48]\tTime 0.038 (0.021)\tLoss 2.0729 (2.1022)\tPrec@1 29.492 (29.160)\n",
      "Epoch: [6][18/48]\tTime 0.018 (0.021)\tLoss 2.0343 (2.0906)\tPrec@1 29.980 (29.323)\n",
      "Epoch: [6][27/48]\tTime 0.015 (0.021)\tLoss 2.0500 (2.0858)\tPrec@1 29.492 (29.360)\n",
      "Epoch: [6][36/48]\tTime 0.014 (0.020)\tLoss 2.0118 (2.0752)\tPrec@1 31.152 (29.426)\n",
      "Epoch: [6][45/48]\tTime 0.014 (0.020)\tLoss 2.0316 (2.0680)\tPrec@1 31.055 (29.445)\n",
      "Epoch: [6][48/48]\tTime 0.015 (0.020)\tLoss 2.0699 (2.0657)\tPrec@1 26.887 (29.396)\n",
      "EPOCH: 6 train Results: Prec@1 29.396 Loss: 2.0657\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.8076 (1.8076)\tPrec@1 35.645 (35.645)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.7571 (1.7651)\tPrec@1 36.607 (37.220)\n",
      "EPOCH: 6 val Results: Prec@1 37.220 Loss: 1.7651\n",
      "Best Prec@1: 37.220\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/48]\tTime 0.031 (0.031)\tLoss 1.9746 (1.9746)\tPrec@1 31.836 (31.836)\n",
      "Epoch: [7][9/48]\tTime 0.019 (0.028)\tLoss 1.9601 (2.0173)\tPrec@1 30.273 (30.146)\n",
      "Epoch: [7][18/48]\tTime 0.033 (0.026)\tLoss 1.9781 (2.0149)\tPrec@1 29.004 (29.893)\n",
      "Epoch: [7][27/48]\tTime 0.023 (0.026)\tLoss 1.9554 (2.0029)\tPrec@1 32.715 (30.315)\n",
      "Epoch: [7][36/48]\tTime 0.018 (0.025)\tLoss 1.9343 (1.9905)\tPrec@1 31.250 (30.566)\n",
      "Epoch: [7][45/48]\tTime 0.031 (0.026)\tLoss 1.9163 (1.9861)\tPrec@1 33.008 (30.692)\n",
      "Epoch: [7][48/48]\tTime 0.018 (0.026)\tLoss 2.0358 (1.9875)\tPrec@1 28.892 (30.666)\n",
      "EPOCH: 7 train Results: Prec@1 30.666 Loss: 1.9875\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.7816 (1.7816)\tPrec@1 36.523 (36.523)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.7362 (1.7439)\tPrec@1 37.500 (38.320)\n",
      "EPOCH: 7 val Results: Prec@1 38.320 Loss: 1.7439\n",
      "Best Prec@1: 38.320\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/48]\tTime 0.021 (0.021)\tLoss 1.9263 (1.9263)\tPrec@1 31.738 (31.738)\n",
      "Epoch: [8][9/48]\tTime 0.018 (0.020)\tLoss 1.9718 (1.9574)\tPrec@1 32.520 (31.807)\n",
      "Epoch: [8][18/48]\tTime 0.033 (0.023)\tLoss 1.9030 (1.9548)\tPrec@1 34.375 (31.363)\n",
      "Epoch: [8][27/48]\tTime 0.040 (0.025)\tLoss 1.9738 (1.9464)\tPrec@1 30.469 (31.522)\n",
      "Epoch: [8][36/48]\tTime 0.020 (0.024)\tLoss 1.9328 (1.9365)\tPrec@1 31.445 (31.617)\n",
      "Epoch: [8][45/48]\tTime 0.018 (0.024)\tLoss 1.9202 (1.9346)\tPrec@1 32.129 (31.660)\n",
      "Epoch: [8][48/48]\tTime 0.019 (0.024)\tLoss 1.9040 (1.9320)\tPrec@1 33.491 (31.666)\n",
      "EPOCH: 8 train Results: Prec@1 31.666 Loss: 1.9320\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.7614 (1.7614)\tPrec@1 37.598 (37.598)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.7181 (1.7259)\tPrec@1 39.158 (39.110)\n",
      "EPOCH: 8 val Results: Prec@1 39.110 Loss: 1.7259\n",
      "Best Prec@1: 39.110\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/48]\tTime 0.021 (0.021)\tLoss 1.8445 (1.8445)\tPrec@1 34.375 (34.375)\n",
      "Epoch: [9][9/48]\tTime 0.023 (0.023)\tLoss 1.8723 (1.8867)\tPrec@1 32.129 (32.090)\n",
      "Epoch: [9][18/48]\tTime 0.023 (0.023)\tLoss 1.9008 (1.8869)\tPrec@1 33.398 (32.874)\n",
      "Epoch: [9][27/48]\tTime 0.021 (0.025)\tLoss 1.8607 (1.8887)\tPrec@1 34.668 (32.840)\n",
      "Epoch: [9][36/48]\tTime 0.018 (0.027)\tLoss 1.8798 (1.8858)\tPrec@1 32.520 (32.921)\n",
      "Epoch: [9][45/48]\tTime 0.033 (0.027)\tLoss 1.9070 (1.8830)\tPrec@1 31.445 (32.863)\n",
      "Epoch: [9][48/48]\tTime 0.016 (0.027)\tLoss 1.8762 (1.8815)\tPrec@1 32.429 (32.888)\n",
      "EPOCH: 9 train Results: Prec@1 32.888 Loss: 1.8815\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.7434 (1.7434)\tPrec@1 38.867 (38.867)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.7035 (1.7107)\tPrec@1 40.051 (39.630)\n",
      "EPOCH: 9 val Results: Prec@1 39.630 Loss: 1.7107\n",
      "Best Prec@1: 39.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/48]\tTime 0.020 (0.020)\tLoss 1.8433 (1.8433)\tPrec@1 33.984 (33.984)\n",
      "Epoch: [10][9/48]\tTime 0.022 (0.021)\tLoss 1.8394 (1.8316)\tPrec@1 34.766 (34.434)\n",
      "Epoch: [10][18/48]\tTime 0.021 (0.021)\tLoss 1.8200 (1.8439)\tPrec@1 35.449 (34.005)\n",
      "Epoch: [10][27/48]\tTime 0.021 (0.021)\tLoss 1.8408 (1.8427)\tPrec@1 33.398 (33.970)\n",
      "Epoch: [10][36/48]\tTime 0.024 (0.023)\tLoss 1.8563 (1.8431)\tPrec@1 33.789 (33.728)\n",
      "Epoch: [10][45/48]\tTime 0.020 (0.023)\tLoss 1.8445 (1.8388)\tPrec@1 33.789 (33.944)\n",
      "Epoch: [10][48/48]\tTime 0.033 (0.023)\tLoss 1.8260 (1.8372)\tPrec@1 34.316 (33.970)\n",
      "EPOCH: 10 train Results: Prec@1 33.970 Loss: 1.8372\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.7251 (1.7251)\tPrec@1 39.453 (39.453)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.6891 (1.6956)\tPrec@1 39.923 (40.410)\n",
      "EPOCH: 10 val Results: Prec@1 40.410 Loss: 1.6956\n",
      "Best Prec@1: 40.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/48]\tTime 0.024 (0.024)\tLoss 1.8199 (1.8199)\tPrec@1 32.910 (32.910)\n",
      "Epoch: [11][9/48]\tTime 0.017 (0.027)\tLoss 1.7935 (1.8183)\tPrec@1 36.133 (34.531)\n",
      "Epoch: [11][18/48]\tTime 0.020 (0.024)\tLoss 1.8194 (1.8212)\tPrec@1 34.082 (34.457)\n",
      "Epoch: [11][27/48]\tTime 0.015 (0.022)\tLoss 1.8373 (1.8203)\tPrec@1 34.082 (34.755)\n",
      "Epoch: [11][36/48]\tTime 0.019 (0.022)\tLoss 1.7940 (1.8154)\tPrec@1 35.645 (34.826)\n",
      "Epoch: [11][45/48]\tTime 0.021 (0.022)\tLoss 1.8089 (1.8121)\tPrec@1 34.375 (34.925)\n",
      "Epoch: [11][48/48]\tTime 0.021 (0.022)\tLoss 1.7940 (1.8119)\tPrec@1 33.373 (34.930)\n",
      "EPOCH: 11 train Results: Prec@1 34.930 Loss: 1.8119\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.7109 (1.7109)\tPrec@1 39.746 (39.746)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.6781 (1.6828)\tPrec@1 40.179 (40.690)\n",
      "EPOCH: 11 val Results: Prec@1 40.690 Loss: 1.6828\n",
      "Best Prec@1: 40.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/48]\tTime 0.020 (0.020)\tLoss 1.8174 (1.8174)\tPrec@1 35.645 (35.645)\n",
      "Epoch: [12][9/48]\tTime 0.027 (0.025)\tLoss 1.7607 (1.7959)\tPrec@1 36.328 (35.430)\n",
      "Epoch: [12][18/48]\tTime 0.019 (0.023)\tLoss 1.7962 (1.7944)\tPrec@1 35.449 (35.639)\n",
      "Epoch: [12][27/48]\tTime 0.019 (0.022)\tLoss 1.7359 (1.7922)\tPrec@1 35.938 (35.557)\n",
      "Epoch: [12][36/48]\tTime 0.018 (0.021)\tLoss 1.7838 (1.7915)\tPrec@1 34.375 (35.647)\n",
      "Epoch: [12][45/48]\tTime 0.015 (0.021)\tLoss 1.7908 (1.7870)\tPrec@1 33.789 (35.723)\n",
      "Epoch: [12][48/48]\tTime 0.018 (0.021)\tLoss 1.7448 (1.7882)\tPrec@1 36.675 (35.692)\n",
      "EPOCH: 12 train Results: Prec@1 35.692 Loss: 1.7882\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.6961 (1.6961)\tPrec@1 40.625 (40.625)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.6641 (1.6700)\tPrec@1 41.454 (41.190)\n",
      "EPOCH: 12 val Results: Prec@1 41.190 Loss: 1.6700\n",
      "Best Prec@1: 41.190\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/48]\tTime 0.018 (0.018)\tLoss 1.7837 (1.7837)\tPrec@1 34.766 (34.766)\n",
      "Epoch: [13][9/48]\tTime 0.022 (0.023)\tLoss 1.7496 (1.7679)\tPrec@1 37.793 (35.898)\n",
      "Epoch: [13][18/48]\tTime 0.015 (0.021)\tLoss 1.7659 (1.7643)\tPrec@1 36.230 (36.292)\n",
      "Epoch: [13][27/48]\tTime 0.021 (0.022)\tLoss 1.7808 (1.7650)\tPrec@1 35.449 (36.391)\n",
      "Epoch: [13][36/48]\tTime 0.022 (0.022)\tLoss 1.7611 (1.7629)\tPrec@1 34.766 (36.405)\n",
      "Epoch: [13][45/48]\tTime 0.012 (0.021)\tLoss 1.7676 (1.7615)\tPrec@1 36.230 (36.419)\n",
      "Epoch: [13][48/48]\tTime 0.022 (0.021)\tLoss 1.7595 (1.7616)\tPrec@1 36.910 (36.412)\n",
      "EPOCH: 13 train Results: Prec@1 36.412 Loss: 1.7616\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.6823 (1.6823)\tPrec@1 40.723 (40.723)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.6532 (1.6584)\tPrec@1 41.071 (41.450)\n",
      "EPOCH: 13 val Results: Prec@1 41.450 Loss: 1.6584\n",
      "Best Prec@1: 41.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/48]\tTime 0.021 (0.021)\tLoss 1.7151 (1.7151)\tPrec@1 39.551 (39.551)\n",
      "Epoch: [14][9/48]\tTime 0.019 (0.028)\tLoss 1.7546 (1.7552)\tPrec@1 37.012 (36.523)\n",
      "Epoch: [14][18/48]\tTime 0.019 (0.024)\tLoss 1.7634 (1.7520)\tPrec@1 36.719 (36.744)\n",
      "Epoch: [14][27/48]\tTime 0.035 (0.025)\tLoss 1.7035 (1.7532)\tPrec@1 36.816 (36.621)\n",
      "Epoch: [14][36/48]\tTime 0.013 (0.024)\tLoss 1.7307 (1.7473)\tPrec@1 38.086 (36.896)\n",
      "Epoch: [14][45/48]\tTime 0.021 (0.024)\tLoss 1.7835 (1.7485)\tPrec@1 36.914 (36.957)\n",
      "Epoch: [14][48/48]\tTime 0.013 (0.023)\tLoss 1.7455 (1.7472)\tPrec@1 36.675 (36.958)\n",
      "EPOCH: 14 train Results: Prec@1 36.958 Loss: 1.7472\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.6718 (1.6718)\tPrec@1 40.234 (40.234)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.6443 (1.6486)\tPrec@1 40.434 (41.770)\n",
      "EPOCH: 14 val Results: Prec@1 41.770 Loss: 1.6486\n",
      "Best Prec@1: 41.770\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/48]\tTime 0.020 (0.020)\tLoss 1.6789 (1.6789)\tPrec@1 38.184 (38.184)\n",
      "Epoch: [15][9/48]\tTime 0.020 (0.025)\tLoss 1.7363 (1.7337)\tPrec@1 39.648 (37.432)\n",
      "Epoch: [15][18/48]\tTime 0.013 (0.021)\tLoss 1.7394 (1.7426)\tPrec@1 36.230 (37.125)\n",
      "Epoch: [15][27/48]\tTime 0.019 (0.025)\tLoss 1.7415 (1.7354)\tPrec@1 37.891 (37.263)\n",
      "Epoch: [15][36/48]\tTime 0.022 (0.024)\tLoss 1.7040 (1.7338)\tPrec@1 37.695 (37.249)\n",
      "Epoch: [15][45/48]\tTime 0.021 (0.023)\tLoss 1.6844 (1.7281)\tPrec@1 38.477 (37.489)\n",
      "Epoch: [15][48/48]\tTime 0.016 (0.023)\tLoss 1.7049 (1.7277)\tPrec@1 36.675 (37.438)\n",
      "EPOCH: 15 train Results: Prec@1 37.438 Loss: 1.7277\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.6588 (1.6588)\tPrec@1 41.406 (41.406)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.6324 (1.6374)\tPrec@1 41.327 (42.220)\n",
      "EPOCH: 15 val Results: Prec@1 42.220 Loss: 1.6374\n",
      "Best Prec@1: 42.220\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/48]\tTime 0.026 (0.026)\tLoss 1.7441 (1.7441)\tPrec@1 38.184 (38.184)\n",
      "Epoch: [16][9/48]\tTime 0.013 (0.025)\tLoss 1.6901 (1.7057)\tPrec@1 37.695 (38.594)\n",
      "Epoch: [16][18/48]\tTime 0.028 (0.025)\tLoss 1.7289 (1.7148)\tPrec@1 37.793 (38.040)\n",
      "Epoch: [16][27/48]\tTime 0.018 (0.027)\tLoss 1.6604 (1.7125)\tPrec@1 40.430 (38.372)\n",
      "Epoch: [16][36/48]\tTime 0.042 (0.027)\tLoss 1.6798 (1.7124)\tPrec@1 38.867 (38.247)\n",
      "Epoch: [16][45/48]\tTime 0.016 (0.028)\tLoss 1.7062 (1.7107)\tPrec@1 38.379 (38.311)\n",
      "Epoch: [16][48/48]\tTime 0.020 (0.028)\tLoss 1.6987 (1.7103)\tPrec@1 41.156 (38.362)\n",
      "EPOCH: 16 train Results: Prec@1 38.362 Loss: 1.7103\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.6494 (1.6494)\tPrec@1 41.699 (41.699)\n",
      "Test: [9/9]\tTime 0.002 (0.004)\tLoss 1.6231 (1.6274)\tPrec@1 41.454 (42.540)\n",
      "EPOCH: 16 val Results: Prec@1 42.540 Loss: 1.6274\n",
      "Best Prec@1: 42.540\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/48]\tTime 0.015 (0.015)\tLoss 1.6684 (1.6684)\tPrec@1 38.672 (38.672)\n",
      "Epoch: [17][9/48]\tTime 0.023 (0.020)\tLoss 1.6615 (1.6930)\tPrec@1 41.699 (38.828)\n",
      "Epoch: [17][18/48]\tTime 0.023 (0.021)\tLoss 1.6797 (1.7001)\tPrec@1 39.453 (38.775)\n",
      "Epoch: [17][27/48]\tTime 0.014 (0.021)\tLoss 1.7054 (1.7010)\tPrec@1 36.914 (38.595)\n",
      "Epoch: [17][36/48]\tTime 0.021 (0.020)\tLoss 1.7358 (1.6999)\tPrec@1 36.914 (38.564)\n",
      "Epoch: [17][45/48]\tTime 0.013 (0.021)\tLoss 1.7226 (1.6979)\tPrec@1 38.184 (38.697)\n",
      "Epoch: [17][48/48]\tTime 0.019 (0.021)\tLoss 1.6820 (1.6970)\tPrec@1 39.151 (38.734)\n",
      "EPOCH: 17 train Results: Prec@1 38.734 Loss: 1.6970\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.6373 (1.6373)\tPrec@1 42.578 (42.578)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.6143 (1.6168)\tPrec@1 42.219 (43.120)\n",
      "EPOCH: 17 val Results: Prec@1 43.120 Loss: 1.6168\n",
      "Best Prec@1: 43.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/48]\tTime 0.043 (0.043)\tLoss 1.6867 (1.6867)\tPrec@1 40.918 (40.918)\n",
      "Epoch: [18][9/48]\tTime 0.027 (0.030)\tLoss 1.7477 (1.6989)\tPrec@1 38.867 (39.160)\n",
      "Epoch: [18][18/48]\tTime 0.029 (0.030)\tLoss 1.7198 (1.6930)\tPrec@1 37.988 (39.278)\n",
      "Epoch: [18][27/48]\tTime 0.031 (0.030)\tLoss 1.7071 (1.6943)\tPrec@1 37.695 (39.108)\n",
      "Epoch: [18][36/48]\tTime 0.028 (0.030)\tLoss 1.6380 (1.6890)\tPrec@1 41.699 (39.345)\n",
      "Epoch: [18][45/48]\tTime 0.022 (0.028)\tLoss 1.6619 (1.6869)\tPrec@1 40.039 (39.483)\n",
      "Epoch: [18][48/48]\tTime 0.038 (0.028)\tLoss 1.7159 (1.6860)\tPrec@1 40.802 (39.518)\n",
      "EPOCH: 18 train Results: Prec@1 39.518 Loss: 1.6860\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.6287 (1.6287)\tPrec@1 42.285 (42.285)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.6047 (1.6079)\tPrec@1 42.219 (43.120)\n",
      "EPOCH: 18 val Results: Prec@1 43.120 Loss: 1.6079\n",
      "Best Prec@1: 43.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/48]\tTime 0.032 (0.032)\tLoss 1.6750 (1.6750)\tPrec@1 37.695 (37.695)\n",
      "Epoch: [19][9/48]\tTime 0.018 (0.027)\tLoss 1.6846 (1.6622)\tPrec@1 40.039 (40.410)\n",
      "Epoch: [19][18/48]\tTime 0.025 (0.023)\tLoss 1.7076 (1.6631)\tPrec@1 40.039 (40.188)\n",
      "Epoch: [19][27/48]\tTime 0.023 (0.024)\tLoss 1.6702 (1.6665)\tPrec@1 38.770 (39.980)\n",
      "Epoch: [19][36/48]\tTime 0.027 (0.025)\tLoss 1.7090 (1.6732)\tPrec@1 37.695 (39.844)\n",
      "Epoch: [19][45/48]\tTime 0.040 (0.025)\tLoss 1.6989 (1.6725)\tPrec@1 39.941 (39.950)\n",
      "Epoch: [19][48/48]\tTime 0.021 (0.026)\tLoss 1.7095 (1.6717)\tPrec@1 37.618 (39.964)\n",
      "EPOCH: 19 train Results: Prec@1 39.964 Loss: 1.6717\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.6182 (1.6182)\tPrec@1 42.969 (42.969)\n",
      "Test: [9/9]\tTime 0.002 (0.006)\tLoss 1.5961 (1.5983)\tPrec@1 42.219 (43.610)\n",
      "EPOCH: 19 val Results: Prec@1 43.610 Loss: 1.5983\n",
      "Best Prec@1: 43.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/48]\tTime 0.028 (0.028)\tLoss 1.6385 (1.6385)\tPrec@1 42.383 (42.383)\n",
      "Epoch: [20][9/48]\tTime 0.021 (0.026)\tLoss 1.6292 (1.6663)\tPrec@1 41.699 (40.801)\n",
      "Epoch: [20][18/48]\tTime 0.020 (0.024)\tLoss 1.6337 (1.6639)\tPrec@1 42.676 (40.682)\n",
      "Epoch: [20][27/48]\tTime 0.044 (0.024)\tLoss 1.6445 (1.6631)\tPrec@1 39.746 (40.395)\n",
      "Epoch: [20][36/48]\tTime 0.035 (0.026)\tLoss 1.6459 (1.6597)\tPrec@1 39.453 (40.625)\n",
      "Epoch: [20][45/48]\tTime 0.041 (0.026)\tLoss 1.6127 (1.6567)\tPrec@1 42.773 (40.778)\n",
      "Epoch: [20][48/48]\tTime 0.016 (0.026)\tLoss 1.6845 (1.6573)\tPrec@1 39.151 (40.758)\n",
      "EPOCH: 20 train Results: Prec@1 40.758 Loss: 1.6573\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.6081 (1.6081)\tPrec@1 42.871 (42.871)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.5873 (1.5889)\tPrec@1 41.964 (43.740)\n",
      "EPOCH: 20 val Results: Prec@1 43.740 Loss: 1.5889\n",
      "Best Prec@1: 43.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/48]\tTime 0.019 (0.019)\tLoss 1.6679 (1.6679)\tPrec@1 39.941 (39.941)\n",
      "Epoch: [21][9/48]\tTime 0.020 (0.027)\tLoss 1.6457 (1.6423)\tPrec@1 40.430 (40.684)\n",
      "Epoch: [21][18/48]\tTime 0.055 (0.027)\tLoss 1.6324 (1.6510)\tPrec@1 41.016 (40.389)\n",
      "Epoch: [21][27/48]\tTime 0.022 (0.027)\tLoss 1.6568 (1.6504)\tPrec@1 41.895 (40.625)\n",
      "Epoch: [21][36/48]\tTime 0.019 (0.025)\tLoss 1.6811 (1.6490)\tPrec@1 40.820 (40.691)\n",
      "Epoch: [21][45/48]\tTime 0.024 (0.025)\tLoss 1.6401 (1.6472)\tPrec@1 41.797 (40.795)\n",
      "Epoch: [21][48/48]\tTime 0.015 (0.024)\tLoss 1.6785 (1.6468)\tPrec@1 39.387 (40.818)\n",
      "EPOCH: 21 train Results: Prec@1 40.818 Loss: 1.6468\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.5984 (1.5984)\tPrec@1 43.652 (43.652)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.5789 (1.5804)\tPrec@1 42.474 (43.980)\n",
      "EPOCH: 21 val Results: Prec@1 43.980 Loss: 1.5804\n",
      "Best Prec@1: 43.980\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/48]\tTime 0.026 (0.026)\tLoss 1.6114 (1.6114)\tPrec@1 43.555 (43.555)\n",
      "Epoch: [22][9/48]\tTime 0.031 (0.026)\tLoss 1.6100 (1.6443)\tPrec@1 41.602 (41.074)\n",
      "Epoch: [22][18/48]\tTime 0.017 (0.023)\tLoss 1.6500 (1.6346)\tPrec@1 41.895 (41.350)\n",
      "Epoch: [22][27/48]\tTime 0.021 (0.024)\tLoss 1.6731 (1.6364)\tPrec@1 38.770 (41.354)\n",
      "Epoch: [22][36/48]\tTime 0.015 (0.022)\tLoss 1.6468 (1.6328)\tPrec@1 39.551 (41.309)\n",
      "Epoch: [22][45/48]\tTime 0.023 (0.022)\tLoss 1.6779 (1.6324)\tPrec@1 38.965 (41.338)\n",
      "Epoch: [22][48/48]\tTime 0.022 (0.022)\tLoss 1.6884 (1.6341)\tPrec@1 38.561 (41.310)\n",
      "EPOCH: 22 train Results: Prec@1 41.310 Loss: 1.6341\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.5880 (1.5880)\tPrec@1 43.750 (43.750)\n",
      "Test: [9/9]\tTime 0.010 (0.006)\tLoss 1.5700 (1.5705)\tPrec@1 42.602 (44.200)\n",
      "EPOCH: 22 val Results: Prec@1 44.200 Loss: 1.5705\n",
      "Best Prec@1: 44.200\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/48]\tTime 0.024 (0.024)\tLoss 1.5979 (1.5979)\tPrec@1 43.164 (43.164)\n",
      "Epoch: [23][9/48]\tTime 0.039 (0.025)\tLoss 1.6254 (1.6260)\tPrec@1 40.918 (41.816)\n",
      "Epoch: [23][18/48]\tTime 0.037 (0.026)\tLoss 1.6171 (1.6304)\tPrec@1 40.234 (41.740)\n",
      "Epoch: [23][27/48]\tTime 0.019 (0.024)\tLoss 1.5525 (1.6204)\tPrec@1 45.117 (42.010)\n",
      "Epoch: [23][36/48]\tTime 0.016 (0.023)\tLoss 1.5902 (1.6189)\tPrec@1 44.043 (42.100)\n",
      "Epoch: [23][45/48]\tTime 0.025 (0.023)\tLoss 1.6133 (1.6179)\tPrec@1 41.113 (42.009)\n",
      "Epoch: [23][48/48]\tTime 0.018 (0.023)\tLoss 1.6293 (1.6196)\tPrec@1 39.741 (41.878)\n",
      "EPOCH: 23 train Results: Prec@1 41.878 Loss: 1.6196\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.5795 (1.5795)\tPrec@1 43.945 (43.945)\n",
      "Test: [9/9]\tTime 0.010 (0.006)\tLoss 1.5618 (1.5619)\tPrec@1 43.495 (44.410)\n",
      "EPOCH: 23 val Results: Prec@1 44.410 Loss: 1.5619\n",
      "Best Prec@1: 44.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/48]\tTime 0.015 (0.015)\tLoss 1.6318 (1.6318)\tPrec@1 43.066 (43.066)\n",
      "Epoch: [24][9/48]\tTime 0.027 (0.019)\tLoss 1.6264 (1.6113)\tPrec@1 42.285 (42.764)\n",
      "Epoch: [24][18/48]\tTime 0.019 (0.019)\tLoss 1.6098 (1.6112)\tPrec@1 41.211 (42.444)\n",
      "Epoch: [24][27/48]\tTime 0.014 (0.023)\tLoss 1.6486 (1.6173)\tPrec@1 41.895 (42.250)\n",
      "Epoch: [24][36/48]\tTime 0.013 (0.022)\tLoss 1.5981 (1.6157)\tPrec@1 41.797 (42.227)\n",
      "Epoch: [24][45/48]\tTime 0.015 (0.021)\tLoss 1.5675 (1.6108)\tPrec@1 43.848 (42.427)\n",
      "Epoch: [24][48/48]\tTime 0.013 (0.021)\tLoss 1.6211 (1.6124)\tPrec@1 43.278 (42.400)\n",
      "EPOCH: 24 train Results: Prec@1 42.400 Loss: 1.6124\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.5726 (1.5726)\tPrec@1 44.531 (44.531)\n",
      "Test: [9/9]\tTime 0.006 (0.004)\tLoss 1.5539 (1.5535)\tPrec@1 44.005 (44.890)\n",
      "EPOCH: 24 val Results: Prec@1 44.890 Loss: 1.5535\n",
      "Best Prec@1: 44.890\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/48]\tTime 0.020 (0.020)\tLoss 1.6008 (1.6008)\tPrec@1 41.895 (41.895)\n",
      "Epoch: [25][9/48]\tTime 0.018 (0.022)\tLoss 1.5508 (1.5996)\tPrec@1 42.578 (42.363)\n",
      "Epoch: [25][18/48]\tTime 0.015 (0.021)\tLoss 1.5705 (1.6017)\tPrec@1 43.555 (42.465)\n",
      "Epoch: [25][27/48]\tTime 0.023 (0.022)\tLoss 1.6051 (1.6004)\tPrec@1 42.578 (42.829)\n",
      "Epoch: [25][36/48]\tTime 0.021 (0.022)\tLoss 1.5978 (1.6018)\tPrec@1 42.090 (42.742)\n",
      "Epoch: [25][45/48]\tTime 0.018 (0.022)\tLoss 1.5764 (1.6005)\tPrec@1 43.164 (42.665)\n",
      "Epoch: [25][48/48]\tTime 0.017 (0.022)\tLoss 1.6082 (1.6009)\tPrec@1 40.212 (42.614)\n",
      "EPOCH: 25 train Results: Prec@1 42.614 Loss: 1.6009\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.5624 (1.5624)\tPrec@1 45.508 (45.508)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.5466 (1.5454)\tPrec@1 43.622 (45.060)\n",
      "EPOCH: 25 val Results: Prec@1 45.060 Loss: 1.5454\n",
      "Best Prec@1: 45.060\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/48]\tTime 0.021 (0.021)\tLoss 1.6042 (1.6042)\tPrec@1 40.820 (40.820)\n",
      "Epoch: [26][9/48]\tTime 0.028 (0.023)\tLoss 1.5798 (1.5908)\tPrec@1 44.238 (43.145)\n",
      "Epoch: [26][18/48]\tTime 0.027 (0.029)\tLoss 1.5678 (1.5953)\tPrec@1 44.043 (43.205)\n",
      "Epoch: [26][27/48]\tTime 0.034 (0.028)\tLoss 1.5587 (1.5917)\tPrec@1 44.531 (43.105)\n",
      "Epoch: [26][36/48]\tTime 0.022 (0.025)\tLoss 1.5154 (1.5906)\tPrec@1 46.484 (43.006)\n",
      "Epoch: [26][45/48]\tTime 0.021 (0.025)\tLoss 1.5341 (1.5878)\tPrec@1 44.238 (43.134)\n",
      "Epoch: [26][48/48]\tTime 0.018 (0.025)\tLoss 1.6174 (1.5885)\tPrec@1 41.627 (43.086)\n",
      "EPOCH: 26 train Results: Prec@1 43.086 Loss: 1.5885\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.5541 (1.5541)\tPrec@1 44.824 (44.824)\n",
      "Test: [9/9]\tTime 0.005 (0.009)\tLoss 1.5369 (1.5356)\tPrec@1 43.878 (45.130)\n",
      "EPOCH: 26 val Results: Prec@1 45.130 Loss: 1.5356\n",
      "Best Prec@1: 45.130\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/48]\tTime 0.030 (0.030)\tLoss 1.5922 (1.5922)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [27][9/48]\tTime 0.022 (0.028)\tLoss 1.5869 (1.5691)\tPrec@1 41.895 (43.311)\n",
      "Epoch: [27][18/48]\tTime 0.021 (0.026)\tLoss 1.6076 (1.5719)\tPrec@1 40.527 (43.210)\n",
      "Epoch: [27][27/48]\tTime 0.022 (0.023)\tLoss 1.5578 (1.5730)\tPrec@1 43.848 (43.415)\n",
      "Epoch: [27][36/48]\tTime 0.021 (0.023)\tLoss 1.6086 (1.5805)\tPrec@1 41.992 (43.143)\n",
      "Epoch: [27][45/48]\tTime 0.014 (0.022)\tLoss 1.6416 (1.5825)\tPrec@1 40.918 (43.066)\n",
      "Epoch: [27][48/48]\tTime 0.012 (0.022)\tLoss 1.5535 (1.5828)\tPrec@1 44.340 (43.102)\n",
      "EPOCH: 27 train Results: Prec@1 43.102 Loss: 1.5828\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.5454 (1.5454)\tPrec@1 45.215 (45.215)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.5300 (1.5279)\tPrec@1 43.750 (45.300)\n",
      "EPOCH: 27 val Results: Prec@1 45.300 Loss: 1.5279\n",
      "Best Prec@1: 45.300\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/48]\tTime 0.015 (0.015)\tLoss 1.5906 (1.5906)\tPrec@1 41.895 (41.895)\n",
      "Epoch: [28][9/48]\tTime 0.018 (0.022)\tLoss 1.5365 (1.5842)\tPrec@1 43.164 (43.262)\n",
      "Epoch: [28][18/48]\tTime 0.018 (0.021)\tLoss 1.5851 (1.5765)\tPrec@1 41.895 (43.421)\n",
      "Epoch: [28][27/48]\tTime 0.013 (0.021)\tLoss 1.5644 (1.5735)\tPrec@1 43.262 (43.474)\n",
      "Epoch: [28][36/48]\tTime 0.018 (0.020)\tLoss 1.5867 (1.5739)\tPrec@1 42.871 (43.565)\n",
      "Epoch: [28][45/48]\tTime 0.020 (0.020)\tLoss 1.5159 (1.5725)\tPrec@1 43.066 (43.652)\n",
      "Epoch: [28][48/48]\tTime 0.016 (0.021)\tLoss 1.5910 (1.5721)\tPrec@1 41.392 (43.612)\n",
      "EPOCH: 28 train Results: Prec@1 43.612 Loss: 1.5721\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.5375 (1.5375)\tPrec@1 45.508 (45.508)\n",
      "Test: [9/9]\tTime 0.004 (0.004)\tLoss 1.5233 (1.5192)\tPrec@1 44.133 (45.550)\n",
      "EPOCH: 28 val Results: Prec@1 45.550 Loss: 1.5192\n",
      "Best Prec@1: 45.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/48]\tTime 0.019 (0.019)\tLoss 1.5870 (1.5870)\tPrec@1 43.652 (43.652)\n",
      "Epoch: [29][9/48]\tTime 0.028 (0.025)\tLoss 1.5219 (1.5698)\tPrec@1 45.508 (43.799)\n",
      "Epoch: [29][18/48]\tTime 0.030 (0.026)\tLoss 1.5539 (1.5618)\tPrec@1 43.164 (43.899)\n",
      "Epoch: [29][27/48]\tTime 0.042 (0.026)\tLoss 1.5453 (1.5593)\tPrec@1 43.945 (43.879)\n",
      "Epoch: [29][36/48]\tTime 0.104 (0.029)\tLoss 1.5665 (1.5616)\tPrec@1 41.504 (43.869)\n",
      "Epoch: [29][45/48]\tTime 0.032 (0.028)\tLoss 1.5433 (1.5580)\tPrec@1 44.824 (43.988)\n",
      "Epoch: [29][48/48]\tTime 0.016 (0.028)\tLoss 1.6055 (1.5590)\tPrec@1 40.920 (43.914)\n",
      "EPOCH: 29 train Results: Prec@1 43.914 Loss: 1.5590\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.5273 (1.5273)\tPrec@1 46.094 (46.094)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.5158 (1.5103)\tPrec@1 44.515 (46.020)\n",
      "EPOCH: 29 val Results: Prec@1 46.020 Loss: 1.5103\n",
      "Best Prec@1: 46.020\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/48]\tTime 0.020 (0.020)\tLoss 1.6109 (1.6109)\tPrec@1 40.137 (40.137)\n",
      "Epoch: [30][9/48]\tTime 0.021 (0.023)\tLoss 1.5808 (1.5572)\tPrec@1 43.262 (44.043)\n",
      "Epoch: [30][18/48]\tTime 0.037 (0.024)\tLoss 1.5475 (1.5511)\tPrec@1 44.043 (44.562)\n",
      "Epoch: [30][27/48]\tTime 0.017 (0.023)\tLoss 1.4970 (1.5472)\tPrec@1 47.656 (44.702)\n",
      "Epoch: [30][36/48]\tTime 0.020 (0.023)\tLoss 1.5658 (1.5490)\tPrec@1 43.750 (44.452)\n",
      "Epoch: [30][45/48]\tTime 0.014 (0.022)\tLoss 1.5706 (1.5505)\tPrec@1 42.188 (44.370)\n",
      "Epoch: [30][48/48]\tTime 0.024 (0.022)\tLoss 1.5385 (1.5511)\tPrec@1 44.458 (44.320)\n",
      "EPOCH: 30 train Results: Prec@1 44.320 Loss: 1.5511\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.5182 (1.5182)\tPrec@1 46.484 (46.484)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.5094 (1.5013)\tPrec@1 44.388 (46.310)\n",
      "EPOCH: 30 val Results: Prec@1 46.310 Loss: 1.5013\n",
      "Best Prec@1: 46.310\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/48]\tTime 0.016 (0.016)\tLoss 1.5109 (1.5109)\tPrec@1 45.605 (45.605)\n",
      "Epoch: [31][9/48]\tTime 0.019 (0.019)\tLoss 1.5446 (1.5352)\tPrec@1 44.727 (44.854)\n",
      "Epoch: [31][18/48]\tTime 0.014 (0.018)\tLoss 1.5396 (1.5361)\tPrec@1 45.508 (44.896)\n",
      "Epoch: [31][27/48]\tTime 0.025 (0.021)\tLoss 1.5334 (1.5383)\tPrec@1 45.020 (44.817)\n",
      "Epoch: [31][36/48]\tTime 0.031 (0.023)\tLoss 1.5207 (1.5370)\tPrec@1 47.754 (44.837)\n",
      "Epoch: [31][45/48]\tTime 0.020 (0.022)\tLoss 1.6083 (1.5396)\tPrec@1 40.527 (44.727)\n",
      "Epoch: [31][48/48]\tTime 0.018 (0.022)\tLoss 1.5653 (1.5389)\tPrec@1 42.689 (44.726)\n",
      "EPOCH: 31 train Results: Prec@1 44.726 Loss: 1.5389\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.5109 (1.5109)\tPrec@1 46.777 (46.777)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.5010 (1.4935)\tPrec@1 45.536 (46.690)\n",
      "EPOCH: 31 val Results: Prec@1 46.690 Loss: 1.4935\n",
      "Best Prec@1: 46.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/48]\tTime 0.016 (0.016)\tLoss 1.5532 (1.5532)\tPrec@1 46.094 (46.094)\n",
      "Epoch: [32][9/48]\tTime 0.015 (0.028)\tLoss 1.5319 (1.5409)\tPrec@1 46.582 (44.775)\n",
      "Epoch: [32][18/48]\tTime 0.021 (0.024)\tLoss 1.5267 (1.5330)\tPrec@1 46.289 (45.097)\n",
      "Epoch: [32][27/48]\tTime 0.018 (0.023)\tLoss 1.5490 (1.5335)\tPrec@1 44.629 (45.079)\n",
      "Epoch: [32][36/48]\tTime 0.022 (0.022)\tLoss 1.4632 (1.5309)\tPrec@1 46.680 (44.961)\n",
      "Epoch: [32][45/48]\tTime 0.024 (0.023)\tLoss 1.5413 (1.5320)\tPrec@1 45.020 (44.956)\n",
      "Epoch: [32][48/48]\tTime 0.019 (0.022)\tLoss 1.5555 (1.5321)\tPrec@1 47.170 (45.050)\n",
      "EPOCH: 32 train Results: Prec@1 45.050 Loss: 1.5321\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.5024 (1.5024)\tPrec@1 47.168 (47.168)\n",
      "Test: [9/9]\tTime 0.002 (0.004)\tLoss 1.4936 (1.4854)\tPrec@1 45.918 (46.950)\n",
      "EPOCH: 32 val Results: Prec@1 46.950 Loss: 1.4854\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/48]\tTime 0.023 (0.023)\tLoss 1.5874 (1.5874)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [33][9/48]\tTime 0.016 (0.020)\tLoss 1.5553 (1.5310)\tPrec@1 44.043 (45.420)\n",
      "Epoch: [33][18/48]\tTime 0.024 (0.023)\tLoss 1.5209 (1.5241)\tPrec@1 47.656 (45.400)\n",
      "Epoch: [33][27/48]\tTime 0.029 (0.024)\tLoss 1.4862 (1.5183)\tPrec@1 49.316 (45.682)\n",
      "Epoch: [33][36/48]\tTime 0.025 (0.023)\tLoss 1.5332 (1.5238)\tPrec@1 46.191 (45.537)\n",
      "Epoch: [33][45/48]\tTime 0.031 (0.024)\tLoss 1.5476 (1.5222)\tPrec@1 43.848 (45.629)\n",
      "Epoch: [33][48/48]\tTime 0.026 (0.024)\tLoss 1.5430 (1.5224)\tPrec@1 43.160 (45.572)\n",
      "EPOCH: 33 train Results: Prec@1 45.572 Loss: 1.5224\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.4937 (1.4937)\tPrec@1 47.266 (47.266)\n",
      "Test: [9/9]\tTime 0.002 (0.006)\tLoss 1.4876 (1.4775)\tPrec@1 46.301 (47.430)\n",
      "EPOCH: 33 val Results: Prec@1 47.430 Loss: 1.4775\n",
      "Best Prec@1: 47.430\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/48]\tTime 0.022 (0.022)\tLoss 1.5460 (1.5460)\tPrec@1 45.508 (45.508)\n",
      "Epoch: [34][9/48]\tTime 0.018 (0.024)\tLoss 1.5343 (1.5146)\tPrec@1 44.824 (46.123)\n",
      "Epoch: [34][18/48]\tTime 0.020 (0.022)\tLoss 1.5252 (1.5142)\tPrec@1 44.434 (45.868)\n",
      "Epoch: [34][27/48]\tTime 0.025 (0.023)\tLoss 1.4982 (1.5122)\tPrec@1 47.070 (45.902)\n",
      "Epoch: [34][36/48]\tTime 0.028 (0.024)\tLoss 1.5275 (1.5141)\tPrec@1 45.898 (45.867)\n",
      "Epoch: [34][45/48]\tTime 0.024 (0.023)\tLoss 1.4700 (1.5142)\tPrec@1 48.145 (45.867)\n",
      "Epoch: [34][48/48]\tTime 0.022 (0.023)\tLoss 1.5485 (1.5138)\tPrec@1 44.340 (45.858)\n",
      "EPOCH: 34 train Results: Prec@1 45.858 Loss: 1.5138\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.4860 (1.4860)\tPrec@1 47.168 (47.168)\n",
      "Test: [9/9]\tTime 0.008 (0.006)\tLoss 1.4799 (1.4687)\tPrec@1 46.301 (47.460)\n",
      "EPOCH: 34 val Results: Prec@1 47.460 Loss: 1.4687\n",
      "Best Prec@1: 47.460\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/48]\tTime 0.016 (0.016)\tLoss 1.4884 (1.4884)\tPrec@1 47.559 (47.559)\n",
      "Epoch: [35][9/48]\tTime 0.020 (0.018)\tLoss 1.5132 (1.5199)\tPrec@1 45.508 (46.191)\n",
      "Epoch: [35][18/48]\tTime 0.020 (0.019)\tLoss 1.4685 (1.5034)\tPrec@1 47.559 (46.186)\n",
      "Epoch: [35][27/48]\tTime 0.021 (0.020)\tLoss 1.4896 (1.5010)\tPrec@1 48.438 (46.380)\n",
      "Epoch: [35][36/48]\tTime 0.020 (0.020)\tLoss 1.5575 (1.5022)\tPrec@1 42.969 (46.228)\n",
      "Epoch: [35][45/48]\tTime 0.016 (0.021)\tLoss 1.5684 (1.5043)\tPrec@1 41.113 (46.098)\n",
      "Epoch: [35][48/48]\tTime 0.012 (0.021)\tLoss 1.4228 (1.5023)\tPrec@1 50.825 (46.266)\n",
      "EPOCH: 35 train Results: Prec@1 46.266 Loss: 1.5023\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.4763 (1.4763)\tPrec@1 47.363 (47.363)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.4732 (1.4606)\tPrec@1 47.066 (47.800)\n",
      "EPOCH: 35 val Results: Prec@1 47.800 Loss: 1.4606\n",
      "Best Prec@1: 47.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/48]\tTime 0.025 (0.025)\tLoss 1.5155 (1.5155)\tPrec@1 47.168 (47.168)\n",
      "Epoch: [36][9/48]\tTime 0.024 (0.022)\tLoss 1.5117 (1.4915)\tPrec@1 44.531 (45.830)\n",
      "Epoch: [36][18/48]\tTime 0.037 (0.024)\tLoss 1.4561 (1.4904)\tPrec@1 48.730 (46.279)\n",
      "Epoch: [36][27/48]\tTime 0.020 (0.023)\tLoss 1.4935 (1.4908)\tPrec@1 47.363 (46.387)\n",
      "Epoch: [36][36/48]\tTime 0.022 (0.023)\tLoss 1.5191 (1.4904)\tPrec@1 46.191 (46.360)\n",
      "Epoch: [36][45/48]\tTime 0.014 (0.023)\tLoss 1.5258 (1.4930)\tPrec@1 44.922 (46.300)\n",
      "Epoch: [36][48/48]\tTime 0.031 (0.023)\tLoss 1.5229 (1.4935)\tPrec@1 44.458 (46.310)\n",
      "EPOCH: 36 train Results: Prec@1 46.310 Loss: 1.4935\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.4663 (1.4663)\tPrec@1 48.145 (48.145)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.4663 (1.4517)\tPrec@1 47.449 (48.210)\n",
      "EPOCH: 36 val Results: Prec@1 48.210 Loss: 1.4517\n",
      "Best Prec@1: 48.210\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/48]\tTime 0.018 (0.018)\tLoss 1.4606 (1.4606)\tPrec@1 47.559 (47.559)\n",
      "Epoch: [37][9/48]\tTime 0.017 (0.022)\tLoss 1.4590 (1.4808)\tPrec@1 46.484 (46.855)\n",
      "Epoch: [37][18/48]\tTime 0.026 (0.021)\tLoss 1.5181 (1.4835)\tPrec@1 45.410 (46.916)\n",
      "Epoch: [37][27/48]\tTime 0.024 (0.021)\tLoss 1.5198 (1.4813)\tPrec@1 44.727 (47.032)\n",
      "Epoch: [37][36/48]\tTime 0.019 (0.021)\tLoss 1.4586 (1.4853)\tPrec@1 48.730 (46.796)\n",
      "Epoch: [37][45/48]\tTime 0.020 (0.021)\tLoss 1.4411 (1.4837)\tPrec@1 48.535 (46.856)\n",
      "Epoch: [37][48/48]\tTime 0.019 (0.021)\tLoss 1.4831 (1.4844)\tPrec@1 46.934 (46.804)\n",
      "EPOCH: 37 train Results: Prec@1 46.804 Loss: 1.4844\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.4595 (1.4595)\tPrec@1 47.461 (47.461)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.4599 (1.4444)\tPrec@1 47.066 (48.360)\n",
      "EPOCH: 37 val Results: Prec@1 48.360 Loss: 1.4444\n",
      "Best Prec@1: 48.360\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/48]\tTime 0.026 (0.026)\tLoss 1.4670 (1.4670)\tPrec@1 48.047 (48.047)\n",
      "Epoch: [38][9/48]\tTime 0.023 (0.022)\tLoss 1.5244 (1.4906)\tPrec@1 44.141 (45.938)\n",
      "Epoch: [38][18/48]\tTime 0.024 (0.021)\tLoss 1.4397 (1.4824)\tPrec@1 48.145 (46.361)\n",
      "Epoch: [38][27/48]\tTime 0.018 (0.022)\tLoss 1.4495 (1.4786)\tPrec@1 49.414 (46.791)\n",
      "Epoch: [38][36/48]\tTime 0.019 (0.021)\tLoss 1.4823 (1.4733)\tPrec@1 44.531 (46.909)\n",
      "Epoch: [38][45/48]\tTime 0.022 (0.021)\tLoss 1.5246 (1.4775)\tPrec@1 46.582 (46.756)\n",
      "Epoch: [38][48/48]\tTime 0.016 (0.021)\tLoss 1.4585 (1.4775)\tPrec@1 46.462 (46.796)\n",
      "EPOCH: 38 train Results: Prec@1 46.796 Loss: 1.4775\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.4477 (1.4477)\tPrec@1 47.852 (47.852)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.4504 (1.4357)\tPrec@1 47.321 (48.550)\n",
      "EPOCH: 38 val Results: Prec@1 48.550 Loss: 1.4357\n",
      "Best Prec@1: 48.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/48]\tTime 0.019 (0.019)\tLoss 1.4675 (1.4675)\tPrec@1 45.508 (45.508)\n",
      "Epoch: [39][9/48]\tTime 0.013 (0.018)\tLoss 1.4695 (1.4702)\tPrec@1 48.242 (46.699)\n",
      "Epoch: [39][18/48]\tTime 0.020 (0.017)\tLoss 1.4947 (1.4666)\tPrec@1 44.336 (47.338)\n",
      "Epoch: [39][27/48]\tTime 0.015 (0.018)\tLoss 1.4808 (1.4707)\tPrec@1 47.363 (47.402)\n",
      "Epoch: [39][36/48]\tTime 0.019 (0.018)\tLoss 1.4559 (1.4673)\tPrec@1 46.777 (47.545)\n",
      "Epoch: [39][45/48]\tTime 0.014 (0.018)\tLoss 1.4625 (1.4647)\tPrec@1 47.266 (47.627)\n",
      "Epoch: [39][48/48]\tTime 0.017 (0.019)\tLoss 1.5029 (1.4667)\tPrec@1 45.401 (47.546)\n",
      "EPOCH: 39 train Results: Prec@1 47.546 Loss: 1.4667\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.4386 (1.4386)\tPrec@1 48.535 (48.535)\n",
      "Test: [9/9]\tTime 0.002 (0.004)\tLoss 1.4422 (1.4281)\tPrec@1 47.704 (49.070)\n",
      "EPOCH: 39 val Results: Prec@1 49.070 Loss: 1.4281\n",
      "Best Prec@1: 49.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/48]\tTime 0.016 (0.016)\tLoss 1.4831 (1.4831)\tPrec@1 45.898 (45.898)\n",
      "Epoch: [40][9/48]\tTime 0.022 (0.020)\tLoss 1.4315 (1.4574)\tPrec@1 47.559 (46.943)\n",
      "Epoch: [40][18/48]\tTime 0.020 (0.019)\tLoss 1.4612 (1.4558)\tPrec@1 47.852 (47.379)\n",
      "Epoch: [40][27/48]\tTime 0.027 (0.019)\tLoss 1.4503 (1.4544)\tPrec@1 48.535 (47.461)\n",
      "Epoch: [40][36/48]\tTime 0.015 (0.019)\tLoss 1.3902 (1.4555)\tPrec@1 50.098 (47.545)\n",
      "Epoch: [40][45/48]\tTime 0.015 (0.019)\tLoss 1.4373 (1.4580)\tPrec@1 48.633 (47.565)\n",
      "Epoch: [40][48/48]\tTime 0.011 (0.019)\tLoss 1.4627 (1.4583)\tPrec@1 47.524 (47.586)\n",
      "EPOCH: 40 train Results: Prec@1 47.586 Loss: 1.4583\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.4298 (1.4298)\tPrec@1 49.023 (49.023)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.4352 (1.4191)\tPrec@1 47.959 (49.230)\n",
      "EPOCH: 40 val Results: Prec@1 49.230 Loss: 1.4191\n",
      "Best Prec@1: 49.230\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/48]\tTime 0.021 (0.021)\tLoss 1.4713 (1.4713)\tPrec@1 45.020 (45.020)\n",
      "Epoch: [41][9/48]\tTime 0.014 (0.020)\tLoss 1.4286 (1.4516)\tPrec@1 49.414 (47.842)\n",
      "Epoch: [41][18/48]\tTime 0.022 (0.021)\tLoss 1.4631 (1.4468)\tPrec@1 47.949 (47.954)\n",
      "Epoch: [41][27/48]\tTime 0.023 (0.022)\tLoss 1.3804 (1.4468)\tPrec@1 49.902 (47.984)\n",
      "Epoch: [41][36/48]\tTime 0.021 (0.022)\tLoss 1.4402 (1.4490)\tPrec@1 44.727 (47.796)\n",
      "Epoch: [41][45/48]\tTime 0.019 (0.023)\tLoss 1.4045 (1.4468)\tPrec@1 49.023 (47.939)\n",
      "Epoch: [41][48/48]\tTime 0.018 (0.023)\tLoss 1.4737 (1.4491)\tPrec@1 48.231 (47.882)\n",
      "EPOCH: 41 train Results: Prec@1 47.882 Loss: 1.4491\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.4191 (1.4191)\tPrec@1 48.828 (48.828)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.4277 (1.4109)\tPrec@1 47.959 (49.560)\n",
      "EPOCH: 41 val Results: Prec@1 49.560 Loss: 1.4109\n",
      "Best Prec@1: 49.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/48]\tTime 0.022 (0.022)\tLoss 1.4333 (1.4333)\tPrec@1 48.535 (48.535)\n",
      "Epoch: [42][9/48]\tTime 0.019 (0.020)\tLoss 1.4651 (1.4368)\tPrec@1 45.117 (48.125)\n",
      "Epoch: [42][18/48]\tTime 0.019 (0.019)\tLoss 1.4263 (1.4360)\tPrec@1 49.707 (48.268)\n",
      "Epoch: [42][27/48]\tTime 0.016 (0.019)\tLoss 1.4185 (1.4389)\tPrec@1 48.438 (48.284)\n",
      "Epoch: [42][36/48]\tTime 0.025 (0.020)\tLoss 1.4674 (1.4360)\tPrec@1 43.652 (48.266)\n",
      "Epoch: [42][45/48]\tTime 0.025 (0.021)\tLoss 1.4074 (1.4379)\tPrec@1 51.172 (48.181)\n",
      "Epoch: [42][48/48]\tTime 0.018 (0.021)\tLoss 1.4106 (1.4375)\tPrec@1 49.646 (48.214)\n",
      "EPOCH: 42 train Results: Prec@1 48.214 Loss: 1.4375\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.4106 (1.4106)\tPrec@1 49.023 (49.023)\n",
      "Test: [9/9]\tTime 0.006 (0.008)\tLoss 1.4199 (1.4030)\tPrec@1 48.469 (49.830)\n",
      "EPOCH: 42 val Results: Prec@1 49.830 Loss: 1.4030\n",
      "Best Prec@1: 49.830\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/48]\tTime 0.022 (0.022)\tLoss 1.4300 (1.4300)\tPrec@1 48.926 (48.926)\n",
      "Epoch: [43][9/48]\tTime 0.021 (0.027)\tLoss 1.4123 (1.4406)\tPrec@1 50.488 (48.281)\n",
      "Epoch: [43][18/48]\tTime 0.013 (0.024)\tLoss 1.3878 (1.4352)\tPrec@1 51.270 (48.283)\n",
      "Epoch: [43][27/48]\tTime 0.013 (0.023)\tLoss 1.3856 (1.4295)\tPrec@1 50.977 (48.615)\n",
      "Epoch: [43][36/48]\tTime 0.014 (0.023)\tLoss 1.4670 (1.4314)\tPrec@1 46.875 (48.620)\n",
      "Epoch: [43][45/48]\tTime 0.020 (0.024)\tLoss 1.4513 (1.4309)\tPrec@1 49.219 (48.622)\n",
      "Epoch: [43][48/48]\tTime 0.014 (0.023)\tLoss 1.3520 (1.4295)\tPrec@1 52.005 (48.674)\n",
      "EPOCH: 43 train Results: Prec@1 48.674 Loss: 1.4295\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.4004 (1.4004)\tPrec@1 49.414 (49.414)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.4110 (1.3943)\tPrec@1 48.597 (50.040)\n",
      "EPOCH: 43 val Results: Prec@1 50.040 Loss: 1.3943\n",
      "Best Prec@1: 50.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/48]\tTime 0.015 (0.015)\tLoss 1.4354 (1.4354)\tPrec@1 48.340 (48.340)\n",
      "Epoch: [44][9/48]\tTime 0.020 (0.023)\tLoss 1.3715 (1.4233)\tPrec@1 49.512 (49.121)\n",
      "Epoch: [44][18/48]\tTime 0.046 (0.026)\tLoss 1.4118 (1.4187)\tPrec@1 49.316 (49.327)\n",
      "Epoch: [44][27/48]\tTime 0.027 (0.025)\tLoss 1.4385 (1.4266)\tPrec@1 47.949 (48.989)\n",
      "Epoch: [44][36/48]\tTime 0.047 (0.026)\tLoss 1.3630 (1.4259)\tPrec@1 50.488 (48.831)\n",
      "Epoch: [44][45/48]\tTime 0.025 (0.025)\tLoss 1.4249 (1.4256)\tPrec@1 49.414 (48.968)\n",
      "Epoch: [44][48/48]\tTime 0.024 (0.026)\tLoss 1.4290 (1.4248)\tPrec@1 47.642 (48.994)\n",
      "EPOCH: 44 train Results: Prec@1 48.994 Loss: 1.4248\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.3942 (1.3942)\tPrec@1 49.219 (49.219)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.4033 (1.3884)\tPrec@1 48.980 (50.490)\n",
      "EPOCH: 44 val Results: Prec@1 50.490 Loss: 1.3884\n",
      "Best Prec@1: 50.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/48]\tTime 0.024 (0.024)\tLoss 1.4118 (1.4118)\tPrec@1 49.121 (49.121)\n",
      "Epoch: [45][9/48]\tTime 0.015 (0.029)\tLoss 1.4409 (1.4132)\tPrec@1 48.145 (48.926)\n",
      "Epoch: [45][18/48]\tTime 0.029 (0.030)\tLoss 1.4519 (1.4088)\tPrec@1 48.145 (49.162)\n",
      "Epoch: [45][27/48]\tTime 0.042 (0.028)\tLoss 1.3984 (1.4097)\tPrec@1 49.414 (49.295)\n",
      "Epoch: [45][36/48]\tTime 0.022 (0.030)\tLoss 1.4570 (1.4108)\tPrec@1 48.145 (49.393)\n",
      "Epoch: [45][45/48]\tTime 0.023 (0.030)\tLoss 1.4674 (1.4128)\tPrec@1 46.387 (49.257)\n",
      "Epoch: [45][48/48]\tTime 0.045 (0.030)\tLoss 1.3814 (1.4116)\tPrec@1 51.061 (49.294)\n",
      "EPOCH: 45 train Results: Prec@1 49.294 Loss: 1.4116\n",
      "Test: [0/9]\tTime 0.020 (0.020)\tLoss 1.3843 (1.3843)\tPrec@1 49.023 (49.023)\n",
      "Test: [9/9]\tTime 0.016 (0.015)\tLoss 1.3985 (1.3787)\tPrec@1 48.342 (50.660)\n",
      "EPOCH: 45 val Results: Prec@1 50.660 Loss: 1.3787\n",
      "Best Prec@1: 50.660\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/48]\tTime 0.028 (0.028)\tLoss 1.4267 (1.4267)\tPrec@1 50.098 (50.098)\n",
      "Epoch: [46][9/48]\tTime 0.053 (0.036)\tLoss 1.4773 (1.4173)\tPrec@1 47.070 (48.975)\n",
      "Epoch: [46][18/48]\tTime 0.031 (0.032)\tLoss 1.4523 (1.4090)\tPrec@1 47.070 (48.987)\n",
      "Epoch: [46][27/48]\tTime 0.041 (0.030)\tLoss 1.3940 (1.4084)\tPrec@1 50.195 (49.090)\n",
      "Epoch: [46][36/48]\tTime 0.043 (0.029)\tLoss 1.3690 (1.4079)\tPrec@1 51.465 (49.221)\n",
      "Epoch: [46][45/48]\tTime 0.014 (0.028)\tLoss 1.3753 (1.4033)\tPrec@1 51.953 (49.418)\n",
      "Epoch: [46][48/48]\tTime 0.027 (0.027)\tLoss 1.4451 (1.4057)\tPrec@1 49.410 (49.318)\n",
      "EPOCH: 46 train Results: Prec@1 49.318 Loss: 1.4057\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.3774 (1.3774)\tPrec@1 50.586 (50.586)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.3913 (1.3724)\tPrec@1 49.362 (51.100)\n",
      "EPOCH: 46 val Results: Prec@1 51.100 Loss: 1.3724\n",
      "Best Prec@1: 51.100\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/48]\tTime 0.025 (0.025)\tLoss 1.4150 (1.4150)\tPrec@1 49.707 (49.707)\n",
      "Epoch: [47][9/48]\tTime 0.026 (0.035)\tLoss 1.3925 (1.3967)\tPrec@1 50.098 (49.990)\n",
      "Epoch: [47][18/48]\tTime 0.018 (0.028)\tLoss 1.3664 (1.4015)\tPrec@1 50.977 (49.789)\n",
      "Epoch: [47][27/48]\tTime 0.031 (0.026)\tLoss 1.3734 (1.3991)\tPrec@1 49.805 (49.920)\n",
      "Epoch: [47][36/48]\tTime 0.020 (0.024)\tLoss 1.4237 (1.3972)\tPrec@1 49.707 (49.884)\n",
      "Epoch: [47][45/48]\tTime 0.016 (0.026)\tLoss 1.3648 (1.3954)\tPrec@1 49.902 (49.983)\n",
      "Epoch: [47][48/48]\tTime 0.016 (0.026)\tLoss 1.3734 (1.3950)\tPrec@1 48.467 (49.980)\n",
      "EPOCH: 47 train Results: Prec@1 49.980 Loss: 1.3950\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.3665 (1.3665)\tPrec@1 50.879 (50.879)\n",
      "Test: [9/9]\tTime 0.007 (0.005)\tLoss 1.3857 (1.3647)\tPrec@1 50.383 (51.530)\n",
      "EPOCH: 47 val Results: Prec@1 51.530 Loss: 1.3647\n",
      "Best Prec@1: 51.530\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/48]\tTime 0.029 (0.029)\tLoss 1.3843 (1.3843)\tPrec@1 49.902 (49.902)\n",
      "Epoch: [48][9/48]\tTime 0.019 (0.022)\tLoss 1.4212 (1.3886)\tPrec@1 49.121 (49.941)\n",
      "Epoch: [48][18/48]\tTime 0.014 (0.020)\tLoss 1.3745 (1.3910)\tPrec@1 51.465 (50.175)\n",
      "Epoch: [48][27/48]\tTime 0.014 (0.020)\tLoss 1.3931 (1.3868)\tPrec@1 49.609 (50.296)\n",
      "Epoch: [48][36/48]\tTime 0.023 (0.020)\tLoss 1.4075 (1.3894)\tPrec@1 47.949 (50.274)\n",
      "Epoch: [48][45/48]\tTime 0.018 (0.020)\tLoss 1.4169 (1.3888)\tPrec@1 48.242 (50.293)\n",
      "Epoch: [48][48/48]\tTime 0.014 (0.020)\tLoss 1.4192 (1.3888)\tPrec@1 48.821 (50.264)\n",
      "EPOCH: 48 train Results: Prec@1 50.264 Loss: 1.3888\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.3562 (1.3562)\tPrec@1 50.781 (50.781)\n",
      "Test: [9/9]\tTime 0.013 (0.005)\tLoss 1.3787 (1.3566)\tPrec@1 50.128 (51.560)\n",
      "EPOCH: 48 val Results: Prec@1 51.560 Loss: 1.3566\n",
      "Best Prec@1: 51.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/48]\tTime 0.019 (0.019)\tLoss 1.4195 (1.4195)\tPrec@1 47.363 (47.363)\n",
      "Epoch: [49][9/48]\tTime 0.021 (0.019)\tLoss 1.3526 (1.3762)\tPrec@1 51.465 (50.508)\n",
      "Epoch: [49][18/48]\tTime 0.019 (0.019)\tLoss 1.3858 (1.3795)\tPrec@1 50.195 (50.899)\n",
      "Epoch: [49][27/48]\tTime 0.014 (0.019)\tLoss 1.4092 (1.3809)\tPrec@1 48.633 (50.764)\n",
      "Epoch: [49][36/48]\tTime 0.029 (0.019)\tLoss 1.3383 (1.3810)\tPrec@1 50.781 (50.713)\n",
      "Epoch: [49][45/48]\tTime 0.031 (0.022)\tLoss 1.3919 (1.3815)\tPrec@1 50.977 (50.590)\n",
      "Epoch: [49][48/48]\tTime 0.015 (0.022)\tLoss 1.3791 (1.3812)\tPrec@1 50.354 (50.602)\n",
      "EPOCH: 49 train Results: Prec@1 50.602 Loss: 1.3812\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.3514 (1.3514)\tPrec@1 51.855 (51.855)\n",
      "Test: [9/9]\tTime 0.002 (0.003)\tLoss 1.3730 (1.3503)\tPrec@1 50.000 (51.970)\n",
      "EPOCH: 49 val Results: Prec@1 51.970 Loss: 1.3503\n",
      "Best Prec@1: 51.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/48]\tTime 0.021 (0.021)\tLoss 1.3879 (1.3879)\tPrec@1 50.391 (50.391)\n",
      "Epoch: [50][9/48]\tTime 0.024 (0.021)\tLoss 1.3902 (1.3660)\tPrec@1 49.414 (51.279)\n",
      "Epoch: [50][18/48]\tTime 0.021 (0.020)\tLoss 1.3634 (1.3693)\tPrec@1 51.074 (50.920)\n",
      "Epoch: [50][27/48]\tTime 0.021 (0.022)\tLoss 1.3265 (1.3685)\tPrec@1 51.172 (50.628)\n",
      "Epoch: [50][36/48]\tTime 0.022 (0.022)\tLoss 1.3773 (1.3681)\tPrec@1 52.539 (50.702)\n",
      "Epoch: [50][45/48]\tTime 0.018 (0.022)\tLoss 1.3707 (1.3701)\tPrec@1 50.586 (50.677)\n",
      "Epoch: [50][48/48]\tTime 0.023 (0.022)\tLoss 1.4253 (1.3717)\tPrec@1 50.118 (50.596)\n",
      "EPOCH: 50 train Results: Prec@1 50.596 Loss: 1.3717\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.3437 (1.3437)\tPrec@1 51.953 (51.953)\n",
      "Test: [9/9]\tTime 0.005 (0.009)\tLoss 1.3662 (1.3427)\tPrec@1 50.765 (52.400)\n",
      "EPOCH: 50 val Results: Prec@1 52.400 Loss: 1.3427\n",
      "Best Prec@1: 52.400\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/48]\tTime 0.031 (0.031)\tLoss 1.3680 (1.3680)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [51][9/48]\tTime 0.028 (0.025)\tLoss 1.3584 (1.3535)\tPrec@1 50.000 (51.191)\n",
      "Epoch: [51][18/48]\tTime 0.017 (0.025)\tLoss 1.3652 (1.3580)\tPrec@1 51.270 (51.244)\n",
      "Epoch: [51][27/48]\tTime 0.025 (0.024)\tLoss 1.4006 (1.3579)\tPrec@1 49.805 (51.259)\n",
      "Epoch: [51][36/48]\tTime 0.020 (0.025)\tLoss 1.3717 (1.3612)\tPrec@1 51.074 (51.101)\n",
      "Epoch: [51][45/48]\tTime 0.026 (0.024)\tLoss 1.3873 (1.3644)\tPrec@1 50.879 (51.098)\n",
      "Epoch: [51][48/48]\tTime 0.012 (0.024)\tLoss 1.3808 (1.3644)\tPrec@1 50.825 (51.108)\n",
      "EPOCH: 51 train Results: Prec@1 51.108 Loss: 1.3644\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.3312 (1.3312)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.002 (0.006)\tLoss 1.3605 (1.3352)\tPrec@1 50.255 (52.730)\n",
      "EPOCH: 51 val Results: Prec@1 52.730 Loss: 1.3352\n",
      "Best Prec@1: 52.730\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/48]\tTime 0.019 (0.019)\tLoss 1.3539 (1.3539)\tPrec@1 52.148 (52.148)\n",
      "Epoch: [52][9/48]\tTime 0.037 (0.021)\tLoss 1.3307 (1.3425)\tPrec@1 52.734 (52.168)\n",
      "Epoch: [52][18/48]\tTime 0.018 (0.022)\tLoss 1.3332 (1.3448)\tPrec@1 53.125 (51.984)\n",
      "Epoch: [52][27/48]\tTime 0.020 (0.022)\tLoss 1.3449 (1.3460)\tPrec@1 49.902 (51.719)\n",
      "Epoch: [52][36/48]\tTime 0.050 (0.024)\tLoss 1.3087 (1.3445)\tPrec@1 52.832 (51.636)\n",
      "Epoch: [52][45/48]\tTime 0.017 (0.024)\tLoss 1.3706 (1.3521)\tPrec@1 52.734 (51.486)\n",
      "Epoch: [52][48/48]\tTime 0.024 (0.024)\tLoss 1.3882 (1.3529)\tPrec@1 50.472 (51.452)\n",
      "EPOCH: 52 train Results: Prec@1 51.452 Loss: 1.3529\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.3252 (1.3252)\tPrec@1 52.441 (52.441)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.3514 (1.3276)\tPrec@1 51.020 (53.040)\n",
      "EPOCH: 52 val Results: Prec@1 53.040 Loss: 1.3276\n",
      "Best Prec@1: 53.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/48]\tTime 0.016 (0.016)\tLoss 1.3224 (1.3224)\tPrec@1 52.832 (52.832)\n",
      "Epoch: [53][9/48]\tTime 0.030 (0.023)\tLoss 1.3535 (1.3345)\tPrec@1 51.465 (52.129)\n",
      "Epoch: [53][18/48]\tTime 0.016 (0.024)\tLoss 1.3128 (1.3381)\tPrec@1 52.148 (51.537)\n",
      "Epoch: [53][27/48]\tTime 0.023 (0.023)\tLoss 1.4241 (1.3407)\tPrec@1 48.926 (51.538)\n",
      "Epoch: [53][36/48]\tTime 0.014 (0.025)\tLoss 1.3350 (1.3400)\tPrec@1 52.246 (51.687)\n",
      "Epoch: [53][45/48]\tTime 0.021 (0.023)\tLoss 1.3231 (1.3414)\tPrec@1 51.855 (51.792)\n",
      "Epoch: [53][48/48]\tTime 0.017 (0.023)\tLoss 1.3465 (1.3420)\tPrec@1 50.354 (51.794)\n",
      "EPOCH: 53 train Results: Prec@1 51.794 Loss: 1.3420\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.3200 (1.3200)\tPrec@1 52.246 (52.246)\n",
      "Test: [9/9]\tTime 0.007 (0.008)\tLoss 1.3496 (1.3234)\tPrec@1 49.745 (52.910)\n",
      "EPOCH: 53 val Results: Prec@1 52.910 Loss: 1.3234\n",
      "Best Prec@1: 53.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/48]\tTime 0.027 (0.027)\tLoss 1.3240 (1.3240)\tPrec@1 50.977 (50.977)\n",
      "Epoch: [54][9/48]\tTime 0.024 (0.031)\tLoss 1.3199 (1.3367)\tPrec@1 54.785 (52.021)\n",
      "Epoch: [54][18/48]\tTime 0.026 (0.029)\tLoss 1.3637 (1.3385)\tPrec@1 50.195 (52.164)\n",
      "Epoch: [54][27/48]\tTime 0.022 (0.031)\tLoss 1.3080 (1.3367)\tPrec@1 51.855 (52.194)\n",
      "Epoch: [54][36/48]\tTime 0.029 (0.030)\tLoss 1.3239 (1.3351)\tPrec@1 53.516 (52.251)\n",
      "Epoch: [54][45/48]\tTime 0.022 (0.028)\tLoss 1.3543 (1.3409)\tPrec@1 50.684 (52.010)\n",
      "Epoch: [54][48/48]\tTime 0.028 (0.028)\tLoss 1.3482 (1.3400)\tPrec@1 52.358 (52.080)\n",
      "EPOCH: 54 train Results: Prec@1 52.080 Loss: 1.3400\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.3124 (1.3124)\tPrec@1 53.906 (53.906)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.3393 (1.3165)\tPrec@1 50.128 (53.510)\n",
      "EPOCH: 54 val Results: Prec@1 53.510 Loss: 1.3165\n",
      "Best Prec@1: 53.510\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/48]\tTime 0.024 (0.024)\tLoss 1.3311 (1.3311)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [55][9/48]\tTime 0.031 (0.024)\tLoss 1.3566 (1.3297)\tPrec@1 51.074 (52.178)\n",
      "Epoch: [55][18/48]\tTime 0.031 (0.025)\tLoss 1.3495 (1.3291)\tPrec@1 50.684 (52.534)\n",
      "Epoch: [55][27/48]\tTime 0.015 (0.024)\tLoss 1.3602 (1.3306)\tPrec@1 50.488 (52.424)\n",
      "Epoch: [55][36/48]\tTime 0.018 (0.025)\tLoss 1.3032 (1.3296)\tPrec@1 54.590 (52.499)\n",
      "Epoch: [55][45/48]\tTime 0.025 (0.025)\tLoss 1.3156 (1.3296)\tPrec@1 54.785 (52.516)\n",
      "Epoch: [55][48/48]\tTime 0.021 (0.025)\tLoss 1.3351 (1.3293)\tPrec@1 51.179 (52.470)\n",
      "EPOCH: 55 train Results: Prec@1 52.470 Loss: 1.3293\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.3071 (1.3071)\tPrec@1 54.297 (54.297)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.3364 (1.3109)\tPrec@1 51.020 (53.600)\n",
      "EPOCH: 55 val Results: Prec@1 53.600 Loss: 1.3109\n",
      "Best Prec@1: 53.600\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/48]\tTime 0.022 (0.022)\tLoss 1.3276 (1.3276)\tPrec@1 54.004 (54.004)\n",
      "Epoch: [56][9/48]\tTime 0.014 (0.021)\tLoss 1.3213 (1.3437)\tPrec@1 53.027 (51.133)\n",
      "Epoch: [56][18/48]\tTime 0.020 (0.020)\tLoss 1.3090 (1.3261)\tPrec@1 54.297 (52.118)\n",
      "Epoch: [56][27/48]\tTime 0.024 (0.022)\tLoss 1.2766 (1.3169)\tPrec@1 54.785 (52.689)\n",
      "Epoch: [56][36/48]\tTime 0.039 (0.024)\tLoss 1.3417 (1.3215)\tPrec@1 52.148 (52.729)\n",
      "Epoch: [56][45/48]\tTime 0.020 (0.024)\tLoss 1.3401 (1.3224)\tPrec@1 50.977 (52.681)\n",
      "Epoch: [56][48/48]\tTime 0.020 (0.024)\tLoss 1.3213 (1.3225)\tPrec@1 50.825 (52.658)\n",
      "EPOCH: 56 train Results: Prec@1 52.658 Loss: 1.3225\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.2972 (1.2972)\tPrec@1 54.883 (54.883)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.3280 (1.3037)\tPrec@1 50.765 (53.780)\n",
      "EPOCH: 56 val Results: Prec@1 53.780 Loss: 1.3037\n",
      "Best Prec@1: 53.780\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/48]\tTime 0.019 (0.019)\tLoss 1.3690 (1.3690)\tPrec@1 50.098 (50.098)\n",
      "Epoch: [57][9/48]\tTime 0.027 (0.024)\tLoss 1.2644 (1.3044)\tPrec@1 55.859 (52.617)\n",
      "Epoch: [57][18/48]\tTime 0.032 (0.024)\tLoss 1.3412 (1.3127)\tPrec@1 52.246 (52.570)\n",
      "Epoch: [57][27/48]\tTime 0.049 (0.028)\tLoss 1.2790 (1.3126)\tPrec@1 54.785 (52.672)\n",
      "Epoch: [57][36/48]\tTime 0.024 (0.027)\tLoss 1.3430 (1.3104)\tPrec@1 50.391 (52.843)\n",
      "Epoch: [57][45/48]\tTime 0.026 (0.026)\tLoss 1.3594 (1.3183)\tPrec@1 50.684 (52.494)\n",
      "Epoch: [57][48/48]\tTime 0.020 (0.027)\tLoss 1.3061 (1.3190)\tPrec@1 54.835 (52.542)\n",
      "EPOCH: 57 train Results: Prec@1 52.542 Loss: 1.3190\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2911 (1.2911)\tPrec@1 54.492 (54.492)\n",
      "Test: [9/9]\tTime 0.005 (0.014)\tLoss 1.3190 (1.2990)\tPrec@1 51.403 (53.730)\n",
      "EPOCH: 57 val Results: Prec@1 53.730 Loss: 1.2990\n",
      "Best Prec@1: 53.780\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/48]\tTime 0.044 (0.044)\tLoss 1.3381 (1.3381)\tPrec@1 50.977 (50.977)\n",
      "Epoch: [58][9/48]\tTime 0.076 (0.035)\tLoss 1.3386 (1.3064)\tPrec@1 53.320 (53.154)\n",
      "Epoch: [58][18/48]\tTime 0.019 (0.031)\tLoss 1.2988 (1.3045)\tPrec@1 53.320 (53.444)\n",
      "Epoch: [58][27/48]\tTime 0.061 (0.031)\tLoss 1.3001 (1.3076)\tPrec@1 52.734 (53.282)\n",
      "Epoch: [58][36/48]\tTime 0.043 (0.030)\tLoss 1.2638 (1.3047)\tPrec@1 54.297 (53.376)\n",
      "Epoch: [58][45/48]\tTime 0.054 (0.029)\tLoss 1.3525 (1.3058)\tPrec@1 51.855 (53.233)\n",
      "Epoch: [58][48/48]\tTime 0.023 (0.029)\tLoss 1.2892 (1.3054)\tPrec@1 53.774 (53.194)\n",
      "EPOCH: 58 train Results: Prec@1 53.194 Loss: 1.3054\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.2833 (1.2833)\tPrec@1 55.078 (55.078)\n",
      "Test: [9/9]\tTime 0.003 (0.008)\tLoss 1.3191 (1.2916)\tPrec@1 50.893 (53.900)\n",
      "EPOCH: 58 val Results: Prec@1 53.900 Loss: 1.2916\n",
      "Best Prec@1: 53.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/48]\tTime 0.033 (0.033)\tLoss 1.3497 (1.3497)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [59][9/48]\tTime 0.076 (0.038)\tLoss 1.3232 (1.2845)\tPrec@1 52.441 (54.014)\n",
      "Epoch: [59][18/48]\tTime 0.023 (0.035)\tLoss 1.3747 (1.2950)\tPrec@1 50.586 (53.510)\n",
      "Epoch: [59][27/48]\tTime 0.034 (0.033)\tLoss 1.2416 (1.2920)\tPrec@1 56.250 (53.903)\n",
      "Epoch: [59][36/48]\tTime 0.037 (0.031)\tLoss 1.2537 (1.2961)\tPrec@1 55.176 (53.560)\n",
      "Epoch: [59][45/48]\tTime 0.019 (0.029)\tLoss 1.3034 (1.2993)\tPrec@1 52.832 (53.382)\n",
      "Epoch: [59][48/48]\tTime 0.016 (0.028)\tLoss 1.3187 (1.3002)\tPrec@1 52.123 (53.368)\n",
      "EPOCH: 59 train Results: Prec@1 53.368 Loss: 1.3002\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2784 (1.2784)\tPrec@1 54.590 (54.590)\n",
      "Test: [9/9]\tTime 0.004 (0.007)\tLoss 1.3085 (1.2851)\tPrec@1 51.913 (54.320)\n",
      "EPOCH: 59 val Results: Prec@1 54.320 Loss: 1.2851\n",
      "Best Prec@1: 54.320\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/48]\tTime 0.038 (0.038)\tLoss 1.2808 (1.2808)\tPrec@1 55.762 (55.762)\n",
      "Epoch: [60][9/48]\tTime 0.016 (0.024)\tLoss 1.2969 (1.2981)\tPrec@1 53.809 (53.867)\n",
      "Epoch: [60][18/48]\tTime 0.020 (0.021)\tLoss 1.3280 (1.3007)\tPrec@1 55.078 (53.881)\n",
      "Epoch: [60][27/48]\tTime 0.029 (0.022)\tLoss 1.2784 (1.2993)\tPrec@1 54.688 (53.938)\n",
      "Epoch: [60][36/48]\tTime 0.022 (0.021)\tLoss 1.2947 (1.3020)\tPrec@1 54.395 (53.645)\n",
      "Epoch: [60][45/48]\tTime 0.022 (0.022)\tLoss 1.2732 (1.3008)\tPrec@1 54.688 (53.550)\n",
      "Epoch: [60][48/48]\tTime 0.017 (0.022)\tLoss 1.3096 (1.2998)\tPrec@1 52.476 (53.510)\n",
      "EPOCH: 60 train Results: Prec@1 53.510 Loss: 1.2998\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2731 (1.2731)\tPrec@1 55.664 (55.664)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.3076 (1.2816)\tPrec@1 51.913 (54.320)\n",
      "EPOCH: 60 val Results: Prec@1 54.320 Loss: 1.2816\n",
      "Best Prec@1: 54.320\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/48]\tTime 0.027 (0.027)\tLoss 1.2762 (1.2762)\tPrec@1 53.320 (53.320)\n",
      "Epoch: [61][9/48]\tTime 0.019 (0.022)\tLoss 1.2818 (1.2726)\tPrec@1 53.906 (54.404)\n",
      "Epoch: [61][18/48]\tTime 0.025 (0.022)\tLoss 1.3309 (1.2805)\tPrec@1 53.125 (54.071)\n",
      "Epoch: [61][27/48]\tTime 0.017 (0.022)\tLoss 1.2632 (1.2871)\tPrec@1 57.520 (53.903)\n",
      "Epoch: [61][36/48]\tTime 0.018 (0.021)\tLoss 1.2649 (1.2895)\tPrec@1 56.836 (53.835)\n",
      "Epoch: [61][45/48]\tTime 0.024 (0.021)\tLoss 1.3294 (1.2920)\tPrec@1 51.953 (53.679)\n",
      "Epoch: [61][48/48]\tTime 0.011 (0.021)\tLoss 1.3054 (1.2928)\tPrec@1 52.594 (53.652)\n",
      "EPOCH: 61 train Results: Prec@1 53.652 Loss: 1.2928\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2661 (1.2661)\tPrec@1 55.762 (55.762)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2991 (1.2759)\tPrec@1 51.531 (54.480)\n",
      "EPOCH: 61 val Results: Prec@1 54.480 Loss: 1.2759\n",
      "Best Prec@1: 54.480\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/48]\tTime 0.017 (0.017)\tLoss 1.2993 (1.2993)\tPrec@1 52.832 (52.832)\n",
      "Epoch: [62][9/48]\tTime 0.016 (0.025)\tLoss 1.2939 (1.2866)\tPrec@1 52.344 (53.633)\n",
      "Epoch: [62][18/48]\tTime 0.021 (0.022)\tLoss 1.2855 (1.2817)\tPrec@1 54.980 (53.988)\n",
      "Epoch: [62][27/48]\tTime 0.020 (0.023)\tLoss 1.2497 (1.2775)\tPrec@1 53.027 (54.189)\n",
      "Epoch: [62][36/48]\tTime 0.025 (0.022)\tLoss 1.2470 (1.2772)\tPrec@1 55.078 (54.173)\n",
      "Epoch: [62][45/48]\tTime 0.019 (0.022)\tLoss 1.3116 (1.2816)\tPrec@1 54.199 (54.108)\n",
      "Epoch: [62][48/48]\tTime 0.017 (0.021)\tLoss 1.3059 (1.2815)\tPrec@1 52.594 (54.064)\n",
      "EPOCH: 62 train Results: Prec@1 54.064 Loss: 1.2815\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2590 (1.2590)\tPrec@1 56.738 (56.738)\n",
      "Test: [9/9]\tTime 0.002 (0.004)\tLoss 1.2915 (1.2712)\tPrec@1 52.168 (55.000)\n",
      "EPOCH: 62 val Results: Prec@1 55.000 Loss: 1.2712\n",
      "Best Prec@1: 55.000\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/48]\tTime 0.016 (0.016)\tLoss 1.2783 (1.2783)\tPrec@1 54.590 (54.590)\n",
      "Epoch: [63][9/48]\tTime 0.015 (0.023)\tLoss 1.2553 (1.2581)\tPrec@1 54.980 (55.342)\n",
      "Epoch: [63][18/48]\tTime 0.019 (0.022)\tLoss 1.3551 (1.2702)\tPrec@1 51.074 (54.636)\n",
      "Epoch: [63][27/48]\tTime 0.038 (0.022)\tLoss 1.3043 (1.2703)\tPrec@1 52.344 (54.583)\n",
      "Epoch: [63][36/48]\tTime 0.033 (0.024)\tLoss 1.3008 (1.2773)\tPrec@1 55.859 (54.416)\n",
      "Epoch: [63][45/48]\tTime 0.017 (0.023)\tLoss 1.2681 (1.2778)\tPrec@1 55.566 (54.409)\n",
      "Epoch: [63][48/48]\tTime 0.017 (0.023)\tLoss 1.2705 (1.2773)\tPrec@1 54.245 (54.368)\n",
      "EPOCH: 63 train Results: Prec@1 54.368 Loss: 1.2773\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.2504 (1.2504)\tPrec@1 56.934 (56.934)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2876 (1.2653)\tPrec@1 51.913 (55.110)\n",
      "EPOCH: 63 val Results: Prec@1 55.110 Loss: 1.2653\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/48]\tTime 0.016 (0.016)\tLoss 1.2413 (1.2413)\tPrec@1 55.762 (55.762)\n",
      "Epoch: [64][9/48]\tTime 0.020 (0.026)\tLoss 1.2866 (1.2553)\tPrec@1 53.711 (55.371)\n",
      "Epoch: [64][18/48]\tTime 0.051 (0.028)\tLoss 1.2651 (1.2636)\tPrec@1 54.688 (54.857)\n",
      "Epoch: [64][27/48]\tTime 0.020 (0.026)\tLoss 1.1792 (1.2631)\tPrec@1 58.398 (54.918)\n",
      "Epoch: [64][36/48]\tTime 0.019 (0.025)\tLoss 1.3332 (1.2669)\tPrec@1 52.441 (54.682)\n",
      "Epoch: [64][45/48]\tTime 0.025 (0.024)\tLoss 1.2327 (1.2691)\tPrec@1 57.324 (54.564)\n",
      "Epoch: [64][48/48]\tTime 0.031 (0.027)\tLoss 1.2925 (1.2719)\tPrec@1 53.420 (54.454)\n",
      "EPOCH: 64 train Results: Prec@1 54.454 Loss: 1.2719\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.2468 (1.2468)\tPrec@1 56.543 (56.543)\n",
      "Test: [9/9]\tTime 0.009 (0.008)\tLoss 1.2843 (1.2629)\tPrec@1 53.061 (55.630)\n",
      "EPOCH: 64 val Results: Prec@1 55.630 Loss: 1.2629\n",
      "Best Prec@1: 55.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/48]\tTime 0.019 (0.019)\tLoss 1.2570 (1.2570)\tPrec@1 54.004 (54.004)\n",
      "Epoch: [65][9/48]\tTime 0.023 (0.022)\tLoss 1.2626 (1.2518)\tPrec@1 54.395 (55.107)\n",
      "Epoch: [65][18/48]\tTime 0.024 (0.025)\tLoss 1.2969 (1.2624)\tPrec@1 54.590 (55.166)\n",
      "Epoch: [65][27/48]\tTime 0.027 (0.024)\tLoss 1.2776 (1.2657)\tPrec@1 52.734 (54.827)\n",
      "Epoch: [65][36/48]\tTime 0.025 (0.024)\tLoss 1.2700 (1.2708)\tPrec@1 56.738 (54.651)\n",
      "Epoch: [65][45/48]\tTime 0.074 (0.024)\tLoss 1.2989 (1.2706)\tPrec@1 52.344 (54.630)\n",
      "Epoch: [65][48/48]\tTime 0.019 (0.024)\tLoss 1.3356 (1.2727)\tPrec@1 53.892 (54.620)\n",
      "EPOCH: 65 train Results: Prec@1 54.620 Loss: 1.2727\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2389 (1.2389)\tPrec@1 56.445 (56.445)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2857 (1.2580)\tPrec@1 52.679 (55.630)\n",
      "EPOCH: 65 val Results: Prec@1 55.630 Loss: 1.2580\n",
      "Best Prec@1: 55.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/48]\tTime 0.026 (0.026)\tLoss 1.2281 (1.2281)\tPrec@1 54.492 (54.492)\n",
      "Epoch: [66][9/48]\tTime 0.032 (0.029)\tLoss 1.2544 (1.2575)\tPrec@1 54.395 (54.414)\n",
      "Epoch: [66][18/48]\tTime 0.022 (0.025)\tLoss 1.2746 (1.2520)\tPrec@1 53.223 (54.862)\n",
      "Epoch: [66][27/48]\tTime 0.022 (0.025)\tLoss 1.2707 (1.2568)\tPrec@1 55.371 (54.855)\n",
      "Epoch: [66][36/48]\tTime 0.021 (0.024)\tLoss 1.2801 (1.2622)\tPrec@1 52.246 (54.643)\n",
      "Epoch: [66][45/48]\tTime 0.023 (0.024)\tLoss 1.2523 (1.2647)\tPrec@1 53.320 (54.560)\n",
      "Epoch: [66][48/48]\tTime 0.012 (0.024)\tLoss 1.3009 (1.2659)\tPrec@1 52.358 (54.480)\n",
      "EPOCH: 66 train Results: Prec@1 54.480 Loss: 1.2659\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2389 (1.2389)\tPrec@1 56.738 (56.738)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.2810 (1.2549)\tPrec@1 53.061 (55.490)\n",
      "EPOCH: 66 val Results: Prec@1 55.490 Loss: 1.2549\n",
      "Best Prec@1: 55.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/48]\tTime 0.023 (0.023)\tLoss 1.1758 (1.1758)\tPrec@1 56.836 (56.836)\n",
      "Epoch: [67][9/48]\tTime 0.020 (0.024)\tLoss 1.2903 (1.2423)\tPrec@1 54.102 (55.068)\n",
      "Epoch: [67][18/48]\tTime 0.020 (0.023)\tLoss 1.2596 (1.2434)\tPrec@1 54.688 (55.176)\n",
      "Epoch: [67][27/48]\tTime 0.029 (0.023)\tLoss 1.2597 (1.2504)\tPrec@1 53.906 (55.068)\n",
      "Epoch: [67][36/48]\tTime 0.016 (0.022)\tLoss 1.2305 (1.2540)\tPrec@1 55.664 (54.975)\n",
      "Epoch: [67][45/48]\tTime 0.014 (0.022)\tLoss 1.2691 (1.2545)\tPrec@1 54.395 (55.053)\n",
      "Epoch: [67][48/48]\tTime 0.014 (0.022)\tLoss 1.2206 (1.2554)\tPrec@1 54.009 (54.976)\n",
      "EPOCH: 67 train Results: Prec@1 54.976 Loss: 1.2554\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.2336 (1.2336)\tPrec@1 57.031 (57.031)\n",
      "Test: [9/9]\tTime 0.004 (0.009)\tLoss 1.2788 (1.2511)\tPrec@1 51.531 (55.650)\n",
      "EPOCH: 67 val Results: Prec@1 55.650 Loss: 1.2511\n",
      "Best Prec@1: 55.650\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/48]\tTime 0.032 (0.032)\tLoss 1.1721 (1.1721)\tPrec@1 58.301 (58.301)\n",
      "Epoch: [68][9/48]\tTime 0.026 (0.024)\tLoss 1.1771 (1.2237)\tPrec@1 58.789 (56.621)\n",
      "Epoch: [68][18/48]\tTime 0.027 (0.023)\tLoss 1.3005 (1.2401)\tPrec@1 52.441 (55.659)\n",
      "Epoch: [68][27/48]\tTime 0.018 (0.023)\tLoss 1.2415 (1.2396)\tPrec@1 54.688 (55.755)\n",
      "Epoch: [68][36/48]\tTime 0.017 (0.022)\tLoss 1.3087 (1.2436)\tPrec@1 53.320 (55.598)\n",
      "Epoch: [68][45/48]\tTime 0.016 (0.023)\tLoss 1.2351 (1.2460)\tPrec@1 54.980 (55.507)\n",
      "Epoch: [68][48/48]\tTime 0.027 (0.023)\tLoss 1.2855 (1.2460)\tPrec@1 54.363 (55.468)\n",
      "EPOCH: 68 train Results: Prec@1 55.468 Loss: 1.2460\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2256 (1.2256)\tPrec@1 57.910 (57.910)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2697 (1.2475)\tPrec@1 52.806 (55.900)\n",
      "EPOCH: 68 val Results: Prec@1 55.900 Loss: 1.2475\n",
      "Best Prec@1: 55.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/48]\tTime 0.023 (0.023)\tLoss 1.1982 (1.1982)\tPrec@1 57.324 (57.324)\n",
      "Epoch: [69][9/48]\tTime 0.018 (0.023)\tLoss 1.2780 (1.2199)\tPrec@1 55.176 (56.729)\n",
      "Epoch: [69][18/48]\tTime 0.018 (0.021)\tLoss 1.2737 (1.2301)\tPrec@1 54.395 (56.240)\n",
      "Epoch: [69][27/48]\tTime 0.018 (0.025)\tLoss 1.2814 (1.2368)\tPrec@1 54.980 (55.831)\n",
      "Epoch: [69][36/48]\tTime 0.027 (0.024)\tLoss 1.2434 (1.2399)\tPrec@1 54.883 (55.656)\n",
      "Epoch: [69][45/48]\tTime 0.017 (0.023)\tLoss 1.2406 (1.2413)\tPrec@1 54.785 (55.560)\n",
      "Epoch: [69][48/48]\tTime 0.012 (0.023)\tLoss 1.2275 (1.2409)\tPrec@1 55.071 (55.640)\n",
      "EPOCH: 69 train Results: Prec@1 55.640 Loss: 1.2409\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2256 (1.2256)\tPrec@1 57.617 (57.617)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.2690 (1.2454)\tPrec@1 52.934 (56.010)\n",
      "EPOCH: 69 val Results: Prec@1 56.010 Loss: 1.2454\n",
      "Best Prec@1: 56.010\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/48]\tTime 0.015 (0.015)\tLoss 1.2186 (1.2186)\tPrec@1 57.520 (57.520)\n",
      "Epoch: [70][9/48]\tTime 0.017 (0.021)\tLoss 1.2615 (1.2297)\tPrec@1 55.664 (56.338)\n",
      "Epoch: [70][18/48]\tTime 0.052 (0.023)\tLoss 1.2421 (1.2320)\tPrec@1 56.543 (56.147)\n",
      "Epoch: [70][27/48]\tTime 0.060 (0.025)\tLoss 1.2919 (1.2361)\tPrec@1 52.832 (55.999)\n",
      "Epoch: [70][36/48]\tTime 0.027 (0.025)\tLoss 1.2433 (1.2402)\tPrec@1 54.102 (55.888)\n",
      "Epoch: [70][45/48]\tTime 0.023 (0.025)\tLoss 1.2353 (1.2408)\tPrec@1 55.664 (55.802)\n",
      "Epoch: [70][48/48]\tTime 0.032 (0.026)\tLoss 1.2274 (1.2413)\tPrec@1 55.307 (55.768)\n",
      "EPOCH: 70 train Results: Prec@1 55.768 Loss: 1.2413\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.2246 (1.2246)\tPrec@1 57.520 (57.520)\n",
      "Test: [9/9]\tTime 0.003 (0.008)\tLoss 1.2699 (1.2429)\tPrec@1 52.296 (55.910)\n",
      "EPOCH: 70 val Results: Prec@1 55.910 Loss: 1.2429\n",
      "Best Prec@1: 56.010\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/48]\tTime 0.023 (0.023)\tLoss 1.2489 (1.2489)\tPrec@1 52.539 (52.539)\n",
      "Epoch: [71][9/48]\tTime 0.019 (0.023)\tLoss 1.1985 (1.2294)\tPrec@1 57.324 (55.791)\n",
      "Epoch: [71][18/48]\tTime 0.015 (0.020)\tLoss 1.2059 (1.2238)\tPrec@1 54.883 (56.173)\n",
      "Epoch: [71][27/48]\tTime 0.016 (0.023)\tLoss 1.2199 (1.2292)\tPrec@1 57.324 (55.922)\n",
      "Epoch: [71][36/48]\tTime 0.012 (0.022)\tLoss 1.1981 (1.2306)\tPrec@1 55.469 (55.873)\n",
      "Epoch: [71][45/48]\tTime 0.022 (0.022)\tLoss 1.2523 (1.2337)\tPrec@1 57.324 (55.906)\n",
      "Epoch: [71][48/48]\tTime 0.015 (0.022)\tLoss 1.3090 (1.2361)\tPrec@1 53.302 (55.814)\n",
      "EPOCH: 71 train Results: Prec@1 55.814 Loss: 1.2361\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2197 (1.2197)\tPrec@1 57.910 (57.910)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.2655 (1.2385)\tPrec@1 53.189 (55.970)\n",
      "EPOCH: 71 val Results: Prec@1 55.970 Loss: 1.2385\n",
      "Best Prec@1: 56.010\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/48]\tTime 0.028 (0.028)\tLoss 1.2062 (1.2062)\tPrec@1 55.859 (55.859)\n",
      "Epoch: [72][9/48]\tTime 0.028 (0.026)\tLoss 1.2544 (1.2213)\tPrec@1 53.613 (56.025)\n",
      "Epoch: [72][18/48]\tTime 0.017 (0.025)\tLoss 1.1467 (1.2183)\tPrec@1 60.742 (56.157)\n",
      "Epoch: [72][27/48]\tTime 0.017 (0.023)\tLoss 1.2257 (1.2236)\tPrec@1 56.445 (56.041)\n",
      "Epoch: [72][36/48]\tTime 0.021 (0.022)\tLoss 1.2465 (1.2315)\tPrec@1 55.762 (55.759)\n",
      "Epoch: [72][45/48]\tTime 0.029 (0.022)\tLoss 1.1775 (1.2322)\tPrec@1 57.617 (55.641)\n",
      "Epoch: [72][48/48]\tTime 0.020 (0.022)\tLoss 1.2416 (1.2331)\tPrec@1 56.604 (55.662)\n",
      "EPOCH: 72 train Results: Prec@1 55.662 Loss: 1.2331\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2133 (1.2133)\tPrec@1 57.520 (57.520)\n",
      "Test: [9/9]\tTime 0.007 (0.005)\tLoss 1.2575 (1.2330)\tPrec@1 54.592 (56.430)\n",
      "EPOCH: 72 val Results: Prec@1 56.430 Loss: 1.2330\n",
      "Best Prec@1: 56.430\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/48]\tTime 0.017 (0.017)\tLoss 1.2509 (1.2509)\tPrec@1 55.273 (55.273)\n",
      "Epoch: [73][9/48]\tTime 0.022 (0.023)\tLoss 1.1959 (1.2280)\tPrec@1 55.664 (56.191)\n",
      "Epoch: [73][18/48]\tTime 0.021 (0.023)\tLoss 1.2541 (1.2202)\tPrec@1 55.078 (56.373)\n",
      "Epoch: [73][27/48]\tTime 0.020 (0.022)\tLoss 1.2057 (1.2205)\tPrec@1 57.324 (56.152)\n",
      "Epoch: [73][36/48]\tTime 0.016 (0.022)\tLoss 1.2648 (1.2255)\tPrec@1 53.809 (55.917)\n",
      "Epoch: [73][45/48]\tTime 0.017 (0.022)\tLoss 1.2903 (1.2265)\tPrec@1 54.297 (56.074)\n",
      "Epoch: [73][48/48]\tTime 0.016 (0.022)\tLoss 1.2730 (1.2273)\tPrec@1 54.717 (56.034)\n",
      "EPOCH: 73 train Results: Prec@1 56.034 Loss: 1.2273\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2120 (1.2120)\tPrec@1 56.934 (56.934)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.2581 (1.2317)\tPrec@1 52.679 (56.300)\n",
      "EPOCH: 73 val Results: Prec@1 56.300 Loss: 1.2317\n",
      "Best Prec@1: 56.430\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/48]\tTime 0.027 (0.027)\tLoss 1.2503 (1.2503)\tPrec@1 54.004 (54.004)\n",
      "Epoch: [74][9/48]\tTime 0.019 (0.023)\tLoss 1.1635 (1.2274)\tPrec@1 59.668 (55.820)\n",
      "Epoch: [74][18/48]\tTime 0.017 (0.022)\tLoss 1.2393 (1.2272)\tPrec@1 56.445 (56.029)\n",
      "Epoch: [74][27/48]\tTime 0.017 (0.022)\tLoss 1.3050 (1.2284)\tPrec@1 52.539 (56.219)\n",
      "Epoch: [74][36/48]\tTime 0.026 (0.021)\tLoss 1.2943 (1.2263)\tPrec@1 54.004 (56.266)\n",
      "Epoch: [74][45/48]\tTime 0.022 (0.022)\tLoss 1.2319 (1.2265)\tPrec@1 56.348 (56.225)\n",
      "Epoch: [74][48/48]\tTime 0.020 (0.021)\tLoss 1.2569 (1.2282)\tPrec@1 54.717 (56.190)\n",
      "EPOCH: 74 train Results: Prec@1 56.190 Loss: 1.2282\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.2008 (1.2008)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.2492 (1.2292)\tPrec@1 53.316 (56.540)\n",
      "EPOCH: 74 val Results: Prec@1 56.540 Loss: 1.2292\n",
      "Best Prec@1: 56.540\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/48]\tTime 0.024 (0.024)\tLoss 1.1368 (1.1368)\tPrec@1 59.082 (59.082)\n",
      "Epoch: [75][9/48]\tTime 0.019 (0.024)\tLoss 1.2334 (1.1925)\tPrec@1 55.957 (57.500)\n",
      "Epoch: [75][18/48]\tTime 0.021 (0.022)\tLoss 1.1885 (1.2128)\tPrec@1 58.203 (56.749)\n",
      "Epoch: [75][27/48]\tTime 0.020 (0.022)\tLoss 1.2596 (1.2140)\tPrec@1 54.395 (56.550)\n",
      "Epoch: [75][36/48]\tTime 0.013 (0.022)\tLoss 1.1905 (1.2158)\tPrec@1 57.520 (56.390)\n",
      "Epoch: [75][45/48]\tTime 0.026 (0.022)\tLoss 1.2513 (1.2190)\tPrec@1 55.762 (56.314)\n",
      "Epoch: [75][48/48]\tTime 0.014 (0.022)\tLoss 1.2015 (1.2206)\tPrec@1 57.783 (56.308)\n",
      "EPOCH: 75 train Results: Prec@1 56.308 Loss: 1.2206\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2009 (1.2009)\tPrec@1 57.520 (57.520)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2395 (1.2259)\tPrec@1 53.444 (56.530)\n",
      "EPOCH: 75 val Results: Prec@1 56.530 Loss: 1.2259\n",
      "Best Prec@1: 56.540\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/48]\tTime 0.015 (0.015)\tLoss 1.2921 (1.2921)\tPrec@1 53.711 (53.711)\n",
      "Epoch: [76][9/48]\tTime 0.024 (0.021)\tLoss 1.2324 (1.2175)\tPrec@1 56.055 (56.455)\n",
      "Epoch: [76][18/48]\tTime 0.019 (0.021)\tLoss 1.2441 (1.2201)\tPrec@1 56.934 (56.363)\n",
      "Epoch: [76][27/48]\tTime 0.015 (0.021)\tLoss 1.1926 (1.2159)\tPrec@1 57.422 (56.428)\n",
      "Epoch: [76][36/48]\tTime 0.022 (0.021)\tLoss 1.2091 (1.2137)\tPrec@1 57.520 (56.464)\n",
      "Epoch: [76][45/48]\tTime 0.017 (0.021)\tLoss 1.1899 (1.2156)\tPrec@1 55.371 (56.307)\n",
      "Epoch: [76][48/48]\tTime 0.014 (0.020)\tLoss 1.1945 (1.2154)\tPrec@1 58.255 (56.376)\n",
      "EPOCH: 76 train Results: Prec@1 56.376 Loss: 1.2154\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2030 (1.2030)\tPrec@1 57.324 (57.324)\n",
      "Test: [9/9]\tTime 0.007 (0.005)\tLoss 1.2417 (1.2237)\tPrec@1 54.082 (56.490)\n",
      "EPOCH: 76 val Results: Prec@1 56.490 Loss: 1.2237\n",
      "Best Prec@1: 56.540\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/48]\tTime 0.016 (0.016)\tLoss 1.1682 (1.1682)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [77][9/48]\tTime 0.020 (0.020)\tLoss 1.1758 (1.1891)\tPrec@1 58.105 (57.646)\n",
      "Epoch: [77][18/48]\tTime 0.017 (0.034)\tLoss 1.1680 (1.1994)\tPrec@1 59.766 (57.139)\n",
      "Epoch: [77][27/48]\tTime 0.024 (0.029)\tLoss 1.2729 (1.2059)\tPrec@1 55.762 (56.993)\n",
      "Epoch: [77][36/48]\tTime 0.019 (0.028)\tLoss 1.2339 (1.2056)\tPrec@1 55.762 (57.137)\n",
      "Epoch: [77][45/48]\tTime 0.017 (0.027)\tLoss 1.1850 (1.2094)\tPrec@1 56.445 (56.917)\n",
      "Epoch: [77][48/48]\tTime 0.016 (0.026)\tLoss 1.2268 (1.2106)\tPrec@1 55.189 (56.832)\n",
      "EPOCH: 77 train Results: Prec@1 56.832 Loss: 1.2106\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.1972 (1.1972)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.006 (0.009)\tLoss 1.2275 (1.2184)\tPrec@1 54.719 (56.530)\n",
      "EPOCH: 77 val Results: Prec@1 56.530 Loss: 1.2184\n",
      "Best Prec@1: 56.540\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/48]\tTime 0.032 (0.032)\tLoss 1.2262 (1.2262)\tPrec@1 54.980 (54.980)\n",
      "Epoch: [78][9/48]\tTime 0.016 (0.022)\tLoss 1.1642 (1.1858)\tPrec@1 58.301 (57.822)\n",
      "Epoch: [78][18/48]\tTime 0.019 (0.022)\tLoss 1.2265 (1.1984)\tPrec@1 54.590 (57.196)\n",
      "Epoch: [78][27/48]\tTime 0.018 (0.027)\tLoss 1.2134 (1.2067)\tPrec@1 58.691 (57.188)\n",
      "Epoch: [78][36/48]\tTime 0.019 (0.025)\tLoss 1.2208 (1.2122)\tPrec@1 54.980 (56.952)\n",
      "Epoch: [78][45/48]\tTime 0.020 (0.025)\tLoss 1.2100 (1.2116)\tPrec@1 58.594 (56.991)\n",
      "Epoch: [78][48/48]\tTime 0.011 (0.025)\tLoss 1.2143 (1.2108)\tPrec@1 54.245 (56.922)\n",
      "EPOCH: 78 train Results: Prec@1 56.922 Loss: 1.2108\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1890 (1.1890)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.2377 (1.2160)\tPrec@1 53.827 (56.720)\n",
      "EPOCH: 78 val Results: Prec@1 56.720 Loss: 1.2160\n",
      "Best Prec@1: 56.720\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/48]\tTime 0.017 (0.017)\tLoss 1.1423 (1.1423)\tPrec@1 60.449 (60.449)\n",
      "Epoch: [79][9/48]\tTime 0.028 (0.023)\tLoss 1.1420 (1.1662)\tPrec@1 60.059 (58.848)\n",
      "Epoch: [79][18/48]\tTime 0.016 (0.022)\tLoss 1.2126 (1.1770)\tPrec@1 55.176 (58.244)\n",
      "Epoch: [79][27/48]\tTime 0.019 (0.022)\tLoss 1.2069 (1.1862)\tPrec@1 58.789 (57.962)\n",
      "Epoch: [79][36/48]\tTime 0.019 (0.022)\tLoss 1.2176 (1.1945)\tPrec@1 56.250 (57.646)\n",
      "Epoch: [79][45/48]\tTime 0.026 (0.022)\tLoss 1.2426 (1.2028)\tPrec@1 54.980 (57.106)\n",
      "Epoch: [79][48/48]\tTime 0.019 (0.022)\tLoss 1.2243 (1.2052)\tPrec@1 56.132 (56.976)\n",
      "EPOCH: 79 train Results: Prec@1 56.976 Loss: 1.2052\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1945 (1.1945)\tPrec@1 57.715 (57.715)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2444 (1.2164)\tPrec@1 54.592 (56.860)\n",
      "EPOCH: 79 val Results: Prec@1 56.860 Loss: 1.2164\n",
      "Best Prec@1: 56.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/48]\tTime 0.015 (0.015)\tLoss 1.1759 (1.1759)\tPrec@1 58.398 (58.398)\n",
      "Epoch: [80][9/48]\tTime 0.022 (0.024)\tLoss 1.2077 (1.1841)\tPrec@1 57.422 (58.369)\n",
      "Epoch: [80][18/48]\tTime 0.014 (0.023)\tLoss 1.1902 (1.1853)\tPrec@1 58.789 (58.003)\n",
      "Epoch: [80][27/48]\tTime 0.015 (0.022)\tLoss 1.2174 (1.1898)\tPrec@1 56.055 (57.610)\n",
      "Epoch: [80][36/48]\tTime 0.013 (0.020)\tLoss 1.2054 (1.1935)\tPrec@1 56.738 (57.483)\n",
      "Epoch: [80][45/48]\tTime 0.019 (0.020)\tLoss 1.2288 (1.1976)\tPrec@1 57.520 (57.449)\n",
      "Epoch: [80][48/48]\tTime 0.013 (0.020)\tLoss 1.2179 (1.1972)\tPrec@1 59.670 (57.510)\n",
      "EPOCH: 80 train Results: Prec@1 57.510 Loss: 1.1972\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1921 (1.1921)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2352 (1.2131)\tPrec@1 54.592 (56.980)\n",
      "EPOCH: 80 val Results: Prec@1 56.980 Loss: 1.2131\n",
      "Best Prec@1: 56.980\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/48]\tTime 0.021 (0.021)\tLoss 1.1728 (1.1728)\tPrec@1 58.496 (58.496)\n",
      "Epoch: [81][9/48]\tTime 0.019 (0.019)\tLoss 1.1735 (1.1740)\tPrec@1 59.766 (58.359)\n",
      "Epoch: [81][18/48]\tTime 0.014 (0.019)\tLoss 1.1650 (1.1819)\tPrec@1 58.887 (57.987)\n",
      "Epoch: [81][27/48]\tTime 0.019 (0.020)\tLoss 1.2595 (1.1867)\tPrec@1 55.957 (57.889)\n",
      "Epoch: [81][36/48]\tTime 0.025 (0.020)\tLoss 1.1468 (1.1899)\tPrec@1 58.105 (57.876)\n",
      "Epoch: [81][45/48]\tTime 0.029 (0.021)\tLoss 1.2004 (1.1972)\tPrec@1 56.152 (57.473)\n",
      "Epoch: [81][48/48]\tTime 0.010 (0.021)\tLoss 1.2338 (1.1987)\tPrec@1 56.486 (57.404)\n",
      "EPOCH: 81 train Results: Prec@1 57.404 Loss: 1.1987\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1946 (1.1946)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.2336 (1.2119)\tPrec@1 55.102 (56.930)\n",
      "EPOCH: 81 val Results: Prec@1 56.930 Loss: 1.2119\n",
      "Best Prec@1: 56.980\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/48]\tTime 0.016 (0.016)\tLoss 1.1823 (1.1823)\tPrec@1 57.324 (57.324)\n",
      "Epoch: [82][9/48]\tTime 0.019 (0.021)\tLoss 1.1957 (1.1744)\tPrec@1 57.129 (58.242)\n",
      "Epoch: [82][18/48]\tTime 0.016 (0.021)\tLoss 1.2039 (1.1817)\tPrec@1 56.445 (57.967)\n",
      "Epoch: [82][27/48]\tTime 0.027 (0.022)\tLoss 1.1780 (1.1838)\tPrec@1 57.422 (57.816)\n",
      "Epoch: [82][36/48]\tTime 0.017 (0.021)\tLoss 1.1690 (1.1871)\tPrec@1 57.715 (57.636)\n",
      "Epoch: [82][45/48]\tTime 0.016 (0.021)\tLoss 1.1895 (1.1938)\tPrec@1 56.445 (57.335)\n",
      "Epoch: [82][48/48]\tTime 0.016 (0.021)\tLoss 1.2628 (1.1974)\tPrec@1 54.245 (57.224)\n",
      "EPOCH: 82 train Results: Prec@1 57.224 Loss: 1.1974\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1885 (1.1885)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.2329 (1.2094)\tPrec@1 53.571 (56.890)\n",
      "EPOCH: 82 val Results: Prec@1 56.890 Loss: 1.2094\n",
      "Best Prec@1: 56.980\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/48]\tTime 0.017 (0.017)\tLoss 1.1964 (1.1964)\tPrec@1 57.129 (57.129)\n",
      "Epoch: [83][9/48]\tTime 0.018 (0.020)\tLoss 1.2178 (1.1767)\tPrec@1 58.691 (58.574)\n",
      "Epoch: [83][18/48]\tTime 0.018 (0.019)\tLoss 1.1753 (1.1831)\tPrec@1 57.715 (58.136)\n",
      "Epoch: [83][27/48]\tTime 0.021 (0.020)\tLoss 1.1993 (1.1843)\tPrec@1 58.105 (57.949)\n",
      "Epoch: [83][36/48]\tTime 0.022 (0.020)\tLoss 1.2019 (1.1881)\tPrec@1 57.031 (57.723)\n",
      "Epoch: [83][45/48]\tTime 0.025 (0.020)\tLoss 1.1210 (1.1921)\tPrec@1 59.766 (57.456)\n",
      "Epoch: [83][48/48]\tTime 0.016 (0.020)\tLoss 1.1825 (1.1937)\tPrec@1 54.009 (57.338)\n",
      "EPOCH: 83 train Results: Prec@1 57.338 Loss: 1.1937\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1942 (1.1942)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.013 (0.006)\tLoss 1.2258 (1.2060)\tPrec@1 55.357 (57.130)\n",
      "EPOCH: 83 val Results: Prec@1 57.130 Loss: 1.2060\n",
      "Best Prec@1: 57.130\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/48]\tTime 0.023 (0.023)\tLoss 1.1083 (1.1083)\tPrec@1 60.059 (60.059)\n",
      "Epoch: [84][9/48]\tTime 0.024 (0.022)\tLoss 1.2384 (1.1725)\tPrec@1 54.688 (58.242)\n",
      "Epoch: [84][18/48]\tTime 0.024 (0.022)\tLoss 1.1522 (1.1727)\tPrec@1 59.277 (58.301)\n",
      "Epoch: [84][27/48]\tTime 0.014 (0.022)\tLoss 1.2298 (1.1855)\tPrec@1 56.445 (57.819)\n",
      "Epoch: [84][36/48]\tTime 0.019 (0.022)\tLoss 1.1881 (1.1873)\tPrec@1 56.836 (57.699)\n",
      "Epoch: [84][45/48]\tTime 0.019 (0.022)\tLoss 1.1923 (1.1865)\tPrec@1 59.570 (57.766)\n",
      "Epoch: [84][48/48]\tTime 0.016 (0.022)\tLoss 1.2737 (1.1888)\tPrec@1 55.425 (57.720)\n",
      "EPOCH: 84 train Results: Prec@1 57.720 Loss: 1.1888\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1954 (1.1954)\tPrec@1 57.422 (57.422)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.2241 (1.2030)\tPrec@1 55.102 (57.260)\n",
      "EPOCH: 84 val Results: Prec@1 57.260 Loss: 1.2030\n",
      "Best Prec@1: 57.260\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/48]\tTime 0.021 (0.021)\tLoss 1.1312 (1.1312)\tPrec@1 58.398 (58.398)\n",
      "Epoch: [85][9/48]\tTime 0.016 (0.026)\tLoss 1.1930 (1.1619)\tPrec@1 58.984 (58.926)\n",
      "Epoch: [85][18/48]\tTime 0.014 (0.023)\tLoss 1.1566 (1.1672)\tPrec@1 60.938 (58.804)\n",
      "Epoch: [85][27/48]\tTime 0.021 (0.024)\tLoss 1.1706 (1.1729)\tPrec@1 57.910 (58.350)\n",
      "Epoch: [85][36/48]\tTime 0.019 (0.023)\tLoss 1.2822 (1.1818)\tPrec@1 54.492 (58.105)\n",
      "Epoch: [85][45/48]\tTime 0.018 (0.023)\tLoss 1.1656 (1.1861)\tPrec@1 59.375 (57.844)\n",
      "Epoch: [85][48/48]\tTime 0.018 (0.023)\tLoss 1.1894 (1.1888)\tPrec@1 56.840 (57.760)\n",
      "EPOCH: 85 train Results: Prec@1 57.760 Loss: 1.1888\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1793 (1.1793)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.2139 (1.1986)\tPrec@1 55.867 (57.270)\n",
      "EPOCH: 85 val Results: Prec@1 57.270 Loss: 1.1986\n",
      "Best Prec@1: 57.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/48]\tTime 0.023 (0.023)\tLoss 1.1587 (1.1587)\tPrec@1 58.496 (58.496)\n",
      "Epoch: [86][9/48]\tTime 0.019 (0.023)\tLoss 1.1477 (1.1770)\tPrec@1 59.863 (58.076)\n",
      "Epoch: [86][18/48]\tTime 0.020 (0.022)\tLoss 1.1973 (1.1724)\tPrec@1 58.203 (58.285)\n",
      "Epoch: [86][27/48]\tTime 0.020 (0.022)\tLoss 1.1846 (1.1721)\tPrec@1 59.473 (58.398)\n",
      "Epoch: [86][36/48]\tTime 0.020 (0.022)\tLoss 1.1358 (1.1775)\tPrec@1 59.961 (58.193)\n",
      "Epoch: [86][45/48]\tTime 0.027 (0.022)\tLoss 1.2415 (1.1814)\tPrec@1 56.445 (57.923)\n",
      "Epoch: [86][48/48]\tTime 0.018 (0.022)\tLoss 1.2092 (1.1821)\tPrec@1 59.198 (57.918)\n",
      "EPOCH: 86 train Results: Prec@1 57.918 Loss: 1.1821\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1731 (1.1731)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.002 (0.006)\tLoss 1.2296 (1.1992)\tPrec@1 56.122 (57.020)\n",
      "EPOCH: 86 val Results: Prec@1 57.020 Loss: 1.1992\n",
      "Best Prec@1: 57.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/48]\tTime 0.024 (0.024)\tLoss 1.1286 (1.1286)\tPrec@1 61.133 (61.133)\n",
      "Epoch: [87][9/48]\tTime 0.015 (0.023)\tLoss 1.1779 (1.1563)\tPrec@1 57.812 (59.355)\n",
      "Epoch: [87][18/48]\tTime 0.021 (0.021)\tLoss 1.1693 (1.1705)\tPrec@1 57.422 (58.671)\n",
      "Epoch: [87][27/48]\tTime 0.015 (0.020)\tLoss 1.1941 (1.1762)\tPrec@1 56.836 (58.343)\n",
      "Epoch: [87][36/48]\tTime 0.013 (0.019)\tLoss 1.2022 (1.1768)\tPrec@1 57.812 (58.319)\n",
      "Epoch: [87][45/48]\tTime 0.019 (0.019)\tLoss 1.2077 (1.1809)\tPrec@1 56.250 (58.105)\n",
      "Epoch: [87][48/48]\tTime 0.015 (0.019)\tLoss 1.1878 (1.1813)\tPrec@1 57.311 (58.054)\n",
      "EPOCH: 87 train Results: Prec@1 58.054 Loss: 1.1813\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1759 (1.1759)\tPrec@1 57.617 (57.617)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.2177 (1.1983)\tPrec@1 54.847 (57.240)\n",
      "EPOCH: 87 val Results: Prec@1 57.240 Loss: 1.1983\n",
      "Best Prec@1: 57.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/48]\tTime 0.018 (0.018)\tLoss 1.0907 (1.0907)\tPrec@1 62.695 (62.695)\n",
      "Epoch: [88][9/48]\tTime 0.025 (0.020)\tLoss 1.1469 (1.1480)\tPrec@1 57.715 (59.316)\n",
      "Epoch: [88][18/48]\tTime 0.018 (0.020)\tLoss 1.1966 (1.1681)\tPrec@1 56.445 (58.362)\n",
      "Epoch: [88][27/48]\tTime 0.020 (0.022)\tLoss 1.1870 (1.1739)\tPrec@1 57.227 (58.133)\n",
      "Epoch: [88][36/48]\tTime 0.025 (0.022)\tLoss 1.2510 (1.1783)\tPrec@1 56.445 (58.005)\n",
      "Epoch: [88][45/48]\tTime 0.020 (0.022)\tLoss 1.1801 (1.1788)\tPrec@1 57.324 (58.025)\n",
      "Epoch: [88][48/48]\tTime 0.016 (0.021)\tLoss 1.2518 (1.1813)\tPrec@1 57.075 (58.002)\n",
      "EPOCH: 88 train Results: Prec@1 58.002 Loss: 1.1813\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1810 (1.1810)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.002 (0.006)\tLoss 1.2118 (1.1998)\tPrec@1 56.505 (57.020)\n",
      "EPOCH: 88 val Results: Prec@1 57.020 Loss: 1.1998\n",
      "Best Prec@1: 57.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/48]\tTime 0.015 (0.015)\tLoss 1.1659 (1.1659)\tPrec@1 56.836 (56.836)\n",
      "Epoch: [89][9/48]\tTime 0.019 (0.022)\tLoss 1.1536 (1.1537)\tPrec@1 59.082 (58.340)\n",
      "Epoch: [89][18/48]\tTime 0.019 (0.021)\tLoss 1.2061 (1.1564)\tPrec@1 57.129 (58.476)\n",
      "Epoch: [89][27/48]\tTime 0.023 (0.022)\tLoss 1.1911 (1.1657)\tPrec@1 56.348 (58.228)\n",
      "Epoch: [89][36/48]\tTime 0.021 (0.021)\tLoss 1.1741 (1.1708)\tPrec@1 58.203 (58.177)\n",
      "Epoch: [89][45/48]\tTime 0.019 (0.021)\tLoss 1.2347 (1.1776)\tPrec@1 57.617 (58.044)\n",
      "Epoch: [89][48/48]\tTime 0.017 (0.021)\tLoss 1.1664 (1.1787)\tPrec@1 58.491 (57.990)\n",
      "EPOCH: 89 train Results: Prec@1 57.990 Loss: 1.1787\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1786 (1.1786)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.2224 (1.1981)\tPrec@1 53.827 (57.190)\n",
      "EPOCH: 89 val Results: Prec@1 57.190 Loss: 1.1981\n",
      "Best Prec@1: 57.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/48]\tTime 0.021 (0.021)\tLoss 1.1531 (1.1531)\tPrec@1 58.301 (58.301)\n",
      "Epoch: [90][9/48]\tTime 0.018 (0.026)\tLoss 1.2125 (1.1552)\tPrec@1 57.324 (58.955)\n",
      "Epoch: [90][18/48]\tTime 0.018 (0.023)\tLoss 1.2013 (1.1670)\tPrec@1 57.910 (58.748)\n",
      "Epoch: [90][27/48]\tTime 0.016 (0.022)\tLoss 1.1693 (1.1681)\tPrec@1 60.352 (58.733)\n",
      "Epoch: [90][36/48]\tTime 0.015 (0.021)\tLoss 1.1425 (1.1666)\tPrec@1 59.473 (58.718)\n",
      "Epoch: [90][45/48]\tTime 0.019 (0.022)\tLoss 1.1798 (1.1691)\tPrec@1 56.250 (58.585)\n",
      "Epoch: [90][48/48]\tTime 0.017 (0.021)\tLoss 1.2073 (1.1716)\tPrec@1 56.604 (58.460)\n",
      "EPOCH: 90 train Results: Prec@1 58.460 Loss: 1.1716\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1770 (1.1770)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2207 (1.1953)\tPrec@1 55.230 (57.440)\n",
      "EPOCH: 90 val Results: Prec@1 57.440 Loss: 1.1953\n",
      "Best Prec@1: 57.440\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/48]\tTime 0.022 (0.022)\tLoss 1.1347 (1.1347)\tPrec@1 59.570 (59.570)\n",
      "Epoch: [91][9/48]\tTime 0.018 (0.022)\tLoss 1.1374 (1.1447)\tPrec@1 59.082 (59.424)\n",
      "Epoch: [91][18/48]\tTime 0.044 (0.025)\tLoss 1.1648 (1.1483)\tPrec@1 58.887 (59.421)\n",
      "Epoch: [91][27/48]\tTime 0.026 (0.024)\tLoss 1.2466 (1.1581)\tPrec@1 56.934 (59.145)\n",
      "Epoch: [91][36/48]\tTime 0.018 (0.024)\tLoss 1.1565 (1.1665)\tPrec@1 59.863 (58.823)\n",
      "Epoch: [91][45/48]\tTime 0.018 (0.027)\tLoss 1.2153 (1.1735)\tPrec@1 55.762 (58.485)\n",
      "Epoch: [91][48/48]\tTime 0.013 (0.026)\tLoss 1.1996 (1.1751)\tPrec@1 58.373 (58.392)\n",
      "EPOCH: 91 train Results: Prec@1 58.392 Loss: 1.1751\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1849 (1.1849)\tPrec@1 59.473 (59.473)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2358 (1.1989)\tPrec@1 54.337 (57.140)\n",
      "EPOCH: 91 val Results: Prec@1 57.140 Loss: 1.1989\n",
      "Best Prec@1: 57.440\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/48]\tTime 0.019 (0.019)\tLoss 1.2187 (1.2187)\tPrec@1 54.980 (54.980)\n",
      "Epoch: [92][9/48]\tTime 0.017 (0.024)\tLoss 1.2291 (1.1530)\tPrec@1 56.055 (58.203)\n",
      "Epoch: [92][18/48]\tTime 0.014 (0.024)\tLoss 1.1733 (1.1578)\tPrec@1 58.984 (58.501)\n",
      "Epoch: [92][27/48]\tTime 0.032 (0.024)\tLoss 1.1249 (1.1609)\tPrec@1 60.449 (58.447)\n",
      "Epoch: [92][36/48]\tTime 0.023 (0.023)\tLoss 1.1713 (1.1628)\tPrec@1 60.156 (58.557)\n",
      "Epoch: [92][45/48]\tTime 0.023 (0.024)\tLoss 1.1757 (1.1702)\tPrec@1 58.398 (58.305)\n",
      "Epoch: [92][48/48]\tTime 0.025 (0.024)\tLoss 1.1860 (1.1726)\tPrec@1 57.311 (58.254)\n",
      "EPOCH: 92 train Results: Prec@1 58.254 Loss: 1.1726\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1796 (1.1796)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.007 (0.008)\tLoss 1.2248 (1.1970)\tPrec@1 56.122 (57.170)\n",
      "EPOCH: 92 val Results: Prec@1 57.170 Loss: 1.1970\n",
      "Best Prec@1: 57.440\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/48]\tTime 0.025 (0.025)\tLoss 1.1210 (1.1210)\tPrec@1 58.496 (58.496)\n",
      "Epoch: [93][9/48]\tTime 0.026 (0.040)\tLoss 1.0813 (1.1329)\tPrec@1 63.867 (59.688)\n",
      "Epoch: [93][18/48]\tTime 0.030 (0.034)\tLoss 1.1763 (1.1443)\tPrec@1 57.715 (59.169)\n",
      "Epoch: [93][27/48]\tTime 0.074 (0.039)\tLoss 1.1981 (1.1539)\tPrec@1 58.301 (58.758)\n",
      "Epoch: [93][36/48]\tTime 0.105 (0.049)\tLoss 1.1318 (1.1578)\tPrec@1 58.887 (58.607)\n",
      "Epoch: [93][45/48]\tTime 0.019 (0.049)\tLoss 1.1918 (1.1689)\tPrec@1 58.398 (58.195)\n",
      "Epoch: [93][48/48]\tTime 0.052 (0.049)\tLoss 1.2228 (1.1697)\tPrec@1 57.193 (58.148)\n",
      "EPOCH: 93 train Results: Prec@1 58.148 Loss: 1.1697\n",
      "Test: [0/9]\tTime 0.023 (0.023)\tLoss 1.1694 (1.1694)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.004 (0.014)\tLoss 1.2074 (1.1918)\tPrec@1 56.122 (57.460)\n",
      "EPOCH: 93 val Results: Prec@1 57.460 Loss: 1.1918\n",
      "Best Prec@1: 57.460\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/48]\tTime 0.024 (0.024)\tLoss 1.1458 (1.1458)\tPrec@1 59.570 (59.570)\n",
      "Epoch: [94][9/48]\tTime 0.034 (0.045)\tLoss 1.1166 (1.1282)\tPrec@1 61.133 (59.746)\n",
      "Epoch: [94][18/48]\tTime 0.021 (0.049)\tLoss 1.1495 (1.1361)\tPrec@1 58.398 (59.524)\n",
      "Epoch: [94][27/48]\tTime 0.017 (0.046)\tLoss 1.1683 (1.1488)\tPrec@1 59.473 (59.086)\n",
      "Epoch: [94][36/48]\tTime 0.025 (0.044)\tLoss 1.2128 (1.1561)\tPrec@1 55.859 (58.847)\n",
      "Epoch: [94][45/48]\tTime 0.025 (0.046)\tLoss 1.1746 (1.1629)\tPrec@1 57.422 (58.626)\n",
      "Epoch: [94][48/48]\tTime 0.017 (0.045)\tLoss 1.2334 (1.1649)\tPrec@1 56.486 (58.560)\n",
      "EPOCH: 94 train Results: Prec@1 58.560 Loss: 1.1649\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1830 (1.1830)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2217 (1.1990)\tPrec@1 55.740 (57.220)\n",
      "EPOCH: 94 val Results: Prec@1 57.220 Loss: 1.1990\n",
      "Best Prec@1: 57.460\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/48]\tTime 0.017 (0.017)\tLoss 1.1755 (1.1755)\tPrec@1 58.691 (58.691)\n",
      "Epoch: [95][9/48]\tTime 0.013 (0.020)\tLoss 1.1161 (1.1319)\tPrec@1 58.984 (60.020)\n",
      "Epoch: [95][18/48]\tTime 0.020 (0.020)\tLoss 1.1699 (1.1461)\tPrec@1 59.180 (59.488)\n",
      "Epoch: [95][27/48]\tTime 0.018 (0.021)\tLoss 1.1298 (1.1506)\tPrec@1 59.570 (59.476)\n",
      "Epoch: [95][36/48]\tTime 0.027 (0.021)\tLoss 1.1966 (1.1563)\tPrec@1 55.762 (59.135)\n",
      "Epoch: [95][45/48]\tTime 0.095 (0.023)\tLoss 1.2685 (1.1667)\tPrec@1 55.859 (58.727)\n",
      "Epoch: [95][48/48]\tTime 0.016 (0.022)\tLoss 1.2005 (1.1675)\tPrec@1 57.193 (58.648)\n",
      "EPOCH: 95 train Results: Prec@1 58.648 Loss: 1.1675\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1691 (1.1691)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.2126 (1.1892)\tPrec@1 55.740 (57.540)\n",
      "EPOCH: 95 val Results: Prec@1 57.540 Loss: 1.1892\n",
      "Best Prec@1: 57.540\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/48]\tTime 0.021 (0.021)\tLoss 1.1415 (1.1415)\tPrec@1 58.691 (58.691)\n",
      "Epoch: [96][9/48]\tTime 0.028 (0.021)\tLoss 1.1680 (1.1351)\tPrec@1 58.887 (59.639)\n",
      "Epoch: [96][18/48]\tTime 0.025 (0.021)\tLoss 1.1663 (1.1416)\tPrec@1 56.055 (59.385)\n",
      "Epoch: [96][27/48]\tTime 0.021 (0.021)\tLoss 1.1729 (1.1537)\tPrec@1 59.766 (59.117)\n",
      "Epoch: [96][36/48]\tTime 0.027 (0.021)\tLoss 1.1347 (1.1543)\tPrec@1 60.938 (59.093)\n",
      "Epoch: [96][45/48]\tTime 0.019 (0.022)\tLoss 1.1444 (1.1571)\tPrec@1 59.180 (58.914)\n",
      "Epoch: [96][48/48]\tTime 0.018 (0.022)\tLoss 1.2133 (1.1608)\tPrec@1 57.075 (58.770)\n",
      "EPOCH: 96 train Results: Prec@1 58.770 Loss: 1.1608\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1617 (1.1617)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.009 (0.006)\tLoss 1.1992 (1.1882)\tPrec@1 57.908 (57.610)\n",
      "EPOCH: 96 val Results: Prec@1 57.610 Loss: 1.1882\n",
      "Best Prec@1: 57.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/48]\tTime 0.028 (0.028)\tLoss 1.1736 (1.1736)\tPrec@1 58.496 (58.496)\n",
      "Epoch: [97][9/48]\tTime 0.087 (0.029)\tLoss 1.1841 (1.1343)\tPrec@1 57.422 (60.127)\n",
      "Epoch: [97][18/48]\tTime 0.025 (0.033)\tLoss 1.1454 (1.1490)\tPrec@1 58.984 (59.354)\n",
      "Epoch: [97][27/48]\tTime 0.030 (0.030)\tLoss 1.1879 (1.1519)\tPrec@1 58.887 (59.253)\n",
      "Epoch: [97][36/48]\tTime 0.034 (0.042)\tLoss 1.1509 (1.1551)\tPrec@1 58.691 (59.135)\n",
      "Epoch: [97][45/48]\tTime 0.041 (0.039)\tLoss 1.1811 (1.1615)\tPrec@1 57.812 (58.906)\n",
      "Epoch: [97][48/48]\tTime 0.019 (0.038)\tLoss 1.1706 (1.1620)\tPrec@1 58.019 (58.816)\n",
      "EPOCH: 97 train Results: Prec@1 58.816 Loss: 1.1620\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1663 (1.1663)\tPrec@1 59.082 (59.082)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1999 (1.1888)\tPrec@1 56.122 (57.160)\n",
      "EPOCH: 97 val Results: Prec@1 57.160 Loss: 1.1888\n",
      "Best Prec@1: 57.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/48]\tTime 0.014 (0.014)\tLoss 1.1662 (1.1662)\tPrec@1 58.496 (58.496)\n",
      "Epoch: [98][9/48]\tTime 0.018 (0.022)\tLoss 1.0952 (1.1342)\tPrec@1 60.449 (59.482)\n",
      "Epoch: [98][18/48]\tTime 0.028 (0.021)\tLoss 1.1264 (1.1418)\tPrec@1 59.961 (59.370)\n",
      "Epoch: [98][27/48]\tTime 0.020 (0.021)\tLoss 1.1752 (1.1472)\tPrec@1 58.789 (59.204)\n",
      "Epoch: [98][36/48]\tTime 0.026 (0.021)\tLoss 1.1691 (1.1507)\tPrec@1 59.570 (59.314)\n",
      "Epoch: [98][45/48]\tTime 0.025 (0.022)\tLoss 1.1518 (1.1576)\tPrec@1 60.938 (59.025)\n",
      "Epoch: [98][48/48]\tTime 0.017 (0.022)\tLoss 1.2067 (1.1605)\tPrec@1 55.660 (58.890)\n",
      "EPOCH: 98 train Results: Prec@1 58.890 Loss: 1.1605\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1687 (1.1687)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2100 (1.1935)\tPrec@1 55.740 (57.000)\n",
      "EPOCH: 98 val Results: Prec@1 57.000 Loss: 1.1935\n",
      "Best Prec@1: 57.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/48]\tTime 0.023 (0.023)\tLoss 1.1540 (1.1540)\tPrec@1 57.520 (57.520)\n",
      "Epoch: [99][9/48]\tTime 0.017 (0.022)\tLoss 1.1394 (1.1467)\tPrec@1 59.473 (58.887)\n",
      "Epoch: [99][18/48]\tTime 0.018 (0.023)\tLoss 1.1161 (1.1505)\tPrec@1 59.668 (58.655)\n",
      "Epoch: [99][27/48]\tTime 0.019 (0.023)\tLoss 1.1571 (1.1506)\tPrec@1 60.059 (58.876)\n",
      "Epoch: [99][36/48]\tTime 0.013 (0.022)\tLoss 1.1831 (1.1576)\tPrec@1 58.203 (58.562)\n",
      "Epoch: [99][45/48]\tTime 0.020 (0.022)\tLoss 1.1292 (1.1606)\tPrec@1 60.156 (58.456)\n",
      "Epoch: [99][48/48]\tTime 0.018 (0.022)\tLoss 1.1512 (1.1610)\tPrec@1 59.434 (58.440)\n",
      "EPOCH: 99 train Results: Prec@1 58.440 Loss: 1.1610\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1765 (1.1765)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.002 (0.007)\tLoss 1.2032 (1.1892)\tPrec@1 55.612 (57.530)\n",
      "EPOCH: 99 val Results: Prec@1 57.530 Loss: 1.1892\n",
      "Best Prec@1: 57.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/48]\tTime 0.015 (0.015)\tLoss 1.1522 (1.1522)\tPrec@1 59.277 (59.277)\n",
      "Epoch: [100][9/48]\tTime 0.026 (0.019)\tLoss 1.1522 (1.1373)\tPrec@1 57.617 (59.590)\n",
      "Epoch: [100][18/48]\tTime 0.025 (0.021)\tLoss 1.1826 (1.1410)\tPrec@1 58.203 (59.565)\n",
      "Epoch: [100][27/48]\tTime 0.022 (0.022)\tLoss 1.1274 (1.1484)\tPrec@1 61.719 (59.406)\n",
      "Epoch: [100][36/48]\tTime 0.017 (0.021)\tLoss 1.1638 (1.1541)\tPrec@1 59.180 (59.172)\n",
      "Epoch: [100][45/48]\tTime 0.015 (0.021)\tLoss 1.1328 (1.1576)\tPrec@1 59.961 (59.012)\n",
      "Epoch: [100][48/48]\tTime 0.023 (0.021)\tLoss 1.1563 (1.1574)\tPrec@1 58.255 (59.012)\n",
      "EPOCH: 100 train Results: Prec@1 59.012 Loss: 1.1574\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1716 (1.1716)\tPrec@1 58.594 (58.594)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2137 (1.1916)\tPrec@1 53.827 (57.230)\n",
      "EPOCH: 100 val Results: Prec@1 57.230 Loss: 1.1916\n",
      "Best Prec@1: 57.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/48]\tTime 0.016 (0.016)\tLoss 1.1262 (1.1262)\tPrec@1 61.133 (61.133)\n",
      "Epoch: [101][9/48]\tTime 0.018 (0.025)\tLoss 1.1540 (1.1312)\tPrec@1 59.570 (59.502)\n",
      "Epoch: [101][18/48]\tTime 0.023 (0.029)\tLoss 1.1504 (1.1432)\tPrec@1 58.789 (59.231)\n",
      "Epoch: [101][27/48]\tTime 0.032 (0.027)\tLoss 1.1233 (1.1471)\tPrec@1 60.645 (59.117)\n",
      "Epoch: [101][36/48]\tTime 0.022 (0.026)\tLoss 1.1278 (1.1454)\tPrec@1 57.520 (58.998)\n",
      "Epoch: [101][45/48]\tTime 0.018 (0.025)\tLoss 1.1676 (1.1511)\tPrec@1 58.008 (58.882)\n",
      "Epoch: [101][48/48]\tTime 0.026 (0.024)\tLoss 1.1286 (1.1514)\tPrec@1 61.085 (58.908)\n",
      "EPOCH: 101 train Results: Prec@1 58.908 Loss: 1.1514\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1698 (1.1698)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.2049 (1.1890)\tPrec@1 54.592 (57.470)\n",
      "EPOCH: 101 val Results: Prec@1 57.470 Loss: 1.1890\n",
      "Best Prec@1: 57.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/48]\tTime 0.018 (0.018)\tLoss 1.0971 (1.0971)\tPrec@1 61.328 (61.328)\n",
      "Epoch: [102][9/48]\tTime 0.020 (0.022)\tLoss 1.1528 (1.1272)\tPrec@1 58.496 (60.010)\n",
      "Epoch: [102][18/48]\tTime 0.021 (0.027)\tLoss 1.1529 (1.1345)\tPrec@1 57.812 (59.987)\n",
      "Epoch: [102][27/48]\tTime 0.020 (0.024)\tLoss 1.1833 (1.1422)\tPrec@1 57.129 (59.462)\n",
      "Epoch: [102][36/48]\tTime 0.019 (0.026)\tLoss 1.1576 (1.1477)\tPrec@1 60.938 (59.248)\n",
      "Epoch: [102][45/48]\tTime 0.028 (0.026)\tLoss 1.1645 (1.1506)\tPrec@1 57.910 (59.226)\n",
      "Epoch: [102][48/48]\tTime 0.026 (0.026)\tLoss 1.1535 (1.1534)\tPrec@1 58.608 (59.062)\n",
      "EPOCH: 102 train Results: Prec@1 59.062 Loss: 1.1534\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.1757 (1.1757)\tPrec@1 57.910 (57.910)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2145 (1.1883)\tPrec@1 55.230 (57.530)\n",
      "EPOCH: 102 val Results: Prec@1 57.530 Loss: 1.1883\n",
      "Best Prec@1: 57.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/48]\tTime 0.024 (0.024)\tLoss 1.1310 (1.1310)\tPrec@1 58.691 (58.691)\n",
      "Epoch: [103][9/48]\tTime 0.024 (0.022)\tLoss 1.1387 (1.1364)\tPrec@1 60.645 (60.059)\n",
      "Epoch: [103][18/48]\tTime 0.025 (0.021)\tLoss 1.1483 (1.1357)\tPrec@1 59.277 (59.699)\n",
      "Epoch: [103][27/48]\tTime 0.030 (0.022)\tLoss 1.1415 (1.1377)\tPrec@1 59.277 (59.734)\n",
      "Epoch: [103][36/48]\tTime 0.021 (0.022)\tLoss 1.1463 (1.1457)\tPrec@1 58.105 (59.254)\n",
      "Epoch: [103][45/48]\tTime 0.016 (0.022)\tLoss 1.1482 (1.1503)\tPrec@1 58.887 (59.086)\n",
      "Epoch: [103][48/48]\tTime 0.015 (0.022)\tLoss 1.1533 (1.1514)\tPrec@1 58.137 (58.982)\n",
      "EPOCH: 103 train Results: Prec@1 58.982 Loss: 1.1514\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1579 (1.1579)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2009 (1.1860)\tPrec@1 55.485 (57.680)\n",
      "EPOCH: 103 val Results: Prec@1 57.680 Loss: 1.1860\n",
      "Best Prec@1: 57.680\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/48]\tTime 0.020 (0.020)\tLoss 1.1375 (1.1375)\tPrec@1 60.742 (60.742)\n",
      "Epoch: [104][9/48]\tTime 0.020 (0.021)\tLoss 1.1461 (1.1162)\tPrec@1 57.617 (60.361)\n",
      "Epoch: [104][18/48]\tTime 0.020 (0.019)\tLoss 1.1170 (1.1268)\tPrec@1 59.570 (59.719)\n",
      "Epoch: [104][27/48]\tTime 0.017 (0.020)\tLoss 1.1528 (1.1352)\tPrec@1 60.742 (59.598)\n",
      "Epoch: [104][36/48]\tTime 0.023 (0.021)\tLoss 1.1923 (1.1440)\tPrec@1 59.277 (59.354)\n",
      "Epoch: [104][45/48]\tTime 0.019 (0.021)\tLoss 1.1813 (1.1471)\tPrec@1 56.543 (59.120)\n",
      "Epoch: [104][48/48]\tTime 0.020 (0.021)\tLoss 1.1322 (1.1490)\tPrec@1 58.726 (59.048)\n",
      "EPOCH: 104 train Results: Prec@1 59.048 Loss: 1.1490\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1573 (1.1573)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.2155 (1.1848)\tPrec@1 54.847 (57.860)\n",
      "EPOCH: 104 val Results: Prec@1 57.860 Loss: 1.1848\n",
      "Best Prec@1: 57.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/48]\tTime 0.020 (0.020)\tLoss 1.1581 (1.1581)\tPrec@1 59.277 (59.277)\n",
      "Epoch: [105][9/48]\tTime 0.019 (0.023)\tLoss 1.1763 (1.1328)\tPrec@1 59.375 (59.902)\n",
      "Epoch: [105][18/48]\tTime 0.026 (0.022)\tLoss 1.1526 (1.1385)\tPrec@1 59.570 (59.550)\n",
      "Epoch: [105][27/48]\tTime 0.022 (0.022)\tLoss 1.1451 (1.1360)\tPrec@1 60.449 (59.598)\n",
      "Epoch: [105][36/48]\tTime 0.025 (0.024)\tLoss 1.1032 (1.1382)\tPrec@1 59.863 (59.452)\n",
      "Epoch: [105][45/48]\tTime 0.029 (0.024)\tLoss 1.1986 (1.1477)\tPrec@1 56.934 (59.069)\n",
      "Epoch: [105][48/48]\tTime 0.015 (0.024)\tLoss 1.1114 (1.1496)\tPrec@1 61.203 (59.046)\n",
      "EPOCH: 105 train Results: Prec@1 59.046 Loss: 1.1496\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1582 (1.1582)\tPrec@1 60.938 (60.938)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1889 (1.1828)\tPrec@1 55.102 (58.100)\n",
      "EPOCH: 105 val Results: Prec@1 58.100 Loss: 1.1828\n",
      "Best Prec@1: 58.100\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/48]\tTime 0.019 (0.019)\tLoss 1.0814 (1.0814)\tPrec@1 62.012 (62.012)\n",
      "Epoch: [106][9/48]\tTime 0.019 (0.020)\tLoss 1.1015 (1.1086)\tPrec@1 59.668 (61.084)\n",
      "Epoch: [106][18/48]\tTime 0.017 (0.020)\tLoss 1.1516 (1.1239)\tPrec@1 58.984 (60.238)\n",
      "Epoch: [106][27/48]\tTime 0.022 (0.021)\tLoss 1.1535 (1.1299)\tPrec@1 59.277 (60.118)\n",
      "Epoch: [106][36/48]\tTime 0.027 (0.023)\tLoss 1.1609 (1.1387)\tPrec@1 57.031 (59.700)\n",
      "Epoch: [106][45/48]\tTime 0.019 (0.022)\tLoss 1.1882 (1.1445)\tPrec@1 57.812 (59.477)\n",
      "Epoch: [106][48/48]\tTime 0.021 (0.022)\tLoss 1.1044 (1.1465)\tPrec@1 61.321 (59.446)\n",
      "EPOCH: 106 train Results: Prec@1 59.446 Loss: 1.1465\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1502 (1.1502)\tPrec@1 60.742 (60.742)\n",
      "Test: [9/9]\tTime 0.003 (0.008)\tLoss 1.1946 (1.1772)\tPrec@1 57.143 (58.410)\n",
      "EPOCH: 106 val Results: Prec@1 58.410 Loss: 1.1772\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/48]\tTime 0.022 (0.022)\tLoss 1.1039 (1.1039)\tPrec@1 62.305 (62.305)\n",
      "Epoch: [107][9/48]\tTime 0.014 (0.025)\tLoss 1.1496 (1.1125)\tPrec@1 58.203 (60.811)\n",
      "Epoch: [107][18/48]\tTime 0.017 (0.022)\tLoss 1.1180 (1.1185)\tPrec@1 61.426 (60.675)\n",
      "Epoch: [107][27/48]\tTime 0.014 (0.022)\tLoss 1.1065 (1.1179)\tPrec@1 61.719 (60.798)\n",
      "Epoch: [107][36/48]\tTime 0.018 (0.021)\tLoss 1.2000 (1.1297)\tPrec@1 57.031 (60.367)\n",
      "Epoch: [107][45/48]\tTime 0.022 (0.023)\tLoss 1.1536 (1.1381)\tPrec@1 58.691 (59.874)\n",
      "Epoch: [107][48/48]\tTime 0.021 (0.023)\tLoss 1.1846 (1.1407)\tPrec@1 58.019 (59.732)\n",
      "EPOCH: 107 train Results: Prec@1 59.732 Loss: 1.1407\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1521 (1.1521)\tPrec@1 59.082 (59.082)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.1908 (1.1835)\tPrec@1 54.847 (57.870)\n",
      "EPOCH: 107 val Results: Prec@1 57.870 Loss: 1.1835\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/48]\tTime 0.014 (0.014)\tLoss 1.0777 (1.0777)\tPrec@1 63.086 (63.086)\n",
      "Epoch: [108][9/48]\tTime 0.022 (0.019)\tLoss 1.0866 (1.1073)\tPrec@1 61.035 (60.996)\n",
      "Epoch: [108][18/48]\tTime 0.019 (0.018)\tLoss 1.1525 (1.1248)\tPrec@1 58.496 (60.151)\n",
      "Epoch: [108][27/48]\tTime 0.015 (0.019)\tLoss 1.1606 (1.1320)\tPrec@1 59.277 (59.710)\n",
      "Epoch: [108][36/48]\tTime 0.019 (0.019)\tLoss 1.1133 (1.1390)\tPrec@1 60.449 (59.459)\n",
      "Epoch: [108][45/48]\tTime 0.024 (0.020)\tLoss 1.1636 (1.1425)\tPrec@1 58.301 (59.405)\n",
      "Epoch: [108][48/48]\tTime 0.023 (0.020)\tLoss 1.2060 (1.1435)\tPrec@1 56.014 (59.354)\n",
      "EPOCH: 108 train Results: Prec@1 59.354 Loss: 1.1435\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1617 (1.1617)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.1919 (1.1820)\tPrec@1 57.015 (57.830)\n",
      "EPOCH: 108 val Results: Prec@1 57.830 Loss: 1.1820\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/48]\tTime 0.033 (0.033)\tLoss 1.1790 (1.1790)\tPrec@1 56.543 (56.543)\n",
      "Epoch: [109][9/48]\tTime 0.019 (0.023)\tLoss 1.1496 (1.1306)\tPrec@1 59.277 (59.795)\n",
      "Epoch: [109][18/48]\tTime 0.046 (0.025)\tLoss 1.1422 (1.1399)\tPrec@1 60.645 (59.642)\n",
      "Epoch: [109][27/48]\tTime 0.031 (0.025)\tLoss 1.1502 (1.1395)\tPrec@1 57.617 (59.584)\n",
      "Epoch: [109][36/48]\tTime 0.017 (0.024)\tLoss 1.1440 (1.1432)\tPrec@1 60.742 (59.560)\n",
      "Epoch: [109][45/48]\tTime 0.016 (0.024)\tLoss 1.1620 (1.1441)\tPrec@1 59.961 (59.468)\n",
      "Epoch: [109][48/48]\tTime 0.016 (0.024)\tLoss 1.1599 (1.1456)\tPrec@1 59.316 (59.438)\n",
      "EPOCH: 109 train Results: Prec@1 59.438 Loss: 1.1456\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1447 (1.1447)\tPrec@1 60.352 (60.352)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.1990 (1.1764)\tPrec@1 57.653 (58.390)\n",
      "EPOCH: 109 val Results: Prec@1 58.390 Loss: 1.1764\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/48]\tTime 0.020 (0.020)\tLoss 1.1140 (1.1140)\tPrec@1 61.230 (61.230)\n",
      "Epoch: [110][9/48]\tTime 0.036 (0.026)\tLoss 1.0386 (1.1101)\tPrec@1 63.477 (60.693)\n",
      "Epoch: [110][18/48]\tTime 0.015 (0.024)\tLoss 1.1097 (1.1189)\tPrec@1 62.012 (60.434)\n",
      "Epoch: [110][27/48]\tTime 0.019 (0.022)\tLoss 1.1772 (1.1249)\tPrec@1 58.105 (60.191)\n",
      "Epoch: [110][36/48]\tTime 0.014 (0.021)\tLoss 1.1630 (1.1343)\tPrec@1 59.473 (59.869)\n",
      "Epoch: [110][45/48]\tTime 0.018 (0.022)\tLoss 1.1056 (1.1395)\tPrec@1 58.301 (59.481)\n",
      "Epoch: [110][48/48]\tTime 0.021 (0.022)\tLoss 1.1861 (1.1399)\tPrec@1 57.901 (59.456)\n",
      "EPOCH: 110 train Results: Prec@1 59.456 Loss: 1.1399\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1487 (1.1487)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1946 (1.1788)\tPrec@1 57.270 (58.040)\n",
      "EPOCH: 110 val Results: Prec@1 58.040 Loss: 1.1788\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/48]\tTime 0.022 (0.022)\tLoss 1.1013 (1.1013)\tPrec@1 60.645 (60.645)\n",
      "Epoch: [111][9/48]\tTime 0.018 (0.023)\tLoss 1.1267 (1.1205)\tPrec@1 60.254 (60.527)\n",
      "Epoch: [111][18/48]\tTime 0.015 (0.022)\tLoss 1.1269 (1.1291)\tPrec@1 61.523 (59.951)\n",
      "Epoch: [111][27/48]\tTime 0.023 (0.026)\tLoss 1.1105 (1.1429)\tPrec@1 61.133 (59.434)\n",
      "Epoch: [111][36/48]\tTime 0.023 (0.026)\tLoss 1.2213 (1.1485)\tPrec@1 57.520 (59.298)\n",
      "Epoch: [111][45/48]\tTime 0.018 (0.025)\tLoss 1.1985 (1.1478)\tPrec@1 59.277 (59.350)\n",
      "Epoch: [111][48/48]\tTime 0.014 (0.025)\tLoss 1.2344 (1.1486)\tPrec@1 56.250 (59.316)\n",
      "EPOCH: 111 train Results: Prec@1 59.316 Loss: 1.1486\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1385 (1.1385)\tPrec@1 60.059 (60.059)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.1895 (1.1806)\tPrec@1 56.122 (57.940)\n",
      "EPOCH: 111 val Results: Prec@1 57.940 Loss: 1.1806\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/48]\tTime 0.013 (0.013)\tLoss 1.1121 (1.1121)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [112][9/48]\tTime 0.017 (0.021)\tLoss 1.0982 (1.1146)\tPrec@1 61.523 (60.156)\n",
      "Epoch: [112][18/48]\tTime 0.032 (0.030)\tLoss 1.1158 (1.1205)\tPrec@1 59.961 (59.997)\n",
      "Epoch: [112][27/48]\tTime 0.020 (0.030)\tLoss 1.0742 (1.1236)\tPrec@1 62.402 (59.832)\n",
      "Epoch: [112][36/48]\tTime 0.024 (0.028)\tLoss 1.1204 (1.1352)\tPrec@1 60.156 (59.415)\n",
      "Epoch: [112][45/48]\tTime 0.021 (0.028)\tLoss 1.1918 (1.1410)\tPrec@1 57.715 (59.163)\n",
      "Epoch: [112][48/48]\tTime 0.014 (0.028)\tLoss 1.2008 (1.1419)\tPrec@1 56.250 (59.096)\n",
      "EPOCH: 112 train Results: Prec@1 59.096 Loss: 1.1419\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1359 (1.1359)\tPrec@1 60.742 (60.742)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.1858 (1.1771)\tPrec@1 58.036 (58.110)\n",
      "EPOCH: 112 val Results: Prec@1 58.110 Loss: 1.1771\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/48]\tTime 0.020 (0.020)\tLoss 1.1550 (1.1550)\tPrec@1 57.910 (57.910)\n",
      "Epoch: [113][9/48]\tTime 0.020 (0.023)\tLoss 1.1718 (1.1160)\tPrec@1 59.375 (60.391)\n",
      "Epoch: [113][18/48]\tTime 0.020 (0.023)\tLoss 1.1608 (1.1226)\tPrec@1 59.766 (60.300)\n",
      "Epoch: [113][27/48]\tTime 0.044 (0.028)\tLoss 1.1226 (1.1310)\tPrec@1 59.863 (59.769)\n",
      "Epoch: [113][36/48]\tTime 0.039 (0.030)\tLoss 1.1614 (1.1326)\tPrec@1 58.203 (59.789)\n",
      "Epoch: [113][45/48]\tTime 0.027 (0.033)\tLoss 1.1577 (1.1382)\tPrec@1 58.887 (59.555)\n",
      "Epoch: [113][48/48]\tTime 0.028 (0.034)\tLoss 1.1287 (1.1400)\tPrec@1 59.434 (59.520)\n",
      "EPOCH: 113 train Results: Prec@1 59.520 Loss: 1.1400\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1450 (1.1450)\tPrec@1 60.059 (60.059)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.1953 (1.1807)\tPrec@1 57.526 (57.890)\n",
      "EPOCH: 113 val Results: Prec@1 57.890 Loss: 1.1807\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/48]\tTime 0.032 (0.032)\tLoss 1.0872 (1.0872)\tPrec@1 61.621 (61.621)\n",
      "Epoch: [114][9/48]\tTime 0.035 (0.074)\tLoss 1.1133 (1.1194)\tPrec@1 60.059 (60.342)\n",
      "Epoch: [114][18/48]\tTime 0.019 (0.050)\tLoss 1.1762 (1.1281)\tPrec@1 59.180 (60.531)\n",
      "Epoch: [114][27/48]\tTime 0.066 (0.073)\tLoss 1.0801 (1.1301)\tPrec@1 63.574 (60.310)\n",
      "Epoch: [114][36/48]\tTime 0.028 (0.062)\tLoss 1.0778 (1.1295)\tPrec@1 63.672 (60.407)\n",
      "Epoch: [114][45/48]\tTime 0.020 (0.054)\tLoss 1.1922 (1.1366)\tPrec@1 56.641 (60.071)\n",
      "Epoch: [114][48/48]\tTime 0.020 (0.053)\tLoss 1.2068 (1.1393)\tPrec@1 53.420 (59.898)\n",
      "EPOCH: 114 train Results: Prec@1 59.898 Loss: 1.1393\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1473 (1.1473)\tPrec@1 60.156 (60.156)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1943 (1.1819)\tPrec@1 55.102 (57.890)\n",
      "EPOCH: 114 val Results: Prec@1 57.890 Loss: 1.1819\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/48]\tTime 0.020 (0.020)\tLoss 1.1224 (1.1224)\tPrec@1 61.035 (61.035)\n",
      "Epoch: [115][9/48]\tTime 0.084 (0.025)\tLoss 1.1240 (1.0993)\tPrec@1 60.059 (60.986)\n",
      "Epoch: [115][18/48]\tTime 0.015 (0.022)\tLoss 1.1159 (1.1148)\tPrec@1 60.254 (60.501)\n",
      "Epoch: [115][27/48]\tTime 0.034 (0.023)\tLoss 1.1866 (1.1193)\tPrec@1 57.617 (60.324)\n",
      "Epoch: [115][36/48]\tTime 0.023 (0.024)\tLoss 1.1326 (1.1273)\tPrec@1 60.156 (60.040)\n",
      "Epoch: [115][45/48]\tTime 0.019 (0.023)\tLoss 1.1470 (1.1330)\tPrec@1 58.008 (59.795)\n",
      "Epoch: [115][48/48]\tTime 0.033 (0.023)\tLoss 1.1298 (1.1327)\tPrec@1 61.439 (59.880)\n",
      "EPOCH: 115 train Results: Prec@1 59.880 Loss: 1.1327\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1408 (1.1408)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.1865 (1.1778)\tPrec@1 57.015 (57.720)\n",
      "EPOCH: 115 val Results: Prec@1 57.720 Loss: 1.1778\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/48]\tTime 0.018 (0.018)\tLoss 1.1173 (1.1173)\tPrec@1 59.473 (59.473)\n",
      "Epoch: [116][9/48]\tTime 0.016 (0.022)\tLoss 1.1319 (1.1149)\tPrec@1 59.473 (60.586)\n",
      "Epoch: [116][18/48]\tTime 0.020 (0.022)\tLoss 1.1291 (1.1157)\tPrec@1 58.105 (60.362)\n",
      "Epoch: [116][27/48]\tTime 0.017 (0.022)\tLoss 1.1094 (1.1185)\tPrec@1 61.523 (60.083)\n",
      "Epoch: [116][36/48]\tTime 0.014 (0.022)\tLoss 1.1776 (1.1263)\tPrec@1 57.617 (59.900)\n",
      "Epoch: [116][45/48]\tTime 0.021 (0.023)\tLoss 1.1697 (1.1294)\tPrec@1 59.277 (59.865)\n",
      "Epoch: [116][48/48]\tTime 0.017 (0.023)\tLoss 1.1726 (1.1319)\tPrec@1 57.901 (59.782)\n",
      "EPOCH: 116 train Results: Prec@1 59.782 Loss: 1.1319\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1569 (1.1569)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.1737 (1.1763)\tPrec@1 58.163 (57.850)\n",
      "EPOCH: 116 val Results: Prec@1 57.850 Loss: 1.1763\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/48]\tTime 0.021 (0.021)\tLoss 1.1026 (1.1026)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [117][9/48]\tTime 0.022 (0.024)\tLoss 1.1315 (1.1107)\tPrec@1 59.766 (60.674)\n",
      "Epoch: [117][18/48]\tTime 0.026 (0.023)\tLoss 1.0849 (1.1191)\tPrec@1 63.574 (60.362)\n",
      "Epoch: [117][27/48]\tTime 0.019 (0.023)\tLoss 1.1124 (1.1205)\tPrec@1 58.984 (60.296)\n",
      "Epoch: [117][36/48]\tTime 0.023 (0.023)\tLoss 1.1376 (1.1290)\tPrec@1 59.570 (59.869)\n",
      "Epoch: [117][45/48]\tTime 0.023 (0.023)\tLoss 1.1644 (1.1328)\tPrec@1 60.059 (59.819)\n",
      "Epoch: [117][48/48]\tTime 0.026 (0.023)\tLoss 1.1585 (1.1332)\tPrec@1 58.137 (59.834)\n",
      "EPOCH: 117 train Results: Prec@1 59.834 Loss: 1.1332\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1398 (1.1398)\tPrec@1 60.742 (60.742)\n",
      "Test: [9/9]\tTime 0.004 (0.007)\tLoss 1.1820 (1.1728)\tPrec@1 56.760 (58.020)\n",
      "EPOCH: 117 val Results: Prec@1 58.020 Loss: 1.1728\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/48]\tTime 0.024 (0.024)\tLoss 1.1160 (1.1160)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [118][9/48]\tTime 0.015 (0.023)\tLoss 1.0750 (1.1101)\tPrec@1 63.672 (60.781)\n",
      "Epoch: [118][18/48]\tTime 0.021 (0.023)\tLoss 1.1563 (1.1136)\tPrec@1 59.082 (60.747)\n",
      "Epoch: [118][27/48]\tTime 0.022 (0.023)\tLoss 1.1427 (1.1190)\tPrec@1 58.594 (60.400)\n",
      "Epoch: [118][36/48]\tTime 0.015 (0.022)\tLoss 1.1794 (1.1258)\tPrec@1 58.984 (60.114)\n",
      "Epoch: [118][45/48]\tTime 0.024 (0.022)\tLoss 1.1472 (1.1322)\tPrec@1 59.180 (59.897)\n",
      "Epoch: [118][48/48]\tTime 0.018 (0.022)\tLoss 1.0963 (1.1321)\tPrec@1 61.321 (59.916)\n",
      "EPOCH: 118 train Results: Prec@1 59.916 Loss: 1.1321\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1379 (1.1379)\tPrec@1 60.938 (60.938)\n",
      "Test: [9/9]\tTime 0.007 (0.005)\tLoss 1.1832 (1.1702)\tPrec@1 55.995 (58.260)\n",
      "EPOCH: 118 val Results: Prec@1 58.260 Loss: 1.1702\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/48]\tTime 0.019 (0.019)\tLoss 1.0779 (1.0779)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [119][9/48]\tTime 0.020 (0.022)\tLoss 1.1319 (1.0987)\tPrec@1 57.422 (61.475)\n",
      "Epoch: [119][18/48]\tTime 0.013 (0.021)\tLoss 1.1431 (1.1093)\tPrec@1 61.328 (60.860)\n",
      "Epoch: [119][27/48]\tTime 0.020 (0.021)\tLoss 1.1063 (1.1173)\tPrec@1 59.473 (60.470)\n",
      "Epoch: [119][36/48]\tTime 0.015 (0.020)\tLoss 1.1221 (1.1197)\tPrec@1 58.691 (60.349)\n",
      "Epoch: [119][45/48]\tTime 0.018 (0.020)\tLoss 1.1866 (1.1266)\tPrec@1 57.715 (60.020)\n",
      "Epoch: [119][48/48]\tTime 0.022 (0.020)\tLoss 1.0953 (1.1272)\tPrec@1 62.618 (60.040)\n",
      "EPOCH: 119 train Results: Prec@1 60.040 Loss: 1.1272\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1342 (1.1342)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.1801 (1.1709)\tPrec@1 56.760 (57.990)\n",
      "EPOCH: 119 val Results: Prec@1 57.990 Loss: 1.1709\n",
      "Best Prec@1: 58.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/48]\tTime 0.015 (0.015)\tLoss 1.0422 (1.0422)\tPrec@1 61.523 (61.523)\n",
      "Epoch: [120][9/48]\tTime 0.033 (0.023)\tLoss 1.1678 (1.1054)\tPrec@1 59.180 (60.635)\n",
      "Epoch: [120][18/48]\tTime 0.012 (0.022)\tLoss 1.1044 (1.1184)\tPrec@1 60.254 (60.151)\n",
      "Epoch: [120][27/48]\tTime 0.018 (0.021)\tLoss 1.1945 (1.1222)\tPrec@1 57.324 (60.118)\n",
      "Epoch: [120][36/48]\tTime 0.023 (0.022)\tLoss 1.1285 (1.1231)\tPrec@1 60.645 (60.037)\n",
      "Epoch: [120][45/48]\tTime 0.022 (0.022)\tLoss 1.1683 (1.1275)\tPrec@1 57.422 (59.901)\n",
      "Epoch: [120][48/48]\tTime 0.031 (0.022)\tLoss 1.1211 (1.1291)\tPrec@1 59.316 (59.828)\n",
      "EPOCH: 120 train Results: Prec@1 59.828 Loss: 1.1291\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1384 (1.1384)\tPrec@1 59.766 (59.766)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.1750 (1.1653)\tPrec@1 58.418 (58.610)\n",
      "EPOCH: 120 val Results: Prec@1 58.610 Loss: 1.1653\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/48]\tTime 0.027 (0.027)\tLoss 1.1276 (1.1276)\tPrec@1 58.203 (58.203)\n",
      "Epoch: [121][9/48]\tTime 0.019 (0.021)\tLoss 1.0859 (1.1036)\tPrec@1 61.523 (61.055)\n",
      "Epoch: [121][18/48]\tTime 0.027 (0.023)\tLoss 1.1282 (1.1182)\tPrec@1 60.645 (60.542)\n",
      "Epoch: [121][27/48]\tTime 0.019 (0.021)\tLoss 1.1005 (1.1188)\tPrec@1 60.059 (60.498)\n",
      "Epoch: [121][36/48]\tTime 0.029 (0.023)\tLoss 1.1108 (1.1233)\tPrec@1 58.887 (60.228)\n",
      "Epoch: [121][45/48]\tTime 0.014 (0.023)\tLoss 1.1172 (1.1276)\tPrec@1 58.789 (59.974)\n",
      "Epoch: [121][48/48]\tTime 0.016 (0.023)\tLoss 1.1117 (1.1284)\tPrec@1 59.788 (59.944)\n",
      "EPOCH: 121 train Results: Prec@1 59.944 Loss: 1.1284\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1445 (1.1445)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.1782 (1.1727)\tPrec@1 55.485 (57.630)\n",
      "EPOCH: 121 val Results: Prec@1 57.630 Loss: 1.1727\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/48]\tTime 0.021 (0.021)\tLoss 1.1277 (1.1277)\tPrec@1 59.277 (59.277)\n",
      "Epoch: [122][9/48]\tTime 0.017 (0.025)\tLoss 1.0863 (1.0965)\tPrec@1 60.840 (61.348)\n",
      "Epoch: [122][18/48]\tTime 0.037 (0.026)\tLoss 1.1075 (1.1073)\tPrec@1 60.449 (60.927)\n",
      "Epoch: [122][27/48]\tTime 0.024 (0.030)\tLoss 1.1376 (1.1145)\tPrec@1 60.840 (60.547)\n",
      "Epoch: [122][36/48]\tTime 0.025 (0.028)\tLoss 1.1476 (1.1219)\tPrec@1 59.766 (60.320)\n",
      "Epoch: [122][45/48]\tTime 0.027 (0.027)\tLoss 1.1242 (1.1297)\tPrec@1 60.645 (60.127)\n",
      "Epoch: [122][48/48]\tTime 0.020 (0.026)\tLoss 1.1664 (1.1313)\tPrec@1 59.316 (60.036)\n",
      "EPOCH: 122 train Results: Prec@1 60.036 Loss: 1.1313\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1525 (1.1525)\tPrec@1 57.520 (57.520)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.1772 (1.1742)\tPrec@1 58.036 (58.260)\n",
      "EPOCH: 122 val Results: Prec@1 58.260 Loss: 1.1742\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/48]\tTime 0.020 (0.020)\tLoss 1.0627 (1.0627)\tPrec@1 61.133 (61.133)\n",
      "Epoch: [123][9/48]\tTime 0.033 (0.023)\tLoss 1.0826 (1.0981)\tPrec@1 62.207 (60.908)\n",
      "Epoch: [123][18/48]\tTime 0.018 (0.022)\tLoss 1.2057 (1.1118)\tPrec@1 57.812 (60.639)\n",
      "Epoch: [123][27/48]\tTime 0.015 (0.022)\tLoss 1.1269 (1.1160)\tPrec@1 62.012 (60.617)\n",
      "Epoch: [123][36/48]\tTime 0.019 (0.021)\tLoss 1.1079 (1.1231)\tPrec@1 59.863 (60.301)\n",
      "Epoch: [123][45/48]\tTime 0.014 (0.021)\tLoss 1.1172 (1.1277)\tPrec@1 60.059 (60.025)\n",
      "Epoch: [123][48/48]\tTime 0.025 (0.021)\tLoss 1.1246 (1.1301)\tPrec@1 60.613 (59.996)\n",
      "EPOCH: 123 train Results: Prec@1 59.996 Loss: 1.1301\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1410 (1.1410)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1754 (1.1679)\tPrec@1 57.526 (58.550)\n",
      "EPOCH: 123 val Results: Prec@1 58.550 Loss: 1.1679\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/48]\tTime 0.023 (0.023)\tLoss 1.1064 (1.1064)\tPrec@1 59.082 (59.082)\n",
      "Epoch: [124][9/48]\tTime 0.026 (0.028)\tLoss 1.1042 (1.0978)\tPrec@1 58.984 (60.859)\n",
      "Epoch: [124][18/48]\tTime 0.034 (0.026)\tLoss 1.0662 (1.1105)\tPrec@1 62.305 (60.403)\n",
      "Epoch: [124][27/48]\tTime 0.049 (0.028)\tLoss 1.0909 (1.1165)\tPrec@1 62.109 (60.247)\n",
      "Epoch: [124][36/48]\tTime 0.029 (0.035)\tLoss 1.1351 (1.1167)\tPrec@1 59.668 (60.214)\n",
      "Epoch: [124][45/48]\tTime 0.041 (0.033)\tLoss 1.1408 (1.1229)\tPrec@1 60.254 (60.084)\n",
      "Epoch: [124][48/48]\tTime 0.026 (0.033)\tLoss 1.1533 (1.1237)\tPrec@1 59.788 (60.068)\n",
      "EPOCH: 124 train Results: Prec@1 60.068 Loss: 1.1237\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1381 (1.1381)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.1667 (1.1694)\tPrec@1 58.036 (58.170)\n",
      "EPOCH: 124 val Results: Prec@1 58.170 Loss: 1.1694\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/48]\tTime 0.029 (0.029)\tLoss 1.1308 (1.1308)\tPrec@1 58.984 (58.984)\n",
      "Epoch: [125][9/48]\tTime 0.022 (0.051)\tLoss 1.0658 (1.0984)\tPrec@1 62.109 (61.094)\n",
      "Epoch: [125][18/48]\tTime 0.018 (0.037)\tLoss 1.1390 (1.1078)\tPrec@1 60.156 (60.809)\n",
      "Epoch: [125][27/48]\tTime 0.036 (0.040)\tLoss 1.1016 (1.1140)\tPrec@1 63.086 (60.784)\n",
      "Epoch: [125][36/48]\tTime 0.028 (0.036)\tLoss 1.1362 (1.1169)\tPrec@1 58.594 (60.634)\n",
      "Epoch: [125][45/48]\tTime 0.021 (0.035)\tLoss 1.1372 (1.1229)\tPrec@1 60.449 (60.475)\n",
      "Epoch: [125][48/48]\tTime 0.022 (0.034)\tLoss 1.1176 (1.1232)\tPrec@1 59.788 (60.454)\n",
      "EPOCH: 125 train Results: Prec@1 60.454 Loss: 1.1232\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1427 (1.1427)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.005 (0.005)\tLoss 1.1737 (1.1719)\tPrec@1 56.633 (57.900)\n",
      "EPOCH: 125 val Results: Prec@1 57.900 Loss: 1.1719\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/48]\tTime 0.019 (0.019)\tLoss 1.1075 (1.1075)\tPrec@1 61.133 (61.133)\n",
      "Epoch: [126][9/48]\tTime 0.018 (0.019)\tLoss 1.1112 (1.0938)\tPrec@1 61.426 (61.084)\n",
      "Epoch: [126][18/48]\tTime 0.016 (0.020)\tLoss 1.0590 (1.1027)\tPrec@1 63.770 (60.943)\n",
      "Epoch: [126][27/48]\tTime 0.020 (0.020)\tLoss 1.0941 (1.1079)\tPrec@1 60.742 (60.795)\n",
      "Epoch: [126][36/48]\tTime 0.013 (0.020)\tLoss 1.0787 (1.1143)\tPrec@1 60.742 (60.510)\n",
      "Epoch: [126][45/48]\tTime 0.019 (0.020)\tLoss 1.1805 (1.1224)\tPrec@1 58.008 (60.184)\n",
      "Epoch: [126][48/48]\tTime 0.015 (0.020)\tLoss 1.0798 (1.1232)\tPrec@1 61.203 (60.126)\n",
      "EPOCH: 126 train Results: Prec@1 60.126 Loss: 1.1232\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1361 (1.1361)\tPrec@1 59.277 (59.277)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1734 (1.1677)\tPrec@1 57.270 (58.400)\n",
      "EPOCH: 126 val Results: Prec@1 58.400 Loss: 1.1677\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/48]\tTime 0.020 (0.020)\tLoss 1.0457 (1.0457)\tPrec@1 63.672 (63.672)\n",
      "Epoch: [127][9/48]\tTime 0.017 (0.020)\tLoss 1.0931 (1.0978)\tPrec@1 61.719 (61.338)\n",
      "Epoch: [127][18/48]\tTime 0.018 (0.020)\tLoss 1.1223 (1.0991)\tPrec@1 59.961 (61.230)\n",
      "Epoch: [127][27/48]\tTime 0.018 (0.020)\tLoss 1.1355 (1.1031)\tPrec@1 58.105 (60.965)\n",
      "Epoch: [127][36/48]\tTime 0.014 (0.020)\tLoss 1.1673 (1.1168)\tPrec@1 57.031 (60.323)\n",
      "Epoch: [127][45/48]\tTime 0.021 (0.019)\tLoss 1.1406 (1.1169)\tPrec@1 59.277 (60.305)\n",
      "Epoch: [127][48/48]\tTime 0.015 (0.019)\tLoss 1.1337 (1.1194)\tPrec@1 59.670 (60.234)\n",
      "EPOCH: 127 train Results: Prec@1 60.234 Loss: 1.1194\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.1505 (1.1505)\tPrec@1 59.277 (59.277)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.1850 (1.1706)\tPrec@1 56.633 (58.200)\n",
      "EPOCH: 127 val Results: Prec@1 58.200 Loss: 1.1706\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/48]\tTime 0.019 (0.019)\tLoss 1.1322 (1.1322)\tPrec@1 59.766 (59.766)\n",
      "Epoch: [128][9/48]\tTime 0.016 (0.019)\tLoss 1.0980 (1.1017)\tPrec@1 62.207 (61.191)\n",
      "Epoch: [128][18/48]\tTime 0.016 (0.018)\tLoss 1.1768 (1.1074)\tPrec@1 59.180 (60.902)\n",
      "Epoch: [128][27/48]\tTime 0.023 (0.019)\tLoss 1.0963 (1.1123)\tPrec@1 61.914 (60.658)\n",
      "Epoch: [128][36/48]\tTime 0.018 (0.019)\tLoss 1.1598 (1.1182)\tPrec@1 59.277 (60.370)\n",
      "Epoch: [128][45/48]\tTime 0.024 (0.020)\tLoss 1.1304 (1.1216)\tPrec@1 58.691 (60.216)\n",
      "Epoch: [128][48/48]\tTime 0.014 (0.020)\tLoss 1.1527 (1.1231)\tPrec@1 62.854 (60.264)\n",
      "EPOCH: 128 train Results: Prec@1 60.264 Loss: 1.1231\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1278 (1.1278)\tPrec@1 59.082 (59.082)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.1749 (1.1685)\tPrec@1 57.015 (57.960)\n",
      "EPOCH: 128 val Results: Prec@1 57.960 Loss: 1.1685\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/48]\tTime 0.017 (0.017)\tLoss 1.0820 (1.0820)\tPrec@1 61.426 (61.426)\n",
      "Epoch: [129][9/48]\tTime 0.018 (0.021)\tLoss 1.1037 (1.0852)\tPrec@1 61.719 (61.699)\n",
      "Epoch: [129][18/48]\tTime 0.018 (0.020)\tLoss 1.1224 (1.0961)\tPrec@1 59.961 (61.369)\n",
      "Epoch: [129][27/48]\tTime 0.023 (0.022)\tLoss 1.1267 (1.1057)\tPrec@1 61.328 (61.101)\n",
      "Epoch: [129][36/48]\tTime 0.022 (0.021)\tLoss 1.1578 (1.1135)\tPrec@1 56.348 (60.576)\n",
      "Epoch: [129][45/48]\tTime 0.021 (0.021)\tLoss 1.1665 (1.1235)\tPrec@1 57.520 (60.192)\n",
      "Epoch: [129][48/48]\tTime 0.018 (0.021)\tLoss 1.1599 (1.1253)\tPrec@1 60.731 (60.142)\n",
      "EPOCH: 129 train Results: Prec@1 60.142 Loss: 1.1253\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1303 (1.1303)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.002 (0.004)\tLoss 1.1757 (1.1662)\tPrec@1 57.653 (58.510)\n",
      "EPOCH: 129 val Results: Prec@1 58.510 Loss: 1.1662\n",
      "Best Prec@1: 58.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/48]\tTime 0.024 (0.024)\tLoss 1.1378 (1.1378)\tPrec@1 61.426 (61.426)\n",
      "Epoch: [130][9/48]\tTime 0.018 (0.021)\tLoss 1.1069 (1.0912)\tPrec@1 61.328 (61.240)\n",
      "Epoch: [130][18/48]\tTime 0.018 (0.019)\tLoss 1.1445 (1.0916)\tPrec@1 61.035 (61.066)\n",
      "Epoch: [130][27/48]\tTime 0.019 (0.024)\tLoss 1.0986 (1.1034)\tPrec@1 60.645 (60.788)\n",
      "Epoch: [130][36/48]\tTime 0.014 (0.023)\tLoss 1.1187 (1.1123)\tPrec@1 59.961 (60.423)\n",
      "Epoch: [130][45/48]\tTime 0.016 (0.022)\tLoss 1.1323 (1.1172)\tPrec@1 58.594 (60.286)\n",
      "Epoch: [130][48/48]\tTime 0.016 (0.022)\tLoss 1.1610 (1.1201)\tPrec@1 58.019 (60.168)\n",
      "EPOCH: 130 train Results: Prec@1 60.168 Loss: 1.1201\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1471 (1.1471)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.004 (0.004)\tLoss 1.1765 (1.1663)\tPrec@1 56.760 (58.640)\n",
      "EPOCH: 130 val Results: Prec@1 58.640 Loss: 1.1663\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/48]\tTime 0.017 (0.017)\tLoss 1.0708 (1.0708)\tPrec@1 61.621 (61.621)\n",
      "Epoch: [131][9/48]\tTime 0.021 (0.019)\tLoss 1.0643 (1.0979)\tPrec@1 60.645 (61.084)\n",
      "Epoch: [131][18/48]\tTime 0.017 (0.018)\tLoss 1.1146 (1.1057)\tPrec@1 60.449 (60.768)\n",
      "Epoch: [131][27/48]\tTime 0.016 (0.019)\tLoss 1.1417 (1.1106)\tPrec@1 58.887 (60.379)\n",
      "Epoch: [131][36/48]\tTime 0.026 (0.020)\tLoss 1.0721 (1.1145)\tPrec@1 61.426 (60.389)\n",
      "Epoch: [131][45/48]\tTime 0.012 (0.020)\tLoss 1.1044 (1.1207)\tPrec@1 62.109 (60.267)\n",
      "Epoch: [131][48/48]\tTime 0.023 (0.020)\tLoss 1.1539 (1.1229)\tPrec@1 60.613 (60.198)\n",
      "EPOCH: 131 train Results: Prec@1 60.198 Loss: 1.1229\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1285 (1.1285)\tPrec@1 61.328 (61.328)\n",
      "Test: [9/9]\tTime 0.002 (0.004)\tLoss 1.1759 (1.1668)\tPrec@1 57.526 (58.350)\n",
      "EPOCH: 131 val Results: Prec@1 58.350 Loss: 1.1668\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/48]\tTime 0.021 (0.021)\tLoss 1.1601 (1.1601)\tPrec@1 58.398 (58.398)\n",
      "Epoch: [132][9/48]\tTime 0.015 (0.020)\tLoss 1.1103 (1.0964)\tPrec@1 60.645 (61.406)\n",
      "Epoch: [132][18/48]\tTime 0.018 (0.020)\tLoss 1.1369 (1.1078)\tPrec@1 60.352 (60.814)\n",
      "Epoch: [132][27/48]\tTime 0.018 (0.020)\tLoss 1.1620 (1.1142)\tPrec@1 58.984 (60.477)\n",
      "Epoch: [132][36/48]\tTime 0.018 (0.020)\tLoss 1.1285 (1.1138)\tPrec@1 58.203 (60.457)\n",
      "Epoch: [132][45/48]\tTime 0.017 (0.021)\tLoss 1.1072 (1.1166)\tPrec@1 61.719 (60.339)\n",
      "Epoch: [132][48/48]\tTime 0.022 (0.022)\tLoss 1.0749 (1.1163)\tPrec@1 61.792 (60.330)\n",
      "EPOCH: 132 train Results: Prec@1 60.330 Loss: 1.1163\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1426 (1.1426)\tPrec@1 60.156 (60.156)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.1656 (1.1647)\tPrec@1 57.015 (58.510)\n",
      "EPOCH: 132 val Results: Prec@1 58.510 Loss: 1.1647\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/48]\tTime 0.030 (0.030)\tLoss 1.0465 (1.0465)\tPrec@1 64.551 (64.551)\n",
      "Epoch: [133][9/48]\tTime 0.019 (0.033)\tLoss 1.1209 (1.0968)\tPrec@1 59.570 (61.562)\n",
      "Epoch: [133][18/48]\tTime 0.067 (0.034)\tLoss 1.0780 (1.0900)\tPrec@1 62.109 (61.518)\n",
      "Epoch: [133][27/48]\tTime 0.021 (0.030)\tLoss 1.0943 (1.0966)\tPrec@1 60.840 (61.283)\n",
      "Epoch: [133][36/48]\tTime 0.025 (0.029)\tLoss 1.1318 (1.1054)\tPrec@1 61.426 (61.101)\n",
      "Epoch: [133][45/48]\tTime 0.020 (0.030)\tLoss 1.1511 (1.1143)\tPrec@1 58.301 (60.793)\n",
      "Epoch: [133][48/48]\tTime 0.018 (0.030)\tLoss 1.0643 (1.1146)\tPrec@1 62.500 (60.824)\n",
      "EPOCH: 133 train Results: Prec@1 60.824 Loss: 1.1146\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1419 (1.1419)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.008 (0.006)\tLoss 1.1662 (1.1641)\tPrec@1 55.995 (58.060)\n",
      "EPOCH: 133 val Results: Prec@1 58.060 Loss: 1.1641\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/48]\tTime 0.043 (0.043)\tLoss 1.1403 (1.1403)\tPrec@1 59.863 (59.863)\n",
      "Epoch: [134][9/48]\tTime 0.025 (0.025)\tLoss 1.1208 (1.1026)\tPrec@1 60.449 (61.113)\n",
      "Epoch: [134][18/48]\tTime 0.018 (0.023)\tLoss 1.0998 (1.1054)\tPrec@1 60.254 (60.912)\n",
      "Epoch: [134][27/48]\tTime 0.023 (0.022)\tLoss 1.0947 (1.1046)\tPrec@1 60.547 (60.965)\n",
      "Epoch: [134][36/48]\tTime 0.017 (0.021)\tLoss 1.1521 (1.1076)\tPrec@1 58.398 (60.779)\n",
      "Epoch: [134][45/48]\tTime 0.052 (0.024)\tLoss 1.1161 (1.1126)\tPrec@1 60.059 (60.462)\n",
      "Epoch: [134][48/48]\tTime 0.013 (0.024)\tLoss 1.1561 (1.1138)\tPrec@1 59.080 (60.438)\n",
      "EPOCH: 134 train Results: Prec@1 60.438 Loss: 1.1138\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1302 (1.1302)\tPrec@1 59.961 (59.961)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1740 (1.1663)\tPrec@1 56.505 (58.230)\n",
      "EPOCH: 134 val Results: Prec@1 58.230 Loss: 1.1663\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/48]\tTime 0.016 (0.016)\tLoss 1.0845 (1.0845)\tPrec@1 61.914 (61.914)\n",
      "Epoch: [135][9/48]\tTime 0.026 (0.022)\tLoss 1.0768 (1.0860)\tPrec@1 62.988 (61.504)\n",
      "Epoch: [135][18/48]\tTime 0.019 (0.021)\tLoss 1.0894 (1.0953)\tPrec@1 62.207 (61.266)\n",
      "Epoch: [135][27/48]\tTime 0.016 (0.020)\tLoss 1.1095 (1.0974)\tPrec@1 61.426 (61.192)\n",
      "Epoch: [135][36/48]\tTime 0.017 (0.020)\tLoss 1.1127 (1.1064)\tPrec@1 61.523 (60.935)\n",
      "Epoch: [135][45/48]\tTime 0.020 (0.020)\tLoss 1.1901 (1.1128)\tPrec@1 57.910 (60.685)\n",
      "Epoch: [135][48/48]\tTime 0.014 (0.020)\tLoss 1.1622 (1.1135)\tPrec@1 58.373 (60.638)\n",
      "EPOCH: 135 train Results: Prec@1 60.638 Loss: 1.1135\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1529 (1.1529)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1752 (1.1687)\tPrec@1 55.485 (58.170)\n",
      "EPOCH: 135 val Results: Prec@1 58.170 Loss: 1.1687\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/48]\tTime 0.021 (0.021)\tLoss 1.0857 (1.0857)\tPrec@1 61.328 (61.328)\n",
      "Epoch: [136][9/48]\tTime 0.014 (0.018)\tLoss 1.1005 (1.0933)\tPrec@1 60.059 (60.928)\n",
      "Epoch: [136][18/48]\tTime 0.018 (0.018)\tLoss 1.0953 (1.0949)\tPrec@1 62.012 (61.236)\n",
      "Epoch: [136][27/48]\tTime 0.021 (0.019)\tLoss 1.1468 (1.1088)\tPrec@1 57.910 (60.613)\n",
      "Epoch: [136][36/48]\tTime 0.016 (0.019)\tLoss 1.1257 (1.1117)\tPrec@1 60.156 (60.610)\n",
      "Epoch: [136][45/48]\tTime 0.022 (0.019)\tLoss 1.1584 (1.1148)\tPrec@1 60.059 (60.509)\n",
      "Epoch: [136][48/48]\tTime 0.017 (0.019)\tLoss 1.1577 (1.1172)\tPrec@1 59.906 (60.496)\n",
      "EPOCH: 136 train Results: Prec@1 60.496 Loss: 1.1172\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1588 (1.1588)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.1736 (1.1721)\tPrec@1 56.633 (57.970)\n",
      "EPOCH: 136 val Results: Prec@1 57.970 Loss: 1.1721\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/48]\tTime 0.021 (0.021)\tLoss 1.0587 (1.0587)\tPrec@1 62.207 (62.207)\n",
      "Epoch: [137][9/48]\tTime 0.018 (0.033)\tLoss 1.0708 (1.1046)\tPrec@1 62.402 (60.391)\n",
      "Epoch: [137][18/48]\tTime 0.027 (0.030)\tLoss 1.1214 (1.1015)\tPrec@1 60.156 (61.061)\n",
      "Epoch: [137][27/48]\tTime 0.051 (0.036)\tLoss 1.1127 (1.1036)\tPrec@1 60.938 (60.777)\n",
      "Epoch: [137][36/48]\tTime 0.082 (0.046)\tLoss 1.1574 (1.1086)\tPrec@1 59.277 (60.621)\n",
      "Epoch: [137][45/48]\tTime 0.219 (0.068)\tLoss 1.0969 (1.1140)\tPrec@1 60.938 (60.430)\n",
      "Epoch: [137][48/48]\tTime 0.086 (0.074)\tLoss 1.1578 (1.1156)\tPrec@1 57.901 (60.416)\n",
      "EPOCH: 137 train Results: Prec@1 60.416 Loss: 1.1156\n",
      "Test: [0/9]\tTime 0.037 (0.037)\tLoss 1.1443 (1.1443)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.026 (0.044)\tLoss 1.1639 (1.1704)\tPrec@1 58.036 (58.190)\n",
      "EPOCH: 137 val Results: Prec@1 58.190 Loss: 1.1704\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/48]\tTime 0.078 (0.078)\tLoss 1.1017 (1.1017)\tPrec@1 62.402 (62.402)\n",
      "Epoch: [138][9/48]\tTime 0.102 (0.101)\tLoss 1.1061 (1.0922)\tPrec@1 60.254 (62.021)\n",
      "Epoch: [138][18/48]\tTime 0.098 (0.097)\tLoss 1.0734 (1.1005)\tPrec@1 64.258 (61.688)\n",
      "Epoch: [138][27/48]\tTime 0.062 (0.094)\tLoss 1.0711 (1.0988)\tPrec@1 60.547 (61.516)\n",
      "Epoch: [138][36/48]\tTime 0.092 (0.096)\tLoss 1.0942 (1.1043)\tPrec@1 62.207 (61.164)\n",
      "Epoch: [138][45/48]\tTime 0.091 (0.103)\tLoss 1.1564 (1.1073)\tPrec@1 58.887 (61.046)\n",
      "Epoch: [138][48/48]\tTime 0.094 (0.104)\tLoss 1.2124 (1.1107)\tPrec@1 55.896 (60.862)\n",
      "EPOCH: 138 train Results: Prec@1 60.862 Loss: 1.1107\n",
      "Test: [0/9]\tTime 0.028 (0.028)\tLoss 1.1394 (1.1394)\tPrec@1 60.449 (60.449)\n",
      "Test: [9/9]\tTime 0.027 (0.032)\tLoss 1.1667 (1.1673)\tPrec@1 56.633 (58.470)\n",
      "EPOCH: 138 val Results: Prec@1 58.470 Loss: 1.1673\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/48]\tTime 0.280 (0.280)\tLoss 1.0576 (1.0576)\tPrec@1 61.621 (61.621)\n",
      "Epoch: [139][9/48]\tTime 0.132 (0.099)\tLoss 1.0815 (1.0852)\tPrec@1 61.133 (60.918)\n",
      "Epoch: [139][18/48]\tTime 0.034 (0.075)\tLoss 1.1184 (1.0919)\tPrec@1 60.449 (60.922)\n",
      "Epoch: [139][27/48]\tTime 0.028 (0.064)\tLoss 1.0859 (1.1000)\tPrec@1 62.012 (60.658)\n",
      "Epoch: [139][36/48]\tTime 0.039 (0.057)\tLoss 1.1048 (1.1061)\tPrec@1 60.156 (60.520)\n",
      "Epoch: [139][45/48]\tTime 0.033 (0.052)\tLoss 1.1766 (1.1116)\tPrec@1 60.352 (60.286)\n",
      "Epoch: [139][48/48]\tTime 0.022 (0.051)\tLoss 1.1761 (1.1137)\tPrec@1 57.429 (60.202)\n",
      "EPOCH: 139 train Results: Prec@1 60.202 Loss: 1.1137\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1480 (1.1480)\tPrec@1 59.766 (59.766)\n",
      "Test: [9/9]\tTime 0.010 (0.008)\tLoss 1.1685 (1.1670)\tPrec@1 57.015 (58.390)\n",
      "EPOCH: 139 val Results: Prec@1 58.390 Loss: 1.1670\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/48]\tTime 0.025 (0.025)\tLoss 1.0493 (1.0493)\tPrec@1 65.039 (65.039)\n",
      "Epoch: [140][9/48]\tTime 0.034 (0.030)\tLoss 1.0536 (1.0829)\tPrec@1 66.309 (62.412)\n",
      "Epoch: [140][18/48]\tTime 0.024 (0.027)\tLoss 1.0778 (1.0923)\tPrec@1 60.547 (61.436)\n",
      "Epoch: [140][27/48]\tTime 0.022 (0.027)\tLoss 1.1066 (1.0956)\tPrec@1 60.547 (61.283)\n",
      "Epoch: [140][36/48]\tTime 0.030 (0.027)\tLoss 1.1380 (1.1024)\tPrec@1 61.230 (61.019)\n",
      "Epoch: [140][45/48]\tTime 0.041 (0.029)\tLoss 1.0986 (1.1072)\tPrec@1 60.254 (60.842)\n",
      "Epoch: [140][48/48]\tTime 0.017 (0.029)\tLoss 1.1567 (1.1112)\tPrec@1 56.958 (60.584)\n",
      "EPOCH: 140 train Results: Prec@1 60.584 Loss: 1.1112\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1469 (1.1469)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.023 (0.006)\tLoss 1.1679 (1.1667)\tPrec@1 55.867 (58.370)\n",
      "EPOCH: 140 val Results: Prec@1 58.370 Loss: 1.1667\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/48]\tTime 0.048 (0.048)\tLoss 1.0351 (1.0351)\tPrec@1 63.379 (63.379)\n",
      "Epoch: [141][9/48]\tTime 0.023 (0.031)\tLoss 1.1080 (1.0675)\tPrec@1 61.914 (62.305)\n",
      "Epoch: [141][18/48]\tTime 0.019 (0.027)\tLoss 1.1074 (1.0865)\tPrec@1 60.059 (61.421)\n",
      "Epoch: [141][27/48]\tTime 0.036 (0.027)\tLoss 1.1328 (1.0923)\tPrec@1 61.133 (61.426)\n",
      "Epoch: [141][36/48]\tTime 0.037 (0.025)\tLoss 1.0916 (1.0996)\tPrec@1 61.816 (60.998)\n",
      "Epoch: [141][45/48]\tTime 0.021 (0.025)\tLoss 1.1529 (1.1063)\tPrec@1 58.691 (60.751)\n",
      "Epoch: [141][48/48]\tTime 0.013 (0.025)\tLoss 1.0991 (1.1065)\tPrec@1 60.613 (60.748)\n",
      "EPOCH: 141 train Results: Prec@1 60.748 Loss: 1.1065\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1353 (1.1353)\tPrec@1 60.645 (60.645)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.1597 (1.1673)\tPrec@1 57.908 (58.410)\n",
      "EPOCH: 141 val Results: Prec@1 58.410 Loss: 1.1673\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/48]\tTime 0.032 (0.032)\tLoss 1.0442 (1.0442)\tPrec@1 63.770 (63.770)\n",
      "Epoch: [142][9/48]\tTime 0.025 (0.023)\tLoss 1.1300 (1.0894)\tPrec@1 61.230 (61.982)\n",
      "Epoch: [142][18/48]\tTime 0.021 (0.022)\tLoss 1.0632 (1.0986)\tPrec@1 60.742 (61.354)\n",
      "Epoch: [142][27/48]\tTime 0.015 (0.021)\tLoss 1.1206 (1.1005)\tPrec@1 59.863 (61.196)\n",
      "Epoch: [142][36/48]\tTime 0.062 (0.061)\tLoss 1.0880 (1.1053)\tPrec@1 60.449 (60.969)\n",
      "Epoch: [142][45/48]\tTime 0.071 (0.064)\tLoss 1.1444 (1.1095)\tPrec@1 57.812 (60.674)\n",
      "Epoch: [142][48/48]\tTime 0.155 (0.069)\tLoss 1.1632 (1.1132)\tPrec@1 57.547 (60.494)\n",
      "EPOCH: 142 train Results: Prec@1 60.494 Loss: 1.1132\n",
      "Test: [0/9]\tTime 0.024 (0.024)\tLoss 1.1432 (1.1432)\tPrec@1 59.961 (59.961)\n",
      "Test: [9/9]\tTime 0.022 (0.022)\tLoss 1.1673 (1.1691)\tPrec@1 57.398 (58.420)\n",
      "EPOCH: 142 val Results: Prec@1 58.420 Loss: 1.1691\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/48]\tTime 0.093 (0.093)\tLoss 1.0232 (1.0232)\tPrec@1 64.355 (64.355)\n",
      "Epoch: [143][9/48]\tTime 0.093 (0.081)\tLoss 1.1231 (1.0919)\tPrec@1 61.426 (61.963)\n",
      "Epoch: [143][18/48]\tTime 0.070 (0.098)\tLoss 1.0979 (1.0917)\tPrec@1 62.891 (61.708)\n",
      "Epoch: [143][27/48]\tTime 0.114 (0.095)\tLoss 1.0984 (1.0951)\tPrec@1 59.863 (61.391)\n",
      "Epoch: [143][36/48]\tTime 0.093 (0.094)\tLoss 1.1351 (1.1026)\tPrec@1 60.254 (61.088)\n",
      "Epoch: [143][45/48]\tTime 0.094 (0.092)\tLoss 1.1312 (1.1092)\tPrec@1 59.277 (60.768)\n",
      "Epoch: [143][48/48]\tTime 0.146 (0.095)\tLoss 1.1451 (1.1099)\tPrec@1 59.670 (60.748)\n",
      "EPOCH: 143 train Results: Prec@1 60.748 Loss: 1.1099\n",
      "Test: [0/9]\tTime 0.027 (0.027)\tLoss 1.1453 (1.1453)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.027 (0.036)\tLoss 1.1725 (1.1700)\tPrec@1 56.250 (58.300)\n",
      "EPOCH: 143 val Results: Prec@1 58.300 Loss: 1.1700\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/48]\tTime 0.063 (0.063)\tLoss 1.1172 (1.1172)\tPrec@1 62.305 (62.305)\n",
      "Epoch: [144][9/48]\tTime 0.084 (0.092)\tLoss 1.0936 (1.0976)\tPrec@1 62.109 (61.045)\n",
      "Epoch: [144][18/48]\tTime 0.023 (0.086)\tLoss 1.1063 (1.1005)\tPrec@1 59.570 (61.009)\n",
      "Epoch: [144][27/48]\tTime 0.017 (0.068)\tLoss 1.0981 (1.1074)\tPrec@1 60.645 (60.983)\n",
      "Epoch: [144][36/48]\tTime 0.018 (0.057)\tLoss 1.1494 (1.1067)\tPrec@1 60.840 (60.980)\n",
      "Epoch: [144][45/48]\tTime 0.030 (0.052)\tLoss 1.0583 (1.1089)\tPrec@1 62.988 (60.927)\n",
      "Epoch: [144][48/48]\tTime 0.019 (0.050)\tLoss 1.1140 (1.1101)\tPrec@1 59.670 (60.826)\n",
      "EPOCH: 144 train Results: Prec@1 60.826 Loss: 1.1101\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.1511 (1.1511)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1579 (1.1646)\tPrec@1 56.760 (58.520)\n",
      "EPOCH: 144 val Results: Prec@1 58.520 Loss: 1.1646\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/48]\tTime 0.025 (0.025)\tLoss 1.1241 (1.1241)\tPrec@1 60.352 (60.352)\n",
      "Epoch: [145][9/48]\tTime 0.024 (0.024)\tLoss 1.1263 (1.0998)\tPrec@1 58.984 (60.352)\n",
      "Epoch: [145][18/48]\tTime 0.021 (0.023)\tLoss 1.0303 (1.0913)\tPrec@1 63.184 (61.051)\n",
      "Epoch: [145][27/48]\tTime 0.019 (0.023)\tLoss 1.1434 (1.1041)\tPrec@1 58.984 (60.641)\n",
      "Epoch: [145][36/48]\tTime 0.049 (0.024)\tLoss 1.0788 (1.1080)\tPrec@1 62.500 (60.600)\n",
      "Epoch: [145][45/48]\tTime 0.029 (0.028)\tLoss 1.1301 (1.1115)\tPrec@1 60.156 (60.585)\n",
      "Epoch: [145][48/48]\tTime 0.023 (0.028)\tLoss 1.1801 (1.1144)\tPrec@1 57.429 (60.462)\n",
      "EPOCH: 145 train Results: Prec@1 60.462 Loss: 1.1144\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1564 (1.1564)\tPrec@1 58.594 (58.594)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1697 (1.1713)\tPrec@1 56.633 (58.090)\n",
      "EPOCH: 145 val Results: Prec@1 58.090 Loss: 1.1713\n",
      "Best Prec@1: 58.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/48]\tTime 0.025 (0.025)\tLoss 1.0560 (1.0560)\tPrec@1 62.695 (62.695)\n",
      "Epoch: [146][9/48]\tTime 0.028 (0.021)\tLoss 1.1008 (1.0820)\tPrec@1 61.719 (61.953)\n",
      "Epoch: [146][18/48]\tTime 0.020 (0.023)\tLoss 1.0759 (1.0894)\tPrec@1 62.500 (61.765)\n",
      "Epoch: [146][27/48]\tTime 0.018 (0.025)\tLoss 1.1060 (1.0912)\tPrec@1 60.059 (61.482)\n",
      "Epoch: [146][36/48]\tTime 0.023 (0.024)\tLoss 1.1569 (1.0992)\tPrec@1 57.520 (61.159)\n",
      "Epoch: [146][45/48]\tTime 0.023 (0.026)\tLoss 1.1554 (1.1048)\tPrec@1 58.691 (60.918)\n",
      "Epoch: [146][48/48]\tTime 0.018 (0.025)\tLoss 1.1578 (1.1055)\tPrec@1 58.844 (60.920)\n",
      "EPOCH: 146 train Results: Prec@1 60.920 Loss: 1.1055\n",
      "Test: [0/9]\tTime 0.003 (0.003)\tLoss 1.1453 (1.1453)\tPrec@1 59.277 (59.277)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1686 (1.1618)\tPrec@1 57.653 (58.750)\n",
      "EPOCH: 146 val Results: Prec@1 58.750 Loss: 1.1618\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/48]\tTime 0.025 (0.025)\tLoss 1.0880 (1.0880)\tPrec@1 61.328 (61.328)\n",
      "Epoch: [147][9/48]\tTime 0.023 (0.032)\tLoss 1.1197 (1.0866)\tPrec@1 59.863 (61.562)\n",
      "Epoch: [147][18/48]\tTime 0.017 (0.026)\tLoss 1.1523 (1.0935)\tPrec@1 59.375 (61.426)\n",
      "Epoch: [147][27/48]\tTime 0.097 (0.036)\tLoss 1.1779 (1.0959)\tPrec@1 58.203 (61.419)\n",
      "Epoch: [147][36/48]\tTime 0.143 (0.036)\tLoss 1.1184 (1.1032)\tPrec@1 61.328 (61.183)\n",
      "Epoch: [147][45/48]\tTime 0.034 (0.035)\tLoss 1.1708 (1.1061)\tPrec@1 60.449 (60.980)\n",
      "Epoch: [147][48/48]\tTime 0.023 (0.034)\tLoss 1.1826 (1.1096)\tPrec@1 58.373 (60.866)\n",
      "EPOCH: 147 train Results: Prec@1 60.866 Loss: 1.1096\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1469 (1.1469)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.1803 (1.1676)\tPrec@1 56.378 (58.130)\n",
      "EPOCH: 147 val Results: Prec@1 58.130 Loss: 1.1676\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/48]\tTime 0.022 (0.022)\tLoss 1.0512 (1.0512)\tPrec@1 61.426 (61.426)\n",
      "Epoch: [148][9/48]\tTime 0.047 (0.034)\tLoss 1.1445 (1.0848)\tPrec@1 59.277 (61.445)\n",
      "Epoch: [148][18/48]\tTime 0.025 (0.032)\tLoss 1.1417 (1.0880)\tPrec@1 62.207 (61.642)\n",
      "Epoch: [148][27/48]\tTime 0.020 (0.032)\tLoss 1.0992 (1.0963)\tPrec@1 59.180 (61.244)\n",
      "Epoch: [148][36/48]\tTime 0.033 (0.031)\tLoss 1.1666 (1.1017)\tPrec@1 58.594 (60.901)\n",
      "Epoch: [148][45/48]\tTime 0.018 (0.029)\tLoss 1.1272 (1.1072)\tPrec@1 58.887 (60.642)\n",
      "Epoch: [148][48/48]\tTime 0.070 (0.030)\tLoss 1.1024 (1.1065)\tPrec@1 62.146 (60.670)\n",
      "EPOCH: 148 train Results: Prec@1 60.670 Loss: 1.1065\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1524 (1.1524)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1748 (1.1635)\tPrec@1 57.270 (58.420)\n",
      "EPOCH: 148 val Results: Prec@1 58.420 Loss: 1.1635\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/48]\tTime 0.017 (0.017)\tLoss 1.0871 (1.0871)\tPrec@1 61.328 (61.328)\n",
      "Epoch: [149][9/48]\tTime 0.024 (0.027)\tLoss 1.0278 (1.0835)\tPrec@1 62.598 (61.836)\n",
      "Epoch: [149][18/48]\tTime 0.040 (0.027)\tLoss 1.0906 (1.0927)\tPrec@1 62.012 (61.565)\n",
      "Epoch: [149][27/48]\tTime 0.030 (0.028)\tLoss 1.0695 (1.0956)\tPrec@1 62.891 (61.401)\n",
      "Epoch: [149][36/48]\tTime 0.060 (0.029)\tLoss 1.0806 (1.0986)\tPrec@1 62.988 (61.236)\n",
      "Epoch: [149][45/48]\tTime 0.031 (0.028)\tLoss 1.1123 (1.1052)\tPrec@1 60.547 (60.914)\n",
      "Epoch: [149][48/48]\tTime 0.020 (0.028)\tLoss 1.1170 (1.1077)\tPrec@1 57.665 (60.788)\n",
      "EPOCH: 149 train Results: Prec@1 60.788 Loss: 1.1077\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1450 (1.1450)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.007 (0.007)\tLoss 1.1628 (1.1657)\tPrec@1 58.163 (58.630)\n",
      "EPOCH: 149 val Results: Prec@1 58.630 Loss: 1.1657\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/48]\tTime 0.023 (0.023)\tLoss 1.0668 (1.0668)\tPrec@1 62.012 (62.012)\n",
      "Epoch: [150][9/48]\tTime 0.027 (0.030)\tLoss 1.0719 (1.0727)\tPrec@1 62.891 (62.490)\n",
      "Epoch: [150][18/48]\tTime 0.053 (0.030)\tLoss 1.0490 (1.0781)\tPrec@1 63.086 (62.079)\n",
      "Epoch: [150][27/48]\tTime 0.140 (0.039)\tLoss 1.1214 (1.0917)\tPrec@1 60.156 (61.408)\n",
      "Epoch: [150][36/48]\tTime 0.041 (0.036)\tLoss 1.1321 (1.1010)\tPrec@1 60.254 (60.977)\n",
      "Epoch: [150][45/48]\tTime 0.026 (0.034)\tLoss 1.0726 (1.1025)\tPrec@1 61.426 (60.846)\n",
      "Epoch: [150][48/48]\tTime 0.116 (0.036)\tLoss 1.1199 (1.1059)\tPrec@1 60.495 (60.680)\n",
      "EPOCH: 150 train Results: Prec@1 60.680 Loss: 1.1059\n",
      "Test: [0/9]\tTime 0.025 (0.025)\tLoss 1.1462 (1.1462)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.011 (0.010)\tLoss 1.1711 (1.1633)\tPrec@1 56.505 (58.580)\n",
      "EPOCH: 150 val Results: Prec@1 58.580 Loss: 1.1633\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/48]\tTime 0.028 (0.028)\tLoss 1.0805 (1.0805)\tPrec@1 61.230 (61.230)\n",
      "Epoch: [151][9/48]\tTime 0.052 (0.047)\tLoss 1.0611 (1.0759)\tPrec@1 63.965 (61.992)\n",
      "Epoch: [151][18/48]\tTime 0.029 (0.041)\tLoss 1.0580 (1.0767)\tPrec@1 62.988 (62.186)\n",
      "Epoch: [151][27/48]\tTime 0.019 (0.037)\tLoss 1.1351 (1.0841)\tPrec@1 61.816 (61.907)\n",
      "Epoch: [151][36/48]\tTime 0.030 (0.035)\tLoss 1.1378 (1.0950)\tPrec@1 59.082 (61.320)\n",
      "Epoch: [151][45/48]\tTime 0.042 (0.034)\tLoss 1.1299 (1.1008)\tPrec@1 59.863 (61.046)\n",
      "Epoch: [151][48/48]\tTime 0.015 (0.034)\tLoss 1.1369 (1.1029)\tPrec@1 61.910 (60.982)\n",
      "EPOCH: 151 train Results: Prec@1 60.982 Loss: 1.1029\n",
      "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1518 (1.1518)\tPrec@1 57.910 (57.910)\n",
      "Test: [9/9]\tTime 0.004 (0.009)\tLoss 1.1484 (1.1623)\tPrec@1 57.908 (58.700)\n",
      "EPOCH: 151 val Results: Prec@1 58.700 Loss: 1.1623\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/48]\tTime 0.023 (0.023)\tLoss 1.0828 (1.0828)\tPrec@1 62.402 (62.402)\n",
      "Epoch: [152][9/48]\tTime 0.024 (0.028)\tLoss 1.0748 (1.0778)\tPrec@1 61.816 (62.344)\n",
      "Epoch: [152][18/48]\tTime 0.122 (0.032)\tLoss 1.0664 (1.0826)\tPrec@1 62.793 (62.043)\n",
      "Epoch: [152][27/48]\tTime 0.019 (0.029)\tLoss 1.1291 (1.0854)\tPrec@1 58.398 (61.952)\n",
      "Epoch: [152][36/48]\tTime 0.023 (0.029)\tLoss 1.0964 (1.0926)\tPrec@1 61.133 (61.574)\n",
      "Epoch: [152][45/48]\tTime 0.035 (0.029)\tLoss 1.1076 (1.0990)\tPrec@1 59.766 (61.303)\n",
      "Epoch: [152][48/48]\tTime 0.028 (0.029)\tLoss 1.0863 (1.0999)\tPrec@1 60.967 (61.236)\n",
      "EPOCH: 152 train Results: Prec@1 61.236 Loss: 1.0999\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1480 (1.1480)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.1634 (1.1645)\tPrec@1 56.250 (58.260)\n",
      "EPOCH: 152 val Results: Prec@1 58.260 Loss: 1.1645\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/48]\tTime 0.022 (0.022)\tLoss 1.0048 (1.0048)\tPrec@1 63.184 (63.184)\n",
      "Epoch: [153][9/48]\tTime 0.022 (0.024)\tLoss 1.0750 (1.0641)\tPrec@1 62.012 (62.256)\n",
      "Epoch: [153][18/48]\tTime 0.017 (0.024)\tLoss 1.1009 (1.0766)\tPrec@1 60.645 (62.027)\n",
      "Epoch: [153][27/48]\tTime 0.027 (0.025)\tLoss 1.0764 (1.0868)\tPrec@1 62.012 (61.572)\n",
      "Epoch: [153][36/48]\tTime 0.025 (0.027)\tLoss 1.0859 (1.0920)\tPrec@1 60.352 (61.397)\n",
      "Epoch: [153][45/48]\tTime 0.040 (0.027)\tLoss 1.1626 (1.0971)\tPrec@1 58.594 (61.137)\n",
      "Epoch: [153][48/48]\tTime 0.016 (0.026)\tLoss 1.1344 (1.1011)\tPrec@1 58.608 (61.010)\n",
      "EPOCH: 153 train Results: Prec@1 61.010 Loss: 1.1011\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1428 (1.1428)\tPrec@1 60.156 (60.156)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.1668 (1.1636)\tPrec@1 57.398 (58.660)\n",
      "EPOCH: 153 val Results: Prec@1 58.660 Loss: 1.1636\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/48]\tTime 0.034 (0.034)\tLoss 1.0593 (1.0593)\tPrec@1 62.012 (62.012)\n",
      "Epoch: [154][9/48]\tTime 0.024 (0.027)\tLoss 1.0432 (1.0784)\tPrec@1 62.793 (61.748)\n",
      "Epoch: [154][18/48]\tTime 0.029 (0.029)\tLoss 1.1488 (1.0852)\tPrec@1 58.691 (61.446)\n",
      "Epoch: [154][27/48]\tTime 0.036 (0.029)\tLoss 1.1121 (1.0856)\tPrec@1 59.082 (61.471)\n",
      "Epoch: [154][36/48]\tTime 0.027 (0.029)\tLoss 1.1503 (1.0943)\tPrec@1 59.375 (61.246)\n",
      "Epoch: [154][45/48]\tTime 0.031 (0.029)\tLoss 1.1265 (1.1003)\tPrec@1 59.082 (60.918)\n",
      "Epoch: [154][48/48]\tTime 0.020 (0.029)\tLoss 1.1594 (1.1008)\tPrec@1 58.491 (60.896)\n",
      "EPOCH: 154 train Results: Prec@1 60.896 Loss: 1.1008\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1439 (1.1439)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.011 (0.009)\tLoss 1.1583 (1.1653)\tPrec@1 57.398 (58.540)\n",
      "EPOCH: 154 val Results: Prec@1 58.540 Loss: 1.1653\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/48]\tTime 0.038 (0.038)\tLoss 1.0173 (1.0173)\tPrec@1 64.160 (64.160)\n",
      "Epoch: [155][9/48]\tTime 0.049 (0.032)\tLoss 1.0986 (1.0561)\tPrec@1 59.277 (62.744)\n",
      "Epoch: [155][18/48]\tTime 0.039 (0.038)\tLoss 1.0678 (1.0711)\tPrec@1 62.891 (62.037)\n",
      "Epoch: [155][27/48]\tTime 0.022 (0.034)\tLoss 1.0951 (1.0826)\tPrec@1 61.719 (61.659)\n",
      "Epoch: [155][36/48]\tTime 0.039 (0.037)\tLoss 1.1094 (1.0892)\tPrec@1 60.156 (61.399)\n",
      "Epoch: [155][45/48]\tTime 0.028 (0.035)\tLoss 1.0753 (1.0966)\tPrec@1 60.449 (61.135)\n",
      "Epoch: [155][48/48]\tTime 0.016 (0.034)\tLoss 1.1511 (1.0987)\tPrec@1 59.434 (61.036)\n",
      "EPOCH: 155 train Results: Prec@1 61.036 Loss: 1.0987\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1541 (1.1541)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.1698 (1.1660)\tPrec@1 57.143 (58.280)\n",
      "EPOCH: 155 val Results: Prec@1 58.280 Loss: 1.1660\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/48]\tTime 0.027 (0.027)\tLoss 1.0639 (1.0639)\tPrec@1 62.207 (62.207)\n",
      "Epoch: [156][9/48]\tTime 0.031 (0.028)\tLoss 1.0038 (1.0621)\tPrec@1 65.918 (62.588)\n",
      "Epoch: [156][18/48]\tTime 0.039 (0.038)\tLoss 1.1477 (1.0729)\tPrec@1 59.668 (62.202)\n",
      "Epoch: [156][27/48]\tTime 0.044 (0.034)\tLoss 1.0339 (1.0767)\tPrec@1 63.574 (62.074)\n",
      "Epoch: [156][36/48]\tTime 0.056 (0.035)\tLoss 1.0755 (1.0884)\tPrec@1 62.500 (61.632)\n",
      "Epoch: [156][45/48]\tTime 0.019 (0.035)\tLoss 1.1119 (1.0939)\tPrec@1 60.352 (61.526)\n",
      "Epoch: [156][48/48]\tTime 0.029 (0.035)\tLoss 1.1418 (1.0948)\tPrec@1 59.906 (61.518)\n",
      "EPOCH: 156 train Results: Prec@1 61.518 Loss: 1.0948\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1590 (1.1590)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.017 (0.009)\tLoss 1.1686 (1.1670)\tPrec@1 55.740 (58.440)\n",
      "EPOCH: 156 val Results: Prec@1 58.440 Loss: 1.1670\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/48]\tTime 0.051 (0.051)\tLoss 1.0963 (1.0963)\tPrec@1 59.863 (59.863)\n",
      "Epoch: [157][9/48]\tTime 0.034 (0.035)\tLoss 1.0857 (1.0763)\tPrec@1 59.766 (61.367)\n",
      "Epoch: [157][18/48]\tTime 0.037 (0.035)\tLoss 1.1022 (1.0812)\tPrec@1 60.742 (61.601)\n",
      "Epoch: [157][27/48]\tTime 0.039 (0.034)\tLoss 1.1793 (1.0909)\tPrec@1 58.398 (61.060)\n",
      "Epoch: [157][36/48]\tTime 0.027 (0.036)\tLoss 1.1209 (1.0951)\tPrec@1 61.133 (60.916)\n",
      "Epoch: [157][45/48]\tTime 0.018 (0.035)\tLoss 1.1392 (1.0985)\tPrec@1 59.180 (60.814)\n",
      "Epoch: [157][48/48]\tTime 0.020 (0.035)\tLoss 1.0648 (1.0999)\tPrec@1 60.613 (60.726)\n",
      "EPOCH: 157 train Results: Prec@1 60.726 Loss: 1.0999\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1501 (1.1501)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.009 (0.005)\tLoss 1.1632 (1.1683)\tPrec@1 57.270 (58.280)\n",
      "EPOCH: 157 val Results: Prec@1 58.280 Loss: 1.1683\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/48]\tTime 0.100 (0.100)\tLoss 1.0677 (1.0677)\tPrec@1 63.379 (63.379)\n",
      "Epoch: [158][9/48]\tTime 0.020 (0.039)\tLoss 1.1229 (1.0786)\tPrec@1 61.035 (61.846)\n",
      "Epoch: [158][18/48]\tTime 0.028 (0.034)\tLoss 1.1172 (1.0792)\tPrec@1 60.156 (61.724)\n",
      "Epoch: [158][27/48]\tTime 0.020 (0.040)\tLoss 1.0912 (1.0856)\tPrec@1 61.914 (61.433)\n",
      "Epoch: [158][36/48]\tTime 0.021 (0.036)\tLoss 1.1423 (1.0926)\tPrec@1 60.742 (61.273)\n",
      "Epoch: [158][45/48]\tTime 0.038 (0.035)\tLoss 1.1123 (1.0970)\tPrec@1 60.449 (61.207)\n",
      "Epoch: [158][48/48]\tTime 0.016 (0.035)\tLoss 1.1846 (1.0983)\tPrec@1 57.547 (61.168)\n",
      "EPOCH: 158 train Results: Prec@1 61.168 Loss: 1.0983\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.1399 (1.1399)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.010 (0.009)\tLoss 1.1517 (1.1615)\tPrec@1 56.633 (58.560)\n",
      "EPOCH: 158 val Results: Prec@1 58.560 Loss: 1.1615\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/48]\tTime 0.017 (0.017)\tLoss 1.0542 (1.0542)\tPrec@1 62.207 (62.207)\n",
      "Epoch: [159][9/48]\tTime 0.032 (0.033)\tLoss 1.0712 (1.0673)\tPrec@1 62.793 (62.354)\n",
      "Epoch: [159][18/48]\tTime 0.041 (0.034)\tLoss 1.0480 (1.0770)\tPrec@1 62.598 (62.017)\n",
      "Epoch: [159][27/48]\tTime 0.033 (0.034)\tLoss 1.1371 (1.0831)\tPrec@1 58.496 (61.705)\n",
      "Epoch: [159][36/48]\tTime 0.031 (0.033)\tLoss 1.1304 (1.0881)\tPrec@1 59.961 (61.576)\n",
      "Epoch: [159][45/48]\tTime 0.016 (0.032)\tLoss 1.0861 (1.0950)\tPrec@1 60.156 (61.277)\n",
      "Epoch: [159][48/48]\tTime 0.017 (0.032)\tLoss 1.1037 (1.0963)\tPrec@1 61.203 (61.236)\n",
      "EPOCH: 159 train Results: Prec@1 61.236 Loss: 1.0963\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1437 (1.1437)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.1629 (1.1636)\tPrec@1 58.418 (58.680)\n",
      "EPOCH: 159 val Results: Prec@1 58.680 Loss: 1.1636\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/48]\tTime 0.026 (0.026)\tLoss 1.0671 (1.0671)\tPrec@1 63.574 (63.574)\n",
      "Epoch: [160][9/48]\tTime 0.032 (0.036)\tLoss 1.0872 (1.0868)\tPrec@1 60.742 (61.807)\n",
      "Epoch: [160][18/48]\tTime 0.026 (0.034)\tLoss 1.0893 (1.0844)\tPrec@1 62.207 (61.580)\n",
      "Epoch: [160][27/48]\tTime 0.080 (0.037)\tLoss 1.0701 (1.0901)\tPrec@1 61.914 (61.370)\n",
      "Epoch: [160][36/48]\tTime 0.035 (0.036)\tLoss 1.1267 (1.0925)\tPrec@1 60.059 (61.386)\n",
      "Epoch: [160][45/48]\tTime 0.063 (0.039)\tLoss 1.0322 (1.0929)\tPrec@1 63.770 (61.426)\n",
      "Epoch: [160][48/48]\tTime 0.016 (0.038)\tLoss 1.0990 (1.0923)\tPrec@1 59.788 (61.372)\n",
      "EPOCH: 160 train Results: Prec@1 61.372 Loss: 1.0923\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1410 (1.1410)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.009 (0.007)\tLoss 1.1612 (1.1677)\tPrec@1 57.270 (58.110)\n",
      "EPOCH: 160 val Results: Prec@1 58.110 Loss: 1.1677\n",
      "Best Prec@1: 58.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/48]\tTime 0.042 (0.042)\tLoss 1.0381 (1.0381)\tPrec@1 63.086 (63.086)\n",
      "Epoch: [161][9/48]\tTime 0.030 (0.039)\tLoss 1.0213 (1.0525)\tPrec@1 65.820 (62.803)\n",
      "Epoch: [161][18/48]\tTime 0.044 (0.034)\tLoss 1.1079 (1.0697)\tPrec@1 61.328 (62.058)\n",
      "Epoch: [161][27/48]\tTime 0.021 (0.034)\tLoss 1.1471 (1.0810)\tPrec@1 59.473 (61.820)\n",
      "Epoch: [161][36/48]\tTime 0.026 (0.035)\tLoss 1.1612 (1.0886)\tPrec@1 58.105 (61.579)\n",
      "Epoch: [161][45/48]\tTime 0.027 (0.034)\tLoss 1.1711 (1.0958)\tPrec@1 58.203 (61.313)\n",
      "Epoch: [161][48/48]\tTime 0.021 (0.034)\tLoss 1.1258 (1.0968)\tPrec@1 58.608 (61.232)\n",
      "EPOCH: 161 train Results: Prec@1 61.232 Loss: 1.0968\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.1352 (1.1352)\tPrec@1 60.059 (60.059)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.1658 (1.1608)\tPrec@1 57.015 (59.040)\n",
      "EPOCH: 161 val Results: Prec@1 59.040 Loss: 1.1608\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/48]\tTime 0.031 (0.031)\tLoss 1.0627 (1.0627)\tPrec@1 62.207 (62.207)\n",
      "Epoch: [162][9/48]\tTime 0.026 (0.029)\tLoss 1.1252 (1.0598)\tPrec@1 59.375 (62.178)\n",
      "Epoch: [162][18/48]\tTime 0.050 (0.028)\tLoss 1.1072 (1.0794)\tPrec@1 60.938 (62.079)\n",
      "Epoch: [162][27/48]\tTime 0.039 (0.028)\tLoss 1.0583 (1.0847)\tPrec@1 62.695 (61.830)\n",
      "Epoch: [162][36/48]\tTime 0.021 (0.028)\tLoss 1.0596 (1.0872)\tPrec@1 61.523 (61.690)\n",
      "Epoch: [162][45/48]\tTime 0.019 (0.027)\tLoss 1.1134 (1.0927)\tPrec@1 61.816 (61.498)\n",
      "Epoch: [162][48/48]\tTime 0.015 (0.028)\tLoss 1.1251 (1.0937)\tPrec@1 61.085 (61.450)\n",
      "EPOCH: 162 train Results: Prec@1 61.450 Loss: 1.0937\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.1219 (1.1219)\tPrec@1 59.473 (59.473)\n",
      "Test: [9/9]\tTime 0.004 (0.007)\tLoss 1.1621 (1.1625)\tPrec@1 55.612 (58.440)\n",
      "EPOCH: 162 val Results: Prec@1 58.440 Loss: 1.1625\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/48]\tTime 0.032 (0.032)\tLoss 1.0686 (1.0686)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [163][9/48]\tTime 0.038 (0.029)\tLoss 1.1030 (1.0632)\tPrec@1 59.863 (61.777)\n",
      "Epoch: [163][18/48]\tTime 0.123 (0.033)\tLoss 1.0466 (1.0642)\tPrec@1 61.328 (62.212)\n",
      "Epoch: [163][27/48]\tTime 0.107 (0.035)\tLoss 1.1026 (1.0743)\tPrec@1 60.742 (61.879)\n",
      "Epoch: [163][36/48]\tTime 0.032 (0.035)\tLoss 1.1269 (1.0865)\tPrec@1 59.473 (61.521)\n",
      "Epoch: [163][45/48]\tTime 0.020 (0.032)\tLoss 1.0865 (1.0944)\tPrec@1 62.012 (61.292)\n",
      "Epoch: [163][48/48]\tTime 0.022 (0.036)\tLoss 1.1030 (1.0961)\tPrec@1 59.434 (61.208)\n",
      "EPOCH: 163 train Results: Prec@1 61.208 Loss: 1.0961\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1475 (1.1475)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.1581 (1.1646)\tPrec@1 57.015 (58.310)\n",
      "EPOCH: 163 val Results: Prec@1 58.310 Loss: 1.1646\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/48]\tTime 0.027 (0.027)\tLoss 1.0922 (1.0922)\tPrec@1 61.133 (61.133)\n",
      "Epoch: [164][9/48]\tTime 0.022 (0.023)\tLoss 1.0443 (1.0509)\tPrec@1 62.305 (62.959)\n",
      "Epoch: [164][18/48]\tTime 0.016 (0.022)\tLoss 1.0839 (1.0697)\tPrec@1 61.328 (62.253)\n",
      "Epoch: [164][27/48]\tTime 0.023 (0.023)\tLoss 1.1421 (1.0785)\tPrec@1 59.277 (61.921)\n",
      "Epoch: [164][36/48]\tTime 0.018 (0.022)\tLoss 1.1138 (1.0851)\tPrec@1 59.375 (61.645)\n",
      "Epoch: [164][45/48]\tTime 0.021 (0.022)\tLoss 1.1107 (1.0906)\tPrec@1 60.059 (61.330)\n",
      "Epoch: [164][48/48]\tTime 0.015 (0.022)\tLoss 1.0848 (1.0898)\tPrec@1 60.259 (61.396)\n",
      "EPOCH: 164 train Results: Prec@1 61.396 Loss: 1.0898\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1402 (1.1402)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1696 (1.1665)\tPrec@1 57.015 (58.310)\n",
      "EPOCH: 164 val Results: Prec@1 58.310 Loss: 1.1665\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/48]\tTime 0.016 (0.016)\tLoss 1.0684 (1.0684)\tPrec@1 62.695 (62.695)\n",
      "Epoch: [165][9/48]\tTime 0.021 (0.019)\tLoss 1.0324 (1.0513)\tPrec@1 63.867 (63.154)\n",
      "Epoch: [165][18/48]\tTime 0.049 (0.024)\tLoss 1.1451 (1.0753)\tPrec@1 58.008 (62.058)\n",
      "Epoch: [165][27/48]\tTime 0.024 (0.026)\tLoss 1.0625 (1.0770)\tPrec@1 61.133 (61.991)\n",
      "Epoch: [165][36/48]\tTime 0.020 (0.025)\tLoss 1.0685 (1.0894)\tPrec@1 61.035 (61.381)\n",
      "Epoch: [165][45/48]\tTime 0.021 (0.026)\tLoss 1.1033 (1.0939)\tPrec@1 62.109 (61.247)\n",
      "Epoch: [165][48/48]\tTime 0.039 (0.026)\tLoss 1.1572 (1.0979)\tPrec@1 58.608 (61.112)\n",
      "EPOCH: 165 train Results: Prec@1 61.112 Loss: 1.0979\n",
      "Test: [0/9]\tTime 0.025 (0.025)\tLoss 1.1497 (1.1497)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.010 (0.010)\tLoss 1.1697 (1.1719)\tPrec@1 57.270 (57.900)\n",
      "EPOCH: 165 val Results: Prec@1 57.900 Loss: 1.1719\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/48]\tTime 0.023 (0.023)\tLoss 1.1019 (1.1019)\tPrec@1 61.523 (61.523)\n",
      "Epoch: [166][9/48]\tTime 0.025 (0.035)\tLoss 1.0473 (1.0690)\tPrec@1 62.988 (62.607)\n",
      "Epoch: [166][18/48]\tTime 0.024 (0.033)\tLoss 1.1244 (1.0720)\tPrec@1 59.961 (62.217)\n",
      "Epoch: [166][27/48]\tTime 0.024 (0.034)\tLoss 1.0927 (1.0797)\tPrec@1 61.035 (61.799)\n",
      "Epoch: [166][36/48]\tTime 0.027 (0.035)\tLoss 1.1040 (1.0841)\tPrec@1 59.180 (61.455)\n",
      "Epoch: [166][45/48]\tTime 0.024 (0.033)\tLoss 1.1425 (1.0900)\tPrec@1 60.742 (61.351)\n",
      "Epoch: [166][48/48]\tTime 0.041 (0.034)\tLoss 1.1766 (1.0925)\tPrec@1 57.783 (61.254)\n",
      "EPOCH: 166 train Results: Prec@1 61.254 Loss: 1.0925\n",
      "Test: [0/9]\tTime 0.015 (0.015)\tLoss 1.1365 (1.1365)\tPrec@1 59.961 (59.961)\n",
      "Test: [9/9]\tTime 0.003 (0.009)\tLoss 1.1644 (1.1614)\tPrec@1 57.781 (58.670)\n",
      "EPOCH: 166 val Results: Prec@1 58.670 Loss: 1.1614\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/48]\tTime 0.027 (0.027)\tLoss 1.0833 (1.0833)\tPrec@1 62.207 (62.207)\n",
      "Epoch: [167][9/48]\tTime 0.031 (0.027)\tLoss 1.1351 (1.0726)\tPrec@1 60.547 (62.217)\n",
      "Epoch: [167][18/48]\tTime 0.032 (0.029)\tLoss 1.0794 (1.0749)\tPrec@1 61.328 (61.986)\n",
      "Epoch: [167][27/48]\tTime 0.045 (0.029)\tLoss 1.0909 (1.0855)\tPrec@1 60.840 (61.593)\n",
      "Epoch: [167][36/48]\tTime 0.025 (0.051)\tLoss 1.0863 (1.0884)\tPrec@1 61.523 (61.539)\n",
      "Epoch: [167][45/48]\tTime 0.102 (0.048)\tLoss 1.0252 (1.0926)\tPrec@1 63.770 (61.434)\n",
      "Epoch: [167][48/48]\tTime 0.026 (0.047)\tLoss 1.1202 (1.0942)\tPrec@1 59.906 (61.330)\n",
      "EPOCH: 167 train Results: Prec@1 61.330 Loss: 1.0942\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.1353 (1.1353)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.011 (0.008)\tLoss 1.1716 (1.1659)\tPrec@1 57.398 (58.480)\n",
      "EPOCH: 167 val Results: Prec@1 58.480 Loss: 1.1659\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/48]\tTime 0.027 (0.027)\tLoss 1.0964 (1.0964)\tPrec@1 60.840 (60.840)\n",
      "Epoch: [168][9/48]\tTime 0.025 (0.026)\tLoss 1.0373 (1.0651)\tPrec@1 63.281 (62.920)\n",
      "Epoch: [168][18/48]\tTime 0.028 (0.024)\tLoss 1.1265 (1.0696)\tPrec@1 61.523 (62.464)\n",
      "Epoch: [168][27/48]\tTime 0.017 (0.023)\tLoss 1.1331 (1.0708)\tPrec@1 60.254 (62.336)\n",
      "Epoch: [168][36/48]\tTime 0.018 (0.023)\tLoss 1.1373 (1.0800)\tPrec@1 59.668 (62.046)\n",
      "Epoch: [168][45/48]\tTime 0.016 (0.023)\tLoss 1.1034 (1.0878)\tPrec@1 61.621 (61.795)\n",
      "Epoch: [168][48/48]\tTime 0.011 (0.023)\tLoss 1.0878 (1.0888)\tPrec@1 61.910 (61.712)\n",
      "EPOCH: 168 train Results: Prec@1 61.712 Loss: 1.0888\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1380 (1.1380)\tPrec@1 60.547 (60.547)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.1668 (1.1663)\tPrec@1 58.418 (58.830)\n",
      "EPOCH: 168 val Results: Prec@1 58.830 Loss: 1.1663\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/48]\tTime 0.028 (0.028)\tLoss 1.0346 (1.0346)\tPrec@1 62.598 (62.598)\n",
      "Epoch: [169][9/48]\tTime 0.018 (0.030)\tLoss 1.0518 (1.0596)\tPrec@1 61.328 (62.305)\n",
      "Epoch: [169][18/48]\tTime 0.023 (0.025)\tLoss 1.0679 (1.0703)\tPrec@1 61.816 (62.166)\n",
      "Epoch: [169][27/48]\tTime 0.023 (0.025)\tLoss 1.1078 (1.0765)\tPrec@1 58.008 (62.001)\n",
      "Epoch: [169][36/48]\tTime 0.028 (0.024)\tLoss 1.1150 (1.0845)\tPrec@1 59.668 (61.727)\n",
      "Epoch: [169][45/48]\tTime 0.018 (0.024)\tLoss 1.0868 (1.0897)\tPrec@1 64.453 (61.492)\n",
      "Epoch: [169][48/48]\tTime 0.012 (0.024)\tLoss 1.1512 (1.0916)\tPrec@1 58.373 (61.400)\n",
      "EPOCH: 169 train Results: Prec@1 61.400 Loss: 1.0916\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1313 (1.1313)\tPrec@1 60.547 (60.547)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1693 (1.1661)\tPrec@1 58.291 (58.620)\n",
      "EPOCH: 169 val Results: Prec@1 58.620 Loss: 1.1661\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/48]\tTime 0.043 (0.043)\tLoss 1.0995 (1.0995)\tPrec@1 62.109 (62.109)\n",
      "Epoch: [170][9/48]\tTime 0.016 (0.026)\tLoss 1.0619 (1.0663)\tPrec@1 63.770 (62.510)\n",
      "Epoch: [170][18/48]\tTime 0.023 (0.023)\tLoss 1.0271 (1.0700)\tPrec@1 62.695 (62.402)\n",
      "Epoch: [170][27/48]\tTime 0.041 (0.024)\tLoss 1.0601 (1.0761)\tPrec@1 62.793 (62.081)\n",
      "Epoch: [170][36/48]\tTime 0.030 (0.026)\tLoss 1.0807 (1.0817)\tPrec@1 62.109 (61.954)\n",
      "Epoch: [170][45/48]\tTime 0.035 (0.027)\tLoss 1.0957 (1.0870)\tPrec@1 62.598 (61.797)\n",
      "Epoch: [170][48/48]\tTime 0.019 (0.026)\tLoss 1.0954 (1.0873)\tPrec@1 62.736 (61.768)\n",
      "EPOCH: 170 train Results: Prec@1 61.768 Loss: 1.0873\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1374 (1.1374)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.005 (0.005)\tLoss 1.1719 (1.1671)\tPrec@1 58.673 (58.100)\n",
      "EPOCH: 170 val Results: Prec@1 58.100 Loss: 1.1671\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/48]\tTime 0.023 (0.023)\tLoss 1.0849 (1.0849)\tPrec@1 61.133 (61.133)\n",
      "Epoch: [171][9/48]\tTime 0.020 (0.031)\tLoss 1.1160 (1.0622)\tPrec@1 60.352 (62.588)\n",
      "Epoch: [171][18/48]\tTime 0.018 (0.027)\tLoss 1.0602 (1.0700)\tPrec@1 62.500 (62.336)\n",
      "Epoch: [171][27/48]\tTime 0.020 (0.026)\tLoss 1.0711 (1.0765)\tPrec@1 63.477 (62.078)\n",
      "Epoch: [171][36/48]\tTime 0.027 (0.027)\tLoss 1.1304 (1.0805)\tPrec@1 59.570 (61.822)\n",
      "Epoch: [171][45/48]\tTime 0.075 (0.028)\tLoss 1.1432 (1.0894)\tPrec@1 59.375 (61.470)\n",
      "Epoch: [171][48/48]\tTime 0.024 (0.028)\tLoss 1.1184 (1.0909)\tPrec@1 59.906 (61.404)\n",
      "EPOCH: 171 train Results: Prec@1 61.404 Loss: 1.0909\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1536 (1.1536)\tPrec@1 59.473 (59.473)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.1640 (1.1651)\tPrec@1 57.781 (58.270)\n",
      "EPOCH: 171 val Results: Prec@1 58.270 Loss: 1.1651\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/48]\tTime 0.028 (0.028)\tLoss 1.0320 (1.0320)\tPrec@1 64.746 (64.746)\n",
      "Epoch: [172][9/48]\tTime 0.022 (0.024)\tLoss 1.0752 (1.0606)\tPrec@1 61.035 (62.627)\n",
      "Epoch: [172][18/48]\tTime 0.021 (0.023)\tLoss 1.0499 (1.0656)\tPrec@1 63.086 (62.449)\n",
      "Epoch: [172][27/48]\tTime 0.026 (0.024)\tLoss 1.1248 (1.0731)\tPrec@1 60.645 (62.113)\n",
      "Epoch: [172][36/48]\tTime 0.027 (0.023)\tLoss 1.0846 (1.0826)\tPrec@1 61.426 (61.711)\n",
      "Epoch: [172][45/48]\tTime 0.028 (0.023)\tLoss 1.1369 (1.0872)\tPrec@1 60.449 (61.585)\n",
      "Epoch: [172][48/48]\tTime 0.030 (0.024)\tLoss 1.1132 (1.0888)\tPrec@1 59.434 (61.466)\n",
      "EPOCH: 172 train Results: Prec@1 61.466 Loss: 1.0888\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1477 (1.1477)\tPrec@1 60.156 (60.156)\n",
      "Test: [9/9]\tTime 0.014 (0.007)\tLoss 1.1550 (1.1620)\tPrec@1 57.653 (58.590)\n",
      "EPOCH: 172 val Results: Prec@1 58.590 Loss: 1.1620\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/48]\tTime 0.065 (0.065)\tLoss 1.0185 (1.0185)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [173][9/48]\tTime 0.022 (0.041)\tLoss 1.0773 (1.0649)\tPrec@1 61.719 (62.490)\n",
      "Epoch: [173][18/48]\tTime 0.017 (0.032)\tLoss 1.0820 (1.0757)\tPrec@1 63.281 (62.063)\n",
      "Epoch: [173][27/48]\tTime 0.024 (0.031)\tLoss 1.0319 (1.0720)\tPrec@1 63.574 (62.068)\n",
      "Epoch: [173][36/48]\tTime 0.036 (0.030)\tLoss 1.0797 (1.0767)\tPrec@1 61.816 (61.896)\n",
      "Epoch: [173][45/48]\tTime 0.020 (0.028)\tLoss 1.0792 (1.0850)\tPrec@1 61.426 (61.492)\n",
      "Epoch: [173][48/48]\tTime 0.039 (0.028)\tLoss 1.0766 (1.0850)\tPrec@1 62.028 (61.498)\n",
      "EPOCH: 173 train Results: Prec@1 61.498 Loss: 1.0850\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1515 (1.1515)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.002 (0.005)\tLoss 1.1562 (1.1660)\tPrec@1 58.546 (58.530)\n",
      "EPOCH: 173 val Results: Prec@1 58.530 Loss: 1.1660\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/48]\tTime 0.017 (0.017)\tLoss 1.0418 (1.0418)\tPrec@1 63.086 (63.086)\n",
      "Epoch: [174][9/48]\tTime 0.015 (0.027)\tLoss 1.0772 (1.0532)\tPrec@1 62.500 (62.744)\n",
      "Epoch: [174][18/48]\tTime 0.016 (0.025)\tLoss 1.0969 (1.0600)\tPrec@1 61.719 (62.315)\n",
      "Epoch: [174][27/48]\tTime 0.021 (0.024)\tLoss 1.1321 (1.0672)\tPrec@1 61.035 (62.120)\n",
      "Epoch: [174][36/48]\tTime 0.105 (0.027)\tLoss 1.1362 (1.0737)\tPrec@1 58.984 (61.964)\n",
      "Epoch: [174][45/48]\tTime 0.034 (0.027)\tLoss 1.1673 (1.0854)\tPrec@1 56.738 (61.383)\n",
      "Epoch: [174][48/48]\tTime 0.014 (0.026)\tLoss 1.0748 (1.0870)\tPrec@1 62.854 (61.354)\n",
      "EPOCH: 174 train Results: Prec@1 61.354 Loss: 1.0870\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1383 (1.1383)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1595 (1.1613)\tPrec@1 57.526 (58.660)\n",
      "EPOCH: 174 val Results: Prec@1 58.660 Loss: 1.1613\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/48]\tTime 0.019 (0.019)\tLoss 1.0375 (1.0375)\tPrec@1 63.672 (63.672)\n",
      "Epoch: [175][9/48]\tTime 0.021 (0.028)\tLoss 1.0480 (1.0435)\tPrec@1 64.453 (63.164)\n",
      "Epoch: [175][18/48]\tTime 0.027 (0.024)\tLoss 1.1224 (1.0591)\tPrec@1 58.008 (62.557)\n",
      "Epoch: [175][27/48]\tTime 0.021 (0.025)\tLoss 1.1460 (1.0678)\tPrec@1 59.668 (62.347)\n",
      "Epoch: [175][36/48]\tTime 0.027 (0.023)\tLoss 1.2070 (1.0772)\tPrec@1 56.152 (61.940)\n",
      "Epoch: [175][45/48]\tTime 0.024 (0.024)\tLoss 1.1505 (1.0839)\tPrec@1 57.227 (61.685)\n",
      "Epoch: [175][48/48]\tTime 0.025 (0.024)\tLoss 1.1352 (1.0864)\tPrec@1 58.491 (61.534)\n",
      "EPOCH: 175 train Results: Prec@1 61.534 Loss: 1.0864\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1456 (1.1456)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.012 (0.006)\tLoss 1.1593 (1.1634)\tPrec@1 58.291 (58.300)\n",
      "EPOCH: 175 val Results: Prec@1 58.300 Loss: 1.1634\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/48]\tTime 0.024 (0.024)\tLoss 1.0801 (1.0801)\tPrec@1 62.012 (62.012)\n",
      "Epoch: [176][9/48]\tTime 0.046 (0.033)\tLoss 1.0776 (1.0659)\tPrec@1 62.012 (62.705)\n",
      "Epoch: [176][18/48]\tTime 0.017 (0.036)\tLoss 1.0407 (1.0706)\tPrec@1 62.891 (62.294)\n",
      "Epoch: [176][27/48]\tTime 0.048 (0.034)\tLoss 1.0710 (1.0738)\tPrec@1 61.719 (62.074)\n",
      "Epoch: [176][36/48]\tTime 0.023 (0.031)\tLoss 1.0989 (1.0776)\tPrec@1 59.961 (61.933)\n",
      "Epoch: [176][45/48]\tTime 0.017 (0.031)\tLoss 1.1023 (1.0815)\tPrec@1 61.133 (61.808)\n",
      "Epoch: [176][48/48]\tTime 0.020 (0.031)\tLoss 1.0897 (1.0830)\tPrec@1 59.198 (61.704)\n",
      "EPOCH: 176 train Results: Prec@1 61.704 Loss: 1.0830\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1416 (1.1416)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.1598 (1.1647)\tPrec@1 58.801 (58.340)\n",
      "EPOCH: 176 val Results: Prec@1 58.340 Loss: 1.1647\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/48]\tTime 0.026 (0.026)\tLoss 1.0883 (1.0883)\tPrec@1 62.695 (62.695)\n",
      "Epoch: [177][9/48]\tTime 0.023 (0.029)\tLoss 1.0858 (1.0594)\tPrec@1 60.547 (62.490)\n",
      "Epoch: [177][18/48]\tTime 0.019 (0.033)\tLoss 1.1573 (1.0673)\tPrec@1 58.789 (62.022)\n",
      "Epoch: [177][27/48]\tTime 0.017 (0.029)\tLoss 1.0498 (1.0768)\tPrec@1 62.305 (61.639)\n",
      "Epoch: [177][36/48]\tTime 0.025 (0.029)\tLoss 1.1275 (1.0823)\tPrec@1 61.133 (61.634)\n",
      "Epoch: [177][45/48]\tTime 0.028 (0.029)\tLoss 1.1221 (1.0877)\tPrec@1 60.840 (61.413)\n",
      "Epoch: [177][48/48]\tTime 0.017 (0.029)\tLoss 1.0807 (1.0872)\tPrec@1 62.618 (61.422)\n",
      "EPOCH: 177 train Results: Prec@1 61.422 Loss: 1.0872\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1372 (1.1372)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.1663 (1.1648)\tPrec@1 57.015 (58.200)\n",
      "EPOCH: 177 val Results: Prec@1 58.200 Loss: 1.1648\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/48]\tTime 0.019 (0.019)\tLoss 1.0809 (1.0809)\tPrec@1 60.645 (60.645)\n",
      "Epoch: [178][9/48]\tTime 0.026 (0.032)\tLoss 1.0431 (1.0546)\tPrec@1 63.672 (62.910)\n",
      "Epoch: [178][18/48]\tTime 0.029 (0.034)\tLoss 1.0451 (1.0635)\tPrec@1 61.914 (62.515)\n",
      "Epoch: [178][27/48]\tTime 0.020 (0.030)\tLoss 1.0599 (1.0730)\tPrec@1 59.863 (61.876)\n",
      "Epoch: [178][36/48]\tTime 0.029 (0.030)\tLoss 1.1397 (1.0799)\tPrec@1 58.301 (61.647)\n",
      "Epoch: [178][45/48]\tTime 0.063 (0.030)\tLoss 1.0919 (1.0824)\tPrec@1 61.133 (61.534)\n",
      "Epoch: [178][48/48]\tTime 0.017 (0.030)\tLoss 1.1021 (1.0840)\tPrec@1 61.439 (61.504)\n",
      "EPOCH: 178 train Results: Prec@1 61.504 Loss: 1.0840\n",
      "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.1373 (1.1373)\tPrec@1 59.082 (59.082)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.1634 (1.1625)\tPrec@1 57.908 (58.500)\n",
      "EPOCH: 178 val Results: Prec@1 58.500 Loss: 1.1625\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/48]\tTime 0.025 (0.025)\tLoss 1.0606 (1.0606)\tPrec@1 61.914 (61.914)\n",
      "Epoch: [179][9/48]\tTime 0.020 (0.033)\tLoss 1.0960 (1.0581)\tPrec@1 60.742 (62.598)\n",
      "Epoch: [179][18/48]\tTime 0.021 (0.028)\tLoss 1.0094 (1.0666)\tPrec@1 63.672 (62.145)\n",
      "Epoch: [179][27/48]\tTime 0.020 (0.027)\tLoss 1.1257 (1.0757)\tPrec@1 60.840 (61.792)\n",
      "Epoch: [179][36/48]\tTime 0.031 (0.028)\tLoss 1.1065 (1.0777)\tPrec@1 60.449 (61.756)\n",
      "Epoch: [179][45/48]\tTime 0.041 (0.028)\tLoss 1.1123 (1.0842)\tPrec@1 60.059 (61.536)\n",
      "Epoch: [179][48/48]\tTime 0.017 (0.028)\tLoss 1.1063 (1.0842)\tPrec@1 59.434 (61.486)\n",
      "EPOCH: 179 train Results: Prec@1 61.486 Loss: 1.0842\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1302 (1.1302)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1710 (1.1614)\tPrec@1 57.908 (58.540)\n",
      "EPOCH: 179 val Results: Prec@1 58.540 Loss: 1.1614\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/48]\tTime 0.037 (0.037)\tLoss 1.0517 (1.0517)\tPrec@1 64.941 (64.941)\n",
      "Epoch: [180][9/48]\tTime 0.028 (0.032)\tLoss 1.0941 (1.0620)\tPrec@1 62.012 (62.832)\n",
      "Epoch: [180][18/48]\tTime 0.045 (0.035)\tLoss 1.0367 (1.0658)\tPrec@1 62.207 (62.469)\n",
      "Epoch: [180][27/48]\tTime 0.026 (0.033)\tLoss 1.0907 (1.0717)\tPrec@1 63.770 (62.207)\n",
      "Epoch: [180][36/48]\tTime 0.023 (0.030)\tLoss 1.1255 (1.0780)\tPrec@1 61.621 (62.001)\n",
      "Epoch: [180][45/48]\tTime 0.017 (0.030)\tLoss 1.1701 (1.0829)\tPrec@1 58.984 (61.742)\n",
      "Epoch: [180][48/48]\tTime 0.017 (0.029)\tLoss 1.1435 (1.0857)\tPrec@1 57.783 (61.582)\n",
      "EPOCH: 180 train Results: Prec@1 61.582 Loss: 1.0857\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1535 (1.1535)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.007 (0.004)\tLoss 1.1627 (1.1654)\tPrec@1 57.398 (58.390)\n",
      "EPOCH: 180 val Results: Prec@1 58.390 Loss: 1.1654\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/48]\tTime 0.026 (0.026)\tLoss 1.0284 (1.0284)\tPrec@1 62.402 (62.402)\n",
      "Epoch: [181][9/48]\tTime 0.048 (0.031)\tLoss 1.1288 (1.0543)\tPrec@1 60.938 (62.695)\n",
      "Epoch: [181][18/48]\tTime 0.030 (0.034)\tLoss 1.0853 (1.0599)\tPrec@1 61.621 (62.202)\n",
      "Epoch: [181][27/48]\tTime 0.048 (0.032)\tLoss 1.0745 (1.0706)\tPrec@1 62.988 (62.005)\n",
      "Epoch: [181][36/48]\tTime 0.033 (0.032)\tLoss 1.0749 (1.0766)\tPrec@1 62.305 (61.811)\n",
      "Epoch: [181][45/48]\tTime 0.041 (0.031)\tLoss 1.1218 (1.0831)\tPrec@1 59.375 (61.528)\n",
      "Epoch: [181][48/48]\tTime 0.019 (0.030)\tLoss 1.1076 (1.0832)\tPrec@1 59.080 (61.528)\n",
      "EPOCH: 181 train Results: Prec@1 61.528 Loss: 1.0832\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1469 (1.1469)\tPrec@1 59.082 (59.082)\n",
      "Test: [9/9]\tTime 0.042 (0.017)\tLoss 1.1602 (1.1594)\tPrec@1 57.015 (58.870)\n",
      "EPOCH: 181 val Results: Prec@1 58.870 Loss: 1.1594\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/48]\tTime 0.027 (0.027)\tLoss 1.0626 (1.0626)\tPrec@1 63.184 (63.184)\n",
      "Epoch: [182][9/48]\tTime 0.032 (0.022)\tLoss 1.0577 (1.0534)\tPrec@1 63.379 (63.213)\n",
      "Epoch: [182][18/48]\tTime 0.036 (0.024)\tLoss 1.0822 (1.0619)\tPrec@1 61.816 (62.721)\n",
      "Epoch: [182][27/48]\tTime 0.022 (0.024)\tLoss 1.0740 (1.0709)\tPrec@1 60.840 (62.315)\n",
      "Epoch: [182][36/48]\tTime 0.016 (0.023)\tLoss 1.1095 (1.0743)\tPrec@1 61.133 (62.178)\n",
      "Epoch: [182][45/48]\tTime 0.022 (0.025)\tLoss 1.0922 (1.0807)\tPrec@1 60.156 (61.816)\n",
      "Epoch: [182][48/48]\tTime 0.016 (0.024)\tLoss 1.1561 (1.0825)\tPrec@1 58.255 (61.750)\n",
      "EPOCH: 182 train Results: Prec@1 61.750 Loss: 1.0825\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1453 (1.1453)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.1512 (1.1612)\tPrec@1 58.291 (58.230)\n",
      "EPOCH: 182 val Results: Prec@1 58.230 Loss: 1.1612\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/48]\tTime 0.022 (0.022)\tLoss 1.0223 (1.0223)\tPrec@1 65.234 (65.234)\n",
      "Epoch: [183][9/48]\tTime 0.018 (0.025)\tLoss 1.0875 (1.0560)\tPrec@1 63.965 (62.021)\n",
      "Epoch: [183][18/48]\tTime 0.020 (0.023)\tLoss 1.0884 (1.0655)\tPrec@1 61.426 (61.822)\n",
      "Epoch: [183][27/48]\tTime 0.021 (0.024)\tLoss 1.0764 (1.0703)\tPrec@1 64.648 (61.890)\n",
      "Epoch: [183][36/48]\tTime 0.017 (0.023)\tLoss 1.0504 (1.0725)\tPrec@1 60.840 (61.835)\n",
      "Epoch: [183][45/48]\tTime 0.026 (0.024)\tLoss 1.1327 (1.0826)\tPrec@1 60.059 (61.538)\n",
      "Epoch: [183][48/48]\tTime 0.021 (0.024)\tLoss 1.1834 (1.0850)\tPrec@1 58.962 (61.504)\n",
      "EPOCH: 183 train Results: Prec@1 61.504 Loss: 1.0850\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.1541 (1.1541)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.005 (0.005)\tLoss 1.1623 (1.1599)\tPrec@1 58.673 (58.620)\n",
      "EPOCH: 183 val Results: Prec@1 58.620 Loss: 1.1599\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/48]\tTime 0.018 (0.018)\tLoss 0.9897 (0.9897)\tPrec@1 66.016 (66.016)\n",
      "Epoch: [184][9/48]\tTime 0.015 (0.022)\tLoss 1.0648 (1.0477)\tPrec@1 63.770 (63.242)\n",
      "Epoch: [184][18/48]\tTime 0.020 (0.022)\tLoss 1.1098 (1.0600)\tPrec@1 60.352 (62.659)\n",
      "Epoch: [184][27/48]\tTime 0.019 (0.023)\tLoss 1.1073 (1.0677)\tPrec@1 61.035 (62.507)\n",
      "Epoch: [184][36/48]\tTime 0.021 (0.022)\tLoss 1.0668 (1.0739)\tPrec@1 62.305 (62.178)\n",
      "Epoch: [184][45/48]\tTime 0.020 (0.023)\tLoss 1.0858 (1.0798)\tPrec@1 60.645 (61.861)\n",
      "Epoch: [184][48/48]\tTime 0.026 (0.023)\tLoss 1.1736 (1.0806)\tPrec@1 58.844 (61.770)\n",
      "EPOCH: 184 train Results: Prec@1 61.770 Loss: 1.0806\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1560 (1.1560)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.005 (0.005)\tLoss 1.1526 (1.1637)\tPrec@1 57.908 (58.590)\n",
      "EPOCH: 184 val Results: Prec@1 58.590 Loss: 1.1637\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/48]\tTime 0.021 (0.021)\tLoss 1.0314 (1.0314)\tPrec@1 65.723 (65.723)\n",
      "Epoch: [185][9/48]\tTime 0.019 (0.024)\tLoss 1.0912 (1.0614)\tPrec@1 61.523 (62.656)\n",
      "Epoch: [185][18/48]\tTime 0.023 (0.029)\tLoss 1.0652 (1.0612)\tPrec@1 60.938 (62.433)\n",
      "Epoch: [185][27/48]\tTime 0.024 (0.027)\tLoss 1.0298 (1.0614)\tPrec@1 62.695 (62.441)\n",
      "Epoch: [185][36/48]\tTime 0.019 (0.027)\tLoss 1.1407 (1.0675)\tPrec@1 59.375 (62.170)\n",
      "Epoch: [185][45/48]\tTime 0.030 (0.026)\tLoss 1.1648 (1.0758)\tPrec@1 59.180 (61.876)\n",
      "Epoch: [185][48/48]\tTime 0.036 (0.026)\tLoss 1.2022 (1.0790)\tPrec@1 57.901 (61.774)\n",
      "EPOCH: 185 train Results: Prec@1 61.774 Loss: 1.0790\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.1537 (1.1537)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.008 (0.007)\tLoss 1.1508 (1.1685)\tPrec@1 57.781 (58.520)\n",
      "EPOCH: 185 val Results: Prec@1 58.520 Loss: 1.1685\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/48]\tTime 0.030 (0.030)\tLoss 1.0462 (1.0462)\tPrec@1 64.258 (64.258)\n",
      "Epoch: [186][9/48]\tTime 0.021 (0.025)\tLoss 1.0829 (1.0571)\tPrec@1 60.059 (62.666)\n",
      "Epoch: [186][18/48]\tTime 0.019 (0.023)\tLoss 1.0320 (1.0641)\tPrec@1 64.258 (62.423)\n",
      "Epoch: [186][27/48]\tTime 0.021 (0.024)\tLoss 1.0573 (1.0688)\tPrec@1 62.695 (62.092)\n",
      "Epoch: [186][36/48]\tTime 0.022 (0.023)\tLoss 1.1125 (1.0790)\tPrec@1 61.133 (61.798)\n",
      "Epoch: [186][45/48]\tTime 0.025 (0.023)\tLoss 1.0205 (1.0818)\tPrec@1 64.551 (61.753)\n",
      "Epoch: [186][48/48]\tTime 0.021 (0.023)\tLoss 1.0914 (1.0811)\tPrec@1 62.618 (61.792)\n",
      "EPOCH: 186 train Results: Prec@1 61.792 Loss: 1.0811\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.1479 (1.1479)\tPrec@1 58.594 (58.594)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.1486 (1.1661)\tPrec@1 58.546 (58.400)\n",
      "EPOCH: 186 val Results: Prec@1 58.400 Loss: 1.1661\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/48]\tTime 0.016 (0.016)\tLoss 1.0400 (1.0400)\tPrec@1 64.648 (64.648)\n",
      "Epoch: [187][9/48]\tTime 0.021 (0.019)\tLoss 1.0091 (1.0481)\tPrec@1 63.281 (63.057)\n",
      "Epoch: [187][18/48]\tTime 0.017 (0.019)\tLoss 1.0379 (1.0602)\tPrec@1 63.477 (62.521)\n",
      "Epoch: [187][27/48]\tTime 0.017 (0.020)\tLoss 1.0356 (1.0615)\tPrec@1 61.816 (62.259)\n",
      "Epoch: [187][36/48]\tTime 0.019 (0.020)\tLoss 1.1390 (1.0755)\tPrec@1 59.668 (61.856)\n",
      "Epoch: [187][45/48]\tTime 0.022 (0.021)\tLoss 1.1320 (1.0819)\tPrec@1 59.570 (61.625)\n",
      "Epoch: [187][48/48]\tTime 0.020 (0.021)\tLoss 1.0901 (1.0831)\tPrec@1 63.090 (61.602)\n",
      "EPOCH: 187 train Results: Prec@1 61.602 Loss: 1.0831\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1499 (1.1499)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.1638 (1.1705)\tPrec@1 56.250 (57.930)\n",
      "EPOCH: 187 val Results: Prec@1 57.930 Loss: 1.1705\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/48]\tTime 0.023 (0.023)\tLoss 1.0097 (1.0097)\tPrec@1 64.160 (64.160)\n",
      "Epoch: [188][9/48]\tTime 0.019 (0.027)\tLoss 1.0873 (1.0343)\tPrec@1 62.500 (63.672)\n",
      "Epoch: [188][18/48]\tTime 0.019 (0.025)\tLoss 1.0447 (1.0556)\tPrec@1 64.453 (62.844)\n",
      "Epoch: [188][27/48]\tTime 0.021 (0.025)\tLoss 1.0383 (1.0609)\tPrec@1 64.258 (62.608)\n",
      "Epoch: [188][36/48]\tTime 0.018 (0.024)\tLoss 1.0645 (1.0697)\tPrec@1 61.621 (62.307)\n",
      "Epoch: [188][45/48]\tTime 0.020 (0.024)\tLoss 1.1942 (1.0791)\tPrec@1 58.887 (61.931)\n",
      "Epoch: [188][48/48]\tTime 0.092 (0.025)\tLoss 1.1226 (1.0802)\tPrec@1 59.552 (61.846)\n",
      "EPOCH: 188 train Results: Prec@1 61.846 Loss: 1.0802\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1576 (1.1576)\tPrec@1 57.520 (57.520)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.1471 (1.1632)\tPrec@1 57.398 (58.560)\n",
      "EPOCH: 188 val Results: Prec@1 58.560 Loss: 1.1632\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/48]\tTime 0.049 (0.049)\tLoss 1.0780 (1.0780)\tPrec@1 61.426 (61.426)\n",
      "Epoch: [189][9/48]\tTime 0.017 (0.023)\tLoss 1.0220 (1.0469)\tPrec@1 63.965 (63.252)\n",
      "Epoch: [189][18/48]\tTime 0.022 (0.023)\tLoss 1.0900 (1.0611)\tPrec@1 58.789 (62.089)\n",
      "Epoch: [189][27/48]\tTime 0.036 (0.025)\tLoss 1.0848 (1.0706)\tPrec@1 61.816 (61.837)\n",
      "Epoch: [189][36/48]\tTime 0.030 (0.028)\tLoss 1.1144 (1.0741)\tPrec@1 59.180 (61.795)\n",
      "Epoch: [189][45/48]\tTime 0.030 (0.030)\tLoss 1.1130 (1.0789)\tPrec@1 62.109 (61.748)\n",
      "Epoch: [189][48/48]\tTime 0.015 (0.030)\tLoss 1.1689 (1.0832)\tPrec@1 58.962 (61.622)\n",
      "EPOCH: 189 train Results: Prec@1 61.622 Loss: 1.0832\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1487 (1.1487)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.1714 (1.1660)\tPrec@1 57.270 (58.290)\n",
      "EPOCH: 189 val Results: Prec@1 58.290 Loss: 1.1660\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/48]\tTime 0.042 (0.042)\tLoss 1.0642 (1.0642)\tPrec@1 62.402 (62.402)\n",
      "Epoch: [190][9/48]\tTime 0.030 (0.044)\tLoss 1.0737 (1.0477)\tPrec@1 61.621 (63.535)\n",
      "Epoch: [190][18/48]\tTime 0.022 (0.044)\tLoss 1.0783 (1.0494)\tPrec@1 61.523 (63.214)\n",
      "Epoch: [190][27/48]\tTime 0.033 (0.041)\tLoss 1.1088 (1.0589)\tPrec@1 60.352 (62.681)\n",
      "Epoch: [190][36/48]\tTime 0.026 (0.042)\tLoss 1.1177 (1.0686)\tPrec@1 59.570 (62.128)\n",
      "Epoch: [190][45/48]\tTime 0.053 (0.039)\tLoss 1.1009 (1.0744)\tPrec@1 60.840 (61.990)\n",
      "Epoch: [190][48/48]\tTime 0.026 (0.039)\tLoss 1.1314 (1.0773)\tPrec@1 58.019 (61.872)\n",
      "EPOCH: 190 train Results: Prec@1 61.872 Loss: 1.0773\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.1472 (1.1472)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.1591 (1.1597)\tPrec@1 58.673 (58.800)\n",
      "EPOCH: 190 val Results: Prec@1 58.800 Loss: 1.1597\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/48]\tTime 0.024 (0.024)\tLoss 1.0812 (1.0812)\tPrec@1 61.230 (61.230)\n",
      "Epoch: [191][9/48]\tTime 0.019 (0.063)\tLoss 1.0708 (1.0617)\tPrec@1 62.012 (62.607)\n",
      "Epoch: [191][18/48]\tTime 0.047 (0.050)\tLoss 1.0929 (1.0684)\tPrec@1 60.254 (62.073)\n",
      "Epoch: [191][27/48]\tTime 0.031 (0.045)\tLoss 1.1074 (1.0718)\tPrec@1 62.207 (62.057)\n",
      "Epoch: [191][36/48]\tTime 0.023 (0.041)\tLoss 1.0769 (1.0739)\tPrec@1 60.938 (61.967)\n",
      "Epoch: [191][45/48]\tTime 0.040 (0.040)\tLoss 1.1424 (1.0814)\tPrec@1 61.035 (61.785)\n",
      "Epoch: [191][48/48]\tTime 0.027 (0.040)\tLoss 1.1814 (1.0856)\tPrec@1 58.491 (61.650)\n",
      "EPOCH: 191 train Results: Prec@1 61.650 Loss: 1.0856\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.1514 (1.1514)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.003 (0.009)\tLoss 1.1620 (1.1614)\tPrec@1 57.653 (58.520)\n",
      "EPOCH: 191 val Results: Prec@1 58.520 Loss: 1.1614\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/48]\tTime 0.035 (0.035)\tLoss 1.0180 (1.0180)\tPrec@1 63.379 (63.379)\n",
      "Epoch: [192][9/48]\tTime 0.029 (0.030)\tLoss 1.0564 (1.0348)\tPrec@1 63.086 (63.428)\n",
      "Epoch: [192][18/48]\tTime 0.041 (0.037)\tLoss 1.1083 (1.0519)\tPrec@1 60.352 (62.670)\n",
      "Epoch: [192][27/48]\tTime 0.043 (0.042)\tLoss 1.0475 (1.0587)\tPrec@1 62.598 (62.354)\n",
      "Epoch: [192][36/48]\tTime 0.022 (0.039)\tLoss 1.0766 (1.0660)\tPrec@1 61.328 (62.030)\n",
      "Epoch: [192][45/48]\tTime 0.035 (0.037)\tLoss 1.0895 (1.0719)\tPrec@1 61.914 (61.969)\n",
      "Epoch: [192][48/48]\tTime 0.020 (0.037)\tLoss 1.1194 (1.0739)\tPrec@1 58.491 (61.920)\n",
      "EPOCH: 192 train Results: Prec@1 61.920 Loss: 1.0739\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1359 (1.1359)\tPrec@1 59.277 (59.277)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.1693 (1.1574)\tPrec@1 56.760 (58.590)\n",
      "EPOCH: 192 val Results: Prec@1 58.590 Loss: 1.1574\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/48]\tTime 0.029 (0.029)\tLoss 1.0024 (1.0024)\tPrec@1 63.770 (63.770)\n",
      "Epoch: [193][9/48]\tTime 0.031 (0.033)\tLoss 1.0970 (1.0541)\tPrec@1 61.719 (62.842)\n",
      "Epoch: [193][18/48]\tTime 0.024 (0.035)\tLoss 1.1162 (1.0625)\tPrec@1 59.473 (62.413)\n",
      "Epoch: [193][27/48]\tTime 0.042 (0.033)\tLoss 1.0740 (1.0677)\tPrec@1 62.793 (62.263)\n",
      "Epoch: [193][36/48]\tTime 0.038 (0.034)\tLoss 1.1354 (1.0775)\tPrec@1 60.742 (61.896)\n",
      "Epoch: [193][45/48]\tTime 0.065 (0.037)\tLoss 1.1292 (1.0808)\tPrec@1 58.691 (61.742)\n",
      "Epoch: [193][48/48]\tTime 0.025 (0.037)\tLoss 1.0762 (1.0815)\tPrec@1 61.557 (61.676)\n",
      "EPOCH: 193 train Results: Prec@1 61.676 Loss: 1.0815\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1349 (1.1349)\tPrec@1 59.668 (59.668)\n",
      "Test: [9/9]\tTime 0.003 (0.008)\tLoss 1.1529 (1.1569)\tPrec@1 59.311 (58.940)\n",
      "EPOCH: 193 val Results: Prec@1 58.940 Loss: 1.1569\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/48]\tTime 0.060 (0.060)\tLoss 1.0916 (1.0916)\tPrec@1 61.035 (61.035)\n",
      "Epoch: [194][9/48]\tTime 0.041 (0.041)\tLoss 1.0180 (1.0497)\tPrec@1 63.574 (62.930)\n",
      "Epoch: [194][18/48]\tTime 0.037 (0.036)\tLoss 1.0850 (1.0633)\tPrec@1 60.840 (62.222)\n",
      "Epoch: [194][27/48]\tTime 0.022 (0.037)\tLoss 1.0387 (1.0668)\tPrec@1 63.574 (61.991)\n",
      "Epoch: [194][36/48]\tTime 0.025 (0.036)\tLoss 1.0279 (1.0713)\tPrec@1 65.527 (61.803)\n",
      "Epoch: [194][45/48]\tTime 0.045 (0.035)\tLoss 1.1514 (1.0767)\tPrec@1 58.496 (61.661)\n",
      "Epoch: [194][48/48]\tTime 0.023 (0.034)\tLoss 1.0750 (1.0760)\tPrec@1 60.849 (61.704)\n",
      "EPOCH: 194 train Results: Prec@1 61.704 Loss: 1.0760\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1402 (1.1402)\tPrec@1 59.473 (59.473)\n",
      "Test: [9/9]\tTime 0.015 (0.009)\tLoss 1.1520 (1.1598)\tPrec@1 58.291 (58.970)\n",
      "EPOCH: 194 val Results: Prec@1 58.970 Loss: 1.1598\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/48]\tTime 0.033 (0.033)\tLoss 0.9866 (0.9866)\tPrec@1 66.113 (66.113)\n",
      "Epoch: [195][9/48]\tTime 0.040 (0.037)\tLoss 1.0475 (1.0343)\tPrec@1 65.430 (63.564)\n",
      "Epoch: [195][18/48]\tTime 0.034 (0.037)\tLoss 1.1429 (1.0561)\tPrec@1 59.766 (62.377)\n",
      "Epoch: [195][27/48]\tTime 0.032 (0.039)\tLoss 1.0656 (1.0594)\tPrec@1 62.695 (62.399)\n",
      "Epoch: [195][36/48]\tTime 0.044 (0.044)\tLoss 1.0855 (1.0662)\tPrec@1 60.840 (62.170)\n",
      "Epoch: [195][45/48]\tTime 0.026 (0.041)\tLoss 1.1141 (1.0708)\tPrec@1 61.816 (62.118)\n",
      "Epoch: [195][48/48]\tTime 0.025 (0.040)\tLoss 1.1057 (1.0740)\tPrec@1 60.142 (61.990)\n",
      "EPOCH: 195 train Results: Prec@1 61.990 Loss: 1.0740\n",
      "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1481 (1.1481)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.002 (0.007)\tLoss 1.1548 (1.1575)\tPrec@1 57.781 (58.450)\n",
      "EPOCH: 195 val Results: Prec@1 58.450 Loss: 1.1575\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/48]\tTime 0.034 (0.034)\tLoss 1.0230 (1.0230)\tPrec@1 64.453 (64.453)\n",
      "Epoch: [196][9/48]\tTime 0.039 (0.030)\tLoss 1.0130 (1.0324)\tPrec@1 64.355 (63.926)\n",
      "Epoch: [196][18/48]\tTime 0.037 (0.043)\tLoss 1.0335 (1.0477)\tPrec@1 63.086 (63.029)\n",
      "Epoch: [196][27/48]\tTime 0.029 (0.042)\tLoss 1.1155 (1.0570)\tPrec@1 59.473 (62.699)\n",
      "Epoch: [196][36/48]\tTime 0.036 (0.045)\tLoss 1.1188 (1.0659)\tPrec@1 60.352 (62.381)\n",
      "Epoch: [196][45/48]\tTime 0.030 (0.042)\tLoss 1.1091 (1.0699)\tPrec@1 60.352 (62.137)\n",
      "Epoch: [196][48/48]\tTime 0.022 (0.041)\tLoss 1.0122 (1.0708)\tPrec@1 65.094 (62.144)\n",
      "EPOCH: 196 train Results: Prec@1 62.144 Loss: 1.0708\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.1444 (1.1444)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.004 (0.008)\tLoss 1.1568 (1.1596)\tPrec@1 56.378 (58.540)\n",
      "EPOCH: 196 val Results: Prec@1 58.540 Loss: 1.1596\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/48]\tTime 0.032 (0.032)\tLoss 1.0231 (1.0231)\tPrec@1 64.648 (64.648)\n",
      "Epoch: [197][9/48]\tTime 0.043 (0.029)\tLoss 1.0384 (1.0229)\tPrec@1 62.500 (63.838)\n",
      "Epoch: [197][18/48]\tTime 0.024 (0.031)\tLoss 1.0632 (1.0418)\tPrec@1 62.305 (63.333)\n",
      "Epoch: [197][27/48]\tTime 0.034 (0.030)\tLoss 1.0626 (1.0549)\tPrec@1 62.500 (62.898)\n",
      "Epoch: [197][36/48]\tTime 0.028 (0.035)\tLoss 1.0967 (1.0672)\tPrec@1 61.133 (62.545)\n",
      "Epoch: [197][45/48]\tTime 0.026 (0.034)\tLoss 1.0534 (1.0736)\tPrec@1 62.695 (62.292)\n",
      "Epoch: [197][48/48]\tTime 0.025 (0.034)\tLoss 1.1161 (1.0770)\tPrec@1 59.788 (62.112)\n",
      "EPOCH: 197 train Results: Prec@1 62.112 Loss: 1.0770\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1448 (1.1448)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.006 (0.009)\tLoss 1.1758 (1.1629)\tPrec@1 57.270 (58.220)\n",
      "EPOCH: 197 val Results: Prec@1 58.220 Loss: 1.1629\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/48]\tTime 0.022 (0.022)\tLoss 1.0041 (1.0041)\tPrec@1 65.723 (65.723)\n",
      "Epoch: [198][9/48]\tTime 0.025 (0.027)\tLoss 1.0856 (1.0438)\tPrec@1 60.449 (63.867)\n",
      "Epoch: [198][18/48]\tTime 0.040 (0.027)\tLoss 1.1056 (1.0580)\tPrec@1 61.426 (63.029)\n",
      "Epoch: [198][27/48]\tTime 0.042 (0.031)\tLoss 1.1478 (1.0638)\tPrec@1 59.082 (62.605)\n",
      "Epoch: [198][36/48]\tTime 0.025 (0.033)\tLoss 1.0795 (1.0671)\tPrec@1 61.426 (62.423)\n",
      "Epoch: [198][45/48]\tTime 0.033 (0.033)\tLoss 1.1070 (1.0717)\tPrec@1 61.328 (62.258)\n",
      "Epoch: [198][48/48]\tTime 0.029 (0.033)\tLoss 1.0933 (1.0731)\tPrec@1 60.259 (62.202)\n",
      "EPOCH: 198 train Results: Prec@1 62.202 Loss: 1.0731\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1449 (1.1449)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.1758 (1.1660)\tPrec@1 57.143 (58.210)\n",
      "EPOCH: 198 val Results: Prec@1 58.210 Loss: 1.1660\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/48]\tTime 0.216 (0.216)\tLoss 1.0615 (1.0615)\tPrec@1 62.695 (62.695)\n",
      "Epoch: [199][9/48]\tTime 0.071 (0.054)\tLoss 1.1080 (1.0399)\tPrec@1 61.621 (63.906)\n",
      "Epoch: [199][18/48]\tTime 0.028 (0.042)\tLoss 1.0883 (1.0393)\tPrec@1 60.938 (63.749)\n",
      "Epoch: [199][27/48]\tTime 0.045 (0.038)\tLoss 1.0897 (1.0535)\tPrec@1 61.328 (62.957)\n",
      "Epoch: [199][36/48]\tTime 0.039 (0.036)\tLoss 1.0902 (1.0617)\tPrec@1 60.938 (62.574)\n",
      "Epoch: [199][45/48]\tTime 0.024 (0.037)\tLoss 1.1145 (1.0710)\tPrec@1 59.277 (62.120)\n",
      "Epoch: [199][48/48]\tTime 0.024 (0.036)\tLoss 1.0530 (1.0732)\tPrec@1 61.203 (62.024)\n",
      "EPOCH: 199 train Results: Prec@1 62.024 Loss: 1.0732\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1492 (1.1492)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.1608 (1.1654)\tPrec@1 58.163 (58.770)\n",
      "EPOCH: 199 val Results: Prec@1 58.770 Loss: 1.1654\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/48]\tTime 0.020 (0.020)\tLoss 1.0633 (1.0633)\tPrec@1 63.477 (63.477)\n",
      "Epoch: [200][9/48]\tTime 0.051 (0.040)\tLoss 1.0807 (1.0571)\tPrec@1 62.988 (63.057)\n",
      "Epoch: [200][18/48]\tTime 0.048 (0.041)\tLoss 1.0878 (1.0625)\tPrec@1 62.598 (62.572)\n",
      "Epoch: [200][27/48]\tTime 0.049 (0.041)\tLoss 1.0895 (1.0620)\tPrec@1 60.156 (62.472)\n",
      "Epoch: [200][36/48]\tTime 0.020 (0.043)\tLoss 0.9842 (1.0618)\tPrec@1 64.648 (62.450)\n",
      "Epoch: [200][45/48]\tTime 0.047 (0.043)\tLoss 1.0180 (1.0689)\tPrec@1 64.160 (62.235)\n",
      "Epoch: [200][48/48]\tTime 0.027 (0.042)\tLoss 1.1502 (1.0707)\tPrec@1 59.198 (62.152)\n",
      "EPOCH: 200 train Results: Prec@1 62.152 Loss: 1.0707\n",
      "Test: [0/9]\tTime 0.023 (0.023)\tLoss 1.1494 (1.1494)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.010 (0.009)\tLoss 1.1605 (1.1653)\tPrec@1 57.526 (58.710)\n",
      "EPOCH: 200 val Results: Prec@1 58.710 Loss: 1.1653\n",
      "Best Prec@1: 59.040\n",
      "\n",
      "End time:  Sun Apr  7 21:43:16 2024\n",
      "train executed in 282.2339 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.3}},\n",
    "    #{'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    #{'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    {'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.3}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 1024\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',     # adam, sgd\n",
    "    'scheduler': None,      # cos, None\n",
    "    'pre-process': 'standardization',      # min-max, standardization, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_cm('./figs/cm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainer):\n",
    "    train_accu = trainer.train_precs\n",
    "    test_accu = trainer.test_precs\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_accu, 'r-', label='Training Accuracy')\n",
    "    plt.plot(x, test_accu, 'b-', label='Testing Accuracy')\n",
    "    plt.title('Training and Testing Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(trainer):\n",
    "    train_accu = trainer.train_losses\n",
    "    test_accu = trainer.test_losses\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_accu, 'r-', label='Training Loss')\n",
    "    plt.plot(x, test_accu, 'b-', label='Testing Loss')\n",
    "    plt.title('Training and Testing Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(trainer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
