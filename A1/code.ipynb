{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 520556528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85672763,  1.89363852, -0.25110738, -1.09727424],\n",
       "       [-0.23227008, -0.11473912, -1.69045798,  1.31190671],\n",
       "       [ 1.44085915,  0.37027676, -0.38280027,  0.4089585 ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.random.randn(3, 4)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Fri Mar 15 20:19:56 2024\n",
      "End time:  Fri Mar 15 20:19:57 2024\n",
      "test_fun executed in 1.0017 seconds\n"
     ]
    }
   ],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timer\n",
    "def test_fun(x):\n",
    "    time.sleep(x)\n",
    "\n",
    "test_fun(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py.\n",
    "\n",
    "Modify tensor to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45850532,  0.58140505,  0.29454185,  0.1586542 ,  0.71893752,\n",
       "        -0.0909816 ],\n",
       "       [ 0.13477889,  0.27962191, -0.11743695, -0.07979469,  0.12256677,\n",
       "        -0.55592582],\n",
       "       [-0.290316  , -0.08481382, -0.53662092,  0.0805716 ,  0.21629346,\n",
       "        -0.8638151 ],\n",
       "       [ 1.12419121,  1.25536453,  0.52773539,  0.26890837, -0.49224248,\n",
       "         0.62645091],\n",
       "       [ 0.11095123, -0.70820161,  0.66312001,  0.43883652, -0.61102968,\n",
       "         0.24015082]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    SELU              :math:`\\frac{3}{4}`\n",
    "    ================= ====================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'leaky_relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)\n",
    "    \n",
    "\n",
    "kaiming_normal_(np.array([0] * 30).reshape(5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    preds = output.argmax(axis=-1, keepdims=True)\n",
    "    return np.mean(preds == target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name \n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.89363852, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.31190671],\n",
       "       [1.44085915, 0.37027676, 0.        , 0.4089585 ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class relu(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_output[self.input <= 0] = 0\n",
    "        return grad_output\n",
    "    \n",
    "\n",
    "test_relu = relu('test_relu')\n",
    "_ = test_relu.forward(test_array)\n",
    "test_relu.backward(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.y = 1. / (1. + np.exp(-input))   # save sigmoid for more convenient grad computation\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return self.y * (1 - self.y) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10369682, 0.68890954, 0.10369682, 0.10369682],\n",
       "       [0.14895921, 0.14895921, 0.14895921, 0.55312236],\n",
       "       [0.5165657 , 0.17708327, 0.12228365, 0.18406737]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class softmax(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "            input.shape = [batch size, num_class]\n",
    "        \"\"\"\n",
    "        x_max = input.max(axis=-1, keepdims=True)       # to avoid overflow\n",
    "        x_exp = np.exp(input - x_max)\n",
    "        return x_exp / x_exp.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # packaged in CrossEntropyLoss\n",
    "        return grad_output\n",
    "\n",
    "softmax('test_softmax').forward(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: more activation, tanh, gelu, leaky_relu ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(Layer):\n",
    "    def __init__(self, name, in_num, out_num):\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.in_num = in_num\n",
    "        self.out_num = out_num\n",
    "\n",
    "        W = kaiming_normal_(np.array([0] * in_num * out_num).reshape(in_num, out_num), a=math.sqrt(5))     # Kaiming Init\n",
    "        self.W = Parameter(W, self.requires_grad)\n",
    "        self.b = Parameter(np.zeros(out_num), self.requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return input @ self.W.data + self.b.data      # [batch size, in_num] @ [in_num, out_num] + [out_num] => [batch size, out_num]\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "            grad_output: [batch size, out_num]\n",
    "        \"\"\"\n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.W.grad = self.input.T @ grad_output / batch_size\n",
    "        self.b.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return grad_output @ self.W.data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = softmax('softmax')\n",
    "\n",
    "    def __call__(self, input, ground_truth):\n",
    "        self.bacth_size = input.shape[0]\n",
    "        self.class_num = input.shape[1]\n",
    "\n",
    "        preds = self.softmax.forward(input)\n",
    "        ground_truth = self.one_hot_encoding(ground_truth)\n",
    "\n",
    "        self.grad = preds - ground_truth    #TODO: 推导要写在report上不？\n",
    "\n",
    "        loss = -1 * (ground_truth * np.log(preds + 1e-8)).sum() / self.bacth_size\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.bacth_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.size), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.num_layers = 0\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if layer.requires_grad:\n",
    "            if hasattr(layer, 'W'):\n",
    "                self.params.append(layer.W)\n",
    "            if hasattr(layer, 'b'):\n",
    "                self.params.append(layer.b)\n",
    "            if hasattr(layer, 'gamma'):\n",
    "                self.params.append(layer.gamma)\n",
    "            if hasattr(layer, 'beta'):\n",
    "                self.params.append(layer.beta)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x):\n",
    "        for layer in self.layers[::-1]:\n",
    "            x = layer.backward(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = True\n",
    "    \n",
    "    def test(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, momentum, lr, weight_decay):\n",
    "        self.parameters = parameters\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        for i, (v, p) in enumerate(zip(self.v, self.parameters)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            v = self.momentum * v + self.lr * p.grad\n",
    "            self.v[i] = v\n",
    "            p.data -= self.v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLR(object):\n",
    "    def __init__(self, optimizer, T_max):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.n = -1\n",
    "        self.base_lr = optimizer.lr\n",
    "        self.step()\n",
    "\n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        lr = self.get_lr()\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        cos = np.cos(np.pi * self.n / self.T_max)\n",
    "        return self.base_lr * (1 + cos) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, config, model=None, train_loader=None, val_loader=None):\n",
    "        self.config = config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr = self.config['lr']\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.optimizer = SGD(self.model.params, self.config['momentum'], self.lr, self.config['weight_decay'])\n",
    "        self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "\n",
    "    def train(self):\n",
    "        best_acc1 = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            self.train_scheduler.step()\n",
    "\n",
    "            # evaluate on validation set\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "\n",
    "    \n",
    "    def train_per_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        data_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        for i, (input, target) in enumerate(self.train_loader):\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            self.model.backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch, i, len(self.train_loader), batch_time=batch_time,\n",
    "                        data_time=data_time, loss=losses, top1=top1))\n",
    "                \n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.test()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        i, len(self.val_loader), batch_time=batch_time, loss=losses,\n",
    "                        top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} '.format(epoch=epoch + 1 , flag='val', top1=top1))\n",
    "        print(output)\n",
    "\n",
    "        return top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "        \n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Assignment1-Dataset/'\n",
    "\n",
    "train_X = np.load(file_path + 'train_data.npy')\n",
    "train_y = np.load(file_path + 'train_label.npy')\n",
    "test_X = np.load(file_path + 'test_data.npy')\n",
    "test_y = np.load(file_path + 'test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([train_y[i][0] for i in range(train_y.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current lr 1.00000e-01\n",
      "Epoch: [0][0/49]\tTime 0.011 (0.011)\tData 0.002 (0.002)\tLoss 2.7202 (2.7202)\tPrec@1 0.100 (0.100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.6218 (1.6218)\tPrec@1 0.431 (0.431)\n",
      "EPOCH: 1 val Results: Prec@1 0.428 \n",
      "Best Prec@1: 0.428\n",
      "\n",
      "current lr 9.99753e-02\n",
      "Epoch: [1][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.5995 (1.5995)\tPrec@1 0.443 (0.443)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.5370 (1.5370)\tPrec@1 0.444 (0.444)\n",
      "EPOCH: 2 val Results: Prec@1 0.453 \n",
      "Best Prec@1: 0.453\n",
      "\n",
      "current lr 9.99013e-02\n",
      "Epoch: [2][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.5724 (1.5724)\tPrec@1 0.460 (0.460)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.4811 (1.4811)\tPrec@1 0.468 (0.468)\n",
      "EPOCH: 3 val Results: Prec@1 0.475 \n",
      "Best Prec@1: 0.475\n",
      "\n",
      "current lr 9.97781e-02\n",
      "Epoch: [3][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.4724 (1.4724)\tPrec@1 0.496 (0.496)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.4243 (1.4243)\tPrec@1 0.493 (0.493)\n",
      "EPOCH: 4 val Results: Prec@1 0.484 \n",
      "Best Prec@1: 0.484\n",
      "\n",
      "current lr 9.96057e-02\n",
      "Epoch: [4][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.4171 (1.4171)\tPrec@1 0.491 (0.491)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.4077 (1.4077)\tPrec@1 0.507 (0.507)\n",
      "EPOCH: 5 val Results: Prec@1 0.486 \n",
      "Best Prec@1: 0.486\n",
      "\n",
      "current lr 9.93844e-02\n",
      "Epoch: [5][0/49]\tTime 0.007 (0.007)\tData 0.001 (0.001)\tLoss 1.3916 (1.3916)\tPrec@1 0.504 (0.504)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3846 (1.3846)\tPrec@1 0.521 (0.521)\n",
      "EPOCH: 6 val Results: Prec@1 0.497 \n",
      "Best Prec@1: 0.497\n",
      "\n",
      "current lr 9.91144e-02\n",
      "Epoch: [6][0/49]\tTime 0.011 (0.011)\tData 0.002 (0.002)\tLoss 1.3892 (1.3892)\tPrec@1 0.505 (0.505)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3715 (1.3715)\tPrec@1 0.507 (0.507)\n",
      "EPOCH: 7 val Results: Prec@1 0.499 \n",
      "Best Prec@1: 0.499\n",
      "\n",
      "current lr 9.87958e-02\n",
      "Epoch: [7][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.3410 (1.3410)\tPrec@1 0.547 (0.547)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3486 (1.3486)\tPrec@1 0.526 (0.526)\n",
      "EPOCH: 8 val Results: Prec@1 0.504 \n",
      "Best Prec@1: 0.504\n",
      "\n",
      "current lr 9.84292e-02\n",
      "Epoch: [8][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.3385 (1.3385)\tPrec@1 0.517 (0.517)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3443 (1.3443)\tPrec@1 0.528 (0.528)\n",
      "EPOCH: 9 val Results: Prec@1 0.503 \n",
      "Best Prec@1: 0.504\n",
      "\n",
      "current lr 9.80147e-02\n",
      "Epoch: [9][0/49]\tTime 0.012 (0.012)\tData 0.001 (0.001)\tLoss 1.2902 (1.2902)\tPrec@1 0.562 (0.562)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3632 (1.3632)\tPrec@1 0.516 (0.516)\n",
      "EPOCH: 10 val Results: Prec@1 0.500 \n",
      "Best Prec@1: 0.504\n",
      "\n",
      "current lr 9.75528e-02\n",
      "Epoch: [10][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.3219 (1.3219)\tPrec@1 0.531 (0.531)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3524 (1.3524)\tPrec@1 0.519 (0.519)\n",
      "EPOCH: 11 val Results: Prec@1 0.502 \n",
      "Best Prec@1: 0.504\n",
      "\n",
      "current lr 9.70440e-02\n",
      "Epoch: [11][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.3308 (1.3308)\tPrec@1 0.541 (0.541)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3240 (1.3240)\tPrec@1 0.532 (0.532)\n",
      "EPOCH: 12 val Results: Prec@1 0.511 \n",
      "Best Prec@1: 0.511\n",
      "\n",
      "current lr 9.64888e-02\n",
      "Epoch: [12][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.3206 (1.3206)\tPrec@1 0.541 (0.541)\n",
      "Test: [0/10]\tTime 0.006 (0.006)\tLoss 1.3500 (1.3500)\tPrec@1 0.527 (0.527)\n",
      "EPOCH: 13 val Results: Prec@1 0.504 \n",
      "Best Prec@1: 0.511\n",
      "\n",
      "current lr 9.58877e-02\n",
      "Epoch: [13][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2914 (1.2914)\tPrec@1 0.559 (0.559)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3298 (1.3298)\tPrec@1 0.525 (0.525)\n",
      "EPOCH: 14 val Results: Prec@1 0.509 \n",
      "Best Prec@1: 0.511\n",
      "\n",
      "current lr 9.52414e-02\n",
      "Epoch: [14][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.2569 (1.2569)\tPrec@1 0.550 (0.550)\n",
      "Test: [0/10]\tTime 0.000 (0.000)\tLoss 1.3266 (1.3266)\tPrec@1 0.521 (0.521)\n",
      "EPOCH: 15 val Results: Prec@1 0.509 \n",
      "Best Prec@1: 0.511\n",
      "\n",
      "current lr 9.45503e-02\n",
      "Epoch: [15][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2557 (1.2557)\tPrec@1 0.556 (0.556)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3122 (1.3122)\tPrec@1 0.530 (0.530)\n",
      "EPOCH: 16 val Results: Prec@1 0.510 \n",
      "Best Prec@1: 0.511\n",
      "\n",
      "current lr 9.38153e-02\n",
      "Epoch: [16][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.2346 (1.2346)\tPrec@1 0.571 (0.571)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3179 (1.3179)\tPrec@1 0.535 (0.535)\n",
      "EPOCH: 17 val Results: Prec@1 0.513 \n",
      "Best Prec@1: 0.513\n",
      "\n",
      "current lr 9.30371e-02\n",
      "Epoch: [17][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.2463 (1.2463)\tPrec@1 0.563 (0.563)\n",
      "Test: [0/10]\tTime 0.004 (0.004)\tLoss 1.2946 (1.2946)\tPrec@1 0.552 (0.552)\n",
      "EPOCH: 18 val Results: Prec@1 0.518 \n",
      "Best Prec@1: 0.518\n",
      "\n",
      "current lr 9.22164e-02\n",
      "Epoch: [18][0/49]\tTime 0.021 (0.021)\tData 0.001 (0.001)\tLoss 1.2695 (1.2695)\tPrec@1 0.557 (0.557)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3095 (1.3095)\tPrec@1 0.532 (0.532)\n",
      "EPOCH: 19 val Results: Prec@1 0.521 \n",
      "Best Prec@1: 0.521\n",
      "\n",
      "current lr 9.13540e-02\n",
      "Epoch: [19][0/49]\tTime 0.009 (0.009)\tData 0.002 (0.002)\tLoss 1.2557 (1.2557)\tPrec@1 0.556 (0.556)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2928 (1.2928)\tPrec@1 0.539 (0.539)\n",
      "EPOCH: 20 val Results: Prec@1 0.516 \n",
      "Best Prec@1: 0.521\n",
      "\n",
      "current lr 9.04508e-02\n",
      "Epoch: [20][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.2309 (1.2309)\tPrec@1 0.589 (0.589)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3091 (1.3091)\tPrec@1 0.533 (0.533)\n",
      "EPOCH: 21 val Results: Prec@1 0.515 \n",
      "Best Prec@1: 0.521\n",
      "\n",
      "current lr 8.95078e-02\n",
      "Epoch: [21][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.2642 (1.2642)\tPrec@1 0.547 (0.547)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3071 (1.3071)\tPrec@1 0.543 (0.543)\n",
      "EPOCH: 22 val Results: Prec@1 0.519 \n",
      "Best Prec@1: 0.521\n",
      "\n",
      "current lr 8.85257e-02\n",
      "Epoch: [22][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2320 (1.2320)\tPrec@1 0.574 (0.574)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3055 (1.3055)\tPrec@1 0.547 (0.547)\n",
      "EPOCH: 23 val Results: Prec@1 0.521 \n",
      "Best Prec@1: 0.521\n",
      "\n",
      "current lr 8.75056e-02\n",
      "Epoch: [23][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2512 (1.2512)\tPrec@1 0.556 (0.556)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3008 (1.3008)\tPrec@1 0.541 (0.541)\n",
      "EPOCH: 24 val Results: Prec@1 0.512 \n",
      "Best Prec@1: 0.521\n",
      "\n",
      "current lr 8.64484e-02\n",
      "Epoch: [24][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2459 (1.2459)\tPrec@1 0.556 (0.556)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3071 (1.3071)\tPrec@1 0.535 (0.535)\n",
      "EPOCH: 25 val Results: Prec@1 0.520 \n",
      "Best Prec@1: 0.521\n",
      "\n",
      "current lr 8.53553e-02\n",
      "Epoch: [25][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2250 (1.2250)\tPrec@1 0.569 (0.569)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3419 (1.3419)\tPrec@1 0.519 (0.519)\n",
      "EPOCH: 26 val Results: Prec@1 0.512 \n",
      "Best Prec@1: 0.521\n",
      "\n",
      "current lr 8.42274e-02\n",
      "Epoch: [26][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2449 (1.2449)\tPrec@1 0.571 (0.571)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2919 (1.2919)\tPrec@1 0.542 (0.542)\n",
      "EPOCH: 27 val Results: Prec@1 0.525 \n",
      "Best Prec@1: 0.525\n",
      "\n",
      "current lr 8.30656e-02\n",
      "Epoch: [27][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1827 (1.1827)\tPrec@1 0.573 (0.573)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3017 (1.3017)\tPrec@1 0.526 (0.526)\n",
      "EPOCH: 28 val Results: Prec@1 0.518 \n",
      "Best Prec@1: 0.525\n",
      "\n",
      "current lr 8.18712e-02\n",
      "Epoch: [28][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1518 (1.1518)\tPrec@1 0.624 (0.624)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2921 (1.2921)\tPrec@1 0.549 (0.549)\n",
      "EPOCH: 29 val Results: Prec@1 0.521 \n",
      "Best Prec@1: 0.525\n",
      "\n",
      "current lr 8.06454e-02\n",
      "Epoch: [29][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2449 (1.2449)\tPrec@1 0.550 (0.550)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2891 (1.2891)\tPrec@1 0.541 (0.541)\n",
      "EPOCH: 30 val Results: Prec@1 0.526 \n",
      "Best Prec@1: 0.526\n",
      "\n",
      "current lr 7.93893e-02\n",
      "Epoch: [30][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.1890 (1.1890)\tPrec@1 0.596 (0.596)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2914 (1.2914)\tPrec@1 0.545 (0.545)\n",
      "EPOCH: 31 val Results: Prec@1 0.526 \n",
      "Best Prec@1: 0.526\n",
      "\n",
      "current lr 7.81042e-02\n",
      "Epoch: [31][0/49]\tTime 0.015 (0.015)\tData 0.001 (0.001)\tLoss 1.2042 (1.2042)\tPrec@1 0.575 (0.575)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3056 (1.3056)\tPrec@1 0.543 (0.543)\n",
      "EPOCH: 32 val Results: Prec@1 0.520 \n",
      "Best Prec@1: 0.526\n",
      "\n",
      "current lr 7.67913e-02\n",
      "Epoch: [32][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2666 (1.2666)\tPrec@1 0.543 (0.543)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2997 (1.2997)\tPrec@1 0.542 (0.542)\n",
      "EPOCH: 33 val Results: Prec@1 0.521 \n",
      "Best Prec@1: 0.526\n",
      "\n",
      "current lr 7.54521e-02\n",
      "Epoch: [33][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1739 (1.1739)\tPrec@1 0.590 (0.590)\n",
      "Test: [0/10]\tTime 0.009 (0.009)\tLoss 1.3051 (1.3051)\tPrec@1 0.540 (0.540)\n",
      "EPOCH: 34 val Results: Prec@1 0.518 \n",
      "Best Prec@1: 0.526\n",
      "\n",
      "current lr 7.40877e-02\n",
      "Epoch: [34][0/49]\tTime 0.007 (0.007)\tData 0.002 (0.002)\tLoss 1.2193 (1.2193)\tPrec@1 0.568 (0.568)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3157 (1.3157)\tPrec@1 0.538 (0.538)\n",
      "EPOCH: 35 val Results: Prec@1 0.521 \n",
      "Best Prec@1: 0.526\n",
      "\n",
      "current lr 7.26995e-02\n",
      "Epoch: [35][0/49]\tTime 0.012 (0.012)\tData 0.001 (0.001)\tLoss 1.2069 (1.2069)\tPrec@1 0.576 (0.576)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2960 (1.2960)\tPrec@1 0.529 (0.529)\n",
      "EPOCH: 36 val Results: Prec@1 0.524 \n",
      "Best Prec@1: 0.526\n",
      "\n",
      "current lr 7.12890e-02\n",
      "Epoch: [36][0/49]\tTime 0.008 (0.008)\tData 0.002 (0.002)\tLoss 1.1363 (1.1363)\tPrec@1 0.615 (0.615)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2954 (1.2954)\tPrec@1 0.549 (0.549)\n",
      "EPOCH: 37 val Results: Prec@1 0.527 \n",
      "Best Prec@1: 0.527\n",
      "\n",
      "current lr 6.98574e-02\n",
      "Epoch: [37][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.2085 (1.2085)\tPrec@1 0.577 (0.577)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2963 (1.2963)\tPrec@1 0.528 (0.528)\n",
      "EPOCH: 38 val Results: Prec@1 0.522 \n",
      "Best Prec@1: 0.527\n",
      "\n",
      "current lr 6.84062e-02\n",
      "Epoch: [38][0/49]\tTime 0.009 (0.009)\tData 0.001 (0.001)\tLoss 1.1610 (1.1610)\tPrec@1 0.583 (0.583)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2715 (1.2715)\tPrec@1 0.536 (0.536)\n",
      "EPOCH: 39 val Results: Prec@1 0.526 \n",
      "Best Prec@1: 0.527\n",
      "\n",
      "current lr 6.69369e-02\n",
      "Epoch: [39][0/49]\tTime 0.009 (0.009)\tData 0.001 (0.001)\tLoss 1.1887 (1.1887)\tPrec@1 0.584 (0.584)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2803 (1.2803)\tPrec@1 0.536 (0.536)\n",
      "EPOCH: 40 val Results: Prec@1 0.525 \n",
      "Best Prec@1: 0.527\n",
      "\n",
      "current lr 6.54508e-02\n",
      "Epoch: [40][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2165 (1.2165)\tPrec@1 0.562 (0.562)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2795 (1.2795)\tPrec@1 0.552 (0.552)\n",
      "EPOCH: 41 val Results: Prec@1 0.526 \n",
      "Best Prec@1: 0.527\n",
      "\n",
      "current lr 6.39496e-02\n",
      "Epoch: [41][0/49]\tTime 0.007 (0.007)\tData 0.001 (0.001)\tLoss 1.1637 (1.1637)\tPrec@1 0.610 (0.610)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2974 (1.2974)\tPrec@1 0.541 (0.541)\n",
      "EPOCH: 42 val Results: Prec@1 0.520 \n",
      "Best Prec@1: 0.527\n",
      "\n",
      "current lr 6.24345e-02\n",
      "Epoch: [42][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1718 (1.1718)\tPrec@1 0.587 (0.587)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2857 (1.2857)\tPrec@1 0.552 (0.552)\n",
      "EPOCH: 43 val Results: Prec@1 0.525 \n",
      "Best Prec@1: 0.527\n",
      "\n",
      "current lr 6.09072e-02\n",
      "Epoch: [43][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2011 (1.2011)\tPrec@1 0.579 (0.579)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2921 (1.2921)\tPrec@1 0.535 (0.535)\n",
      "EPOCH: 44 val Results: Prec@1 0.524 \n",
      "Best Prec@1: 0.527\n",
      "\n",
      "current lr 5.93691e-02\n",
      "Epoch: [44][0/49]\tTime 0.026 (0.026)\tData 0.001 (0.001)\tLoss 1.1623 (1.1623)\tPrec@1 0.590 (0.590)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2785 (1.2785)\tPrec@1 0.538 (0.538)\n",
      "EPOCH: 45 val Results: Prec@1 0.528 \n",
      "Best Prec@1: 0.528\n",
      "\n",
      "current lr 5.78217e-02\n",
      "Epoch: [45][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2154 (1.2154)\tPrec@1 0.596 (0.596)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2857 (1.2857)\tPrec@1 0.537 (0.537)\n",
      "EPOCH: 46 val Results: Prec@1 0.523 \n",
      "Best Prec@1: 0.528\n",
      "\n",
      "current lr 5.62667e-02\n",
      "Epoch: [46][0/49]\tTime 0.006 (0.006)\tData 0.001 (0.001)\tLoss 1.1915 (1.1915)\tPrec@1 0.576 (0.576)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2765 (1.2765)\tPrec@1 0.544 (0.544)\n",
      "EPOCH: 47 val Results: Prec@1 0.529 \n",
      "Best Prec@1: 0.529\n",
      "\n",
      "current lr 5.47054e-02\n",
      "Epoch: [47][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.1874 (1.1874)\tPrec@1 0.594 (0.594)\n",
      "Test: [0/10]\tTime 0.002 (0.002)\tLoss 1.2703 (1.2703)\tPrec@1 0.562 (0.562)\n",
      "EPOCH: 48 val Results: Prec@1 0.529 \n",
      "Best Prec@1: 0.529\n",
      "\n",
      "current lr 5.31395e-02\n",
      "Epoch: [48][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1835 (1.1835)\tPrec@1 0.604 (0.604)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2739 (1.2739)\tPrec@1 0.526 (0.526)\n",
      "EPOCH: 49 val Results: Prec@1 0.525 \n",
      "Best Prec@1: 0.529\n",
      "\n",
      "current lr 5.15705e-02\n",
      "Epoch: [49][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.1704 (1.1704)\tPrec@1 0.600 (0.600)\n",
      "Test: [0/10]\tTime 0.002 (0.002)\tLoss 1.2767 (1.2767)\tPrec@1 0.540 (0.540)\n",
      "EPOCH: 50 val Results: Prec@1 0.524 \n",
      "Best Prec@1: 0.529\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [50][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1860 (1.1860)\tPrec@1 0.563 (0.563)\n",
      "Test: [0/10]\tTime 0.002 (0.002)\tLoss 1.2679 (1.2679)\tPrec@1 0.538 (0.538)\n",
      "EPOCH: 51 val Results: Prec@1 0.527 \n",
      "Best Prec@1: 0.529\n",
      "\n",
      "current lr 4.84295e-02\n",
      "Epoch: [51][0/49]\tTime 0.004 (0.004)\tData 0.002 (0.002)\tLoss 1.1683 (1.1683)\tPrec@1 0.594 (0.594)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2678 (1.2678)\tPrec@1 0.541 (0.541)\n",
      "EPOCH: 52 val Results: Prec@1 0.526 \n",
      "Best Prec@1: 0.529\n",
      "\n",
      "current lr 4.68605e-02\n",
      "Epoch: [52][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1725 (1.1725)\tPrec@1 0.590 (0.590)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2744 (1.2744)\tPrec@1 0.531 (0.531)\n",
      "EPOCH: 53 val Results: Prec@1 0.525 \n",
      "Best Prec@1: 0.529\n",
      "\n",
      "current lr 4.52946e-02\n",
      "Epoch: [53][0/49]\tTime 0.009 (0.009)\tData 0.001 (0.001)\tLoss 1.1647 (1.1647)\tPrec@1 0.595 (0.595)\n",
      "Test: [0/10]\tTime 0.002 (0.002)\tLoss 1.2690 (1.2690)\tPrec@1 0.547 (0.547)\n",
      "EPOCH: 54 val Results: Prec@1 0.533 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 4.37333e-02\n",
      "Epoch: [54][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1202 (1.1202)\tPrec@1 0.612 (0.612)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2743 (1.2743)\tPrec@1 0.545 (0.545)\n",
      "EPOCH: 55 val Results: Prec@1 0.527 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 4.21783e-02\n",
      "Epoch: [55][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1841 (1.1841)\tPrec@1 0.576 (0.576)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2584 (1.2584)\tPrec@1 0.557 (0.557)\n",
      "EPOCH: 56 val Results: Prec@1 0.531 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 4.06309e-02\n",
      "Epoch: [56][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1729 (1.1729)\tPrec@1 0.583 (0.583)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2644 (1.2644)\tPrec@1 0.537 (0.537)\n",
      "EPOCH: 57 val Results: Prec@1 0.529 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 3.90928e-02\n",
      "Epoch: [57][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1485 (1.1485)\tPrec@1 0.580 (0.580)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2669 (1.2669)\tPrec@1 0.545 (0.545)\n",
      "EPOCH: 58 val Results: Prec@1 0.530 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 3.75655e-02\n",
      "Epoch: [58][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.2394 (1.2394)\tPrec@1 0.572 (0.572)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2510 (1.2510)\tPrec@1 0.552 (0.552)\n",
      "EPOCH: 59 val Results: Prec@1 0.528 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 3.60504e-02\n",
      "Epoch: [59][0/49]\tTime 0.006 (0.006)\tData 0.001 (0.001)\tLoss 1.1486 (1.1486)\tPrec@1 0.603 (0.603)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2763 (1.2763)\tPrec@1 0.541 (0.541)\n",
      "EPOCH: 60 val Results: Prec@1 0.528 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 3.45492e-02\n",
      "Epoch: [60][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1289 (1.1289)\tPrec@1 0.613 (0.613)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2524 (1.2524)\tPrec@1 0.551 (0.551)\n",
      "EPOCH: 61 val Results: Prec@1 0.530 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 3.30631e-02\n",
      "Epoch: [61][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1362 (1.1362)\tPrec@1 0.625 (0.625)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2696 (1.2696)\tPrec@1 0.548 (0.548)\n",
      "EPOCH: 62 val Results: Prec@1 0.531 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 3.15938e-02\n",
      "Epoch: [62][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1734 (1.1734)\tPrec@1 0.577 (0.577)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2542 (1.2542)\tPrec@1 0.543 (0.543)\n",
      "EPOCH: 63 val Results: Prec@1 0.531 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 3.01426e-02\n",
      "Epoch: [63][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1587 (1.1587)\tPrec@1 0.587 (0.587)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2526 (1.2526)\tPrec@1 0.551 (0.551)\n",
      "EPOCH: 64 val Results: Prec@1 0.533 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 2.87110e-02\n",
      "Epoch: [64][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 0.579 (0.579)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2555 (1.2555)\tPrec@1 0.542 (0.542)\n",
      "EPOCH: 65 val Results: Prec@1 0.533 \n",
      "Best Prec@1: 0.533\n",
      "\n",
      "current lr 2.73005e-02\n",
      "Epoch: [65][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.1529 (1.1529)\tPrec@1 0.600 (0.600)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2605 (1.2605)\tPrec@1 0.556 (0.556)\n",
      "EPOCH: 66 val Results: Prec@1 0.534 \n",
      "Best Prec@1: 0.534\n",
      "\n",
      "current lr 2.59123e-02\n",
      "Epoch: [66][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.1095 (1.1095)\tPrec@1 0.616 (0.616)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2484 (1.2484)\tPrec@1 0.556 (0.556)\n",
      "EPOCH: 67 val Results: Prec@1 0.538 \n",
      "Best Prec@1: 0.538\n",
      "\n",
      "current lr 2.45479e-02\n",
      "Epoch: [67][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1185 (1.1185)\tPrec@1 0.632 (0.632)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2599 (1.2599)\tPrec@1 0.552 (0.552)\n",
      "EPOCH: 68 val Results: Prec@1 0.531 \n",
      "Best Prec@1: 0.538\n",
      "\n",
      "current lr 2.32087e-02\n",
      "Epoch: [68][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.1208 (1.1208)\tPrec@1 0.607 (0.607)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2520 (1.2520)\tPrec@1 0.559 (0.559)\n",
      "EPOCH: 69 val Results: Prec@1 0.537 \n",
      "Best Prec@1: 0.538\n",
      "\n",
      "current lr 2.18958e-02\n",
      "Epoch: [69][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.1341 (1.1341)\tPrec@1 0.611 (0.611)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2580 (1.2580)\tPrec@1 0.550 (0.550)\n",
      "EPOCH: 70 val Results: Prec@1 0.530 \n",
      "Best Prec@1: 0.538\n",
      "\n",
      "current lr 2.06107e-02\n",
      "Epoch: [70][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.1725 (1.1725)\tPrec@1 0.584 (0.584)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2520 (1.2520)\tPrec@1 0.558 (0.558)\n",
      "EPOCH: 71 val Results: Prec@1 0.537 \n",
      "Best Prec@1: 0.538\n",
      "\n",
      "current lr 1.93546e-02\n",
      "Epoch: [71][0/49]\tTime 0.006 (0.006)\tData 0.002 (0.002)\tLoss 1.1168 (1.1168)\tPrec@1 0.602 (0.602)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2538 (1.2538)\tPrec@1 0.550 (0.550)\n",
      "EPOCH: 72 val Results: Prec@1 0.534 \n",
      "Best Prec@1: 0.538\n",
      "\n",
      "current lr 1.81288e-02\n",
      "Epoch: [72][0/49]\tTime 0.006 (0.006)\tData 0.002 (0.002)\tLoss 1.1960 (1.1960)\tPrec@1 0.596 (0.596)\n",
      "Test: [0/10]\tTime 0.004 (0.004)\tLoss 1.2584 (1.2584)\tPrec@1 0.555 (0.555)\n",
      "EPOCH: 73 val Results: Prec@1 0.540 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.69344e-02\n",
      "Epoch: [73][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1861 (1.1861)\tPrec@1 0.586 (0.586)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2572 (1.2572)\tPrec@1 0.556 (0.556)\n",
      "EPOCH: 74 val Results: Prec@1 0.536 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.57726e-02\n",
      "Epoch: [74][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1377 (1.1377)\tPrec@1 0.601 (0.601)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2512 (1.2512)\tPrec@1 0.554 (0.554)\n",
      "EPOCH: 75 val Results: Prec@1 0.536 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.46447e-02\n",
      "Epoch: [75][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2798 (1.2798)\tPrec@1 0.561 (0.561)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2584 (1.2584)\tPrec@1 0.549 (0.549)\n",
      "EPOCH: 76 val Results: Prec@1 0.532 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.35516e-02\n",
      "Epoch: [76][0/49]\tTime 0.006 (0.006)\tData 0.001 (0.001)\tLoss 1.1696 (1.1696)\tPrec@1 0.593 (0.593)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2554 (1.2554)\tPrec@1 0.555 (0.555)\n",
      "EPOCH: 77 val Results: Prec@1 0.539 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.24944e-02\n",
      "Epoch: [77][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.2069 (1.2069)\tPrec@1 0.578 (0.578)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2556 (1.2556)\tPrec@1 0.559 (0.559)\n",
      "EPOCH: 78 val Results: Prec@1 0.539 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.14743e-02\n",
      "Epoch: [78][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1334 (1.1334)\tPrec@1 0.628 (0.628)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2585 (1.2585)\tPrec@1 0.554 (0.554)\n",
      "EPOCH: 79 val Results: Prec@1 0.537 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.04922e-02\n",
      "Epoch: [79][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1934 (1.1934)\tPrec@1 0.583 (0.583)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2622 (1.2622)\tPrec@1 0.554 (0.554)\n",
      "EPOCH: 80 val Results: Prec@1 0.537 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 9.54915e-03\n",
      "Epoch: [80][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.1933 (1.1933)\tPrec@1 0.568 (0.568)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2601 (1.2601)\tPrec@1 0.557 (0.557)\n",
      "EPOCH: 81 val Results: Prec@1 0.538 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 8.64597e-03\n",
      "Epoch: [81][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.2176 (1.2176)\tPrec@1 0.594 (0.594)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2597 (1.2597)\tPrec@1 0.555 (0.555)\n",
      "EPOCH: 82 val Results: Prec@1 0.537 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 7.78360e-03\n",
      "Epoch: [82][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.2433 (1.2433)\tPrec@1 0.571 (0.571)\n",
      "Test: [0/10]\tTime 0.002 (0.002)\tLoss 1.2659 (1.2659)\tPrec@1 0.556 (0.556)\n",
      "EPOCH: 83 val Results: Prec@1 0.537 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 6.96290e-03\n",
      "Epoch: [83][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1494 (1.1494)\tPrec@1 0.623 (0.623)\n",
      "Test: [0/10]\tTime 0.004 (0.004)\tLoss 1.2693 (1.2693)\tPrec@1 0.551 (0.551)\n",
      "EPOCH: 84 val Results: Prec@1 0.536 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 6.18467e-03\n",
      "Epoch: [84][0/49]\tTime 0.007 (0.007)\tData 0.001 (0.001)\tLoss 1.1681 (1.1681)\tPrec@1 0.599 (0.599)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2706 (1.2706)\tPrec@1 0.554 (0.554)\n",
      "EPOCH: 85 val Results: Prec@1 0.536 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 5.44967e-03\n",
      "Epoch: [85][0/49]\tTime 0.012 (0.012)\tData 0.001 (0.001)\tLoss 1.2174 (1.2174)\tPrec@1 0.605 (0.605)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2745 (1.2745)\tPrec@1 0.556 (0.556)\n",
      "EPOCH: 86 val Results: Prec@1 0.536 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 4.75865e-03\n",
      "Epoch: [86][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.1956 (1.1956)\tPrec@1 0.596 (0.596)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2795 (1.2795)\tPrec@1 0.559 (0.559)\n",
      "EPOCH: 87 val Results: Prec@1 0.535 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 4.11227e-03\n",
      "Epoch: [87][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.1658 (1.1658)\tPrec@1 0.620 (0.620)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2832 (1.2832)\tPrec@1 0.560 (0.560)\n",
      "EPOCH: 88 val Results: Prec@1 0.534 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 3.51118e-03\n",
      "Epoch: [88][0/49]\tTime 0.005 (0.005)\tData 0.001 (0.001)\tLoss 1.2597 (1.2597)\tPrec@1 0.555 (0.555)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2888 (1.2888)\tPrec@1 0.555 (0.555)\n",
      "EPOCH: 89 val Results: Prec@1 0.535 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 2.95596e-03\n",
      "Epoch: [89][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.2246 (1.2246)\tPrec@1 0.590 (0.590)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.2953 (1.2953)\tPrec@1 0.554 (0.554)\n",
      "EPOCH: 90 val Results: Prec@1 0.534 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 2.44717e-03\n",
      "Epoch: [90][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2548 (1.2548)\tPrec@1 0.557 (0.557)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3018 (1.3018)\tPrec@1 0.555 (0.555)\n",
      "EPOCH: 91 val Results: Prec@1 0.534 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.98532e-03\n",
      "Epoch: [91][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.3119 (1.3119)\tPrec@1 0.541 (0.541)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3106 (1.3106)\tPrec@1 0.554 (0.554)\n",
      "EPOCH: 92 val Results: Prec@1 0.534 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.57084e-03\n",
      "Epoch: [92][0/49]\tTime 0.006 (0.006)\tData 0.001 (0.001)\tLoss 1.2654 (1.2654)\tPrec@1 0.573 (0.573)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3181 (1.3181)\tPrec@1 0.553 (0.553)\n",
      "EPOCH: 93 val Results: Prec@1 0.532 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 1.20416e-03\n",
      "Epoch: [93][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.2778 (1.2778)\tPrec@1 0.583 (0.583)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3276 (1.3276)\tPrec@1 0.554 (0.554)\n",
      "EPOCH: 94 val Results: Prec@1 0.532 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 8.85637e-04\n",
      "Epoch: [94][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.3259 (1.3259)\tPrec@1 0.567 (0.567)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3389 (1.3389)\tPrec@1 0.553 (0.553)\n",
      "EPOCH: 95 val Results: Prec@1 0.531 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 6.15583e-04\n",
      "Epoch: [95][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.3048 (1.3048)\tPrec@1 0.583 (0.583)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3504 (1.3504)\tPrec@1 0.555 (0.555)\n",
      "EPOCH: 96 val Results: Prec@1 0.530 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 3.94265e-04\n",
      "Epoch: [96][0/49]\tTime 0.006 (0.006)\tData 0.001 (0.001)\tLoss 1.3409 (1.3409)\tPrec@1 0.551 (0.551)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3636 (1.3636)\tPrec@1 0.557 (0.557)\n",
      "EPOCH: 97 val Results: Prec@1 0.528 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 2.21902e-04\n",
      "Epoch: [97][0/49]\tTime 0.003 (0.003)\tData 0.001 (0.001)\tLoss 1.3683 (1.3683)\tPrec@1 0.557 (0.557)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.3784 (1.3784)\tPrec@1 0.556 (0.556)\n",
      "EPOCH: 98 val Results: Prec@1 0.528 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 9.86636e-05\n",
      "Epoch: [98][0/49]\tTime 0.004 (0.004)\tData 0.001 (0.001)\tLoss 1.3449 (1.3449)\tPrec@1 0.554 (0.554)\n",
      "Test: [0/10]\tTime 0.000 (0.000)\tLoss 1.3947 (1.3947)\tPrec@1 0.556 (0.556)\n",
      "EPOCH: 99 val Results: Prec@1 0.528 \n",
      "Best Prec@1: 0.540\n",
      "\n",
      "current lr 2.46720e-05\n",
      "Epoch: [99][0/49]\tTime 0.002 (0.002)\tData 0.001 (0.001)\tLoss 1.3845 (1.3845)\tPrec@1 0.564 (0.564)\n",
      "Test: [0/10]\tTime 0.001 (0.001)\tLoss 1.4126 (1.4126)\tPrec@1 0.552 (0.552)\n",
      "EPOCH: 100 val Results: Prec@1 0.527 \n",
      "Best Prec@1: 0.540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'sigmoid': sigmoid, \n",
    "        'softmax': softmax,\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "\n",
    "    return model\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 64}},\n",
    "    # {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 64}}, \n",
    "    {'type': 'relu', 'params': {'name': 'relu1'}}, \n",
    "    # {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    # {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 64, 'out_num': 10}},\n",
    "]\n",
    "lr = 0.1\n",
    "bs = 1024\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4     # 2e-4, 1e-4\n",
    "seed = 0\n",
    "epoch = 100\n",
    "\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': lr, \n",
    "    'bs': bs,\n",
    "    'momentum': momentum,\n",
    "    'weight_decay': weight_decay,\n",
    "    'seed': seed,\n",
    "    'epoch': epoch,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
