{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 520556528 + 530101303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Assignment1-Dataset/'\n",
    "\n",
    "train_X = np.load(file_path + 'train_data.npy')\n",
    "train_y = np.load(file_path + 'train_label.npy')\n",
    "test_X = np.load(file_path + 'test_data.npy')\n",
    "test_y = np.load(file_path + 'test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39615947, -0.16328172,  1.37454247, -1.03385666],\n",
       "       [ 0.49859358,  1.62180414, -0.7437893 ,  0.62175915],\n",
       "       [-0.1047481 , -1.7754748 , -1.32378547,  2.44563907]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.random.randn(3, 4)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 20:38:20 2024\n",
      "End time:  Thu Apr  4 20:38:21 2024\n",
      "test_fun executed in 1.0051 seconds\n"
     ]
    }
   ],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timer\n",
    "def test_fun(x):\n",
    "    time.sleep(x)\n",
    "\n",
    "test_fun(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py.\n",
    "\n",
    "Modify tensor to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22851887,  0.67344942, -0.08416528,  0.91288123, -0.28973851,\n",
       "         0.52757336],\n",
       "       [ 0.33943484, -0.2016671 ,  0.35458889,  1.22751891, -0.24812361,\n",
       "         0.30106563],\n",
       "       [ 0.15816333, -0.8091541 ,  0.92615411, -0.38409243, -0.09891134,\n",
       "         0.89994069],\n",
       "       [ 0.06845438,  0.43470678, -0.68940179,  0.53433163, -1.03653986,\n",
       "         0.0991071 ],\n",
       "       [ 0.87129993, -0.53651349, -0.30734143, -0.05438511, -0.11202156,\n",
       "        -0.15260799]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    SELU              :math:`\\frac{3}{4}`\n",
    "    ================= ====================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)\n",
    "\n",
    "kaiming_normal_(np.array([0] * 30).reshape(5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    preds = output.argmax(axis=-1, keepdims=True)\n",
    "    return np.mean(preds == target) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train_X, test_X, mode=None):\n",
    "    if mode == 'min-max':\n",
    "        min_each_feature = np.min(train_X, axis=0)\n",
    "        max_each_feature = np.max(train_X, axis=0)\n",
    "        scale = max_each_feature - min_each_feature\n",
    "        scale[scale == 0] = 1   # To avoid divided by 0\n",
    "        scaled_train = (train_X - min_each_feature) / scale\n",
    "        scaled_test = (test_X - min_each_feature) / scale\n",
    "        return scaled_train, scaled_test\n",
    "\n",
    "    if mode == 'norm':\n",
    "        std_each_feature = np.std(train_X, axis=0)\n",
    "        mean_each_feature = np.mean(train_X, axis=0)\n",
    "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
    "        norm_train = (train_X - mean_each_feature) / std_each_feature\n",
    "        norm_test = (test_X - mean_each_feature) / std_each_feature\n",
    "        return norm_train, norm_test\n",
    "\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name \n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39615947, 0.        , 1.37454247, 0.        ],\n",
       "       [0.49859358, 1.62180414, 0.        , 0.62175915],\n",
       "       [0.        , 0.        , 0.        , 2.44563907]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class relu(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_output[self.input <= 0] = 0\n",
    "        return grad_output\n",
    "    \n",
    "\n",
    "test_relu = relu('test_relu')\n",
    "_ = test_relu.forward(test_array)\n",
    "test_relu.backward(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaky_relu(Layer):\n",
    "    def __init__(self, name, alpha, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.where(input > 0, input, self.alpha * input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        tmp = np.where(self.input > 0, 1, self.alpha)\n",
    "        return tmp * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.y = 1. / (1. + np.exp(-input))   # save sigmoid for more convenient grad computation\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return self.y * (1 - self.y) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.y = np.tanh(input)\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return (1 - self.y ** 2) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19976228, 0.13441991, 0.53139789, 0.13441991],\n",
       "       [0.1720233 , 0.52892176, 0.10448425, 0.19457069],\n",
       "       [0.06878563, 0.06878563, 0.06878563, 0.79364312]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class softmax(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "            input.shape = [batch size, num_class]\n",
    "        \"\"\"\n",
    "        x_max = input.max(axis=-1, keepdims=True)       # to avoid overflow\n",
    "        x_exp = np.exp(input - x_max)\n",
    "        return x_exp / x_exp.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # packaged in CrossEntropyLoss\n",
    "        return grad_output\n",
    "\n",
    "softmax('test_softmax').forward(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(Layer):\n",
    "    def __init__(self, name, in_num, out_num):\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.in_num = in_num\n",
    "        self.out_num = out_num\n",
    "\n",
    "        W = kaiming_normal_(np.array([0] * in_num * out_num).reshape(in_num, out_num), a=math.sqrt(5))     # Kaiming Init\n",
    "        self.W = Parameter(W, self.requires_grad)\n",
    "        self.b = Parameter(np.zeros(out_num), self.requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return input @ self.W.data + self.b.data      # [batch size, in_num] @ [in_num, out_num] + [out_num] => [batch size, out_num]\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "            grad_output: [batch size, out_num]\n",
    "        \"\"\"\n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.W.grad = self.input.T @ grad_output / batch_size\n",
    "        self.b.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return grad_output @ self.W.data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = softmax('softmax')\n",
    "\n",
    "    def __call__(self, input, ground_truth):\n",
    "        self.bacth_size = input.shape[0]\n",
    "        self.class_num = input.shape[1]\n",
    "\n",
    "        preds = self.softmax.forward(input)\n",
    "        ground_truth = self.one_hot_encoding(ground_truth)\n",
    "\n",
    "        self.grad = preds - ground_truth    #TODO: 推导要写在report上不？\n",
    "\n",
    "        loss = -1 * (ground_truth * np.log(preds + 1e-8)).sum() / self.bacth_size\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.bacth_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.size), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchnorm(Layer):\n",
    "    def __init__(self, name, shape, requires_grad=True):\n",
    "        super().__init__(name)\n",
    "        self.gamma = Parameter(np.random.uniform(0.9, 1.1, shape), requires_grad, skip_decay=True)\n",
    "        self.beta = Parameter(np.random.uniform(-0.1, 0.1, shape), requires_grad, skip_decay=True)\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        self.running_mean = Parameter(np.zeros(shape), False)\n",
    "        self.running_var = Parameter(np.zeros(shape), False)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            batch_mean = input.mean(axis=0)\n",
    "            batch_var = input.var(axis=0)\n",
    "            batch_std = np.sqrt(batch_var + 1e-8)    # To avoid divided by 0\n",
    "\n",
    "            momentum = 0.9\n",
    "            self.running_mean.data = momentum * self.running_mean.data + (1 - momentum) * batch_mean\n",
    "            self.running_var.data = momentum * self.running_var.data + (1 - momentum) * batch_var\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            batch_mean = self.running_mean.data\n",
    "            batch_std = np.sqrt(self.running_var.data)\n",
    "\n",
    "        self.norm = (input - batch_mean) / batch_std\n",
    "        self.gamma_norm = self.gamma.data / batch_std\n",
    "\n",
    "        return self.gamma.data * self.norm + self.beta.data\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):        \n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.gamma.grad = (grad_output * self.norm).sum(axis=0) / batch_size\n",
    "        self.beta.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return self.gamma_norm * (grad_output - self.norm * self.gamma.grad - self.beta.grad)       # TODO: 推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropout(Layer):\n",
    "    def __init__(self, name, drop_rate, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, input.shape) > self.drop_rate\n",
    "            return input * self.mask * self.fix_value\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.num_layers = 0\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if layer.requires_grad:\n",
    "            if hasattr(layer, 'W'):\n",
    "                self.params.append(layer.W)\n",
    "            if hasattr(layer, 'b'):\n",
    "                self.params.append(layer.b)\n",
    "            if hasattr(layer, 'gamma'):\n",
    "                self.params.append(layer.gamma)\n",
    "            if hasattr(layer, 'beta'):\n",
    "                self.params.append(layer.beta)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x):\n",
    "        for layer in self.layers[::-1]:\n",
    "            x = layer.backward(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = True\n",
    "    \n",
    "    def test(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, momentum, lr, weight_decay):\n",
    "        self.parameters = parameters\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        for i, (v, p) in enumerate(zip(self.v, self.parameters)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            v = self.momentum * v + self.lr * p.grad\n",
    "            self.v[i] = v\n",
    "            p.data -= self.v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    def __init__(self, parameters, lr, weight_decay=0, beta=(0.9, 0.999), eps=1e-8):\n",
    "        self.beta1 = beta[0]\n",
    "        self.beta2 = beta[1]\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.parameters = parameters\n",
    "        self.m = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.iterations += 1\n",
    "        for i, (p, m, v) in enumerate(zip(self.parameters, self.m, self.v)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            m = self.beta1 * m + (1 - self.beta1) * p.grad\n",
    "            v = self.beta2 * v + (1 - self.beta2) * np.power(p.grad, 2)\n",
    "\n",
    "            self.m[i] = m\n",
    "            self.v[i] = v\n",
    "            \n",
    "            # bias correction\n",
    "            m = m / (1 - np.power(self.beta1, self.iterations))\n",
    "            v = v / (1 - np.power(self.beta2, self.iterations))\n",
    "\n",
    "            p.data -= self.lr * m / (np.sqrt(v + self.eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLR(object):\n",
    "    def __init__(self, optimizer, T_max):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.n = -1\n",
    "        self.base_lr = optimizer.lr\n",
    "        self.step()\n",
    "\n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        lr = self.get_lr()\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.base_lr\n",
    "        cos = np.cos(np.pi * self.n / self.T_max)\n",
    "        return self.base_lr * (1 + cos) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, config, model=None, train_loader=None, val_loader=None):\n",
    "        self.config = config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr = self.config['lr']\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.print_freq = self.config['print_freq']\n",
    "        self.scheduler = self.config['scheduler']\n",
    "        self.train_precs = []\n",
    "        self.test_precs = []\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = SGD(self.model.params, self.config['momentum'], self.lr, self.config['weight_decay'])\n",
    "        elif self.config['optimizer'] == 'adam':\n",
    "            self.optimizer = Adam(self.model.params, self.lr, self.config['weight_decay'])\n",
    "        if self.scheduler:\n",
    "            self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "\n",
    "    @timer\n",
    "    def train(self):\n",
    "        best_acc1 = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            if self.scheduler:\n",
    "                self.train_scheduler.step()\n",
    "\n",
    "            # evaluate on validation set\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "\n",
    "    \n",
    "    def train_per_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        for i, (input, target) in enumerate(self.train_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            self.model.backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.train_loader) - 1):\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch + 1, i, len(self.train_loader) - 1, batch_time=batch_time,\n",
    "                        loss=losses, top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='train', top1=top1, losses=losses))\n",
    "        print(output)\n",
    "        self.train_losses.append(losses.avg)\n",
    "        self.train_precs.append(top1.avg)\n",
    "                \n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.test()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.val_loader) - 1):\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        i, len(self.val_loader) - 1, batch_time=batch_time, loss=losses,\n",
    "                        top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='val', top1=top1, losses=losses))\n",
    "        print(output)\n",
    "        self.test_losses.append(losses.avg)\n",
    "        self.test_precs.append(top1.avg)\n",
    "\n",
    "        return top1.avg\n",
    "    \n",
    "    def plot_cm(self, save_path):\n",
    "        self.model.test()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            output = np.argmax(output, axis=1)\n",
    "            y_pred += list(output)\n",
    "            y_true += list(target.flatten())\n",
    "            \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"Ground Truth\")\n",
    "        plt.xlabel(\"Prediction\")\n",
    "        plt.savefig(save_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "        \n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configs of Baseline vs Best Model\n",
    "\n",
    "| Modules                | Baseline          | Best Model |\n",
    "| ---------------------- | ----------------- | ---------- |\n",
    "| Batch size             | 128               |            |\n",
    "| Learning rate          | 0.1               |            |\n",
    "| Scheduler              | CosineAnnealingLR |            |\n",
    "| Epoch                  | 100               |            |\n",
    "| Pre-processing         | Yes               |            |\n",
    "| Number of Hidden layer | 2                 | 2          |\n",
    "| Hidden units           | [64, 32]          |            |\n",
    "| Activations            | [Relu, Relu]      |            |\n",
    "| Weight initialisation  | Kaiming           | Kaiming    |\n",
    "| Weight decay           | 5e-4              |            |\n",
    "| Optimizer              | SGD with Momentum |            |\n",
    "| Momentum               | 0.9               |            |\n",
    "| Batch Normalisation    | Yes               |            |\n",
    "| Dropout rate           | 0.1               |            |\n",
    "| Accuracy               | 53.03%            |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:19:33 2024\n",
      "current lr 1.00000e-01\n",
      "Epoch: [1][0/390]\tTime 0.003 (0.003)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.002 (0.005)\tLoss 1.7725 (2.3693)\tPrec@1 38.281 (29.173)\n",
      "Epoch: [1][156/390]\tTime 0.003 (0.005)\tLoss 1.9584 (2.0442)\tPrec@1 32.031 (33.853)\n",
      "Epoch: [1][234/390]\tTime 0.002 (0.005)\tLoss 1.6622 (1.9111)\tPrec@1 46.875 (36.260)\n",
      "Epoch: [1][312/390]\tTime 0.002 (0.005)\tLoss 1.5865 (1.8313)\tPrec@1 43.750 (37.894)\n",
      "Epoch: [1][390/390]\tTime 0.001 (0.004)\tLoss 1.5506 (1.7805)\tPrec@1 50.000 (38.952)\n",
      "EPOCH: 1 train Results: Prec@1 38.952 Loss: 1.7805\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.3998 (1.3998)\tPrec@1 44.531 (44.531)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4954 (1.4697)\tPrec@1 31.250 (46.970)\n",
      "EPOCH: 1 val Results: Prec@1 46.970 Loss: 1.4697\n",
      "Best Prec@1: 46.970\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [2][0/390]\tTime 0.002 (0.002)\tLoss 1.5362 (1.5362)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [2][78/390]\tTime 0.002 (0.004)\tLoss 1.3715 (1.4610)\tPrec@1 53.125 (47.528)\n",
      "Epoch: [2][156/390]\tTime 0.002 (0.004)\tLoss 1.4792 (1.4500)\tPrec@1 45.312 (47.806)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.004)\tLoss 1.5123 (1.4521)\tPrec@1 44.531 (47.766)\n",
      "Epoch: [2][312/390]\tTime 0.006 (0.003)\tLoss 1.6870 (1.4486)\tPrec@1 40.625 (47.816)\n",
      "Epoch: [2][390/390]\tTime 0.001 (0.003)\tLoss 1.4896 (1.4419)\tPrec@1 52.500 (47.996)\n",
      "EPOCH: 2 train Results: Prec@1 47.996 Loss: 1.4419\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3014 (1.3014)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.005 (0.001)\tLoss 1.4651 (1.3933)\tPrec@1 31.250 (49.470)\n",
      "EPOCH: 2 val Results: Prec@1 49.470 Loss: 1.3933\n",
      "Best Prec@1: 49.470\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [3][0/390]\tTime 0.002 (0.002)\tLoss 1.3946 (1.3946)\tPrec@1 47.656 (47.656)\n",
      "Epoch: [3][78/390]\tTime 0.004 (0.003)\tLoss 1.3520 (1.3587)\tPrec@1 53.906 (51.553)\n",
      "Epoch: [3][156/390]\tTime 0.007 (0.003)\tLoss 1.3932 (1.3513)\tPrec@1 49.219 (51.592)\n",
      "Epoch: [3][234/390]\tTime 0.002 (0.003)\tLoss 1.3970 (1.3586)\tPrec@1 48.438 (51.516)\n",
      "Epoch: [3][312/390]\tTime 0.003 (0.003)\tLoss 1.4538 (1.3629)\tPrec@1 48.438 (51.320)\n",
      "Epoch: [3][390/390]\tTime 0.012 (0.003)\tLoss 1.4803 (1.3659)\tPrec@1 48.750 (51.170)\n",
      "EPOCH: 3 train Results: Prec@1 51.170 Loss: 1.3659\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1991 (1.1991)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2110 (1.3405)\tPrec@1 50.000 (51.420)\n",
      "EPOCH: 3 val Results: Prec@1 51.420 Loss: 1.3405\n",
      "Best Prec@1: 51.420\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [4][0/390]\tTime 0.003 (0.003)\tLoss 1.3969 (1.3969)\tPrec@1 47.656 (47.656)\n",
      "Epoch: [4][78/390]\tTime 0.002 (0.003)\tLoss 1.2951 (1.2926)\tPrec@1 57.812 (53.066)\n",
      "Epoch: [4][156/390]\tTime 0.002 (0.003)\tLoss 1.2714 (1.2999)\tPrec@1 56.250 (52.598)\n",
      "Epoch: [4][234/390]\tTime 0.013 (0.003)\tLoss 1.3604 (1.3078)\tPrec@1 48.438 (52.583)\n",
      "Epoch: [4][312/390]\tTime 0.002 (0.003)\tLoss 1.3909 (1.3164)\tPrec@1 48.438 (52.444)\n",
      "Epoch: [4][390/390]\tTime 0.001 (0.003)\tLoss 1.2722 (1.3215)\tPrec@1 55.000 (52.422)\n",
      "EPOCH: 4 train Results: Prec@1 52.422 Loss: 1.3215\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2228 (1.2228)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2633 (1.3248)\tPrec@1 50.000 (52.410)\n",
      "EPOCH: 4 val Results: Prec@1 52.410 Loss: 1.3248\n",
      "Best Prec@1: 52.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [5][0/390]\tTime 0.002 (0.002)\tLoss 1.3000 (1.3000)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [5][78/390]\tTime 0.004 (0.003)\tLoss 1.2801 (1.2236)\tPrec@1 57.031 (56.052)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.003)\tLoss 1.3495 (1.2611)\tPrec@1 53.906 (54.892)\n",
      "Epoch: [5][234/390]\tTime 0.002 (0.003)\tLoss 1.3993 (1.2669)\tPrec@1 54.688 (54.707)\n",
      "Epoch: [5][312/390]\tTime 0.004 (0.003)\tLoss 1.2289 (1.2719)\tPrec@1 53.125 (54.328)\n",
      "Epoch: [5][390/390]\tTime 0.004 (0.003)\tLoss 1.1996 (1.2817)\tPrec@1 61.250 (53.868)\n",
      "EPOCH: 5 train Results: Prec@1 53.868 Loss: 1.2817\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2114 (1.2114)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2259 (1.3232)\tPrec@1 50.000 (52.670)\n",
      "EPOCH: 5 val Results: Prec@1 52.670 Loss: 1.3232\n",
      "Best Prec@1: 52.670\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [6][0/390]\tTime 0.009 (0.009)\tLoss 1.1819 (1.1819)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [6][78/390]\tTime 0.004 (0.003)\tLoss 1.3614 (1.1965)\tPrec@1 51.562 (57.308)\n",
      "Epoch: [6][156/390]\tTime 0.003 (0.003)\tLoss 1.0839 (1.2190)\tPrec@1 57.812 (56.146)\n",
      "Epoch: [6][234/390]\tTime 0.003 (0.003)\tLoss 1.2897 (1.2346)\tPrec@1 56.250 (55.738)\n",
      "Epoch: [6][312/390]\tTime 0.004 (0.003)\tLoss 1.4184 (1.2507)\tPrec@1 42.969 (55.057)\n",
      "Epoch: [6][390/390]\tTime 0.001 (0.003)\tLoss 1.6014 (1.2578)\tPrec@1 47.500 (54.850)\n",
      "EPOCH: 6 train Results: Prec@1 54.850 Loss: 1.2578\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2358 (1.2358)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3634 (1.3081)\tPrec@1 37.500 (52.880)\n",
      "EPOCH: 6 val Results: Prec@1 52.880 Loss: 1.3081\n",
      "Best Prec@1: 52.880\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [7][0/390]\tTime 0.002 (0.002)\tLoss 1.3186 (1.3186)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [7][78/390]\tTime 0.004 (0.003)\tLoss 1.1914 (1.1914)\tPrec@1 52.344 (57.288)\n",
      "Epoch: [7][156/390]\tTime 0.002 (0.003)\tLoss 1.3631 (1.2093)\tPrec@1 50.000 (56.807)\n",
      "Epoch: [7][234/390]\tTime 0.004 (0.003)\tLoss 1.2051 (1.2219)\tPrec@1 52.344 (56.034)\n",
      "Epoch: [7][312/390]\tTime 0.006 (0.003)\tLoss 1.3103 (1.2273)\tPrec@1 57.031 (56.020)\n",
      "Epoch: [7][390/390]\tTime 0.001 (0.003)\tLoss 1.3496 (1.2384)\tPrec@1 50.000 (55.776)\n",
      "EPOCH: 7 train Results: Prec@1 55.776 Loss: 1.2384\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2446 (1.2446)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2185 (1.3034)\tPrec@1 50.000 (53.550)\n",
      "EPOCH: 7 val Results: Prec@1 53.550 Loss: 1.3034\n",
      "Best Prec@1: 53.550\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [8][0/390]\tTime 0.004 (0.004)\tLoss 1.1105 (1.1105)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [8][78/390]\tTime 0.004 (0.003)\tLoss 1.1471 (1.1684)\tPrec@1 57.812 (58.376)\n",
      "Epoch: [8][156/390]\tTime 0.002 (0.003)\tLoss 1.3024 (1.1962)\tPrec@1 54.688 (57.026)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.003)\tLoss 1.1675 (1.2061)\tPrec@1 59.375 (56.639)\n",
      "Epoch: [8][312/390]\tTime 0.002 (0.003)\tLoss 1.0599 (1.2105)\tPrec@1 59.375 (56.584)\n",
      "Epoch: [8][390/390]\tTime 0.002 (0.003)\tLoss 1.2712 (1.2196)\tPrec@1 56.250 (56.266)\n",
      "EPOCH: 8 train Results: Prec@1 56.266 Loss: 1.2196\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1437 (1.1437)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3872 (1.3056)\tPrec@1 37.500 (53.700)\n",
      "EPOCH: 8 val Results: Prec@1 53.700 Loss: 1.3056\n",
      "Best Prec@1: 53.700\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [9][0/390]\tTime 0.002 (0.002)\tLoss 1.1406 (1.1406)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [9][78/390]\tTime 0.002 (0.003)\tLoss 1.2174 (1.1437)\tPrec@1 58.594 (59.612)\n",
      "Epoch: [9][156/390]\tTime 0.002 (0.003)\tLoss 1.3555 (1.1692)\tPrec@1 53.906 (58.325)\n",
      "Epoch: [9][234/390]\tTime 0.002 (0.003)\tLoss 1.3949 (1.1871)\tPrec@1 51.562 (57.600)\n",
      "Epoch: [9][312/390]\tTime 0.002 (0.003)\tLoss 1.1457 (1.1983)\tPrec@1 57.031 (57.216)\n",
      "Epoch: [9][390/390]\tTime 0.001 (0.003)\tLoss 1.4514 (1.2066)\tPrec@1 45.000 (56.862)\n",
      "EPOCH: 9 train Results: Prec@1 56.862 Loss: 1.2066\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1451 (1.1451)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3967 (1.2878)\tPrec@1 43.750 (53.550)\n",
      "EPOCH: 9 val Results: Prec@1 53.550 Loss: 1.2878\n",
      "Best Prec@1: 53.700\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [10][0/390]\tTime 0.004 (0.004)\tLoss 1.1381 (1.1381)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [10][78/390]\tTime 0.009 (0.004)\tLoss 0.9806 (1.1283)\tPrec@1 64.844 (59.326)\n",
      "Epoch: [10][156/390]\tTime 0.003 (0.004)\tLoss 1.1396 (1.1559)\tPrec@1 57.031 (58.415)\n",
      "Epoch: [10][234/390]\tTime 0.004 (0.004)\tLoss 1.1601 (1.1715)\tPrec@1 58.594 (57.896)\n",
      "Epoch: [10][312/390]\tTime 0.009 (0.004)\tLoss 1.2649 (1.1858)\tPrec@1 52.344 (57.393)\n",
      "Epoch: [10][390/390]\tTime 0.001 (0.004)\tLoss 1.0216 (1.1928)\tPrec@1 62.500 (57.280)\n",
      "EPOCH: 10 train Results: Prec@1 57.280 Loss: 1.1928\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2308 (1.2308)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4131 (1.2831)\tPrec@1 43.750 (53.690)\n",
      "EPOCH: 10 val Results: Prec@1 53.690 Loss: 1.2831\n",
      "Best Prec@1: 53.700\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [11][0/390]\tTime 0.002 (0.002)\tLoss 1.0305 (1.0305)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [11][78/390]\tTime 0.002 (0.003)\tLoss 1.1744 (1.1218)\tPrec@1 58.594 (59.830)\n",
      "Epoch: [11][156/390]\tTime 0.008 (0.003)\tLoss 1.1064 (1.1443)\tPrec@1 59.375 (59.156)\n",
      "Epoch: [11][234/390]\tTime 0.003 (0.003)\tLoss 1.2547 (1.1610)\tPrec@1 54.688 (58.477)\n",
      "Epoch: [11][312/390]\tTime 0.002 (0.003)\tLoss 1.0898 (1.1729)\tPrec@1 60.156 (58.139)\n",
      "Epoch: [11][390/390]\tTime 0.006 (0.003)\tLoss 1.2170 (1.1817)\tPrec@1 61.250 (57.886)\n",
      "EPOCH: 11 train Results: Prec@1 57.886 Loss: 1.1817\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1689 (1.1689)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3427 (1.2949)\tPrec@1 50.000 (54.140)\n",
      "EPOCH: 11 val Results: Prec@1 54.140 Loss: 1.2949\n",
      "Best Prec@1: 54.140\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [12][0/390]\tTime 0.002 (0.002)\tLoss 0.9846 (0.9846)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [12][78/390]\tTime 0.002 (0.003)\tLoss 1.0898 (1.1065)\tPrec@1 61.719 (60.107)\n",
      "Epoch: [12][156/390]\tTime 0.003 (0.003)\tLoss 1.2561 (1.1344)\tPrec@1 53.906 (59.017)\n",
      "Epoch: [12][234/390]\tTime 0.002 (0.003)\tLoss 1.0110 (1.1521)\tPrec@1 67.188 (58.338)\n",
      "Epoch: [12][312/390]\tTime 0.010 (0.003)\tLoss 1.1819 (1.1661)\tPrec@1 57.031 (58.110)\n",
      "Epoch: [12][390/390]\tTime 0.001 (0.003)\tLoss 0.9587 (1.1772)\tPrec@1 67.500 (57.792)\n",
      "EPOCH: 12 train Results: Prec@1 57.792 Loss: 1.1772\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1915 (1.1915)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3577 (1.2905)\tPrec@1 31.250 (53.910)\n",
      "EPOCH: 12 val Results: Prec@1 53.910 Loss: 1.2905\n",
      "Best Prec@1: 54.140\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [13][0/390]\tTime 0.003 (0.003)\tLoss 1.0228 (1.0228)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.1391 (1.1070)\tPrec@1 58.594 (59.771)\n",
      "Epoch: [13][156/390]\tTime 0.003 (0.003)\tLoss 1.0242 (1.1303)\tPrec@1 67.188 (59.136)\n",
      "Epoch: [13][234/390]\tTime 0.002 (0.003)\tLoss 1.2638 (1.1516)\tPrec@1 57.812 (58.391)\n",
      "Epoch: [13][312/390]\tTime 0.002 (0.003)\tLoss 1.1968 (1.1672)\tPrec@1 57.031 (57.890)\n",
      "Epoch: [13][390/390]\tTime 0.007 (0.003)\tLoss 1.0241 (1.1710)\tPrec@1 60.000 (57.842)\n",
      "EPOCH: 13 train Results: Prec@1 57.842 Loss: 1.1710\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1468 (1.1468)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3888 (1.2790)\tPrec@1 50.000 (53.840)\n",
      "EPOCH: 13 val Results: Prec@1 53.840 Loss: 1.2790\n",
      "Best Prec@1: 54.140\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [14][0/390]\tTime 0.002 (0.002)\tLoss 1.0405 (1.0405)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [14][78/390]\tTime 0.004 (0.003)\tLoss 1.1017 (1.1009)\tPrec@1 62.500 (60.819)\n",
      "Epoch: [14][156/390]\tTime 0.004 (0.003)\tLoss 1.2060 (1.1284)\tPrec@1 64.062 (59.733)\n",
      "Epoch: [14][234/390]\tTime 0.002 (0.003)\tLoss 1.1799 (1.1467)\tPrec@1 53.125 (59.189)\n",
      "Epoch: [14][312/390]\tTime 0.008 (0.004)\tLoss 1.0614 (1.1543)\tPrec@1 63.281 (58.973)\n",
      "Epoch: [14][390/390]\tTime 0.003 (0.004)\tLoss 1.4707 (1.1647)\tPrec@1 48.750 (58.564)\n",
      "EPOCH: 14 train Results: Prec@1 58.564 Loss: 1.1647\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1669 (1.1669)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5534 (1.2826)\tPrec@1 25.000 (54.070)\n",
      "EPOCH: 14 val Results: Prec@1 54.070 Loss: 1.2826\n",
      "Best Prec@1: 54.140\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [15][0/390]\tTime 0.004 (0.004)\tLoss 1.0245 (1.0245)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [15][78/390]\tTime 0.002 (0.003)\tLoss 1.2714 (1.0830)\tPrec@1 53.125 (61.086)\n",
      "Epoch: [15][156/390]\tTime 0.003 (0.003)\tLoss 0.9930 (1.1111)\tPrec@1 62.500 (60.500)\n",
      "Epoch: [15][234/390]\tTime 0.016 (0.004)\tLoss 1.2380 (1.1300)\tPrec@1 53.906 (59.678)\n",
      "Epoch: [15][312/390]\tTime 0.004 (0.004)\tLoss 1.2511 (1.1465)\tPrec@1 57.812 (59.008)\n",
      "Epoch: [15][390/390]\tTime 0.002 (0.004)\tLoss 1.2420 (1.1520)\tPrec@1 51.250 (58.876)\n",
      "EPOCH: 15 train Results: Prec@1 58.876 Loss: 1.1520\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.1844 (1.1844)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5309 (1.2902)\tPrec@1 43.750 (54.530)\n",
      "EPOCH: 15 val Results: Prec@1 54.530 Loss: 1.2902\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [16][0/390]\tTime 0.004 (0.004)\tLoss 1.0097 (1.0097)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [16][78/390]\tTime 0.003 (0.005)\tLoss 1.0184 (1.0608)\tPrec@1 62.500 (62.075)\n",
      "Epoch: [16][156/390]\tTime 0.005 (0.004)\tLoss 1.0265 (1.0990)\tPrec@1 60.156 (60.549)\n",
      "Epoch: [16][234/390]\tTime 0.002 (0.004)\tLoss 1.3835 (1.1267)\tPrec@1 52.344 (59.697)\n",
      "Epoch: [16][312/390]\tTime 0.002 (0.004)\tLoss 1.1864 (1.1400)\tPrec@1 58.594 (59.228)\n",
      "Epoch: [16][390/390]\tTime 0.002 (0.004)\tLoss 1.3005 (1.1524)\tPrec@1 51.250 (58.786)\n",
      "EPOCH: 16 train Results: Prec@1 58.786 Loss: 1.1524\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1375 (1.1375)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1887 (1.2840)\tPrec@1 50.000 (54.170)\n",
      "EPOCH: 16 val Results: Prec@1 54.170 Loss: 1.2840\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [17][0/390]\tTime 0.004 (0.004)\tLoss 1.0602 (1.0602)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [17][78/390]\tTime 0.003 (0.004)\tLoss 1.2458 (1.0788)\tPrec@1 55.469 (61.195)\n",
      "Epoch: [17][156/390]\tTime 0.003 (0.003)\tLoss 1.3652 (1.1049)\tPrec@1 51.562 (60.291)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.003)\tLoss 1.1845 (1.1282)\tPrec@1 53.906 (59.491)\n",
      "Epoch: [17][312/390]\tTime 0.003 (0.003)\tLoss 1.0982 (1.1395)\tPrec@1 59.375 (59.130)\n",
      "Epoch: [17][390/390]\tTime 0.001 (0.003)\tLoss 1.1924 (1.1513)\tPrec@1 58.750 (58.716)\n",
      "EPOCH: 17 train Results: Prec@1 58.716 Loss: 1.1513\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1633 (1.1633)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2654 (1.2867)\tPrec@1 50.000 (54.220)\n",
      "EPOCH: 17 val Results: Prec@1 54.220 Loss: 1.2867\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [18][0/390]\tTime 0.002 (0.002)\tLoss 1.0153 (1.0153)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [18][78/390]\tTime 0.002 (0.003)\tLoss 1.1388 (1.0705)\tPrec@1 60.938 (61.472)\n",
      "Epoch: [18][156/390]\tTime 0.006 (0.003)\tLoss 0.9891 (1.0987)\tPrec@1 64.844 (60.420)\n",
      "Epoch: [18][234/390]\tTime 0.004 (0.003)\tLoss 1.0242 (1.1250)\tPrec@1 62.500 (59.378)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.003)\tLoss 1.2129 (1.1370)\tPrec@1 57.031 (58.938)\n",
      "Epoch: [18][390/390]\tTime 0.002 (0.003)\tLoss 1.2353 (1.1461)\tPrec@1 51.250 (58.708)\n",
      "EPOCH: 18 train Results: Prec@1 58.708 Loss: 1.1461\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1436 (1.1436)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8940 (1.2748)\tPrec@1 56.250 (54.040)\n",
      "EPOCH: 18 val Results: Prec@1 54.040 Loss: 1.2748\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [19][0/390]\tTime 0.007 (0.007)\tLoss 1.1328 (1.1328)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [19][78/390]\tTime 0.002 (0.003)\tLoss 1.2205 (1.0560)\tPrec@1 56.250 (61.976)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.003)\tLoss 1.2154 (1.0914)\tPrec@1 60.938 (60.942)\n",
      "Epoch: [19][234/390]\tTime 0.002 (0.003)\tLoss 1.0936 (1.1104)\tPrec@1 61.719 (60.306)\n",
      "Epoch: [19][312/390]\tTime 0.003 (0.003)\tLoss 0.9438 (1.1263)\tPrec@1 64.844 (59.660)\n",
      "Epoch: [19][390/390]\tTime 0.001 (0.003)\tLoss 1.2738 (1.1352)\tPrec@1 51.250 (59.388)\n",
      "EPOCH: 19 train Results: Prec@1 59.388 Loss: 1.1352\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1983 (1.1983)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6881 (1.2886)\tPrec@1 37.500 (54.140)\n",
      "EPOCH: 19 val Results: Prec@1 54.140 Loss: 1.2886\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [20][0/390]\tTime 0.002 (0.002)\tLoss 1.0384 (1.0384)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [20][78/390]\tTime 0.010 (0.003)\tLoss 0.9765 (1.0726)\tPrec@1 64.844 (61.570)\n",
      "Epoch: [20][156/390]\tTime 0.004 (0.003)\tLoss 1.0806 (1.0954)\tPrec@1 59.375 (60.813)\n",
      "Epoch: [20][234/390]\tTime 0.002 (0.003)\tLoss 1.1690 (1.1185)\tPrec@1 55.469 (60.020)\n",
      "Epoch: [20][312/390]\tTime 0.004 (0.003)\tLoss 1.1264 (1.1295)\tPrec@1 57.031 (59.625)\n",
      "Epoch: [20][390/390]\tTime 0.002 (0.003)\tLoss 1.2080 (1.1383)\tPrec@1 55.000 (59.288)\n",
      "EPOCH: 20 train Results: Prec@1 59.288 Loss: 1.1383\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1877 (1.1877)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3517 (1.2815)\tPrec@1 50.000 (54.050)\n",
      "EPOCH: 20 val Results: Prec@1 54.050 Loss: 1.2815\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [21][0/390]\tTime 0.002 (0.002)\tLoss 1.0356 (1.0356)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [21][78/390]\tTime 0.002 (0.003)\tLoss 1.1647 (1.0485)\tPrec@1 55.469 (62.579)\n",
      "Epoch: [21][156/390]\tTime 0.002 (0.003)\tLoss 1.3639 (1.0871)\tPrec@1 50.781 (61.256)\n",
      "Epoch: [21][234/390]\tTime 0.002 (0.003)\tLoss 1.1759 (1.1051)\tPrec@1 61.719 (60.545)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.1218 (1.1215)\tPrec@1 60.938 (59.877)\n",
      "Epoch: [21][390/390]\tTime 0.002 (0.003)\tLoss 1.2030 (1.1330)\tPrec@1 60.000 (59.458)\n",
      "EPOCH: 21 train Results: Prec@1 59.458 Loss: 1.1330\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2158 (1.2158)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4865 (1.2903)\tPrec@1 25.000 (53.810)\n",
      "EPOCH: 21 val Results: Prec@1 53.810 Loss: 1.2903\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [22][0/390]\tTime 0.003 (0.003)\tLoss 1.1025 (1.1025)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [22][78/390]\tTime 0.005 (0.003)\tLoss 1.0891 (1.0726)\tPrec@1 61.719 (61.650)\n",
      "Epoch: [22][156/390]\tTime 0.002 (0.003)\tLoss 1.0756 (1.0956)\tPrec@1 63.281 (60.629)\n",
      "Epoch: [22][234/390]\tTime 0.004 (0.003)\tLoss 1.1364 (1.1101)\tPrec@1 56.250 (60.146)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.003)\tLoss 1.1601 (1.1215)\tPrec@1 56.250 (59.647)\n",
      "Epoch: [22][390/390]\tTime 0.001 (0.003)\tLoss 1.0083 (1.1337)\tPrec@1 60.000 (59.204)\n",
      "EPOCH: 22 train Results: Prec@1 59.204 Loss: 1.1337\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1179 (1.3030)\tPrec@1 31.250 (53.540)\n",
      "EPOCH: 22 val Results: Prec@1 53.540 Loss: 1.3030\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [23][0/390]\tTime 0.003 (0.003)\tLoss 1.1120 (1.1120)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [23][78/390]\tTime 0.003 (0.003)\tLoss 1.1840 (1.0477)\tPrec@1 54.688 (62.520)\n",
      "Epoch: [23][156/390]\tTime 0.003 (0.003)\tLoss 1.3354 (1.0878)\tPrec@1 55.469 (61.087)\n",
      "Epoch: [23][234/390]\tTime 0.002 (0.003)\tLoss 1.1384 (1.1001)\tPrec@1 60.156 (60.635)\n",
      "Epoch: [23][312/390]\tTime 0.004 (0.003)\tLoss 1.2791 (1.1177)\tPrec@1 53.906 (59.977)\n",
      "Epoch: [23][390/390]\tTime 0.002 (0.003)\tLoss 1.4697 (1.1289)\tPrec@1 50.000 (59.644)\n",
      "EPOCH: 23 train Results: Prec@1 59.644 Loss: 1.1289\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1446 (1.1446)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.1473 (1.2872)\tPrec@1 43.750 (54.130)\n",
      "EPOCH: 23 val Results: Prec@1 54.130 Loss: 1.2872\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [24][0/390]\tTime 0.005 (0.005)\tLoss 1.1356 (1.1356)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.002)\tLoss 1.1629 (1.0394)\tPrec@1 64.844 (62.994)\n",
      "Epoch: [24][156/390]\tTime 0.002 (0.002)\tLoss 1.1247 (1.0797)\tPrec@1 62.500 (61.286)\n",
      "Epoch: [24][234/390]\tTime 0.002 (0.002)\tLoss 1.2053 (1.1030)\tPrec@1 60.156 (60.578)\n",
      "Epoch: [24][312/390]\tTime 0.003 (0.002)\tLoss 1.3153 (1.1149)\tPrec@1 57.031 (60.089)\n",
      "Epoch: [24][390/390]\tTime 0.002 (0.003)\tLoss 1.2832 (1.1241)\tPrec@1 60.000 (59.750)\n",
      "EPOCH: 24 train Results: Prec@1 59.750 Loss: 1.1241\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2689 (1.2689)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1431 (1.3034)\tPrec@1 56.250 (54.010)\n",
      "EPOCH: 24 val Results: Prec@1 54.010 Loss: 1.3034\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [25][0/390]\tTime 0.004 (0.004)\tLoss 0.9690 (0.9690)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [25][78/390]\tTime 0.003 (0.003)\tLoss 1.0788 (1.0452)\tPrec@1 56.250 (62.421)\n",
      "Epoch: [25][156/390]\tTime 0.003 (0.003)\tLoss 1.2385 (1.0738)\tPrec@1 57.031 (61.415)\n",
      "Epoch: [25][234/390]\tTime 0.003 (0.003)\tLoss 1.1554 (1.0974)\tPrec@1 60.156 (60.578)\n",
      "Epoch: [25][312/390]\tTime 0.002 (0.003)\tLoss 1.3015 (1.1130)\tPrec@1 57.031 (60.199)\n",
      "Epoch: [25][390/390]\tTime 0.004 (0.003)\tLoss 1.2718 (1.1206)\tPrec@1 45.000 (59.980)\n",
      "EPOCH: 25 train Results: Prec@1 59.980 Loss: 1.1206\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1047 (1.1047)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1195 (1.2770)\tPrec@1 43.750 (54.390)\n",
      "EPOCH: 25 val Results: Prec@1 54.390 Loss: 1.2770\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [26][0/390]\tTime 0.004 (0.004)\tLoss 1.0875 (1.0875)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [26][78/390]\tTime 0.003 (0.003)\tLoss 1.0670 (1.0342)\tPrec@1 60.938 (62.757)\n",
      "Epoch: [26][156/390]\tTime 0.002 (0.003)\tLoss 1.0830 (1.0762)\tPrec@1 61.719 (61.883)\n",
      "Epoch: [26][234/390]\tTime 0.004 (0.003)\tLoss 1.0521 (1.0969)\tPrec@1 59.375 (60.811)\n",
      "Epoch: [26][312/390]\tTime 0.002 (0.003)\tLoss 1.1635 (1.1164)\tPrec@1 53.906 (60.124)\n",
      "Epoch: [26][390/390]\tTime 0.006 (0.003)\tLoss 1.1869 (1.1243)\tPrec@1 58.750 (59.734)\n",
      "EPOCH: 26 train Results: Prec@1 59.734 Loss: 1.1243\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1557 (1.1557)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1741 (1.2914)\tPrec@1 43.750 (53.690)\n",
      "EPOCH: 26 val Results: Prec@1 53.690 Loss: 1.2914\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [27][0/390]\tTime 0.005 (0.005)\tLoss 1.0884 (1.0884)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [27][78/390]\tTime 0.002 (0.003)\tLoss 1.0233 (1.0380)\tPrec@1 67.969 (62.638)\n",
      "Epoch: [27][156/390]\tTime 0.002 (0.003)\tLoss 0.9501 (1.0698)\tPrec@1 69.531 (61.599)\n",
      "Epoch: [27][234/390]\tTime 0.003 (0.003)\tLoss 1.0276 (1.0908)\tPrec@1 67.188 (60.838)\n",
      "Epoch: [27][312/390]\tTime 0.002 (0.003)\tLoss 1.1595 (1.1081)\tPrec@1 57.812 (60.209)\n",
      "Epoch: [27][390/390]\tTime 0.002 (0.003)\tLoss 1.4346 (1.1232)\tPrec@1 42.500 (59.714)\n",
      "EPOCH: 27 train Results: Prec@1 59.714 Loss: 1.1232\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1251 (1.1251)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2073 (1.2923)\tPrec@1 50.000 (54.310)\n",
      "EPOCH: 27 val Results: Prec@1 54.310 Loss: 1.2923\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [28][0/390]\tTime 0.004 (0.004)\tLoss 0.9365 (0.9365)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [28][78/390]\tTime 0.002 (0.003)\tLoss 0.9735 (1.0399)\tPrec@1 65.625 (62.866)\n",
      "Epoch: [28][156/390]\tTime 0.002 (0.003)\tLoss 1.1959 (1.0744)\tPrec@1 55.469 (61.445)\n",
      "Epoch: [28][234/390]\tTime 0.002 (0.003)\tLoss 1.1485 (1.0914)\tPrec@1 57.031 (60.947)\n",
      "Epoch: [28][312/390]\tTime 0.002 (0.003)\tLoss 1.1481 (1.1055)\tPrec@1 63.281 (60.546)\n",
      "Epoch: [28][390/390]\tTime 0.002 (0.003)\tLoss 1.4186 (1.1175)\tPrec@1 51.250 (60.116)\n",
      "EPOCH: 28 train Results: Prec@1 60.116 Loss: 1.1175\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2106 (1.2106)\tPrec@1 52.344 (52.344)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4153 (1.2793)\tPrec@1 37.500 (53.920)\n",
      "EPOCH: 28 val Results: Prec@1 53.920 Loss: 1.2793\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [29][0/390]\tTime 0.005 (0.005)\tLoss 0.8661 (0.8661)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [29][78/390]\tTime 0.003 (0.003)\tLoss 1.0415 (1.0427)\tPrec@1 61.719 (62.935)\n",
      "Epoch: [29][156/390]\tTime 0.002 (0.003)\tLoss 1.0706 (1.0735)\tPrec@1 59.375 (61.943)\n",
      "Epoch: [29][234/390]\tTime 0.002 (0.003)\tLoss 1.0035 (1.0951)\tPrec@1 64.844 (61.051)\n",
      "Epoch: [29][312/390]\tTime 0.004 (0.003)\tLoss 1.1685 (1.1117)\tPrec@1 59.375 (60.368)\n",
      "Epoch: [29][390/390]\tTime 0.001 (0.003)\tLoss 1.1687 (1.1221)\tPrec@1 47.500 (59.906)\n",
      "EPOCH: 29 train Results: Prec@1 59.906 Loss: 1.1221\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1094 (1.1094)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8965 (1.2712)\tPrec@1 68.750 (54.530)\n",
      "EPOCH: 29 val Results: Prec@1 54.530 Loss: 1.2712\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [30][0/390]\tTime 0.003 (0.003)\tLoss 1.0508 (1.0508)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [30][78/390]\tTime 0.003 (0.003)\tLoss 1.0572 (1.0397)\tPrec@1 58.594 (62.559)\n",
      "Epoch: [30][156/390]\tTime 0.003 (0.003)\tLoss 0.9737 (1.0683)\tPrec@1 62.500 (61.739)\n",
      "Epoch: [30][234/390]\tTime 0.002 (0.003)\tLoss 1.0854 (1.0891)\tPrec@1 62.500 (61.110)\n",
      "Epoch: [30][312/390]\tTime 0.002 (0.003)\tLoss 1.1376 (1.1025)\tPrec@1 61.719 (60.625)\n",
      "Epoch: [30][390/390]\tTime 0.001 (0.003)\tLoss 1.0920 (1.1193)\tPrec@1 63.750 (60.112)\n",
      "EPOCH: 30 train Results: Prec@1 60.112 Loss: 1.1193\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1634 (1.1634)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3254 (1.2801)\tPrec@1 50.000 (54.530)\n",
      "EPOCH: 30 val Results: Prec@1 54.530 Loss: 1.2801\n",
      "Best Prec@1: 54.530\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [31][0/390]\tTime 0.002 (0.002)\tLoss 0.9432 (0.9432)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [31][78/390]\tTime 0.003 (0.003)\tLoss 0.9946 (1.0354)\tPrec@1 60.938 (63.103)\n",
      "Epoch: [31][156/390]\tTime 0.002 (0.003)\tLoss 1.0778 (1.0639)\tPrec@1 60.156 (62.082)\n",
      "Epoch: [31][234/390]\tTime 0.002 (0.002)\tLoss 0.9630 (1.0808)\tPrec@1 65.625 (61.400)\n",
      "Epoch: [31][312/390]\tTime 0.002 (0.002)\tLoss 1.0666 (1.0988)\tPrec@1 62.500 (60.683)\n",
      "Epoch: [31][390/390]\tTime 0.001 (0.003)\tLoss 1.1442 (1.1124)\tPrec@1 60.000 (60.216)\n",
      "EPOCH: 31 train Results: Prec@1 60.216 Loss: 1.1124\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2153 (1.2153)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9384 (1.2799)\tPrec@1 62.500 (54.610)\n",
      "EPOCH: 31 val Results: Prec@1 54.610 Loss: 1.2799\n",
      "Best Prec@1: 54.610\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [32][0/390]\tTime 0.003 (0.003)\tLoss 1.0687 (1.0687)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [32][78/390]\tTime 0.025 (0.003)\tLoss 0.9706 (1.0441)\tPrec@1 64.844 (62.668)\n",
      "Epoch: [32][156/390]\tTime 0.004 (0.003)\tLoss 0.8724 (1.0636)\tPrec@1 67.188 (61.783)\n",
      "Epoch: [32][234/390]\tTime 0.002 (0.003)\tLoss 1.1865 (1.0858)\tPrec@1 58.594 (60.954)\n",
      "Epoch: [32][312/390]\tTime 0.003 (0.004)\tLoss 1.0546 (1.1032)\tPrec@1 64.844 (60.363)\n",
      "Epoch: [32][390/390]\tTime 0.002 (0.004)\tLoss 1.1745 (1.1158)\tPrec@1 56.250 (59.966)\n",
      "EPOCH: 32 train Results: Prec@1 59.966 Loss: 1.1158\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1985 (1.1985)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0470 (1.2886)\tPrec@1 43.750 (54.080)\n",
      "EPOCH: 32 val Results: Prec@1 54.080 Loss: 1.2886\n",
      "Best Prec@1: 54.610\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [33][0/390]\tTime 0.002 (0.002)\tLoss 1.1716 (1.1716)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [33][78/390]\tTime 0.002 (0.004)\tLoss 1.1265 (1.0515)\tPrec@1 61.719 (62.114)\n",
      "Epoch: [33][156/390]\tTime 0.003 (0.003)\tLoss 1.2097 (1.0679)\tPrec@1 53.906 (61.550)\n",
      "Epoch: [33][234/390]\tTime 0.002 (0.003)\tLoss 1.1606 (1.0854)\tPrec@1 62.500 (61.021)\n",
      "Epoch: [33][312/390]\tTime 0.002 (0.003)\tLoss 1.2538 (1.1015)\tPrec@1 57.031 (60.566)\n",
      "Epoch: [33][390/390]\tTime 0.002 (0.003)\tLoss 1.2222 (1.1149)\tPrec@1 55.000 (60.102)\n",
      "EPOCH: 33 train Results: Prec@1 60.102 Loss: 1.1149\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1772 (1.1772)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2520 (1.2763)\tPrec@1 62.500 (55.050)\n",
      "EPOCH: 33 val Results: Prec@1 55.050 Loss: 1.2763\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [34][0/390]\tTime 0.004 (0.004)\tLoss 1.0151 (1.0151)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [34][78/390]\tTime 0.002 (0.003)\tLoss 1.0828 (1.0218)\tPrec@1 60.156 (63.647)\n",
      "Epoch: [34][156/390]\tTime 0.003 (0.003)\tLoss 1.0846 (1.0600)\tPrec@1 65.625 (62.147)\n",
      "Epoch: [34][234/390]\tTime 0.002 (0.003)\tLoss 1.1555 (1.0773)\tPrec@1 56.250 (61.496)\n",
      "Epoch: [34][312/390]\tTime 0.003 (0.003)\tLoss 1.2103 (1.0959)\tPrec@1 58.594 (60.795)\n",
      "Epoch: [34][390/390]\tTime 0.001 (0.003)\tLoss 1.2278 (1.1101)\tPrec@1 58.750 (60.344)\n",
      "EPOCH: 34 train Results: Prec@1 60.344 Loss: 1.1101\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1708 (1.1708)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3844 (1.2881)\tPrec@1 43.750 (54.200)\n",
      "EPOCH: 34 val Results: Prec@1 54.200 Loss: 1.2881\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [35][0/390]\tTime 0.002 (0.002)\tLoss 1.0116 (1.0116)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [35][78/390]\tTime 0.006 (0.003)\tLoss 1.1497 (1.0378)\tPrec@1 57.031 (63.766)\n",
      "Epoch: [35][156/390]\tTime 0.002 (0.003)\tLoss 1.4125 (1.0547)\tPrec@1 55.469 (62.634)\n",
      "Epoch: [35][234/390]\tTime 0.003 (0.003)\tLoss 1.1639 (1.0814)\tPrec@1 57.031 (61.566)\n",
      "Epoch: [35][312/390]\tTime 0.002 (0.003)\tLoss 1.1389 (1.0960)\tPrec@1 55.469 (61.005)\n",
      "Epoch: [35][390/390]\tTime 0.003 (0.003)\tLoss 0.9830 (1.1094)\tPrec@1 65.000 (60.402)\n",
      "EPOCH: 35 train Results: Prec@1 60.402 Loss: 1.1094\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1840 (1.1840)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5577 (1.2556)\tPrec@1 37.500 (54.840)\n",
      "EPOCH: 35 val Results: Prec@1 54.840 Loss: 1.2556\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [36][0/390]\tTime 0.002 (0.002)\tLoss 1.1931 (1.1931)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [36][78/390]\tTime 0.002 (0.003)\tLoss 1.0753 (1.0277)\tPrec@1 62.500 (63.341)\n",
      "Epoch: [36][156/390]\tTime 0.002 (0.003)\tLoss 1.1795 (1.0604)\tPrec@1 57.812 (61.943)\n",
      "Epoch: [36][234/390]\tTime 0.002 (0.003)\tLoss 1.1616 (1.0776)\tPrec@1 57.812 (61.400)\n",
      "Epoch: [36][312/390]\tTime 0.002 (0.003)\tLoss 1.1943 (1.0906)\tPrec@1 54.688 (60.980)\n",
      "Epoch: [36][390/390]\tTime 0.001 (0.003)\tLoss 1.2471 (1.1040)\tPrec@1 52.500 (60.496)\n",
      "EPOCH: 36 train Results: Prec@1 60.496 Loss: 1.1040\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2266 (1.2266)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2141 (1.2686)\tPrec@1 50.000 (54.630)\n",
      "EPOCH: 36 val Results: Prec@1 54.630 Loss: 1.2686\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [37][0/390]\tTime 0.003 (0.003)\tLoss 0.8472 (0.8472)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 0.9347 (1.0172)\tPrec@1 63.281 (63.993)\n",
      "Epoch: [37][156/390]\tTime 0.002 (0.003)\tLoss 1.0345 (1.0616)\tPrec@1 64.844 (62.251)\n",
      "Epoch: [37][234/390]\tTime 0.002 (0.003)\tLoss 1.2224 (1.0845)\tPrec@1 53.125 (61.383)\n",
      "Epoch: [37][312/390]\tTime 0.002 (0.003)\tLoss 1.0793 (1.1033)\tPrec@1 62.500 (60.625)\n",
      "Epoch: [37][390/390]\tTime 0.002 (0.003)\tLoss 1.1403 (1.1098)\tPrec@1 56.250 (60.352)\n",
      "EPOCH: 37 train Results: Prec@1 60.352 Loss: 1.1098\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1778 (1.1778)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4707 (1.2691)\tPrec@1 31.250 (54.820)\n",
      "EPOCH: 37 val Results: Prec@1 54.820 Loss: 1.2691\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [38][0/390]\tTime 0.005 (0.005)\tLoss 0.9957 (0.9957)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [38][78/390]\tTime 0.004 (0.003)\tLoss 1.2703 (1.0475)\tPrec@1 56.250 (62.470)\n",
      "Epoch: [38][156/390]\tTime 0.075 (0.003)\tLoss 1.0198 (1.0659)\tPrec@1 61.719 (61.689)\n",
      "Epoch: [38][234/390]\tTime 0.008 (0.003)\tLoss 0.9815 (1.0864)\tPrec@1 60.938 (61.027)\n",
      "Epoch: [38][312/390]\tTime 0.002 (0.003)\tLoss 1.2704 (1.0952)\tPrec@1 58.594 (60.775)\n",
      "Epoch: [38][390/390]\tTime 0.003 (0.003)\tLoss 1.1791 (1.1074)\tPrec@1 56.250 (60.302)\n",
      "EPOCH: 38 train Results: Prec@1 60.302 Loss: 1.1074\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1162 (1.1162)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3265 (1.2698)\tPrec@1 25.000 (54.500)\n",
      "EPOCH: 38 val Results: Prec@1 54.500 Loss: 1.2698\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [39][0/390]\tTime 0.002 (0.002)\tLoss 1.0525 (1.0525)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.003)\tLoss 1.2303 (1.0301)\tPrec@1 58.594 (63.014)\n",
      "Epoch: [39][156/390]\tTime 0.002 (0.003)\tLoss 1.0691 (1.0569)\tPrec@1 60.156 (62.336)\n",
      "Epoch: [39][234/390]\tTime 0.002 (0.003)\tLoss 1.0718 (1.0808)\tPrec@1 65.625 (61.576)\n",
      "Epoch: [39][312/390]\tTime 0.002 (0.003)\tLoss 1.4890 (1.0932)\tPrec@1 41.406 (61.162)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.1988 (1.1067)\tPrec@1 55.000 (60.570)\n",
      "EPOCH: 39 train Results: Prec@1 60.570 Loss: 1.1067\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1094 (1.1094)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4275 (1.2852)\tPrec@1 43.750 (54.690)\n",
      "EPOCH: 39 val Results: Prec@1 54.690 Loss: 1.2852\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [40][0/390]\tTime 0.002 (0.002)\tLoss 0.8983 (0.8983)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.003)\tLoss 1.0712 (1.0291)\tPrec@1 60.938 (63.192)\n",
      "Epoch: [40][156/390]\tTime 0.002 (0.003)\tLoss 1.1419 (1.0511)\tPrec@1 64.062 (62.321)\n",
      "Epoch: [40][234/390]\tTime 0.019 (0.003)\tLoss 1.2227 (1.0684)\tPrec@1 55.469 (61.762)\n",
      "Epoch: [40][312/390]\tTime 0.003 (0.003)\tLoss 1.2246 (1.0862)\tPrec@1 59.375 (61.142)\n",
      "Epoch: [40][390/390]\tTime 0.001 (0.003)\tLoss 1.2146 (1.1012)\tPrec@1 58.750 (60.584)\n",
      "EPOCH: 40 train Results: Prec@1 60.584 Loss: 1.1012\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1872 (1.1872)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4042 (1.2863)\tPrec@1 37.500 (53.770)\n",
      "EPOCH: 40 val Results: Prec@1 53.770 Loss: 1.2863\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [41][0/390]\tTime 0.002 (0.002)\tLoss 1.1179 (1.1179)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [41][78/390]\tTime 0.002 (0.003)\tLoss 1.0445 (1.0077)\tPrec@1 60.938 (63.687)\n",
      "Epoch: [41][156/390]\tTime 0.002 (0.003)\tLoss 1.1718 (1.0525)\tPrec@1 62.500 (62.624)\n",
      "Epoch: [41][234/390]\tTime 0.002 (0.003)\tLoss 1.1938 (1.0758)\tPrec@1 56.250 (61.649)\n",
      "Epoch: [41][312/390]\tTime 0.017 (0.003)\tLoss 1.2139 (1.0905)\tPrec@1 59.375 (61.052)\n",
      "Epoch: [41][390/390]\tTime 0.002 (0.003)\tLoss 1.1008 (1.1008)\tPrec@1 62.500 (60.788)\n",
      "EPOCH: 41 train Results: Prec@1 60.788 Loss: 1.1008\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1602 (1.1602)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3249 (1.2711)\tPrec@1 43.750 (54.690)\n",
      "EPOCH: 41 val Results: Prec@1 54.690 Loss: 1.2711\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [42][0/390]\tTime 0.007 (0.007)\tLoss 0.9934 (0.9934)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [42][78/390]\tTime 0.002 (0.003)\tLoss 1.0463 (1.0217)\tPrec@1 63.281 (63.449)\n",
      "Epoch: [42][156/390]\tTime 0.002 (0.003)\tLoss 1.1689 (1.0487)\tPrec@1 58.594 (62.818)\n",
      "Epoch: [42][234/390]\tTime 0.002 (0.003)\tLoss 1.1065 (1.0681)\tPrec@1 57.812 (62.111)\n",
      "Epoch: [42][312/390]\tTime 0.002 (0.003)\tLoss 1.2635 (1.0849)\tPrec@1 54.688 (61.422)\n",
      "Epoch: [42][390/390]\tTime 0.007 (0.003)\tLoss 1.2197 (1.0971)\tPrec@1 57.500 (61.018)\n",
      "EPOCH: 42 train Results: Prec@1 61.018 Loss: 1.0971\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1285 (1.1285)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4676 (1.2819)\tPrec@1 18.750 (54.680)\n",
      "EPOCH: 42 val Results: Prec@1 54.680 Loss: 1.2819\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [43][0/390]\tTime 0.005 (0.005)\tLoss 1.0557 (1.0557)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [43][78/390]\tTime 0.002 (0.003)\tLoss 0.9713 (1.0261)\tPrec@1 64.844 (63.281)\n",
      "Epoch: [43][156/390]\tTime 0.007 (0.004)\tLoss 1.0538 (1.0562)\tPrec@1 60.156 (62.331)\n",
      "Epoch: [43][234/390]\tTime 0.003 (0.003)\tLoss 1.1226 (1.0745)\tPrec@1 59.375 (61.695)\n",
      "Epoch: [43][312/390]\tTime 0.017 (0.004)\tLoss 1.1621 (1.0919)\tPrec@1 59.375 (61.087)\n",
      "Epoch: [43][390/390]\tTime 0.001 (0.004)\tLoss 0.9567 (1.1013)\tPrec@1 66.250 (60.796)\n",
      "EPOCH: 43 train Results: Prec@1 60.796 Loss: 1.1013\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1467 (1.1467)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3464 (1.2798)\tPrec@1 62.500 (54.950)\n",
      "EPOCH: 43 val Results: Prec@1 54.950 Loss: 1.2798\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [44][0/390]\tTime 0.002 (0.002)\tLoss 1.0085 (1.0085)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.005)\tLoss 0.9564 (1.0312)\tPrec@1 69.531 (64.023)\n",
      "Epoch: [44][156/390]\tTime 0.005 (0.004)\tLoss 1.0949 (1.0546)\tPrec@1 60.156 (62.580)\n",
      "Epoch: [44][234/390]\tTime 0.002 (0.004)\tLoss 0.9847 (1.0730)\tPrec@1 60.938 (61.732)\n",
      "Epoch: [44][312/390]\tTime 0.007 (0.004)\tLoss 1.1484 (1.0888)\tPrec@1 58.594 (61.095)\n",
      "Epoch: [44][390/390]\tTime 0.001 (0.003)\tLoss 1.1327 (1.0987)\tPrec@1 58.750 (60.684)\n",
      "EPOCH: 44 train Results: Prec@1 60.684 Loss: 1.0987\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1181 (1.1181)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4036 (1.2771)\tPrec@1 50.000 (54.750)\n",
      "EPOCH: 44 val Results: Prec@1 54.750 Loss: 1.2771\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [45][0/390]\tTime 0.006 (0.006)\tLoss 1.0642 (1.0642)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [45][78/390]\tTime 0.024 (0.004)\tLoss 1.1672 (1.0248)\tPrec@1 57.812 (63.835)\n",
      "Epoch: [45][156/390]\tTime 0.002 (0.005)\tLoss 1.2044 (1.0446)\tPrec@1 55.469 (63.037)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.004)\tLoss 1.1078 (1.0678)\tPrec@1 65.625 (62.104)\n",
      "Epoch: [45][312/390]\tTime 0.004 (0.004)\tLoss 1.0075 (1.0824)\tPrec@1 65.625 (61.581)\n",
      "Epoch: [45][390/390]\tTime 0.001 (0.004)\tLoss 1.2598 (1.0967)\tPrec@1 57.500 (61.016)\n",
      "EPOCH: 45 train Results: Prec@1 61.016 Loss: 1.0967\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1590 (1.1590)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2954 (1.2923)\tPrec@1 31.250 (54.380)\n",
      "EPOCH: 45 val Results: Prec@1 54.380 Loss: 1.2923\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [46][0/390]\tTime 0.003 (0.003)\tLoss 1.0761 (1.0761)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [46][78/390]\tTime 0.003 (0.004)\tLoss 1.0364 (1.0378)\tPrec@1 60.938 (62.549)\n",
      "Epoch: [46][156/390]\tTime 0.002 (0.004)\tLoss 1.1765 (1.0551)\tPrec@1 57.812 (62.042)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.004)\tLoss 1.1329 (1.0748)\tPrec@1 57.031 (61.489)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.003)\tLoss 1.1921 (1.0850)\tPrec@1 48.438 (61.177)\n",
      "Epoch: [46][390/390]\tTime 0.003 (0.004)\tLoss 1.1216 (1.0996)\tPrec@1 58.750 (60.612)\n",
      "EPOCH: 46 train Results: Prec@1 60.612 Loss: 1.0996\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1569 (1.1569)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7150 (1.2968)\tPrec@1 43.750 (54.010)\n",
      "EPOCH: 46 val Results: Prec@1 54.010 Loss: 1.2968\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [47][0/390]\tTime 0.004 (0.004)\tLoss 1.2372 (1.2372)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [47][78/390]\tTime 0.008 (0.004)\tLoss 1.1791 (1.0209)\tPrec@1 57.031 (63.232)\n",
      "Epoch: [47][156/390]\tTime 0.003 (0.004)\tLoss 1.0531 (1.0454)\tPrec@1 65.625 (62.724)\n",
      "Epoch: [47][234/390]\tTime 0.002 (0.004)\tLoss 1.0108 (1.0706)\tPrec@1 59.375 (61.619)\n",
      "Epoch: [47][312/390]\tTime 0.003 (0.004)\tLoss 1.0722 (1.0812)\tPrec@1 60.938 (61.259)\n",
      "Epoch: [47][390/390]\tTime 0.007 (0.004)\tLoss 1.3011 (1.0951)\tPrec@1 43.750 (60.770)\n",
      "EPOCH: 47 train Results: Prec@1 60.770 Loss: 1.0951\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0693 (1.0693)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3668 (1.2836)\tPrec@1 37.500 (54.670)\n",
      "EPOCH: 47 val Results: Prec@1 54.670 Loss: 1.2836\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [48][0/390]\tTime 0.003 (0.003)\tLoss 0.9832 (0.9832)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [48][78/390]\tTime 0.002 (0.003)\tLoss 1.0490 (1.0351)\tPrec@1 57.031 (63.103)\n",
      "Epoch: [48][156/390]\tTime 0.003 (0.003)\tLoss 1.1413 (1.0597)\tPrec@1 56.250 (61.838)\n",
      "Epoch: [48][234/390]\tTime 0.002 (0.004)\tLoss 0.9759 (1.0698)\tPrec@1 60.156 (61.589)\n",
      "Epoch: [48][312/390]\tTime 0.002 (0.004)\tLoss 1.0922 (1.0839)\tPrec@1 60.156 (61.055)\n",
      "Epoch: [48][390/390]\tTime 0.002 (0.003)\tLoss 1.3543 (1.0968)\tPrec@1 56.250 (60.686)\n",
      "EPOCH: 48 train Results: Prec@1 60.686 Loss: 1.0968\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1338 (1.1338)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4454 (1.2823)\tPrec@1 43.750 (54.450)\n",
      "EPOCH: 48 val Results: Prec@1 54.450 Loss: 1.2823\n",
      "Best Prec@1: 55.050\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [49][0/390]\tTime 0.005 (0.005)\tLoss 1.0669 (1.0669)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [49][78/390]\tTime 0.002 (0.002)\tLoss 1.1039 (1.0048)\tPrec@1 64.844 (64.636)\n",
      "Epoch: [49][156/390]\tTime 0.009 (0.003)\tLoss 1.1825 (1.0415)\tPrec@1 58.594 (63.097)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.003)\tLoss 1.1332 (1.0660)\tPrec@1 62.500 (62.164)\n",
      "Epoch: [49][312/390]\tTime 0.005 (0.003)\tLoss 1.0771 (1.0819)\tPrec@1 60.938 (61.462)\n",
      "Epoch: [49][390/390]\tTime 0.002 (0.003)\tLoss 1.2539 (1.0952)\tPrec@1 52.500 (60.954)\n",
      "EPOCH: 49 train Results: Prec@1 60.954 Loss: 1.0952\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1651 (1.1651)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2644 (1.2689)\tPrec@1 43.750 (55.410)\n",
      "EPOCH: 49 val Results: Prec@1 55.410 Loss: 1.2689\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [50][0/390]\tTime 0.002 (0.002)\tLoss 0.9584 (0.9584)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [50][78/390]\tTime 0.002 (0.003)\tLoss 1.0693 (1.0100)\tPrec@1 63.281 (63.934)\n",
      "Epoch: [50][156/390]\tTime 0.003 (0.003)\tLoss 1.1474 (1.0393)\tPrec@1 60.156 (62.903)\n",
      "Epoch: [50][234/390]\tTime 0.003 (0.003)\tLoss 1.2273 (1.0665)\tPrec@1 55.469 (62.078)\n",
      "Epoch: [50][312/390]\tTime 0.002 (0.003)\tLoss 1.1214 (1.0803)\tPrec@1 59.375 (61.472)\n",
      "Epoch: [50][390/390]\tTime 0.001 (0.003)\tLoss 1.1107 (1.0951)\tPrec@1 57.500 (60.820)\n",
      "EPOCH: 50 train Results: Prec@1 60.820 Loss: 1.0951\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1261 (1.1261)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3029 (1.2703)\tPrec@1 43.750 (54.890)\n",
      "EPOCH: 50 val Results: Prec@1 54.890 Loss: 1.2703\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [51][0/390]\tTime 0.005 (0.005)\tLoss 1.1099 (1.1099)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [51][78/390]\tTime 0.003 (0.004)\tLoss 1.0748 (1.0208)\tPrec@1 61.719 (63.855)\n",
      "Epoch: [51][156/390]\tTime 0.002 (0.003)\tLoss 1.0408 (1.0488)\tPrec@1 64.062 (62.714)\n",
      "Epoch: [51][234/390]\tTime 0.002 (0.003)\tLoss 1.1307 (1.0652)\tPrec@1 57.031 (61.968)\n",
      "Epoch: [51][312/390]\tTime 0.003 (0.003)\tLoss 0.9656 (1.0759)\tPrec@1 70.312 (61.726)\n",
      "Epoch: [51][390/390]\tTime 0.003 (0.003)\tLoss 1.0294 (1.0911)\tPrec@1 62.500 (61.230)\n",
      "EPOCH: 51 train Results: Prec@1 61.230 Loss: 1.0911\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2416 (1.2416)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1815 (1.2753)\tPrec@1 31.250 (54.740)\n",
      "EPOCH: 51 val Results: Prec@1 54.740 Loss: 1.2753\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [52][0/390]\tTime 0.002 (0.002)\tLoss 0.9703 (0.9703)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [52][78/390]\tTime 0.004 (0.005)\tLoss 1.0898 (1.0222)\tPrec@1 58.594 (63.113)\n",
      "Epoch: [52][156/390]\tTime 0.002 (0.004)\tLoss 1.1308 (1.0422)\tPrec@1 59.375 (62.515)\n",
      "Epoch: [52][234/390]\tTime 0.004 (0.003)\tLoss 1.0064 (1.0668)\tPrec@1 62.500 (61.812)\n",
      "Epoch: [52][312/390]\tTime 0.007 (0.004)\tLoss 1.2770 (1.0845)\tPrec@1 51.562 (61.212)\n",
      "Epoch: [52][390/390]\tTime 0.004 (0.003)\tLoss 1.1723 (1.0989)\tPrec@1 60.000 (60.718)\n",
      "EPOCH: 52 train Results: Prec@1 60.718 Loss: 1.0989\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1796 (1.1796)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3213 (1.2845)\tPrec@1 43.750 (54.510)\n",
      "EPOCH: 52 val Results: Prec@1 54.510 Loss: 1.2845\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [53][0/390]\tTime 0.002 (0.002)\tLoss 1.0438 (1.0438)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [53][78/390]\tTime 0.002 (0.003)\tLoss 1.0290 (1.0047)\tPrec@1 62.500 (64.725)\n",
      "Epoch: [53][156/390]\tTime 0.004 (0.004)\tLoss 1.0626 (1.0303)\tPrec@1 66.406 (63.391)\n",
      "Epoch: [53][234/390]\tTime 0.004 (0.004)\tLoss 1.1392 (1.0575)\tPrec@1 56.250 (62.224)\n",
      "Epoch: [53][312/390]\tTime 0.003 (0.003)\tLoss 1.3563 (1.0749)\tPrec@1 50.000 (61.549)\n",
      "Epoch: [53][390/390]\tTime 0.001 (0.003)\tLoss 1.2209 (1.0883)\tPrec@1 55.000 (61.168)\n",
      "EPOCH: 53 train Results: Prec@1 61.168 Loss: 1.0883\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2301 (1.2301)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6162 (1.2749)\tPrec@1 43.750 (54.660)\n",
      "EPOCH: 53 val Results: Prec@1 54.660 Loss: 1.2749\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [54][0/390]\tTime 0.002 (0.002)\tLoss 0.9304 (0.9304)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [54][78/390]\tTime 0.002 (0.003)\tLoss 1.0479 (0.9977)\tPrec@1 60.938 (65.220)\n",
      "Epoch: [54][156/390]\tTime 0.002 (0.003)\tLoss 0.9909 (1.0368)\tPrec@1 66.406 (63.436)\n",
      "Epoch: [54][234/390]\tTime 0.002 (0.003)\tLoss 1.0303 (1.0613)\tPrec@1 65.625 (62.447)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2199 (1.0781)\tPrec@1 60.156 (61.789)\n",
      "Epoch: [54][390/390]\tTime 0.004 (0.003)\tLoss 1.1270 (1.0927)\tPrec@1 61.250 (61.200)\n",
      "EPOCH: 54 train Results: Prec@1 61.200 Loss: 1.0927\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0689 (1.0689)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0538 (1.2830)\tPrec@1 50.000 (54.430)\n",
      "EPOCH: 54 val Results: Prec@1 54.430 Loss: 1.2830\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.0087 (1.0087)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [55][78/390]\tTime 0.003 (0.003)\tLoss 1.0932 (0.9937)\tPrec@1 64.844 (64.775)\n",
      "Epoch: [55][156/390]\tTime 0.002 (0.003)\tLoss 1.1064 (1.0344)\tPrec@1 57.812 (63.097)\n",
      "Epoch: [55][234/390]\tTime 0.002 (0.003)\tLoss 1.1784 (1.0635)\tPrec@1 59.375 (62.091)\n",
      "Epoch: [55][312/390]\tTime 0.002 (0.003)\tLoss 0.9679 (1.0751)\tPrec@1 70.312 (61.686)\n",
      "Epoch: [55][390/390]\tTime 0.001 (0.003)\tLoss 1.0870 (1.0906)\tPrec@1 61.250 (61.170)\n",
      "EPOCH: 55 train Results: Prec@1 61.170 Loss: 1.0906\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1032 (1.1032)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0161 (1.2724)\tPrec@1 50.000 (54.370)\n",
      "EPOCH: 55 val Results: Prec@1 54.370 Loss: 1.2724\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [56][0/390]\tTime 0.002 (0.002)\tLoss 0.8574 (0.8574)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [56][78/390]\tTime 0.002 (0.002)\tLoss 0.9633 (1.0319)\tPrec@1 61.719 (62.876)\n",
      "Epoch: [56][156/390]\tTime 0.006 (0.003)\tLoss 0.9593 (1.0520)\tPrec@1 67.188 (62.580)\n",
      "Epoch: [56][234/390]\tTime 0.003 (0.003)\tLoss 1.0832 (1.0647)\tPrec@1 60.938 (62.168)\n",
      "Epoch: [56][312/390]\tTime 0.002 (0.003)\tLoss 0.9986 (1.0796)\tPrec@1 61.719 (61.594)\n",
      "Epoch: [56][390/390]\tTime 0.002 (0.003)\tLoss 1.1853 (1.0937)\tPrec@1 58.750 (61.122)\n",
      "EPOCH: 56 train Results: Prec@1 61.122 Loss: 1.0937\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0321 (1.0321)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2146 (1.2692)\tPrec@1 37.500 (54.570)\n",
      "EPOCH: 56 val Results: Prec@1 54.570 Loss: 1.2692\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [57][0/390]\tTime 0.005 (0.005)\tLoss 0.9509 (0.9509)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [57][78/390]\tTime 0.002 (0.003)\tLoss 1.1029 (1.0001)\tPrec@1 60.156 (64.626)\n",
      "Epoch: [57][156/390]\tTime 0.004 (0.003)\tLoss 1.0333 (1.0309)\tPrec@1 64.062 (63.540)\n",
      "Epoch: [57][234/390]\tTime 0.002 (0.003)\tLoss 1.1763 (1.0513)\tPrec@1 56.250 (62.653)\n",
      "Epoch: [57][312/390]\tTime 0.002 (0.003)\tLoss 1.2008 (1.0708)\tPrec@1 53.125 (61.704)\n",
      "Epoch: [57][390/390]\tTime 0.001 (0.003)\tLoss 1.1889 (1.0888)\tPrec@1 53.750 (61.136)\n",
      "EPOCH: 57 train Results: Prec@1 61.136 Loss: 1.0888\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1413 (1.1413)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2550 (1.2719)\tPrec@1 50.000 (54.960)\n",
      "EPOCH: 57 val Results: Prec@1 54.960 Loss: 1.2719\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [58][0/390]\tTime 0.012 (0.012)\tLoss 0.9783 (0.9783)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [58][78/390]\tTime 0.002 (0.003)\tLoss 1.1314 (1.0150)\tPrec@1 59.375 (63.341)\n",
      "Epoch: [58][156/390]\tTime 0.002 (0.003)\tLoss 1.2052 (1.0450)\tPrec@1 54.688 (62.694)\n",
      "Epoch: [58][234/390]\tTime 0.002 (0.003)\tLoss 1.0285 (1.0623)\tPrec@1 63.281 (62.201)\n",
      "Epoch: [58][312/390]\tTime 0.005 (0.003)\tLoss 0.9889 (1.0765)\tPrec@1 60.156 (61.586)\n",
      "Epoch: [58][390/390]\tTime 0.002 (0.003)\tLoss 1.1154 (1.0899)\tPrec@1 60.000 (61.102)\n",
      "EPOCH: 58 train Results: Prec@1 61.102 Loss: 1.0899\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1241 (1.1241)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4217 (1.2658)\tPrec@1 43.750 (54.410)\n",
      "EPOCH: 58 val Results: Prec@1 54.410 Loss: 1.2658\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [59][0/390]\tTime 0.005 (0.005)\tLoss 1.0110 (1.0110)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [59][78/390]\tTime 0.002 (0.002)\tLoss 1.0989 (1.0028)\tPrec@1 58.594 (64.695)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.2312 (1.0346)\tPrec@1 60.938 (63.376)\n",
      "Epoch: [59][234/390]\tTime 0.002 (0.003)\tLoss 1.1227 (1.0594)\tPrec@1 57.031 (62.234)\n",
      "Epoch: [59][312/390]\tTime 0.004 (0.003)\tLoss 1.0577 (1.0776)\tPrec@1 58.594 (61.594)\n",
      "Epoch: [59][390/390]\tTime 0.001 (0.003)\tLoss 1.1718 (1.0913)\tPrec@1 57.500 (61.220)\n",
      "EPOCH: 59 train Results: Prec@1 61.220 Loss: 1.0913\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2531 (1.2531)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1164 (1.2888)\tPrec@1 56.250 (54.800)\n",
      "EPOCH: 59 val Results: Prec@1 54.800 Loss: 1.2888\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [60][0/390]\tTime 0.004 (0.004)\tLoss 0.8652 (0.8652)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [60][78/390]\tTime 0.002 (0.004)\tLoss 0.9375 (1.0059)\tPrec@1 59.375 (64.072)\n",
      "Epoch: [60][156/390]\tTime 0.002 (0.003)\tLoss 0.9011 (1.0324)\tPrec@1 68.750 (63.222)\n",
      "Epoch: [60][234/390]\tTime 0.003 (0.003)\tLoss 1.1935 (1.0613)\tPrec@1 55.469 (61.928)\n",
      "Epoch: [60][312/390]\tTime 0.004 (0.003)\tLoss 1.1167 (1.0764)\tPrec@1 57.812 (61.429)\n",
      "Epoch: [60][390/390]\tTime 0.003 (0.003)\tLoss 1.1779 (1.0867)\tPrec@1 56.250 (61.084)\n",
      "EPOCH: 60 train Results: Prec@1 61.084 Loss: 1.0867\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1787 (1.1787)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0983 (1.2780)\tPrec@1 62.500 (54.750)\n",
      "EPOCH: 60 val Results: Prec@1 54.750 Loss: 1.2780\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [61][0/390]\tTime 0.003 (0.003)\tLoss 0.9075 (0.9075)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [61][78/390]\tTime 0.003 (0.003)\tLoss 1.0261 (1.0005)\tPrec@1 59.375 (64.122)\n",
      "Epoch: [61][156/390]\tTime 0.004 (0.003)\tLoss 1.1166 (1.0397)\tPrec@1 60.156 (62.575)\n",
      "Epoch: [61][234/390]\tTime 0.002 (0.003)\tLoss 1.0934 (1.0593)\tPrec@1 58.594 (61.779)\n",
      "Epoch: [61][312/390]\tTime 0.005 (0.003)\tLoss 1.1051 (1.0757)\tPrec@1 60.156 (61.200)\n",
      "Epoch: [61][390/390]\tTime 0.016 (0.003)\tLoss 1.0181 (1.0848)\tPrec@1 63.750 (60.834)\n",
      "EPOCH: 61 train Results: Prec@1 60.834 Loss: 1.0848\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1905 (1.1905)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3664 (1.2783)\tPrec@1 50.000 (54.530)\n",
      "EPOCH: 61 val Results: Prec@1 54.530 Loss: 1.2783\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [62][0/390]\tTime 0.002 (0.002)\tLoss 1.0156 (1.0156)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.1704 (1.0076)\tPrec@1 60.156 (64.498)\n",
      "Epoch: [62][156/390]\tTime 0.003 (0.003)\tLoss 1.0527 (1.0435)\tPrec@1 53.906 (63.092)\n",
      "Epoch: [62][234/390]\tTime 0.002 (0.003)\tLoss 1.2734 (1.0589)\tPrec@1 58.594 (62.533)\n",
      "Epoch: [62][312/390]\tTime 0.008 (0.003)\tLoss 1.0644 (1.0724)\tPrec@1 60.156 (61.876)\n",
      "Epoch: [62][390/390]\tTime 0.001 (0.003)\tLoss 0.9744 (1.0838)\tPrec@1 62.500 (61.356)\n",
      "EPOCH: 62 train Results: Prec@1 61.356 Loss: 1.0838\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2059 (1.2059)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1523 (1.2854)\tPrec@1 37.500 (54.610)\n",
      "EPOCH: 62 val Results: Prec@1 54.610 Loss: 1.2854\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [63][0/390]\tTime 0.003 (0.003)\tLoss 0.9111 (0.9111)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [63][78/390]\tTime 0.002 (0.003)\tLoss 1.2227 (1.0122)\tPrec@1 57.812 (64.112)\n",
      "Epoch: [63][156/390]\tTime 0.003 (0.003)\tLoss 1.1086 (1.0414)\tPrec@1 58.594 (62.838)\n",
      "Epoch: [63][234/390]\tTime 0.004 (0.003)\tLoss 1.0811 (1.0643)\tPrec@1 59.375 (62.131)\n",
      "Epoch: [63][312/390]\tTime 0.002 (0.003)\tLoss 1.2302 (1.0807)\tPrec@1 53.125 (61.502)\n",
      "Epoch: [63][390/390]\tTime 0.003 (0.003)\tLoss 1.1333 (1.0882)\tPrec@1 60.000 (61.210)\n",
      "EPOCH: 63 train Results: Prec@1 61.210 Loss: 1.0882\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2550 (1.2550)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4176 (1.2831)\tPrec@1 43.750 (54.320)\n",
      "EPOCH: 63 val Results: Prec@1 54.320 Loss: 1.2831\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [64][0/390]\tTime 0.004 (0.004)\tLoss 0.9607 (0.9607)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [64][78/390]\tTime 0.006 (0.004)\tLoss 1.1410 (0.9897)\tPrec@1 61.719 (64.142)\n",
      "Epoch: [64][156/390]\tTime 0.002 (0.004)\tLoss 1.1844 (1.0290)\tPrec@1 57.031 (63.013)\n",
      "Epoch: [64][234/390]\tTime 0.002 (0.003)\tLoss 1.1173 (1.0520)\tPrec@1 59.375 (62.327)\n",
      "Epoch: [64][312/390]\tTime 0.004 (0.003)\tLoss 1.2203 (1.0712)\tPrec@1 57.812 (61.714)\n",
      "Epoch: [64][390/390]\tTime 0.004 (0.003)\tLoss 1.4297 (1.0863)\tPrec@1 50.000 (61.262)\n",
      "EPOCH: 64 train Results: Prec@1 61.262 Loss: 1.0863\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2267 (1.2267)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1542 (1.2918)\tPrec@1 37.500 (54.490)\n",
      "EPOCH: 64 val Results: Prec@1 54.490 Loss: 1.2918\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [65][0/390]\tTime 0.002 (0.002)\tLoss 1.0159 (1.0159)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.003)\tLoss 1.0150 (1.0142)\tPrec@1 67.188 (63.914)\n",
      "Epoch: [65][156/390]\tTime 0.002 (0.003)\tLoss 1.0189 (1.0310)\tPrec@1 60.156 (63.197)\n",
      "Epoch: [65][234/390]\tTime 0.005 (0.003)\tLoss 1.1765 (1.0530)\tPrec@1 60.156 (62.397)\n",
      "Epoch: [65][312/390]\tTime 0.003 (0.003)\tLoss 1.0195 (1.0708)\tPrec@1 62.500 (61.799)\n",
      "Epoch: [65][390/390]\tTime 0.001 (0.003)\tLoss 1.0883 (1.0848)\tPrec@1 57.500 (61.306)\n",
      "EPOCH: 65 train Results: Prec@1 61.306 Loss: 1.0848\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.2230 (1.2230)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3126 (1.2674)\tPrec@1 50.000 (55.140)\n",
      "EPOCH: 65 val Results: Prec@1 55.140 Loss: 1.2674\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [66][0/390]\tTime 0.006 (0.006)\tLoss 0.9403 (0.9403)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [66][78/390]\tTime 0.003 (0.003)\tLoss 1.3549 (1.0096)\tPrec@1 48.438 (63.964)\n",
      "Epoch: [66][156/390]\tTime 0.003 (0.003)\tLoss 0.9404 (1.0313)\tPrec@1 68.750 (63.097)\n",
      "Epoch: [66][234/390]\tTime 0.002 (0.003)\tLoss 1.1959 (1.0531)\tPrec@1 56.250 (62.118)\n",
      "Epoch: [66][312/390]\tTime 0.002 (0.003)\tLoss 1.1908 (1.0730)\tPrec@1 59.375 (61.519)\n",
      "Epoch: [66][390/390]\tTime 0.002 (0.003)\tLoss 1.0376 (1.0836)\tPrec@1 60.000 (61.198)\n",
      "EPOCH: 66 train Results: Prec@1 61.198 Loss: 1.0836\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2836 (1.2836)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3214 (1.2964)\tPrec@1 56.250 (54.140)\n",
      "EPOCH: 66 val Results: Prec@1 54.140 Loss: 1.2964\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [67][0/390]\tTime 0.004 (0.004)\tLoss 1.0849 (1.0849)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [67][78/390]\tTime 0.006 (0.005)\tLoss 1.0404 (1.0011)\tPrec@1 60.156 (64.231)\n",
      "Epoch: [67][156/390]\tTime 0.002 (0.005)\tLoss 1.2238 (1.0307)\tPrec@1 50.781 (63.027)\n",
      "Epoch: [67][234/390]\tTime 0.002 (0.004)\tLoss 1.0731 (1.0578)\tPrec@1 63.281 (62.171)\n",
      "Epoch: [67][312/390]\tTime 0.002 (0.004)\tLoss 1.0631 (1.0713)\tPrec@1 64.062 (61.691)\n",
      "Epoch: [67][390/390]\tTime 0.001 (0.004)\tLoss 1.1055 (1.0863)\tPrec@1 62.500 (61.154)\n",
      "EPOCH: 67 train Results: Prec@1 61.154 Loss: 1.0863\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2239 (1.2239)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1957 (1.2860)\tPrec@1 56.250 (54.130)\n",
      "EPOCH: 67 val Results: Prec@1 54.130 Loss: 1.2860\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [68][0/390]\tTime 0.003 (0.003)\tLoss 0.9132 (0.9132)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [68][78/390]\tTime 0.002 (0.003)\tLoss 0.9608 (0.9955)\tPrec@1 65.625 (64.043)\n",
      "Epoch: [68][156/390]\tTime 0.003 (0.003)\tLoss 1.2240 (1.0403)\tPrec@1 58.594 (62.779)\n",
      "Epoch: [68][234/390]\tTime 0.004 (0.003)\tLoss 1.1511 (1.0593)\tPrec@1 63.281 (62.188)\n",
      "Epoch: [68][312/390]\tTime 0.005 (0.003)\tLoss 1.2838 (1.0724)\tPrec@1 57.812 (61.851)\n",
      "Epoch: [68][390/390]\tTime 0.003 (0.003)\tLoss 1.2565 (1.0832)\tPrec@1 58.750 (61.364)\n",
      "EPOCH: 68 train Results: Prec@1 61.364 Loss: 1.0832\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2173 (1.2173)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2183 (1.2840)\tPrec@1 37.500 (54.720)\n",
      "EPOCH: 68 val Results: Prec@1 54.720 Loss: 1.2840\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [69][0/390]\tTime 0.002 (0.002)\tLoss 0.9840 (0.9840)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [69][78/390]\tTime 0.003 (0.003)\tLoss 0.9776 (0.9914)\tPrec@1 61.719 (64.695)\n",
      "Epoch: [69][156/390]\tTime 0.002 (0.003)\tLoss 1.0735 (1.0310)\tPrec@1 60.156 (63.600)\n",
      "Epoch: [69][234/390]\tTime 0.003 (0.003)\tLoss 0.9827 (1.0536)\tPrec@1 65.625 (62.636)\n",
      "Epoch: [69][312/390]\tTime 0.002 (0.003)\tLoss 1.1644 (1.0717)\tPrec@1 56.250 (61.809)\n",
      "Epoch: [69][390/390]\tTime 0.001 (0.003)\tLoss 1.1553 (1.0849)\tPrec@1 61.250 (61.430)\n",
      "EPOCH: 69 train Results: Prec@1 61.430 Loss: 1.0849\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2355 (1.2355)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2687 (1.2830)\tPrec@1 31.250 (54.370)\n",
      "EPOCH: 69 val Results: Prec@1 54.370 Loss: 1.2830\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [70][0/390]\tTime 0.004 (0.004)\tLoss 0.8297 (0.8297)\tPrec@1 76.562 (76.562)\n",
      "Epoch: [70][78/390]\tTime 0.003 (0.003)\tLoss 1.2222 (1.0047)\tPrec@1 52.344 (64.359)\n",
      "Epoch: [70][156/390]\tTime 0.003 (0.003)\tLoss 1.2291 (1.0385)\tPrec@1 53.906 (63.047)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.003)\tLoss 0.9640 (1.0564)\tPrec@1 68.750 (62.207)\n",
      "Epoch: [70][312/390]\tTime 0.003 (0.003)\tLoss 0.9801 (1.0779)\tPrec@1 67.969 (61.369)\n",
      "Epoch: [70][390/390]\tTime 0.001 (0.002)\tLoss 1.1339 (1.0878)\tPrec@1 61.250 (61.012)\n",
      "EPOCH: 70 train Results: Prec@1 61.012 Loss: 1.0878\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1402 (1.2802)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 70 val Results: Prec@1 54.630 Loss: 1.2802\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [71][0/390]\tTime 0.005 (0.005)\tLoss 0.9452 (0.9452)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [71][78/390]\tTime 0.002 (0.003)\tLoss 1.1362 (1.0068)\tPrec@1 62.500 (63.598)\n",
      "Epoch: [71][156/390]\tTime 0.002 (0.003)\tLoss 1.0143 (1.0345)\tPrec@1 66.406 (62.809)\n",
      "Epoch: [71][234/390]\tTime 0.002 (0.003)\tLoss 1.0165 (1.0616)\tPrec@1 63.281 (61.915)\n",
      "Epoch: [71][312/390]\tTime 0.002 (0.003)\tLoss 1.1155 (1.0738)\tPrec@1 61.719 (61.497)\n",
      "Epoch: [71][390/390]\tTime 0.001 (0.003)\tLoss 0.9079 (1.0886)\tPrec@1 70.000 (60.946)\n",
      "EPOCH: 71 train Results: Prec@1 60.946 Loss: 1.0886\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1985 (1.1985)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0685 (1.2816)\tPrec@1 43.750 (54.520)\n",
      "EPOCH: 71 val Results: Prec@1 54.520 Loss: 1.2816\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [72][0/390]\tTime 0.002 (0.002)\tLoss 1.1339 (1.1339)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [72][78/390]\tTime 0.003 (0.003)\tLoss 1.2597 (1.0103)\tPrec@1 54.688 (64.013)\n",
      "Epoch: [72][156/390]\tTime 0.003 (0.003)\tLoss 1.1522 (1.0339)\tPrec@1 59.375 (63.212)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.003)\tLoss 1.1445 (1.0611)\tPrec@1 57.031 (62.181)\n",
      "Epoch: [72][312/390]\tTime 0.003 (0.003)\tLoss 1.2119 (1.0755)\tPrec@1 53.906 (61.736)\n",
      "Epoch: [72][390/390]\tTime 0.002 (0.003)\tLoss 0.9934 (1.0876)\tPrec@1 58.750 (61.220)\n",
      "EPOCH: 72 train Results: Prec@1 61.220 Loss: 1.0876\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1348 (1.1348)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1809 (1.2809)\tPrec@1 43.750 (54.540)\n",
      "EPOCH: 72 val Results: Prec@1 54.540 Loss: 1.2809\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [73][0/390]\tTime 0.003 (0.003)\tLoss 0.9702 (0.9702)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.003)\tLoss 1.1191 (1.0182)\tPrec@1 57.812 (63.835)\n",
      "Epoch: [73][156/390]\tTime 0.003 (0.003)\tLoss 1.1718 (1.0332)\tPrec@1 61.719 (62.878)\n",
      "Epoch: [73][234/390]\tTime 0.002 (0.003)\tLoss 1.0440 (1.0531)\tPrec@1 58.594 (62.031)\n",
      "Epoch: [73][312/390]\tTime 0.002 (0.003)\tLoss 0.9621 (1.0704)\tPrec@1 64.062 (61.497)\n",
      "Epoch: [73][390/390]\tTime 0.002 (0.003)\tLoss 1.1013 (1.0821)\tPrec@1 57.500 (61.188)\n",
      "EPOCH: 73 train Results: Prec@1 61.188 Loss: 1.0821\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1168 (1.1168)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2634 (1.2842)\tPrec@1 25.000 (55.060)\n",
      "EPOCH: 73 val Results: Prec@1 55.060 Loss: 1.2842\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [74][0/390]\tTime 0.012 (0.012)\tLoss 1.1154 (1.1154)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [74][78/390]\tTime 0.012 (0.007)\tLoss 0.9839 (1.0122)\tPrec@1 67.188 (64.310)\n",
      "Epoch: [74][156/390]\tTime 0.002 (0.006)\tLoss 1.1138 (1.0454)\tPrec@1 58.594 (63.077)\n",
      "Epoch: [74][234/390]\tTime 0.002 (0.005)\tLoss 0.9811 (1.0630)\tPrec@1 60.156 (62.237)\n",
      "Epoch: [74][312/390]\tTime 0.003 (0.004)\tLoss 1.1664 (1.0738)\tPrec@1 60.938 (61.719)\n",
      "Epoch: [74][390/390]\tTime 0.008 (0.004)\tLoss 1.1703 (1.0851)\tPrec@1 56.250 (61.292)\n",
      "EPOCH: 74 train Results: Prec@1 61.292 Loss: 1.0851\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1411 (1.1411)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4568 (1.2879)\tPrec@1 37.500 (54.920)\n",
      "EPOCH: 74 val Results: Prec@1 54.920 Loss: 1.2879\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [75][0/390]\tTime 0.004 (0.004)\tLoss 1.0750 (1.0750)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [75][78/390]\tTime 0.003 (0.005)\tLoss 1.1899 (0.9951)\tPrec@1 60.156 (64.359)\n",
      "Epoch: [75][156/390]\tTime 0.009 (0.004)\tLoss 0.8384 (1.0359)\tPrec@1 70.312 (62.928)\n",
      "Epoch: [75][234/390]\tTime 0.007 (0.004)\tLoss 1.1940 (1.0571)\tPrec@1 52.344 (62.111)\n",
      "Epoch: [75][312/390]\tTime 0.004 (0.005)\tLoss 1.3186 (1.0706)\tPrec@1 57.031 (61.714)\n",
      "Epoch: [75][390/390]\tTime 0.002 (0.004)\tLoss 1.0363 (1.0836)\tPrec@1 62.500 (61.274)\n",
      "EPOCH: 75 train Results: Prec@1 61.274 Loss: 1.0836\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1479 (1.1479)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2853 (1.2751)\tPrec@1 37.500 (55.090)\n",
      "EPOCH: 75 val Results: Prec@1 55.090 Loss: 1.2751\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [76][0/390]\tTime 0.003 (0.003)\tLoss 1.0391 (1.0391)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [76][78/390]\tTime 0.003 (0.004)\tLoss 1.1484 (1.0070)\tPrec@1 58.594 (63.657)\n",
      "Epoch: [76][156/390]\tTime 0.002 (0.004)\tLoss 1.0233 (1.0377)\tPrec@1 63.281 (62.565)\n",
      "Epoch: [76][234/390]\tTime 0.004 (0.004)\tLoss 1.0989 (1.0592)\tPrec@1 58.594 (61.769)\n",
      "Epoch: [76][312/390]\tTime 0.003 (0.003)\tLoss 0.9983 (1.0708)\tPrec@1 60.938 (61.566)\n",
      "Epoch: [76][390/390]\tTime 0.001 (0.003)\tLoss 1.0464 (1.0825)\tPrec@1 61.250 (61.280)\n",
      "EPOCH: 76 train Results: Prec@1 61.280 Loss: 1.0825\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1323 (1.1323)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5212 (1.2763)\tPrec@1 50.000 (54.900)\n",
      "EPOCH: 76 val Results: Prec@1 54.900 Loss: 1.2763\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [77][0/390]\tTime 0.002 (0.002)\tLoss 1.0530 (1.0530)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [77][78/390]\tTime 0.002 (0.003)\tLoss 1.2245 (1.0088)\tPrec@1 57.812 (63.786)\n",
      "Epoch: [77][156/390]\tTime 0.003 (0.004)\tLoss 1.1864 (1.0342)\tPrec@1 58.594 (62.958)\n",
      "Epoch: [77][234/390]\tTime 0.005 (0.004)\tLoss 1.0215 (1.0560)\tPrec@1 63.281 (62.387)\n",
      "Epoch: [77][312/390]\tTime 0.013 (0.004)\tLoss 1.1793 (1.0680)\tPrec@1 60.156 (61.953)\n",
      "Epoch: [77][390/390]\tTime 0.001 (0.004)\tLoss 1.2741 (1.0817)\tPrec@1 52.500 (61.496)\n",
      "EPOCH: 77 train Results: Prec@1 61.496 Loss: 1.0817\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1642 (1.1642)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2386 (1.2810)\tPrec@1 43.750 (54.890)\n",
      "EPOCH: 77 val Results: Prec@1 54.890 Loss: 1.2810\n",
      "Best Prec@1: 55.410\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [78][0/390]\tTime 0.003 (0.003)\tLoss 0.9463 (0.9463)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [78][78/390]\tTime 0.002 (0.003)\tLoss 1.2001 (1.0029)\tPrec@1 56.250 (64.310)\n",
      "Epoch: [78][156/390]\tTime 0.004 (0.004)\tLoss 1.0656 (1.0387)\tPrec@1 64.062 (63.276)\n",
      "Epoch: [78][234/390]\tTime 0.007 (0.004)\tLoss 1.2030 (1.0602)\tPrec@1 54.688 (62.583)\n",
      "Epoch: [78][312/390]\tTime 0.005 (0.003)\tLoss 1.0537 (1.0783)\tPrec@1 60.156 (61.756)\n",
      "Epoch: [78][390/390]\tTime 0.001 (0.003)\tLoss 1.1002 (1.0874)\tPrec@1 61.250 (61.452)\n",
      "EPOCH: 78 train Results: Prec@1 61.452 Loss: 1.0874\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1098 (1.1098)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3196 (1.2720)\tPrec@1 43.750 (55.750)\n",
      "EPOCH: 78 val Results: Prec@1 55.750 Loss: 1.2720\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [79][0/390]\tTime 0.003 (0.003)\tLoss 0.9042 (0.9042)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [79][78/390]\tTime 0.003 (0.003)\tLoss 1.0125 (0.9987)\tPrec@1 61.719 (65.121)\n",
      "Epoch: [79][156/390]\tTime 0.005 (0.003)\tLoss 0.9845 (1.0239)\tPrec@1 61.719 (64.023)\n",
      "Epoch: [79][234/390]\tTime 0.008 (0.003)\tLoss 1.2117 (1.0465)\tPrec@1 52.344 (63.022)\n",
      "Epoch: [79][312/390]\tTime 0.003 (0.003)\tLoss 1.2343 (1.0675)\tPrec@1 58.594 (62.161)\n",
      "Epoch: [79][390/390]\tTime 0.002 (0.003)\tLoss 1.0100 (1.0835)\tPrec@1 65.000 (61.522)\n",
      "EPOCH: 79 train Results: Prec@1 61.522 Loss: 1.0835\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1662 (1.1662)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3238 (1.2607)\tPrec@1 37.500 (55.250)\n",
      "EPOCH: 79 val Results: Prec@1 55.250 Loss: 1.2607\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [80][0/390]\tTime 0.005 (0.005)\tLoss 1.0469 (1.0469)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [80][78/390]\tTime 0.003 (0.003)\tLoss 0.9851 (1.0001)\tPrec@1 65.625 (64.428)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.1101 (1.0366)\tPrec@1 60.938 (62.674)\n",
      "Epoch: [80][234/390]\tTime 0.003 (0.003)\tLoss 1.1785 (1.0593)\tPrec@1 55.469 (62.025)\n",
      "Epoch: [80][312/390]\tTime 0.004 (0.004)\tLoss 1.2973 (1.0728)\tPrec@1 50.781 (61.519)\n",
      "Epoch: [80][390/390]\tTime 0.002 (0.003)\tLoss 1.1034 (1.0854)\tPrec@1 62.500 (61.076)\n",
      "EPOCH: 80 train Results: Prec@1 61.076 Loss: 1.0854\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1129 (1.1129)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2070 (1.2834)\tPrec@1 56.250 (54.590)\n",
      "EPOCH: 80 val Results: Prec@1 54.590 Loss: 1.2834\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [81][0/390]\tTime 0.006 (0.006)\tLoss 0.9722 (0.9722)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [81][78/390]\tTime 0.005 (0.002)\tLoss 1.0381 (1.0067)\tPrec@1 64.062 (64.458)\n",
      "Epoch: [81][156/390]\tTime 0.011 (0.003)\tLoss 0.8835 (1.0345)\tPrec@1 67.969 (63.276)\n",
      "Epoch: [81][234/390]\tTime 0.002 (0.003)\tLoss 1.0652 (1.0546)\tPrec@1 60.938 (62.460)\n",
      "Epoch: [81][312/390]\tTime 0.002 (0.003)\tLoss 1.3292 (1.0718)\tPrec@1 50.781 (61.819)\n",
      "Epoch: [81][390/390]\tTime 0.001 (0.003)\tLoss 0.9066 (1.0858)\tPrec@1 67.500 (61.186)\n",
      "EPOCH: 81 train Results: Prec@1 61.186 Loss: 1.0858\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2348 (1.2348)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1713 (1.2779)\tPrec@1 31.250 (54.670)\n",
      "EPOCH: 81 val Results: Prec@1 54.670 Loss: 1.2779\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [82][0/390]\tTime 0.004 (0.004)\tLoss 0.9638 (0.9638)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [82][78/390]\tTime 0.002 (0.004)\tLoss 0.9524 (1.0009)\tPrec@1 61.719 (64.330)\n",
      "Epoch: [82][156/390]\tTime 0.002 (0.004)\tLoss 1.0425 (1.0224)\tPrec@1 64.844 (63.555)\n",
      "Epoch: [82][234/390]\tTime 0.004 (0.003)\tLoss 1.0164 (1.0459)\tPrec@1 62.500 (62.729)\n",
      "Epoch: [82][312/390]\tTime 0.002 (0.003)\tLoss 1.0072 (1.0612)\tPrec@1 64.844 (62.061)\n",
      "Epoch: [82][390/390]\tTime 0.001 (0.003)\tLoss 1.1654 (1.0812)\tPrec@1 48.750 (61.398)\n",
      "EPOCH: 82 train Results: Prec@1 61.398 Loss: 1.0812\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.2151 (1.2151)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1796 (1.2666)\tPrec@1 50.000 (55.410)\n",
      "EPOCH: 82 val Results: Prec@1 55.410 Loss: 1.2666\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [83][0/390]\tTime 0.005 (0.005)\tLoss 0.9611 (0.9611)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [83][78/390]\tTime 0.002 (0.002)\tLoss 1.0640 (0.9992)\tPrec@1 66.406 (64.428)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.002)\tLoss 1.2545 (1.0409)\tPrec@1 53.125 (62.933)\n",
      "Epoch: [83][234/390]\tTime 0.003 (0.002)\tLoss 1.1070 (1.0615)\tPrec@1 57.812 (62.084)\n",
      "Epoch: [83][312/390]\tTime 0.002 (0.003)\tLoss 1.2411 (1.0757)\tPrec@1 55.469 (61.554)\n",
      "Epoch: [83][390/390]\tTime 0.001 (0.003)\tLoss 1.1519 (1.0845)\tPrec@1 61.250 (61.212)\n",
      "EPOCH: 83 train Results: Prec@1 61.212 Loss: 1.0845\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1838 (1.1838)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2389 (1.2743)\tPrec@1 37.500 (55.110)\n",
      "EPOCH: 83 val Results: Prec@1 55.110 Loss: 1.2743\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [84][0/390]\tTime 0.004 (0.004)\tLoss 0.7921 (0.7921)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [84][78/390]\tTime 0.002 (0.003)\tLoss 0.9598 (1.0236)\tPrec@1 69.531 (63.479)\n",
      "Epoch: [84][156/390]\tTime 0.006 (0.003)\tLoss 1.0326 (1.0374)\tPrec@1 67.969 (62.843)\n",
      "Epoch: [84][234/390]\tTime 0.002 (0.003)\tLoss 1.0188 (1.0592)\tPrec@1 63.281 (62.108)\n",
      "Epoch: [84][312/390]\tTime 0.005 (0.003)\tLoss 0.9564 (1.0709)\tPrec@1 69.531 (61.731)\n",
      "Epoch: [84][390/390]\tTime 0.001 (0.003)\tLoss 1.4626 (1.0813)\tPrec@1 43.750 (61.334)\n",
      "EPOCH: 84 train Results: Prec@1 61.334 Loss: 1.0813\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1941 (1.1941)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0037 (1.2935)\tPrec@1 68.750 (54.860)\n",
      "EPOCH: 84 val Results: Prec@1 54.860 Loss: 1.2935\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [85][0/390]\tTime 0.005 (0.005)\tLoss 0.8715 (0.8715)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [85][78/390]\tTime 0.003 (0.003)\tLoss 0.8763 (0.9886)\tPrec@1 70.312 (65.160)\n",
      "Epoch: [85][156/390]\tTime 0.003 (0.003)\tLoss 0.8629 (1.0207)\tPrec@1 68.750 (63.734)\n",
      "Epoch: [85][234/390]\tTime 0.003 (0.003)\tLoss 1.2602 (1.0461)\tPrec@1 53.125 (62.643)\n",
      "Epoch: [85][312/390]\tTime 0.007 (0.003)\tLoss 0.9861 (1.0647)\tPrec@1 65.625 (62.096)\n",
      "Epoch: [85][390/390]\tTime 0.001 (0.003)\tLoss 1.1460 (1.0815)\tPrec@1 60.000 (61.372)\n",
      "EPOCH: 85 train Results: Prec@1 61.372 Loss: 1.0815\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1734 (1.1734)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2423 (1.2802)\tPrec@1 37.500 (54.720)\n",
      "EPOCH: 85 val Results: Prec@1 54.720 Loss: 1.2802\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [86][0/390]\tTime 0.004 (0.004)\tLoss 1.0046 (1.0046)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [86][78/390]\tTime 0.002 (0.004)\tLoss 0.9176 (1.0197)\tPrec@1 66.406 (63.637)\n",
      "Epoch: [86][156/390]\tTime 0.007 (0.003)\tLoss 0.9319 (1.0286)\tPrec@1 65.625 (63.575)\n",
      "Epoch: [86][234/390]\tTime 0.002 (0.003)\tLoss 1.1375 (1.0493)\tPrec@1 57.031 (62.749)\n",
      "Epoch: [86][312/390]\tTime 0.003 (0.003)\tLoss 1.2378 (1.0635)\tPrec@1 56.250 (62.288)\n",
      "Epoch: [86][390/390]\tTime 0.001 (0.003)\tLoss 1.1754 (1.0788)\tPrec@1 51.250 (61.590)\n",
      "EPOCH: 86 train Results: Prec@1 61.590 Loss: 1.0788\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1388 (1.1388)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1452 (1.2783)\tPrec@1 50.000 (54.900)\n",
      "EPOCH: 86 val Results: Prec@1 54.900 Loss: 1.2783\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [87][0/390]\tTime 0.003 (0.003)\tLoss 0.9953 (0.9953)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [87][78/390]\tTime 0.002 (0.003)\tLoss 0.8337 (0.9786)\tPrec@1 67.969 (64.953)\n",
      "Epoch: [87][156/390]\tTime 0.002 (0.003)\tLoss 1.1659 (1.0223)\tPrec@1 57.031 (63.356)\n",
      "Epoch: [87][234/390]\tTime 0.002 (0.003)\tLoss 1.1176 (1.0459)\tPrec@1 58.594 (62.430)\n",
      "Epoch: [87][312/390]\tTime 0.002 (0.002)\tLoss 1.2256 (1.0666)\tPrec@1 54.688 (61.796)\n",
      "Epoch: [87][390/390]\tTime 0.002 (0.003)\tLoss 1.1633 (1.0779)\tPrec@1 56.250 (61.324)\n",
      "EPOCH: 87 train Results: Prec@1 61.324 Loss: 1.0779\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1215 (1.1215)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9687 (1.2809)\tPrec@1 50.000 (55.040)\n",
      "EPOCH: 87 val Results: Prec@1 55.040 Loss: 1.2809\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [88][0/390]\tTime 0.002 (0.002)\tLoss 1.0346 (1.0346)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [88][78/390]\tTime 0.002 (0.003)\tLoss 0.7994 (0.9916)\tPrec@1 74.219 (65.081)\n",
      "Epoch: [88][156/390]\tTime 0.002 (0.003)\tLoss 1.1364 (1.0330)\tPrec@1 53.906 (63.286)\n",
      "Epoch: [88][234/390]\tTime 0.003 (0.004)\tLoss 1.1582 (1.0582)\tPrec@1 58.594 (62.284)\n",
      "Epoch: [88][312/390]\tTime 0.003 (0.003)\tLoss 1.0129 (1.0734)\tPrec@1 60.938 (61.881)\n",
      "Epoch: [88][390/390]\tTime 0.001 (0.003)\tLoss 1.3663 (1.0855)\tPrec@1 52.500 (61.424)\n",
      "EPOCH: 88 train Results: Prec@1 61.424 Loss: 1.0855\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2538 (1.2538)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0255 (1.2754)\tPrec@1 62.500 (55.150)\n",
      "EPOCH: 88 val Results: Prec@1 55.150 Loss: 1.2754\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [89][0/390]\tTime 0.004 (0.004)\tLoss 0.9878 (0.9878)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.003)\tLoss 0.9846 (1.0128)\tPrec@1 62.500 (63.875)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1321 (1.0318)\tPrec@1 57.812 (63.102)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 1.0720 (1.0478)\tPrec@1 60.156 (62.434)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.1276 (1.0663)\tPrec@1 58.594 (62.036)\n",
      "Epoch: [89][390/390]\tTime 0.001 (0.003)\tLoss 1.2138 (1.0804)\tPrec@1 52.500 (61.382)\n",
      "EPOCH: 89 train Results: Prec@1 61.382 Loss: 1.0804\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2078 (1.2078)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4813 (1.2789)\tPrec@1 43.750 (55.170)\n",
      "EPOCH: 89 val Results: Prec@1 55.170 Loss: 1.2789\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [90][0/390]\tTime 0.002 (0.002)\tLoss 1.0404 (1.0404)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [90][78/390]\tTime 0.002 (0.003)\tLoss 1.0199 (0.9944)\tPrec@1 67.188 (65.150)\n",
      "Epoch: [90][156/390]\tTime 0.002 (0.003)\tLoss 1.1561 (1.0269)\tPrec@1 61.719 (63.898)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.1131 (1.0527)\tPrec@1 58.594 (62.680)\n",
      "Epoch: [90][312/390]\tTime 0.003 (0.003)\tLoss 1.1972 (1.0637)\tPrec@1 62.500 (62.193)\n",
      "Epoch: [90][390/390]\tTime 0.002 (0.003)\tLoss 0.9744 (1.0773)\tPrec@1 63.750 (61.660)\n",
      "EPOCH: 90 train Results: Prec@1 61.660 Loss: 1.0773\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2024 (1.2024)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3285 (1.3007)\tPrec@1 50.000 (53.690)\n",
      "EPOCH: 90 val Results: Prec@1 53.690 Loss: 1.3007\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [91][0/390]\tTime 0.005 (0.005)\tLoss 1.1000 (1.1000)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [91][78/390]\tTime 0.007 (0.003)\tLoss 1.0058 (1.0033)\tPrec@1 59.375 (64.676)\n",
      "Epoch: [91][156/390]\tTime 0.003 (0.003)\tLoss 1.1491 (1.0329)\tPrec@1 53.125 (63.331)\n",
      "Epoch: [91][234/390]\tTime 0.003 (0.003)\tLoss 1.0264 (1.0504)\tPrec@1 63.281 (62.660)\n",
      "Epoch: [91][312/390]\tTime 0.002 (0.003)\tLoss 1.0933 (1.0698)\tPrec@1 58.594 (61.883)\n",
      "Epoch: [91][390/390]\tTime 0.001 (0.003)\tLoss 1.3490 (1.0846)\tPrec@1 52.500 (61.344)\n",
      "EPOCH: 91 train Results: Prec@1 61.344 Loss: 1.0846\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2286 (1.2286)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0113 (1.2943)\tPrec@1 50.000 (54.000)\n",
      "EPOCH: 91 val Results: Prec@1 54.000 Loss: 1.2943\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [92][0/390]\tTime 0.005 (0.005)\tLoss 1.1220 (1.1220)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [92][78/390]\tTime 0.004 (0.003)\tLoss 0.9709 (1.0014)\tPrec@1 64.844 (64.053)\n",
      "Epoch: [92][156/390]\tTime 0.011 (0.003)\tLoss 1.1035 (1.0437)\tPrec@1 60.156 (62.624)\n",
      "Epoch: [92][234/390]\tTime 0.002 (0.003)\tLoss 1.1086 (1.0552)\tPrec@1 62.500 (62.244)\n",
      "Epoch: [92][312/390]\tTime 0.002 (0.003)\tLoss 1.1370 (1.0682)\tPrec@1 62.500 (61.871)\n",
      "Epoch: [92][390/390]\tTime 0.006 (0.003)\tLoss 1.0692 (1.0802)\tPrec@1 63.750 (61.450)\n",
      "EPOCH: 92 train Results: Prec@1 61.450 Loss: 1.0802\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2136 (1.2136)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1938 (1.2760)\tPrec@1 37.500 (54.830)\n",
      "EPOCH: 92 val Results: Prec@1 54.830 Loss: 1.2760\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [93][0/390]\tTime 0.004 (0.004)\tLoss 1.0199 (1.0199)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.004)\tLoss 0.9869 (0.9749)\tPrec@1 67.969 (65.576)\n",
      "Epoch: [93][156/390]\tTime 0.004 (0.003)\tLoss 1.1175 (1.0159)\tPrec@1 62.500 (64.271)\n",
      "Epoch: [93][234/390]\tTime 0.002 (0.003)\tLoss 1.1473 (1.0442)\tPrec@1 57.812 (62.906)\n",
      "Epoch: [93][312/390]\tTime 0.003 (0.003)\tLoss 1.2491 (1.0633)\tPrec@1 57.031 (62.198)\n",
      "Epoch: [93][390/390]\tTime 0.001 (0.003)\tLoss 1.0890 (1.0773)\tPrec@1 58.750 (61.604)\n",
      "EPOCH: 93 train Results: Prec@1 61.604 Loss: 1.0773\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1027 (1.1027)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.7556 (1.2832)\tPrec@1 75.000 (54.490)\n",
      "EPOCH: 93 val Results: Prec@1 54.490 Loss: 1.2832\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [94][0/390]\tTime 0.005 (0.005)\tLoss 0.8897 (0.8897)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [94][78/390]\tTime 0.003 (0.003)\tLoss 1.1343 (0.9867)\tPrec@1 59.375 (64.666)\n",
      "Epoch: [94][156/390]\tTime 0.003 (0.003)\tLoss 1.0315 (1.0228)\tPrec@1 64.844 (63.311)\n",
      "Epoch: [94][234/390]\tTime 0.002 (0.003)\tLoss 0.9525 (1.0496)\tPrec@1 64.062 (62.330)\n",
      "Epoch: [94][312/390]\tTime 0.002 (0.003)\tLoss 1.1564 (1.0654)\tPrec@1 57.031 (61.806)\n",
      "Epoch: [94][390/390]\tTime 0.001 (0.003)\tLoss 1.3364 (1.0789)\tPrec@1 63.750 (61.446)\n",
      "EPOCH: 94 train Results: Prec@1 61.446 Loss: 1.0789\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2195 (1.2195)\tPrec@1 49.219 (49.219)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5944 (1.2776)\tPrec@1 43.750 (54.460)\n",
      "EPOCH: 94 val Results: Prec@1 54.460 Loss: 1.2776\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [95][0/390]\tTime 0.002 (0.002)\tLoss 0.8709 (0.8709)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [95][78/390]\tTime 0.003 (0.003)\tLoss 1.0887 (0.9855)\tPrec@1 60.938 (65.071)\n",
      "Epoch: [95][156/390]\tTime 0.002 (0.003)\tLoss 0.9701 (1.0339)\tPrec@1 63.281 (63.227)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.002)\tLoss 1.0828 (1.0509)\tPrec@1 62.500 (62.503)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.002)\tLoss 1.0806 (1.0656)\tPrec@1 57.812 (61.926)\n",
      "Epoch: [95][390/390]\tTime 0.001 (0.002)\tLoss 1.2926 (1.0800)\tPrec@1 53.750 (61.260)\n",
      "EPOCH: 95 train Results: Prec@1 61.260 Loss: 1.0800\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1857 (1.1857)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0441 (1.2682)\tPrec@1 50.000 (55.570)\n",
      "EPOCH: 95 val Results: Prec@1 55.570 Loss: 1.2682\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [96][0/390]\tTime 0.004 (0.004)\tLoss 0.9730 (0.9730)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [96][78/390]\tTime 0.003 (0.003)\tLoss 1.0957 (0.9939)\tPrec@1 58.594 (64.646)\n",
      "Epoch: [96][156/390]\tTime 0.003 (0.002)\tLoss 1.0395 (1.0299)\tPrec@1 60.156 (63.301)\n",
      "Epoch: [96][234/390]\tTime 0.002 (0.003)\tLoss 1.0087 (1.0571)\tPrec@1 64.062 (62.301)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 1.1273 (1.0658)\tPrec@1 57.812 (61.913)\n",
      "Epoch: [96][390/390]\tTime 0.005 (0.003)\tLoss 1.1218 (1.0776)\tPrec@1 56.250 (61.426)\n",
      "EPOCH: 96 train Results: Prec@1 61.426 Loss: 1.0776\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1726 (1.1726)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0853 (1.2807)\tPrec@1 50.000 (54.810)\n",
      "EPOCH: 96 val Results: Prec@1 54.810 Loss: 1.2807\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [97][0/390]\tTime 0.003 (0.003)\tLoss 1.0558 (1.0558)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [97][78/390]\tTime 0.002 (0.003)\tLoss 1.0347 (0.9880)\tPrec@1 67.188 (65.378)\n",
      "Epoch: [97][156/390]\tTime 0.002 (0.003)\tLoss 1.1157 (1.0205)\tPrec@1 57.031 (63.893)\n",
      "Epoch: [97][234/390]\tTime 0.003 (0.003)\tLoss 1.3426 (1.0427)\tPrec@1 50.000 (63.098)\n",
      "Epoch: [97][312/390]\tTime 0.003 (0.003)\tLoss 1.1280 (1.0613)\tPrec@1 59.375 (62.463)\n",
      "Epoch: [97][390/390]\tTime 0.002 (0.003)\tLoss 0.9662 (1.0754)\tPrec@1 70.000 (61.940)\n",
      "EPOCH: 97 train Results: Prec@1 61.940 Loss: 1.0754\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1958 (1.1958)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0810 (1.3002)\tPrec@1 43.750 (54.280)\n",
      "EPOCH: 97 val Results: Prec@1 54.280 Loss: 1.3002\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [98][0/390]\tTime 0.003 (0.003)\tLoss 1.0367 (1.0367)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [98][78/390]\tTime 0.002 (0.003)\tLoss 0.8078 (0.9894)\tPrec@1 71.094 (64.587)\n",
      "Epoch: [98][156/390]\tTime 0.002 (0.003)\tLoss 1.2158 (1.0284)\tPrec@1 55.469 (63.291)\n",
      "Epoch: [98][234/390]\tTime 0.002 (0.003)\tLoss 1.0697 (1.0465)\tPrec@1 62.500 (62.666)\n",
      "Epoch: [98][312/390]\tTime 0.004 (0.003)\tLoss 1.2104 (1.0641)\tPrec@1 52.344 (62.146)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.003)\tLoss 1.2273 (1.0787)\tPrec@1 56.250 (61.650)\n",
      "EPOCH: 98 train Results: Prec@1 61.650 Loss: 1.0787\n",
      "Test: [0/78]\tTime 0.014 (0.014)\tLoss 1.1914 (1.1914)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2972 (1.2886)\tPrec@1 50.000 (54.100)\n",
      "EPOCH: 98 val Results: Prec@1 54.100 Loss: 1.2886\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [99][0/390]\tTime 0.005 (0.005)\tLoss 0.9418 (0.9418)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [99][78/390]\tTime 0.003 (0.005)\tLoss 0.8428 (0.9928)\tPrec@1 67.188 (64.844)\n",
      "Epoch: [99][156/390]\tTime 0.002 (0.004)\tLoss 1.0182 (1.0267)\tPrec@1 67.969 (63.436)\n",
      "Epoch: [99][234/390]\tTime 0.015 (0.004)\tLoss 1.3174 (1.0489)\tPrec@1 54.688 (62.537)\n",
      "Epoch: [99][312/390]\tTime 0.002 (0.003)\tLoss 1.0939 (1.0636)\tPrec@1 59.375 (62.083)\n",
      "Epoch: [99][390/390]\tTime 0.001 (0.003)\tLoss 1.0615 (1.0744)\tPrec@1 61.250 (61.672)\n",
      "EPOCH: 99 train Results: Prec@1 61.672 Loss: 1.0744\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1867 (1.1867)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4689 (1.2822)\tPrec@1 43.750 (55.280)\n",
      "EPOCH: 99 val Results: Prec@1 55.280 Loss: 1.2822\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [100][0/390]\tTime 0.002 (0.002)\tLoss 0.9315 (0.9315)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [100][78/390]\tTime 0.002 (0.003)\tLoss 1.0473 (1.0006)\tPrec@1 57.031 (64.478)\n",
      "Epoch: [100][156/390]\tTime 0.002 (0.003)\tLoss 1.2566 (1.0327)\tPrec@1 55.469 (63.336)\n",
      "Epoch: [100][234/390]\tTime 0.004 (0.003)\tLoss 1.1432 (1.0553)\tPrec@1 59.375 (62.360)\n",
      "Epoch: [100][312/390]\tTime 0.002 (0.003)\tLoss 1.0871 (1.0734)\tPrec@1 61.719 (61.711)\n",
      "Epoch: [100][390/390]\tTime 0.003 (0.003)\tLoss 1.2778 (1.0832)\tPrec@1 56.250 (61.412)\n",
      "EPOCH: 100 train Results: Prec@1 61.412 Loss: 1.0832\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2924 (1.2924)\tPrec@1 52.344 (52.344)\n",
      "Test: [78/78]\tTime 0.006 (0.001)\tLoss 1.0518 (1.2763)\tPrec@1 50.000 (55.030)\n",
      "EPOCH: 100 val Results: Prec@1 55.030 Loss: 1.2763\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [101][0/390]\tTime 0.013 (0.013)\tLoss 0.9703 (0.9703)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.003)\tLoss 0.9242 (1.0013)\tPrec@1 69.531 (64.646)\n",
      "Epoch: [101][156/390]\tTime 0.002 (0.003)\tLoss 1.1456 (1.0303)\tPrec@1 60.156 (63.346)\n",
      "Epoch: [101][234/390]\tTime 0.007 (0.003)\tLoss 1.0393 (1.0548)\tPrec@1 66.406 (62.440)\n",
      "Epoch: [101][312/390]\tTime 0.003 (0.003)\tLoss 1.0942 (1.0650)\tPrec@1 59.375 (62.076)\n",
      "Epoch: [101][390/390]\tTime 0.002 (0.003)\tLoss 1.3067 (1.0788)\tPrec@1 56.250 (61.642)\n",
      "EPOCH: 101 train Results: Prec@1 61.642 Loss: 1.0788\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.2279 (1.2279)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1665 (1.2940)\tPrec@1 43.750 (53.870)\n",
      "EPOCH: 101 val Results: Prec@1 53.870 Loss: 1.2940\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [102][0/390]\tTime 0.004 (0.004)\tLoss 1.0263 (1.0263)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [102][78/390]\tTime 0.005 (0.004)\tLoss 1.0344 (0.9848)\tPrec@1 59.375 (64.765)\n",
      "Epoch: [102][156/390]\tTime 0.004 (0.004)\tLoss 0.9238 (1.0248)\tPrec@1 68.750 (63.525)\n",
      "Epoch: [102][234/390]\tTime 0.002 (0.003)\tLoss 1.1358 (1.0458)\tPrec@1 60.156 (62.776)\n",
      "Epoch: [102][312/390]\tTime 0.003 (0.003)\tLoss 1.1823 (1.0604)\tPrec@1 58.594 (62.330)\n",
      "Epoch: [102][390/390]\tTime 0.028 (0.004)\tLoss 1.0826 (1.0705)\tPrec@1 56.250 (61.856)\n",
      "EPOCH: 102 train Results: Prec@1 61.856 Loss: 1.0705\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1666 (1.1666)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3288 (1.2882)\tPrec@1 50.000 (54.530)\n",
      "EPOCH: 102 val Results: Prec@1 54.530 Loss: 1.2882\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [103][0/390]\tTime 0.002 (0.002)\tLoss 1.0088 (1.0088)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [103][78/390]\tTime 0.002 (0.004)\tLoss 1.0558 (1.0053)\tPrec@1 60.156 (64.003)\n",
      "Epoch: [103][156/390]\tTime 0.005 (0.004)\tLoss 1.0181 (1.0314)\tPrec@1 64.062 (63.261)\n",
      "Epoch: [103][234/390]\tTime 0.003 (0.005)\tLoss 1.1320 (1.0502)\tPrec@1 63.281 (62.736)\n",
      "Epoch: [103][312/390]\tTime 0.002 (0.004)\tLoss 1.1865 (1.0661)\tPrec@1 52.344 (62.195)\n",
      "Epoch: [103][390/390]\tTime 0.001 (0.005)\tLoss 1.3461 (1.0780)\tPrec@1 58.750 (61.776)\n",
      "EPOCH: 103 train Results: Prec@1 61.776 Loss: 1.0780\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1620 (1.1620)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.1799 (1.2962)\tPrec@1 43.750 (53.670)\n",
      "EPOCH: 103 val Results: Prec@1 53.670 Loss: 1.2962\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [104][0/390]\tTime 0.005 (0.005)\tLoss 0.9669 (0.9669)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.005)\tLoss 1.0013 (1.0127)\tPrec@1 62.500 (64.161)\n",
      "Epoch: [104][156/390]\tTime 0.007 (0.005)\tLoss 1.0112 (1.0332)\tPrec@1 63.281 (63.376)\n",
      "Epoch: [104][234/390]\tTime 0.009 (0.005)\tLoss 1.0231 (1.0485)\tPrec@1 62.500 (62.812)\n",
      "Epoch: [104][312/390]\tTime 0.007 (0.004)\tLoss 1.0191 (1.0686)\tPrec@1 64.062 (61.981)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.005)\tLoss 1.1089 (1.0799)\tPrec@1 58.750 (61.514)\n",
      "EPOCH: 104 train Results: Prec@1 61.514 Loss: 1.0799\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0722 (1.0722)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3888 (1.2717)\tPrec@1 50.000 (55.410)\n",
      "EPOCH: 104 val Results: Prec@1 55.410 Loss: 1.2717\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [105][0/390]\tTime 0.003 (0.003)\tLoss 0.8870 (0.8870)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [105][78/390]\tTime 0.006 (0.004)\tLoss 1.0611 (1.0098)\tPrec@1 59.375 (64.231)\n",
      "Epoch: [105][156/390]\tTime 0.003 (0.004)\tLoss 1.1061 (1.0347)\tPrec@1 62.500 (63.271)\n",
      "Epoch: [105][234/390]\tTime 0.004 (0.004)\tLoss 1.2284 (1.0501)\tPrec@1 55.469 (62.806)\n",
      "Epoch: [105][312/390]\tTime 0.004 (0.004)\tLoss 1.2297 (1.0651)\tPrec@1 58.594 (62.233)\n",
      "Epoch: [105][390/390]\tTime 0.001 (0.004)\tLoss 0.8889 (1.0803)\tPrec@1 66.250 (61.536)\n",
      "EPOCH: 105 train Results: Prec@1 61.536 Loss: 1.0803\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0876 (1.0876)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3277 (1.2935)\tPrec@1 37.500 (54.660)\n",
      "EPOCH: 105 val Results: Prec@1 54.660 Loss: 1.2935\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [106][0/390]\tTime 0.002 (0.002)\tLoss 0.9348 (0.9348)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [106][78/390]\tTime 0.002 (0.004)\tLoss 0.9721 (0.9825)\tPrec@1 64.062 (64.686)\n",
      "Epoch: [106][156/390]\tTime 0.002 (0.004)\tLoss 1.1280 (1.0226)\tPrec@1 60.938 (63.316)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.004)\tLoss 1.1222 (1.0459)\tPrec@1 63.281 (62.540)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.003)\tLoss 1.1280 (1.0657)\tPrec@1 59.375 (61.878)\n",
      "Epoch: [106][390/390]\tTime 0.003 (0.003)\tLoss 1.3041 (1.0771)\tPrec@1 60.000 (61.500)\n",
      "EPOCH: 106 train Results: Prec@1 61.500 Loss: 1.0771\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0495 (1.0495)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4662 (1.2970)\tPrec@1 25.000 (54.920)\n",
      "EPOCH: 106 val Results: Prec@1 54.920 Loss: 1.2970\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [107][0/390]\tTime 0.003 (0.003)\tLoss 0.8212 (0.8212)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [107][78/390]\tTime 0.002 (0.003)\tLoss 1.2024 (0.9978)\tPrec@1 55.469 (64.794)\n",
      "Epoch: [107][156/390]\tTime 0.008 (0.003)\tLoss 1.0881 (1.0225)\tPrec@1 59.375 (63.371)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.004)\tLoss 1.1052 (1.0406)\tPrec@1 63.281 (62.699)\n",
      "Epoch: [107][312/390]\tTime 0.003 (0.003)\tLoss 1.0400 (1.0638)\tPrec@1 60.938 (61.981)\n",
      "Epoch: [107][390/390]\tTime 0.001 (0.003)\tLoss 1.1164 (1.0785)\tPrec@1 61.250 (61.592)\n",
      "EPOCH: 107 train Results: Prec@1 61.592 Loss: 1.0785\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2432 (1.2432)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2828 (1.2997)\tPrec@1 43.750 (54.180)\n",
      "EPOCH: 107 val Results: Prec@1 54.180 Loss: 1.2997\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [108][0/390]\tTime 0.002 (0.002)\tLoss 1.0110 (1.0110)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [108][78/390]\tTime 0.005 (0.004)\tLoss 1.0814 (0.9932)\tPrec@1 60.156 (64.854)\n",
      "Epoch: [108][156/390]\tTime 0.002 (0.004)\tLoss 0.9991 (1.0288)\tPrec@1 65.625 (63.540)\n",
      "Epoch: [108][234/390]\tTime 0.002 (0.004)\tLoss 1.1876 (1.0485)\tPrec@1 53.906 (62.660)\n",
      "Epoch: [108][312/390]\tTime 0.002 (0.004)\tLoss 1.2056 (1.0646)\tPrec@1 54.688 (62.018)\n",
      "Epoch: [108][390/390]\tTime 0.002 (0.004)\tLoss 1.1498 (1.0782)\tPrec@1 55.000 (61.604)\n",
      "EPOCH: 108 train Results: Prec@1 61.604 Loss: 1.0782\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2123 (1.2123)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4597 (1.2873)\tPrec@1 43.750 (53.840)\n",
      "EPOCH: 108 val Results: Prec@1 53.840 Loss: 1.2873\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [109][0/390]\tTime 0.003 (0.003)\tLoss 0.9678 (0.9678)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [109][78/390]\tTime 0.002 (0.003)\tLoss 1.0274 (1.0082)\tPrec@1 63.281 (64.072)\n",
      "Epoch: [109][156/390]\tTime 0.002 (0.003)\tLoss 1.1793 (1.0312)\tPrec@1 60.156 (63.147)\n",
      "Epoch: [109][234/390]\tTime 0.003 (0.004)\tLoss 1.0479 (1.0507)\tPrec@1 64.062 (62.493)\n",
      "Epoch: [109][312/390]\tTime 0.003 (0.004)\tLoss 1.0509 (1.0613)\tPrec@1 60.156 (62.146)\n",
      "Epoch: [109][390/390]\tTime 0.002 (0.004)\tLoss 1.1272 (1.0729)\tPrec@1 61.250 (61.688)\n",
      "EPOCH: 109 train Results: Prec@1 61.688 Loss: 1.0729\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1462 (1.1462)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4520 (1.2767)\tPrec@1 18.750 (55.470)\n",
      "EPOCH: 109 val Results: Prec@1 55.470 Loss: 1.2767\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [110][0/390]\tTime 0.004 (0.004)\tLoss 1.0590 (1.0590)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 1.0304 (1.0051)\tPrec@1 61.719 (64.171)\n",
      "Epoch: [110][156/390]\tTime 0.004 (0.003)\tLoss 0.9433 (1.0296)\tPrec@1 66.406 (63.261)\n",
      "Epoch: [110][234/390]\tTime 0.007 (0.003)\tLoss 1.0981 (1.0481)\tPrec@1 56.250 (62.520)\n",
      "Epoch: [110][312/390]\tTime 0.002 (0.003)\tLoss 1.0585 (1.0648)\tPrec@1 62.500 (61.986)\n",
      "Epoch: [110][390/390]\tTime 0.003 (0.003)\tLoss 1.1803 (1.0753)\tPrec@1 57.500 (61.620)\n",
      "EPOCH: 110 train Results: Prec@1 61.620 Loss: 1.0753\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1419 (1.1419)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3221 (1.2858)\tPrec@1 43.750 (54.560)\n",
      "EPOCH: 110 val Results: Prec@1 54.560 Loss: 1.2858\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [111][0/390]\tTime 0.005 (0.005)\tLoss 0.9990 (0.9990)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [111][78/390]\tTime 0.004 (0.004)\tLoss 1.0188 (0.9932)\tPrec@1 60.938 (64.636)\n",
      "Epoch: [111][156/390]\tTime 0.002 (0.004)\tLoss 1.0545 (1.0308)\tPrec@1 54.688 (63.291)\n",
      "Epoch: [111][234/390]\tTime 0.003 (0.004)\tLoss 1.2339 (1.0533)\tPrec@1 58.594 (62.626)\n",
      "Epoch: [111][312/390]\tTime 0.005 (0.004)\tLoss 1.1178 (1.0684)\tPrec@1 62.500 (62.066)\n",
      "Epoch: [111][390/390]\tTime 0.001 (0.004)\tLoss 1.0285 (1.0777)\tPrec@1 65.000 (61.792)\n",
      "EPOCH: 111 train Results: Prec@1 61.792 Loss: 1.0777\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1902 (1.1902)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2592 (1.3060)\tPrec@1 56.250 (54.060)\n",
      "EPOCH: 111 val Results: Prec@1 54.060 Loss: 1.3060\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [112][0/390]\tTime 0.006 (0.006)\tLoss 0.7914 (0.7914)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [112][78/390]\tTime 0.003 (0.003)\tLoss 0.9890 (0.9957)\tPrec@1 63.281 (64.616)\n",
      "Epoch: [112][156/390]\tTime 0.013 (0.003)\tLoss 1.1362 (1.0291)\tPrec@1 57.031 (63.177)\n",
      "Epoch: [112][234/390]\tTime 0.002 (0.003)\tLoss 1.1448 (1.0454)\tPrec@1 62.500 (62.663)\n",
      "Epoch: [112][312/390]\tTime 0.003 (0.003)\tLoss 1.0694 (1.0694)\tPrec@1 58.594 (61.854)\n",
      "Epoch: [112][390/390]\tTime 0.002 (0.003)\tLoss 1.2623 (1.0779)\tPrec@1 55.000 (61.572)\n",
      "EPOCH: 112 train Results: Prec@1 61.572 Loss: 1.0779\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0937 (1.0937)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2804 (1.2980)\tPrec@1 37.500 (53.780)\n",
      "EPOCH: 112 val Results: Prec@1 53.780 Loss: 1.2980\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [113][0/390]\tTime 0.002 (0.002)\tLoss 0.9806 (0.9806)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [113][78/390]\tTime 0.002 (0.003)\tLoss 0.9443 (0.9923)\tPrec@1 67.969 (64.972)\n",
      "Epoch: [113][156/390]\tTime 0.003 (0.003)\tLoss 1.0400 (1.0168)\tPrec@1 60.938 (63.689)\n",
      "Epoch: [113][234/390]\tTime 0.003 (0.003)\tLoss 0.8918 (1.0391)\tPrec@1 72.656 (62.919)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.003)\tLoss 1.0155 (1.0564)\tPrec@1 67.969 (62.350)\n",
      "Epoch: [113][390/390]\tTime 0.001 (0.003)\tLoss 1.1906 (1.0706)\tPrec@1 57.500 (61.846)\n",
      "EPOCH: 113 train Results: Prec@1 61.846 Loss: 1.0706\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1604 (1.1604)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1136 (1.2636)\tPrec@1 56.250 (55.620)\n",
      "EPOCH: 113 val Results: Prec@1 55.620 Loss: 1.2636\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [114][0/390]\tTime 0.002 (0.002)\tLoss 1.0828 (1.0828)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.003)\tLoss 0.7757 (0.9952)\tPrec@1 68.750 (64.824)\n",
      "Epoch: [114][156/390]\tTime 0.002 (0.003)\tLoss 1.1551 (1.0285)\tPrec@1 57.031 (63.585)\n",
      "Epoch: [114][234/390]\tTime 0.002 (0.003)\tLoss 1.0923 (1.0551)\tPrec@1 61.719 (62.553)\n",
      "Epoch: [114][312/390]\tTime 0.002 (0.003)\tLoss 1.0276 (1.0659)\tPrec@1 64.844 (62.240)\n",
      "Epoch: [114][390/390]\tTime 0.003 (0.003)\tLoss 1.1635 (1.0779)\tPrec@1 57.500 (61.714)\n",
      "EPOCH: 114 train Results: Prec@1 61.714 Loss: 1.0779\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1487 (1.1487)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5781 (1.2890)\tPrec@1 43.750 (54.510)\n",
      "EPOCH: 114 val Results: Prec@1 54.510 Loss: 1.2890\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [115][0/390]\tTime 0.008 (0.008)\tLoss 0.7624 (0.7624)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [115][78/390]\tTime 0.004 (0.004)\tLoss 0.8539 (0.9952)\tPrec@1 71.094 (64.577)\n",
      "Epoch: [115][156/390]\tTime 0.003 (0.005)\tLoss 1.2685 (1.0233)\tPrec@1 54.688 (63.316)\n",
      "Epoch: [115][234/390]\tTime 0.010 (0.004)\tLoss 1.1601 (1.0479)\tPrec@1 59.375 (62.527)\n",
      "Epoch: [115][312/390]\tTime 0.006 (0.004)\tLoss 1.2188 (1.0624)\tPrec@1 56.250 (61.983)\n",
      "Epoch: [115][390/390]\tTime 0.001 (0.004)\tLoss 1.2164 (1.0733)\tPrec@1 56.250 (61.578)\n",
      "EPOCH: 115 train Results: Prec@1 61.578 Loss: 1.0733\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1490 (1.1490)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1056 (1.2980)\tPrec@1 50.000 (54.370)\n",
      "EPOCH: 115 val Results: Prec@1 54.370 Loss: 1.2980\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [116][0/390]\tTime 0.002 (0.002)\tLoss 0.9616 (0.9616)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [116][78/390]\tTime 0.002 (0.003)\tLoss 1.0473 (0.9999)\tPrec@1 63.281 (64.557)\n",
      "Epoch: [116][156/390]\tTime 0.002 (0.003)\tLoss 1.2052 (1.0301)\tPrec@1 57.031 (63.460)\n",
      "Epoch: [116][234/390]\tTime 0.005 (0.003)\tLoss 1.0458 (1.0535)\tPrec@1 60.156 (62.743)\n",
      "Epoch: [116][312/390]\tTime 0.002 (0.003)\tLoss 1.2273 (1.0679)\tPrec@1 60.156 (62.183)\n",
      "Epoch: [116][390/390]\tTime 0.008 (0.003)\tLoss 1.2157 (1.0756)\tPrec@1 55.000 (61.782)\n",
      "EPOCH: 116 train Results: Prec@1 61.782 Loss: 1.0756\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0876 (1.0876)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0694 (1.2831)\tPrec@1 50.000 (54.530)\n",
      "EPOCH: 116 val Results: Prec@1 54.530 Loss: 1.2831\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [117][0/390]\tTime 0.002 (0.002)\tLoss 0.9327 (0.9327)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [117][78/390]\tTime 0.004 (0.003)\tLoss 0.9326 (0.9892)\tPrec@1 66.406 (64.765)\n",
      "Epoch: [117][156/390]\tTime 0.003 (0.003)\tLoss 1.1657 (1.0206)\tPrec@1 61.719 (63.540)\n",
      "Epoch: [117][234/390]\tTime 0.002 (0.003)\tLoss 1.0268 (1.0428)\tPrec@1 64.062 (62.719)\n",
      "Epoch: [117][312/390]\tTime 0.002 (0.003)\tLoss 0.9951 (1.0625)\tPrec@1 60.938 (61.946)\n",
      "Epoch: [117][390/390]\tTime 0.001 (0.003)\tLoss 1.2405 (1.0729)\tPrec@1 53.750 (61.608)\n",
      "EPOCH: 117 train Results: Prec@1 61.608 Loss: 1.0729\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0667 (1.0667)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1983 (1.2692)\tPrec@1 56.250 (54.960)\n",
      "EPOCH: 117 val Results: Prec@1 54.960 Loss: 1.2692\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [118][0/390]\tTime 0.004 (0.004)\tLoss 0.9672 (0.9672)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [118][78/390]\tTime 0.005 (0.003)\tLoss 1.0564 (1.0190)\tPrec@1 62.500 (63.004)\n",
      "Epoch: [118][156/390]\tTime 0.002 (0.003)\tLoss 0.9939 (1.0255)\tPrec@1 64.844 (63.067)\n",
      "Epoch: [118][234/390]\tTime 0.003 (0.003)\tLoss 1.1377 (1.0439)\tPrec@1 54.688 (62.550)\n",
      "Epoch: [118][312/390]\tTime 0.015 (0.003)\tLoss 1.0486 (1.0630)\tPrec@1 64.062 (61.961)\n",
      "Epoch: [118][390/390]\tTime 0.001 (0.003)\tLoss 1.1931 (1.0769)\tPrec@1 51.250 (61.558)\n",
      "EPOCH: 118 train Results: Prec@1 61.558 Loss: 1.0769\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1993 (1.1993)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.4030 (1.2824)\tPrec@1 31.250 (54.770)\n",
      "EPOCH: 118 val Results: Prec@1 54.770 Loss: 1.2824\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [119][0/390]\tTime 0.003 (0.003)\tLoss 0.9307 (0.9307)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [119][78/390]\tTime 0.003 (0.005)\tLoss 1.0788 (0.9799)\tPrec@1 60.156 (64.943)\n",
      "Epoch: [119][156/390]\tTime 0.003 (0.004)\tLoss 1.2736 (1.0199)\tPrec@1 56.250 (63.580)\n",
      "Epoch: [119][234/390]\tTime 0.003 (0.004)\tLoss 0.9317 (1.0421)\tPrec@1 65.625 (62.660)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.004)\tLoss 1.2169 (1.0608)\tPrec@1 60.938 (62.041)\n",
      "Epoch: [119][390/390]\tTime 0.001 (0.004)\tLoss 0.9804 (1.0731)\tPrec@1 66.250 (61.624)\n",
      "EPOCH: 119 train Results: Prec@1 61.624 Loss: 1.0731\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1555 (1.1555)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2560 (1.2905)\tPrec@1 37.500 (55.120)\n",
      "EPOCH: 119 val Results: Prec@1 55.120 Loss: 1.2905\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [120][0/390]\tTime 0.009 (0.009)\tLoss 0.8359 (0.8359)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.004)\tLoss 1.0378 (1.0049)\tPrec@1 61.719 (63.430)\n",
      "Epoch: [120][156/390]\tTime 0.002 (0.003)\tLoss 0.9813 (1.0354)\tPrec@1 68.750 (62.580)\n",
      "Epoch: [120][234/390]\tTime 0.002 (0.003)\tLoss 1.1826 (1.0531)\tPrec@1 60.156 (62.370)\n",
      "Epoch: [120][312/390]\tTime 0.002 (0.003)\tLoss 1.3020 (1.0631)\tPrec@1 55.469 (62.033)\n",
      "Epoch: [120][390/390]\tTime 0.001 (0.003)\tLoss 1.0415 (1.0762)\tPrec@1 62.500 (61.516)\n",
      "EPOCH: 120 train Results: Prec@1 61.516 Loss: 1.0762\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1806 (1.1806)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1170 (1.2708)\tPrec@1 43.750 (55.510)\n",
      "EPOCH: 120 val Results: Prec@1 55.510 Loss: 1.2708\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [121][0/390]\tTime 0.002 (0.002)\tLoss 0.8098 (0.8098)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [121][78/390]\tTime 0.002 (0.003)\tLoss 1.0617 (0.9778)\tPrec@1 61.719 (64.883)\n",
      "Epoch: [121][156/390]\tTime 0.005 (0.003)\tLoss 1.1194 (1.0258)\tPrec@1 60.156 (63.112)\n",
      "Epoch: [121][234/390]\tTime 0.002 (0.003)\tLoss 1.0490 (1.0461)\tPrec@1 59.375 (62.650)\n",
      "Epoch: [121][312/390]\tTime 0.003 (0.003)\tLoss 0.9703 (1.0608)\tPrec@1 69.531 (62.056)\n",
      "Epoch: [121][390/390]\tTime 0.001 (0.003)\tLoss 0.8365 (1.0755)\tPrec@1 63.750 (61.590)\n",
      "EPOCH: 121 train Results: Prec@1 61.590 Loss: 1.0755\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2380 (1.2380)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4080 (1.2989)\tPrec@1 43.750 (54.520)\n",
      "EPOCH: 121 val Results: Prec@1 54.520 Loss: 1.2989\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [122][0/390]\tTime 0.002 (0.002)\tLoss 0.9927 (0.9927)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [122][78/390]\tTime 0.006 (0.003)\tLoss 1.0555 (0.9863)\tPrec@1 60.156 (65.299)\n",
      "Epoch: [122][156/390]\tTime 0.018 (0.003)\tLoss 1.0445 (1.0246)\tPrec@1 64.062 (63.938)\n",
      "Epoch: [122][234/390]\tTime 0.013 (0.003)\tLoss 1.2533 (1.0537)\tPrec@1 59.375 (62.866)\n",
      "Epoch: [122][312/390]\tTime 0.002 (0.004)\tLoss 1.0879 (1.0666)\tPrec@1 66.406 (62.328)\n",
      "Epoch: [122][390/390]\tTime 0.003 (0.004)\tLoss 1.3351 (1.0775)\tPrec@1 57.500 (61.806)\n",
      "EPOCH: 122 train Results: Prec@1 61.806 Loss: 1.0775\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2113 (1.2113)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2724 (1.2886)\tPrec@1 43.750 (55.030)\n",
      "EPOCH: 122 val Results: Prec@1 55.030 Loss: 1.2886\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [123][0/390]\tTime 0.005 (0.005)\tLoss 0.8868 (0.8868)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [123][78/390]\tTime 0.003 (0.003)\tLoss 0.9986 (0.9885)\tPrec@1 64.062 (64.784)\n",
      "Epoch: [123][156/390]\tTime 0.002 (0.003)\tLoss 1.1645 (1.0241)\tPrec@1 58.594 (63.595)\n",
      "Epoch: [123][234/390]\tTime 0.003 (0.003)\tLoss 1.4380 (1.0468)\tPrec@1 52.344 (62.922)\n",
      "Epoch: [123][312/390]\tTime 0.002 (0.003)\tLoss 1.1181 (1.0673)\tPrec@1 58.594 (62.303)\n",
      "Epoch: [123][390/390]\tTime 0.005 (0.003)\tLoss 1.0752 (1.0788)\tPrec@1 56.250 (61.864)\n",
      "EPOCH: 123 train Results: Prec@1 61.864 Loss: 1.0788\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2055 (1.2055)\tPrec@1 50.781 (50.781)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1248 (1.2792)\tPrec@1 62.500 (54.940)\n",
      "EPOCH: 123 val Results: Prec@1 54.940 Loss: 1.2792\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [124][0/390]\tTime 0.003 (0.003)\tLoss 0.9264 (0.9264)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [124][78/390]\tTime 0.002 (0.004)\tLoss 1.0263 (1.0001)\tPrec@1 64.844 (64.191)\n",
      "Epoch: [124][156/390]\tTime 0.005 (0.003)\tLoss 1.0924 (1.0323)\tPrec@1 59.375 (62.878)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.1023 (1.0539)\tPrec@1 59.375 (62.224)\n",
      "Epoch: [124][312/390]\tTime 0.004 (0.003)\tLoss 1.1117 (1.0669)\tPrec@1 63.281 (61.726)\n",
      "Epoch: [124][390/390]\tTime 0.001 (0.003)\tLoss 1.1339 (1.0759)\tPrec@1 58.750 (61.428)\n",
      "EPOCH: 124 train Results: Prec@1 61.428 Loss: 1.0759\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1796 (1.1796)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4863 (1.2812)\tPrec@1 43.750 (54.760)\n",
      "EPOCH: 124 val Results: Prec@1 54.760 Loss: 1.2812\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [125][0/390]\tTime 0.004 (0.004)\tLoss 0.9066 (0.9066)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [125][78/390]\tTime 0.002 (0.003)\tLoss 0.9120 (0.9902)\tPrec@1 66.406 (64.389)\n",
      "Epoch: [125][156/390]\tTime 0.002 (0.003)\tLoss 0.9708 (1.0338)\tPrec@1 63.281 (63.197)\n",
      "Epoch: [125][234/390]\tTime 0.017 (0.003)\tLoss 1.1486 (1.0503)\tPrec@1 56.250 (62.520)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.0574 (1.0632)\tPrec@1 59.375 (62.205)\n",
      "Epoch: [125][390/390]\tTime 0.003 (0.003)\tLoss 0.9832 (1.0721)\tPrec@1 62.500 (61.870)\n",
      "EPOCH: 125 train Results: Prec@1 61.870 Loss: 1.0721\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1117 (1.1117)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2040 (1.2854)\tPrec@1 56.250 (54.450)\n",
      "EPOCH: 125 val Results: Prec@1 54.450 Loss: 1.2854\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [126][0/390]\tTime 0.002 (0.002)\tLoss 0.9338 (0.9338)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [126][78/390]\tTime 0.003 (0.003)\tLoss 1.0370 (0.9966)\tPrec@1 60.938 (64.577)\n",
      "Epoch: [126][156/390]\tTime 0.002 (0.003)\tLoss 1.0871 (1.0331)\tPrec@1 65.625 (63.222)\n",
      "Epoch: [126][234/390]\tTime 0.004 (0.004)\tLoss 1.2776 (1.0534)\tPrec@1 53.125 (62.573)\n",
      "Epoch: [126][312/390]\tTime 0.003 (0.004)\tLoss 1.1126 (1.0676)\tPrec@1 58.594 (62.096)\n",
      "Epoch: [126][390/390]\tTime 0.003 (0.003)\tLoss 1.0532 (1.0821)\tPrec@1 61.250 (61.570)\n",
      "EPOCH: 126 train Results: Prec@1 61.570 Loss: 1.0821\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1919 (1.1919)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5441 (1.2884)\tPrec@1 37.500 (54.630)\n",
      "EPOCH: 126 val Results: Prec@1 54.630 Loss: 1.2884\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [127][0/390]\tTime 0.002 (0.002)\tLoss 1.0574 (1.0574)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.003)\tLoss 0.7523 (0.9796)\tPrec@1 75.000 (65.566)\n",
      "Epoch: [127][156/390]\tTime 0.002 (0.004)\tLoss 0.9386 (1.0193)\tPrec@1 60.938 (63.759)\n",
      "Epoch: [127][234/390]\tTime 0.003 (0.004)\tLoss 1.3268 (1.0411)\tPrec@1 52.344 (62.896)\n",
      "Epoch: [127][312/390]\tTime 0.002 (0.003)\tLoss 1.1841 (1.0614)\tPrec@1 58.594 (62.096)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.003)\tLoss 1.0230 (1.0747)\tPrec@1 65.000 (61.694)\n",
      "EPOCH: 127 train Results: Prec@1 61.694 Loss: 1.0747\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2072 (1.2072)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3925 (1.2818)\tPrec@1 37.500 (54.710)\n",
      "EPOCH: 127 val Results: Prec@1 54.710 Loss: 1.2818\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [128][0/390]\tTime 0.007 (0.007)\tLoss 0.9729 (0.9729)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [128][78/390]\tTime 0.003 (0.003)\tLoss 1.0928 (0.9821)\tPrec@1 64.062 (65.249)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.003)\tLoss 1.0670 (1.0191)\tPrec@1 64.062 (63.769)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.003)\tLoss 1.0222 (1.0347)\tPrec@1 64.062 (63.182)\n",
      "Epoch: [128][312/390]\tTime 0.004 (0.003)\tLoss 1.0169 (1.0513)\tPrec@1 64.844 (62.575)\n",
      "Epoch: [128][390/390]\tTime 0.003 (0.003)\tLoss 0.9967 (1.0659)\tPrec@1 73.750 (62.138)\n",
      "EPOCH: 128 train Results: Prec@1 62.138 Loss: 1.0659\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1676 (1.1676)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0042 (1.2930)\tPrec@1 56.250 (54.950)\n",
      "EPOCH: 128 val Results: Prec@1 54.950 Loss: 1.2930\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [129][0/390]\tTime 0.005 (0.005)\tLoss 1.0954 (1.0954)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 1.0136 (0.9684)\tPrec@1 61.719 (65.556)\n",
      "Epoch: [129][156/390]\tTime 0.005 (0.003)\tLoss 1.0563 (1.0029)\tPrec@1 58.594 (64.013)\n",
      "Epoch: [129][234/390]\tTime 0.002 (0.004)\tLoss 1.1889 (1.0361)\tPrec@1 55.469 (62.866)\n",
      "Epoch: [129][312/390]\tTime 0.003 (0.004)\tLoss 1.0018 (1.0564)\tPrec@1 60.156 (62.275)\n",
      "Epoch: [129][390/390]\tTime 0.001 (0.004)\tLoss 1.0598 (1.0703)\tPrec@1 58.750 (61.852)\n",
      "EPOCH: 129 train Results: Prec@1 61.852 Loss: 1.0703\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1284 (1.1284)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1488 (1.2740)\tPrec@1 37.500 (55.070)\n",
      "EPOCH: 129 val Results: Prec@1 55.070 Loss: 1.2740\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [130][0/390]\tTime 0.004 (0.004)\tLoss 0.9357 (0.9357)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [130][78/390]\tTime 0.014 (0.003)\tLoss 0.9016 (0.9866)\tPrec@1 70.312 (64.844)\n",
      "Epoch: [130][156/390]\tTime 0.003 (0.004)\tLoss 1.1274 (1.0154)\tPrec@1 59.375 (63.649)\n",
      "Epoch: [130][234/390]\tTime 0.003 (0.003)\tLoss 1.1767 (1.0487)\tPrec@1 60.156 (62.563)\n",
      "Epoch: [130][312/390]\tTime 0.003 (0.003)\tLoss 1.0870 (1.0662)\tPrec@1 64.062 (61.871)\n",
      "Epoch: [130][390/390]\tTime 0.002 (0.003)\tLoss 1.2373 (1.0797)\tPrec@1 57.500 (61.496)\n",
      "EPOCH: 130 train Results: Prec@1 61.496 Loss: 1.0797\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1430 (1.1430)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1282 (1.2818)\tPrec@1 43.750 (54.740)\n",
      "EPOCH: 130 val Results: Prec@1 54.740 Loss: 1.2818\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [131][0/390]\tTime 0.006 (0.006)\tLoss 0.7922 (0.7922)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [131][78/390]\tTime 0.002 (0.003)\tLoss 0.9573 (0.9812)\tPrec@1 61.719 (65.932)\n",
      "Epoch: [131][156/390]\tTime 0.003 (0.003)\tLoss 1.1289 (1.0158)\tPrec@1 58.594 (64.058)\n",
      "Epoch: [131][234/390]\tTime 0.004 (0.003)\tLoss 1.2403 (1.0414)\tPrec@1 57.031 (63.052)\n",
      "Epoch: [131][312/390]\tTime 0.003 (0.003)\tLoss 1.2852 (1.0603)\tPrec@1 53.906 (62.313)\n",
      "Epoch: [131][390/390]\tTime 0.001 (0.003)\tLoss 1.0402 (1.0723)\tPrec@1 62.500 (61.964)\n",
      "EPOCH: 131 train Results: Prec@1 61.964 Loss: 1.0723\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1219 (1.1219)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1824 (1.2724)\tPrec@1 50.000 (55.490)\n",
      "EPOCH: 131 val Results: Prec@1 55.490 Loss: 1.2724\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [132][0/390]\tTime 0.002 (0.002)\tLoss 0.8953 (0.8953)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [132][78/390]\tTime 0.003 (0.003)\tLoss 1.1341 (0.9884)\tPrec@1 60.938 (65.299)\n",
      "Epoch: [132][156/390]\tTime 0.002 (0.003)\tLoss 1.1162 (1.0307)\tPrec@1 67.188 (63.436)\n",
      "Epoch: [132][234/390]\tTime 0.005 (0.004)\tLoss 0.9679 (1.0537)\tPrec@1 66.406 (62.557)\n",
      "Epoch: [132][312/390]\tTime 0.002 (0.003)\tLoss 1.3354 (1.0647)\tPrec@1 53.906 (62.131)\n",
      "Epoch: [132][390/390]\tTime 0.007 (0.003)\tLoss 1.0789 (1.0722)\tPrec@1 61.250 (61.750)\n",
      "EPOCH: 132 train Results: Prec@1 61.750 Loss: 1.0722\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1566 (1.1566)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3346 (1.2880)\tPrec@1 37.500 (53.850)\n",
      "EPOCH: 132 val Results: Prec@1 53.850 Loss: 1.2880\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [133][0/390]\tTime 0.002 (0.002)\tLoss 0.9403 (0.9403)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [133][78/390]\tTime 0.004 (0.005)\tLoss 0.8806 (0.9976)\tPrec@1 68.750 (64.834)\n",
      "Epoch: [133][156/390]\tTime 0.002 (0.004)\tLoss 0.9524 (1.0149)\tPrec@1 66.406 (63.649)\n",
      "Epoch: [133][234/390]\tTime 0.002 (0.004)\tLoss 1.1695 (1.0417)\tPrec@1 57.031 (62.776)\n",
      "Epoch: [133][312/390]\tTime 0.003 (0.004)\tLoss 1.0359 (1.0605)\tPrec@1 62.500 (62.220)\n",
      "Epoch: [133][390/390]\tTime 0.003 (0.004)\tLoss 1.1963 (1.0731)\tPrec@1 57.500 (61.780)\n",
      "EPOCH: 133 train Results: Prec@1 61.780 Loss: 1.0731\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1302 (1.1302)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.0169 (1.2841)\tPrec@1 62.500 (54.480)\n",
      "EPOCH: 133 val Results: Prec@1 54.480 Loss: 1.2841\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [134][0/390]\tTime 0.002 (0.002)\tLoss 0.9674 (0.9674)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [134][78/390]\tTime 0.002 (0.004)\tLoss 1.0196 (1.0067)\tPrec@1 62.500 (63.954)\n",
      "Epoch: [134][156/390]\tTime 0.003 (0.003)\tLoss 1.0560 (1.0344)\tPrec@1 66.406 (63.052)\n",
      "Epoch: [134][234/390]\tTime 0.003 (0.003)\tLoss 1.0720 (1.0515)\tPrec@1 63.281 (62.364)\n",
      "Epoch: [134][312/390]\tTime 0.003 (0.003)\tLoss 1.0880 (1.0634)\tPrec@1 58.594 (61.941)\n",
      "Epoch: [134][390/390]\tTime 0.004 (0.003)\tLoss 1.0708 (1.0727)\tPrec@1 60.000 (61.668)\n",
      "EPOCH: 134 train Results: Prec@1 61.668 Loss: 1.0727\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1898 (1.1898)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5297 (1.2840)\tPrec@1 43.750 (54.520)\n",
      "EPOCH: 134 val Results: Prec@1 54.520 Loss: 1.2840\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [135][0/390]\tTime 0.003 (0.003)\tLoss 0.9600 (0.9600)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [135][78/390]\tTime 0.004 (0.003)\tLoss 1.0634 (0.9809)\tPrec@1 66.406 (65.012)\n",
      "Epoch: [135][156/390]\tTime 0.010 (0.004)\tLoss 1.0833 (1.0174)\tPrec@1 57.031 (63.560)\n",
      "Epoch: [135][234/390]\tTime 0.003 (0.003)\tLoss 1.2569 (1.0405)\tPrec@1 55.469 (62.586)\n",
      "Epoch: [135][312/390]\tTime 0.003 (0.003)\tLoss 0.9622 (1.0579)\tPrec@1 67.969 (62.126)\n",
      "Epoch: [135][390/390]\tTime 0.003 (0.003)\tLoss 1.1389 (1.0713)\tPrec@1 62.500 (61.744)\n",
      "EPOCH: 135 train Results: Prec@1 61.744 Loss: 1.0713\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1620 (1.1620)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1522 (1.2820)\tPrec@1 56.250 (54.570)\n",
      "EPOCH: 135 val Results: Prec@1 54.570 Loss: 1.2820\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [136][0/390]\tTime 0.002 (0.002)\tLoss 0.8605 (0.8605)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [136][78/390]\tTime 0.007 (0.003)\tLoss 1.0116 (0.9939)\tPrec@1 62.500 (64.606)\n",
      "Epoch: [136][156/390]\tTime 0.005 (0.003)\tLoss 0.9464 (1.0248)\tPrec@1 65.625 (63.754)\n",
      "Epoch: [136][234/390]\tTime 0.002 (0.003)\tLoss 1.0799 (1.0469)\tPrec@1 62.500 (62.796)\n",
      "Epoch: [136][312/390]\tTime 0.002 (0.003)\tLoss 1.1447 (1.0612)\tPrec@1 60.938 (62.098)\n",
      "Epoch: [136][390/390]\tTime 0.002 (0.003)\tLoss 1.2046 (1.0722)\tPrec@1 61.250 (61.768)\n",
      "EPOCH: 136 train Results: Prec@1 61.768 Loss: 1.0722\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1364 (1.1364)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2331 (1.2901)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 136 val Results: Prec@1 55.230 Loss: 1.2901\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [137][0/390]\tTime 0.012 (0.012)\tLoss 1.0438 (1.0438)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [137][78/390]\tTime 0.003 (0.004)\tLoss 1.0309 (1.0170)\tPrec@1 65.625 (63.578)\n",
      "Epoch: [137][156/390]\tTime 0.003 (0.003)\tLoss 0.9762 (1.0270)\tPrec@1 63.281 (63.401)\n",
      "Epoch: [137][234/390]\tTime 0.003 (0.003)\tLoss 1.0586 (1.0413)\tPrec@1 61.719 (62.876)\n",
      "Epoch: [137][312/390]\tTime 0.008 (0.003)\tLoss 1.0752 (1.0554)\tPrec@1 64.844 (62.453)\n",
      "Epoch: [137][390/390]\tTime 0.001 (0.003)\tLoss 1.2611 (1.0700)\tPrec@1 60.000 (62.012)\n",
      "EPOCH: 137 train Results: Prec@1 62.012 Loss: 1.0700\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0255 (1.0255)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2232 (1.2865)\tPrec@1 37.500 (54.440)\n",
      "EPOCH: 137 val Results: Prec@1 54.440 Loss: 1.2865\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [138][0/390]\tTime 0.002 (0.002)\tLoss 1.0590 (1.0590)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [138][78/390]\tTime 0.009 (0.004)\tLoss 1.0444 (0.9979)\tPrec@1 67.969 (64.330)\n",
      "Epoch: [138][156/390]\tTime 0.002 (0.003)\tLoss 0.9732 (1.0253)\tPrec@1 67.188 (63.520)\n",
      "Epoch: [138][234/390]\tTime 0.002 (0.003)\tLoss 1.1442 (1.0462)\tPrec@1 60.156 (62.945)\n",
      "Epoch: [138][312/390]\tTime 0.002 (0.003)\tLoss 1.2717 (1.0609)\tPrec@1 55.469 (62.433)\n",
      "Epoch: [138][390/390]\tTime 0.002 (0.003)\tLoss 1.0638 (1.0745)\tPrec@1 60.000 (61.836)\n",
      "EPOCH: 138 train Results: Prec@1 61.836 Loss: 1.0745\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1207 (1.1207)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1500 (1.2766)\tPrec@1 50.000 (54.880)\n",
      "EPOCH: 138 val Results: Prec@1 54.880 Loss: 1.2766\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [139][0/390]\tTime 0.002 (0.002)\tLoss 1.0067 (1.0067)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [139][78/390]\tTime 0.004 (0.003)\tLoss 0.8383 (0.9996)\tPrec@1 70.312 (63.845)\n",
      "Epoch: [139][156/390]\tTime 0.002 (0.003)\tLoss 1.0510 (1.0269)\tPrec@1 59.375 (63.366)\n",
      "Epoch: [139][234/390]\tTime 0.002 (0.004)\tLoss 1.1220 (1.0483)\tPrec@1 57.812 (62.603)\n",
      "Epoch: [139][312/390]\tTime 0.014 (0.003)\tLoss 1.1906 (1.0605)\tPrec@1 59.375 (62.253)\n",
      "Epoch: [139][390/390]\tTime 0.011 (0.003)\tLoss 1.1441 (1.0715)\tPrec@1 57.500 (61.946)\n",
      "EPOCH: 139 train Results: Prec@1 61.946 Loss: 1.0715\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2672 (1.2672)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1049 (1.2872)\tPrec@1 56.250 (53.960)\n",
      "EPOCH: 139 val Results: Prec@1 53.960 Loss: 1.2872\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [140][0/390]\tTime 0.003 (0.003)\tLoss 0.9884 (0.9884)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [140][78/390]\tTime 0.002 (0.003)\tLoss 0.9942 (0.9934)\tPrec@1 62.500 (65.170)\n",
      "Epoch: [140][156/390]\tTime 0.002 (0.003)\tLoss 1.1427 (1.0244)\tPrec@1 60.938 (63.719)\n",
      "Epoch: [140][234/390]\tTime 0.002 (0.003)\tLoss 0.9977 (1.0393)\tPrec@1 61.719 (63.045)\n",
      "Epoch: [140][312/390]\tTime 0.009 (0.003)\tLoss 1.0161 (1.0542)\tPrec@1 62.500 (62.450)\n",
      "Epoch: [140][390/390]\tTime 0.003 (0.003)\tLoss 1.2751 (1.0713)\tPrec@1 52.500 (61.698)\n",
      "EPOCH: 140 train Results: Prec@1 61.698 Loss: 1.0713\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2263 (1.2263)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.2040 (1.2999)\tPrec@1 43.750 (54.470)\n",
      "EPOCH: 140 val Results: Prec@1 54.470 Loss: 1.2999\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [141][0/390]\tTime 0.008 (0.008)\tLoss 0.7957 (0.7957)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [141][78/390]\tTime 0.002 (0.003)\tLoss 1.2450 (1.0036)\tPrec@1 54.688 (64.399)\n",
      "Epoch: [141][156/390]\tTime 0.002 (0.003)\tLoss 1.0796 (1.0332)\tPrec@1 53.906 (63.470)\n",
      "Epoch: [141][234/390]\tTime 0.002 (0.003)\tLoss 1.2611 (1.0567)\tPrec@1 57.812 (62.443)\n",
      "Epoch: [141][312/390]\tTime 0.003 (0.003)\tLoss 1.1398 (1.0712)\tPrec@1 55.469 (61.826)\n",
      "Epoch: [141][390/390]\tTime 0.001 (0.003)\tLoss 1.2564 (1.0800)\tPrec@1 52.500 (61.522)\n",
      "EPOCH: 141 train Results: Prec@1 61.522 Loss: 1.0800\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1645 (1.1645)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2880 (1.2868)\tPrec@1 43.750 (54.190)\n",
      "EPOCH: 141 val Results: Prec@1 54.190 Loss: 1.2868\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [142][0/390]\tTime 0.008 (0.008)\tLoss 0.8534 (0.8534)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [142][78/390]\tTime 0.003 (0.003)\tLoss 0.9593 (0.9856)\tPrec@1 58.594 (64.913)\n",
      "Epoch: [142][156/390]\tTime 0.004 (0.003)\tLoss 0.9520 (1.0209)\tPrec@1 67.969 (63.500)\n",
      "Epoch: [142][234/390]\tTime 0.002 (0.003)\tLoss 1.1291 (1.0395)\tPrec@1 59.375 (62.936)\n",
      "Epoch: [142][312/390]\tTime 0.026 (0.003)\tLoss 1.1007 (1.0572)\tPrec@1 62.500 (62.273)\n",
      "Epoch: [142][390/390]\tTime 0.001 (0.003)\tLoss 1.0989 (1.0722)\tPrec@1 60.000 (61.640)\n",
      "EPOCH: 142 train Results: Prec@1 61.640 Loss: 1.0722\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2899 (1.2899)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2356 (1.3172)\tPrec@1 56.250 (53.840)\n",
      "EPOCH: 142 val Results: Prec@1 53.840 Loss: 1.3172\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [143][0/390]\tTime 0.006 (0.006)\tLoss 0.9632 (0.9632)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [143][78/390]\tTime 0.003 (0.003)\tLoss 0.9942 (0.9907)\tPrec@1 68.750 (64.656)\n",
      "Epoch: [143][156/390]\tTime 0.002 (0.003)\tLoss 1.0428 (1.0228)\tPrec@1 57.031 (63.540)\n",
      "Epoch: [143][234/390]\tTime 0.002 (0.003)\tLoss 1.1464 (1.0423)\tPrec@1 57.812 (62.786)\n",
      "Epoch: [143][312/390]\tTime 0.008 (0.003)\tLoss 1.1936 (1.0607)\tPrec@1 57.031 (62.086)\n",
      "Epoch: [143][390/390]\tTime 0.003 (0.003)\tLoss 1.0988 (1.0719)\tPrec@1 61.250 (61.686)\n",
      "EPOCH: 143 train Results: Prec@1 61.686 Loss: 1.0719\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2897 (1.2897)\tPrec@1 51.562 (51.562)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1645 (1.2890)\tPrec@1 50.000 (54.790)\n",
      "EPOCH: 143 val Results: Prec@1 54.790 Loss: 1.2890\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [144][0/390]\tTime 0.003 (0.003)\tLoss 0.9741 (0.9741)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [144][78/390]\tTime 0.004 (0.003)\tLoss 1.0261 (0.9937)\tPrec@1 67.188 (64.280)\n",
      "Epoch: [144][156/390]\tTime 0.002 (0.004)\tLoss 1.2683 (1.0203)\tPrec@1 55.469 (63.570)\n",
      "Epoch: [144][234/390]\tTime 0.003 (0.004)\tLoss 1.1431 (1.0418)\tPrec@1 61.719 (62.812)\n",
      "Epoch: [144][312/390]\tTime 0.002 (0.003)\tLoss 1.1157 (1.0581)\tPrec@1 60.156 (62.263)\n",
      "Epoch: [144][390/390]\tTime 0.001 (0.004)\tLoss 1.0558 (1.0702)\tPrec@1 61.250 (61.774)\n",
      "EPOCH: 144 train Results: Prec@1 61.774 Loss: 1.0702\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1785 (1.1785)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.003 (0.002)\tLoss 1.3567 (1.2976)\tPrec@1 50.000 (54.830)\n",
      "EPOCH: 144 val Results: Prec@1 54.830 Loss: 1.2976\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [145][0/390]\tTime 0.004 (0.004)\tLoss 1.0259 (1.0259)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [145][78/390]\tTime 0.003 (0.006)\tLoss 0.8977 (0.9939)\tPrec@1 65.625 (64.033)\n",
      "Epoch: [145][156/390]\tTime 0.002 (0.005)\tLoss 0.9473 (1.0140)\tPrec@1 67.188 (63.754)\n",
      "Epoch: [145][234/390]\tTime 0.005 (0.004)\tLoss 0.9971 (1.0462)\tPrec@1 60.156 (62.640)\n",
      "Epoch: [145][312/390]\tTime 0.007 (0.004)\tLoss 1.1193 (1.0637)\tPrec@1 58.594 (62.086)\n",
      "Epoch: [145][390/390]\tTime 0.003 (0.004)\tLoss 1.1043 (1.0739)\tPrec@1 61.250 (61.728)\n",
      "EPOCH: 145 train Results: Prec@1 61.728 Loss: 1.0739\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1536 (1.1536)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5159 (1.2882)\tPrec@1 43.750 (54.840)\n",
      "EPOCH: 145 val Results: Prec@1 54.840 Loss: 1.2882\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [146][0/390]\tTime 0.003 (0.003)\tLoss 1.0053 (1.0053)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [146][78/390]\tTime 0.004 (0.003)\tLoss 1.0741 (0.9948)\tPrec@1 53.906 (64.646)\n",
      "Epoch: [146][156/390]\tTime 0.003 (0.003)\tLoss 0.9048 (1.0309)\tPrec@1 67.188 (62.968)\n",
      "Epoch: [146][234/390]\tTime 0.002 (0.003)\tLoss 0.9770 (1.0490)\tPrec@1 68.750 (62.547)\n",
      "Epoch: [146][312/390]\tTime 0.005 (0.004)\tLoss 0.9527 (1.0638)\tPrec@1 64.062 (61.963)\n",
      "Epoch: [146][390/390]\tTime 0.002 (0.004)\tLoss 0.8745 (1.0741)\tPrec@1 68.750 (61.618)\n",
      "EPOCH: 146 train Results: Prec@1 61.618 Loss: 1.0741\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1927 (1.1927)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1821 (1.2759)\tPrec@1 43.750 (54.540)\n",
      "EPOCH: 146 val Results: Prec@1 54.540 Loss: 1.2759\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [147][0/390]\tTime 0.007 (0.007)\tLoss 1.0664 (1.0664)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [147][78/390]\tTime 0.002 (0.004)\tLoss 1.1340 (1.0027)\tPrec@1 61.719 (64.359)\n",
      "Epoch: [147][156/390]\tTime 0.002 (0.004)\tLoss 1.1724 (1.0235)\tPrec@1 59.375 (63.699)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.004)\tLoss 1.0527 (1.0398)\tPrec@1 63.281 (63.022)\n",
      "Epoch: [147][312/390]\tTime 0.013 (0.004)\tLoss 1.1047 (1.0534)\tPrec@1 61.719 (62.552)\n",
      "Epoch: [147][390/390]\tTime 0.001 (0.004)\tLoss 1.2765 (1.0690)\tPrec@1 55.000 (62.016)\n",
      "EPOCH: 147 train Results: Prec@1 62.016 Loss: 1.0690\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1350 (1.1350)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.3688 (1.2793)\tPrec@1 43.750 (55.090)\n",
      "EPOCH: 147 val Results: Prec@1 55.090 Loss: 1.2793\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [148][0/390]\tTime 0.005 (0.005)\tLoss 1.0366 (1.0366)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [148][78/390]\tTime 0.002 (0.003)\tLoss 1.1104 (0.9880)\tPrec@1 61.719 (64.636)\n",
      "Epoch: [148][156/390]\tTime 0.003 (0.003)\tLoss 1.1926 (1.0185)\tPrec@1 61.719 (63.455)\n",
      "Epoch: [148][234/390]\tTime 0.006 (0.004)\tLoss 1.0135 (1.0421)\tPrec@1 62.500 (62.640)\n",
      "Epoch: [148][312/390]\tTime 0.002 (0.004)\tLoss 1.0356 (1.0573)\tPrec@1 60.156 (62.121)\n",
      "Epoch: [148][390/390]\tTime 0.003 (0.004)\tLoss 1.0988 (1.0687)\tPrec@1 68.750 (61.862)\n",
      "EPOCH: 148 train Results: Prec@1 61.862 Loss: 1.0687\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1325 (1.1325)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4252 (1.2914)\tPrec@1 43.750 (54.570)\n",
      "EPOCH: 148 val Results: Prec@1 54.570 Loss: 1.2914\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [149][0/390]\tTime 0.002 (0.002)\tLoss 0.8640 (0.8640)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [149][78/390]\tTime 0.002 (0.003)\tLoss 1.0359 (0.9740)\tPrec@1 60.938 (65.051)\n",
      "Epoch: [149][156/390]\tTime 0.003 (0.003)\tLoss 1.0579 (1.0100)\tPrec@1 69.531 (63.943)\n",
      "Epoch: [149][234/390]\tTime 0.003 (0.003)\tLoss 0.9663 (1.0354)\tPrec@1 67.969 (63.029)\n",
      "Epoch: [149][312/390]\tTime 0.002 (0.003)\tLoss 1.1324 (1.0542)\tPrec@1 54.688 (62.303)\n",
      "Epoch: [149][390/390]\tTime 0.001 (0.003)\tLoss 1.2599 (1.0716)\tPrec@1 52.500 (61.736)\n",
      "EPOCH: 149 train Results: Prec@1 61.736 Loss: 1.0716\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1707 (1.1707)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2809 (1.2730)\tPrec@1 37.500 (54.610)\n",
      "EPOCH: 149 val Results: Prec@1 54.610 Loss: 1.2730\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [150][0/390]\tTime 0.004 (0.004)\tLoss 1.0450 (1.0450)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [150][78/390]\tTime 0.005 (0.004)\tLoss 1.3173 (0.9744)\tPrec@1 55.469 (65.131)\n",
      "Epoch: [150][156/390]\tTime 0.004 (0.003)\tLoss 1.0948 (1.0070)\tPrec@1 66.406 (63.968)\n",
      "Epoch: [150][234/390]\tTime 0.002 (0.004)\tLoss 1.2861 (1.0468)\tPrec@1 57.812 (62.580)\n",
      "Epoch: [150][312/390]\tTime 0.004 (0.004)\tLoss 1.1359 (1.0616)\tPrec@1 62.500 (61.996)\n",
      "Epoch: [150][390/390]\tTime 0.003 (0.004)\tLoss 0.9416 (1.0743)\tPrec@1 67.500 (61.568)\n",
      "EPOCH: 150 train Results: Prec@1 61.568 Loss: 1.0743\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1660 (1.1660)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3046 (1.2780)\tPrec@1 50.000 (54.620)\n",
      "EPOCH: 150 val Results: Prec@1 54.620 Loss: 1.2780\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [151][0/390]\tTime 0.005 (0.005)\tLoss 1.0611 (1.0611)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [151][78/390]\tTime 0.002 (0.003)\tLoss 1.1279 (0.9789)\tPrec@1 57.031 (65.180)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.003)\tLoss 0.9782 (1.0133)\tPrec@1 63.281 (63.903)\n",
      "Epoch: [151][234/390]\tTime 0.002 (0.003)\tLoss 1.1051 (1.0367)\tPrec@1 57.031 (63.208)\n",
      "Epoch: [151][312/390]\tTime 0.003 (0.003)\tLoss 1.1232 (1.0528)\tPrec@1 62.500 (62.625)\n",
      "Epoch: [151][390/390]\tTime 0.003 (0.003)\tLoss 1.2201 (1.0692)\tPrec@1 61.250 (61.970)\n",
      "EPOCH: 151 train Results: Prec@1 61.970 Loss: 1.0692\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1904 (1.1904)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2842 (1.2773)\tPrec@1 50.000 (54.800)\n",
      "EPOCH: 151 val Results: Prec@1 54.800 Loss: 1.2773\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [152][0/390]\tTime 0.004 (0.004)\tLoss 0.9439 (0.9439)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [152][78/390]\tTime 0.003 (0.003)\tLoss 1.0418 (1.0000)\tPrec@1 58.594 (64.062)\n",
      "Epoch: [152][156/390]\tTime 0.004 (0.003)\tLoss 0.9949 (1.0236)\tPrec@1 65.625 (63.366)\n",
      "Epoch: [152][234/390]\tTime 0.008 (0.003)\tLoss 1.1087 (1.0456)\tPrec@1 64.844 (62.769)\n",
      "Epoch: [152][312/390]\tTime 0.003 (0.003)\tLoss 1.1704 (1.0575)\tPrec@1 56.250 (62.373)\n",
      "Epoch: [152][390/390]\tTime 0.003 (0.003)\tLoss 1.0697 (1.0676)\tPrec@1 61.250 (61.968)\n",
      "EPOCH: 152 train Results: Prec@1 61.968 Loss: 1.0676\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2164 (1.2164)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1270 (1.2805)\tPrec@1 43.750 (54.720)\n",
      "EPOCH: 152 val Results: Prec@1 54.720 Loss: 1.2805\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [153][0/390]\tTime 0.005 (0.005)\tLoss 0.8177 (0.8177)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [153][78/390]\tTime 0.003 (0.003)\tLoss 1.0209 (0.9815)\tPrec@1 61.719 (65.061)\n",
      "Epoch: [153][156/390]\tTime 0.002 (0.003)\tLoss 0.9298 (1.0163)\tPrec@1 69.531 (63.873)\n",
      "Epoch: [153][234/390]\tTime 0.004 (0.003)\tLoss 1.1507 (1.0379)\tPrec@1 61.719 (62.929)\n",
      "Epoch: [153][312/390]\tTime 0.004 (0.003)\tLoss 1.1629 (1.0533)\tPrec@1 55.469 (62.275)\n",
      "Epoch: [153][390/390]\tTime 0.016 (0.003)\tLoss 1.1964 (1.0665)\tPrec@1 57.500 (61.794)\n",
      "EPOCH: 153 train Results: Prec@1 61.794 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2257 (1.2257)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3195 (1.2764)\tPrec@1 43.750 (54.670)\n",
      "EPOCH: 153 val Results: Prec@1 54.670 Loss: 1.2764\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [154][0/390]\tTime 0.004 (0.004)\tLoss 0.8510 (0.8510)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.003)\tLoss 0.8783 (0.9982)\tPrec@1 69.531 (64.349)\n",
      "Epoch: [154][156/390]\tTime 0.005 (0.003)\tLoss 1.1411 (1.0202)\tPrec@1 54.688 (63.605)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.004)\tLoss 1.2273 (1.0382)\tPrec@1 55.469 (62.869)\n",
      "Epoch: [154][312/390]\tTime 0.009 (0.003)\tLoss 1.0590 (1.0597)\tPrec@1 61.719 (62.353)\n",
      "Epoch: [154][390/390]\tTime 0.001 (0.003)\tLoss 1.2868 (1.0745)\tPrec@1 53.750 (61.722)\n",
      "EPOCH: 154 train Results: Prec@1 61.722 Loss: 1.0745\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2134 (1.2134)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2847 (1.2726)\tPrec@1 50.000 (55.350)\n",
      "EPOCH: 154 val Results: Prec@1 55.350 Loss: 1.2726\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [155][0/390]\tTime 0.006 (0.006)\tLoss 1.0107 (1.0107)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [155][78/390]\tTime 0.003 (0.004)\tLoss 1.1104 (0.9780)\tPrec@1 59.375 (65.012)\n",
      "Epoch: [155][156/390]\tTime 0.092 (0.004)\tLoss 1.0134 (1.0084)\tPrec@1 59.375 (64.177)\n",
      "Epoch: [155][234/390]\tTime 0.003 (0.004)\tLoss 1.1751 (1.0336)\tPrec@1 57.031 (63.125)\n",
      "Epoch: [155][312/390]\tTime 0.003 (0.003)\tLoss 1.0921 (1.0523)\tPrec@1 64.844 (62.522)\n",
      "Epoch: [155][390/390]\tTime 0.001 (0.003)\tLoss 1.0838 (1.0679)\tPrec@1 62.500 (61.996)\n",
      "EPOCH: 155 train Results: Prec@1 61.996 Loss: 1.0679\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1504 (1.1504)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5417 (1.2937)\tPrec@1 37.500 (54.280)\n",
      "EPOCH: 155 val Results: Prec@1 54.280 Loss: 1.2937\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [156][0/390]\tTime 0.002 (0.002)\tLoss 0.9836 (0.9836)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [156][78/390]\tTime 0.002 (0.003)\tLoss 1.0539 (0.9781)\tPrec@1 67.969 (65.368)\n",
      "Epoch: [156][156/390]\tTime 0.002 (0.003)\tLoss 1.1516 (1.0146)\tPrec@1 60.938 (63.694)\n",
      "Epoch: [156][234/390]\tTime 0.003 (0.003)\tLoss 1.1059 (1.0412)\tPrec@1 60.156 (62.613)\n",
      "Epoch: [156][312/390]\tTime 0.004 (0.003)\tLoss 1.2473 (1.0605)\tPrec@1 51.562 (62.126)\n",
      "Epoch: [156][390/390]\tTime 0.001 (0.003)\tLoss 1.1427 (1.0714)\tPrec@1 60.000 (61.926)\n",
      "EPOCH: 156 train Results: Prec@1 61.926 Loss: 1.0714\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2025 (1.2025)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2675 (1.2807)\tPrec@1 43.750 (54.850)\n",
      "EPOCH: 156 val Results: Prec@1 54.850 Loss: 1.2807\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [157][0/390]\tTime 0.005 (0.005)\tLoss 1.1038 (1.1038)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [157][78/390]\tTime 0.002 (0.003)\tLoss 0.9704 (0.9945)\tPrec@1 60.156 (64.260)\n",
      "Epoch: [157][156/390]\tTime 0.003 (0.003)\tLoss 0.9175 (1.0280)\tPrec@1 69.531 (63.455)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.0540 (1.0489)\tPrec@1 58.594 (62.623)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.003)\tLoss 1.1586 (1.0631)\tPrec@1 59.375 (62.028)\n",
      "Epoch: [157][390/390]\tTime 0.001 (0.004)\tLoss 1.2342 (1.0704)\tPrec@1 52.500 (61.878)\n",
      "EPOCH: 157 train Results: Prec@1 61.878 Loss: 1.0704\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0318 (1.0318)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1577 (1.2929)\tPrec@1 43.750 (54.300)\n",
      "EPOCH: 157 val Results: Prec@1 54.300 Loss: 1.2929\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [158][0/390]\tTime 0.010 (0.010)\tLoss 1.0494 (1.0494)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [158][78/390]\tTime 0.004 (0.005)\tLoss 1.1384 (0.9939)\tPrec@1 55.469 (64.824)\n",
      "Epoch: [158][156/390]\tTime 0.002 (0.005)\tLoss 1.1344 (1.0204)\tPrec@1 59.375 (63.679)\n",
      "Epoch: [158][234/390]\tTime 0.003 (0.005)\tLoss 1.0200 (1.0418)\tPrec@1 65.625 (62.696)\n",
      "Epoch: [158][312/390]\tTime 0.010 (0.004)\tLoss 1.0964 (1.0554)\tPrec@1 59.375 (62.250)\n",
      "Epoch: [158][390/390]\tTime 0.001 (0.004)\tLoss 1.1161 (1.0699)\tPrec@1 57.500 (61.830)\n",
      "EPOCH: 158 train Results: Prec@1 61.830 Loss: 1.0699\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1156 (1.1156)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1120 (1.2957)\tPrec@1 50.000 (54.070)\n",
      "EPOCH: 158 val Results: Prec@1 54.070 Loss: 1.2957\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [159][0/390]\tTime 0.012 (0.012)\tLoss 0.9059 (0.9059)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [159][78/390]\tTime 0.004 (0.003)\tLoss 0.9746 (0.9991)\tPrec@1 62.500 (64.112)\n",
      "Epoch: [159][156/390]\tTime 0.003 (0.003)\tLoss 1.0803 (1.0263)\tPrec@1 61.719 (63.266)\n",
      "Epoch: [159][234/390]\tTime 0.002 (0.003)\tLoss 0.9812 (1.0398)\tPrec@1 68.750 (62.832)\n",
      "Epoch: [159][312/390]\tTime 0.002 (0.003)\tLoss 1.1098 (1.0556)\tPrec@1 56.250 (62.235)\n",
      "Epoch: [159][390/390]\tTime 0.001 (0.004)\tLoss 1.1957 (1.0696)\tPrec@1 57.500 (61.698)\n",
      "EPOCH: 159 train Results: Prec@1 61.698 Loss: 1.0696\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1151 (1.1151)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3142 (1.2803)\tPrec@1 50.000 (54.640)\n",
      "EPOCH: 159 val Results: Prec@1 54.640 Loss: 1.2803\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [160][0/390]\tTime 0.009 (0.009)\tLoss 0.8254 (0.8254)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [160][78/390]\tTime 0.002 (0.005)\tLoss 1.0304 (1.0150)\tPrec@1 64.844 (64.082)\n",
      "Epoch: [160][156/390]\tTime 0.002 (0.004)\tLoss 1.1722 (1.0207)\tPrec@1 58.594 (63.704)\n",
      "Epoch: [160][234/390]\tTime 0.002 (0.004)\tLoss 1.0069 (1.0419)\tPrec@1 64.062 (62.826)\n",
      "Epoch: [160][312/390]\tTime 0.002 (0.004)\tLoss 1.0721 (1.0542)\tPrec@1 61.719 (62.353)\n",
      "Epoch: [160][390/390]\tTime 0.001 (0.004)\tLoss 1.0335 (1.0605)\tPrec@1 70.000 (62.194)\n",
      "EPOCH: 160 train Results: Prec@1 62.194 Loss: 1.0605\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0970 (1.0970)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9943 (1.2808)\tPrec@1 56.250 (54.950)\n",
      "EPOCH: 160 val Results: Prec@1 54.950 Loss: 1.2808\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [161][0/390]\tTime 0.002 (0.002)\tLoss 1.1289 (1.1289)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [161][78/390]\tTime 0.006 (0.004)\tLoss 0.9062 (0.9824)\tPrec@1 70.312 (64.844)\n",
      "Epoch: [161][156/390]\tTime 0.007 (0.004)\tLoss 1.0309 (1.0181)\tPrec@1 64.844 (63.734)\n",
      "Epoch: [161][234/390]\tTime 0.002 (0.004)\tLoss 0.9327 (1.0431)\tPrec@1 64.062 (62.773)\n",
      "Epoch: [161][312/390]\tTime 0.009 (0.004)\tLoss 1.0734 (1.0560)\tPrec@1 62.500 (62.320)\n",
      "Epoch: [161][390/390]\tTime 0.003 (0.004)\tLoss 1.4023 (1.0706)\tPrec@1 50.000 (61.770)\n",
      "EPOCH: 161 train Results: Prec@1 61.770 Loss: 1.0706\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0767 (1.0767)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2962 (1.2765)\tPrec@1 37.500 (54.920)\n",
      "EPOCH: 161 val Results: Prec@1 54.920 Loss: 1.2765\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [162][0/390]\tTime 0.005 (0.005)\tLoss 0.8334 (0.8334)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [162][78/390]\tTime 0.002 (0.003)\tLoss 1.1432 (0.9953)\tPrec@1 63.281 (64.250)\n",
      "Epoch: [162][156/390]\tTime 0.003 (0.003)\tLoss 0.8539 (1.0250)\tPrec@1 71.875 (63.366)\n",
      "Epoch: [162][234/390]\tTime 0.003 (0.003)\tLoss 1.0425 (1.0411)\tPrec@1 60.938 (62.749)\n",
      "Epoch: [162][312/390]\tTime 0.002 (0.004)\tLoss 1.0721 (1.0542)\tPrec@1 61.719 (62.195)\n",
      "Epoch: [162][390/390]\tTime 0.001 (0.003)\tLoss 1.0725 (1.0681)\tPrec@1 61.250 (61.688)\n",
      "EPOCH: 162 train Results: Prec@1 61.688 Loss: 1.0681\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2136 (1.2136)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4671 (1.2961)\tPrec@1 31.250 (54.550)\n",
      "EPOCH: 162 val Results: Prec@1 54.550 Loss: 1.2961\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [163][0/390]\tTime 0.006 (0.006)\tLoss 1.1332 (1.1332)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [163][78/390]\tTime 0.003 (0.003)\tLoss 0.9070 (0.9945)\tPrec@1 66.406 (65.051)\n",
      "Epoch: [163][156/390]\tTime 0.002 (0.003)\tLoss 1.0766 (1.0161)\tPrec@1 60.938 (63.968)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.003)\tLoss 1.0918 (1.0459)\tPrec@1 60.156 (62.922)\n",
      "Epoch: [163][312/390]\tTime 0.011 (0.003)\tLoss 1.1524 (1.0652)\tPrec@1 60.938 (62.228)\n",
      "Epoch: [163][390/390]\tTime 0.002 (0.003)\tLoss 1.0259 (1.0763)\tPrec@1 62.500 (61.782)\n",
      "EPOCH: 163 train Results: Prec@1 61.782 Loss: 1.0763\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1504 (1.1504)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0948 (1.2881)\tPrec@1 56.250 (55.000)\n",
      "EPOCH: 163 val Results: Prec@1 55.000 Loss: 1.2881\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [164][0/390]\tTime 0.004 (0.004)\tLoss 0.9493 (0.9493)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [164][78/390]\tTime 0.003 (0.003)\tLoss 1.1180 (0.9842)\tPrec@1 60.156 (64.369)\n",
      "Epoch: [164][156/390]\tTime 0.002 (0.003)\tLoss 1.0749 (1.0180)\tPrec@1 62.500 (63.490)\n",
      "Epoch: [164][234/390]\tTime 0.002 (0.004)\tLoss 1.1336 (1.0429)\tPrec@1 52.344 (62.816)\n",
      "Epoch: [164][312/390]\tTime 0.002 (0.004)\tLoss 1.2181 (1.0614)\tPrec@1 55.469 (62.011)\n",
      "Epoch: [164][390/390]\tTime 0.002 (0.004)\tLoss 0.8496 (1.0709)\tPrec@1 70.000 (61.760)\n",
      "EPOCH: 164 train Results: Prec@1 61.760 Loss: 1.0709\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1569 (1.1569)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1605 (1.2893)\tPrec@1 43.750 (54.740)\n",
      "EPOCH: 164 val Results: Prec@1 54.740 Loss: 1.2893\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [165][0/390]\tTime 0.004 (0.004)\tLoss 1.0847 (1.0847)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [165][78/390]\tTime 0.003 (0.003)\tLoss 0.9791 (0.9721)\tPrec@1 67.969 (65.843)\n",
      "Epoch: [165][156/390]\tTime 0.004 (0.003)\tLoss 1.1121 (1.0167)\tPrec@1 57.031 (63.495)\n",
      "Epoch: [165][234/390]\tTime 0.003 (0.003)\tLoss 1.1662 (1.0418)\tPrec@1 61.719 (62.739)\n",
      "Epoch: [165][312/390]\tTime 0.002 (0.003)\tLoss 1.0468 (1.0563)\tPrec@1 65.625 (62.215)\n",
      "Epoch: [165][390/390]\tTime 0.002 (0.003)\tLoss 1.3037 (1.0699)\tPrec@1 65.000 (61.872)\n",
      "EPOCH: 165 train Results: Prec@1 61.872 Loss: 1.0699\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1842 (1.1842)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2322 (1.2975)\tPrec@1 56.250 (53.950)\n",
      "EPOCH: 165 val Results: Prec@1 53.950 Loss: 1.2975\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [166][0/390]\tTime 0.002 (0.002)\tLoss 1.1316 (1.1316)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [166][78/390]\tTime 0.005 (0.003)\tLoss 0.9954 (0.9910)\tPrec@1 64.844 (64.804)\n",
      "Epoch: [166][156/390]\tTime 0.005 (0.004)\tLoss 1.0752 (1.0223)\tPrec@1 62.500 (63.595)\n",
      "Epoch: [166][234/390]\tTime 0.003 (0.004)\tLoss 1.2486 (1.0421)\tPrec@1 60.156 (62.945)\n",
      "Epoch: [166][312/390]\tTime 0.002 (0.004)\tLoss 1.1659 (1.0551)\tPrec@1 57.031 (62.455)\n",
      "Epoch: [166][390/390]\tTime 0.001 (0.004)\tLoss 1.0750 (1.0669)\tPrec@1 67.500 (62.154)\n",
      "EPOCH: 166 train Results: Prec@1 62.154 Loss: 1.0669\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1556 (1.1556)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.2838 (1.2835)\tPrec@1 62.500 (54.560)\n",
      "EPOCH: 166 val Results: Prec@1 54.560 Loss: 1.2835\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [167][0/390]\tTime 0.002 (0.002)\tLoss 1.0475 (1.0475)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [167][78/390]\tTime 0.002 (0.004)\tLoss 1.0111 (0.9918)\tPrec@1 64.844 (64.310)\n",
      "Epoch: [167][156/390]\tTime 0.002 (0.004)\tLoss 1.0612 (1.0174)\tPrec@1 64.844 (63.411)\n",
      "Epoch: [167][234/390]\tTime 0.003 (0.004)\tLoss 1.0216 (1.0454)\tPrec@1 61.719 (62.367)\n",
      "Epoch: [167][312/390]\tTime 0.002 (0.003)\tLoss 1.2130 (1.0543)\tPrec@1 57.812 (62.031)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.003)\tLoss 1.1862 (1.0683)\tPrec@1 60.000 (61.612)\n",
      "EPOCH: 167 train Results: Prec@1 61.612 Loss: 1.0683\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1803 (1.1803)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3045 (1.2857)\tPrec@1 50.000 (54.130)\n",
      "EPOCH: 167 val Results: Prec@1 54.130 Loss: 1.2857\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [168][0/390]\tTime 0.005 (0.005)\tLoss 0.9703 (0.9703)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [168][78/390]\tTime 0.005 (0.004)\tLoss 0.8478 (0.9725)\tPrec@1 73.438 (65.388)\n",
      "Epoch: [168][156/390]\tTime 0.002 (0.003)\tLoss 1.0309 (1.0102)\tPrec@1 59.375 (64.077)\n",
      "Epoch: [168][234/390]\tTime 0.008 (0.003)\tLoss 1.1969 (1.0370)\tPrec@1 59.375 (63.095)\n",
      "Epoch: [168][312/390]\tTime 0.002 (0.003)\tLoss 1.0591 (1.0559)\tPrec@1 66.406 (62.532)\n",
      "Epoch: [168][390/390]\tTime 0.002 (0.003)\tLoss 1.0562 (1.0681)\tPrec@1 63.750 (62.128)\n",
      "EPOCH: 168 train Results: Prec@1 62.128 Loss: 1.0681\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2144 (1.2144)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4614 (1.2856)\tPrec@1 37.500 (54.910)\n",
      "EPOCH: 168 val Results: Prec@1 54.910 Loss: 1.2856\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [169][0/390]\tTime 0.002 (0.002)\tLoss 0.8344 (0.8344)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [169][78/390]\tTime 0.002 (0.003)\tLoss 0.9911 (0.9970)\tPrec@1 63.281 (64.814)\n",
      "Epoch: [169][156/390]\tTime 0.002 (0.004)\tLoss 0.9941 (1.0252)\tPrec@1 59.375 (63.839)\n",
      "Epoch: [169][234/390]\tTime 0.002 (0.004)\tLoss 1.0817 (1.0447)\tPrec@1 66.406 (63.035)\n",
      "Epoch: [169][312/390]\tTime 0.002 (0.003)\tLoss 1.0302 (1.0637)\tPrec@1 60.156 (62.338)\n",
      "Epoch: [169][390/390]\tTime 0.002 (0.003)\tLoss 1.0838 (1.0751)\tPrec@1 63.750 (61.840)\n",
      "EPOCH: 169 train Results: Prec@1 61.840 Loss: 1.0751\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1708 (1.1708)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2349 (1.2848)\tPrec@1 50.000 (54.460)\n",
      "EPOCH: 169 val Results: Prec@1 54.460 Loss: 1.2848\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [170][0/390]\tTime 0.002 (0.002)\tLoss 0.9775 (0.9775)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [170][78/390]\tTime 0.002 (0.004)\tLoss 1.0616 (0.9918)\tPrec@1 59.375 (64.597)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.004)\tLoss 0.8613 (1.0206)\tPrec@1 67.969 (63.605)\n",
      "Epoch: [170][234/390]\tTime 0.002 (0.003)\tLoss 1.1705 (1.0391)\tPrec@1 65.625 (62.699)\n",
      "Epoch: [170][312/390]\tTime 0.008 (0.003)\tLoss 1.0399 (1.0599)\tPrec@1 63.281 (61.976)\n",
      "Epoch: [170][390/390]\tTime 0.015 (0.003)\tLoss 1.0515 (1.0689)\tPrec@1 65.000 (61.718)\n",
      "EPOCH: 170 train Results: Prec@1 61.718 Loss: 1.0689\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2358 (1.2358)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0886 (1.2889)\tPrec@1 50.000 (55.500)\n",
      "EPOCH: 170 val Results: Prec@1 55.500 Loss: 1.2889\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [171][0/390]\tTime 0.004 (0.004)\tLoss 0.8231 (0.8231)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [171][78/390]\tTime 0.006 (0.003)\tLoss 1.0112 (0.9963)\tPrec@1 66.406 (64.221)\n",
      "Epoch: [171][156/390]\tTime 0.005 (0.003)\tLoss 1.1222 (1.0309)\tPrec@1 60.156 (62.883)\n",
      "Epoch: [171][234/390]\tTime 0.002 (0.003)\tLoss 0.9943 (1.0457)\tPrec@1 66.406 (62.580)\n",
      "Epoch: [171][312/390]\tTime 0.002 (0.004)\tLoss 0.9441 (1.0571)\tPrec@1 66.406 (62.210)\n",
      "Epoch: [171][390/390]\tTime 0.001 (0.004)\tLoss 1.3591 (1.0706)\tPrec@1 53.750 (61.718)\n",
      "EPOCH: 171 train Results: Prec@1 61.718 Loss: 1.0706\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2441 (1.2441)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3155 (1.3011)\tPrec@1 43.750 (54.070)\n",
      "EPOCH: 171 val Results: Prec@1 54.070 Loss: 1.3011\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [172][0/390]\tTime 0.003 (0.003)\tLoss 0.8155 (0.8155)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [172][78/390]\tTime 0.002 (0.003)\tLoss 0.9529 (0.9779)\tPrec@1 64.062 (65.328)\n",
      "Epoch: [172][156/390]\tTime 0.003 (0.003)\tLoss 0.9651 (1.0122)\tPrec@1 68.750 (64.152)\n",
      "Epoch: [172][234/390]\tTime 0.005 (0.004)\tLoss 1.0613 (1.0421)\tPrec@1 65.625 (62.892)\n",
      "Epoch: [172][312/390]\tTime 0.003 (0.004)\tLoss 1.0991 (1.0574)\tPrec@1 62.500 (62.273)\n",
      "Epoch: [172][390/390]\tTime 0.004 (0.004)\tLoss 1.2133 (1.0698)\tPrec@1 57.500 (61.858)\n",
      "EPOCH: 172 train Results: Prec@1 61.858 Loss: 1.0698\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1047 (1.1047)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3139 (1.2774)\tPrec@1 50.000 (55.160)\n",
      "EPOCH: 172 val Results: Prec@1 55.160 Loss: 1.2774\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [173][0/390]\tTime 0.004 (0.004)\tLoss 0.9665 (0.9665)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [173][78/390]\tTime 0.002 (0.003)\tLoss 1.2247 (1.0104)\tPrec@1 62.500 (64.142)\n",
      "Epoch: [173][156/390]\tTime 0.014 (0.003)\tLoss 1.1844 (1.0333)\tPrec@1 60.938 (63.326)\n",
      "Epoch: [173][234/390]\tTime 0.003 (0.003)\tLoss 0.9352 (1.0446)\tPrec@1 67.188 (62.773)\n",
      "Epoch: [173][312/390]\tTime 0.003 (0.003)\tLoss 1.1299 (1.0582)\tPrec@1 56.250 (62.193)\n",
      "Epoch: [173][390/390]\tTime 0.002 (0.003)\tLoss 1.0939 (1.0698)\tPrec@1 57.500 (61.824)\n",
      "EPOCH: 173 train Results: Prec@1 61.824 Loss: 1.0698\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1619 (1.1619)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0965 (1.2790)\tPrec@1 50.000 (55.100)\n",
      "EPOCH: 173 val Results: Prec@1 55.100 Loss: 1.2790\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [174][0/390]\tTime 0.005 (0.005)\tLoss 1.0009 (1.0009)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [174][78/390]\tTime 0.003 (0.004)\tLoss 0.8854 (0.9773)\tPrec@1 67.969 (65.368)\n",
      "Epoch: [174][156/390]\tTime 0.003 (0.004)\tLoss 0.9963 (1.0162)\tPrec@1 64.062 (63.863)\n",
      "Epoch: [174][234/390]\tTime 0.002 (0.004)\tLoss 1.1406 (1.0365)\tPrec@1 62.500 (63.138)\n",
      "Epoch: [174][312/390]\tTime 0.005 (0.004)\tLoss 1.0230 (1.0542)\tPrec@1 62.500 (62.498)\n",
      "Epoch: [174][390/390]\tTime 0.003 (0.003)\tLoss 1.0041 (1.0708)\tPrec@1 65.000 (61.902)\n",
      "EPOCH: 174 train Results: Prec@1 61.902 Loss: 1.0708\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1151 (1.1151)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1941 (1.2773)\tPrec@1 56.250 (54.930)\n",
      "EPOCH: 174 val Results: Prec@1 54.930 Loss: 1.2773\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [175][0/390]\tTime 0.003 (0.003)\tLoss 0.9064 (0.9064)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [175][78/390]\tTime 0.003 (0.004)\tLoss 1.1582 (0.9782)\tPrec@1 57.812 (65.358)\n",
      "Epoch: [175][156/390]\tTime 0.003 (0.004)\tLoss 1.0080 (1.0106)\tPrec@1 65.625 (64.336)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.004)\tLoss 1.1906 (1.0386)\tPrec@1 55.469 (63.215)\n",
      "Epoch: [175][312/390]\tTime 0.005 (0.004)\tLoss 1.0137 (1.0534)\tPrec@1 64.844 (62.647)\n",
      "Epoch: [175][390/390]\tTime 0.002 (0.004)\tLoss 1.3502 (1.0664)\tPrec@1 52.500 (61.984)\n",
      "EPOCH: 175 train Results: Prec@1 61.984 Loss: 1.0664\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2121 (1.2121)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2863 (1.2859)\tPrec@1 50.000 (54.940)\n",
      "EPOCH: 175 val Results: Prec@1 54.940 Loss: 1.2859\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [176][0/390]\tTime 0.003 (0.003)\tLoss 1.0921 (1.0921)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [176][78/390]\tTime 0.002 (0.004)\tLoss 1.1482 (1.0010)\tPrec@1 59.375 (64.280)\n",
      "Epoch: [176][156/390]\tTime 0.003 (0.003)\tLoss 1.0150 (1.0291)\tPrec@1 60.938 (63.635)\n",
      "Epoch: [176][234/390]\tTime 0.003 (0.003)\tLoss 1.0584 (1.0489)\tPrec@1 59.375 (62.882)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.003)\tLoss 0.9616 (1.0602)\tPrec@1 62.500 (62.425)\n",
      "Epoch: [176][390/390]\tTime 0.001 (0.003)\tLoss 1.1089 (1.0682)\tPrec@1 55.000 (62.082)\n",
      "EPOCH: 176 train Results: Prec@1 62.082 Loss: 1.0682\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1459 (1.1459)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5223 (1.2859)\tPrec@1 37.500 (54.700)\n",
      "EPOCH: 176 val Results: Prec@1 54.700 Loss: 1.2859\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [177][0/390]\tTime 0.005 (0.005)\tLoss 0.9967 (0.9967)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [177][78/390]\tTime 0.002 (0.004)\tLoss 0.8461 (0.9741)\tPrec@1 71.094 (65.417)\n",
      "Epoch: [177][156/390]\tTime 0.005 (0.004)\tLoss 0.9766 (1.0150)\tPrec@1 61.719 (63.734)\n",
      "Epoch: [177][234/390]\tTime 0.005 (0.003)\tLoss 1.0360 (1.0378)\tPrec@1 60.938 (62.859)\n",
      "Epoch: [177][312/390]\tTime 0.002 (0.003)\tLoss 1.1450 (1.0566)\tPrec@1 60.938 (62.136)\n",
      "Epoch: [177][390/390]\tTime 0.004 (0.003)\tLoss 1.1380 (1.0672)\tPrec@1 66.250 (61.928)\n",
      "EPOCH: 177 train Results: Prec@1 61.928 Loss: 1.0672\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3012 (1.3012)\tPrec@1 51.562 (51.562)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5408 (1.3062)\tPrec@1 37.500 (54.030)\n",
      "EPOCH: 177 val Results: Prec@1 54.030 Loss: 1.3062\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [178][0/390]\tTime 0.006 (0.006)\tLoss 1.0965 (1.0965)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [178][78/390]\tTime 0.002 (0.003)\tLoss 1.0513 (0.9977)\tPrec@1 59.375 (64.152)\n",
      "Epoch: [178][156/390]\tTime 0.009 (0.003)\tLoss 1.1281 (1.0245)\tPrec@1 61.719 (63.450)\n",
      "Epoch: [178][234/390]\tTime 0.003 (0.003)\tLoss 1.2164 (1.0477)\tPrec@1 57.031 (62.463)\n",
      "Epoch: [178][312/390]\tTime 0.003 (0.004)\tLoss 1.0431 (1.0577)\tPrec@1 57.812 (62.248)\n",
      "Epoch: [178][390/390]\tTime 0.003 (0.004)\tLoss 1.1849 (1.0693)\tPrec@1 65.000 (61.868)\n",
      "EPOCH: 178 train Results: Prec@1 61.868 Loss: 1.0693\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2642 (1.2642)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1423 (1.2815)\tPrec@1 50.000 (54.180)\n",
      "EPOCH: 178 val Results: Prec@1 54.180 Loss: 1.2815\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [179][0/390]\tTime 0.002 (0.002)\tLoss 1.0823 (1.0823)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [179][78/390]\tTime 0.002 (0.004)\tLoss 1.0323 (0.9751)\tPrec@1 62.500 (65.833)\n",
      "Epoch: [179][156/390]\tTime 0.003 (0.003)\tLoss 1.1187 (1.0131)\tPrec@1 57.031 (64.232)\n",
      "Epoch: [179][234/390]\tTime 0.004 (0.003)\tLoss 1.2558 (1.0478)\tPrec@1 57.812 (63.059)\n",
      "Epoch: [179][312/390]\tTime 0.008 (0.003)\tLoss 1.0109 (1.0577)\tPrec@1 65.625 (62.450)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.4016 (1.0707)\tPrec@1 56.250 (61.926)\n",
      "EPOCH: 179 train Results: Prec@1 61.926 Loss: 1.0707\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1496 (1.1496)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1089 (1.2940)\tPrec@1 62.500 (54.670)\n",
      "EPOCH: 179 val Results: Prec@1 54.670 Loss: 1.2940\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [180][0/390]\tTime 0.004 (0.004)\tLoss 1.0996 (1.0996)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [180][78/390]\tTime 0.002 (0.004)\tLoss 1.0283 (0.9916)\tPrec@1 64.062 (64.794)\n",
      "Epoch: [180][156/390]\tTime 0.003 (0.003)\tLoss 1.0429 (1.0198)\tPrec@1 64.062 (63.809)\n",
      "Epoch: [180][234/390]\tTime 0.002 (0.003)\tLoss 1.0553 (1.0453)\tPrec@1 59.375 (62.942)\n",
      "Epoch: [180][312/390]\tTime 0.003 (0.003)\tLoss 0.9244 (1.0599)\tPrec@1 64.062 (62.345)\n",
      "Epoch: [180][390/390]\tTime 0.003 (0.003)\tLoss 1.3376 (1.0698)\tPrec@1 52.500 (62.014)\n",
      "EPOCH: 180 train Results: Prec@1 62.014 Loss: 1.0698\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1717 (1.1717)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0433 (1.2929)\tPrec@1 56.250 (54.920)\n",
      "EPOCH: 180 val Results: Prec@1 54.920 Loss: 1.2929\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [181][0/390]\tTime 0.004 (0.004)\tLoss 0.9701 (0.9701)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [181][78/390]\tTime 0.002 (0.003)\tLoss 1.1025 (0.9805)\tPrec@1 61.719 (65.140)\n",
      "Epoch: [181][156/390]\tTime 0.003 (0.004)\tLoss 1.0484 (1.0153)\tPrec@1 60.156 (63.948)\n",
      "Epoch: [181][234/390]\tTime 0.010 (0.004)\tLoss 1.1681 (1.0434)\tPrec@1 60.156 (63.045)\n",
      "Epoch: [181][312/390]\tTime 0.004 (0.004)\tLoss 1.1626 (1.0600)\tPrec@1 57.031 (62.348)\n",
      "Epoch: [181][390/390]\tTime 0.001 (0.003)\tLoss 1.1942 (1.0696)\tPrec@1 61.250 (62.096)\n",
      "EPOCH: 181 train Results: Prec@1 62.096 Loss: 1.0696\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1334 (1.1334)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9785 (1.2848)\tPrec@1 62.500 (54.860)\n",
      "EPOCH: 181 val Results: Prec@1 54.860 Loss: 1.2848\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [182][0/390]\tTime 0.002 (0.002)\tLoss 1.0934 (1.0934)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [182][78/390]\tTime 0.002 (0.003)\tLoss 1.2946 (0.9768)\tPrec@1 53.125 (64.755)\n",
      "Epoch: [182][156/390]\tTime 0.002 (0.003)\tLoss 0.9525 (1.0159)\tPrec@1 64.844 (63.570)\n",
      "Epoch: [182][234/390]\tTime 0.002 (0.003)\tLoss 1.2525 (1.0390)\tPrec@1 48.438 (62.886)\n",
      "Epoch: [182][312/390]\tTime 0.002 (0.003)\tLoss 1.0585 (1.0548)\tPrec@1 67.188 (62.470)\n",
      "Epoch: [182][390/390]\tTime 0.001 (0.003)\tLoss 1.2706 (1.0678)\tPrec@1 56.250 (61.966)\n",
      "EPOCH: 182 train Results: Prec@1 61.966 Loss: 1.0678\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2142 (1.2142)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0027 (1.2909)\tPrec@1 43.750 (54.440)\n",
      "EPOCH: 182 val Results: Prec@1 54.440 Loss: 1.2909\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [183][0/390]\tTime 0.004 (0.004)\tLoss 0.9519 (0.9519)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [183][78/390]\tTime 0.004 (0.004)\tLoss 0.9556 (0.9891)\tPrec@1 65.625 (64.537)\n",
      "Epoch: [183][156/390]\tTime 0.004 (0.004)\tLoss 0.9927 (1.0187)\tPrec@1 59.375 (63.316)\n",
      "Epoch: [183][234/390]\tTime 0.003 (0.004)\tLoss 1.0369 (1.0432)\tPrec@1 57.812 (62.570)\n",
      "Epoch: [183][312/390]\tTime 0.003 (0.004)\tLoss 1.0390 (1.0572)\tPrec@1 63.281 (62.131)\n",
      "Epoch: [183][390/390]\tTime 0.001 (0.004)\tLoss 1.3292 (1.0712)\tPrec@1 56.250 (61.752)\n",
      "EPOCH: 183 train Results: Prec@1 61.752 Loss: 1.0712\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1683 (1.1683)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5116 (1.2900)\tPrec@1 56.250 (54.570)\n",
      "EPOCH: 183 val Results: Prec@1 54.570 Loss: 1.2900\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [184][0/390]\tTime 0.003 (0.003)\tLoss 0.8999 (0.8999)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [184][78/390]\tTime 0.003 (0.003)\tLoss 1.0912 (0.9839)\tPrec@1 63.281 (65.022)\n",
      "Epoch: [184][156/390]\tTime 0.004 (0.003)\tLoss 1.1392 (1.0174)\tPrec@1 65.625 (63.769)\n",
      "Epoch: [184][234/390]\tTime 0.004 (0.003)\tLoss 1.1374 (1.0444)\tPrec@1 56.250 (62.653)\n",
      "Epoch: [184][312/390]\tTime 0.002 (0.003)\tLoss 1.0139 (1.0633)\tPrec@1 63.281 (61.948)\n",
      "Epoch: [184][390/390]\tTime 0.002 (0.003)\tLoss 1.2161 (1.0723)\tPrec@1 56.250 (61.684)\n",
      "EPOCH: 184 train Results: Prec@1 61.684 Loss: 1.0723\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1178 (1.1178)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1164 (1.2918)\tPrec@1 50.000 (54.670)\n",
      "EPOCH: 184 val Results: Prec@1 54.670 Loss: 1.2918\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [185][0/390]\tTime 0.002 (0.002)\tLoss 0.9539 (0.9539)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [185][78/390]\tTime 0.003 (0.003)\tLoss 1.2206 (1.0026)\tPrec@1 57.812 (64.468)\n",
      "Epoch: [185][156/390]\tTime 0.002 (0.003)\tLoss 0.9245 (1.0204)\tPrec@1 66.406 (63.431)\n",
      "Epoch: [185][234/390]\tTime 0.003 (0.003)\tLoss 1.1355 (1.0379)\tPrec@1 63.281 (63.055)\n",
      "Epoch: [185][312/390]\tTime 0.004 (0.003)\tLoss 0.8720 (1.0494)\tPrec@1 67.188 (62.567)\n",
      "Epoch: [185][390/390]\tTime 0.001 (0.003)\tLoss 1.0957 (1.0676)\tPrec@1 63.750 (61.946)\n",
      "EPOCH: 185 train Results: Prec@1 61.946 Loss: 1.0676\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2771 (1.2771)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3206 (1.3000)\tPrec@1 37.500 (54.080)\n",
      "EPOCH: 185 val Results: Prec@1 54.080 Loss: 1.3000\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [186][0/390]\tTime 0.005 (0.005)\tLoss 1.0915 (1.0915)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [186][78/390]\tTime 0.002 (0.003)\tLoss 1.0634 (0.9923)\tPrec@1 63.281 (64.666)\n",
      "Epoch: [186][156/390]\tTime 0.003 (0.003)\tLoss 1.0709 (1.0306)\tPrec@1 58.594 (63.003)\n",
      "Epoch: [186][234/390]\tTime 0.003 (0.004)\tLoss 1.0560 (1.0482)\tPrec@1 59.375 (62.377)\n",
      "Epoch: [186][312/390]\tTime 0.017 (0.004)\tLoss 0.9933 (1.0645)\tPrec@1 64.062 (61.961)\n",
      "Epoch: [186][390/390]\tTime 0.001 (0.004)\tLoss 0.9707 (1.0699)\tPrec@1 66.250 (61.836)\n",
      "EPOCH: 186 train Results: Prec@1 61.836 Loss: 1.0699\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1281 (1.1281)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3109 (1.2874)\tPrec@1 43.750 (55.490)\n",
      "EPOCH: 186 val Results: Prec@1 55.490 Loss: 1.2874\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [187][0/390]\tTime 0.003 (0.003)\tLoss 1.0749 (1.0749)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [187][78/390]\tTime 0.002 (0.003)\tLoss 0.8622 (0.9843)\tPrec@1 74.219 (64.962)\n",
      "Epoch: [187][156/390]\tTime 0.007 (0.003)\tLoss 1.1442 (1.0163)\tPrec@1 59.375 (63.689)\n",
      "Epoch: [187][234/390]\tTime 0.002 (0.003)\tLoss 1.1001 (1.0345)\tPrec@1 61.719 (63.005)\n",
      "Epoch: [187][312/390]\tTime 0.003 (0.003)\tLoss 1.0574 (1.0530)\tPrec@1 64.844 (62.355)\n",
      "Epoch: [187][390/390]\tTime 0.001 (0.003)\tLoss 1.3469 (1.0657)\tPrec@1 50.000 (61.890)\n",
      "EPOCH: 187 train Results: Prec@1 61.890 Loss: 1.0657\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2151 (1.2151)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2301 (1.2832)\tPrec@1 43.750 (54.880)\n",
      "EPOCH: 187 val Results: Prec@1 54.880 Loss: 1.2832\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [188][0/390]\tTime 0.003 (0.003)\tLoss 0.9425 (0.9425)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [188][78/390]\tTime 0.002 (0.003)\tLoss 1.2488 (0.9776)\tPrec@1 58.594 (65.912)\n",
      "Epoch: [188][156/390]\tTime 0.007 (0.003)\tLoss 1.1002 (1.0084)\tPrec@1 63.281 (64.351)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.003)\tLoss 1.2358 (1.0306)\tPrec@1 53.125 (63.600)\n",
      "Epoch: [188][312/390]\tTime 0.002 (0.003)\tLoss 1.1568 (1.0503)\tPrec@1 61.719 (62.810)\n",
      "Epoch: [188][390/390]\tTime 0.001 (0.003)\tLoss 1.2524 (1.0672)\tPrec@1 51.250 (62.114)\n",
      "EPOCH: 188 train Results: Prec@1 62.114 Loss: 1.0672\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1607 (1.1607)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3235 (1.2944)\tPrec@1 50.000 (53.980)\n",
      "EPOCH: 188 val Results: Prec@1 53.980 Loss: 1.2944\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [189][0/390]\tTime 0.002 (0.002)\tLoss 1.0161 (1.0161)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.004)\tLoss 0.8720 (0.9829)\tPrec@1 71.875 (65.022)\n",
      "Epoch: [189][156/390]\tTime 0.002 (0.003)\tLoss 1.2456 (1.0181)\tPrec@1 55.469 (63.704)\n",
      "Epoch: [189][234/390]\tTime 0.002 (0.003)\tLoss 1.1458 (1.0342)\tPrec@1 57.031 (63.128)\n",
      "Epoch: [189][312/390]\tTime 0.003 (0.003)\tLoss 1.1308 (1.0558)\tPrec@1 56.250 (62.318)\n",
      "Epoch: [189][390/390]\tTime 0.001 (0.003)\tLoss 1.2353 (1.0720)\tPrec@1 51.250 (61.832)\n",
      "EPOCH: 189 train Results: Prec@1 61.832 Loss: 1.0720\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1891 (1.1891)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4398 (1.2746)\tPrec@1 37.500 (54.910)\n",
      "EPOCH: 189 val Results: Prec@1 54.910 Loss: 1.2746\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [190][0/390]\tTime 0.003 (0.003)\tLoss 0.9843 (0.9843)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.004)\tLoss 1.0640 (0.9878)\tPrec@1 62.500 (64.715)\n",
      "Epoch: [190][156/390]\tTime 0.006 (0.003)\tLoss 1.1256 (1.0156)\tPrec@1 65.625 (63.849)\n",
      "Epoch: [190][234/390]\tTime 0.018 (0.003)\tLoss 1.0429 (1.0401)\tPrec@1 61.719 (62.839)\n",
      "Epoch: [190][312/390]\tTime 0.006 (0.003)\tLoss 1.0833 (1.0582)\tPrec@1 56.250 (62.248)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.003)\tLoss 1.1252 (1.0706)\tPrec@1 58.750 (61.818)\n",
      "EPOCH: 190 train Results: Prec@1 61.818 Loss: 1.0706\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1787 (1.1787)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.001 (0.002)\tLoss 1.4874 (1.2712)\tPrec@1 43.750 (54.930)\n",
      "EPOCH: 190 val Results: Prec@1 54.930 Loss: 1.2712\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [191][0/390]\tTime 0.003 (0.003)\tLoss 1.0168 (1.0168)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.003)\tLoss 0.8128 (0.9884)\tPrec@1 69.531 (64.320)\n",
      "Epoch: [191][156/390]\tTime 0.004 (0.003)\tLoss 1.0006 (1.0174)\tPrec@1 66.406 (63.346)\n",
      "Epoch: [191][234/390]\tTime 0.002 (0.003)\tLoss 0.9799 (1.0375)\tPrec@1 64.844 (62.852)\n",
      "Epoch: [191][312/390]\tTime 0.002 (0.003)\tLoss 1.0263 (1.0502)\tPrec@1 64.062 (62.350)\n",
      "Epoch: [191][390/390]\tTime 0.001 (0.003)\tLoss 1.1857 (1.0649)\tPrec@1 56.250 (61.702)\n",
      "EPOCH: 191 train Results: Prec@1 61.702 Loss: 1.0649\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1501 (1.1501)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1981 (1.2633)\tPrec@1 50.000 (54.860)\n",
      "EPOCH: 191 val Results: Prec@1 54.860 Loss: 1.2633\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [192][0/390]\tTime 0.005 (0.005)\tLoss 0.8921 (0.8921)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [192][78/390]\tTime 0.003 (0.003)\tLoss 1.1266 (0.9838)\tPrec@1 53.906 (65.190)\n",
      "Epoch: [192][156/390]\tTime 0.002 (0.003)\tLoss 1.1443 (1.0076)\tPrec@1 58.594 (64.008)\n",
      "Epoch: [192][234/390]\tTime 0.003 (0.003)\tLoss 1.0878 (1.0337)\tPrec@1 57.031 (63.178)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.003)\tLoss 1.2743 (1.0489)\tPrec@1 53.125 (62.610)\n",
      "Epoch: [192][390/390]\tTime 0.002 (0.003)\tLoss 1.0362 (1.0626)\tPrec@1 65.000 (62.204)\n",
      "EPOCH: 192 train Results: Prec@1 62.204 Loss: 1.0626\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1833 (1.1833)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0240 (1.2700)\tPrec@1 56.250 (55.600)\n",
      "EPOCH: 192 val Results: Prec@1 55.600 Loss: 1.2700\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [193][0/390]\tTime 0.005 (0.005)\tLoss 0.7627 (0.7627)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [193][78/390]\tTime 0.003 (0.005)\tLoss 0.9364 (0.9852)\tPrec@1 67.188 (65.289)\n",
      "Epoch: [193][156/390]\tTime 0.002 (0.004)\tLoss 1.0145 (1.0166)\tPrec@1 61.719 (63.963)\n",
      "Epoch: [193][234/390]\tTime 0.003 (0.004)\tLoss 0.9913 (1.0362)\tPrec@1 67.969 (63.182)\n",
      "Epoch: [193][312/390]\tTime 0.002 (0.004)\tLoss 1.0473 (1.0570)\tPrec@1 69.531 (62.485)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.004)\tLoss 0.9392 (1.0671)\tPrec@1 70.000 (62.042)\n",
      "EPOCH: 193 train Results: Prec@1 62.042 Loss: 1.0671\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1792 (1.1792)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3410 (1.2703)\tPrec@1 50.000 (55.010)\n",
      "EPOCH: 193 val Results: Prec@1 55.010 Loss: 1.2703\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [194][0/390]\tTime 0.005 (0.005)\tLoss 0.9214 (0.9214)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [194][78/390]\tTime 0.002 (0.003)\tLoss 0.9102 (0.9945)\tPrec@1 67.969 (64.715)\n",
      "Epoch: [194][156/390]\tTime 0.003 (0.003)\tLoss 0.9227 (1.0204)\tPrec@1 69.531 (63.794)\n",
      "Epoch: [194][234/390]\tTime 0.003 (0.003)\tLoss 1.0569 (1.0418)\tPrec@1 64.062 (62.955)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.003)\tLoss 1.0947 (1.0563)\tPrec@1 57.812 (62.305)\n",
      "Epoch: [194][390/390]\tTime 0.002 (0.003)\tLoss 1.4258 (1.0689)\tPrec@1 58.750 (61.986)\n",
      "EPOCH: 194 train Results: Prec@1 61.986 Loss: 1.0689\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.2084 (1.2084)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3705 (1.2684)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 194 val Results: Prec@1 55.370 Loss: 1.2684\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [195][0/390]\tTime 0.002 (0.002)\tLoss 0.9317 (0.9317)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [195][78/390]\tTime 0.004 (0.003)\tLoss 0.9697 (0.9792)\tPrec@1 64.844 (65.279)\n",
      "Epoch: [195][156/390]\tTime 0.004 (0.003)\tLoss 1.0311 (1.0165)\tPrec@1 67.188 (63.595)\n",
      "Epoch: [195][234/390]\tTime 0.016 (0.003)\tLoss 1.1018 (1.0383)\tPrec@1 60.156 (62.939)\n",
      "Epoch: [195][312/390]\tTime 0.002 (0.003)\tLoss 1.2084 (1.0553)\tPrec@1 60.156 (62.555)\n",
      "Epoch: [195][390/390]\tTime 0.002 (0.003)\tLoss 1.0977 (1.0634)\tPrec@1 55.000 (62.208)\n",
      "EPOCH: 195 train Results: Prec@1 62.208 Loss: 1.0634\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2561 (1.2561)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3607 (1.2917)\tPrec@1 50.000 (54.340)\n",
      "EPOCH: 195 val Results: Prec@1 54.340 Loss: 1.2917\n",
      "Best Prec@1: 55.750\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [196][0/390]\tTime 0.002 (0.002)\tLoss 0.9026 (0.9026)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 1.0355 (0.9875)\tPrec@1 62.500 (64.715)\n",
      "Epoch: [196][156/390]\tTime 0.002 (0.003)\tLoss 1.0839 (1.0157)\tPrec@1 60.938 (63.938)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.003)\tLoss 1.0553 (1.0423)\tPrec@1 62.500 (62.989)\n",
      "Epoch: [196][312/390]\tTime 0.002 (0.003)\tLoss 1.2097 (1.0523)\tPrec@1 57.812 (62.542)\n",
      "Epoch: [196][390/390]\tTime 0.002 (0.004)\tLoss 1.0378 (1.0627)\tPrec@1 62.500 (62.174)\n",
      "EPOCH: 196 train Results: Prec@1 62.174 Loss: 1.0627\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1541 (1.1541)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9620 (1.2610)\tPrec@1 56.250 (55.770)\n",
      "EPOCH: 196 val Results: Prec@1 55.770 Loss: 1.2610\n",
      "Best Prec@1: 55.770\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [197][0/390]\tTime 0.002 (0.002)\tLoss 0.8671 (0.8671)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [197][78/390]\tTime 0.002 (0.004)\tLoss 1.1345 (0.9820)\tPrec@1 54.688 (65.358)\n",
      "Epoch: [197][156/390]\tTime 0.015 (0.003)\tLoss 1.1216 (1.0112)\tPrec@1 61.719 (64.157)\n",
      "Epoch: [197][234/390]\tTime 0.005 (0.003)\tLoss 1.0305 (1.0411)\tPrec@1 67.188 (63.221)\n",
      "Epoch: [197][312/390]\tTime 0.002 (0.004)\tLoss 1.0039 (1.0598)\tPrec@1 64.062 (62.577)\n",
      "Epoch: [197][390/390]\tTime 0.005 (0.004)\tLoss 0.9659 (1.0693)\tPrec@1 67.500 (62.202)\n",
      "EPOCH: 197 train Results: Prec@1 62.202 Loss: 1.0693\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3041 (1.3041)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0312 (1.2824)\tPrec@1 50.000 (54.540)\n",
      "EPOCH: 197 val Results: Prec@1 54.540 Loss: 1.2824\n",
      "Best Prec@1: 55.770\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [198][0/390]\tTime 0.013 (0.013)\tLoss 0.9807 (0.9807)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [198][78/390]\tTime 0.002 (0.003)\tLoss 0.9141 (0.9808)\tPrec@1 63.281 (64.913)\n",
      "Epoch: [198][156/390]\tTime 0.005 (0.003)\tLoss 0.9711 (1.0239)\tPrec@1 67.188 (63.724)\n",
      "Epoch: [198][234/390]\tTime 0.002 (0.003)\tLoss 1.1907 (1.0489)\tPrec@1 59.375 (62.892)\n",
      "Epoch: [198][312/390]\tTime 0.004 (0.003)\tLoss 0.9615 (1.0557)\tPrec@1 64.844 (62.662)\n",
      "Epoch: [198][390/390]\tTime 0.003 (0.003)\tLoss 1.2003 (1.0669)\tPrec@1 61.250 (62.198)\n",
      "EPOCH: 198 train Results: Prec@1 62.198 Loss: 1.0669\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2698 (1.2698)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1147 (1.2733)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 198 val Results: Prec@1 54.630 Loss: 1.2733\n",
      "Best Prec@1: 55.770\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [199][0/390]\tTime 0.002 (0.002)\tLoss 1.0663 (1.0663)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [199][78/390]\tTime 0.003 (0.004)\tLoss 1.0391 (0.9737)\tPrec@1 64.062 (64.982)\n",
      "Epoch: [199][156/390]\tTime 0.004 (0.003)\tLoss 1.1115 (1.0077)\tPrec@1 61.719 (63.988)\n",
      "Epoch: [199][234/390]\tTime 0.012 (0.003)\tLoss 1.1020 (1.0371)\tPrec@1 58.594 (62.779)\n",
      "Epoch: [199][312/390]\tTime 0.003 (0.003)\tLoss 1.0933 (1.0570)\tPrec@1 60.938 (62.056)\n",
      "Epoch: [199][390/390]\tTime 0.002 (0.003)\tLoss 1.0238 (1.0689)\tPrec@1 63.750 (61.724)\n",
      "EPOCH: 199 train Results: Prec@1 61.724 Loss: 1.0689\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1527 (1.1527)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5223 (1.2854)\tPrec@1 25.000 (54.580)\n",
      "EPOCH: 199 val Results: Prec@1 54.580 Loss: 1.2854\n",
      "Best Prec@1: 55.770\n",
      "\n",
      "current lr 1.00000e-01\n",
      "Epoch: [200][0/390]\tTime 0.003 (0.003)\tLoss 0.9159 (0.9159)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [200][78/390]\tTime 0.002 (0.004)\tLoss 1.1395 (0.9933)\tPrec@1 62.500 (65.012)\n",
      "Epoch: [200][156/390]\tTime 0.008 (0.003)\tLoss 0.8675 (1.0208)\tPrec@1 67.969 (63.903)\n",
      "Epoch: [200][234/390]\tTime 0.003 (0.003)\tLoss 0.9512 (1.0383)\tPrec@1 65.625 (63.175)\n",
      "Epoch: [200][312/390]\tTime 0.003 (0.003)\tLoss 1.1490 (1.0498)\tPrec@1 60.156 (62.680)\n",
      "Epoch: [200][390/390]\tTime 0.002 (0.003)\tLoss 1.2310 (1.0619)\tPrec@1 57.500 (62.140)\n",
      "EPOCH: 200 train Results: Prec@1 62.140 Loss: 1.0619\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1965 (1.1965)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0760 (1.2669)\tPrec@1 56.250 (55.180)\n",
      "EPOCH: 200 val Results: Prec@1 55.180 Loss: 1.2669\n",
      "Best Prec@1: 55.770\n",
      "\n",
      "End time:  Thu Apr  4 23:24:05 2024\n",
      "train executed in 272.1905 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.1, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_cm('./figs/cm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainer):\n",
    "    train_accu = trainer.train_precs\n",
    "    test_accu = trainer.test_precs\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_accu, 'r-', label='Training Accuracy')\n",
    "    plt.plot(x, test_accu, 'b-', label='Testing Accuracy')\n",
    "    plt.title('Training and Testing Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(trainer):\n",
    "    train_accu = trainer.train_losses\n",
    "    test_accu = trainer.test_losses\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_accu, 'r-', label='Training Loss')\n",
    "    plt.plot(x, test_accu, 'b-', label='Testing Loss')\n",
    "    plt.title('Training and Testing Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 22:55:29 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/390]\tTime 0.011 (0.011)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.002 (0.005)\tLoss 2.4991 (3.3599)\tPrec@1 32.031 (22.636)\n",
      "Epoch: [1][156/390]\tTime 0.009 (0.004)\tLoss 2.4369 (2.8159)\tPrec@1 24.219 (26.866)\n",
      "Epoch: [1][234/390]\tTime 0.002 (0.004)\tLoss 1.9684 (2.5443)\tPrec@1 34.375 (29.451)\n",
      "Epoch: [1][312/390]\tTime 0.005 (0.005)\tLoss 1.8863 (2.3738)\tPrec@1 34.375 (31.178)\n",
      "Epoch: [1][390/390]\tTime 0.001 (0.005)\tLoss 1.6280 (2.2570)\tPrec@1 41.250 (32.432)\n",
      "EPOCH: 1 train Results: Prec@1 32.432 Loss: 2.2570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.5203 (1.5203)\tPrec@1 47.656 (47.656)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4963 (1.6156)\tPrec@1 31.250 (42.860)\n",
      "EPOCH: 1 val Results: Prec@1 42.860 Loss: 1.6156\n",
      "Best Prec@1: 42.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/390]\tTime 0.005 (0.005)\tLoss 1.6112 (1.6112)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [2][78/390]\tTime 0.002 (0.003)\tLoss 1.5126 (1.6195)\tPrec@1 45.312 (43.008)\n",
      "Epoch: [2][156/390]\tTime 0.002 (0.004)\tLoss 1.6208 (1.6061)\tPrec@1 44.531 (43.252)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.004)\tLoss 1.7222 (1.5965)\tPrec@1 42.969 (43.544)\n",
      "Epoch: [2][312/390]\tTime 0.003 (0.004)\tLoss 1.7094 (1.5855)\tPrec@1 40.625 (43.960)\n",
      "Epoch: [2][390/390]\tTime 0.007 (0.004)\tLoss 1.4745 (1.5758)\tPrec@1 52.500 (44.314)\n",
      "EPOCH: 2 train Results: Prec@1 44.314 Loss: 1.5758\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3885 (1.3885)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4268 (1.5008)\tPrec@1 31.250 (46.490)\n",
      "EPOCH: 2 val Results: Prec@1 46.490 Loss: 1.5008\n",
      "Best Prec@1: 46.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/390]\tTime 0.005 (0.005)\tLoss 1.4851 (1.4851)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [3][78/390]\tTime 0.008 (0.009)\tLoss 1.4391 (1.4743)\tPrec@1 41.406 (47.646)\n",
      "Epoch: [3][156/390]\tTime 0.002 (0.008)\tLoss 1.3910 (1.4640)\tPrec@1 50.781 (48.134)\n",
      "Epoch: [3][234/390]\tTime 0.008 (0.006)\tLoss 1.4514 (1.4645)\tPrec@1 42.969 (48.311)\n",
      "Epoch: [3][312/390]\tTime 0.002 (0.006)\tLoss 1.5078 (1.4632)\tPrec@1 50.000 (48.230)\n",
      "Epoch: [3][390/390]\tTime 0.001 (0.006)\tLoss 1.4212 (1.4601)\tPrec@1 51.250 (48.280)\n",
      "EPOCH: 3 train Results: Prec@1 48.280 Loss: 1.4601\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.3230 (1.3230)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4953 (1.4318)\tPrec@1 43.750 (49.310)\n",
      "EPOCH: 3 val Results: Prec@1 49.310 Loss: 1.4318\n",
      "Best Prec@1: 49.310\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/390]\tTime 0.002 (0.002)\tLoss 1.4993 (1.4993)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [4][78/390]\tTime 0.006 (0.008)\tLoss 1.4838 (1.3895)\tPrec@1 55.469 (51.226)\n",
      "Epoch: [4][156/390]\tTime 0.002 (0.006)\tLoss 1.4162 (1.3892)\tPrec@1 46.094 (50.931)\n",
      "Epoch: [4][234/390]\tTime 0.002 (0.005)\tLoss 1.5272 (1.3868)\tPrec@1 47.656 (50.775)\n",
      "Epoch: [4][312/390]\tTime 0.002 (0.005)\tLoss 1.3532 (1.3878)\tPrec@1 52.344 (50.774)\n",
      "Epoch: [4][390/390]\tTime 0.003 (0.005)\tLoss 1.3277 (1.3876)\tPrec@1 51.250 (50.776)\n",
      "EPOCH: 4 train Results: Prec@1 50.776 Loss: 1.3876\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2728 (1.2728)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4631 (1.3819)\tPrec@1 31.250 (50.290)\n",
      "EPOCH: 4 val Results: Prec@1 50.290 Loss: 1.3819\n",
      "Best Prec@1: 50.290\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/390]\tTime 0.004 (0.004)\tLoss 1.3705 (1.3705)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.005)\tLoss 1.3997 (1.3053)\tPrec@1 50.000 (53.807)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.005)\tLoss 1.3703 (1.3289)\tPrec@1 54.688 (52.831)\n",
      "Epoch: [5][234/390]\tTime 0.007 (0.005)\tLoss 1.3619 (1.3279)\tPrec@1 58.594 (52.836)\n",
      "Epoch: [5][312/390]\tTime 0.002 (0.005)\tLoss 1.2987 (1.3298)\tPrec@1 54.688 (52.733)\n",
      "Epoch: [5][390/390]\tTime 0.005 (0.005)\tLoss 1.3211 (1.3338)\tPrec@1 58.750 (52.574)\n",
      "EPOCH: 5 train Results: Prec@1 52.574 Loss: 1.3338\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2136 (1.2136)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.006 (0.001)\tLoss 1.3947 (1.3527)\tPrec@1 37.500 (51.490)\n",
      "EPOCH: 5 val Results: Prec@1 51.490 Loss: 1.3527\n",
      "Best Prec@1: 51.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/390]\tTime 0.003 (0.003)\tLoss 1.2183 (1.2183)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [6][78/390]\tTime 0.002 (0.005)\tLoss 1.3751 (1.2636)\tPrec@1 43.750 (55.667)\n",
      "Epoch: [6][156/390]\tTime 0.003 (0.005)\tLoss 1.1498 (1.2735)\tPrec@1 60.156 (55.096)\n",
      "Epoch: [6][234/390]\tTime 0.002 (0.004)\tLoss 1.3359 (1.2845)\tPrec@1 53.125 (54.731)\n",
      "Epoch: [6][312/390]\tTime 0.004 (0.004)\tLoss 1.3691 (1.2956)\tPrec@1 49.219 (54.288)\n",
      "Epoch: [6][390/390]\tTime 0.007 (0.005)\tLoss 1.5191 (1.3019)\tPrec@1 50.000 (54.026)\n",
      "EPOCH: 6 train Results: Prec@1 54.026 Loss: 1.3019\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.2339 (1.2339)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.004)\tLoss 1.3587 (1.3346)\tPrec@1 43.750 (51.320)\n",
      "EPOCH: 6 val Results: Prec@1 51.320 Loss: 1.3346\n",
      "Best Prec@1: 51.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/390]\tTime 0.005 (0.005)\tLoss 1.3259 (1.3259)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [7][78/390]\tTime 0.002 (0.005)\tLoss 1.2340 (1.2446)\tPrec@1 57.812 (55.973)\n",
      "Epoch: [7][156/390]\tTime 0.004 (0.004)\tLoss 1.4204 (1.2565)\tPrec@1 46.094 (55.519)\n",
      "Epoch: [7][234/390]\tTime 0.009 (0.006)\tLoss 1.3231 (1.2662)\tPrec@1 50.781 (54.990)\n",
      "Epoch: [7][312/390]\tTime 0.002 (0.006)\tLoss 1.2994 (1.2704)\tPrec@1 53.906 (54.825)\n",
      "Epoch: [7][390/390]\tTime 0.001 (0.005)\tLoss 1.3562 (1.2776)\tPrec@1 51.250 (54.622)\n",
      "EPOCH: 7 train Results: Prec@1 54.622 Loss: 1.2776\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2084 (1.2084)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4724 (1.3130)\tPrec@1 18.750 (52.880)\n",
      "EPOCH: 7 val Results: Prec@1 52.880 Loss: 1.3130\n",
      "Best Prec@1: 52.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/390]\tTime 0.005 (0.005)\tLoss 1.2051 (1.2051)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [8][78/390]\tTime 0.002 (0.004)\tLoss 1.2271 (1.2241)\tPrec@1 55.469 (56.458)\n",
      "Epoch: [8][156/390]\tTime 0.002 (0.004)\tLoss 1.3590 (1.2454)\tPrec@1 48.438 (55.419)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.004)\tLoss 1.2339 (1.2477)\tPrec@1 55.469 (55.416)\n",
      "Epoch: [8][312/390]\tTime 0.010 (0.004)\tLoss 1.1045 (1.2505)\tPrec@1 64.062 (55.511)\n",
      "Epoch: [8][390/390]\tTime 0.003 (0.004)\tLoss 1.3797 (1.2581)\tPrec@1 50.000 (55.340)\n",
      "EPOCH: 8 train Results: Prec@1 55.340 Loss: 1.2581\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1711 (1.1711)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4565 (1.3065)\tPrec@1 31.250 (53.270)\n",
      "EPOCH: 8 val Results: Prec@1 53.270 Loss: 1.3065\n",
      "Best Prec@1: 53.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/390]\tTime 0.004 (0.004)\tLoss 1.1923 (1.1923)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [9][78/390]\tTime 0.002 (0.004)\tLoss 1.1151 (1.2010)\tPrec@1 64.844 (57.466)\n",
      "Epoch: [9][156/390]\tTime 0.007 (0.005)\tLoss 1.3641 (1.2135)\tPrec@1 50.781 (57.011)\n",
      "Epoch: [9][234/390]\tTime 0.008 (0.005)\tLoss 1.3672 (1.2309)\tPrec@1 47.656 (56.220)\n",
      "Epoch: [9][312/390]\tTime 0.006 (0.005)\tLoss 1.3375 (1.2389)\tPrec@1 48.438 (55.960)\n",
      "Epoch: [9][390/390]\tTime 0.002 (0.005)\tLoss 1.4929 (1.2480)\tPrec@1 47.500 (55.558)\n",
      "EPOCH: 9 train Results: Prec@1 55.558 Loss: 1.2480\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1522 (1.1522)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3399 (1.3017)\tPrec@1 31.250 (53.560)\n",
      "EPOCH: 9 val Results: Prec@1 53.560 Loss: 1.3017\n",
      "Best Prec@1: 53.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/390]\tTime 0.013 (0.013)\tLoss 1.1641 (1.1641)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [10][78/390]\tTime 0.004 (0.004)\tLoss 1.0087 (1.1868)\tPrec@1 65.625 (57.812)\n",
      "Epoch: [10][156/390]\tTime 0.007 (0.004)\tLoss 1.2396 (1.2072)\tPrec@1 56.250 (57.195)\n",
      "Epoch: [10][234/390]\tTime 0.003 (0.005)\tLoss 1.1993 (1.2202)\tPrec@1 60.156 (56.533)\n",
      "Epoch: [10][312/390]\tTime 0.003 (0.005)\tLoss 1.3262 (1.2287)\tPrec@1 49.219 (56.257)\n",
      "Epoch: [10][390/390]\tTime 0.002 (0.005)\tLoss 1.0870 (1.2354)\tPrec@1 62.500 (55.962)\n",
      "EPOCH: 10 train Results: Prec@1 55.962 Loss: 1.2354\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2086 (1.2086)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.007 (0.002)\tLoss 1.7199 (1.2792)\tPrec@1 25.000 (53.790)\n",
      "EPOCH: 10 val Results: Prec@1 53.790 Loss: 1.2792\n",
      "Best Prec@1: 53.790\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/390]\tTime 0.016 (0.016)\tLoss 1.1264 (1.1264)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [11][78/390]\tTime 0.002 (0.006)\tLoss 1.1389 (1.1872)\tPrec@1 57.812 (57.575)\n",
      "Epoch: [11][156/390]\tTime 0.004 (0.004)\tLoss 1.0176 (1.1970)\tPrec@1 65.625 (57.300)\n",
      "Epoch: [11][234/390]\tTime 0.003 (0.006)\tLoss 1.2753 (1.2061)\tPrec@1 53.906 (56.805)\n",
      "Epoch: [11][312/390]\tTime 0.011 (0.006)\tLoss 1.1048 (1.2123)\tPrec@1 63.281 (56.684)\n",
      "Epoch: [11][390/390]\tTime 0.012 (0.006)\tLoss 1.2133 (1.2213)\tPrec@1 58.750 (56.304)\n",
      "EPOCH: 11 train Results: Prec@1 56.304 Loss: 1.2213\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1051 (1.1051)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4246 (1.2827)\tPrec@1 37.500 (54.020)\n",
      "EPOCH: 11 val Results: Prec@1 54.020 Loss: 1.2827\n",
      "Best Prec@1: 54.020\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/390]\tTime 0.008 (0.008)\tLoss 1.0834 (1.0834)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [12][78/390]\tTime 0.002 (0.005)\tLoss 1.1179 (1.1682)\tPrec@1 60.156 (58.534)\n",
      "Epoch: [12][156/390]\tTime 0.002 (0.005)\tLoss 1.2895 (1.1816)\tPrec@1 54.688 (58.081)\n",
      "Epoch: [12][234/390]\tTime 0.006 (0.005)\tLoss 1.0579 (1.1943)\tPrec@1 70.312 (57.437)\n",
      "Epoch: [12][312/390]\tTime 0.015 (0.005)\tLoss 1.2523 (1.2032)\tPrec@1 58.594 (57.336)\n",
      "Epoch: [12][390/390]\tTime 0.005 (0.005)\tLoss 1.0246 (1.2108)\tPrec@1 61.250 (57.030)\n",
      "EPOCH: 12 train Results: Prec@1 57.030 Loss: 1.2108\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1413 (1.1413)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3414 (1.2785)\tPrec@1 50.000 (54.350)\n",
      "EPOCH: 12 val Results: Prec@1 54.350 Loss: 1.2785\n",
      "Best Prec@1: 54.350\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/390]\tTime 0.009 (0.009)\tLoss 1.1582 (1.1582)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [13][78/390]\tTime 0.016 (0.007)\tLoss 1.1310 (1.1562)\tPrec@1 58.594 (58.633)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.006)\tLoss 1.1211 (1.1674)\tPrec@1 60.938 (58.569)\n",
      "Epoch: [13][234/390]\tTime 0.003 (0.005)\tLoss 1.2753 (1.1844)\tPrec@1 56.250 (57.985)\n",
      "Epoch: [13][312/390]\tTime 0.003 (0.005)\tLoss 1.1894 (1.1968)\tPrec@1 55.469 (57.431)\n",
      "Epoch: [13][390/390]\tTime 0.001 (0.005)\tLoss 1.1324 (1.2011)\tPrec@1 57.500 (57.298)\n",
      "EPOCH: 13 train Results: Prec@1 57.298 Loss: 1.2011\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1078 (1.1078)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4721 (1.2647)\tPrec@1 25.000 (54.510)\n",
      "EPOCH: 13 val Results: Prec@1 54.510 Loss: 1.2647\n",
      "Best Prec@1: 54.510\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/390]\tTime 0.002 (0.002)\tLoss 1.0893 (1.0893)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [14][78/390]\tTime 0.003 (0.007)\tLoss 1.1517 (1.1479)\tPrec@1 55.469 (59.246)\n",
      "Epoch: [14][156/390]\tTime 0.002 (0.005)\tLoss 1.2876 (1.1618)\tPrec@1 53.125 (58.489)\n",
      "Epoch: [14][234/390]\tTime 0.002 (0.005)\tLoss 1.2420 (1.1779)\tPrec@1 57.031 (58.042)\n",
      "Epoch: [14][312/390]\tTime 0.003 (0.004)\tLoss 1.0632 (1.1844)\tPrec@1 57.812 (57.852)\n",
      "Epoch: [14][390/390]\tTime 0.002 (0.005)\tLoss 1.3009 (1.1905)\tPrec@1 51.250 (57.620)\n",
      "EPOCH: 14 train Results: Prec@1 57.620 Loss: 1.1905\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1148 (1.1148)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4374 (1.2633)\tPrec@1 37.500 (54.690)\n",
      "EPOCH: 14 val Results: Prec@1 54.690 Loss: 1.2633\n",
      "Best Prec@1: 54.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/390]\tTime 0.013 (0.013)\tLoss 1.0361 (1.0361)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [15][78/390]\tTime 0.007 (0.007)\tLoss 1.2745 (1.1277)\tPrec@1 53.125 (59.939)\n",
      "Epoch: [15][156/390]\tTime 0.002 (0.005)\tLoss 0.9859 (1.1443)\tPrec@1 67.188 (59.494)\n",
      "Epoch: [15][234/390]\tTime 0.004 (0.005)\tLoss 1.1105 (1.1587)\tPrec@1 63.281 (58.780)\n",
      "Epoch: [15][312/390]\tTime 0.004 (0.005)\tLoss 1.2683 (1.1706)\tPrec@1 57.031 (58.451)\n",
      "Epoch: [15][390/390]\tTime 0.001 (0.005)\tLoss 1.1553 (1.1773)\tPrec@1 47.500 (58.204)\n",
      "EPOCH: 15 train Results: Prec@1 58.204 Loss: 1.1773\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1225 (1.1225)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4208 (1.2652)\tPrec@1 43.750 (54.590)\n",
      "EPOCH: 15 val Results: Prec@1 54.590 Loss: 1.2652\n",
      "Best Prec@1: 54.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/390]\tTime 0.005 (0.005)\tLoss 1.0229 (1.0229)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.003)\tLoss 1.0704 (1.1061)\tPrec@1 65.625 (60.750)\n",
      "Epoch: [16][156/390]\tTime 0.003 (0.004)\tLoss 1.1696 (1.1312)\tPrec@1 55.469 (59.753)\n",
      "Epoch: [16][234/390]\tTime 0.006 (0.005)\tLoss 1.2645 (1.1511)\tPrec@1 53.906 (58.976)\n",
      "Epoch: [16][312/390]\tTime 0.002 (0.005)\tLoss 1.2219 (1.1660)\tPrec@1 52.344 (58.377)\n",
      "Epoch: [16][390/390]\tTime 0.001 (0.005)\tLoss 1.4107 (1.1748)\tPrec@1 50.000 (58.036)\n",
      "EPOCH: 16 train Results: Prec@1 58.036 Loss: 1.1748\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0294 (1.0294)\tPrec@1 70.312 (70.312)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2924 (1.2579)\tPrec@1 37.500 (54.960)\n",
      "EPOCH: 16 val Results: Prec@1 54.960 Loss: 1.2579\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/390]\tTime 0.005 (0.005)\tLoss 1.1394 (1.1394)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [17][78/390]\tTime 0.003 (0.003)\tLoss 1.2286 (1.1129)\tPrec@1 57.031 (60.789)\n",
      "Epoch: [17][156/390]\tTime 0.004 (0.004)\tLoss 1.2811 (1.1306)\tPrec@1 57.812 (60.062)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.004)\tLoss 1.1470 (1.1487)\tPrec@1 58.594 (59.292)\n",
      "Epoch: [17][312/390]\tTime 0.002 (0.004)\tLoss 1.2060 (1.1553)\tPrec@1 56.250 (58.891)\n",
      "Epoch: [17][390/390]\tTime 0.002 (0.004)\tLoss 1.1106 (1.1647)\tPrec@1 63.750 (58.556)\n",
      "EPOCH: 17 train Results: Prec@1 58.556 Loss: 1.1647\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1790 (1.1790)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4220 (1.2552)\tPrec@1 25.000 (54.870)\n",
      "EPOCH: 17 val Results: Prec@1 54.870 Loss: 1.2552\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/390]\tTime 0.007 (0.007)\tLoss 0.9817 (0.9817)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [18][78/390]\tTime 0.004 (0.009)\tLoss 1.1556 (1.1110)\tPrec@1 58.594 (60.542)\n",
      "Epoch: [18][156/390]\tTime 0.008 (0.008)\tLoss 1.0814 (1.1243)\tPrec@1 63.281 (60.171)\n",
      "Epoch: [18][234/390]\tTime 0.004 (0.006)\tLoss 1.0179 (1.1460)\tPrec@1 64.844 (59.235)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.005)\tLoss 1.1984 (1.1534)\tPrec@1 59.375 (58.886)\n",
      "Epoch: [18][390/390]\tTime 0.002 (0.005)\tLoss 1.2003 (1.1603)\tPrec@1 55.000 (58.688)\n",
      "EPOCH: 18 train Results: Prec@1 58.688 Loss: 1.1603\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1063 (1.1063)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1349 (1.2546)\tPrec@1 56.250 (54.920)\n",
      "EPOCH: 18 val Results: Prec@1 54.920 Loss: 1.2546\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/390]\tTime 0.002 (0.002)\tLoss 1.1313 (1.1313)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [19][78/390]\tTime 0.026 (0.003)\tLoss 1.2312 (1.0887)\tPrec@1 56.250 (61.333)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.005)\tLoss 1.0781 (1.1179)\tPrec@1 63.281 (60.395)\n",
      "Epoch: [19][234/390]\tTime 0.003 (0.004)\tLoss 1.1585 (1.1304)\tPrec@1 54.688 (59.927)\n",
      "Epoch: [19][312/390]\tTime 0.007 (0.004)\tLoss 1.0056 (1.1429)\tPrec@1 64.844 (59.542)\n",
      "Epoch: [19][390/390]\tTime 0.006 (0.004)\tLoss 1.4464 (1.1508)\tPrec@1 47.500 (59.166)\n",
      "EPOCH: 19 train Results: Prec@1 59.166 Loss: 1.1508\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1915 (1.1915)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.6055 (1.2502)\tPrec@1 37.500 (55.210)\n",
      "EPOCH: 19 val Results: Prec@1 55.210 Loss: 1.2502\n",
      "Best Prec@1: 55.210\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/390]\tTime 0.007 (0.007)\tLoss 1.0732 (1.0732)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [20][78/390]\tTime 0.002 (0.004)\tLoss 1.0319 (1.0932)\tPrec@1 60.938 (61.076)\n",
      "Epoch: [20][156/390]\tTime 0.004 (0.003)\tLoss 1.0047 (1.1082)\tPrec@1 65.625 (60.644)\n",
      "Epoch: [20][234/390]\tTime 0.004 (0.004)\tLoss 1.0367 (1.1296)\tPrec@1 64.062 (59.797)\n",
      "Epoch: [20][312/390]\tTime 0.004 (0.004)\tLoss 1.1510 (1.1378)\tPrec@1 60.938 (59.567)\n",
      "Epoch: [20][390/390]\tTime 0.002 (0.004)\tLoss 1.3127 (1.1448)\tPrec@1 50.000 (59.306)\n",
      "EPOCH: 20 train Results: Prec@1 59.306 Loss: 1.1448\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1366 (1.1366)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2634 (1.2421)\tPrec@1 43.750 (55.820)\n",
      "EPOCH: 20 val Results: Prec@1 55.820 Loss: 1.2421\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/390]\tTime 0.003 (0.003)\tLoss 1.2056 (1.2056)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [21][78/390]\tTime 0.002 (0.005)\tLoss 1.2114 (1.0702)\tPrec@1 52.344 (62.223)\n",
      "Epoch: [21][156/390]\tTime 0.003 (0.005)\tLoss 1.1794 (1.1005)\tPrec@1 52.344 (60.928)\n",
      "Epoch: [21][234/390]\tTime 0.007 (0.005)\tLoss 1.1575 (1.1193)\tPrec@1 57.812 (60.166)\n",
      "Epoch: [21][312/390]\tTime 0.006 (0.005)\tLoss 1.1194 (1.1337)\tPrec@1 64.062 (59.732)\n",
      "Epoch: [21][390/390]\tTime 0.003 (0.005)\tLoss 1.1345 (1.1430)\tPrec@1 58.750 (59.432)\n",
      "EPOCH: 21 train Results: Prec@1 59.432 Loss: 1.1430\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1490 (1.1490)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1926 (1.2412)\tPrec@1 43.750 (55.430)\n",
      "EPOCH: 21 val Results: Prec@1 55.430 Loss: 1.2412\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/390]\tTime 0.006 (0.006)\tLoss 1.1429 (1.1429)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [22][78/390]\tTime 0.005 (0.004)\tLoss 1.0447 (1.0988)\tPrec@1 65.625 (61.234)\n",
      "Epoch: [22][156/390]\tTime 0.002 (0.004)\tLoss 1.1255 (1.1106)\tPrec@1 60.938 (60.584)\n",
      "Epoch: [22][234/390]\tTime 0.002 (0.004)\tLoss 1.1368 (1.1206)\tPrec@1 62.500 (60.170)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.005)\tLoss 1.1809 (1.1331)\tPrec@1 57.812 (59.667)\n",
      "Epoch: [22][390/390]\tTime 0.002 (0.005)\tLoss 1.1565 (1.1427)\tPrec@1 65.000 (59.338)\n",
      "EPOCH: 22 train Results: Prec@1 59.338 Loss: 1.1427\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1340 (1.1340)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.005)\tLoss 1.0669 (1.2483)\tPrec@1 62.500 (55.290)\n",
      "EPOCH: 22 val Results: Prec@1 55.290 Loss: 1.2483\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/390]\tTime 0.005 (0.005)\tLoss 1.0761 (1.0761)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [23][78/390]\tTime 0.002 (0.006)\tLoss 1.0389 (1.0739)\tPrec@1 64.062 (61.897)\n",
      "Epoch: [23][156/390]\tTime 0.004 (0.006)\tLoss 1.2716 (1.0976)\tPrec@1 52.344 (61.286)\n",
      "Epoch: [23][234/390]\tTime 0.004 (0.006)\tLoss 1.1561 (1.1066)\tPrec@1 59.375 (60.954)\n",
      "Epoch: [23][312/390]\tTime 0.006 (0.006)\tLoss 1.2206 (1.1222)\tPrec@1 52.344 (60.304)\n",
      "Epoch: [23][390/390]\tTime 0.002 (0.006)\tLoss 1.3163 (1.1350)\tPrec@1 52.500 (59.750)\n",
      "EPOCH: 23 train Results: Prec@1 59.750 Loss: 1.1350\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1983 (1.1983)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2615 (1.2485)\tPrec@1 37.500 (55.490)\n",
      "EPOCH: 23 val Results: Prec@1 55.490 Loss: 1.2485\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/390]\tTime 0.004 (0.004)\tLoss 1.1168 (1.1168)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.006)\tLoss 1.2432 (1.0667)\tPrec@1 60.938 (62.381)\n",
      "Epoch: [24][156/390]\tTime 0.016 (0.006)\tLoss 1.2218 (1.0929)\tPrec@1 54.688 (60.997)\n",
      "Epoch: [24][234/390]\tTime 0.002 (0.005)\tLoss 1.2551 (1.1149)\tPrec@1 54.688 (60.273)\n",
      "Epoch: [24][312/390]\tTime 0.002 (0.005)\tLoss 1.2940 (1.1250)\tPrec@1 53.906 (59.989)\n",
      "Epoch: [24][390/390]\tTime 0.004 (0.005)\tLoss 1.3143 (1.1309)\tPrec@1 57.500 (59.654)\n",
      "EPOCH: 24 train Results: Prec@1 59.654 Loss: 1.1309\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1614 (1.1614)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1625 (1.2555)\tPrec@1 43.750 (55.440)\n",
      "EPOCH: 24 val Results: Prec@1 55.440 Loss: 1.2555\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/390]\tTime 0.002 (0.002)\tLoss 1.0410 (1.0410)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [25][78/390]\tTime 0.003 (0.004)\tLoss 1.0798 (1.0687)\tPrec@1 61.719 (62.104)\n",
      "Epoch: [25][156/390]\tTime 0.002 (0.005)\tLoss 1.2158 (1.0910)\tPrec@1 57.812 (61.346)\n",
      "Epoch: [25][234/390]\tTime 0.002 (0.005)\tLoss 1.1880 (1.1078)\tPrec@1 57.812 (60.701)\n",
      "Epoch: [25][312/390]\tTime 0.003 (0.005)\tLoss 1.2508 (1.1194)\tPrec@1 57.031 (60.296)\n",
      "Epoch: [25][390/390]\tTime 0.008 (0.005)\tLoss 1.2591 (1.1245)\tPrec@1 51.250 (60.066)\n",
      "EPOCH: 25 train Results: Prec@1 60.066 Loss: 1.1245\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1582 (1.1582)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1056 (1.2498)\tPrec@1 56.250 (55.190)\n",
      "EPOCH: 25 val Results: Prec@1 55.190 Loss: 1.2498\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/390]\tTime 0.006 (0.006)\tLoss 1.0919 (1.0919)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [26][78/390]\tTime 0.002 (0.005)\tLoss 1.1185 (1.0567)\tPrec@1 63.281 (62.905)\n",
      "Epoch: [26][156/390]\tTime 0.007 (0.004)\tLoss 1.1518 (1.0837)\tPrec@1 59.375 (61.858)\n",
      "Epoch: [26][234/390]\tTime 0.006 (0.005)\tLoss 1.0749 (1.1009)\tPrec@1 60.938 (61.084)\n",
      "Epoch: [26][312/390]\tTime 0.002 (0.004)\tLoss 1.0487 (1.1133)\tPrec@1 63.281 (60.518)\n",
      "Epoch: [26][390/390]\tTime 0.001 (0.004)\tLoss 1.2085 (1.1202)\tPrec@1 56.250 (60.256)\n",
      "EPOCH: 26 train Results: Prec@1 60.256 Loss: 1.1202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1329 (1.1329)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3095 (1.2434)\tPrec@1 43.750 (55.870)\n",
      "EPOCH: 26 val Results: Prec@1 55.870 Loss: 1.2434\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/390]\tTime 0.003 (0.003)\tLoss 1.0716 (1.0716)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [27][78/390]\tTime 0.002 (0.003)\tLoss 0.9578 (1.0553)\tPrec@1 62.500 (62.747)\n",
      "Epoch: [27][156/390]\tTime 0.002 (0.003)\tLoss 0.9521 (1.0790)\tPrec@1 64.844 (61.699)\n",
      "Epoch: [27][234/390]\tTime 0.005 (0.003)\tLoss 0.9027 (1.0924)\tPrec@1 71.875 (61.184)\n",
      "Epoch: [27][312/390]\tTime 0.002 (0.003)\tLoss 1.1286 (1.1067)\tPrec@1 62.500 (60.698)\n",
      "Epoch: [27][390/390]\tTime 0.003 (0.003)\tLoss 1.4782 (1.1192)\tPrec@1 52.500 (60.334)\n",
      "EPOCH: 27 train Results: Prec@1 60.334 Loss: 1.1192\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1028 (1.1028)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1252 (1.2488)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 27 val Results: Prec@1 55.150 Loss: 1.2488\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/390]\tTime 0.003 (0.003)\tLoss 1.0103 (1.0103)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [28][78/390]\tTime 0.002 (0.003)\tLoss 1.0020 (1.0605)\tPrec@1 65.625 (62.727)\n",
      "Epoch: [28][156/390]\tTime 0.002 (0.003)\tLoss 1.1067 (1.0780)\tPrec@1 64.062 (61.958)\n",
      "Epoch: [28][234/390]\tTime 0.002 (0.003)\tLoss 1.1031 (1.0940)\tPrec@1 57.031 (61.316)\n",
      "Epoch: [28][312/390]\tTime 0.002 (0.003)\tLoss 1.2498 (1.1068)\tPrec@1 54.688 (60.788)\n",
      "Epoch: [28][390/390]\tTime 0.003 (0.003)\tLoss 1.3011 (1.1152)\tPrec@1 43.750 (60.450)\n",
      "EPOCH: 28 train Results: Prec@1 60.450 Loss: 1.1152\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1445 (1.1445)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1534 (1.2398)\tPrec@1 50.000 (55.800)\n",
      "EPOCH: 28 val Results: Prec@1 55.800 Loss: 1.2398\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/390]\tTime 0.002 (0.002)\tLoss 1.0195 (1.0195)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [29][78/390]\tTime 0.002 (0.003)\tLoss 1.2215 (1.0559)\tPrec@1 57.031 (62.747)\n",
      "Epoch: [29][156/390]\tTime 0.004 (0.003)\tLoss 1.0964 (1.0778)\tPrec@1 61.719 (61.709)\n",
      "Epoch: [29][234/390]\tTime 0.003 (0.003)\tLoss 0.9127 (1.0925)\tPrec@1 72.656 (61.094)\n",
      "Epoch: [29][312/390]\tTime 0.003 (0.004)\tLoss 1.0782 (1.1070)\tPrec@1 57.812 (60.655)\n",
      "Epoch: [29][390/390]\tTime 0.001 (0.004)\tLoss 1.3130 (1.1164)\tPrec@1 50.000 (60.320)\n",
      "EPOCH: 29 train Results: Prec@1 60.320 Loss: 1.1164\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.0754 (1.0754)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9349 (1.2405)\tPrec@1 62.500 (55.870)\n",
      "EPOCH: 29 val Results: Prec@1 55.870 Loss: 1.2405\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/390]\tTime 0.005 (0.005)\tLoss 1.0915 (1.0915)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [30][78/390]\tTime 0.005 (0.003)\tLoss 1.0368 (1.0532)\tPrec@1 61.719 (62.352)\n",
      "Epoch: [30][156/390]\tTime 0.003 (0.003)\tLoss 1.0874 (1.0745)\tPrec@1 64.844 (61.828)\n",
      "Epoch: [30][234/390]\tTime 0.004 (0.003)\tLoss 1.0740 (1.0891)\tPrec@1 62.500 (61.293)\n",
      "Epoch: [30][312/390]\tTime 0.002 (0.003)\tLoss 1.1225 (1.1003)\tPrec@1 56.250 (60.763)\n",
      "Epoch: [30][390/390]\tTime 0.002 (0.003)\tLoss 1.0556 (1.1112)\tPrec@1 62.500 (60.394)\n",
      "EPOCH: 30 train Results: Prec@1 60.394 Loss: 1.1112\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0682 (1.0682)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0768 (1.2401)\tPrec@1 43.750 (55.880)\n",
      "EPOCH: 30 val Results: Prec@1 55.880 Loss: 1.2401\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/390]\tTime 0.002 (0.002)\tLoss 1.0081 (1.0081)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [31][78/390]\tTime 0.008 (0.003)\tLoss 0.9807 (1.0456)\tPrec@1 64.062 (63.093)\n",
      "Epoch: [31][156/390]\tTime 0.002 (0.003)\tLoss 1.1179 (1.0709)\tPrec@1 60.156 (61.878)\n",
      "Epoch: [31][234/390]\tTime 0.002 (0.003)\tLoss 1.1012 (1.0883)\tPrec@1 63.281 (61.230)\n",
      "Epoch: [31][312/390]\tTime 0.005 (0.003)\tLoss 1.1029 (1.0990)\tPrec@1 62.500 (60.798)\n",
      "Epoch: [31][390/390]\tTime 0.003 (0.003)\tLoss 1.1829 (1.1097)\tPrec@1 53.750 (60.354)\n",
      "EPOCH: 31 train Results: Prec@1 60.354 Loss: 1.1097\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1585 (1.1585)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2368 (1.2523)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 31 val Results: Prec@1 55.370 Loss: 1.2523\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/390]\tTime 0.002 (0.002)\tLoss 1.0223 (1.0223)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [32][78/390]\tTime 0.002 (0.003)\tLoss 1.0849 (1.0482)\tPrec@1 60.156 (63.222)\n",
      "Epoch: [32][156/390]\tTime 0.003 (0.003)\tLoss 0.9339 (1.0689)\tPrec@1 64.062 (62.097)\n",
      "Epoch: [32][234/390]\tTime 0.003 (0.003)\tLoss 1.1358 (1.0869)\tPrec@1 60.156 (61.443)\n",
      "Epoch: [32][312/390]\tTime 0.003 (0.003)\tLoss 1.0108 (1.0986)\tPrec@1 63.281 (60.895)\n",
      "Epoch: [32][390/390]\tTime 0.002 (0.003)\tLoss 1.0163 (1.1098)\tPrec@1 63.750 (60.520)\n",
      "EPOCH: 32 train Results: Prec@1 60.520 Loss: 1.1098\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1212 (1.1212)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2386 (1.2425)\tPrec@1 43.750 (55.810)\n",
      "EPOCH: 32 val Results: Prec@1 55.810 Loss: 1.2425\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/390]\tTime 0.003 (0.003)\tLoss 1.1745 (1.1745)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [33][78/390]\tTime 0.002 (0.003)\tLoss 1.1843 (1.0674)\tPrec@1 57.031 (61.828)\n",
      "Epoch: [33][156/390]\tTime 0.003 (0.003)\tLoss 1.2053 (1.0794)\tPrec@1 61.719 (61.291)\n",
      "Epoch: [33][234/390]\tTime 0.003 (0.003)\tLoss 1.2638 (1.0852)\tPrec@1 55.469 (60.981)\n",
      "Epoch: [33][312/390]\tTime 0.002 (0.004)\tLoss 1.1696 (1.0985)\tPrec@1 57.812 (60.506)\n",
      "Epoch: [33][390/390]\tTime 0.001 (0.004)\tLoss 1.0993 (1.1076)\tPrec@1 67.500 (60.340)\n",
      "EPOCH: 33 train Results: Prec@1 60.340 Loss: 1.1076\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0798 (1.0798)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3097 (1.2408)\tPrec@1 37.500 (55.510)\n",
      "EPOCH: 33 val Results: Prec@1 55.510 Loss: 1.2408\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/390]\tTime 0.003 (0.003)\tLoss 0.9312 (0.9312)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [34][78/390]\tTime 0.002 (0.004)\tLoss 1.1217 (1.0341)\tPrec@1 57.812 (63.627)\n",
      "Epoch: [34][156/390]\tTime 0.004 (0.003)\tLoss 1.0754 (1.0626)\tPrec@1 61.719 (62.371)\n",
      "Epoch: [34][234/390]\tTime 0.003 (0.003)\tLoss 1.0885 (1.0775)\tPrec@1 58.594 (61.825)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.0903 (1.0921)\tPrec@1 66.406 (61.212)\n",
      "Epoch: [34][390/390]\tTime 0.001 (0.003)\tLoss 1.0063 (1.1043)\tPrec@1 61.250 (60.778)\n",
      "EPOCH: 34 train Results: Prec@1 60.778 Loss: 1.1043\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1051 (1.1051)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2968 (1.2378)\tPrec@1 37.500 (56.120)\n",
      "EPOCH: 34 val Results: Prec@1 56.120 Loss: 1.2378\n",
      "Best Prec@1: 56.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/390]\tTime 0.008 (0.008)\tLoss 1.0633 (1.0633)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [35][78/390]\tTime 0.002 (0.003)\tLoss 1.2407 (1.0375)\tPrec@1 51.562 (62.905)\n",
      "Epoch: [35][156/390]\tTime 0.006 (0.003)\tLoss 1.2310 (1.0547)\tPrec@1 57.812 (62.261)\n",
      "Epoch: [35][234/390]\tTime 0.002 (0.003)\tLoss 1.1838 (1.0792)\tPrec@1 57.812 (61.443)\n",
      "Epoch: [35][312/390]\tTime 0.005 (0.003)\tLoss 1.1745 (1.0906)\tPrec@1 57.812 (61.007)\n",
      "Epoch: [35][390/390]\tTime 0.002 (0.003)\tLoss 1.0454 (1.0995)\tPrec@1 65.000 (60.804)\n",
      "EPOCH: 35 train Results: Prec@1 60.804 Loss: 1.0995\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1081 (1.1081)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2095 (1.2361)\tPrec@1 56.250 (55.930)\n",
      "EPOCH: 35 val Results: Prec@1 55.930 Loss: 1.2361\n",
      "Best Prec@1: 56.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/390]\tTime 0.002 (0.002)\tLoss 1.1356 (1.1356)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [36][78/390]\tTime 0.002 (0.003)\tLoss 1.0478 (1.0352)\tPrec@1 64.844 (63.143)\n",
      "Epoch: [36][156/390]\tTime 0.005 (0.003)\tLoss 1.1877 (1.0575)\tPrec@1 57.031 (62.515)\n",
      "Epoch: [36][234/390]\tTime 0.003 (0.003)\tLoss 1.0784 (1.0703)\tPrec@1 60.938 (61.772)\n",
      "Epoch: [36][312/390]\tTime 0.003 (0.003)\tLoss 1.1817 (1.0824)\tPrec@1 59.375 (61.367)\n",
      "Epoch: [36][390/390]\tTime 0.004 (0.003)\tLoss 1.2017 (1.0933)\tPrec@1 56.250 (60.980)\n",
      "EPOCH: 36 train Results: Prec@1 60.980 Loss: 1.0933\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0659 (1.0659)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1774 (1.2335)\tPrec@1 37.500 (56.570)\n",
      "EPOCH: 36 val Results: Prec@1 56.570 Loss: 1.2335\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/390]\tTime 0.002 (0.002)\tLoss 0.9658 (0.9658)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 0.9547 (1.0238)\tPrec@1 66.406 (63.528)\n",
      "Epoch: [37][156/390]\tTime 0.009 (0.003)\tLoss 0.9859 (1.0574)\tPrec@1 57.812 (62.435)\n",
      "Epoch: [37][234/390]\tTime 0.004 (0.003)\tLoss 1.3719 (1.0739)\tPrec@1 48.438 (61.765)\n",
      "Epoch: [37][312/390]\tTime 0.004 (0.003)\tLoss 0.9941 (1.0918)\tPrec@1 64.062 (61.122)\n",
      "Epoch: [37][390/390]\tTime 0.004 (0.003)\tLoss 1.0434 (1.0977)\tPrec@1 61.250 (60.952)\n",
      "EPOCH: 37 train Results: Prec@1 60.952 Loss: 1.0977\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1059 (1.1059)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1030 (1.2337)\tPrec@1 56.250 (55.810)\n",
      "EPOCH: 37 val Results: Prec@1 55.810 Loss: 1.2337\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/390]\tTime 0.002 (0.002)\tLoss 1.0013 (1.0013)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.004)\tLoss 1.2116 (1.0476)\tPrec@1 58.594 (62.648)\n",
      "Epoch: [38][156/390]\tTime 0.004 (0.003)\tLoss 0.9461 (1.0573)\tPrec@1 66.406 (62.619)\n",
      "Epoch: [38][234/390]\tTime 0.003 (0.003)\tLoss 1.0352 (1.0743)\tPrec@1 65.625 (61.932)\n",
      "Epoch: [38][312/390]\tTime 0.003 (0.003)\tLoss 1.2506 (1.0854)\tPrec@1 55.469 (61.539)\n",
      "Epoch: [38][390/390]\tTime 0.001 (0.003)\tLoss 1.1811 (1.0945)\tPrec@1 61.250 (61.218)\n",
      "EPOCH: 38 train Results: Prec@1 61.218 Loss: 1.0945\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.0928 (1.0928)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0500 (1.2259)\tPrec@1 56.250 (56.260)\n",
      "EPOCH: 38 val Results: Prec@1 56.260 Loss: 1.2259\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/390]\tTime 0.003 (0.003)\tLoss 1.0577 (1.0577)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.003)\tLoss 1.2317 (1.0186)\tPrec@1 53.125 (63.845)\n",
      "Epoch: [39][156/390]\tTime 0.002 (0.003)\tLoss 1.0833 (1.0463)\tPrec@1 59.375 (62.883)\n",
      "Epoch: [39][234/390]\tTime 0.002 (0.003)\tLoss 1.0763 (1.0677)\tPrec@1 61.719 (62.161)\n",
      "Epoch: [39][312/390]\tTime 0.002 (0.003)\tLoss 1.4371 (1.0786)\tPrec@1 50.781 (61.791)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2381 (1.0928)\tPrec@1 52.500 (61.236)\n",
      "EPOCH: 39 train Results: Prec@1 61.236 Loss: 1.0928\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0667 (1.0667)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3399 (1.2383)\tPrec@1 50.000 (56.050)\n",
      "EPOCH: 39 val Results: Prec@1 56.050 Loss: 1.2383\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/390]\tTime 0.003 (0.003)\tLoss 1.0323 (1.0323)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [40][78/390]\tTime 0.003 (0.003)\tLoss 1.0073 (1.0323)\tPrec@1 64.062 (63.281)\n",
      "Epoch: [40][156/390]\tTime 0.002 (0.002)\tLoss 1.0334 (1.0403)\tPrec@1 60.938 (63.052)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.003)\tLoss 1.3641 (1.0554)\tPrec@1 47.656 (62.686)\n",
      "Epoch: [40][312/390]\tTime 0.003 (0.003)\tLoss 1.3373 (1.0719)\tPrec@1 53.906 (62.043)\n",
      "Epoch: [40][390/390]\tTime 0.001 (0.003)\tLoss 1.2058 (1.0879)\tPrec@1 55.000 (61.406)\n",
      "EPOCH: 40 train Results: Prec@1 61.406 Loss: 1.0879\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1085 (1.1085)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4187 (1.2358)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 40 val Results: Prec@1 55.610 Loss: 1.2358\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/390]\tTime 0.002 (0.002)\tLoss 1.1918 (1.1918)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [41][78/390]\tTime 0.002 (0.003)\tLoss 0.9655 (1.0191)\tPrec@1 62.500 (63.578)\n",
      "Epoch: [41][156/390]\tTime 0.002 (0.004)\tLoss 1.2946 (1.0488)\tPrec@1 53.906 (62.440)\n",
      "Epoch: [41][234/390]\tTime 0.004 (0.004)\tLoss 1.1530 (1.0651)\tPrec@1 59.375 (62.001)\n",
      "Epoch: [41][312/390]\tTime 0.002 (0.004)\tLoss 1.1300 (1.0800)\tPrec@1 62.500 (61.472)\n",
      "Epoch: [41][390/390]\tTime 0.001 (0.004)\tLoss 1.0270 (1.0883)\tPrec@1 67.500 (61.230)\n",
      "EPOCH: 41 train Results: Prec@1 61.230 Loss: 1.0883\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0386 (1.0386)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2643 (1.2309)\tPrec@1 50.000 (56.180)\n",
      "EPOCH: 41 val Results: Prec@1 56.180 Loss: 1.2309\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/390]\tTime 0.002 (0.002)\tLoss 1.0428 (1.0428)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [42][78/390]\tTime 0.003 (0.004)\tLoss 0.9533 (1.0209)\tPrec@1 60.938 (64.132)\n",
      "Epoch: [42][156/390]\tTime 0.002 (0.003)\tLoss 1.0046 (1.0367)\tPrec@1 62.500 (63.112)\n",
      "Epoch: [42][234/390]\tTime 0.003 (0.004)\tLoss 1.0839 (1.0601)\tPrec@1 60.156 (62.284)\n",
      "Epoch: [42][312/390]\tTime 0.004 (0.004)\tLoss 1.2025 (1.0756)\tPrec@1 53.125 (61.646)\n",
      "Epoch: [42][390/390]\tTime 0.010 (0.003)\tLoss 1.1462 (1.0884)\tPrec@1 62.500 (61.116)\n",
      "EPOCH: 42 train Results: Prec@1 61.116 Loss: 1.0884\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0363 (1.0363)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1616 (1.2306)\tPrec@1 50.000 (56.430)\n",
      "EPOCH: 42 val Results: Prec@1 56.430 Loss: 1.2306\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/390]\tTime 0.002 (0.002)\tLoss 1.0596 (1.0596)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [43][78/390]\tTime 0.006 (0.004)\tLoss 0.9660 (1.0152)\tPrec@1 66.406 (64.201)\n",
      "Epoch: [43][156/390]\tTime 0.011 (0.003)\tLoss 1.2549 (1.0487)\tPrec@1 55.469 (62.709)\n",
      "Epoch: [43][234/390]\tTime 0.002 (0.003)\tLoss 1.0750 (1.0593)\tPrec@1 60.156 (62.294)\n",
      "Epoch: [43][312/390]\tTime 0.002 (0.003)\tLoss 1.0265 (1.0779)\tPrec@1 69.531 (61.721)\n",
      "Epoch: [43][390/390]\tTime 0.003 (0.003)\tLoss 0.8992 (1.0855)\tPrec@1 67.500 (61.370)\n",
      "EPOCH: 43 train Results: Prec@1 61.370 Loss: 1.0855\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1254 (1.1254)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1500 (1.2354)\tPrec@1 56.250 (56.190)\n",
      "EPOCH: 43 val Results: Prec@1 56.190 Loss: 1.2354\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/390]\tTime 0.002 (0.002)\tLoss 1.0153 (1.0153)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.003)\tLoss 1.0246 (1.0219)\tPrec@1 64.844 (64.181)\n",
      "Epoch: [44][156/390]\tTime 0.002 (0.003)\tLoss 1.0080 (1.0385)\tPrec@1 67.188 (63.122)\n",
      "Epoch: [44][234/390]\tTime 0.004 (0.003)\tLoss 1.0068 (1.0586)\tPrec@1 63.281 (62.507)\n",
      "Epoch: [44][312/390]\tTime 0.002 (0.003)\tLoss 1.0897 (1.0716)\tPrec@1 63.281 (61.906)\n",
      "Epoch: [44][390/390]\tTime 0.001 (0.003)\tLoss 1.1486 (1.0830)\tPrec@1 58.750 (61.478)\n",
      "EPOCH: 44 train Results: Prec@1 61.478 Loss: 1.0830\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0574 (1.0574)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3908 (1.2333)\tPrec@1 43.750 (56.000)\n",
      "EPOCH: 44 val Results: Prec@1 56.000 Loss: 1.2333\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/390]\tTime 0.005 (0.005)\tLoss 1.1085 (1.1085)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [45][78/390]\tTime 0.003 (0.003)\tLoss 1.1525 (1.0191)\tPrec@1 57.812 (63.973)\n",
      "Epoch: [45][156/390]\tTime 0.006 (0.003)\tLoss 1.1550 (1.0364)\tPrec@1 57.031 (63.172)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.003)\tLoss 1.1217 (1.0544)\tPrec@1 53.906 (62.497)\n",
      "Epoch: [45][312/390]\tTime 0.005 (0.003)\tLoss 0.9422 (1.0699)\tPrec@1 64.844 (61.901)\n",
      "Epoch: [45][390/390]\tTime 0.001 (0.003)\tLoss 1.3568 (1.0803)\tPrec@1 46.250 (61.514)\n",
      "EPOCH: 45 train Results: Prec@1 61.514 Loss: 1.0803\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1004 (1.1004)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2943 (1.2446)\tPrec@1 50.000 (55.620)\n",
      "EPOCH: 45 val Results: Prec@1 55.620 Loss: 1.2446\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/390]\tTime 0.003 (0.003)\tLoss 1.0538 (1.0538)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [46][78/390]\tTime 0.003 (0.003)\tLoss 1.0264 (1.0125)\tPrec@1 64.062 (64.547)\n",
      "Epoch: [46][156/390]\tTime 0.010 (0.003)\tLoss 1.1429 (1.0294)\tPrec@1 57.031 (63.640)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.1672 (1.0524)\tPrec@1 57.031 (62.696)\n",
      "Epoch: [46][312/390]\tTime 0.003 (0.003)\tLoss 1.1250 (1.0636)\tPrec@1 60.156 (62.293)\n",
      "Epoch: [46][390/390]\tTime 0.001 (0.004)\tLoss 1.0907 (1.0779)\tPrec@1 60.000 (61.614)\n",
      "EPOCH: 46 train Results: Prec@1 61.614 Loss: 1.0779\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.0525 (1.0525)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2440 (1.2547)\tPrec@1 37.500 (55.910)\n",
      "EPOCH: 46 val Results: Prec@1 55.910 Loss: 1.2547\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/390]\tTime 0.003 (0.003)\tLoss 1.3587 (1.3587)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [47][78/390]\tTime 0.008 (0.003)\tLoss 1.1388 (1.0075)\tPrec@1 56.250 (63.914)\n",
      "Epoch: [47][156/390]\tTime 0.023 (0.004)\tLoss 1.0526 (1.0319)\tPrec@1 70.312 (63.152)\n",
      "Epoch: [47][234/390]\tTime 0.002 (0.004)\tLoss 1.0682 (1.0534)\tPrec@1 57.031 (62.384)\n",
      "Epoch: [47][312/390]\tTime 0.008 (0.004)\tLoss 1.1603 (1.0641)\tPrec@1 55.469 (61.951)\n",
      "Epoch: [47][390/390]\tTime 0.003 (0.004)\tLoss 1.2064 (1.0796)\tPrec@1 61.250 (61.462)\n",
      "EPOCH: 47 train Results: Prec@1 61.462 Loss: 1.0796\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 0.9457 (0.9457)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1423 (1.2275)\tPrec@1 50.000 (56.560)\n",
      "EPOCH: 47 val Results: Prec@1 56.560 Loss: 1.2275\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/390]\tTime 0.010 (0.010)\tLoss 1.0413 (1.0413)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [48][78/390]\tTime 0.003 (0.003)\tLoss 1.0637 (1.0243)\tPrec@1 61.719 (63.548)\n",
      "Epoch: [48][156/390]\tTime 0.005 (0.003)\tLoss 1.1119 (1.0402)\tPrec@1 64.062 (62.938)\n",
      "Epoch: [48][234/390]\tTime 0.004 (0.003)\tLoss 0.9855 (1.0504)\tPrec@1 66.406 (62.387)\n",
      "Epoch: [48][312/390]\tTime 0.002 (0.003)\tLoss 1.0270 (1.0639)\tPrec@1 60.156 (61.938)\n",
      "Epoch: [48][390/390]\tTime 0.003 (0.003)\tLoss 1.3480 (1.0775)\tPrec@1 55.000 (61.432)\n",
      "EPOCH: 48 train Results: Prec@1 61.432 Loss: 1.0775\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1088 (1.1088)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2407 (1.2477)\tPrec@1 37.500 (55.310)\n",
      "EPOCH: 48 val Results: Prec@1 55.310 Loss: 1.2477\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/390]\tTime 0.002 (0.002)\tLoss 0.9801 (0.9801)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [49][78/390]\tTime 0.002 (0.003)\tLoss 1.1149 (1.0033)\tPrec@1 65.625 (65.081)\n",
      "Epoch: [49][156/390]\tTime 0.010 (0.003)\tLoss 1.1125 (1.0324)\tPrec@1 58.594 (63.699)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.003)\tLoss 1.0695 (1.0503)\tPrec@1 59.375 (62.743)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.1795 (1.0629)\tPrec@1 58.594 (62.250)\n",
      "Epoch: [49][390/390]\tTime 0.002 (0.003)\tLoss 1.0962 (1.0766)\tPrec@1 61.250 (61.776)\n",
      "EPOCH: 49 train Results: Prec@1 61.776 Loss: 1.0766\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1238 (1.1238)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0022 (1.2289)\tPrec@1 50.000 (56.190)\n",
      "EPOCH: 49 val Results: Prec@1 56.190 Loss: 1.2289\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/390]\tTime 0.002 (0.002)\tLoss 0.9661 (0.9661)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [50][78/390]\tTime 0.003 (0.003)\tLoss 1.0167 (1.0110)\tPrec@1 62.500 (64.290)\n",
      "Epoch: [50][156/390]\tTime 0.002 (0.003)\tLoss 1.0452 (1.0311)\tPrec@1 57.031 (63.097)\n",
      "Epoch: [50][234/390]\tTime 0.003 (0.003)\tLoss 1.1962 (1.0550)\tPrec@1 54.688 (62.360)\n",
      "Epoch: [50][312/390]\tTime 0.002 (0.003)\tLoss 1.0505 (1.0665)\tPrec@1 59.375 (61.928)\n",
      "Epoch: [50][390/390]\tTime 0.003 (0.003)\tLoss 0.9876 (1.0771)\tPrec@1 65.000 (61.580)\n",
      "EPOCH: 50 train Results: Prec@1 61.580 Loss: 1.0771\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0859 (1.0859)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3232 (1.2388)\tPrec@1 31.250 (55.980)\n",
      "EPOCH: 50 val Results: Prec@1 55.980 Loss: 1.2388\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/390]\tTime 0.003 (0.003)\tLoss 1.1422 (1.1422)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [51][78/390]\tTime 0.002 (0.005)\tLoss 1.0377 (1.0164)\tPrec@1 66.406 (63.865)\n",
      "Epoch: [51][156/390]\tTime 0.003 (0.004)\tLoss 0.9774 (1.0377)\tPrec@1 67.969 (63.112)\n",
      "Epoch: [51][234/390]\tTime 0.005 (0.004)\tLoss 1.2104 (1.0526)\tPrec@1 55.469 (62.477)\n",
      "Epoch: [51][312/390]\tTime 0.002 (0.004)\tLoss 0.9558 (1.0638)\tPrec@1 67.188 (62.123)\n",
      "Epoch: [51][390/390]\tTime 0.001 (0.004)\tLoss 1.0942 (1.0755)\tPrec@1 61.250 (61.696)\n",
      "EPOCH: 51 train Results: Prec@1 61.696 Loss: 1.0755\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1376 (1.1376)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1632 (1.2343)\tPrec@1 56.250 (57.080)\n",
      "EPOCH: 51 val Results: Prec@1 57.080 Loss: 1.2343\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/390]\tTime 0.003 (0.003)\tLoss 0.9675 (0.9675)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [52][78/390]\tTime 0.004 (0.003)\tLoss 1.0334 (1.0123)\tPrec@1 60.938 (64.260)\n",
      "Epoch: [52][156/390]\tTime 0.003 (0.003)\tLoss 1.0596 (1.0256)\tPrec@1 59.375 (63.659)\n",
      "Epoch: [52][234/390]\tTime 0.003 (0.003)\tLoss 0.9318 (1.0468)\tPrec@1 70.312 (62.689)\n",
      "Epoch: [52][312/390]\tTime 0.002 (0.003)\tLoss 1.2748 (1.0629)\tPrec@1 57.812 (62.098)\n",
      "Epoch: [52][390/390]\tTime 0.002 (0.003)\tLoss 1.3121 (1.0773)\tPrec@1 52.500 (61.684)\n",
      "EPOCH: 52 train Results: Prec@1 61.684 Loss: 1.0773\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0846 (1.0846)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1441 (1.2409)\tPrec@1 50.000 (55.960)\n",
      "EPOCH: 52 val Results: Prec@1 55.960 Loss: 1.2409\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/390]\tTime 0.006 (0.006)\tLoss 1.0499 (1.0499)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [53][78/390]\tTime 0.005 (0.003)\tLoss 0.9750 (1.0093)\tPrec@1 63.281 (64.419)\n",
      "Epoch: [53][156/390]\tTime 0.003 (0.003)\tLoss 1.0038 (1.0230)\tPrec@1 63.281 (63.868)\n",
      "Epoch: [53][234/390]\tTime 0.002 (0.003)\tLoss 1.0473 (1.0411)\tPrec@1 57.812 (63.098)\n",
      "Epoch: [53][312/390]\tTime 0.002 (0.003)\tLoss 1.4865 (1.0589)\tPrec@1 43.750 (62.380)\n",
      "Epoch: [53][390/390]\tTime 0.003 (0.003)\tLoss 1.0523 (1.0695)\tPrec@1 70.000 (62.024)\n",
      "EPOCH: 53 train Results: Prec@1 62.024 Loss: 1.0695\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0960 (1.0960)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4281 (1.2446)\tPrec@1 37.500 (55.780)\n",
      "EPOCH: 53 val Results: Prec@1 55.780 Loss: 1.2446\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/390]\tTime 0.006 (0.006)\tLoss 0.9632 (0.9632)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [54][78/390]\tTime 0.005 (0.003)\tLoss 1.0023 (0.9966)\tPrec@1 61.719 (65.111)\n",
      "Epoch: [54][156/390]\tTime 0.002 (0.003)\tLoss 0.9930 (1.0211)\tPrec@1 64.844 (63.948)\n",
      "Epoch: [54][234/390]\tTime 0.003 (0.003)\tLoss 1.0908 (1.0434)\tPrec@1 60.938 (63.092)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2466 (1.0603)\tPrec@1 58.594 (62.368)\n",
      "Epoch: [54][390/390]\tTime 0.001 (0.003)\tLoss 1.2099 (1.0752)\tPrec@1 53.750 (61.856)\n",
      "EPOCH: 54 train Results: Prec@1 61.856 Loss: 1.0752\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0638 (1.0638)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9847 (1.2517)\tPrec@1 50.000 (55.380)\n",
      "EPOCH: 54 val Results: Prec@1 55.380 Loss: 1.2517\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/390]\tTime 0.007 (0.007)\tLoss 1.0683 (1.0683)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.0507 (0.9965)\tPrec@1 68.750 (64.933)\n",
      "Epoch: [55][156/390]\tTime 0.004 (0.003)\tLoss 1.1328 (1.0263)\tPrec@1 57.031 (63.436)\n",
      "Epoch: [55][234/390]\tTime 0.002 (0.003)\tLoss 1.2275 (1.0462)\tPrec@1 60.156 (62.653)\n",
      "Epoch: [55][312/390]\tTime 0.002 (0.003)\tLoss 0.9427 (1.0572)\tPrec@1 66.406 (62.200)\n",
      "Epoch: [55][390/390]\tTime 0.002 (0.003)\tLoss 1.1144 (1.0700)\tPrec@1 61.250 (61.828)\n",
      "EPOCH: 55 train Results: Prec@1 61.828 Loss: 1.0700\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0683 (1.0683)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2787 (1.2460)\tPrec@1 43.750 (55.740)\n",
      "EPOCH: 55 val Results: Prec@1 55.740 Loss: 1.2460\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/390]\tTime 0.003 (0.003)\tLoss 0.9113 (0.9113)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [56][78/390]\tTime 0.002 (0.003)\tLoss 1.0051 (1.0181)\tPrec@1 63.281 (64.033)\n",
      "Epoch: [56][156/390]\tTime 0.003 (0.004)\tLoss 0.9790 (1.0301)\tPrec@1 66.406 (63.236)\n",
      "Epoch: [56][234/390]\tTime 0.003 (0.004)\tLoss 1.0947 (1.0430)\tPrec@1 59.375 (62.819)\n",
      "Epoch: [56][312/390]\tTime 0.002 (0.004)\tLoss 1.0068 (1.0574)\tPrec@1 64.062 (62.323)\n",
      "Epoch: [56][390/390]\tTime 0.002 (0.003)\tLoss 1.2188 (1.0703)\tPrec@1 58.750 (61.924)\n",
      "EPOCH: 56 train Results: Prec@1 61.924 Loss: 1.0703\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0349 (1.0349)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0674 (1.2510)\tPrec@1 56.250 (55.270)\n",
      "EPOCH: 56 val Results: Prec@1 55.270 Loss: 1.2510\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/390]\tTime 0.002 (0.002)\tLoss 1.0261 (1.0261)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [57][78/390]\tTime 0.002 (0.003)\tLoss 1.0285 (0.9987)\tPrec@1 64.844 (64.537)\n",
      "Epoch: [57][156/390]\tTime 0.002 (0.003)\tLoss 0.9721 (1.0147)\tPrec@1 67.188 (63.858)\n",
      "Epoch: [57][234/390]\tTime 0.002 (0.003)\tLoss 1.0475 (1.0347)\tPrec@1 60.156 (63.032)\n",
      "Epoch: [57][312/390]\tTime 0.013 (0.003)\tLoss 1.1036 (1.0514)\tPrec@1 60.156 (62.285)\n",
      "Epoch: [57][390/390]\tTime 0.003 (0.003)\tLoss 1.1143 (1.0674)\tPrec@1 58.750 (61.780)\n",
      "EPOCH: 57 train Results: Prec@1 61.780 Loss: 1.0674\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1661 (1.1661)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2095 (1.2447)\tPrec@1 37.500 (56.100)\n",
      "EPOCH: 57 val Results: Prec@1 56.100 Loss: 1.2447\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/390]\tTime 0.004 (0.004)\tLoss 0.9584 (0.9584)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [58][78/390]\tTime 0.002 (0.003)\tLoss 1.0581 (0.9995)\tPrec@1 67.188 (64.330)\n",
      "Epoch: [58][156/390]\tTime 0.007 (0.003)\tLoss 1.0664 (1.0252)\tPrec@1 57.812 (63.530)\n",
      "Epoch: [58][234/390]\tTime 0.002 (0.003)\tLoss 0.9978 (1.0422)\tPrec@1 60.938 (62.896)\n",
      "Epoch: [58][312/390]\tTime 0.002 (0.003)\tLoss 1.0153 (1.0561)\tPrec@1 64.062 (62.572)\n",
      "Epoch: [58][390/390]\tTime 0.002 (0.003)\tLoss 1.2186 (1.0687)\tPrec@1 57.500 (61.948)\n",
      "EPOCH: 58 train Results: Prec@1 61.948 Loss: 1.0687\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0766 (1.0766)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2192 (1.2240)\tPrec@1 56.250 (56.560)\n",
      "EPOCH: 58 val Results: Prec@1 56.560 Loss: 1.2240\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/390]\tTime 0.005 (0.005)\tLoss 1.0632 (1.0632)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [59][78/390]\tTime 0.005 (0.003)\tLoss 1.0865 (1.0017)\tPrec@1 60.938 (64.458)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.1633 (1.0186)\tPrec@1 61.719 (63.679)\n",
      "Epoch: [59][234/390]\tTime 0.004 (0.003)\tLoss 1.0424 (1.0427)\tPrec@1 62.500 (62.866)\n",
      "Epoch: [59][312/390]\tTime 0.003 (0.003)\tLoss 1.0632 (1.0563)\tPrec@1 63.281 (62.335)\n",
      "Epoch: [59][390/390]\tTime 0.005 (0.003)\tLoss 1.0799 (1.0675)\tPrec@1 66.250 (61.976)\n",
      "EPOCH: 59 train Results: Prec@1 61.976 Loss: 1.0675\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1553 (1.1553)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9773 (1.2481)\tPrec@1 56.250 (55.620)\n",
      "EPOCH: 59 val Results: Prec@1 55.620 Loss: 1.2481\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/390]\tTime 0.002 (0.002)\tLoss 0.8647 (0.8647)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [60][78/390]\tTime 0.002 (0.003)\tLoss 1.0055 (1.0043)\tPrec@1 60.938 (64.775)\n",
      "Epoch: [60][156/390]\tTime 0.002 (0.003)\tLoss 0.9821 (1.0252)\tPrec@1 68.750 (64.053)\n",
      "Epoch: [60][234/390]\tTime 0.002 (0.003)\tLoss 1.0309 (1.0450)\tPrec@1 58.594 (62.979)\n",
      "Epoch: [60][312/390]\tTime 0.002 (0.003)\tLoss 1.0896 (1.0616)\tPrec@1 60.938 (62.213)\n",
      "Epoch: [60][390/390]\tTime 0.002 (0.003)\tLoss 1.1470 (1.0712)\tPrec@1 61.250 (61.962)\n",
      "EPOCH: 60 train Results: Prec@1 61.962 Loss: 1.0712\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1229 (1.1229)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9329 (1.2547)\tPrec@1 56.250 (55.160)\n",
      "EPOCH: 60 val Results: Prec@1 55.160 Loss: 1.2547\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/390]\tTime 0.004 (0.004)\tLoss 0.9310 (0.9310)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [61][78/390]\tTime 0.004 (0.003)\tLoss 1.0692 (0.9968)\tPrec@1 60.156 (64.597)\n",
      "Epoch: [61][156/390]\tTime 0.004 (0.003)\tLoss 1.1953 (1.0219)\tPrec@1 58.594 (63.749)\n",
      "Epoch: [61][234/390]\tTime 0.002 (0.003)\tLoss 1.1054 (1.0386)\tPrec@1 59.375 (62.919)\n",
      "Epoch: [61][312/390]\tTime 0.002 (0.003)\tLoss 1.1067 (1.0536)\tPrec@1 65.625 (62.410)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.003)\tLoss 1.0956 (1.0650)\tPrec@1 65.000 (62.032)\n",
      "EPOCH: 61 train Results: Prec@1 62.032 Loss: 1.0650\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0990 (1.0990)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2317 (1.2480)\tPrec@1 43.750 (55.400)\n",
      "EPOCH: 61 val Results: Prec@1 55.400 Loss: 1.2480\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/390]\tTime 0.003 (0.003)\tLoss 0.9615 (0.9615)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.1075 (1.0019)\tPrec@1 57.812 (64.320)\n",
      "Epoch: [62][156/390]\tTime 0.002 (0.004)\tLoss 0.9611 (1.0216)\tPrec@1 70.312 (63.470)\n",
      "Epoch: [62][234/390]\tTime 0.004 (0.003)\tLoss 1.1981 (1.0439)\tPrec@1 58.594 (62.726)\n",
      "Epoch: [62][312/390]\tTime 0.002 (0.003)\tLoss 1.1427 (1.0557)\tPrec@1 62.500 (62.278)\n",
      "Epoch: [62][390/390]\tTime 0.002 (0.003)\tLoss 0.8561 (1.0664)\tPrec@1 71.250 (61.920)\n",
      "EPOCH: 62 train Results: Prec@1 61.920 Loss: 1.0664\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1448 (1.1448)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1346 (1.2385)\tPrec@1 37.500 (56.200)\n",
      "EPOCH: 62 val Results: Prec@1 56.200 Loss: 1.2385\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/390]\tTime 0.002 (0.002)\tLoss 1.0115 (1.0115)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [63][78/390]\tTime 0.002 (0.004)\tLoss 1.0446 (0.9993)\tPrec@1 61.719 (64.310)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.003)\tLoss 0.9818 (1.0238)\tPrec@1 65.625 (63.421)\n",
      "Epoch: [63][234/390]\tTime 0.002 (0.003)\tLoss 1.1920 (1.0435)\tPrec@1 59.375 (62.945)\n",
      "Epoch: [63][312/390]\tTime 0.002 (0.003)\tLoss 1.2560 (1.0597)\tPrec@1 53.906 (62.418)\n",
      "Epoch: [63][390/390]\tTime 0.003 (0.003)\tLoss 1.1675 (1.0670)\tPrec@1 60.000 (62.110)\n",
      "EPOCH: 63 train Results: Prec@1 62.110 Loss: 1.0670\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1781 (1.1781)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9917 (1.2371)\tPrec@1 43.750 (56.150)\n",
      "EPOCH: 63 val Results: Prec@1 56.150 Loss: 1.2371\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/390]\tTime 0.004 (0.004)\tLoss 0.8951 (0.8951)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [64][78/390]\tTime 0.003 (0.003)\tLoss 1.0898 (0.9904)\tPrec@1 61.719 (64.656)\n",
      "Epoch: [64][156/390]\tTime 0.002 (0.003)\tLoss 1.1889 (1.0202)\tPrec@1 55.469 (63.854)\n",
      "Epoch: [64][234/390]\tTime 0.012 (0.003)\tLoss 1.1720 (1.0371)\tPrec@1 57.812 (63.088)\n",
      "Epoch: [64][312/390]\tTime 0.002 (0.003)\tLoss 1.1169 (1.0526)\tPrec@1 59.375 (62.468)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.003)\tLoss 1.1646 (1.0668)\tPrec@1 61.250 (62.112)\n",
      "EPOCH: 64 train Results: Prec@1 62.112 Loss: 1.0668\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1184 (1.1184)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8996 (1.2500)\tPrec@1 68.750 (55.540)\n",
      "EPOCH: 64 val Results: Prec@1 55.540 Loss: 1.2500\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/390]\tTime 0.002 (0.002)\tLoss 0.9510 (0.9510)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [65][78/390]\tTime 0.004 (0.003)\tLoss 1.1174 (0.9966)\tPrec@1 60.938 (64.488)\n",
      "Epoch: [65][156/390]\tTime 0.002 (0.003)\tLoss 0.9726 (1.0168)\tPrec@1 68.750 (63.943)\n",
      "Epoch: [65][234/390]\tTime 0.002 (0.003)\tLoss 1.0382 (1.0373)\tPrec@1 62.500 (63.112)\n",
      "Epoch: [65][312/390]\tTime 0.004 (0.003)\tLoss 1.1794 (1.0537)\tPrec@1 58.594 (62.567)\n",
      "Epoch: [65][390/390]\tTime 0.001 (0.003)\tLoss 1.0932 (1.0663)\tPrec@1 56.250 (62.114)\n",
      "EPOCH: 65 train Results: Prec@1 62.114 Loss: 1.0663\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1215 (1.1215)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1999 (1.2359)\tPrec@1 56.250 (55.890)\n",
      "EPOCH: 65 val Results: Prec@1 55.890 Loss: 1.2359\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/390]\tTime 0.004 (0.004)\tLoss 0.9631 (0.9631)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [66][78/390]\tTime 0.002 (0.003)\tLoss 1.1053 (0.9818)\tPrec@1 54.688 (64.992)\n",
      "Epoch: [66][156/390]\tTime 0.002 (0.003)\tLoss 1.1588 (1.0144)\tPrec@1 57.812 (63.505)\n",
      "Epoch: [66][234/390]\tTime 0.003 (0.003)\tLoss 1.1047 (1.0327)\tPrec@1 63.281 (63.075)\n",
      "Epoch: [66][312/390]\tTime 0.002 (0.003)\tLoss 1.1727 (1.0542)\tPrec@1 59.375 (62.373)\n",
      "Epoch: [66][390/390]\tTime 0.003 (0.003)\tLoss 1.1304 (1.0648)\tPrec@1 58.750 (62.138)\n",
      "EPOCH: 66 train Results: Prec@1 62.138 Loss: 1.0648\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0908 (1.0908)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1754 (1.2458)\tPrec@1 56.250 (55.910)\n",
      "EPOCH: 66 val Results: Prec@1 55.910 Loss: 1.2458\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/390]\tTime 0.003 (0.003)\tLoss 0.9933 (0.9933)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [67][78/390]\tTime 0.002 (0.003)\tLoss 0.9975 (0.9930)\tPrec@1 65.625 (64.765)\n",
      "Epoch: [67][156/390]\tTime 0.007 (0.003)\tLoss 1.2347 (1.0141)\tPrec@1 54.688 (64.087)\n",
      "Epoch: [67][234/390]\tTime 0.002 (0.003)\tLoss 1.0290 (1.0364)\tPrec@1 60.156 (63.285)\n",
      "Epoch: [67][312/390]\tTime 0.007 (0.003)\tLoss 1.0252 (1.0485)\tPrec@1 66.406 (62.800)\n",
      "Epoch: [67][390/390]\tTime 0.004 (0.003)\tLoss 1.1149 (1.0631)\tPrec@1 57.500 (62.190)\n",
      "EPOCH: 67 train Results: Prec@1 62.190 Loss: 1.0631\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0907 (1.0907)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8853 (1.2416)\tPrec@1 56.250 (55.520)\n",
      "EPOCH: 67 val Results: Prec@1 55.520 Loss: 1.2416\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/390]\tTime 0.003 (0.003)\tLoss 0.9180 (0.9180)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [68][78/390]\tTime 0.002 (0.003)\tLoss 0.9186 (0.9878)\tPrec@1 67.969 (65.002)\n",
      "Epoch: [68][156/390]\tTime 0.002 (0.003)\tLoss 1.1380 (1.0260)\tPrec@1 63.281 (63.555)\n",
      "Epoch: [68][234/390]\tTime 0.002 (0.003)\tLoss 1.0983 (1.0414)\tPrec@1 61.719 (62.932)\n",
      "Epoch: [68][312/390]\tTime 0.002 (0.003)\tLoss 1.2538 (1.0535)\tPrec@1 56.250 (62.617)\n",
      "Epoch: [68][390/390]\tTime 0.003 (0.003)\tLoss 1.3653 (1.0636)\tPrec@1 55.000 (62.248)\n",
      "EPOCH: 68 train Results: Prec@1 62.248 Loss: 1.0636\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0771 (1.0771)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0894 (1.2453)\tPrec@1 62.500 (55.830)\n",
      "EPOCH: 68 val Results: Prec@1 55.830 Loss: 1.2453\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/390]\tTime 0.006 (0.006)\tLoss 1.0494 (1.0494)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.003)\tLoss 0.9851 (0.9678)\tPrec@1 66.406 (65.645)\n",
      "Epoch: [69][156/390]\tTime 0.003 (0.004)\tLoss 1.0701 (1.0097)\tPrec@1 65.625 (64.137)\n",
      "Epoch: [69][234/390]\tTime 0.004 (0.003)\tLoss 0.9368 (1.0330)\tPrec@1 61.719 (63.191)\n",
      "Epoch: [69][312/390]\tTime 0.006 (0.003)\tLoss 1.0984 (1.0491)\tPrec@1 62.500 (62.413)\n",
      "Epoch: [69][390/390]\tTime 0.007 (0.003)\tLoss 1.0791 (1.0627)\tPrec@1 62.500 (61.974)\n",
      "EPOCH: 69 train Results: Prec@1 61.974 Loss: 1.0627\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0781 (1.0781)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1498 (1.2327)\tPrec@1 56.250 (56.460)\n",
      "EPOCH: 69 val Results: Prec@1 56.460 Loss: 1.2327\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/390]\tTime 0.002 (0.002)\tLoss 0.8643 (0.8643)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [70][78/390]\tTime 0.002 (0.003)\tLoss 1.1463 (0.9966)\tPrec@1 56.250 (64.597)\n",
      "Epoch: [70][156/390]\tTime 0.002 (0.003)\tLoss 1.0156 (1.0151)\tPrec@1 61.719 (63.893)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.004)\tLoss 0.9795 (1.0342)\tPrec@1 62.500 (63.195)\n",
      "Epoch: [70][312/390]\tTime 0.002 (0.004)\tLoss 0.9757 (1.0557)\tPrec@1 64.062 (62.562)\n",
      "Epoch: [70][390/390]\tTime 0.001 (0.004)\tLoss 1.0775 (1.0636)\tPrec@1 60.000 (62.214)\n",
      "EPOCH: 70 train Results: Prec@1 62.214 Loss: 1.0636\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0963 (1.0963)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2400 (1.2473)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 70 val Results: Prec@1 55.370 Loss: 1.2473\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/390]\tTime 0.007 (0.007)\tLoss 0.8830 (0.8830)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [71][78/390]\tTime 0.003 (0.003)\tLoss 1.0149 (0.9896)\tPrec@1 60.156 (65.190)\n",
      "Epoch: [71][156/390]\tTime 0.003 (0.003)\tLoss 1.0093 (1.0203)\tPrec@1 60.938 (63.814)\n",
      "Epoch: [71][234/390]\tTime 0.026 (0.004)\tLoss 0.9486 (1.0403)\tPrec@1 67.969 (62.972)\n",
      "Epoch: [71][312/390]\tTime 0.002 (0.003)\tLoss 0.9758 (1.0526)\tPrec@1 61.719 (62.443)\n",
      "Epoch: [71][390/390]\tTime 0.001 (0.003)\tLoss 0.8306 (1.0665)\tPrec@1 73.750 (61.888)\n",
      "EPOCH: 71 train Results: Prec@1 61.888 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0804 (1.0804)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0180 (1.2509)\tPrec@1 62.500 (55.770)\n",
      "EPOCH: 71 val Results: Prec@1 55.770 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/390]\tTime 0.004 (0.004)\tLoss 0.9683 (0.9683)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [72][78/390]\tTime 0.003 (0.003)\tLoss 1.0971 (0.9957)\tPrec@1 57.812 (65.111)\n",
      "Epoch: [72][156/390]\tTime 0.003 (0.003)\tLoss 0.9829 (1.0094)\tPrec@1 67.188 (64.356)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.003)\tLoss 1.0428 (1.0326)\tPrec@1 60.156 (63.188)\n",
      "Epoch: [72][312/390]\tTime 0.002 (0.003)\tLoss 1.2079 (1.0484)\tPrec@1 60.156 (62.607)\n",
      "Epoch: [72][390/390]\tTime 0.002 (0.003)\tLoss 0.9681 (1.0605)\tPrec@1 66.250 (62.256)\n",
      "EPOCH: 72 train Results: Prec@1 62.256 Loss: 1.0605\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0527 (1.0527)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1151 (1.2390)\tPrec@1 50.000 (55.620)\n",
      "EPOCH: 72 val Results: Prec@1 55.620 Loss: 1.2390\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/390]\tTime 0.004 (0.004)\tLoss 0.9666 (0.9666)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [73][78/390]\tTime 0.003 (0.003)\tLoss 0.9904 (1.0043)\tPrec@1 60.938 (64.409)\n",
      "Epoch: [73][156/390]\tTime 0.005 (0.003)\tLoss 1.1971 (1.0127)\tPrec@1 57.031 (63.699)\n",
      "Epoch: [73][234/390]\tTime 0.004 (0.003)\tLoss 0.9964 (1.0312)\tPrec@1 64.062 (63.112)\n",
      "Epoch: [73][312/390]\tTime 0.004 (0.004)\tLoss 0.9593 (1.0495)\tPrec@1 66.406 (62.455)\n",
      "Epoch: [73][390/390]\tTime 0.001 (0.004)\tLoss 1.0205 (1.0598)\tPrec@1 58.750 (62.196)\n",
      "EPOCH: 73 train Results: Prec@1 62.196 Loss: 1.0598\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0460 (1.0460)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9960 (1.2463)\tPrec@1 62.500 (56.190)\n",
      "EPOCH: 73 val Results: Prec@1 56.190 Loss: 1.2463\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/390]\tTime 0.003 (0.003)\tLoss 1.0569 (1.0569)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [74][78/390]\tTime 0.002 (0.003)\tLoss 0.9938 (0.9875)\tPrec@1 64.062 (65.328)\n",
      "Epoch: [74][156/390]\tTime 0.007 (0.004)\tLoss 1.1053 (1.0177)\tPrec@1 64.844 (64.072)\n",
      "Epoch: [74][234/390]\tTime 0.002 (0.004)\tLoss 0.9630 (1.0327)\tPrec@1 69.531 (63.617)\n",
      "Epoch: [74][312/390]\tTime 0.003 (0.004)\tLoss 1.0407 (1.0462)\tPrec@1 66.406 (62.974)\n",
      "Epoch: [74][390/390]\tTime 0.001 (0.003)\tLoss 1.1614 (1.0583)\tPrec@1 61.250 (62.544)\n",
      "EPOCH: 74 train Results: Prec@1 62.544 Loss: 1.0583\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1146 (1.1146)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2241 (1.2450)\tPrec@1 37.500 (55.550)\n",
      "EPOCH: 74 val Results: Prec@1 55.550 Loss: 1.2450\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/390]\tTime 0.004 (0.004)\tLoss 0.9704 (0.9704)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [75][78/390]\tTime 0.002 (0.004)\tLoss 1.1996 (0.9696)\tPrec@1 60.156 (65.813)\n",
      "Epoch: [75][156/390]\tTime 0.002 (0.003)\tLoss 0.8171 (1.0142)\tPrec@1 71.875 (63.993)\n",
      "Epoch: [75][234/390]\tTime 0.003 (0.003)\tLoss 1.2150 (1.0327)\tPrec@1 57.812 (63.348)\n",
      "Epoch: [75][312/390]\tTime 0.009 (0.003)\tLoss 1.2596 (1.0476)\tPrec@1 55.469 (62.785)\n",
      "Epoch: [75][390/390]\tTime 0.001 (0.003)\tLoss 1.0817 (1.0617)\tPrec@1 65.000 (62.334)\n",
      "EPOCH: 75 train Results: Prec@1 62.334 Loss: 1.0617\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0885 (1.0885)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2118 (1.2487)\tPrec@1 50.000 (55.540)\n",
      "EPOCH: 75 val Results: Prec@1 55.540 Loss: 1.2487\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/390]\tTime 0.005 (0.005)\tLoss 1.0850 (1.0850)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [76][78/390]\tTime 0.003 (0.003)\tLoss 1.0743 (0.9924)\tPrec@1 64.844 (64.705)\n",
      "Epoch: [76][156/390]\tTime 0.002 (0.003)\tLoss 0.9073 (1.0187)\tPrec@1 66.406 (63.515)\n",
      "Epoch: [76][234/390]\tTime 0.004 (0.003)\tLoss 1.0721 (1.0377)\tPrec@1 64.844 (62.856)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 0.9264 (1.0502)\tPrec@1 66.406 (62.522)\n",
      "Epoch: [76][390/390]\tTime 0.001 (0.003)\tLoss 0.8953 (1.0582)\tPrec@1 71.250 (62.310)\n",
      "EPOCH: 76 train Results: Prec@1 62.310 Loss: 1.0582\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1226 (1.1226)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3837 (1.2391)\tPrec@1 43.750 (56.110)\n",
      "EPOCH: 76 val Results: Prec@1 56.110 Loss: 1.2391\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/390]\tTime 0.003 (0.003)\tLoss 1.0682 (1.0682)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [77][78/390]\tTime 0.008 (0.003)\tLoss 1.1511 (0.9934)\tPrec@1 62.500 (65.042)\n",
      "Epoch: [77][156/390]\tTime 0.002 (0.003)\tLoss 1.0579 (1.0165)\tPrec@1 58.594 (63.824)\n",
      "Epoch: [77][234/390]\tTime 0.002 (0.003)\tLoss 0.9579 (1.0358)\tPrec@1 67.188 (63.075)\n",
      "Epoch: [77][312/390]\tTime 0.004 (0.003)\tLoss 1.1555 (1.0471)\tPrec@1 56.250 (62.562)\n",
      "Epoch: [77][390/390]\tTime 0.005 (0.003)\tLoss 1.2501 (1.0592)\tPrec@1 58.750 (62.104)\n",
      "EPOCH: 77 train Results: Prec@1 62.104 Loss: 1.0592\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0743 (1.0743)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3370 (1.2435)\tPrec@1 37.500 (56.360)\n",
      "EPOCH: 77 val Results: Prec@1 56.360 Loss: 1.2435\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/390]\tTime 0.002 (0.002)\tLoss 0.9345 (0.9345)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [78][78/390]\tTime 0.002 (0.003)\tLoss 1.0865 (0.9747)\tPrec@1 63.281 (65.951)\n",
      "Epoch: [78][156/390]\tTime 0.002 (0.003)\tLoss 1.0512 (1.0067)\tPrec@1 57.812 (64.291)\n",
      "Epoch: [78][234/390]\tTime 0.003 (0.003)\tLoss 1.0499 (1.0307)\tPrec@1 58.594 (63.414)\n",
      "Epoch: [78][312/390]\tTime 0.003 (0.003)\tLoss 1.0263 (1.0494)\tPrec@1 63.281 (62.725)\n",
      "Epoch: [78][390/390]\tTime 0.003 (0.003)\tLoss 0.9560 (1.0601)\tPrec@1 65.000 (62.236)\n",
      "EPOCH: 78 train Results: Prec@1 62.236 Loss: 1.0601\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0642 (1.0642)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1562 (1.2264)\tPrec@1 50.000 (56.410)\n",
      "EPOCH: 78 val Results: Prec@1 56.410 Loss: 1.2264\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/390]\tTime 0.002 (0.002)\tLoss 0.9350 (0.9350)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [79][78/390]\tTime 0.003 (0.004)\tLoss 0.9212 (0.9652)\tPrec@1 66.406 (65.536)\n",
      "Epoch: [79][156/390]\tTime 0.003 (0.004)\tLoss 1.0073 (0.9898)\tPrec@1 63.281 (64.779)\n",
      "Epoch: [79][234/390]\tTime 0.002 (0.004)\tLoss 1.0801 (1.0174)\tPrec@1 57.812 (63.684)\n",
      "Epoch: [79][312/390]\tTime 0.002 (0.003)\tLoss 1.1711 (1.0402)\tPrec@1 61.719 (62.807)\n",
      "Epoch: [79][390/390]\tTime 0.003 (0.003)\tLoss 1.0613 (1.0582)\tPrec@1 62.500 (62.166)\n",
      "EPOCH: 79 train Results: Prec@1 62.166 Loss: 1.0582\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1012 (1.1012)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0789 (1.2368)\tPrec@1 50.000 (56.130)\n",
      "EPOCH: 79 val Results: Prec@1 56.130 Loss: 1.2368\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/390]\tTime 0.004 (0.004)\tLoss 1.0653 (1.0653)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [80][78/390]\tTime 0.003 (0.003)\tLoss 0.9917 (0.9768)\tPrec@1 67.969 (65.338)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.0682 (1.0098)\tPrec@1 64.844 (64.048)\n",
      "Epoch: [80][234/390]\tTime 0.050 (0.003)\tLoss 0.9361 (1.0345)\tPrec@1 71.875 (63.191)\n",
      "Epoch: [80][312/390]\tTime 0.003 (0.003)\tLoss 1.1971 (1.0509)\tPrec@1 55.469 (62.662)\n",
      "Epoch: [80][390/390]\tTime 0.004 (0.003)\tLoss 0.9871 (1.0588)\tPrec@1 63.750 (62.376)\n",
      "EPOCH: 80 train Results: Prec@1 62.376 Loss: 1.0588\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0967 (1.0967)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3533 (1.2382)\tPrec@1 37.500 (56.360)\n",
      "EPOCH: 80 val Results: Prec@1 56.360 Loss: 1.2382\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/390]\tTime 0.004 (0.004)\tLoss 1.0092 (1.0092)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [81][78/390]\tTime 0.005 (0.004)\tLoss 0.9845 (0.9870)\tPrec@1 65.625 (64.893)\n",
      "Epoch: [81][156/390]\tTime 0.004 (0.003)\tLoss 0.9456 (1.0194)\tPrec@1 65.625 (63.878)\n",
      "Epoch: [81][234/390]\tTime 0.009 (0.003)\tLoss 0.9140 (1.0297)\tPrec@1 67.969 (63.381)\n",
      "Epoch: [81][312/390]\tTime 0.008 (0.003)\tLoss 1.3317 (1.0470)\tPrec@1 53.906 (62.819)\n",
      "Epoch: [81][390/390]\tTime 0.001 (0.003)\tLoss 0.9416 (1.0591)\tPrec@1 65.000 (62.296)\n",
      "EPOCH: 81 train Results: Prec@1 62.296 Loss: 1.0591\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0528 (1.0528)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2105 (1.2388)\tPrec@1 50.000 (56.070)\n",
      "EPOCH: 81 val Results: Prec@1 56.070 Loss: 1.2388\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/390]\tTime 0.002 (0.002)\tLoss 0.9328 (0.9328)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [82][78/390]\tTime 0.002 (0.003)\tLoss 0.8914 (0.9804)\tPrec@1 68.750 (64.972)\n",
      "Epoch: [82][156/390]\tTime 0.002 (0.003)\tLoss 1.1088 (1.0058)\tPrec@1 58.594 (64.033)\n",
      "Epoch: [82][234/390]\tTime 0.007 (0.003)\tLoss 1.0809 (1.0249)\tPrec@1 54.688 (63.477)\n",
      "Epoch: [82][312/390]\tTime 0.002 (0.003)\tLoss 0.9501 (1.0394)\tPrec@1 67.188 (63.114)\n",
      "Epoch: [82][390/390]\tTime 0.002 (0.003)\tLoss 1.1346 (1.0595)\tPrec@1 65.000 (62.384)\n",
      "EPOCH: 82 train Results: Prec@1 62.384 Loss: 1.0595\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0937 (1.0937)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0692 (1.2347)\tPrec@1 50.000 (55.730)\n",
      "EPOCH: 82 val Results: Prec@1 55.730 Loss: 1.2347\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/390]\tTime 0.004 (0.004)\tLoss 0.8932 (0.8932)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [83][78/390]\tTime 0.002 (0.003)\tLoss 1.1726 (0.9789)\tPrec@1 60.156 (65.398)\n",
      "Epoch: [83][156/390]\tTime 0.003 (0.003)\tLoss 1.2147 (1.0056)\tPrec@1 54.688 (64.122)\n",
      "Epoch: [83][234/390]\tTime 0.002 (0.003)\tLoss 1.0634 (1.0267)\tPrec@1 59.375 (63.338)\n",
      "Epoch: [83][312/390]\tTime 0.014 (0.003)\tLoss 1.1273 (1.0444)\tPrec@1 59.375 (62.790)\n",
      "Epoch: [83][390/390]\tTime 0.002 (0.003)\tLoss 1.2299 (1.0543)\tPrec@1 61.250 (62.356)\n",
      "EPOCH: 83 train Results: Prec@1 62.356 Loss: 1.0543\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0239 (1.0239)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1605 (1.2462)\tPrec@1 37.500 (55.440)\n",
      "EPOCH: 83 val Results: Prec@1 55.440 Loss: 1.2462\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/390]\tTime 0.005 (0.005)\tLoss 0.8401 (0.8401)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [84][78/390]\tTime 0.002 (0.003)\tLoss 0.9206 (0.9969)\tPrec@1 69.531 (64.715)\n",
      "Epoch: [84][156/390]\tTime 0.002 (0.003)\tLoss 1.1842 (1.0151)\tPrec@1 60.156 (63.649)\n",
      "Epoch: [84][234/390]\tTime 0.002 (0.003)\tLoss 0.9738 (1.0361)\tPrec@1 67.188 (63.098)\n",
      "Epoch: [84][312/390]\tTime 0.002 (0.003)\tLoss 1.0543 (1.0467)\tPrec@1 60.156 (62.627)\n",
      "Epoch: [84][390/390]\tTime 0.001 (0.003)\tLoss 1.3391 (1.0536)\tPrec@1 55.000 (62.484)\n",
      "EPOCH: 84 train Results: Prec@1 62.484 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0489 (1.0489)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9687 (1.2547)\tPrec@1 62.500 (55.430)\n",
      "EPOCH: 84 val Results: Prec@1 55.430 Loss: 1.2547\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/390]\tTime 0.006 (0.006)\tLoss 0.9323 (0.9323)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [85][78/390]\tTime 0.008 (0.003)\tLoss 0.8057 (0.9630)\tPrec@1 70.312 (65.635)\n",
      "Epoch: [85][156/390]\tTime 0.004 (0.003)\tLoss 0.7996 (1.0001)\tPrec@1 71.094 (64.157)\n",
      "Epoch: [85][234/390]\tTime 0.003 (0.003)\tLoss 1.2340 (1.0238)\tPrec@1 53.906 (63.255)\n",
      "Epoch: [85][312/390]\tTime 0.010 (0.003)\tLoss 1.0246 (1.0428)\tPrec@1 66.406 (62.580)\n",
      "Epoch: [85][390/390]\tTime 0.001 (0.003)\tLoss 1.0914 (1.0555)\tPrec@1 56.250 (62.134)\n",
      "EPOCH: 85 train Results: Prec@1 62.134 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.0969 (1.0969)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2887 (1.2496)\tPrec@1 31.250 (55.460)\n",
      "EPOCH: 85 val Results: Prec@1 55.460 Loss: 1.2496\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/390]\tTime 0.009 (0.009)\tLoss 0.8980 (0.8980)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [86][78/390]\tTime 0.003 (0.003)\tLoss 0.9312 (0.9929)\tPrec@1 67.969 (64.864)\n",
      "Epoch: [86][156/390]\tTime 0.002 (0.003)\tLoss 0.9313 (1.0079)\tPrec@1 66.406 (64.222)\n",
      "Epoch: [86][234/390]\tTime 0.003 (0.003)\tLoss 1.0326 (1.0247)\tPrec@1 65.625 (63.597)\n",
      "Epoch: [86][312/390]\tTime 0.002 (0.003)\tLoss 1.2417 (1.0376)\tPrec@1 57.812 (63.109)\n",
      "Epoch: [86][390/390]\tTime 0.002 (0.003)\tLoss 1.2067 (1.0521)\tPrec@1 57.500 (62.514)\n",
      "EPOCH: 86 train Results: Prec@1 62.514 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1562 (1.1562)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0967 (1.2604)\tPrec@1 43.750 (55.510)\n",
      "EPOCH: 86 val Results: Prec@1 55.510 Loss: 1.2604\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/390]\tTime 0.002 (0.002)\tLoss 1.0135 (1.0135)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [87][78/390]\tTime 0.002 (0.005)\tLoss 0.8647 (0.9788)\tPrec@1 69.531 (65.150)\n",
      "Epoch: [87][156/390]\tTime 0.003 (0.004)\tLoss 1.0418 (1.0100)\tPrec@1 59.375 (63.923)\n",
      "Epoch: [87][234/390]\tTime 0.002 (0.004)\tLoss 0.9866 (1.0246)\tPrec@1 68.750 (63.491)\n",
      "Epoch: [87][312/390]\tTime 0.009 (0.004)\tLoss 1.2780 (1.0428)\tPrec@1 55.469 (62.802)\n",
      "Epoch: [87][390/390]\tTime 0.001 (0.003)\tLoss 1.2523 (1.0552)\tPrec@1 55.000 (62.298)\n",
      "EPOCH: 87 train Results: Prec@1 62.298 Loss: 1.0552\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0651 (1.0651)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9280 (1.2507)\tPrec@1 81.250 (55.830)\n",
      "EPOCH: 87 val Results: Prec@1 55.830 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/390]\tTime 0.003 (0.003)\tLoss 0.9995 (0.9995)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [88][78/390]\tTime 0.002 (0.003)\tLoss 0.9000 (0.9654)\tPrec@1 61.719 (65.961)\n",
      "Epoch: [88][156/390]\tTime 0.003 (0.003)\tLoss 1.1129 (1.0053)\tPrec@1 58.594 (64.237)\n",
      "Epoch: [88][234/390]\tTime 0.004 (0.003)\tLoss 1.0010 (1.0249)\tPrec@1 64.062 (63.414)\n",
      "Epoch: [88][312/390]\tTime 0.002 (0.003)\tLoss 1.0291 (1.0408)\tPrec@1 60.938 (62.767)\n",
      "Epoch: [88][390/390]\tTime 0.001 (0.003)\tLoss 1.2130 (1.0526)\tPrec@1 53.750 (62.344)\n",
      "EPOCH: 88 train Results: Prec@1 62.344 Loss: 1.0526\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1739 (1.1739)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9819 (1.2612)\tPrec@1 56.250 (55.480)\n",
      "EPOCH: 88 val Results: Prec@1 55.480 Loss: 1.2612\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/390]\tTime 0.005 (0.005)\tLoss 0.8559 (0.8559)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [89][78/390]\tTime 0.003 (0.003)\tLoss 0.9956 (0.9863)\tPrec@1 67.969 (64.814)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1244 (1.0013)\tPrec@1 60.938 (64.003)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 1.0521 (1.0211)\tPrec@1 64.844 (63.338)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.1018 (1.0424)\tPrec@1 64.062 (62.742)\n",
      "Epoch: [89][390/390]\tTime 0.002 (0.003)\tLoss 1.2623 (1.0571)\tPrec@1 47.500 (62.278)\n",
      "EPOCH: 89 train Results: Prec@1 62.278 Loss: 1.0571\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0993 (1.0993)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5994 (1.2553)\tPrec@1 43.750 (55.680)\n",
      "EPOCH: 89 val Results: Prec@1 55.680 Loss: 1.2553\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/390]\tTime 0.010 (0.010)\tLoss 1.0333 (1.0333)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [90][78/390]\tTime 0.003 (0.003)\tLoss 1.0047 (0.9770)\tPrec@1 59.375 (65.269)\n",
      "Epoch: [90][156/390]\tTime 0.008 (0.003)\tLoss 1.1533 (1.0010)\tPrec@1 55.469 (64.545)\n",
      "Epoch: [90][234/390]\tTime 0.004 (0.004)\tLoss 1.1356 (1.0262)\tPrec@1 55.469 (63.467)\n",
      "Epoch: [90][312/390]\tTime 0.002 (0.004)\tLoss 1.1258 (1.0369)\tPrec@1 62.500 (63.136)\n",
      "Epoch: [90][390/390]\tTime 0.003 (0.004)\tLoss 0.9424 (1.0507)\tPrec@1 61.250 (62.734)\n",
      "EPOCH: 90 train Results: Prec@1 62.734 Loss: 1.0507\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0546 (1.0546)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4359 (1.2632)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 90 val Results: Prec@1 55.300 Loss: 1.2632\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/390]\tTime 0.006 (0.006)\tLoss 1.1339 (1.1339)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [91][78/390]\tTime 0.004 (0.003)\tLoss 1.0576 (0.9768)\tPrec@1 63.281 (65.833)\n",
      "Epoch: [91][156/390]\tTime 0.004 (0.003)\tLoss 1.1931 (1.0058)\tPrec@1 53.125 (64.306)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.003)\tLoss 1.1554 (1.0209)\tPrec@1 55.469 (63.893)\n",
      "Epoch: [91][312/390]\tTime 0.004 (0.003)\tLoss 1.1233 (1.0426)\tPrec@1 63.281 (63.186)\n",
      "Epoch: [91][390/390]\tTime 0.003 (0.003)\tLoss 1.1399 (1.0569)\tPrec@1 61.250 (62.574)\n",
      "EPOCH: 91 train Results: Prec@1 62.574 Loss: 1.0569\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0760 (1.0760)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1623 (1.2602)\tPrec@1 43.750 (55.310)\n",
      "EPOCH: 91 val Results: Prec@1 55.310 Loss: 1.2602\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/390]\tTime 0.005 (0.005)\tLoss 0.9916 (0.9916)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.004)\tLoss 0.9857 (0.9783)\tPrec@1 64.062 (65.051)\n",
      "Epoch: [92][156/390]\tTime 0.002 (0.003)\tLoss 1.1312 (1.0153)\tPrec@1 59.375 (63.878)\n",
      "Epoch: [92][234/390]\tTime 0.007 (0.003)\tLoss 1.0429 (1.0284)\tPrec@1 62.500 (63.338)\n",
      "Epoch: [92][312/390]\tTime 0.002 (0.003)\tLoss 1.1341 (1.0430)\tPrec@1 54.688 (62.834)\n",
      "Epoch: [92][390/390]\tTime 0.003 (0.003)\tLoss 1.0723 (1.0556)\tPrec@1 55.000 (62.358)\n",
      "EPOCH: 92 train Results: Prec@1 62.358 Loss: 1.0556\n",
      "Test: [0/78]\tTime 0.013 (0.013)\tLoss 1.1095 (1.1095)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2882 (1.2490)\tPrec@1 56.250 (55.770)\n",
      "EPOCH: 92 val Results: Prec@1 55.770 Loss: 1.2490\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 1.1153 (1.1153)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.003)\tLoss 0.9583 (0.9637)\tPrec@1 61.719 (66.011)\n",
      "Epoch: [93][156/390]\tTime 0.002 (0.003)\tLoss 1.0579 (0.9988)\tPrec@1 62.500 (64.485)\n",
      "Epoch: [93][234/390]\tTime 0.006 (0.003)\tLoss 1.0428 (1.0191)\tPrec@1 60.938 (63.800)\n",
      "Epoch: [93][312/390]\tTime 0.002 (0.004)\tLoss 1.0828 (1.0350)\tPrec@1 61.719 (63.151)\n",
      "Epoch: [93][390/390]\tTime 0.001 (0.003)\tLoss 1.0638 (1.0515)\tPrec@1 67.500 (62.572)\n",
      "EPOCH: 93 train Results: Prec@1 62.572 Loss: 1.0515\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1133 (1.1133)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0813 (1.2490)\tPrec@1 50.000 (56.050)\n",
      "EPOCH: 93 val Results: Prec@1 56.050 Loss: 1.2490\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/390]\tTime 0.007 (0.007)\tLoss 0.8406 (0.8406)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [94][78/390]\tTime 0.004 (0.004)\tLoss 0.9173 (0.9631)\tPrec@1 61.719 (66.119)\n",
      "Epoch: [94][156/390]\tTime 0.004 (0.003)\tLoss 1.0470 (0.9945)\tPrec@1 64.062 (64.804)\n",
      "Epoch: [94][234/390]\tTime 0.003 (0.003)\tLoss 0.9596 (1.0226)\tPrec@1 67.188 (63.680)\n",
      "Epoch: [94][312/390]\tTime 0.004 (0.003)\tLoss 1.1657 (1.0376)\tPrec@1 59.375 (63.174)\n",
      "Epoch: [94][390/390]\tTime 0.005 (0.003)\tLoss 1.3645 (1.0508)\tPrec@1 61.250 (62.662)\n",
      "EPOCH: 94 train Results: Prec@1 62.662 Loss: 1.0508\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1694 (1.1694)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2530 (1.2521)\tPrec@1 50.000 (55.590)\n",
      "EPOCH: 94 val Results: Prec@1 55.590 Loss: 1.2521\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/390]\tTime 0.002 (0.002)\tLoss 0.9955 (0.9955)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [95][78/390]\tTime 0.003 (0.003)\tLoss 1.0948 (0.9753)\tPrec@1 58.594 (65.615)\n",
      "Epoch: [95][156/390]\tTime 0.004 (0.003)\tLoss 0.9292 (1.0011)\tPrec@1 65.625 (64.570)\n",
      "Epoch: [95][234/390]\tTime 0.004 (0.003)\tLoss 1.0815 (1.0234)\tPrec@1 60.938 (63.780)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.003)\tLoss 1.0083 (1.0413)\tPrec@1 69.531 (63.034)\n",
      "Epoch: [95][390/390]\tTime 0.001 (0.003)\tLoss 1.2892 (1.0548)\tPrec@1 55.000 (62.504)\n",
      "EPOCH: 95 train Results: Prec@1 62.504 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0758 (1.0758)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2688 (1.2507)\tPrec@1 56.250 (56.000)\n",
      "EPOCH: 95 val Results: Prec@1 56.000 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/390]\tTime 0.004 (0.004)\tLoss 1.0427 (1.0427)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.003)\tLoss 0.9270 (0.9719)\tPrec@1 61.719 (65.645)\n",
      "Epoch: [96][156/390]\tTime 0.002 (0.003)\tLoss 0.9314 (1.0010)\tPrec@1 70.312 (64.391)\n",
      "Epoch: [96][234/390]\tTime 0.002 (0.003)\tLoss 0.9764 (1.0256)\tPrec@1 60.938 (63.497)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 1.0744 (1.0378)\tPrec@1 62.500 (63.047)\n",
      "Epoch: [96][390/390]\tTime 0.002 (0.003)\tLoss 1.1342 (1.0506)\tPrec@1 56.250 (62.570)\n",
      "EPOCH: 96 train Results: Prec@1 62.570 Loss: 1.0506\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1188 (1.1188)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1228 (1.2556)\tPrec@1 56.250 (55.160)\n",
      "EPOCH: 96 val Results: Prec@1 55.160 Loss: 1.2556\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/390]\tTime 0.003 (0.003)\tLoss 1.0088 (1.0088)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [97][78/390]\tTime 0.002 (0.003)\tLoss 0.9967 (0.9887)\tPrec@1 66.406 (64.547)\n",
      "Epoch: [97][156/390]\tTime 0.004 (0.003)\tLoss 1.1829 (1.0084)\tPrec@1 57.812 (63.893)\n",
      "Epoch: [97][234/390]\tTime 0.005 (0.003)\tLoss 1.2903 (1.0248)\tPrec@1 52.344 (63.251)\n",
      "Epoch: [97][312/390]\tTime 0.004 (0.003)\tLoss 1.0482 (1.0389)\tPrec@1 64.844 (62.837)\n",
      "Epoch: [97][390/390]\tTime 0.001 (0.003)\tLoss 1.0343 (1.0521)\tPrec@1 60.000 (62.310)\n",
      "EPOCH: 97 train Results: Prec@1 62.310 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1167 (1.1167)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0366 (1.2541)\tPrec@1 50.000 (55.810)\n",
      "EPOCH: 97 val Results: Prec@1 55.810 Loss: 1.2541\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/390]\tTime 0.002 (0.002)\tLoss 1.0014 (1.0014)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [98][78/390]\tTime 0.002 (0.004)\tLoss 0.8379 (0.9678)\tPrec@1 71.094 (65.506)\n",
      "Epoch: [98][156/390]\tTime 0.003 (0.004)\tLoss 1.2156 (0.9963)\tPrec@1 57.031 (64.704)\n",
      "Epoch: [98][234/390]\tTime 0.004 (0.003)\tLoss 0.9183 (1.0151)\tPrec@1 67.188 (63.873)\n",
      "Epoch: [98][312/390]\tTime 0.003 (0.004)\tLoss 1.2128 (1.0313)\tPrec@1 57.812 (63.256)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.004)\tLoss 1.0868 (1.0448)\tPrec@1 58.750 (62.808)\n",
      "EPOCH: 98 train Results: Prec@1 62.808 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1962 (1.1962)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4396 (1.2620)\tPrec@1 25.000 (55.340)\n",
      "EPOCH: 98 val Results: Prec@1 55.340 Loss: 1.2620\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/390]\tTime 0.003 (0.003)\tLoss 0.9243 (0.9243)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.003)\tLoss 0.8636 (0.9757)\tPrec@1 70.312 (65.674)\n",
      "Epoch: [99][156/390]\tTime 0.003 (0.003)\tLoss 1.0347 (1.0092)\tPrec@1 60.156 (64.197)\n",
      "Epoch: [99][234/390]\tTime 0.003 (0.003)\tLoss 1.2268 (1.0236)\tPrec@1 52.344 (63.710)\n",
      "Epoch: [99][312/390]\tTime 0.004 (0.003)\tLoss 1.1035 (1.0377)\tPrec@1 56.250 (63.069)\n",
      "Epoch: [99][390/390]\tTime 0.002 (0.003)\tLoss 1.0416 (1.0497)\tPrec@1 60.000 (62.538)\n",
      "EPOCH: 99 train Results: Prec@1 62.538 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1437 (1.1437)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4257 (1.2477)\tPrec@1 50.000 (55.610)\n",
      "EPOCH: 99 val Results: Prec@1 55.610 Loss: 1.2477\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/390]\tTime 0.002 (0.002)\tLoss 0.8875 (0.8875)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [100][78/390]\tTime 0.002 (0.003)\tLoss 1.0347 (0.9757)\tPrec@1 58.594 (64.666)\n",
      "Epoch: [100][156/390]\tTime 0.003 (0.003)\tLoss 1.0858 (1.0032)\tPrec@1 57.812 (64.003)\n",
      "Epoch: [100][234/390]\tTime 0.005 (0.003)\tLoss 1.2427 (1.0248)\tPrec@1 53.906 (63.291)\n",
      "Epoch: [100][312/390]\tTime 0.002 (0.003)\tLoss 1.0586 (1.0436)\tPrec@1 64.062 (62.527)\n",
      "Epoch: [100][390/390]\tTime 0.003 (0.003)\tLoss 1.1984 (1.0530)\tPrec@1 56.250 (62.242)\n",
      "EPOCH: 100 train Results: Prec@1 62.242 Loss: 1.0530\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1344 (1.1344)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2422 (1.2494)\tPrec@1 37.500 (56.000)\n",
      "EPOCH: 100 val Results: Prec@1 56.000 Loss: 1.2494\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/390]\tTime 0.002 (0.002)\tLoss 0.9254 (0.9254)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.004)\tLoss 1.0826 (0.9830)\tPrec@1 53.906 (65.140)\n",
      "Epoch: [101][156/390]\tTime 0.010 (0.003)\tLoss 1.1910 (1.0097)\tPrec@1 56.250 (63.988)\n",
      "Epoch: [101][234/390]\tTime 0.003 (0.003)\tLoss 1.0678 (1.0299)\tPrec@1 67.188 (63.275)\n",
      "Epoch: [101][312/390]\tTime 0.004 (0.003)\tLoss 1.0244 (1.0359)\tPrec@1 64.844 (62.992)\n",
      "Epoch: [101][390/390]\tTime 0.001 (0.004)\tLoss 1.1678 (1.0487)\tPrec@1 60.000 (62.598)\n",
      "EPOCH: 101 train Results: Prec@1 62.598 Loss: 1.0487\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1532 (1.1532)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3408 (1.2666)\tPrec@1 18.750 (54.950)\n",
      "EPOCH: 101 val Results: Prec@1 54.950 Loss: 1.2666\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/390]\tTime 0.004 (0.004)\tLoss 1.0317 (1.0317)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [102][78/390]\tTime 0.003 (0.004)\tLoss 0.9895 (0.9583)\tPrec@1 59.375 (66.080)\n",
      "Epoch: [102][156/390]\tTime 0.003 (0.003)\tLoss 1.0654 (0.9975)\tPrec@1 63.281 (64.645)\n",
      "Epoch: [102][234/390]\tTime 0.004 (0.003)\tLoss 1.0530 (1.0209)\tPrec@1 64.062 (63.797)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.003)\tLoss 1.0630 (1.0392)\tPrec@1 54.688 (62.992)\n",
      "Epoch: [102][390/390]\tTime 0.003 (0.003)\tLoss 1.0967 (1.0507)\tPrec@1 58.750 (62.600)\n",
      "EPOCH: 102 train Results: Prec@1 62.600 Loss: 1.0507\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1662 (1.1662)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3014 (1.2577)\tPrec@1 37.500 (55.270)\n",
      "EPOCH: 102 val Results: Prec@1 55.270 Loss: 1.2577\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/390]\tTime 0.009 (0.009)\tLoss 0.9682 (0.9682)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [103][78/390]\tTime 0.002 (0.003)\tLoss 0.8984 (0.9784)\tPrec@1 64.062 (65.239)\n",
      "Epoch: [103][156/390]\tTime 0.008 (0.003)\tLoss 1.1070 (1.0056)\tPrec@1 61.719 (64.043)\n",
      "Epoch: [103][234/390]\tTime 0.003 (0.003)\tLoss 1.1402 (1.0241)\tPrec@1 62.500 (63.457)\n",
      "Epoch: [103][312/390]\tTime 0.003 (0.003)\tLoss 1.2198 (1.0367)\tPrec@1 57.031 (63.144)\n",
      "Epoch: [103][390/390]\tTime 0.001 (0.003)\tLoss 1.1931 (1.0488)\tPrec@1 60.000 (62.696)\n",
      "EPOCH: 103 train Results: Prec@1 62.696 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0719 (1.0719)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2487 (1.2660)\tPrec@1 56.250 (55.050)\n",
      "EPOCH: 103 val Results: Prec@1 55.050 Loss: 1.2660\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/390]\tTime 0.003 (0.003)\tLoss 1.0331 (1.0331)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 1.0011 (0.9796)\tPrec@1 66.406 (65.121)\n",
      "Epoch: [104][156/390]\tTime 0.005 (0.003)\tLoss 0.9663 (1.0074)\tPrec@1 63.281 (64.077)\n",
      "Epoch: [104][234/390]\tTime 0.004 (0.003)\tLoss 0.9742 (1.0189)\tPrec@1 64.062 (63.680)\n",
      "Epoch: [104][312/390]\tTime 0.002 (0.003)\tLoss 0.9747 (1.0376)\tPrec@1 64.062 (62.927)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.003)\tLoss 1.0613 (1.0492)\tPrec@1 65.000 (62.474)\n",
      "EPOCH: 104 train Results: Prec@1 62.474 Loss: 1.0492\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1067 (1.1067)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1503 (1.2508)\tPrec@1 37.500 (55.550)\n",
      "EPOCH: 104 val Results: Prec@1 55.550 Loss: 1.2508\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/390]\tTime 0.007 (0.007)\tLoss 0.8887 (0.8887)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [105][78/390]\tTime 0.002 (0.003)\tLoss 0.9612 (0.9843)\tPrec@1 63.281 (65.368)\n",
      "Epoch: [105][156/390]\tTime 0.003 (0.003)\tLoss 1.1261 (1.0031)\tPrec@1 59.375 (64.540)\n",
      "Epoch: [105][234/390]\tTime 0.010 (0.003)\tLoss 1.2823 (1.0195)\tPrec@1 58.594 (63.757)\n",
      "Epoch: [105][312/390]\tTime 0.003 (0.003)\tLoss 1.1225 (1.0329)\tPrec@1 61.719 (63.289)\n",
      "Epoch: [105][390/390]\tTime 0.001 (0.003)\tLoss 1.1396 (1.0504)\tPrec@1 58.750 (62.800)\n",
      "EPOCH: 105 train Results: Prec@1 62.800 Loss: 1.0504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0322 (1.0322)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1672 (1.2508)\tPrec@1 43.750 (55.840)\n",
      "EPOCH: 105 val Results: Prec@1 55.840 Loss: 1.2508\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/390]\tTime 0.002 (0.002)\tLoss 0.8676 (0.8676)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [106][78/390]\tTime 0.004 (0.003)\tLoss 0.8870 (0.9637)\tPrec@1 64.844 (65.388)\n",
      "Epoch: [106][156/390]\tTime 0.003 (0.003)\tLoss 1.0311 (0.9966)\tPrec@1 60.156 (64.416)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.003)\tLoss 1.0268 (1.0190)\tPrec@1 63.281 (63.541)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.003)\tLoss 1.0719 (1.0369)\tPrec@1 61.719 (62.947)\n",
      "Epoch: [106][390/390]\tTime 0.003 (0.003)\tLoss 1.2130 (1.0498)\tPrec@1 63.750 (62.414)\n",
      "EPOCH: 106 train Results: Prec@1 62.414 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 0.9733 (0.9733)\tPrec@1 69.531 (69.531)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3981 (1.2439)\tPrec@1 37.500 (55.830)\n",
      "EPOCH: 106 val Results: Prec@1 55.830 Loss: 1.2439\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/390]\tTime 0.002 (0.002)\tLoss 0.8487 (0.8487)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [107][78/390]\tTime 0.002 (0.003)\tLoss 1.0857 (0.9655)\tPrec@1 66.406 (65.773)\n",
      "Epoch: [107][156/390]\tTime 0.008 (0.004)\tLoss 1.2772 (0.9917)\tPrec@1 49.219 (64.515)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.004)\tLoss 1.2490 (1.0126)\tPrec@1 56.250 (63.830)\n",
      "Epoch: [107][312/390]\tTime 0.003 (0.003)\tLoss 1.1009 (1.0383)\tPrec@1 60.156 (62.892)\n",
      "Epoch: [107][390/390]\tTime 0.003 (0.003)\tLoss 1.0090 (1.0536)\tPrec@1 66.250 (62.330)\n",
      "EPOCH: 107 train Results: Prec@1 62.330 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1421 (1.1421)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4340 (1.2430)\tPrec@1 31.250 (56.090)\n",
      "EPOCH: 107 val Results: Prec@1 56.090 Loss: 1.2430\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/390]\tTime 0.003 (0.003)\tLoss 0.9561 (0.9561)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [108][78/390]\tTime 0.003 (0.003)\tLoss 1.0664 (0.9586)\tPrec@1 65.625 (65.427)\n",
      "Epoch: [108][156/390]\tTime 0.010 (0.003)\tLoss 1.0058 (0.9943)\tPrec@1 66.406 (64.431)\n",
      "Epoch: [108][234/390]\tTime 0.002 (0.003)\tLoss 1.1536 (1.0170)\tPrec@1 57.031 (63.687)\n",
      "Epoch: [108][312/390]\tTime 0.002 (0.003)\tLoss 1.1132 (1.0324)\tPrec@1 63.281 (63.107)\n",
      "Epoch: [108][390/390]\tTime 0.001 (0.003)\tLoss 1.0228 (1.0460)\tPrec@1 62.500 (62.678)\n",
      "EPOCH: 108 train Results: Prec@1 62.678 Loss: 1.0460\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0986 (1.0986)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3707 (1.2564)\tPrec@1 31.250 (55.430)\n",
      "EPOCH: 108 val Results: Prec@1 55.430 Loss: 1.2564\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/390]\tTime 0.004 (0.004)\tLoss 1.0514 (1.0514)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [109][78/390]\tTime 0.003 (0.003)\tLoss 1.0287 (0.9789)\tPrec@1 60.938 (65.496)\n",
      "Epoch: [109][156/390]\tTime 0.002 (0.003)\tLoss 1.1056 (1.0128)\tPrec@1 63.281 (64.147)\n",
      "Epoch: [109][234/390]\tTime 0.012 (0.003)\tLoss 0.8665 (1.0242)\tPrec@1 63.281 (63.541)\n",
      "Epoch: [109][312/390]\tTime 0.004 (0.003)\tLoss 1.0765 (1.0368)\tPrec@1 59.375 (63.062)\n",
      "Epoch: [109][390/390]\tTime 0.002 (0.003)\tLoss 1.0126 (1.0458)\tPrec@1 66.250 (62.826)\n",
      "EPOCH: 109 train Results: Prec@1 62.826 Loss: 1.0458\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0866 (1.0866)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4049 (1.2472)\tPrec@1 43.750 (55.830)\n",
      "EPOCH: 109 val Results: Prec@1 55.830 Loss: 1.2472\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/390]\tTime 0.003 (0.003)\tLoss 0.9575 (0.9575)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 0.9617 (0.9704)\tPrec@1 64.844 (65.754)\n",
      "Epoch: [110][156/390]\tTime 0.003 (0.003)\tLoss 0.9745 (1.0015)\tPrec@1 66.406 (64.366)\n",
      "Epoch: [110][234/390]\tTime 0.002 (0.004)\tLoss 1.1295 (1.0162)\tPrec@1 54.688 (63.737)\n",
      "Epoch: [110][312/390]\tTime 0.002 (0.004)\tLoss 1.0311 (1.0375)\tPrec@1 60.938 (62.964)\n",
      "Epoch: [110][390/390]\tTime 0.007 (0.003)\tLoss 1.2203 (1.0472)\tPrec@1 60.000 (62.600)\n",
      "EPOCH: 110 train Results: Prec@1 62.600 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0518 (1.0518)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4546 (1.2513)\tPrec@1 31.250 (55.780)\n",
      "EPOCH: 110 val Results: Prec@1 55.780 Loss: 1.2513\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/390]\tTime 0.004 (0.004)\tLoss 0.9275 (0.9275)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [111][78/390]\tTime 0.011 (0.003)\tLoss 1.0038 (0.9714)\tPrec@1 65.625 (65.388)\n",
      "Epoch: [111][156/390]\tTime 0.002 (0.004)\tLoss 0.9791 (1.0022)\tPrec@1 63.281 (64.082)\n",
      "Epoch: [111][234/390]\tTime 0.002 (0.004)\tLoss 1.2667 (1.0251)\tPrec@1 57.031 (63.298)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.004)\tLoss 1.0145 (1.0410)\tPrec@1 63.281 (62.610)\n",
      "Epoch: [111][390/390]\tTime 0.001 (0.004)\tLoss 1.0910 (1.0516)\tPrec@1 63.750 (62.280)\n",
      "EPOCH: 111 train Results: Prec@1 62.280 Loss: 1.0516\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0741 (1.0741)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1872 (1.2686)\tPrec@1 56.250 (55.430)\n",
      "EPOCH: 111 val Results: Prec@1 55.430 Loss: 1.2686\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/390]\tTime 0.004 (0.004)\tLoss 0.8688 (0.8688)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.0722 (0.9618)\tPrec@1 64.844 (66.386)\n",
      "Epoch: [112][156/390]\tTime 0.002 (0.003)\tLoss 1.0915 (0.9913)\tPrec@1 57.031 (65.048)\n",
      "Epoch: [112][234/390]\tTime 0.004 (0.003)\tLoss 1.1248 (1.0128)\tPrec@1 57.812 (64.219)\n",
      "Epoch: [112][312/390]\tTime 0.007 (0.003)\tLoss 0.9686 (1.0398)\tPrec@1 67.188 (63.256)\n",
      "Epoch: [112][390/390]\tTime 0.002 (0.003)\tLoss 1.2718 (1.0478)\tPrec@1 52.500 (62.950)\n",
      "EPOCH: 112 train Results: Prec@1 62.950 Loss: 1.0478\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0866 (1.0866)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2353 (1.2671)\tPrec@1 56.250 (55.170)\n",
      "EPOCH: 112 val Results: Prec@1 55.170 Loss: 1.2671\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/390]\tTime 0.007 (0.007)\tLoss 0.9353 (0.9353)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [113][78/390]\tTime 0.002 (0.003)\tLoss 0.9818 (0.9667)\tPrec@1 67.969 (66.021)\n",
      "Epoch: [113][156/390]\tTime 0.002 (0.004)\tLoss 1.0095 (0.9895)\tPrec@1 64.062 (64.864)\n",
      "Epoch: [113][234/390]\tTime 0.002 (0.004)\tLoss 0.8883 (1.0094)\tPrec@1 67.188 (64.026)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.004)\tLoss 1.0436 (1.0262)\tPrec@1 60.938 (63.441)\n",
      "Epoch: [113][390/390]\tTime 0.002 (0.004)\tLoss 1.1023 (1.0431)\tPrec@1 60.000 (62.872)\n",
      "EPOCH: 113 train Results: Prec@1 62.872 Loss: 1.0431\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1274 (1.1274)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9470 (1.2451)\tPrec@1 56.250 (55.880)\n",
      "EPOCH: 113 val Results: Prec@1 55.880 Loss: 1.2451\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/390]\tTime 0.003 (0.003)\tLoss 0.9677 (0.9677)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.003)\tLoss 0.7941 (0.9793)\tPrec@1 75.000 (65.506)\n",
      "Epoch: [114][156/390]\tTime 0.002 (0.003)\tLoss 1.0040 (1.0067)\tPrec@1 65.625 (64.286)\n",
      "Epoch: [114][234/390]\tTime 0.009 (0.003)\tLoss 1.0510 (1.0231)\tPrec@1 60.156 (63.604)\n",
      "Epoch: [114][312/390]\tTime 0.002 (0.003)\tLoss 1.0073 (1.0352)\tPrec@1 65.625 (63.221)\n",
      "Epoch: [114][390/390]\tTime 0.001 (0.003)\tLoss 0.9860 (1.0466)\tPrec@1 66.250 (62.690)\n",
      "EPOCH: 114 train Results: Prec@1 62.690 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0382 (1.0382)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3069 (1.2575)\tPrec@1 50.000 (55.460)\n",
      "EPOCH: 114 val Results: Prec@1 55.460 Loss: 1.2575\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/390]\tTime 0.005 (0.005)\tLoss 0.7814 (0.7814)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [115][78/390]\tTime 0.004 (0.003)\tLoss 0.8657 (0.9689)\tPrec@1 71.094 (65.645)\n",
      "Epoch: [115][156/390]\tTime 0.002 (0.003)\tLoss 1.1498 (0.9985)\tPrec@1 57.031 (64.331)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.003)\tLoss 1.1475 (1.0192)\tPrec@1 56.250 (63.501)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.004)\tLoss 1.2164 (1.0376)\tPrec@1 54.688 (62.927)\n",
      "Epoch: [115][390/390]\tTime 0.002 (0.003)\tLoss 1.3975 (1.0477)\tPrec@1 55.000 (62.592)\n",
      "EPOCH: 115 train Results: Prec@1 62.592 Loss: 1.0477\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0696 (1.0696)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3099 (1.2569)\tPrec@1 50.000 (55.730)\n",
      "EPOCH: 115 val Results: Prec@1 55.730 Loss: 1.2569\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/390]\tTime 0.006 (0.006)\tLoss 0.9875 (0.9875)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [116][78/390]\tTime 0.002 (0.003)\tLoss 0.8823 (0.9635)\tPrec@1 69.531 (65.734)\n",
      "Epoch: [116][156/390]\tTime 0.005 (0.003)\tLoss 1.1125 (0.9933)\tPrec@1 61.719 (64.421)\n",
      "Epoch: [116][234/390]\tTime 0.004 (0.003)\tLoss 1.0027 (1.0165)\tPrec@1 65.625 (63.737)\n",
      "Epoch: [116][312/390]\tTime 0.015 (0.003)\tLoss 1.1709 (1.0354)\tPrec@1 57.031 (63.176)\n",
      "Epoch: [116][390/390]\tTime 0.001 (0.003)\tLoss 1.1687 (1.0459)\tPrec@1 61.250 (62.776)\n",
      "EPOCH: 116 train Results: Prec@1 62.776 Loss: 1.0459\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0481 (1.0481)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0719 (1.2552)\tPrec@1 43.750 (55.720)\n",
      "EPOCH: 116 val Results: Prec@1 55.720 Loss: 1.2552\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/390]\tTime 0.003 (0.003)\tLoss 0.9939 (0.9939)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [117][78/390]\tTime 0.002 (0.003)\tLoss 0.9940 (0.9623)\tPrec@1 63.281 (66.119)\n",
      "Epoch: [117][156/390]\tTime 0.002 (0.004)\tLoss 1.1332 (0.9940)\tPrec@1 61.719 (64.670)\n",
      "Epoch: [117][234/390]\tTime 0.007 (0.003)\tLoss 1.1604 (1.0174)\tPrec@1 59.375 (63.836)\n",
      "Epoch: [117][312/390]\tTime 0.003 (0.003)\tLoss 1.0499 (1.0337)\tPrec@1 56.250 (63.156)\n",
      "Epoch: [117][390/390]\tTime 0.001 (0.003)\tLoss 1.0532 (1.0426)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 117 train Results: Prec@1 62.850 Loss: 1.0426\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 0.9877 (0.9877)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4792 (1.2551)\tPrec@1 25.000 (55.770)\n",
      "EPOCH: 117 val Results: Prec@1 55.770 Loss: 1.2551\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/390]\tTime 0.009 (0.009)\tLoss 1.0176 (1.0176)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [118][78/390]\tTime 0.003 (0.004)\tLoss 0.9719 (0.9888)\tPrec@1 69.531 (65.506)\n",
      "Epoch: [118][156/390]\tTime 0.003 (0.003)\tLoss 1.0682 (0.9991)\tPrec@1 63.281 (64.625)\n",
      "Epoch: [118][234/390]\tTime 0.009 (0.003)\tLoss 1.1744 (1.0164)\tPrec@1 60.156 (63.973)\n",
      "Epoch: [118][312/390]\tTime 0.002 (0.003)\tLoss 1.1091 (1.0370)\tPrec@1 64.844 (63.057)\n",
      "Epoch: [118][390/390]\tTime 0.001 (0.003)\tLoss 1.1544 (1.0515)\tPrec@1 60.000 (62.562)\n",
      "EPOCH: 118 train Results: Prec@1 62.562 Loss: 1.0515\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0916 (1.0916)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1223 (1.2681)\tPrec@1 37.500 (55.330)\n",
      "EPOCH: 118 val Results: Prec@1 55.330 Loss: 1.2681\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/390]\tTime 0.002 (0.002)\tLoss 0.9615 (0.9615)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [119][78/390]\tTime 0.002 (0.003)\tLoss 1.0151 (0.9596)\tPrec@1 64.844 (66.317)\n",
      "Epoch: [119][156/390]\tTime 0.002 (0.003)\tLoss 1.0276 (0.9860)\tPrec@1 63.281 (65.147)\n",
      "Epoch: [119][234/390]\tTime 0.003 (0.003)\tLoss 0.9933 (1.0087)\tPrec@1 66.406 (64.119)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.003)\tLoss 1.0602 (1.0250)\tPrec@1 63.281 (63.498)\n",
      "Epoch: [119][390/390]\tTime 0.001 (0.003)\tLoss 1.2058 (1.0399)\tPrec@1 58.750 (62.928)\n",
      "EPOCH: 119 train Results: Prec@1 62.928 Loss: 1.0399\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.0256 (1.0256)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2464 (1.2539)\tPrec@1 43.750 (55.800)\n",
      "EPOCH: 119 val Results: Prec@1 55.800 Loss: 1.2539\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/390]\tTime 0.003 (0.003)\tLoss 0.8039 (0.8039)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.003)\tLoss 1.0426 (0.9665)\tPrec@1 61.719 (65.872)\n",
      "Epoch: [120][156/390]\tTime 0.004 (0.003)\tLoss 0.9273 (0.9957)\tPrec@1 65.625 (64.759)\n",
      "Epoch: [120][234/390]\tTime 0.045 (0.003)\tLoss 1.0779 (1.0159)\tPrec@1 64.062 (63.973)\n",
      "Epoch: [120][312/390]\tTime 0.002 (0.004)\tLoss 1.2875 (1.0279)\tPrec@1 53.125 (63.354)\n",
      "Epoch: [120][390/390]\tTime 0.002 (0.004)\tLoss 1.0466 (1.0410)\tPrec@1 62.500 (62.834)\n",
      "EPOCH: 120 train Results: Prec@1 62.834 Loss: 1.0410\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0010 (1.0010)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0320 (1.2478)\tPrec@1 56.250 (56.240)\n",
      "EPOCH: 120 val Results: Prec@1 56.240 Loss: 1.2478\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/390]\tTime 0.004 (0.004)\tLoss 0.8149 (0.8149)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [121][78/390]\tTime 0.002 (0.003)\tLoss 1.0520 (0.9612)\tPrec@1 60.938 (65.783)\n",
      "Epoch: [121][156/390]\tTime 0.002 (0.003)\tLoss 1.0134 (0.9966)\tPrec@1 63.281 (64.396)\n",
      "Epoch: [121][234/390]\tTime 0.002 (0.003)\tLoss 1.0476 (1.0121)\tPrec@1 58.594 (63.976)\n",
      "Epoch: [121][312/390]\tTime 0.004 (0.003)\tLoss 0.8982 (1.0299)\tPrec@1 72.656 (63.291)\n",
      "Epoch: [121][390/390]\tTime 0.002 (0.003)\tLoss 0.8254 (1.0419)\tPrec@1 71.250 (62.956)\n",
      "EPOCH: 121 train Results: Prec@1 62.956 Loss: 1.0419\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0666 (1.0666)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2321 (1.2533)\tPrec@1 37.500 (55.430)\n",
      "EPOCH: 121 val Results: Prec@1 55.430 Loss: 1.2533\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/390]\tTime 0.005 (0.005)\tLoss 0.9738 (0.9738)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [122][78/390]\tTime 0.003 (0.003)\tLoss 0.9679 (0.9587)\tPrec@1 65.625 (66.317)\n",
      "Epoch: [122][156/390]\tTime 0.004 (0.003)\tLoss 1.0490 (0.9898)\tPrec@1 66.406 (65.112)\n",
      "Epoch: [122][234/390]\tTime 0.002 (0.003)\tLoss 1.1844 (1.0146)\tPrec@1 60.156 (64.209)\n",
      "Epoch: [122][312/390]\tTime 0.002 (0.003)\tLoss 1.2389 (1.0327)\tPrec@1 55.469 (63.319)\n",
      "Epoch: [122][390/390]\tTime 0.020 (0.003)\tLoss 1.2570 (1.0456)\tPrec@1 60.000 (62.876)\n",
      "EPOCH: 122 train Results: Prec@1 62.876 Loss: 1.0456\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1073 (1.1073)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2241 (1.2696)\tPrec@1 37.500 (55.730)\n",
      "EPOCH: 122 val Results: Prec@1 55.730 Loss: 1.2696\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/390]\tTime 0.006 (0.006)\tLoss 0.8362 (0.8362)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [123][78/390]\tTime 0.002 (0.006)\tLoss 1.0265 (0.9625)\tPrec@1 57.031 (65.496)\n",
      "Epoch: [123][156/390]\tTime 0.009 (0.005)\tLoss 1.1098 (0.9918)\tPrec@1 60.156 (64.202)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.004)\tLoss 1.3122 (1.0198)\tPrec@1 53.906 (63.418)\n",
      "Epoch: [123][312/390]\tTime 0.002 (0.005)\tLoss 1.1775 (1.0357)\tPrec@1 58.594 (62.992)\n",
      "Epoch: [123][390/390]\tTime 0.003 (0.004)\tLoss 1.0744 (1.0457)\tPrec@1 56.250 (62.558)\n",
      "EPOCH: 123 train Results: Prec@1 62.558 Loss: 1.0457\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1335 (1.1335)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2625 (1.2468)\tPrec@1 31.250 (56.130)\n",
      "EPOCH: 123 val Results: Prec@1 56.130 Loss: 1.2468\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/390]\tTime 0.005 (0.005)\tLoss 0.9352 (0.9352)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [124][78/390]\tTime 0.004 (0.004)\tLoss 1.0013 (0.9687)\tPrec@1 61.719 (65.813)\n",
      "Epoch: [124][156/390]\tTime 0.013 (0.003)\tLoss 0.9886 (1.0036)\tPrec@1 63.281 (64.316)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.0894 (1.0214)\tPrec@1 57.031 (63.723)\n",
      "Epoch: [124][312/390]\tTime 0.003 (0.003)\tLoss 1.1819 (1.0314)\tPrec@1 61.719 (63.284)\n",
      "Epoch: [124][390/390]\tTime 0.003 (0.003)\tLoss 1.1828 (1.0461)\tPrec@1 57.500 (62.700)\n",
      "EPOCH: 124 train Results: Prec@1 62.700 Loss: 1.0461\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1242 (1.1242)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5048 (1.2539)\tPrec@1 43.750 (55.380)\n",
      "EPOCH: 124 val Results: Prec@1 55.380 Loss: 1.2539\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/390]\tTime 0.002 (0.002)\tLoss 0.9336 (0.9336)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [125][78/390]\tTime 0.003 (0.003)\tLoss 0.8606 (0.9742)\tPrec@1 71.875 (65.566)\n",
      "Epoch: [125][156/390]\tTime 0.002 (0.003)\tLoss 0.9661 (0.9989)\tPrec@1 65.625 (64.704)\n",
      "Epoch: [125][234/390]\tTime 0.003 (0.003)\tLoss 1.0395 (1.0173)\tPrec@1 60.938 (63.949)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.1245 (1.0322)\tPrec@1 63.281 (63.356)\n",
      "Epoch: [125][390/390]\tTime 0.009 (0.003)\tLoss 0.8532 (1.0448)\tPrec@1 75.000 (62.880)\n",
      "EPOCH: 125 train Results: Prec@1 62.880 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0923 (1.0923)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.1707 (1.2563)\tPrec@1 43.750 (55.810)\n",
      "EPOCH: 125 val Results: Prec@1 55.810 Loss: 1.2563\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/390]\tTime 0.004 (0.004)\tLoss 0.8251 (0.8251)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [126][78/390]\tTime 0.002 (0.003)\tLoss 0.8869 (0.9596)\tPrec@1 69.531 (65.843)\n",
      "Epoch: [126][156/390]\tTime 0.002 (0.003)\tLoss 1.1164 (0.9909)\tPrec@1 64.062 (64.819)\n",
      "Epoch: [126][234/390]\tTime 0.004 (0.003)\tLoss 1.1743 (1.0155)\tPrec@1 60.156 (63.900)\n",
      "Epoch: [126][312/390]\tTime 0.002 (0.003)\tLoss 1.0575 (1.0336)\tPrec@1 62.500 (63.201)\n",
      "Epoch: [126][390/390]\tTime 0.001 (0.003)\tLoss 1.0516 (1.0475)\tPrec@1 63.750 (62.786)\n",
      "EPOCH: 126 train Results: Prec@1 62.786 Loss: 1.0475\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1671 (1.1671)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3018 (1.2702)\tPrec@1 31.250 (55.350)\n",
      "EPOCH: 126 val Results: Prec@1 55.350 Loss: 1.2702\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/390]\tTime 0.002 (0.002)\tLoss 1.1470 (1.1470)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.003)\tLoss 0.8391 (0.9719)\tPrec@1 71.875 (65.773)\n",
      "Epoch: [127][156/390]\tTime 0.003 (0.003)\tLoss 0.9970 (0.9967)\tPrec@1 64.844 (64.794)\n",
      "Epoch: [127][234/390]\tTime 0.003 (0.003)\tLoss 1.3657 (1.0135)\tPrec@1 52.344 (64.345)\n",
      "Epoch: [127][312/390]\tTime 0.002 (0.003)\tLoss 1.2052 (1.0350)\tPrec@1 56.250 (63.354)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.003)\tLoss 0.9823 (1.0467)\tPrec@1 67.500 (62.916)\n",
      "EPOCH: 127 train Results: Prec@1 62.916 Loss: 1.0467\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1420 (1.1420)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3432 (1.2447)\tPrec@1 37.500 (56.460)\n",
      "EPOCH: 127 val Results: Prec@1 56.460 Loss: 1.2447\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/390]\tTime 0.004 (0.004)\tLoss 0.9997 (0.9997)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [128][78/390]\tTime 0.005 (0.003)\tLoss 1.1222 (0.9664)\tPrec@1 63.281 (65.437)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.003)\tLoss 0.9906 (0.9957)\tPrec@1 63.281 (64.476)\n",
      "Epoch: [128][234/390]\tTime 0.003 (0.003)\tLoss 1.0852 (1.0107)\tPrec@1 67.188 (64.033)\n",
      "Epoch: [128][312/390]\tTime 0.002 (0.003)\tLoss 1.0654 (1.0281)\tPrec@1 62.500 (63.406)\n",
      "Epoch: [128][390/390]\tTime 0.001 (0.003)\tLoss 1.2253 (1.0422)\tPrec@1 61.250 (62.930)\n",
      "EPOCH: 128 train Results: Prec@1 62.930 Loss: 1.0422\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0613 (1.0613)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1367 (1.2634)\tPrec@1 50.000 (55.410)\n",
      "EPOCH: 128 val Results: Prec@1 55.410 Loss: 1.2634\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/390]\tTime 0.003 (0.003)\tLoss 1.0806 (1.0806)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 0.8898 (0.9509)\tPrec@1 71.094 (66.367)\n",
      "Epoch: [129][156/390]\tTime 0.005 (0.003)\tLoss 0.9814 (0.9820)\tPrec@1 62.500 (65.043)\n",
      "Epoch: [129][234/390]\tTime 0.003 (0.003)\tLoss 1.2676 (1.0111)\tPrec@1 52.344 (64.036)\n",
      "Epoch: [129][312/390]\tTime 0.007 (0.003)\tLoss 0.9256 (1.0284)\tPrec@1 62.500 (63.256)\n",
      "Epoch: [129][390/390]\tTime 0.006 (0.003)\tLoss 0.9818 (1.0432)\tPrec@1 61.250 (62.686)\n",
      "EPOCH: 129 train Results: Prec@1 62.686 Loss: 1.0432\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1689 (1.1689)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2535 (1.2553)\tPrec@1 50.000 (55.600)\n",
      "EPOCH: 129 val Results: Prec@1 55.600 Loss: 1.2553\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/390]\tTime 0.005 (0.005)\tLoss 0.8594 (0.8594)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [130][78/390]\tTime 0.002 (0.003)\tLoss 0.9041 (0.9534)\tPrec@1 64.844 (66.297)\n",
      "Epoch: [130][156/390]\tTime 0.007 (0.003)\tLoss 1.1269 (0.9805)\tPrec@1 61.719 (65.351)\n",
      "Epoch: [130][234/390]\tTime 0.004 (0.003)\tLoss 1.2835 (1.0102)\tPrec@1 59.375 (64.279)\n",
      "Epoch: [130][312/390]\tTime 0.005 (0.003)\tLoss 1.0876 (1.0296)\tPrec@1 59.375 (63.491)\n",
      "Epoch: [130][390/390]\tTime 0.002 (0.003)\tLoss 1.1877 (1.0431)\tPrec@1 58.750 (63.012)\n",
      "EPOCH: 130 train Results: Prec@1 63.012 Loss: 1.0431\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1415 (1.1415)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2079 (1.2564)\tPrec@1 50.000 (55.260)\n",
      "EPOCH: 130 val Results: Prec@1 55.260 Loss: 1.2564\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/390]\tTime 0.003 (0.003)\tLoss 0.8249 (0.8249)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [131][78/390]\tTime 0.002 (0.003)\tLoss 0.9226 (0.9712)\tPrec@1 69.531 (65.625)\n",
      "Epoch: [131][156/390]\tTime 0.003 (0.003)\tLoss 1.1400 (0.9979)\tPrec@1 55.469 (64.346)\n",
      "Epoch: [131][234/390]\tTime 0.003 (0.004)\tLoss 1.1151 (1.0174)\tPrec@1 59.375 (63.590)\n",
      "Epoch: [131][312/390]\tTime 0.004 (0.004)\tLoss 1.2877 (1.0350)\tPrec@1 53.906 (62.919)\n",
      "Epoch: [131][390/390]\tTime 0.006 (0.004)\tLoss 1.0121 (1.0483)\tPrec@1 58.750 (62.502)\n",
      "EPOCH: 131 train Results: Prec@1 62.502 Loss: 1.0483\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0985 (1.0985)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3842 (1.2578)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 131 val Results: Prec@1 55.630 Loss: 1.2578\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/390]\tTime 0.006 (0.006)\tLoss 0.9006 (0.9006)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [132][78/390]\tTime 0.003 (0.004)\tLoss 0.9782 (0.9633)\tPrec@1 70.312 (65.892)\n",
      "Epoch: [132][156/390]\tTime 0.002 (0.004)\tLoss 1.1256 (1.0025)\tPrec@1 64.844 (64.401)\n",
      "Epoch: [132][234/390]\tTime 0.003 (0.003)\tLoss 0.9438 (1.0249)\tPrec@1 68.750 (63.657)\n",
      "Epoch: [132][312/390]\tTime 0.003 (0.003)\tLoss 1.3169 (1.0375)\tPrec@1 53.906 (63.144)\n",
      "Epoch: [132][390/390]\tTime 0.001 (0.003)\tLoss 1.1137 (1.0466)\tPrec@1 66.250 (62.684)\n",
      "EPOCH: 132 train Results: Prec@1 62.684 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0834 (1.0834)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0564 (1.2543)\tPrec@1 37.500 (55.450)\n",
      "EPOCH: 132 val Results: Prec@1 55.450 Loss: 1.2543\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/390]\tTime 0.005 (0.005)\tLoss 0.8773 (0.8773)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [133][78/390]\tTime 0.003 (0.003)\tLoss 0.8910 (0.9653)\tPrec@1 62.500 (66.070)\n",
      "Epoch: [133][156/390]\tTime 0.002 (0.003)\tLoss 0.9027 (0.9879)\tPrec@1 67.969 (65.013)\n",
      "Epoch: [133][234/390]\tTime 0.005 (0.003)\tLoss 1.0643 (1.0107)\tPrec@1 60.938 (64.129)\n",
      "Epoch: [133][312/390]\tTime 0.003 (0.003)\tLoss 1.2222 (1.0328)\tPrec@1 61.719 (63.344)\n",
      "Epoch: [133][390/390]\tTime 0.008 (0.003)\tLoss 1.0638 (1.0448)\tPrec@1 60.000 (62.896)\n",
      "EPOCH: 133 train Results: Prec@1 62.896 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1949 (1.1949)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3216 (1.2540)\tPrec@1 43.750 (55.380)\n",
      "EPOCH: 133 val Results: Prec@1 55.380 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/390]\tTime 0.002 (0.002)\tLoss 0.9230 (0.9230)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [134][78/390]\tTime 0.003 (0.003)\tLoss 0.9859 (0.9703)\tPrec@1 67.969 (65.724)\n",
      "Epoch: [134][156/390]\tTime 0.003 (0.003)\tLoss 1.0846 (1.0032)\tPrec@1 64.844 (64.466)\n",
      "Epoch: [134][234/390]\tTime 0.011 (0.003)\tLoss 1.1020 (1.0200)\tPrec@1 62.500 (63.747)\n",
      "Epoch: [134][312/390]\tTime 0.004 (0.003)\tLoss 1.1761 (1.0334)\tPrec@1 55.469 (63.181)\n",
      "Epoch: [134][390/390]\tTime 0.003 (0.003)\tLoss 0.9589 (1.0449)\tPrec@1 62.500 (62.738)\n",
      "EPOCH: 134 train Results: Prec@1 62.738 Loss: 1.0449\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1199 (1.1199)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1088 (1.2456)\tPrec@1 43.750 (56.060)\n",
      "EPOCH: 134 val Results: Prec@1 56.060 Loss: 1.2456\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/390]\tTime 0.007 (0.007)\tLoss 0.9780 (0.9780)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [135][78/390]\tTime 0.003 (0.003)\tLoss 0.9668 (0.9591)\tPrec@1 60.938 (66.169)\n",
      "Epoch: [135][156/390]\tTime 0.003 (0.004)\tLoss 0.9644 (0.9900)\tPrec@1 68.750 (64.918)\n",
      "Epoch: [135][234/390]\tTime 0.003 (0.003)\tLoss 1.1231 (1.0134)\tPrec@1 60.938 (63.986)\n",
      "Epoch: [135][312/390]\tTime 0.002 (0.003)\tLoss 0.9859 (1.0338)\tPrec@1 65.625 (63.244)\n",
      "Epoch: [135][390/390]\tTime 0.011 (0.003)\tLoss 1.2863 (1.0495)\tPrec@1 56.250 (62.644)\n",
      "EPOCH: 135 train Results: Prec@1 62.644 Loss: 1.0495\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1453 (1.1453)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2387 (1.2515)\tPrec@1 56.250 (55.600)\n",
      "EPOCH: 135 val Results: Prec@1 55.600 Loss: 1.2515\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/390]\tTime 0.004 (0.004)\tLoss 0.8147 (0.8147)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [136][78/390]\tTime 0.002 (0.003)\tLoss 1.1504 (0.9609)\tPrec@1 58.594 (66.090)\n",
      "Epoch: [136][156/390]\tTime 0.003 (0.003)\tLoss 0.9398 (0.9911)\tPrec@1 65.625 (64.998)\n",
      "Epoch: [136][234/390]\tTime 0.005 (0.003)\tLoss 0.9710 (1.0157)\tPrec@1 68.750 (64.059)\n",
      "Epoch: [136][312/390]\tTime 0.005 (0.003)\tLoss 1.0954 (1.0301)\tPrec@1 57.031 (63.451)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.1552 (1.0433)\tPrec@1 52.500 (62.794)\n",
      "EPOCH: 136 train Results: Prec@1 62.794 Loss: 1.0433\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1034 (1.1034)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3191 (1.2559)\tPrec@1 50.000 (55.370)\n",
      "EPOCH: 136 val Results: Prec@1 55.370 Loss: 1.2559\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/390]\tTime 0.003 (0.003)\tLoss 0.9621 (0.9621)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [137][78/390]\tTime 0.002 (0.003)\tLoss 1.0570 (0.9898)\tPrec@1 55.469 (65.309)\n",
      "Epoch: [137][156/390]\tTime 0.002 (0.003)\tLoss 0.9577 (1.0047)\tPrec@1 72.656 (64.426)\n",
      "Epoch: [137][234/390]\tTime 0.003 (0.003)\tLoss 0.9403 (1.0140)\tPrec@1 64.062 (63.890)\n",
      "Epoch: [137][312/390]\tTime 0.002 (0.003)\tLoss 1.1225 (1.0299)\tPrec@1 60.938 (63.326)\n",
      "Epoch: [137][390/390]\tTime 0.002 (0.003)\tLoss 1.2078 (1.0455)\tPrec@1 55.000 (62.838)\n",
      "EPOCH: 137 train Results: Prec@1 62.838 Loss: 1.0455\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0365 (1.0365)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1414 (1.2563)\tPrec@1 50.000 (55.760)\n",
      "EPOCH: 137 val Results: Prec@1 55.760 Loss: 1.2563\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/390]\tTime 0.005 (0.005)\tLoss 1.0435 (1.0435)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [138][78/390]\tTime 0.003 (0.003)\tLoss 0.9471 (0.9741)\tPrec@1 65.625 (65.417)\n",
      "Epoch: [138][156/390]\tTime 0.015 (0.003)\tLoss 0.9413 (1.0032)\tPrec@1 67.969 (64.276)\n",
      "Epoch: [138][234/390]\tTime 0.004 (0.003)\tLoss 1.1344 (1.0196)\tPrec@1 60.938 (63.896)\n",
      "Epoch: [138][312/390]\tTime 0.012 (0.003)\tLoss 1.1732 (1.0360)\tPrec@1 57.812 (63.176)\n",
      "Epoch: [138][390/390]\tTime 0.001 (0.003)\tLoss 1.0621 (1.0490)\tPrec@1 60.000 (62.674)\n",
      "EPOCH: 138 train Results: Prec@1 62.674 Loss: 1.0490\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0948 (1.0948)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2067 (1.2515)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 138 val Results: Prec@1 55.370 Loss: 1.2515\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/390]\tTime 0.002 (0.002)\tLoss 0.9109 (0.9109)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [139][78/390]\tTime 0.002 (0.004)\tLoss 0.9646 (0.9668)\tPrec@1 67.969 (65.773)\n",
      "Epoch: [139][156/390]\tTime 0.002 (0.004)\tLoss 0.9542 (0.9938)\tPrec@1 67.969 (65.117)\n",
      "Epoch: [139][234/390]\tTime 0.002 (0.003)\tLoss 1.1142 (1.0165)\tPrec@1 65.625 (64.249)\n",
      "Epoch: [139][312/390]\tTime 0.003 (0.003)\tLoss 1.0781 (1.0313)\tPrec@1 60.156 (63.633)\n",
      "Epoch: [139][390/390]\tTime 0.004 (0.003)\tLoss 1.1061 (1.0440)\tPrec@1 67.500 (63.142)\n",
      "EPOCH: 139 train Results: Prec@1 63.142 Loss: 1.0440\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1426 (1.1426)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1866 (1.2560)\tPrec@1 50.000 (55.410)\n",
      "EPOCH: 139 val Results: Prec@1 55.410 Loss: 1.2560\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/390]\tTime 0.002 (0.002)\tLoss 0.9686 (0.9686)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [140][78/390]\tTime 0.003 (0.003)\tLoss 1.0099 (0.9689)\tPrec@1 57.812 (65.892)\n",
      "Epoch: [140][156/390]\tTime 0.007 (0.003)\tLoss 1.0686 (0.9939)\tPrec@1 58.594 (64.849)\n",
      "Epoch: [140][234/390]\tTime 0.002 (0.003)\tLoss 1.0380 (1.0122)\tPrec@1 63.281 (64.166)\n",
      "Epoch: [140][312/390]\tTime 0.003 (0.003)\tLoss 1.1139 (1.0251)\tPrec@1 58.594 (63.628)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.003)\tLoss 1.2424 (1.0417)\tPrec@1 55.000 (62.974)\n",
      "EPOCH: 140 train Results: Prec@1 62.974 Loss: 1.0417\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1572 (1.1572)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4207 (1.2619)\tPrec@1 37.500 (55.010)\n",
      "EPOCH: 140 val Results: Prec@1 55.010 Loss: 1.2619\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/390]\tTime 0.002 (0.002)\tLoss 0.7216 (0.7216)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [141][78/390]\tTime 0.002 (0.003)\tLoss 1.0543 (0.9561)\tPrec@1 66.406 (66.070)\n",
      "Epoch: [141][156/390]\tTime 0.004 (0.003)\tLoss 1.1572 (1.0004)\tPrec@1 54.688 (64.381)\n",
      "Epoch: [141][234/390]\tTime 0.003 (0.003)\tLoss 1.0562 (1.0214)\tPrec@1 62.500 (63.561)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.003)\tLoss 1.1364 (1.0376)\tPrec@1 64.062 (62.897)\n",
      "Epoch: [141][390/390]\tTime 0.003 (0.003)\tLoss 1.1157 (1.0468)\tPrec@1 60.000 (62.634)\n",
      "EPOCH: 141 train Results: Prec@1 62.634 Loss: 1.0468\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1252 (1.1252)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3861 (1.2561)\tPrec@1 56.250 (55.230)\n",
      "EPOCH: 141 val Results: Prec@1 55.230 Loss: 1.2561\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/390]\tTime 0.004 (0.004)\tLoss 0.9118 (0.9118)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [142][78/390]\tTime 0.002 (0.003)\tLoss 0.9621 (0.9564)\tPrec@1 64.844 (66.466)\n",
      "Epoch: [142][156/390]\tTime 0.003 (0.003)\tLoss 0.9968 (0.9877)\tPrec@1 60.938 (64.689)\n",
      "Epoch: [142][234/390]\tTime 0.006 (0.003)\tLoss 1.1511 (1.0084)\tPrec@1 63.281 (63.966)\n",
      "Epoch: [142][312/390]\tTime 0.008 (0.003)\tLoss 1.0986 (1.0274)\tPrec@1 62.500 (63.431)\n",
      "Epoch: [142][390/390]\tTime 0.001 (0.003)\tLoss 1.2483 (1.0449)\tPrec@1 52.500 (62.702)\n",
      "EPOCH: 142 train Results: Prec@1 62.702 Loss: 1.0449\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1646 (1.1646)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.2183 (1.2665)\tPrec@1 56.250 (55.030)\n",
      "EPOCH: 142 val Results: Prec@1 55.030 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/390]\tTime 0.003 (0.003)\tLoss 0.9452 (0.9452)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [143][78/390]\tTime 0.002 (0.003)\tLoss 0.9839 (0.9681)\tPrec@1 72.656 (65.763)\n",
      "Epoch: [143][156/390]\tTime 0.002 (0.003)\tLoss 0.9165 (0.9951)\tPrec@1 65.625 (64.675)\n",
      "Epoch: [143][234/390]\tTime 0.004 (0.003)\tLoss 1.0934 (1.0143)\tPrec@1 65.625 (63.893)\n",
      "Epoch: [143][312/390]\tTime 0.005 (0.003)\tLoss 1.1003 (1.0295)\tPrec@1 59.375 (63.339)\n",
      "Epoch: [143][390/390]\tTime 0.008 (0.003)\tLoss 0.9992 (1.0423)\tPrec@1 63.750 (62.804)\n",
      "EPOCH: 143 train Results: Prec@1 62.804 Loss: 1.0423\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1058 (1.1058)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0594 (1.2537)\tPrec@1 62.500 (55.590)\n",
      "EPOCH: 143 val Results: Prec@1 55.590 Loss: 1.2537\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/390]\tTime 0.002 (0.002)\tLoss 0.8698 (0.8698)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [144][78/390]\tTime 0.008 (0.003)\tLoss 1.0005 (0.9737)\tPrec@1 64.062 (65.546)\n",
      "Epoch: [144][156/390]\tTime 0.003 (0.003)\tLoss 1.2666 (0.9986)\tPrec@1 58.594 (64.500)\n",
      "Epoch: [144][234/390]\tTime 0.005 (0.003)\tLoss 1.1271 (1.0187)\tPrec@1 60.938 (63.906)\n",
      "Epoch: [144][312/390]\tTime 0.002 (0.003)\tLoss 1.0188 (1.0289)\tPrec@1 60.938 (63.496)\n",
      "Epoch: [144][390/390]\tTime 0.004 (0.003)\tLoss 0.9962 (1.0391)\tPrec@1 60.000 (63.096)\n",
      "EPOCH: 144 train Results: Prec@1 63.096 Loss: 1.0391\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0925 (1.0925)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3413 (1.2509)\tPrec@1 37.500 (56.040)\n",
      "EPOCH: 144 val Results: Prec@1 56.040 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/390]\tTime 0.004 (0.004)\tLoss 0.9920 (0.9920)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [145][78/390]\tTime 0.003 (0.004)\tLoss 0.9096 (0.9706)\tPrec@1 69.531 (65.358)\n",
      "Epoch: [145][156/390]\tTime 0.002 (0.004)\tLoss 1.0535 (0.9841)\tPrec@1 64.062 (65.068)\n",
      "Epoch: [145][234/390]\tTime 0.010 (0.004)\tLoss 0.9857 (1.0115)\tPrec@1 66.406 (64.029)\n",
      "Epoch: [145][312/390]\tTime 0.002 (0.004)\tLoss 1.2132 (1.0310)\tPrec@1 60.938 (63.249)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.004)\tLoss 1.1423 (1.0425)\tPrec@1 57.500 (62.912)\n",
      "EPOCH: 145 train Results: Prec@1 62.912 Loss: 1.0425\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0954 (1.0954)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3588 (1.2584)\tPrec@1 43.750 (55.740)\n",
      "EPOCH: 145 val Results: Prec@1 55.740 Loss: 1.2584\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/390]\tTime 0.004 (0.004)\tLoss 1.0094 (1.0094)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [146][78/390]\tTime 0.003 (0.005)\tLoss 0.9580 (0.9682)\tPrec@1 65.625 (65.180)\n",
      "Epoch: [146][156/390]\tTime 0.005 (0.004)\tLoss 1.0319 (0.9979)\tPrec@1 62.500 (64.167)\n",
      "Epoch: [146][234/390]\tTime 0.002 (0.004)\tLoss 0.8903 (1.0146)\tPrec@1 71.094 (63.630)\n",
      "Epoch: [146][312/390]\tTime 0.003 (0.004)\tLoss 0.9520 (1.0332)\tPrec@1 68.750 (63.109)\n",
      "Epoch: [146][390/390]\tTime 0.002 (0.004)\tLoss 0.9097 (1.0450)\tPrec@1 66.250 (62.646)\n",
      "EPOCH: 146 train Results: Prec@1 62.646 Loss: 1.0450\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0285 (1.0285)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1653 (1.2540)\tPrec@1 50.000 (55.100)\n",
      "EPOCH: 146 val Results: Prec@1 55.100 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/390]\tTime 0.003 (0.003)\tLoss 0.8659 (0.8659)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [147][78/390]\tTime 0.003 (0.003)\tLoss 0.9897 (0.9682)\tPrec@1 64.062 (65.318)\n",
      "Epoch: [147][156/390]\tTime 0.002 (0.003)\tLoss 1.1112 (0.9906)\tPrec@1 62.500 (64.446)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.003)\tLoss 1.2271 (1.0086)\tPrec@1 57.031 (63.933)\n",
      "Epoch: [147][312/390]\tTime 0.006 (0.003)\tLoss 1.1038 (1.0258)\tPrec@1 62.500 (63.286)\n",
      "Epoch: [147][390/390]\tTime 0.002 (0.003)\tLoss 1.0875 (1.0383)\tPrec@1 60.000 (62.922)\n",
      "EPOCH: 147 train Results: Prec@1 62.922 Loss: 1.0383\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1317 (1.1317)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6665 (1.2587)\tPrec@1 25.000 (55.460)\n",
      "EPOCH: 147 val Results: Prec@1 55.460 Loss: 1.2587\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/390]\tTime 0.003 (0.003)\tLoss 1.0457 (1.0457)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [148][78/390]\tTime 0.003 (0.004)\tLoss 1.0713 (0.9785)\tPrec@1 61.719 (65.239)\n",
      "Epoch: [148][156/390]\tTime 0.004 (0.004)\tLoss 1.1139 (1.0046)\tPrec@1 62.500 (64.202)\n",
      "Epoch: [148][234/390]\tTime 0.002 (0.004)\tLoss 1.0792 (1.0243)\tPrec@1 59.375 (63.524)\n",
      "Epoch: [148][312/390]\tTime 0.003 (0.004)\tLoss 0.9690 (1.0360)\tPrec@1 67.188 (63.194)\n",
      "Epoch: [148][390/390]\tTime 0.001 (0.004)\tLoss 0.9833 (1.0474)\tPrec@1 62.500 (62.804)\n",
      "EPOCH: 148 train Results: Prec@1 62.804 Loss: 1.0474\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1394 (1.1394)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5144 (1.2580)\tPrec@1 37.500 (55.710)\n",
      "EPOCH: 148 val Results: Prec@1 55.710 Loss: 1.2580\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/390]\tTime 0.010 (0.010)\tLoss 0.8219 (0.8219)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [149][78/390]\tTime 0.003 (0.003)\tLoss 1.0457 (0.9596)\tPrec@1 61.719 (66.199)\n",
      "Epoch: [149][156/390]\tTime 0.003 (0.003)\tLoss 1.0027 (0.9873)\tPrec@1 63.281 (65.098)\n",
      "Epoch: [149][234/390]\tTime 0.002 (0.003)\tLoss 0.9663 (1.0108)\tPrec@1 64.062 (64.295)\n",
      "Epoch: [149][312/390]\tTime 0.004 (0.003)\tLoss 1.1420 (1.0235)\tPrec@1 57.031 (63.820)\n",
      "Epoch: [149][390/390]\tTime 0.002 (0.003)\tLoss 1.0933 (1.0401)\tPrec@1 60.000 (63.184)\n",
      "EPOCH: 149 train Results: Prec@1 63.184 Loss: 1.0401\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0873 (1.0873)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3228 (1.2584)\tPrec@1 50.000 (55.700)\n",
      "EPOCH: 149 val Results: Prec@1 55.700 Loss: 1.2584\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/390]\tTime 0.002 (0.002)\tLoss 0.9870 (0.9870)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [150][78/390]\tTime 0.003 (0.003)\tLoss 1.1757 (0.9565)\tPrec@1 60.938 (66.317)\n",
      "Epoch: [150][156/390]\tTime 0.002 (0.003)\tLoss 1.0894 (0.9821)\tPrec@1 63.281 (65.272)\n",
      "Epoch: [150][234/390]\tTime 0.004 (0.003)\tLoss 1.2217 (1.0191)\tPrec@1 63.281 (63.677)\n",
      "Epoch: [150][312/390]\tTime 0.004 (0.003)\tLoss 1.0654 (1.0349)\tPrec@1 59.375 (63.042)\n",
      "Epoch: [150][390/390]\tTime 0.001 (0.003)\tLoss 1.0127 (1.0466)\tPrec@1 66.250 (62.584)\n",
      "EPOCH: 150 train Results: Prec@1 62.584 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1446 (1.1446)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1663 (1.2527)\tPrec@1 50.000 (55.580)\n",
      "EPOCH: 150 val Results: Prec@1 55.580 Loss: 1.2527\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/390]\tTime 0.004 (0.004)\tLoss 0.9232 (0.9232)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [151][78/390]\tTime 0.002 (0.003)\tLoss 1.1241 (0.9616)\tPrec@1 60.938 (65.823)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.003)\tLoss 1.0124 (0.9863)\tPrec@1 63.281 (64.809)\n",
      "Epoch: [151][234/390]\tTime 0.002 (0.003)\tLoss 1.1055 (1.0094)\tPrec@1 60.938 (63.979)\n",
      "Epoch: [151][312/390]\tTime 0.004 (0.003)\tLoss 1.1386 (1.0230)\tPrec@1 59.375 (63.366)\n",
      "Epoch: [151][390/390]\tTime 0.003 (0.003)\tLoss 1.1851 (1.0416)\tPrec@1 58.750 (62.646)\n",
      "EPOCH: 151 train Results: Prec@1 62.646 Loss: 1.0416\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1213 (1.1213)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1523 (1.2588)\tPrec@1 56.250 (55.730)\n",
      "EPOCH: 151 val Results: Prec@1 55.730 Loss: 1.2588\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/390]\tTime 0.002 (0.002)\tLoss 0.7736 (0.7736)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [152][78/390]\tTime 0.002 (0.003)\tLoss 1.0548 (0.9701)\tPrec@1 60.156 (65.882)\n",
      "Epoch: [152][156/390]\tTime 0.005 (0.003)\tLoss 1.0168 (0.9966)\tPrec@1 60.156 (64.759)\n",
      "Epoch: [152][234/390]\tTime 0.003 (0.003)\tLoss 1.1780 (1.0149)\tPrec@1 57.812 (63.953)\n",
      "Epoch: [152][312/390]\tTime 0.007 (0.003)\tLoss 1.0821 (1.0279)\tPrec@1 59.375 (63.531)\n",
      "Epoch: [152][390/390]\tTime 0.003 (0.003)\tLoss 0.9581 (1.0403)\tPrec@1 62.500 (63.074)\n",
      "EPOCH: 152 train Results: Prec@1 63.074 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1255 (1.1255)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3912 (1.2665)\tPrec@1 31.250 (54.960)\n",
      "EPOCH: 152 val Results: Prec@1 54.960 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/390]\tTime 0.007 (0.007)\tLoss 0.7744 (0.7744)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [153][78/390]\tTime 0.002 (0.003)\tLoss 1.0226 (0.9600)\tPrec@1 61.719 (65.684)\n",
      "Epoch: [153][156/390]\tTime 0.002 (0.003)\tLoss 0.9712 (0.9910)\tPrec@1 67.969 (64.515)\n",
      "Epoch: [153][234/390]\tTime 0.003 (0.004)\tLoss 1.0107 (1.0126)\tPrec@1 64.844 (63.850)\n",
      "Epoch: [153][312/390]\tTime 0.004 (0.004)\tLoss 1.0050 (1.0290)\tPrec@1 62.500 (63.314)\n",
      "Epoch: [153][390/390]\tTime 0.003 (0.004)\tLoss 1.1472 (1.0391)\tPrec@1 61.250 (62.974)\n",
      "EPOCH: 153 train Results: Prec@1 62.974 Loss: 1.0391\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1421 (1.1421)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4642 (1.2509)\tPrec@1 31.250 (55.630)\n",
      "EPOCH: 153 val Results: Prec@1 55.630 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/390]\tTime 0.007 (0.007)\tLoss 0.8329 (0.8329)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [154][78/390]\tTime 0.004 (0.003)\tLoss 0.8981 (0.9786)\tPrec@1 67.188 (64.972)\n",
      "Epoch: [154][156/390]\tTime 0.002 (0.003)\tLoss 1.1282 (0.9967)\tPrec@1 57.812 (64.276)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.003)\tLoss 1.2091 (1.0173)\tPrec@1 57.031 (63.577)\n",
      "Epoch: [154][312/390]\tTime 0.002 (0.003)\tLoss 1.1627 (1.0389)\tPrec@1 51.562 (62.869)\n",
      "Epoch: [154][390/390]\tTime 0.006 (0.003)\tLoss 1.1018 (1.0486)\tPrec@1 60.000 (62.574)\n",
      "EPOCH: 154 train Results: Prec@1 62.574 Loss: 1.0486\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2229 (1.2229)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5655 (1.2557)\tPrec@1 37.500 (55.640)\n",
      "EPOCH: 154 val Results: Prec@1 55.640 Loss: 1.2557\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/390]\tTime 0.002 (0.002)\tLoss 0.9959 (0.9959)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [155][78/390]\tTime 0.002 (0.003)\tLoss 0.9013 (0.9648)\tPrec@1 66.406 (65.605)\n",
      "Epoch: [155][156/390]\tTime 0.003 (0.003)\tLoss 0.9608 (0.9879)\tPrec@1 66.406 (64.615)\n",
      "Epoch: [155][234/390]\tTime 0.002 (0.003)\tLoss 1.1423 (1.0052)\tPrec@1 57.812 (64.059)\n",
      "Epoch: [155][312/390]\tTime 0.002 (0.003)\tLoss 1.2328 (1.0259)\tPrec@1 55.469 (63.386)\n",
      "Epoch: [155][390/390]\tTime 0.001 (0.003)\tLoss 0.9871 (1.0434)\tPrec@1 66.250 (62.756)\n",
      "EPOCH: 155 train Results: Prec@1 62.756 Loss: 1.0434\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1525 (1.1525)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3506 (1.2752)\tPrec@1 37.500 (54.860)\n",
      "EPOCH: 155 val Results: Prec@1 54.860 Loss: 1.2752\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/390]\tTime 0.003 (0.003)\tLoss 0.9833 (0.9833)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [156][78/390]\tTime 0.002 (0.003)\tLoss 0.9597 (0.9653)\tPrec@1 65.625 (64.745)\n",
      "Epoch: [156][156/390]\tTime 0.002 (0.003)\tLoss 1.2532 (0.9929)\tPrec@1 57.812 (64.117)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.003)\tLoss 1.0005 (1.0143)\tPrec@1 64.844 (63.418)\n",
      "Epoch: [156][312/390]\tTime 0.002 (0.003)\tLoss 1.1343 (1.0348)\tPrec@1 60.156 (62.725)\n",
      "Epoch: [156][390/390]\tTime 0.010 (0.003)\tLoss 1.0664 (1.0467)\tPrec@1 58.750 (62.420)\n",
      "EPOCH: 156 train Results: Prec@1 62.420 Loss: 1.0467\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1723 (1.1723)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5726 (1.2541)\tPrec@1 50.000 (55.360)\n",
      "EPOCH: 156 val Results: Prec@1 55.360 Loss: 1.2541\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/390]\tTime 0.003 (0.003)\tLoss 1.1631 (1.1631)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [157][78/390]\tTime 0.008 (0.003)\tLoss 0.9838 (0.9671)\tPrec@1 59.375 (65.912)\n",
      "Epoch: [157][156/390]\tTime 0.002 (0.003)\tLoss 0.8294 (0.9881)\tPrec@1 67.969 (64.988)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.0183 (1.0123)\tPrec@1 62.500 (63.833)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.004)\tLoss 1.0516 (1.0275)\tPrec@1 64.844 (63.219)\n",
      "Epoch: [157][390/390]\tTime 0.008 (0.003)\tLoss 1.1160 (1.0368)\tPrec@1 60.000 (62.976)\n",
      "EPOCH: 157 train Results: Prec@1 62.976 Loss: 1.0368\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1468 (1.1468)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3970 (1.2591)\tPrec@1 43.750 (55.360)\n",
      "EPOCH: 157 val Results: Prec@1 55.360 Loss: 1.2591\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/390]\tTime 0.005 (0.005)\tLoss 0.9229 (0.9229)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [158][78/390]\tTime 0.004 (0.003)\tLoss 1.0895 (0.9698)\tPrec@1 61.719 (65.328)\n",
      "Epoch: [158][156/390]\tTime 0.003 (0.004)\tLoss 1.1152 (0.9942)\tPrec@1 66.406 (64.655)\n",
      "Epoch: [158][234/390]\tTime 0.002 (0.004)\tLoss 0.9208 (1.0167)\tPrec@1 68.750 (63.820)\n",
      "Epoch: [158][312/390]\tTime 0.002 (0.003)\tLoss 1.1618 (1.0328)\tPrec@1 57.812 (63.166)\n",
      "Epoch: [158][390/390]\tTime 0.001 (0.003)\tLoss 1.1040 (1.0457)\tPrec@1 62.500 (62.790)\n",
      "EPOCH: 158 train Results: Prec@1 62.790 Loss: 1.0457\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0743 (1.0743)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0856 (1.2540)\tPrec@1 68.750 (55.420)\n",
      "EPOCH: 158 val Results: Prec@1 55.420 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/390]\tTime 0.004 (0.004)\tLoss 0.8832 (0.8832)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [159][78/390]\tTime 0.002 (0.003)\tLoss 1.0089 (0.9702)\tPrec@1 61.719 (65.536)\n",
      "Epoch: [159][156/390]\tTime 0.003 (0.003)\tLoss 0.9773 (0.9899)\tPrec@1 67.188 (64.431)\n",
      "Epoch: [159][234/390]\tTime 0.002 (0.003)\tLoss 0.9889 (1.0100)\tPrec@1 69.531 (63.797)\n",
      "Epoch: [159][312/390]\tTime 0.002 (0.003)\tLoss 1.0462 (1.0263)\tPrec@1 61.719 (63.306)\n",
      "Epoch: [159][390/390]\tTime 0.056 (0.003)\tLoss 1.1535 (1.0385)\tPrec@1 62.500 (62.852)\n",
      "EPOCH: 159 train Results: Prec@1 62.852 Loss: 1.0385\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1455 (1.1455)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.6612 (1.2542)\tPrec@1 37.500 (55.410)\n",
      "EPOCH: 159 val Results: Prec@1 55.410 Loss: 1.2542\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/390]\tTime 0.002 (0.002)\tLoss 0.8923 (0.8923)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [160][78/390]\tTime 0.002 (0.003)\tLoss 1.0480 (0.9943)\tPrec@1 62.500 (64.814)\n",
      "Epoch: [160][156/390]\tTime 0.006 (0.003)\tLoss 0.9889 (1.0003)\tPrec@1 67.188 (64.585)\n",
      "Epoch: [160][234/390]\tTime 0.005 (0.003)\tLoss 0.9305 (1.0150)\tPrec@1 63.281 (64.023)\n",
      "Epoch: [160][312/390]\tTime 0.003 (0.003)\tLoss 0.9514 (1.0298)\tPrec@1 60.156 (63.548)\n",
      "Epoch: [160][390/390]\tTime 0.002 (0.003)\tLoss 0.9708 (1.0347)\tPrec@1 66.250 (63.348)\n",
      "EPOCH: 160 train Results: Prec@1 63.348 Loss: 1.0347\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0861 (1.0861)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.0772 (1.2633)\tPrec@1 56.250 (55.640)\n",
      "EPOCH: 160 val Results: Prec@1 55.640 Loss: 1.2633\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/390]\tTime 0.003 (0.003)\tLoss 0.9541 (0.9541)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [161][78/390]\tTime 0.002 (0.003)\tLoss 0.9255 (0.9601)\tPrec@1 67.969 (65.546)\n",
      "Epoch: [161][156/390]\tTime 0.002 (0.003)\tLoss 1.0636 (0.9898)\tPrec@1 61.719 (64.605)\n",
      "Epoch: [161][234/390]\tTime 0.003 (0.003)\tLoss 1.0757 (1.0124)\tPrec@1 57.812 (63.763)\n",
      "Epoch: [161][312/390]\tTime 0.002 (0.003)\tLoss 1.0445 (1.0275)\tPrec@1 63.281 (63.274)\n",
      "Epoch: [161][390/390]\tTime 0.001 (0.003)\tLoss 1.3551 (1.0414)\tPrec@1 51.250 (62.740)\n",
      "EPOCH: 161 train Results: Prec@1 62.740 Loss: 1.0414\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1011 (1.1011)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9777 (1.2394)\tPrec@1 56.250 (56.190)\n",
      "EPOCH: 161 val Results: Prec@1 56.190 Loss: 1.2394\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.8686 (0.8686)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [162][78/390]\tTime 0.002 (0.003)\tLoss 1.1031 (0.9683)\tPrec@1 64.062 (65.566)\n",
      "Epoch: [162][156/390]\tTime 0.004 (0.003)\tLoss 0.9650 (0.9976)\tPrec@1 64.844 (64.426)\n",
      "Epoch: [162][234/390]\tTime 0.010 (0.003)\tLoss 1.0176 (1.0142)\tPrec@1 61.719 (63.797)\n",
      "Epoch: [162][312/390]\tTime 0.005 (0.003)\tLoss 0.9812 (1.0233)\tPrec@1 68.750 (63.406)\n",
      "Epoch: [162][390/390]\tTime 0.001 (0.003)\tLoss 1.0465 (1.0380)\tPrec@1 61.250 (62.884)\n",
      "EPOCH: 162 train Results: Prec@1 62.884 Loss: 1.0380\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1441 (1.1441)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.5393 (1.2560)\tPrec@1 37.500 (55.840)\n",
      "EPOCH: 162 val Results: Prec@1 55.840 Loss: 1.2560\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/390]\tTime 0.005 (0.005)\tLoss 0.9694 (0.9694)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [163][78/390]\tTime 0.002 (0.003)\tLoss 0.9485 (0.9602)\tPrec@1 60.938 (65.991)\n",
      "Epoch: [163][156/390]\tTime 0.005 (0.003)\tLoss 0.9904 (0.9834)\tPrec@1 59.375 (65.028)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.003)\tLoss 0.9693 (1.0145)\tPrec@1 64.062 (63.823)\n",
      "Epoch: [163][312/390]\tTime 0.008 (0.003)\tLoss 1.0528 (1.0293)\tPrec@1 65.625 (63.239)\n",
      "Epoch: [163][390/390]\tTime 0.002 (0.003)\tLoss 1.0161 (1.0421)\tPrec@1 62.500 (62.860)\n",
      "EPOCH: 163 train Results: Prec@1 62.860 Loss: 1.0421\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1744 (1.1744)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9451 (1.2603)\tPrec@1 56.250 (56.150)\n",
      "EPOCH: 163 val Results: Prec@1 56.150 Loss: 1.2603\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/390]\tTime 0.004 (0.004)\tLoss 1.0625 (1.0625)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [164][78/390]\tTime 0.003 (0.003)\tLoss 1.0075 (0.9629)\tPrec@1 65.625 (66.080)\n",
      "Epoch: [164][156/390]\tTime 0.002 (0.003)\tLoss 1.0998 (0.9887)\tPrec@1 58.594 (64.884)\n",
      "Epoch: [164][234/390]\tTime 0.002 (0.003)\tLoss 1.0351 (1.0071)\tPrec@1 61.719 (64.076)\n",
      "Epoch: [164][312/390]\tTime 0.004 (0.003)\tLoss 1.2434 (1.0277)\tPrec@1 54.688 (63.274)\n",
      "Epoch: [164][390/390]\tTime 0.029 (0.003)\tLoss 1.2739 (1.0386)\tPrec@1 58.750 (62.882)\n",
      "EPOCH: 164 train Results: Prec@1 62.882 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.1496 (1.1496)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1512 (1.2575)\tPrec@1 50.000 (56.150)\n",
      "EPOCH: 164 val Results: Prec@1 56.150 Loss: 1.2575\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/390]\tTime 0.002 (0.002)\tLoss 1.1056 (1.1056)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [165][78/390]\tTime 0.002 (0.004)\tLoss 0.9257 (0.9413)\tPrec@1 69.531 (66.762)\n",
      "Epoch: [165][156/390]\tTime 0.002 (0.004)\tLoss 1.0825 (0.9834)\tPrec@1 58.594 (65.182)\n",
      "Epoch: [165][234/390]\tTime 0.004 (0.004)\tLoss 1.2299 (1.0073)\tPrec@1 56.250 (64.262)\n",
      "Epoch: [165][312/390]\tTime 0.002 (0.004)\tLoss 0.9964 (1.0268)\tPrec@1 66.406 (63.488)\n",
      "Epoch: [165][390/390]\tTime 0.002 (0.004)\tLoss 1.1832 (1.0409)\tPrec@1 60.000 (62.902)\n",
      "EPOCH: 165 train Results: Prec@1 62.902 Loss: 1.0409\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1954 (1.1954)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1757 (1.2632)\tPrec@1 50.000 (55.270)\n",
      "EPOCH: 165 val Results: Prec@1 55.270 Loss: 1.2632\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/390]\tTime 0.003 (0.003)\tLoss 1.0573 (1.0573)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 0.9817 (0.9603)\tPrec@1 67.188 (65.803)\n",
      "Epoch: [166][156/390]\tTime 0.008 (0.003)\tLoss 1.0635 (0.9875)\tPrec@1 64.844 (64.495)\n",
      "Epoch: [166][234/390]\tTime 0.002 (0.003)\tLoss 1.1614 (1.0132)\tPrec@1 58.594 (63.517)\n",
      "Epoch: [166][312/390]\tTime 0.002 (0.003)\tLoss 1.1649 (1.0281)\tPrec@1 62.500 (63.097)\n",
      "Epoch: [166][390/390]\tTime 0.001 (0.003)\tLoss 1.0196 (1.0415)\tPrec@1 65.000 (62.708)\n",
      "EPOCH: 166 train Results: Prec@1 62.708 Loss: 1.0415\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1410 (1.1410)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1646 (1.2630)\tPrec@1 56.250 (54.990)\n",
      "EPOCH: 166 val Results: Prec@1 54.990 Loss: 1.2630\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/390]\tTime 0.003 (0.003)\tLoss 1.0606 (1.0606)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [167][78/390]\tTime 0.007 (0.003)\tLoss 1.0835 (0.9738)\tPrec@1 61.719 (65.595)\n",
      "Epoch: [167][156/390]\tTime 0.002 (0.005)\tLoss 0.9967 (0.9954)\tPrec@1 72.656 (64.779)\n",
      "Epoch: [167][234/390]\tTime 0.004 (0.004)\tLoss 1.0381 (1.0199)\tPrec@1 63.281 (63.930)\n",
      "Epoch: [167][312/390]\tTime 0.008 (0.004)\tLoss 1.0867 (1.0309)\tPrec@1 57.031 (63.458)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.004)\tLoss 1.0525 (1.0437)\tPrec@1 66.250 (63.020)\n",
      "EPOCH: 167 train Results: Prec@1 63.020 Loss: 1.0437\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1413 (1.1413)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0818 (1.2648)\tPrec@1 37.500 (54.970)\n",
      "EPOCH: 167 val Results: Prec@1 54.970 Loss: 1.2648\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/390]\tTime 0.002 (0.002)\tLoss 0.9201 (0.9201)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [168][78/390]\tTime 0.005 (0.003)\tLoss 0.9706 (0.9535)\tPrec@1 66.406 (66.307)\n",
      "Epoch: [168][156/390]\tTime 0.002 (0.003)\tLoss 0.9558 (0.9846)\tPrec@1 64.844 (65.043)\n",
      "Epoch: [168][234/390]\tTime 0.003 (0.003)\tLoss 1.0849 (1.0090)\tPrec@1 59.375 (63.993)\n",
      "Epoch: [168][312/390]\tTime 0.009 (0.004)\tLoss 1.0765 (1.0320)\tPrec@1 59.375 (63.181)\n",
      "Epoch: [168][390/390]\tTime 0.002 (0.004)\tLoss 1.1060 (1.0446)\tPrec@1 57.500 (62.666)\n",
      "EPOCH: 168 train Results: Prec@1 62.666 Loss: 1.0446\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0970 (1.0970)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3915 (1.2582)\tPrec@1 37.500 (54.840)\n",
      "EPOCH: 168 val Results: Prec@1 54.840 Loss: 1.2582\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/390]\tTime 0.005 (0.005)\tLoss 0.9263 (0.9263)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [169][78/390]\tTime 0.002 (0.003)\tLoss 1.0027 (0.9684)\tPrec@1 62.500 (65.615)\n",
      "Epoch: [169][156/390]\tTime 0.005 (0.003)\tLoss 0.9978 (0.9858)\tPrec@1 57.812 (64.953)\n",
      "Epoch: [169][234/390]\tTime 0.003 (0.003)\tLoss 0.9767 (1.0105)\tPrec@1 66.406 (63.963)\n",
      "Epoch: [169][312/390]\tTime 0.026 (0.003)\tLoss 1.0819 (1.0301)\tPrec@1 57.031 (63.276)\n",
      "Epoch: [169][390/390]\tTime 0.003 (0.004)\tLoss 0.9615 (1.0426)\tPrec@1 65.000 (62.772)\n",
      "EPOCH: 169 train Results: Prec@1 62.772 Loss: 1.0426\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1625 (1.1625)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1572 (1.2659)\tPrec@1 56.250 (55.320)\n",
      "EPOCH: 169 val Results: Prec@1 55.320 Loss: 1.2659\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/390]\tTime 0.005 (0.005)\tLoss 1.0642 (1.0642)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [170][78/390]\tTime 0.003 (0.004)\tLoss 1.0309 (0.9646)\tPrec@1 60.156 (65.289)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.003)\tLoss 0.8946 (0.9901)\tPrec@1 64.062 (64.421)\n",
      "Epoch: [170][234/390]\tTime 0.002 (0.004)\tLoss 1.2221 (1.0105)\tPrec@1 61.719 (63.667)\n",
      "Epoch: [170][312/390]\tTime 0.003 (0.004)\tLoss 0.9918 (1.0303)\tPrec@1 63.281 (63.064)\n",
      "Epoch: [170][390/390]\tTime 0.001 (0.004)\tLoss 0.9428 (1.0386)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 170 train Results: Prec@1 62.850 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1521 (1.1521)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0672 (1.2534)\tPrec@1 50.000 (55.990)\n",
      "EPOCH: 170 val Results: Prec@1 55.990 Loss: 1.2534\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/390]\tTime 0.002 (0.002)\tLoss 0.9359 (0.9359)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [171][78/390]\tTime 0.003 (0.003)\tLoss 0.9667 (0.9650)\tPrec@1 61.719 (65.971)\n",
      "Epoch: [171][156/390]\tTime 0.009 (0.003)\tLoss 1.0672 (0.9942)\tPrec@1 63.281 (64.884)\n",
      "Epoch: [171][234/390]\tTime 0.011 (0.003)\tLoss 0.9533 (1.0139)\tPrec@1 64.062 (64.026)\n",
      "Epoch: [171][312/390]\tTime 0.003 (0.003)\tLoss 0.9727 (1.0269)\tPrec@1 68.750 (63.693)\n",
      "Epoch: [171][390/390]\tTime 0.004 (0.003)\tLoss 1.2712 (1.0398)\tPrec@1 57.500 (63.082)\n",
      "EPOCH: 171 train Results: Prec@1 63.082 Loss: 1.0398\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1526 (1.1526)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3498 (1.2671)\tPrec@1 31.250 (55.290)\n",
      "EPOCH: 171 val Results: Prec@1 55.290 Loss: 1.2671\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/390]\tTime 0.003 (0.003)\tLoss 0.8262 (0.8262)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [172][78/390]\tTime 0.002 (0.003)\tLoss 0.9509 (0.9558)\tPrec@1 66.406 (66.258)\n",
      "Epoch: [172][156/390]\tTime 0.003 (0.003)\tLoss 0.9302 (0.9860)\tPrec@1 64.844 (64.953)\n",
      "Epoch: [172][234/390]\tTime 0.002 (0.003)\tLoss 1.0324 (1.0095)\tPrec@1 61.719 (64.102)\n",
      "Epoch: [172][312/390]\tTime 0.003 (0.003)\tLoss 1.2029 (1.0288)\tPrec@1 60.156 (63.274)\n",
      "Epoch: [172][390/390]\tTime 0.001 (0.003)\tLoss 1.1008 (1.0412)\tPrec@1 62.500 (62.804)\n",
      "EPOCH: 172 train Results: Prec@1 62.804 Loss: 1.0412\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1372 (1.1372)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3897 (1.2439)\tPrec@1 37.500 (55.830)\n",
      "EPOCH: 172 val Results: Prec@1 55.830 Loss: 1.2439\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/390]\tTime 0.005 (0.005)\tLoss 1.0715 (1.0715)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [173][78/390]\tTime 0.003 (0.003)\tLoss 1.1467 (0.9815)\tPrec@1 60.156 (65.388)\n",
      "Epoch: [173][156/390]\tTime 0.087 (0.003)\tLoss 1.1837 (1.0014)\tPrec@1 58.594 (64.456)\n",
      "Epoch: [173][234/390]\tTime 0.003 (0.003)\tLoss 1.0020 (1.0120)\tPrec@1 64.844 (64.116)\n",
      "Epoch: [173][312/390]\tTime 0.002 (0.003)\tLoss 1.0244 (1.0303)\tPrec@1 60.938 (63.304)\n",
      "Epoch: [173][390/390]\tTime 0.003 (0.003)\tLoss 1.0039 (1.0441)\tPrec@1 58.750 (62.878)\n",
      "EPOCH: 173 train Results: Prec@1 62.878 Loss: 1.0441\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1869 (1.1869)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3126 (1.2601)\tPrec@1 31.250 (55.140)\n",
      "EPOCH: 173 val Results: Prec@1 55.140 Loss: 1.2601\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/390]\tTime 0.003 (0.003)\tLoss 0.8756 (0.8756)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [174][78/390]\tTime 0.002 (0.004)\tLoss 0.8887 (0.9568)\tPrec@1 71.094 (65.536)\n",
      "Epoch: [174][156/390]\tTime 0.002 (0.004)\tLoss 0.9676 (0.9892)\tPrec@1 60.938 (64.227)\n",
      "Epoch: [174][234/390]\tTime 0.002 (0.004)\tLoss 0.9939 (1.0075)\tPrec@1 68.750 (63.680)\n",
      "Epoch: [174][312/390]\tTime 0.002 (0.004)\tLoss 1.0733 (1.0225)\tPrec@1 64.844 (63.161)\n",
      "Epoch: [174][390/390]\tTime 0.001 (0.003)\tLoss 1.1157 (1.0400)\tPrec@1 58.750 (62.586)\n",
      "EPOCH: 174 train Results: Prec@1 62.586 Loss: 1.0400\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1234 (1.1234)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3508 (1.2568)\tPrec@1 43.750 (55.830)\n",
      "EPOCH: 174 val Results: Prec@1 55.830 Loss: 1.2568\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/390]\tTime 0.004 (0.004)\tLoss 0.9405 (0.9405)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [175][78/390]\tTime 0.002 (0.003)\tLoss 1.1308 (0.9747)\tPrec@1 61.719 (65.684)\n",
      "Epoch: [175][156/390]\tTime 0.003 (0.003)\tLoss 1.0733 (0.9950)\tPrec@1 60.156 (64.933)\n",
      "Epoch: [175][234/390]\tTime 0.009 (0.003)\tLoss 1.1571 (1.0136)\tPrec@1 57.812 (64.062)\n",
      "Epoch: [175][312/390]\tTime 0.005 (0.004)\tLoss 1.1721 (1.0262)\tPrec@1 57.031 (63.618)\n",
      "Epoch: [175][390/390]\tTime 0.002 (0.003)\tLoss 1.1606 (1.0405)\tPrec@1 57.500 (62.988)\n",
      "EPOCH: 175 train Results: Prec@1 62.988 Loss: 1.0405\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2057 (1.2057)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9283 (1.2571)\tPrec@1 62.500 (55.720)\n",
      "EPOCH: 175 val Results: Prec@1 55.720 Loss: 1.2571\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/390]\tTime 0.002 (0.002)\tLoss 1.0280 (1.0280)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [176][78/390]\tTime 0.002 (0.004)\tLoss 1.1981 (0.9637)\tPrec@1 64.062 (66.119)\n",
      "Epoch: [176][156/390]\tTime 0.002 (0.004)\tLoss 1.0241 (0.9952)\tPrec@1 61.719 (64.620)\n",
      "Epoch: [176][234/390]\tTime 0.004 (0.004)\tLoss 1.0635 (1.0163)\tPrec@1 66.406 (63.770)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.004)\tLoss 0.9901 (1.0298)\tPrec@1 65.625 (63.161)\n",
      "Epoch: [176][390/390]\tTime 0.001 (0.004)\tLoss 1.1216 (1.0397)\tPrec@1 57.500 (62.768)\n",
      "EPOCH: 176 train Results: Prec@1 62.768 Loss: 1.0397\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1012 (1.1012)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3045 (1.2630)\tPrec@1 31.250 (55.470)\n",
      "EPOCH: 176 val Results: Prec@1 55.470 Loss: 1.2630\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/390]\tTime 0.002 (0.002)\tLoss 0.9615 (0.9615)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [177][78/390]\tTime 0.003 (0.003)\tLoss 0.8913 (0.9604)\tPrec@1 72.656 (65.892)\n",
      "Epoch: [177][156/390]\tTime 0.002 (0.003)\tLoss 1.0596 (0.9974)\tPrec@1 57.812 (64.301)\n",
      "Epoch: [177][234/390]\tTime 0.009 (0.003)\tLoss 1.0258 (1.0140)\tPrec@1 61.719 (63.737)\n",
      "Epoch: [177][312/390]\tTime 0.002 (0.003)\tLoss 1.1249 (1.0330)\tPrec@1 58.594 (63.139)\n",
      "Epoch: [177][390/390]\tTime 0.001 (0.003)\tLoss 1.0529 (1.0403)\tPrec@1 66.250 (62.958)\n",
      "EPOCH: 177 train Results: Prec@1 62.958 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2380 (1.2380)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2157 (1.2621)\tPrec@1 56.250 (55.770)\n",
      "EPOCH: 177 val Results: Prec@1 55.770 Loss: 1.2621\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/390]\tTime 0.004 (0.004)\tLoss 0.9500 (0.9500)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [178][78/390]\tTime 0.006 (0.003)\tLoss 1.0915 (0.9695)\tPrec@1 59.375 (65.813)\n",
      "Epoch: [178][156/390]\tTime 0.003 (0.003)\tLoss 1.0604 (0.9924)\tPrec@1 59.375 (64.555)\n",
      "Epoch: [178][234/390]\tTime 0.005 (0.003)\tLoss 1.1284 (1.0165)\tPrec@1 62.500 (63.577)\n",
      "Epoch: [178][312/390]\tTime 0.002 (0.003)\tLoss 1.0617 (1.0315)\tPrec@1 58.594 (63.159)\n",
      "Epoch: [178][390/390]\tTime 0.003 (0.003)\tLoss 1.0887 (1.0417)\tPrec@1 60.000 (62.774)\n",
      "EPOCH: 178 train Results: Prec@1 62.774 Loss: 1.0417\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0856 (1.0856)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3111 (1.2658)\tPrec@1 50.000 (55.440)\n",
      "EPOCH: 178 val Results: Prec@1 55.440 Loss: 1.2658\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/390]\tTime 0.006 (0.006)\tLoss 1.0824 (1.0824)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [179][78/390]\tTime 0.004 (0.003)\tLoss 0.9673 (0.9579)\tPrec@1 63.281 (66.129)\n",
      "Epoch: [179][156/390]\tTime 0.003 (0.003)\tLoss 0.9908 (0.9860)\tPrec@1 64.844 (64.933)\n",
      "Epoch: [179][234/390]\tTime 0.002 (0.003)\tLoss 1.2443 (1.0125)\tPrec@1 57.031 (63.813)\n",
      "Epoch: [179][312/390]\tTime 0.003 (0.003)\tLoss 1.0227 (1.0253)\tPrec@1 60.938 (63.341)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.4208 (1.0395)\tPrec@1 50.000 (62.872)\n",
      "EPOCH: 179 train Results: Prec@1 62.872 Loss: 1.0395\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1275 (1.1275)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0097 (1.2590)\tPrec@1 62.500 (55.630)\n",
      "EPOCH: 179 val Results: Prec@1 55.630 Loss: 1.2590\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/390]\tTime 0.009 (0.009)\tLoss 1.0008 (1.0008)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [180][78/390]\tTime 0.002 (0.003)\tLoss 1.0472 (0.9675)\tPrec@1 58.594 (64.913)\n",
      "Epoch: [180][156/390]\tTime 0.067 (0.004)\tLoss 0.9197 (0.9893)\tPrec@1 67.188 (64.207)\n",
      "Epoch: [180][234/390]\tTime 0.002 (0.004)\tLoss 1.0607 (1.0130)\tPrec@1 61.719 (63.441)\n",
      "Epoch: [180][312/390]\tTime 0.002 (0.004)\tLoss 0.9945 (1.0305)\tPrec@1 64.844 (62.989)\n",
      "Epoch: [180][390/390]\tTime 0.003 (0.004)\tLoss 1.0556 (1.0413)\tPrec@1 62.500 (62.552)\n",
      "EPOCH: 180 train Results: Prec@1 62.552 Loss: 1.0413\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0998 (1.0998)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3197 (1.2742)\tPrec@1 50.000 (55.120)\n",
      "EPOCH: 180 val Results: Prec@1 55.120 Loss: 1.2742\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/390]\tTime 0.007 (0.007)\tLoss 0.9546 (0.9546)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [181][78/390]\tTime 0.002 (0.004)\tLoss 0.9079 (0.9498)\tPrec@1 64.844 (66.416)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.004)\tLoss 0.9227 (0.9772)\tPrec@1 64.844 (65.132)\n",
      "Epoch: [181][234/390]\tTime 0.003 (0.004)\tLoss 1.0000 (1.0033)\tPrec@1 62.500 (64.315)\n",
      "Epoch: [181][312/390]\tTime 0.002 (0.004)\tLoss 1.1410 (1.0234)\tPrec@1 61.719 (63.703)\n",
      "Epoch: [181][390/390]\tTime 0.005 (0.004)\tLoss 1.2092 (1.0355)\tPrec@1 57.500 (63.256)\n",
      "EPOCH: 181 train Results: Prec@1 63.256 Loss: 1.0355\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0896 (1.0896)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1791 (1.2583)\tPrec@1 56.250 (55.710)\n",
      "EPOCH: 181 val Results: Prec@1 55.710 Loss: 1.2583\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/390]\tTime 0.006 (0.006)\tLoss 1.0062 (1.0062)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [182][78/390]\tTime 0.006 (0.003)\tLoss 0.9986 (0.9589)\tPrec@1 62.500 (65.783)\n",
      "Epoch: [182][156/390]\tTime 0.002 (0.003)\tLoss 1.0123 (0.9858)\tPrec@1 69.531 (64.874)\n",
      "Epoch: [182][234/390]\tTime 0.002 (0.003)\tLoss 1.1027 (1.0121)\tPrec@1 53.906 (63.813)\n",
      "Epoch: [182][312/390]\tTime 0.002 (0.003)\tLoss 1.0014 (1.0275)\tPrec@1 67.969 (63.366)\n",
      "Epoch: [182][390/390]\tTime 0.001 (0.003)\tLoss 1.2717 (1.0386)\tPrec@1 50.000 (62.818)\n",
      "EPOCH: 182 train Results: Prec@1 62.818 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1373 (1.1373)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2751 (1.2650)\tPrec@1 43.750 (55.010)\n",
      "EPOCH: 182 val Results: Prec@1 55.010 Loss: 1.2650\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/390]\tTime 0.004 (0.004)\tLoss 0.8945 (0.8945)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [183][78/390]\tTime 0.002 (0.003)\tLoss 1.0455 (0.9624)\tPrec@1 60.156 (65.773)\n",
      "Epoch: [183][156/390]\tTime 0.002 (0.003)\tLoss 0.9910 (0.9884)\tPrec@1 67.969 (64.645)\n",
      "Epoch: [183][234/390]\tTime 0.003 (0.003)\tLoss 1.0674 (1.0139)\tPrec@1 60.938 (63.670)\n",
      "Epoch: [183][312/390]\tTime 0.006 (0.004)\tLoss 0.9209 (1.0322)\tPrec@1 67.188 (63.014)\n",
      "Epoch: [183][390/390]\tTime 0.003 (0.004)\tLoss 1.2227 (1.0433)\tPrec@1 57.500 (62.580)\n",
      "EPOCH: 183 train Results: Prec@1 62.580 Loss: 1.0433\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1607 (1.1607)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4244 (1.2612)\tPrec@1 43.750 (55.160)\n",
      "EPOCH: 183 val Results: Prec@1 55.160 Loss: 1.2612\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/390]\tTime 0.006 (0.006)\tLoss 0.7851 (0.7851)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [184][78/390]\tTime 0.002 (0.003)\tLoss 1.1139 (0.9569)\tPrec@1 64.062 (66.466)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.003)\tLoss 1.1260 (0.9866)\tPrec@1 59.375 (65.063)\n",
      "Epoch: [184][234/390]\tTime 0.002 (0.003)\tLoss 1.2253 (1.0129)\tPrec@1 57.031 (64.122)\n",
      "Epoch: [184][312/390]\tTime 0.003 (0.003)\tLoss 1.1117 (1.0303)\tPrec@1 63.281 (63.468)\n",
      "Epoch: [184][390/390]\tTime 0.002 (0.003)\tLoss 1.3250 (1.0377)\tPrec@1 55.000 (63.236)\n",
      "EPOCH: 184 train Results: Prec@1 63.236 Loss: 1.0377\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0549 (1.0549)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1437 (1.2665)\tPrec@1 50.000 (55.680)\n",
      "EPOCH: 184 val Results: Prec@1 55.680 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/390]\tTime 0.003 (0.003)\tLoss 0.9262 (0.9262)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [185][78/390]\tTime 0.003 (0.003)\tLoss 1.1399 (0.9653)\tPrec@1 53.906 (65.655)\n",
      "Epoch: [185][156/390]\tTime 0.004 (0.003)\tLoss 0.8836 (0.9868)\tPrec@1 71.094 (64.764)\n",
      "Epoch: [185][234/390]\tTime 0.002 (0.003)\tLoss 1.1568 (1.0044)\tPrec@1 60.938 (64.212)\n",
      "Epoch: [185][312/390]\tTime 0.004 (0.003)\tLoss 0.9169 (1.0175)\tPrec@1 66.406 (63.606)\n",
      "Epoch: [185][390/390]\tTime 0.009 (0.003)\tLoss 1.1631 (1.0364)\tPrec@1 56.250 (63.002)\n",
      "EPOCH: 185 train Results: Prec@1 63.002 Loss: 1.0364\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1206 (1.1206)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0623 (1.2678)\tPrec@1 43.750 (54.900)\n",
      "EPOCH: 185 val Results: Prec@1 54.900 Loss: 1.2678\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/390]\tTime 0.002 (0.002)\tLoss 0.9338 (0.9338)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [186][78/390]\tTime 0.002 (0.004)\tLoss 0.9244 (0.9696)\tPrec@1 67.969 (65.902)\n",
      "Epoch: [186][156/390]\tTime 0.003 (0.003)\tLoss 1.0440 (0.9948)\tPrec@1 60.938 (64.515)\n",
      "Epoch: [186][234/390]\tTime 0.010 (0.003)\tLoss 1.0264 (1.0189)\tPrec@1 64.062 (63.564)\n",
      "Epoch: [186][312/390]\tTime 0.002 (0.003)\tLoss 1.1488 (1.0378)\tPrec@1 60.156 (62.957)\n",
      "Epoch: [186][390/390]\tTime 0.001 (0.003)\tLoss 0.9323 (1.0458)\tPrec@1 66.250 (62.668)\n",
      "EPOCH: 186 train Results: Prec@1 62.668 Loss: 1.0458\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0556 (1.0556)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3919 (1.2554)\tPrec@1 31.250 (55.460)\n",
      "EPOCH: 186 val Results: Prec@1 55.460 Loss: 1.2554\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/390]\tTime 0.003 (0.003)\tLoss 1.0179 (1.0179)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [187][78/390]\tTime 0.003 (0.003)\tLoss 0.8661 (0.9578)\tPrec@1 71.094 (66.406)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.2731 (0.9769)\tPrec@1 57.031 (65.431)\n",
      "Epoch: [187][234/390]\tTime 0.004 (0.003)\tLoss 1.0739 (1.0033)\tPrec@1 61.719 (64.412)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.0026 (1.0246)\tPrec@1 63.281 (63.623)\n",
      "Epoch: [187][390/390]\tTime 0.001 (0.003)\tLoss 1.2358 (1.0395)\tPrec@1 55.000 (63.058)\n",
      "EPOCH: 187 train Results: Prec@1 63.058 Loss: 1.0395\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1415 (1.1415)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2319 (1.2586)\tPrec@1 31.250 (55.810)\n",
      "EPOCH: 187 val Results: Prec@1 55.810 Loss: 1.2586\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/390]\tTime 0.012 (0.012)\tLoss 0.8565 (0.8565)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [188][78/390]\tTime 0.002 (0.003)\tLoss 1.2340 (0.9463)\tPrec@1 57.812 (66.466)\n",
      "Epoch: [188][156/390]\tTime 0.002 (0.003)\tLoss 1.2041 (0.9773)\tPrec@1 58.594 (65.197)\n",
      "Epoch: [188][234/390]\tTime 0.004 (0.003)\tLoss 1.1170 (0.9991)\tPrec@1 57.031 (64.229)\n",
      "Epoch: [188][312/390]\tTime 0.003 (0.003)\tLoss 1.2157 (1.0180)\tPrec@1 60.156 (63.638)\n",
      "Epoch: [188][390/390]\tTime 0.053 (0.003)\tLoss 1.1162 (1.0337)\tPrec@1 60.000 (63.126)\n",
      "EPOCH: 188 train Results: Prec@1 63.126 Loss: 1.0337\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0716 (1.0716)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1874 (1.2582)\tPrec@1 43.750 (55.330)\n",
      "EPOCH: 188 val Results: Prec@1 55.330 Loss: 1.2582\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/390]\tTime 0.003 (0.003)\tLoss 1.0638 (1.0638)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.003)\tLoss 0.9302 (0.9464)\tPrec@1 65.625 (66.831)\n",
      "Epoch: [189][156/390]\tTime 0.005 (0.003)\tLoss 1.1635 (0.9861)\tPrec@1 53.906 (65.103)\n",
      "Epoch: [189][234/390]\tTime 0.006 (0.004)\tLoss 1.1479 (1.0076)\tPrec@1 58.594 (64.342)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.004)\tLoss 1.1609 (1.0242)\tPrec@1 55.469 (63.616)\n",
      "Epoch: [189][390/390]\tTime 0.002 (0.004)\tLoss 1.2353 (1.0403)\tPrec@1 60.000 (63.100)\n",
      "EPOCH: 189 train Results: Prec@1 63.100 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0850 (1.0850)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3733 (1.2507)\tPrec@1 37.500 (55.800)\n",
      "EPOCH: 189 val Results: Prec@1 55.800 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/390]\tTime 0.002 (0.002)\tLoss 0.7902 (0.7902)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.003)\tLoss 0.9821 (0.9530)\tPrec@1 64.844 (65.961)\n",
      "Epoch: [190][156/390]\tTime 0.002 (0.003)\tLoss 0.9590 (0.9805)\tPrec@1 62.500 (64.744)\n",
      "Epoch: [190][234/390]\tTime 0.003 (0.003)\tLoss 0.9350 (1.0050)\tPrec@1 67.969 (63.906)\n",
      "Epoch: [190][312/390]\tTime 0.002 (0.003)\tLoss 1.0562 (1.0234)\tPrec@1 58.594 (63.301)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.003)\tLoss 1.2047 (1.0384)\tPrec@1 51.250 (62.862)\n",
      "EPOCH: 190 train Results: Prec@1 62.862 Loss: 1.0384\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.0754 (1.0754)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3310 (1.2581)\tPrec@1 37.500 (55.530)\n",
      "EPOCH: 190 val Results: Prec@1 55.530 Loss: 1.2581\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/390]\tTime 0.003 (0.003)\tLoss 1.0238 (1.0238)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.004)\tLoss 0.8718 (0.9624)\tPrec@1 70.312 (65.200)\n",
      "Epoch: [191][156/390]\tTime 0.004 (0.004)\tLoss 1.1122 (0.9889)\tPrec@1 57.812 (64.530)\n",
      "Epoch: [191][234/390]\tTime 0.003 (0.003)\tLoss 0.9893 (1.0103)\tPrec@1 64.062 (63.640)\n",
      "Epoch: [191][312/390]\tTime 0.005 (0.004)\tLoss 0.9334 (1.0239)\tPrec@1 66.406 (63.149)\n",
      "Epoch: [191][390/390]\tTime 0.002 (0.004)\tLoss 0.9803 (1.0404)\tPrec@1 60.000 (62.558)\n",
      "EPOCH: 191 train Results: Prec@1 62.558 Loss: 1.0404\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0181 (1.0181)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0932 (1.2414)\tPrec@1 37.500 (55.970)\n",
      "EPOCH: 191 val Results: Prec@1 55.970 Loss: 1.2414\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/390]\tTime 0.003 (0.003)\tLoss 0.8081 (0.8081)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [192][78/390]\tTime 0.002 (0.003)\tLoss 0.9439 (0.9649)\tPrec@1 60.938 (66.218)\n",
      "Epoch: [192][156/390]\tTime 0.002 (0.003)\tLoss 1.0101 (0.9939)\tPrec@1 67.188 (64.520)\n",
      "Epoch: [192][234/390]\tTime 0.003 (0.003)\tLoss 1.0538 (1.0166)\tPrec@1 65.625 (63.750)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.003)\tLoss 1.2486 (1.0294)\tPrec@1 60.156 (63.336)\n",
      "Epoch: [192][390/390]\tTime 0.001 (0.003)\tLoss 1.0758 (1.0418)\tPrec@1 61.250 (62.892)\n",
      "EPOCH: 192 train Results: Prec@1 62.892 Loss: 1.0418\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0804 (1.0804)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.2320 (1.2535)\tPrec@1 37.500 (55.650)\n",
      "EPOCH: 192 val Results: Prec@1 55.650 Loss: 1.2535\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/390]\tTime 0.002 (0.002)\tLoss 0.8078 (0.8078)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [193][78/390]\tTime 0.003 (0.003)\tLoss 0.9409 (0.9689)\tPrec@1 69.531 (65.902)\n",
      "Epoch: [193][156/390]\tTime 0.003 (0.003)\tLoss 1.1053 (0.9943)\tPrec@1 57.031 (64.694)\n",
      "Epoch: [193][234/390]\tTime 0.002 (0.003)\tLoss 1.0052 (1.0099)\tPrec@1 61.719 (64.016)\n",
      "Epoch: [193][312/390]\tTime 0.003 (0.004)\tLoss 1.2498 (1.0304)\tPrec@1 60.938 (63.276)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.004)\tLoss 1.0358 (1.0410)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 193 train Results: Prec@1 62.850 Loss: 1.0410\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0595 (1.0595)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2559 (1.2607)\tPrec@1 25.000 (55.140)\n",
      "EPOCH: 193 val Results: Prec@1 55.140 Loss: 1.2607\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/390]\tTime 0.003 (0.003)\tLoss 0.9384 (0.9384)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [194][78/390]\tTime 0.008 (0.004)\tLoss 0.9088 (0.9649)\tPrec@1 64.062 (65.635)\n",
      "Epoch: [194][156/390]\tTime 0.003 (0.004)\tLoss 0.9268 (0.9914)\tPrec@1 67.969 (64.719)\n",
      "Epoch: [194][234/390]\tTime 0.002 (0.003)\tLoss 1.0127 (1.0074)\tPrec@1 63.281 (64.126)\n",
      "Epoch: [194][312/390]\tTime 0.003 (0.003)\tLoss 1.0544 (1.0227)\tPrec@1 60.938 (63.661)\n",
      "Epoch: [194][390/390]\tTime 0.002 (0.003)\tLoss 1.0680 (1.0365)\tPrec@1 57.500 (63.264)\n",
      "EPOCH: 194 train Results: Prec@1 63.264 Loss: 1.0365\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1395 (1.1395)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3373 (1.2526)\tPrec@1 43.750 (55.260)\n",
      "EPOCH: 194 val Results: Prec@1 55.260 Loss: 1.2526\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/390]\tTime 0.004 (0.004)\tLoss 0.8456 (0.8456)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [195][78/390]\tTime 0.004 (0.003)\tLoss 0.9408 (0.9636)\tPrec@1 67.969 (65.585)\n",
      "Epoch: [195][156/390]\tTime 0.003 (0.003)\tLoss 0.9584 (0.9905)\tPrec@1 67.969 (64.515)\n",
      "Epoch: [195][234/390]\tTime 0.004 (0.003)\tLoss 1.0783 (1.0095)\tPrec@1 58.594 (63.873)\n",
      "Epoch: [195][312/390]\tTime 0.007 (0.003)\tLoss 1.1658 (1.0264)\tPrec@1 57.031 (63.344)\n",
      "Epoch: [195][390/390]\tTime 0.003 (0.003)\tLoss 1.1824 (1.0386)\tPrec@1 57.500 (62.882)\n",
      "EPOCH: 195 train Results: Prec@1 62.882 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0615 (1.0615)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3907 (1.2681)\tPrec@1 37.500 (55.180)\n",
      "EPOCH: 195 val Results: Prec@1 55.180 Loss: 1.2681\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/390]\tTime 0.003 (0.003)\tLoss 0.8733 (0.8733)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 0.9902 (0.9458)\tPrec@1 62.500 (66.307)\n",
      "Epoch: [196][156/390]\tTime 0.003 (0.003)\tLoss 1.0167 (0.9765)\tPrec@1 63.281 (65.192)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.003)\tLoss 0.9534 (1.0095)\tPrec@1 66.406 (64.079)\n",
      "Epoch: [196][312/390]\tTime 0.002 (0.003)\tLoss 1.0817 (1.0230)\tPrec@1 59.375 (63.621)\n",
      "Epoch: [196][390/390]\tTime 0.005 (0.003)\tLoss 0.9751 (1.0372)\tPrec@1 61.250 (63.076)\n",
      "EPOCH: 196 train Results: Prec@1 63.076 Loss: 1.0372\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1523 (1.1523)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3476 (1.2492)\tPrec@1 43.750 (55.790)\n",
      "EPOCH: 196 val Results: Prec@1 55.790 Loss: 1.2492\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/390]\tTime 0.003 (0.003)\tLoss 0.8293 (0.8293)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [197][78/390]\tTime 0.002 (0.003)\tLoss 0.9901 (0.9463)\tPrec@1 63.281 (66.426)\n",
      "Epoch: [197][156/390]\tTime 0.003 (0.003)\tLoss 1.1288 (0.9772)\tPrec@1 63.281 (65.341)\n",
      "Epoch: [197][234/390]\tTime 0.002 (0.003)\tLoss 1.0022 (1.0065)\tPrec@1 61.719 (64.365)\n",
      "Epoch: [197][312/390]\tTime 0.002 (0.003)\tLoss 1.0794 (1.0269)\tPrec@1 58.594 (63.563)\n",
      "Epoch: [197][390/390]\tTime 0.002 (0.003)\tLoss 0.9502 (1.0383)\tPrec@1 66.250 (63.214)\n",
      "EPOCH: 197 train Results: Prec@1 63.214 Loss: 1.0383\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1654 (1.1654)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3319 (1.2544)\tPrec@1 43.750 (55.090)\n",
      "EPOCH: 197 val Results: Prec@1 55.090 Loss: 1.2544\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/390]\tTime 0.002 (0.002)\tLoss 0.9726 (0.9726)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [198][78/390]\tTime 0.003 (0.003)\tLoss 0.9118 (0.9646)\tPrec@1 62.500 (65.259)\n",
      "Epoch: [198][156/390]\tTime 0.003 (0.003)\tLoss 1.0433 (0.9876)\tPrec@1 68.750 (64.381)\n",
      "Epoch: [198][234/390]\tTime 0.002 (0.003)\tLoss 1.3244 (1.0101)\tPrec@1 60.156 (63.531)\n",
      "Epoch: [198][312/390]\tTime 0.006 (0.003)\tLoss 0.9022 (1.0238)\tPrec@1 75.000 (63.112)\n",
      "Epoch: [198][390/390]\tTime 0.008 (0.003)\tLoss 0.9602 (1.0346)\tPrec@1 68.750 (62.922)\n",
      "EPOCH: 198 train Results: Prec@1 62.922 Loss: 1.0346\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1381 (1.1381)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1587 (1.2621)\tPrec@1 43.750 (55.140)\n",
      "EPOCH: 198 val Results: Prec@1 55.140 Loss: 1.2621\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/390]\tTime 0.004 (0.004)\tLoss 1.0403 (1.0403)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [199][78/390]\tTime 0.002 (0.003)\tLoss 0.9257 (0.9463)\tPrec@1 65.625 (66.515)\n",
      "Epoch: [199][156/390]\tTime 0.003 (0.003)\tLoss 1.0147 (0.9715)\tPrec@1 67.188 (65.525)\n",
      "Epoch: [199][234/390]\tTime 0.010 (0.003)\tLoss 1.0157 (1.0004)\tPrec@1 64.062 (64.405)\n",
      "Epoch: [199][312/390]\tTime 0.005 (0.003)\tLoss 1.1317 (1.0233)\tPrec@1 60.156 (63.486)\n",
      "Epoch: [199][390/390]\tTime 0.001 (0.003)\tLoss 1.0976 (1.0372)\tPrec@1 63.750 (62.988)\n",
      "EPOCH: 199 train Results: Prec@1 62.988 Loss: 1.0372\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0649 (1.0649)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2117 (1.2525)\tPrec@1 37.500 (55.640)\n",
      "EPOCH: 199 val Results: Prec@1 55.640 Loss: 1.2525\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/390]\tTime 0.003 (0.003)\tLoss 0.8200 (0.8200)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [200][78/390]\tTime 0.002 (0.004)\tLoss 1.0655 (0.9640)\tPrec@1 64.062 (65.566)\n",
      "Epoch: [200][156/390]\tTime 0.002 (0.003)\tLoss 0.7913 (0.9894)\tPrec@1 72.656 (64.859)\n",
      "Epoch: [200][234/390]\tTime 0.002 (0.003)\tLoss 1.0190 (1.0121)\tPrec@1 60.156 (63.790)\n",
      "Epoch: [200][312/390]\tTime 0.003 (0.004)\tLoss 1.0874 (1.0254)\tPrec@1 61.719 (63.429)\n",
      "Epoch: [200][390/390]\tTime 0.017 (0.004)\tLoss 1.3638 (1.0393)\tPrec@1 56.250 (63.014)\n",
      "EPOCH: 200 train Results: Prec@1 63.014 Loss: 1.0393\n",
      "Test: [0/78]\tTime 0.013 (0.013)\tLoss 1.1261 (1.1261)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0260 (1.2463)\tPrec@1 56.250 (55.920)\n",
      "EPOCH: 200 val Results: Prec@1 55.920 Loss: 1.2463\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "End time:  Thu Apr  4 23:00:15 2024\n",
      "train executed in 286.7670 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'scheduler': 'cos', # cos, None\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer2 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:00:15 2024\n",
      "current lr 1.00000e-03\n",
      "Epoch: [1][0/390]\tTime 0.006 (0.006)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.003 (0.004)\tLoss 4.2110 (4.8313)\tPrec@1 11.719 (12.233)\n",
      "Epoch: [1][156/390]\tTime 0.002 (0.003)\tLoss 3.7790 (4.2625)\tPrec@1 14.062 (14.734)\n",
      "Epoch: [1][234/390]\tTime 0.006 (0.003)\tLoss 2.8561 (3.9043)\tPrec@1 25.781 (16.702)\n",
      "Epoch: [1][312/390]\tTime 0.003 (0.003)\tLoss 2.9212 (3.6516)\tPrec@1 18.750 (18.136)\n",
      "Epoch: [1][390/390]\tTime 0.003 (0.003)\tLoss 2.4238 (3.4550)\tPrec@1 27.500 (19.672)\n",
      "EPOCH: 1 train Results: Prec@1 19.672 Loss: 3.4550\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 2.3102 (2.3102)\tPrec@1 29.688 (29.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 2.1503 (2.2151)\tPrec@1 25.000 (29.720)\n",
      "EPOCH: 1 val Results: Prec@1 29.720 Loss: 2.2151\n",
      "Best Prec@1: 29.720\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [2][0/390]\tTime 0.002 (0.002)\tLoss 2.5185 (2.5185)\tPrec@1 24.219 (24.219)\n",
      "Epoch: [2][78/390]\tTime 0.002 (0.003)\tLoss 2.1063 (2.4423)\tPrec@1 32.812 (27.027)\n",
      "Epoch: [2][156/390]\tTime 0.003 (0.003)\tLoss 2.2830 (2.3822)\tPrec@1 25.781 (28.598)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.003)\tLoss 2.5502 (2.3290)\tPrec@1 24.219 (29.129)\n",
      "Epoch: [2][312/390]\tTime 0.008 (0.003)\tLoss 2.3671 (2.2837)\tPrec@1 28.906 (29.620)\n",
      "Epoch: [2][390/390]\tTime 0.001 (0.003)\tLoss 1.9201 (2.2434)\tPrec@1 35.000 (30.060)\n",
      "EPOCH: 2 train Results: Prec@1 30.060 Loss: 2.2434\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.8905 (1.8905)\tPrec@1 32.812 (32.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7333 (1.8615)\tPrec@1 18.750 (35.990)\n",
      "EPOCH: 2 val Results: Prec@1 35.990 Loss: 1.8615\n",
      "Best Prec@1: 35.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [3][0/390]\tTime 0.004 (0.004)\tLoss 1.9196 (1.9196)\tPrec@1 28.906 (28.906)\n",
      "Epoch: [3][78/390]\tTime 0.002 (0.003)\tLoss 1.8936 (1.9947)\tPrec@1 31.250 (33.900)\n",
      "Epoch: [3][156/390]\tTime 0.008 (0.003)\tLoss 1.8343 (1.9638)\tPrec@1 34.375 (34.345)\n",
      "Epoch: [3][234/390]\tTime 0.003 (0.003)\tLoss 1.9050 (1.9446)\tPrec@1 31.250 (34.591)\n",
      "Epoch: [3][312/390]\tTime 0.002 (0.003)\tLoss 1.9053 (1.9284)\tPrec@1 35.156 (34.867)\n",
      "Epoch: [3][390/390]\tTime 0.009 (0.003)\tLoss 1.7400 (1.9093)\tPrec@1 32.500 (35.180)\n",
      "EPOCH: 3 train Results: Prec@1 35.180 Loss: 1.9093\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.7195 (1.7195)\tPrec@1 38.281 (38.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5686 (1.7150)\tPrec@1 31.250 (39.410)\n",
      "EPOCH: 3 val Results: Prec@1 39.410 Loss: 1.7150\n",
      "Best Prec@1: 39.410\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [4][0/390]\tTime 0.002 (0.002)\tLoss 1.8887 (1.8887)\tPrec@1 28.906 (28.906)\n",
      "Epoch: [4][78/390]\tTime 0.002 (0.003)\tLoss 1.8872 (1.7806)\tPrec@1 40.625 (37.846)\n",
      "Epoch: [4][156/390]\tTime 0.005 (0.003)\tLoss 1.7901 (1.7652)\tPrec@1 40.625 (37.973)\n",
      "Epoch: [4][234/390]\tTime 0.003 (0.003)\tLoss 1.8730 (1.7546)\tPrec@1 26.562 (38.162)\n",
      "Epoch: [4][312/390]\tTime 0.009 (0.003)\tLoss 1.6024 (1.7463)\tPrec@1 39.844 (38.396)\n",
      "Epoch: [4][390/390]\tTime 0.002 (0.003)\tLoss 1.5602 (1.7361)\tPrec@1 40.000 (38.588)\n",
      "EPOCH: 4 train Results: Prec@1 38.588 Loss: 1.7361\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.6037 (1.6037)\tPrec@1 44.531 (44.531)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5198 (1.6368)\tPrec@1 31.250 (41.960)\n",
      "EPOCH: 4 val Results: Prec@1 41.960 Loss: 1.6368\n",
      "Best Prec@1: 41.960\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [5][0/390]\tTime 0.002 (0.002)\tLoss 1.8193 (1.8193)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.003)\tLoss 1.7869 (1.6379)\tPrec@1 38.281 (41.614)\n",
      "Epoch: [5][156/390]\tTime 0.003 (0.003)\tLoss 1.7283 (1.6530)\tPrec@1 39.844 (41.182)\n",
      "Epoch: [5][234/390]\tTime 0.015 (0.003)\tLoss 1.6349 (1.6443)\tPrec@1 42.188 (41.656)\n",
      "Epoch: [5][312/390]\tTime 0.004 (0.003)\tLoss 1.6002 (1.6414)\tPrec@1 42.969 (41.818)\n",
      "Epoch: [5][390/390]\tTime 0.001 (0.003)\tLoss 1.5735 (1.6365)\tPrec@1 41.250 (42.068)\n",
      "EPOCH: 5 train Results: Prec@1 42.068 Loss: 1.6365\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.5390 (1.5390)\tPrec@1 46.094 (46.094)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4554 (1.5870)\tPrec@1 31.250 (43.780)\n",
      "EPOCH: 5 val Results: Prec@1 43.780 Loss: 1.5870\n",
      "Best Prec@1: 43.780\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [6][0/390]\tTime 0.002 (0.002)\tLoss 1.6425 (1.6425)\tPrec@1 44.531 (44.531)\n",
      "Epoch: [6][78/390]\tTime 0.002 (0.003)\tLoss 1.6826 (1.5690)\tPrec@1 39.062 (45.253)\n",
      "Epoch: [6][156/390]\tTime 0.002 (0.003)\tLoss 1.4346 (1.5711)\tPrec@1 44.531 (44.909)\n",
      "Epoch: [6][234/390]\tTime 0.009 (0.003)\tLoss 1.6878 (1.5721)\tPrec@1 42.969 (44.774)\n",
      "Epoch: [6][312/390]\tTime 0.002 (0.003)\tLoss 1.6759 (1.5733)\tPrec@1 39.844 (44.761)\n",
      "Epoch: [6][390/390]\tTime 0.003 (0.003)\tLoss 1.7868 (1.5715)\tPrec@1 31.250 (44.900)\n",
      "EPOCH: 6 train Results: Prec@1 44.900 Loss: 1.5715\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4983 (1.4983)\tPrec@1 50.000 (50.000)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4741 (1.5517)\tPrec@1 31.250 (45.220)\n",
      "EPOCH: 6 val Results: Prec@1 45.220 Loss: 1.5517\n",
      "Best Prec@1: 45.220\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [7][0/390]\tTime 0.005 (0.005)\tLoss 1.5970 (1.5970)\tPrec@1 44.531 (44.531)\n",
      "Epoch: [7][78/390]\tTime 0.002 (0.003)\tLoss 1.5572 (1.5335)\tPrec@1 44.531 (46.746)\n",
      "Epoch: [7][156/390]\tTime 0.002 (0.003)\tLoss 1.5695 (1.5323)\tPrec@1 45.312 (47.094)\n",
      "Epoch: [7][234/390]\tTime 0.005 (0.003)\tLoss 1.5319 (1.5291)\tPrec@1 42.969 (46.649)\n",
      "Epoch: [7][312/390]\tTime 0.002 (0.003)\tLoss 1.4742 (1.5259)\tPrec@1 53.125 (46.728)\n",
      "Epoch: [7][390/390]\tTime 0.001 (0.003)\tLoss 1.5525 (1.5271)\tPrec@1 53.750 (46.738)\n",
      "EPOCH: 7 train Results: Prec@1 46.738 Loss: 1.5271\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4678 (1.4678)\tPrec@1 47.656 (47.656)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4615 (1.5206)\tPrec@1 25.000 (46.670)\n",
      "EPOCH: 7 val Results: Prec@1 46.670 Loss: 1.5206\n",
      "Best Prec@1: 46.670\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [8][0/390]\tTime 0.002 (0.002)\tLoss 1.5043 (1.5043)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [8][78/390]\tTime 0.002 (0.003)\tLoss 1.5064 (1.4984)\tPrec@1 49.219 (48.517)\n",
      "Epoch: [8][156/390]\tTime 0.003 (0.003)\tLoss 1.5475 (1.5006)\tPrec@1 42.188 (48.308)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.003)\tLoss 1.4685 (1.4955)\tPrec@1 48.438 (48.394)\n",
      "Epoch: [8][312/390]\tTime 0.002 (0.003)\tLoss 1.3896 (1.4920)\tPrec@1 53.906 (48.585)\n",
      "Epoch: [8][390/390]\tTime 0.002 (0.003)\tLoss 1.6164 (1.4916)\tPrec@1 40.000 (48.540)\n",
      "EPOCH: 8 train Results: Prec@1 48.540 Loss: 1.4916\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.4392 (1.4392)\tPrec@1 50.000 (50.000)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4359 (1.4909)\tPrec@1 25.000 (47.700)\n",
      "EPOCH: 8 val Results: Prec@1 47.700 Loss: 1.4909\n",
      "Best Prec@1: 47.700\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [9][0/390]\tTime 0.002 (0.002)\tLoss 1.4724 (1.4724)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [9][78/390]\tTime 0.002 (0.003)\tLoss 1.3937 (1.4494)\tPrec@1 54.688 (50.188)\n",
      "Epoch: [9][156/390]\tTime 0.003 (0.003)\tLoss 1.5986 (1.4491)\tPrec@1 40.625 (50.269)\n",
      "Epoch: [9][234/390]\tTime 0.004 (0.003)\tLoss 1.5204 (1.4569)\tPrec@1 48.438 (49.797)\n",
      "Epoch: [9][312/390]\tTime 0.005 (0.003)\tLoss 1.4896 (1.4569)\tPrec@1 51.562 (49.790)\n",
      "Epoch: [9][390/390]\tTime 0.006 (0.003)\tLoss 1.5971 (1.4577)\tPrec@1 43.750 (49.650)\n",
      "EPOCH: 9 train Results: Prec@1 49.650 Loss: 1.4577\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4086 (1.4086)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4565 (1.4636)\tPrec@1 25.000 (48.540)\n",
      "EPOCH: 9 val Results: Prec@1 48.540 Loss: 1.4636\n",
      "Best Prec@1: 48.540\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [10][0/390]\tTime 0.004 (0.004)\tLoss 1.4356 (1.4356)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [10][78/390]\tTime 0.002 (0.003)\tLoss 1.3544 (1.4152)\tPrec@1 53.906 (51.444)\n",
      "Epoch: [10][156/390]\tTime 0.002 (0.003)\tLoss 1.4932 (1.4295)\tPrec@1 52.344 (50.766)\n",
      "Epoch: [10][234/390]\tTime 0.002 (0.003)\tLoss 1.4627 (1.4312)\tPrec@1 44.531 (50.509)\n",
      "Epoch: [10][312/390]\tTime 0.002 (0.003)\tLoss 1.4602 (1.4293)\tPrec@1 52.344 (50.604)\n",
      "Epoch: [10][390/390]\tTime 0.002 (0.003)\tLoss 1.2831 (1.4287)\tPrec@1 60.000 (50.622)\n",
      "EPOCH: 10 train Results: Prec@1 50.622 Loss: 1.4287\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.3934 (1.3934)\tPrec@1 50.000 (50.000)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5305 (1.4259)\tPrec@1 25.000 (49.650)\n",
      "EPOCH: 10 val Results: Prec@1 49.650 Loss: 1.4259\n",
      "Best Prec@1: 49.650\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [11][0/390]\tTime 0.002 (0.002)\tLoss 1.3277 (1.3277)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [11][78/390]\tTime 0.009 (0.003)\tLoss 1.3634 (1.3851)\tPrec@1 51.562 (52.245)\n",
      "Epoch: [11][156/390]\tTime 0.008 (0.003)\tLoss 1.3062 (1.3904)\tPrec@1 53.906 (51.971)\n",
      "Epoch: [11][234/390]\tTime 0.004 (0.003)\tLoss 1.4672 (1.3945)\tPrec@1 50.781 (51.745)\n",
      "Epoch: [11][312/390]\tTime 0.005 (0.003)\tLoss 1.2731 (1.3934)\tPrec@1 60.156 (51.934)\n",
      "Epoch: [11][390/390]\tTime 0.003 (0.003)\tLoss 1.4546 (1.3950)\tPrec@1 48.750 (51.734)\n",
      "EPOCH: 11 train Results: Prec@1 51.734 Loss: 1.3950\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3406 (1.3406)\tPrec@1 50.000 (50.000)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.5129 (1.4033)\tPrec@1 31.250 (50.710)\n",
      "EPOCH: 11 val Results: Prec@1 50.710 Loss: 1.4033\n",
      "Best Prec@1: 50.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [12][0/390]\tTime 0.008 (0.008)\tLoss 1.3373 (1.3373)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [12][78/390]\tTime 0.004 (0.004)\tLoss 1.2698 (1.3540)\tPrec@1 55.469 (53.610)\n",
      "Epoch: [12][156/390]\tTime 0.004 (0.004)\tLoss 1.4288 (1.3602)\tPrec@1 53.125 (53.210)\n",
      "Epoch: [12][234/390]\tTime 0.022 (0.003)\tLoss 1.2068 (1.3617)\tPrec@1 60.938 (52.872)\n",
      "Epoch: [12][312/390]\tTime 0.002 (0.003)\tLoss 1.3963 (1.3641)\tPrec@1 52.344 (52.636)\n",
      "Epoch: [12][390/390]\tTime 0.003 (0.003)\tLoss 1.2294 (1.3671)\tPrec@1 60.000 (52.564)\n",
      "EPOCH: 12 train Results: Prec@1 52.564 Loss: 1.3671\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.3086 (1.3086)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3157 (1.3765)\tPrec@1 43.750 (51.630)\n",
      "EPOCH: 12 val Results: Prec@1 51.630 Loss: 1.3765\n",
      "Best Prec@1: 51.630\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [13][0/390]\tTime 0.006 (0.006)\tLoss 1.3183 (1.3183)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.2663 (1.3308)\tPrec@1 52.344 (53.600)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.003)\tLoss 1.2375 (1.3293)\tPrec@1 57.031 (54.120)\n",
      "Epoch: [13][234/390]\tTime 0.003 (0.003)\tLoss 1.4448 (1.3365)\tPrec@1 45.312 (53.803)\n",
      "Epoch: [13][312/390]\tTime 0.003 (0.003)\tLoss 1.3267 (1.3425)\tPrec@1 51.562 (53.365)\n",
      "Epoch: [13][390/390]\tTime 0.001 (0.003)\tLoss 1.3160 (1.3450)\tPrec@1 48.750 (53.292)\n",
      "EPOCH: 13 train Results: Prec@1 53.292 Loss: 1.3450\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2861 (1.2861)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4394 (1.3613)\tPrec@1 37.500 (51.870)\n",
      "EPOCH: 13 val Results: Prec@1 51.870 Loss: 1.3613\n",
      "Best Prec@1: 51.870\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [14][0/390]\tTime 0.002 (0.002)\tLoss 1.2310 (1.2310)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [14][78/390]\tTime 0.002 (0.003)\tLoss 1.3282 (1.3080)\tPrec@1 53.906 (54.589)\n",
      "Epoch: [14][156/390]\tTime 0.002 (0.003)\tLoss 1.3704 (1.3132)\tPrec@1 52.344 (54.284)\n",
      "Epoch: [14][234/390]\tTime 0.003 (0.003)\tLoss 1.3695 (1.3230)\tPrec@1 53.125 (53.730)\n",
      "Epoch: [14][312/390]\tTime 0.003 (0.003)\tLoss 1.2319 (1.3249)\tPrec@1 53.125 (53.727)\n",
      "Epoch: [14][390/390]\tTime 0.001 (0.003)\tLoss 1.4837 (1.3281)\tPrec@1 48.750 (53.580)\n",
      "EPOCH: 14 train Results: Prec@1 53.580 Loss: 1.3281\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2465 (1.2465)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2850 (1.3485)\tPrec@1 37.500 (52.260)\n",
      "EPOCH: 14 val Results: Prec@1 52.260 Loss: 1.3485\n",
      "Best Prec@1: 52.260\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [15][0/390]\tTime 0.002 (0.002)\tLoss 1.2006 (1.2006)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [15][78/390]\tTime 0.002 (0.003)\tLoss 1.4104 (1.2808)\tPrec@1 49.219 (55.439)\n",
      "Epoch: [15][156/390]\tTime 0.002 (0.003)\tLoss 1.1833 (1.2915)\tPrec@1 59.375 (55.329)\n",
      "Epoch: [15][234/390]\tTime 0.004 (0.003)\tLoss 1.2644 (1.2999)\tPrec@1 59.375 (54.741)\n",
      "Epoch: [15][312/390]\tTime 0.002 (0.003)\tLoss 1.4677 (1.3059)\tPrec@1 47.656 (54.415)\n",
      "Epoch: [15][390/390]\tTime 0.001 (0.003)\tLoss 1.2634 (1.3085)\tPrec@1 53.750 (54.394)\n",
      "EPOCH: 15 train Results: Prec@1 54.394 Loss: 1.3085\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2043 (1.2043)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1997 (1.3327)\tPrec@1 43.750 (52.740)\n",
      "EPOCH: 15 val Results: Prec@1 52.740 Loss: 1.3327\n",
      "Best Prec@1: 52.740\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [16][0/390]\tTime 0.003 (0.003)\tLoss 1.1712 (1.1712)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.003)\tLoss 1.2143 (1.2475)\tPrec@1 58.594 (56.913)\n",
      "Epoch: [16][156/390]\tTime 0.002 (0.003)\tLoss 1.2929 (1.2697)\tPrec@1 53.906 (55.802)\n",
      "Epoch: [16][234/390]\tTime 0.007 (0.003)\tLoss 1.4332 (1.2828)\tPrec@1 48.438 (55.213)\n",
      "Epoch: [16][312/390]\tTime 0.007 (0.003)\tLoss 1.2888 (1.2899)\tPrec@1 53.906 (54.922)\n",
      "Epoch: [16][390/390]\tTime 0.003 (0.003)\tLoss 1.4975 (1.2973)\tPrec@1 52.500 (54.676)\n",
      "EPOCH: 16 train Results: Prec@1 54.676 Loss: 1.2973\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1723 (1.1723)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3152 (1.3287)\tPrec@1 37.500 (53.320)\n",
      "EPOCH: 16 val Results: Prec@1 53.320 Loss: 1.3287\n",
      "Best Prec@1: 53.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [17][0/390]\tTime 0.003 (0.003)\tLoss 1.2942 (1.2942)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [17][78/390]\tTime 0.003 (0.003)\tLoss 1.3306 (1.2544)\tPrec@1 53.125 (56.794)\n",
      "Epoch: [17][156/390]\tTime 0.002 (0.003)\tLoss 1.4411 (1.2647)\tPrec@1 49.219 (56.170)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.003)\tLoss 1.3188 (1.2739)\tPrec@1 54.688 (55.725)\n",
      "Epoch: [17][312/390]\tTime 0.006 (0.003)\tLoss 1.3304 (1.2783)\tPrec@1 53.906 (55.479)\n",
      "Epoch: [17][390/390]\tTime 0.001 (0.003)\tLoss 1.2118 (1.2848)\tPrec@1 63.750 (55.288)\n",
      "EPOCH: 17 train Results: Prec@1 55.288 Loss: 1.2848\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2122 (1.2122)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1694 (1.3196)\tPrec@1 37.500 (53.410)\n",
      "EPOCH: 17 val Results: Prec@1 53.410 Loss: 1.3196\n",
      "Best Prec@1: 53.410\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [18][0/390]\tTime 0.004 (0.004)\tLoss 1.1946 (1.1946)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [18][78/390]\tTime 0.002 (0.003)\tLoss 1.2744 (1.2453)\tPrec@1 55.469 (56.744)\n",
      "Epoch: [18][156/390]\tTime 0.002 (0.003)\tLoss 1.2000 (1.2583)\tPrec@1 61.719 (56.111)\n",
      "Epoch: [18][234/390]\tTime 0.002 (0.003)\tLoss 1.1433 (1.2749)\tPrec@1 59.375 (55.512)\n",
      "Epoch: [18][312/390]\tTime 0.007 (0.003)\tLoss 1.2714 (1.2771)\tPrec@1 57.812 (55.431)\n",
      "Epoch: [18][390/390]\tTime 0.011 (0.003)\tLoss 1.1708 (1.2796)\tPrec@1 61.250 (55.372)\n",
      "EPOCH: 18 train Results: Prec@1 55.372 Loss: 1.2796\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1808 (1.1808)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3140 (1.3071)\tPrec@1 50.000 (53.490)\n",
      "EPOCH: 18 val Results: Prec@1 53.490 Loss: 1.3071\n",
      "Best Prec@1: 53.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [19][0/390]\tTime 0.003 (0.003)\tLoss 1.2323 (1.2323)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [19][78/390]\tTime 0.004 (0.003)\tLoss 1.4044 (1.2215)\tPrec@1 50.000 (57.882)\n",
      "Epoch: [19][156/390]\tTime 0.003 (0.003)\tLoss 1.2105 (1.2456)\tPrec@1 57.812 (56.937)\n",
      "Epoch: [19][234/390]\tTime 0.002 (0.003)\tLoss 1.2609 (1.2515)\tPrec@1 56.250 (56.629)\n",
      "Epoch: [19][312/390]\tTime 0.003 (0.003)\tLoss 1.1265 (1.2633)\tPrec@1 60.938 (56.043)\n",
      "Epoch: [19][390/390]\tTime 0.001 (0.003)\tLoss 1.4561 (1.2682)\tPrec@1 48.750 (55.668)\n",
      "EPOCH: 19 train Results: Prec@1 55.668 Loss: 1.2682\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2142 (1.2142)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5328 (1.3055)\tPrec@1 25.000 (53.620)\n",
      "EPOCH: 19 val Results: Prec@1 53.620 Loss: 1.3055\n",
      "Best Prec@1: 53.620\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [20][0/390]\tTime 0.003 (0.003)\tLoss 1.2077 (1.2077)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [20][78/390]\tTime 0.003 (0.003)\tLoss 1.0808 (1.2389)\tPrec@1 58.594 (57.071)\n",
      "Epoch: [20][156/390]\tTime 0.009 (0.003)\tLoss 1.1797 (1.2440)\tPrec@1 54.688 (56.807)\n",
      "Epoch: [20][234/390]\tTime 0.004 (0.004)\tLoss 1.2548 (1.2563)\tPrec@1 56.250 (56.031)\n",
      "Epoch: [20][312/390]\tTime 0.010 (0.003)\tLoss 1.3231 (1.2590)\tPrec@1 52.344 (55.936)\n",
      "Epoch: [20][390/390]\tTime 0.003 (0.003)\tLoss 1.3673 (1.2616)\tPrec@1 53.750 (55.770)\n",
      "EPOCH: 20 train Results: Prec@1 55.770 Loss: 1.2616\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1719 (1.1719)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2444 (1.3001)\tPrec@1 50.000 (53.870)\n",
      "EPOCH: 20 val Results: Prec@1 53.870 Loss: 1.3001\n",
      "Best Prec@1: 53.870\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [21][0/390]\tTime 0.006 (0.006)\tLoss 1.3077 (1.3077)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [21][78/390]\tTime 0.006 (0.003)\tLoss 1.3507 (1.2134)\tPrec@1 52.344 (58.396)\n",
      "Epoch: [21][156/390]\tTime 0.002 (0.003)\tLoss 1.3618 (1.2306)\tPrec@1 50.781 (57.404)\n",
      "Epoch: [21][234/390]\tTime 0.002 (0.003)\tLoss 1.2522 (1.2420)\tPrec@1 58.594 (56.951)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.1937 (1.2531)\tPrec@1 60.156 (56.467)\n",
      "Epoch: [21][390/390]\tTime 0.002 (0.003)\tLoss 1.2527 (1.2596)\tPrec@1 60.000 (56.118)\n",
      "EPOCH: 21 train Results: Prec@1 56.118 Loss: 1.2596\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2377 (1.2377)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1476 (1.2950)\tPrec@1 56.250 (54.210)\n",
      "EPOCH: 21 val Results: Prec@1 54.210 Loss: 1.2950\n",
      "Best Prec@1: 54.210\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [22][0/390]\tTime 0.002 (0.002)\tLoss 1.2462 (1.2462)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [22][78/390]\tTime 0.002 (0.003)\tLoss 1.1894 (1.2203)\tPrec@1 60.156 (58.030)\n",
      "Epoch: [22][156/390]\tTime 0.008 (0.003)\tLoss 1.1672 (1.2292)\tPrec@1 55.469 (57.370)\n",
      "Epoch: [22][234/390]\tTime 0.004 (0.003)\tLoss 1.2370 (1.2383)\tPrec@1 57.031 (56.932)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.003)\tLoss 1.2323 (1.2462)\tPrec@1 61.719 (56.564)\n",
      "Epoch: [22][390/390]\tTime 0.002 (0.003)\tLoss 1.1614 (1.2540)\tPrec@1 62.500 (56.200)\n",
      "EPOCH: 22 train Results: Prec@1 56.200 Loss: 1.2540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1902 (1.1902)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1482 (1.2935)\tPrec@1 56.250 (54.250)\n",
      "EPOCH: 22 val Results: Prec@1 54.250 Loss: 1.2935\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [23][0/390]\tTime 0.003 (0.003)\tLoss 1.2041 (1.2041)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [23][78/390]\tTime 0.002 (0.003)\tLoss 1.3602 (1.2158)\tPrec@1 54.688 (58.050)\n",
      "Epoch: [23][156/390]\tTime 0.006 (0.003)\tLoss 1.4187 (1.2317)\tPrec@1 54.688 (57.414)\n",
      "Epoch: [23][234/390]\tTime 0.002 (0.003)\tLoss 1.1843 (1.2328)\tPrec@1 63.281 (57.330)\n",
      "Epoch: [23][312/390]\tTime 0.003 (0.003)\tLoss 1.3568 (1.2424)\tPrec@1 54.688 (56.941)\n",
      "Epoch: [23][390/390]\tTime 0.002 (0.003)\tLoss 1.3833 (1.2504)\tPrec@1 50.000 (56.522)\n",
      "EPOCH: 23 train Results: Prec@1 56.522 Loss: 1.2504\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1708 (1.1708)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3363 (1.3008)\tPrec@1 37.500 (53.650)\n",
      "EPOCH: 23 val Results: Prec@1 53.650 Loss: 1.3008\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [24][0/390]\tTime 0.002 (0.002)\tLoss 1.2027 (1.2027)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.2558 (1.1900)\tPrec@1 60.156 (59.009)\n",
      "Epoch: [24][156/390]\tTime 0.002 (0.003)\tLoss 1.2777 (1.2153)\tPrec@1 57.031 (57.837)\n",
      "Epoch: [24][234/390]\tTime 0.002 (0.003)\tLoss 1.3026 (1.2324)\tPrec@1 54.688 (57.241)\n",
      "Epoch: [24][312/390]\tTime 0.002 (0.003)\tLoss 1.3514 (1.2400)\tPrec@1 52.344 (56.919)\n",
      "Epoch: [24][390/390]\tTime 0.002 (0.003)\tLoss 1.3063 (1.2435)\tPrec@1 55.000 (56.728)\n",
      "EPOCH: 24 train Results: Prec@1 56.728 Loss: 1.2435\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1712 (1.1712)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2113 (1.2939)\tPrec@1 50.000 (53.840)\n",
      "EPOCH: 24 val Results: Prec@1 53.840 Loss: 1.2939\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [25][0/390]\tTime 0.003 (0.003)\tLoss 1.2126 (1.2126)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [25][78/390]\tTime 0.003 (0.003)\tLoss 1.1414 (1.2019)\tPrec@1 58.594 (58.248)\n",
      "Epoch: [25][156/390]\tTime 0.005 (0.003)\tLoss 1.2451 (1.2187)\tPrec@1 57.031 (57.633)\n",
      "Epoch: [25][234/390]\tTime 0.002 (0.003)\tLoss 1.3126 (1.2280)\tPrec@1 50.000 (57.111)\n",
      "Epoch: [25][312/390]\tTime 0.003 (0.003)\tLoss 1.3784 (1.2355)\tPrec@1 50.781 (56.817)\n",
      "Epoch: [25][390/390]\tTime 0.004 (0.003)\tLoss 1.3782 (1.2395)\tPrec@1 48.750 (56.754)\n",
      "EPOCH: 25 train Results: Prec@1 56.754 Loss: 1.2395\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1392 (1.1392)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4426 (1.2920)\tPrec@1 31.250 (53.750)\n",
      "EPOCH: 25 val Results: Prec@1 53.750 Loss: 1.2920\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [26][0/390]\tTime 0.005 (0.005)\tLoss 1.2427 (1.2427)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [26][78/390]\tTime 0.005 (0.003)\tLoss 1.2066 (1.1891)\tPrec@1 57.031 (59.108)\n",
      "Epoch: [26][156/390]\tTime 0.007 (0.003)\tLoss 1.2069 (1.2083)\tPrec@1 55.469 (58.639)\n",
      "Epoch: [26][234/390]\tTime 0.002 (0.003)\tLoss 1.1917 (1.2200)\tPrec@1 58.594 (57.902)\n",
      "Epoch: [26][312/390]\tTime 0.009 (0.003)\tLoss 1.2242 (1.2308)\tPrec@1 54.688 (57.483)\n",
      "Epoch: [26][390/390]\tTime 0.001 (0.003)\tLoss 1.3103 (1.2358)\tPrec@1 55.000 (57.156)\n",
      "EPOCH: 26 train Results: Prec@1 57.156 Loss: 1.2358\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2012 (1.2012)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3542 (1.2925)\tPrec@1 43.750 (54.330)\n",
      "EPOCH: 26 val Results: Prec@1 54.330 Loss: 1.2925\n",
      "Best Prec@1: 54.330\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [27][0/390]\tTime 0.002 (0.002)\tLoss 1.3190 (1.3190)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [27][78/390]\tTime 0.002 (0.003)\tLoss 1.1632 (1.1946)\tPrec@1 60.156 (58.574)\n",
      "Epoch: [27][156/390]\tTime 0.002 (0.003)\tLoss 1.0880 (1.2080)\tPrec@1 57.031 (57.678)\n",
      "Epoch: [27][234/390]\tTime 0.002 (0.003)\tLoss 1.0996 (1.2173)\tPrec@1 64.062 (57.320)\n",
      "Epoch: [27][312/390]\tTime 0.002 (0.003)\tLoss 1.1819 (1.2264)\tPrec@1 58.594 (57.079)\n",
      "Epoch: [27][390/390]\tTime 0.005 (0.003)\tLoss 1.5788 (1.2350)\tPrec@1 48.750 (56.818)\n",
      "EPOCH: 27 train Results: Prec@1 56.818 Loss: 1.2350\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1502 (1.1502)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.3372 (1.2886)\tPrec@1 50.000 (53.970)\n",
      "EPOCH: 27 val Results: Prec@1 53.970 Loss: 1.2886\n",
      "Best Prec@1: 54.330\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [28][0/390]\tTime 0.005 (0.005)\tLoss 1.1592 (1.1592)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [28][78/390]\tTime 0.002 (0.003)\tLoss 1.1381 (1.1914)\tPrec@1 67.188 (59.335)\n",
      "Epoch: [28][156/390]\tTime 0.003 (0.003)\tLoss 1.2066 (1.2008)\tPrec@1 63.281 (58.753)\n",
      "Epoch: [28][234/390]\tTime 0.004 (0.003)\tLoss 1.2028 (1.2128)\tPrec@1 53.906 (58.268)\n",
      "Epoch: [28][312/390]\tTime 0.003 (0.003)\tLoss 1.2703 (1.2211)\tPrec@1 54.688 (57.862)\n",
      "Epoch: [28][390/390]\tTime 0.003 (0.003)\tLoss 1.4067 (1.2284)\tPrec@1 42.500 (57.446)\n",
      "EPOCH: 28 train Results: Prec@1 57.446 Loss: 1.2284\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1834 (1.1834)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1461 (1.2817)\tPrec@1 50.000 (54.630)\n",
      "EPOCH: 28 val Results: Prec@1 54.630 Loss: 1.2817\n",
      "Best Prec@1: 54.630\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [29][0/390]\tTime 0.003 (0.003)\tLoss 1.1016 (1.1016)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [29][78/390]\tTime 0.002 (0.003)\tLoss 1.3245 (1.2001)\tPrec@1 54.688 (58.752)\n",
      "Epoch: [29][156/390]\tTime 0.013 (0.003)\tLoss 1.1831 (1.2082)\tPrec@1 56.250 (58.395)\n",
      "Epoch: [29][234/390]\tTime 0.002 (0.003)\tLoss 1.1496 (1.2175)\tPrec@1 67.188 (57.949)\n",
      "Epoch: [29][312/390]\tTime 0.005 (0.003)\tLoss 1.2090 (1.2256)\tPrec@1 53.906 (57.500)\n",
      "Epoch: [29][390/390]\tTime 0.001 (0.003)\tLoss 1.3753 (1.2306)\tPrec@1 46.250 (57.292)\n",
      "EPOCH: 29 train Results: Prec@1 57.292 Loss: 1.2306\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1090 (1.1090)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1134 (1.2782)\tPrec@1 50.000 (54.990)\n",
      "EPOCH: 29 val Results: Prec@1 54.990 Loss: 1.2782\n",
      "Best Prec@1: 54.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [30][0/390]\tTime 0.004 (0.004)\tLoss 1.2258 (1.2258)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.003)\tLoss 1.2279 (1.1886)\tPrec@1 54.688 (58.653)\n",
      "Epoch: [30][156/390]\tTime 0.003 (0.003)\tLoss 1.1968 (1.2022)\tPrec@1 56.250 (58.425)\n",
      "Epoch: [30][234/390]\tTime 0.002 (0.003)\tLoss 1.2190 (1.2083)\tPrec@1 56.250 (57.989)\n",
      "Epoch: [30][312/390]\tTime 0.003 (0.003)\tLoss 1.1369 (1.2163)\tPrec@1 63.281 (57.578)\n",
      "Epoch: [30][390/390]\tTime 0.001 (0.003)\tLoss 1.1593 (1.2241)\tPrec@1 62.500 (57.290)\n",
      "EPOCH: 30 train Results: Prec@1 57.290 Loss: 1.2241\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1556 (1.1556)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2441 (1.2843)\tPrec@1 50.000 (54.690)\n",
      "EPOCH: 30 val Results: Prec@1 54.690 Loss: 1.2843\n",
      "Best Prec@1: 54.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [31][0/390]\tTime 0.004 (0.004)\tLoss 1.1747 (1.1747)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [31][78/390]\tTime 0.002 (0.003)\tLoss 1.2008 (1.1950)\tPrec@1 56.250 (58.633)\n",
      "Epoch: [31][156/390]\tTime 0.006 (0.003)\tLoss 1.1883 (1.2041)\tPrec@1 58.594 (58.400)\n",
      "Epoch: [31][234/390]\tTime 0.004 (0.003)\tLoss 1.2127 (1.2114)\tPrec@1 57.812 (58.168)\n",
      "Epoch: [31][312/390]\tTime 0.007 (0.003)\tLoss 1.1834 (1.2185)\tPrec@1 60.938 (57.835)\n",
      "Epoch: [31][390/390]\tTime 0.002 (0.003)\tLoss 1.2975 (1.2250)\tPrec@1 57.500 (57.458)\n",
      "EPOCH: 31 train Results: Prec@1 57.458 Loss: 1.2250\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2190 (1.2190)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2836 (1.2900)\tPrec@1 37.500 (54.180)\n",
      "EPOCH: 31 val Results: Prec@1 54.180 Loss: 1.2900\n",
      "Best Prec@1: 54.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [32][0/390]\tTime 0.011 (0.011)\tLoss 1.2279 (1.2279)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [32][78/390]\tTime 0.003 (0.003)\tLoss 1.1756 (1.1876)\tPrec@1 59.375 (59.118)\n",
      "Epoch: [32][156/390]\tTime 0.003 (0.003)\tLoss 0.9978 (1.1966)\tPrec@1 64.844 (58.444)\n",
      "Epoch: [32][234/390]\tTime 0.005 (0.003)\tLoss 1.2499 (1.2087)\tPrec@1 60.938 (57.862)\n",
      "Epoch: [32][312/390]\tTime 0.006 (0.003)\tLoss 1.1654 (1.2131)\tPrec@1 62.500 (57.753)\n",
      "Epoch: [32][390/390]\tTime 0.001 (0.003)\tLoss 1.1802 (1.2218)\tPrec@1 58.750 (57.480)\n",
      "EPOCH: 32 train Results: Prec@1 57.480 Loss: 1.2218\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2015 (1.2015)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2753 (1.2835)\tPrec@1 56.250 (54.890)\n",
      "EPOCH: 32 val Results: Prec@1 54.890 Loss: 1.2835\n",
      "Best Prec@1: 54.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [33][0/390]\tTime 0.004 (0.004)\tLoss 1.2607 (1.2607)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [33][78/390]\tTime 0.003 (0.003)\tLoss 1.3247 (1.1949)\tPrec@1 52.344 (58.534)\n",
      "Epoch: [33][156/390]\tTime 0.007 (0.003)\tLoss 1.2547 (1.2060)\tPrec@1 50.781 (57.892)\n",
      "Epoch: [33][234/390]\tTime 0.004 (0.003)\tLoss 1.2908 (1.2085)\tPrec@1 59.375 (57.902)\n",
      "Epoch: [33][312/390]\tTime 0.003 (0.003)\tLoss 1.2120 (1.2163)\tPrec@1 58.594 (57.535)\n",
      "Epoch: [33][390/390]\tTime 0.004 (0.003)\tLoss 1.2494 (1.2205)\tPrec@1 53.750 (57.434)\n",
      "EPOCH: 33 train Results: Prec@1 57.434 Loss: 1.2205\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1930 (1.1930)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3353 (1.2762)\tPrec@1 43.750 (55.060)\n",
      "EPOCH: 33 val Results: Prec@1 55.060 Loss: 1.2762\n",
      "Best Prec@1: 55.060\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [34][0/390]\tTime 0.002 (0.002)\tLoss 1.1239 (1.1239)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [34][78/390]\tTime 0.004 (0.003)\tLoss 1.2651 (1.1673)\tPrec@1 51.562 (60.028)\n",
      "Epoch: [34][156/390]\tTime 0.003 (0.003)\tLoss 1.2616 (1.1890)\tPrec@1 55.469 (58.862)\n",
      "Epoch: [34][234/390]\tTime 0.002 (0.003)\tLoss 1.1928 (1.1977)\tPrec@1 57.812 (58.511)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.1850 (1.2087)\tPrec@1 64.062 (58.105)\n",
      "Epoch: [34][390/390]\tTime 0.002 (0.003)\tLoss 1.2184 (1.2165)\tPrec@1 51.250 (57.720)\n",
      "EPOCH: 34 train Results: Prec@1 57.720 Loss: 1.2165\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1508 (1.1508)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2498 (1.2767)\tPrec@1 43.750 (55.250)\n",
      "EPOCH: 34 val Results: Prec@1 55.250 Loss: 1.2767\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [35][0/390]\tTime 0.005 (0.005)\tLoss 1.1959 (1.1959)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [35][78/390]\tTime 0.003 (0.003)\tLoss 1.2540 (1.1760)\tPrec@1 55.469 (59.484)\n",
      "Epoch: [35][156/390]\tTime 0.003 (0.003)\tLoss 1.4318 (1.1875)\tPrec@1 49.219 (58.658)\n",
      "Epoch: [35][234/390]\tTime 0.002 (0.003)\tLoss 1.1797 (1.2041)\tPrec@1 62.500 (57.979)\n",
      "Epoch: [35][312/390]\tTime 0.005 (0.003)\tLoss 1.2246 (1.2109)\tPrec@1 58.594 (57.817)\n",
      "Epoch: [35][390/390]\tTime 0.001 (0.003)\tLoss 1.1884 (1.2163)\tPrec@1 61.250 (57.718)\n",
      "EPOCH: 35 train Results: Prec@1 57.718 Loss: 1.2163\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1770 (1.1770)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2683 (1.2858)\tPrec@1 43.750 (54.540)\n",
      "EPOCH: 35 val Results: Prec@1 54.540 Loss: 1.2858\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [36][0/390]\tTime 0.003 (0.003)\tLoss 1.3414 (1.3414)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [36][78/390]\tTime 0.002 (0.003)\tLoss 1.2640 (1.1816)\tPrec@1 52.344 (58.920)\n",
      "Epoch: [36][156/390]\tTime 0.004 (0.003)\tLoss 1.2713 (1.1921)\tPrec@1 54.688 (58.703)\n",
      "Epoch: [36][234/390]\tTime 0.004 (0.003)\tLoss 1.2284 (1.1997)\tPrec@1 55.469 (58.301)\n",
      "Epoch: [36][312/390]\tTime 0.002 (0.003)\tLoss 1.2378 (1.2066)\tPrec@1 63.281 (58.075)\n",
      "Epoch: [36][390/390]\tTime 0.003 (0.003)\tLoss 1.2879 (1.2137)\tPrec@1 53.750 (57.724)\n",
      "EPOCH: 36 train Results: Prec@1 57.724 Loss: 1.2137\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1748 (1.1748)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2513 (1.2727)\tPrec@1 50.000 (54.740)\n",
      "EPOCH: 36 val Results: Prec@1 54.740 Loss: 1.2727\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [37][0/390]\tTime 0.003 (0.003)\tLoss 1.0349 (1.0349)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [37][78/390]\tTime 0.003 (0.003)\tLoss 1.0753 (1.1594)\tPrec@1 62.500 (60.483)\n",
      "Epoch: [37][156/390]\tTime 0.007 (0.003)\tLoss 1.1401 (1.1836)\tPrec@1 58.594 (59.395)\n",
      "Epoch: [37][234/390]\tTime 0.003 (0.003)\tLoss 1.3818 (1.1995)\tPrec@1 50.000 (58.557)\n",
      "Epoch: [37][312/390]\tTime 0.004 (0.003)\tLoss 1.1246 (1.2104)\tPrec@1 62.500 (57.967)\n",
      "Epoch: [37][390/390]\tTime 0.001 (0.003)\tLoss 1.2474 (1.2131)\tPrec@1 53.750 (57.914)\n",
      "EPOCH: 37 train Results: Prec@1 57.914 Loss: 1.2131\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.2057 (1.2057)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2860 (1.2761)\tPrec@1 56.250 (55.180)\n",
      "EPOCH: 37 val Results: Prec@1 55.180 Loss: 1.2761\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [38][0/390]\tTime 0.003 (0.003)\tLoss 1.1408 (1.1408)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.003)\tLoss 1.2491 (1.1890)\tPrec@1 54.688 (58.584)\n",
      "Epoch: [38][156/390]\tTime 0.006 (0.003)\tLoss 1.1607 (1.1969)\tPrec@1 63.281 (58.514)\n",
      "Epoch: [38][234/390]\tTime 0.002 (0.003)\tLoss 1.2032 (1.2037)\tPrec@1 55.469 (58.275)\n",
      "Epoch: [38][312/390]\tTime 0.002 (0.003)\tLoss 1.3407 (1.2073)\tPrec@1 52.344 (58.025)\n",
      "Epoch: [38][390/390]\tTime 0.001 (0.003)\tLoss 1.1961 (1.2142)\tPrec@1 58.750 (57.778)\n",
      "EPOCH: 38 train Results: Prec@1 57.778 Loss: 1.2142\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1779 (1.1779)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2356 (1.2788)\tPrec@1 50.000 (54.840)\n",
      "EPOCH: 38 val Results: Prec@1 54.840 Loss: 1.2788\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [39][0/390]\tTime 0.003 (0.003)\tLoss 1.2553 (1.2553)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.003)\tLoss 1.2630 (1.1591)\tPrec@1 55.469 (59.780)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.003)\tLoss 1.1733 (1.1814)\tPrec@1 56.250 (59.062)\n",
      "Epoch: [39][234/390]\tTime 0.008 (0.003)\tLoss 1.2593 (1.1948)\tPrec@1 52.344 (58.514)\n",
      "Epoch: [39][312/390]\tTime 0.003 (0.003)\tLoss 1.5311 (1.2013)\tPrec@1 44.531 (58.247)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2517 (1.2100)\tPrec@1 53.750 (57.914)\n",
      "EPOCH: 39 train Results: Prec@1 57.914 Loss: 1.2100\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1283 (1.1283)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4240 (1.2768)\tPrec@1 56.250 (54.540)\n",
      "EPOCH: 39 val Results: Prec@1 54.540 Loss: 1.2768\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [40][0/390]\tTime 0.003 (0.003)\tLoss 1.1427 (1.1427)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.004)\tLoss 1.1581 (1.1666)\tPrec@1 55.469 (59.998)\n",
      "Epoch: [40][156/390]\tTime 0.005 (0.004)\tLoss 1.2694 (1.1737)\tPrec@1 51.562 (59.519)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.004)\tLoss 1.4154 (1.1870)\tPrec@1 48.438 (58.910)\n",
      "Epoch: [40][312/390]\tTime 0.007 (0.003)\tLoss 1.3963 (1.1997)\tPrec@1 50.000 (58.409)\n",
      "Epoch: [40][390/390]\tTime 0.001 (0.004)\tLoss 1.3813 (1.2089)\tPrec@1 51.250 (58.030)\n",
      "EPOCH: 40 train Results: Prec@1 58.030 Loss: 1.2089\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1980 (1.1980)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3826 (1.2681)\tPrec@1 56.250 (55.130)\n",
      "EPOCH: 40 val Results: Prec@1 55.130 Loss: 1.2681\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [41][0/390]\tTime 0.007 (0.007)\tLoss 1.2963 (1.2963)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [41][78/390]\tTime 0.007 (0.006)\tLoss 1.1824 (1.1593)\tPrec@1 57.031 (59.869)\n",
      "Epoch: [41][156/390]\tTime 0.004 (0.005)\tLoss 1.2751 (1.1812)\tPrec@1 54.688 (59.151)\n",
      "Epoch: [41][234/390]\tTime 0.005 (0.005)\tLoss 1.3213 (1.1901)\tPrec@1 53.125 (58.733)\n",
      "Epoch: [41][312/390]\tTime 0.002 (0.005)\tLoss 1.1866 (1.2003)\tPrec@1 61.719 (58.432)\n",
      "Epoch: [41][390/390]\tTime 0.001 (0.004)\tLoss 1.1477 (1.2063)\tPrec@1 62.500 (58.210)\n",
      "EPOCH: 41 train Results: Prec@1 58.210 Loss: 1.2063\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0969 (1.0969)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3143 (1.2671)\tPrec@1 56.250 (55.190)\n",
      "EPOCH: 41 val Results: Prec@1 55.190 Loss: 1.2671\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [42][0/390]\tTime 0.002 (0.002)\tLoss 1.2393 (1.2393)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [42][78/390]\tTime 0.002 (0.003)\tLoss 1.1612 (1.1614)\tPrec@1 59.375 (60.196)\n",
      "Epoch: [42][156/390]\tTime 0.004 (0.003)\tLoss 1.1469 (1.1709)\tPrec@1 62.500 (59.629)\n",
      "Epoch: [42][234/390]\tTime 0.004 (0.003)\tLoss 1.2153 (1.1880)\tPrec@1 57.031 (58.807)\n",
      "Epoch: [42][312/390]\tTime 0.008 (0.003)\tLoss 1.2929 (1.1980)\tPrec@1 58.594 (58.434)\n",
      "Epoch: [42][390/390]\tTime 0.005 (0.003)\tLoss 1.2123 (1.2064)\tPrec@1 58.750 (58.060)\n",
      "EPOCH: 42 train Results: Prec@1 58.060 Loss: 1.2064\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1625 (1.1625)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2785 (1.2649)\tPrec@1 50.000 (54.990)\n",
      "EPOCH: 42 val Results: Prec@1 54.990 Loss: 1.2649\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [43][0/390]\tTime 0.010 (0.010)\tLoss 1.2356 (1.2356)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [43][78/390]\tTime 0.002 (0.003)\tLoss 1.1653 (1.1741)\tPrec@1 60.938 (59.276)\n",
      "Epoch: [43][156/390]\tTime 0.002 (0.003)\tLoss 1.2720 (1.1882)\tPrec@1 50.781 (58.544)\n",
      "Epoch: [43][234/390]\tTime 0.003 (0.003)\tLoss 1.1324 (1.1930)\tPrec@1 57.031 (58.467)\n",
      "Epoch: [43][312/390]\tTime 0.002 (0.003)\tLoss 1.1543 (1.2034)\tPrec@1 64.062 (57.990)\n",
      "Epoch: [43][390/390]\tTime 0.001 (0.003)\tLoss 1.0691 (1.2071)\tPrec@1 65.000 (57.818)\n",
      "EPOCH: 43 train Results: Prec@1 57.818 Loss: 1.2071\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1792 (1.1792)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2862 (1.2743)\tPrec@1 31.250 (55.170)\n",
      "EPOCH: 43 val Results: Prec@1 55.170 Loss: 1.2743\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [44][0/390]\tTime 0.002 (0.002)\tLoss 1.2228 (1.2228)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.003)\tLoss 0.9948 (1.1626)\tPrec@1 66.406 (60.047)\n",
      "Epoch: [44][156/390]\tTime 0.002 (0.003)\tLoss 1.0945 (1.1712)\tPrec@1 60.156 (59.445)\n",
      "Epoch: [44][234/390]\tTime 0.002 (0.003)\tLoss 1.1339 (1.1889)\tPrec@1 60.938 (58.674)\n",
      "Epoch: [44][312/390]\tTime 0.002 (0.003)\tLoss 1.1909 (1.1993)\tPrec@1 58.594 (58.142)\n",
      "Epoch: [44][390/390]\tTime 0.004 (0.003)\tLoss 1.1295 (1.2056)\tPrec@1 57.500 (57.882)\n",
      "EPOCH: 44 train Results: Prec@1 57.882 Loss: 1.2056\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1135 (1.1135)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3938 (1.2667)\tPrec@1 43.750 (55.250)\n",
      "EPOCH: 44 val Results: Prec@1 55.250 Loss: 1.2667\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [45][0/390]\tTime 0.003 (0.003)\tLoss 1.2090 (1.2090)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [45][78/390]\tTime 0.003 (0.003)\tLoss 1.2449 (1.1672)\tPrec@1 57.031 (59.385)\n",
      "Epoch: [45][156/390]\tTime 0.010 (0.003)\tLoss 1.1892 (1.1726)\tPrec@1 57.031 (59.280)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.003)\tLoss 1.2657 (1.1853)\tPrec@1 60.156 (58.787)\n",
      "Epoch: [45][312/390]\tTime 0.007 (0.003)\tLoss 1.1643 (1.1944)\tPrec@1 57.031 (58.486)\n",
      "Epoch: [45][390/390]\tTime 0.003 (0.003)\tLoss 1.3688 (1.2002)\tPrec@1 51.250 (58.242)\n",
      "EPOCH: 45 train Results: Prec@1 58.242 Loss: 1.2002\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1631 (1.1631)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2766 (1.2662)\tPrec@1 56.250 (55.320)\n",
      "EPOCH: 45 val Results: Prec@1 55.320 Loss: 1.2662\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [46][0/390]\tTime 0.006 (0.006)\tLoss 1.2206 (1.2206)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [46][78/390]\tTime 0.002 (0.003)\tLoss 1.1939 (1.1674)\tPrec@1 59.375 (59.227)\n",
      "Epoch: [46][156/390]\tTime 0.002 (0.003)\tLoss 1.2856 (1.1734)\tPrec@1 54.688 (59.042)\n",
      "Epoch: [46][234/390]\tTime 0.003 (0.003)\tLoss 1.2804 (1.1867)\tPrec@1 50.000 (58.547)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.003)\tLoss 1.2214 (1.1926)\tPrec@1 57.031 (58.352)\n",
      "Epoch: [46][390/390]\tTime 0.001 (0.003)\tLoss 1.1739 (1.2020)\tPrec@1 53.750 (57.920)\n",
      "EPOCH: 46 train Results: Prec@1 57.920 Loss: 1.2020\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1869 (1.1869)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5157 (1.2714)\tPrec@1 25.000 (55.050)\n",
      "EPOCH: 46 val Results: Prec@1 55.050 Loss: 1.2714\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [47][0/390]\tTime 0.003 (0.003)\tLoss 1.4628 (1.4628)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [47][78/390]\tTime 0.003 (0.003)\tLoss 1.3353 (1.1560)\tPrec@1 53.906 (60.087)\n",
      "Epoch: [47][156/390]\tTime 0.002 (0.003)\tLoss 1.2117 (1.1744)\tPrec@1 58.594 (59.360)\n",
      "Epoch: [47][234/390]\tTime 0.003 (0.003)\tLoss 1.1832 (1.1856)\tPrec@1 57.031 (58.750)\n",
      "Epoch: [47][312/390]\tTime 0.002 (0.003)\tLoss 1.1713 (1.1886)\tPrec@1 55.469 (58.499)\n",
      "Epoch: [47][390/390]\tTime 0.001 (0.003)\tLoss 1.1609 (1.1963)\tPrec@1 56.250 (58.214)\n",
      "EPOCH: 47 train Results: Prec@1 58.214 Loss: 1.1963\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0444 (1.0444)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3231 (1.2650)\tPrec@1 56.250 (54.950)\n",
      "EPOCH: 47 val Results: Prec@1 54.950 Loss: 1.2650\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [48][0/390]\tTime 0.004 (0.004)\tLoss 1.1714 (1.1714)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [48][78/390]\tTime 0.003 (0.003)\tLoss 1.1976 (1.1694)\tPrec@1 61.719 (59.612)\n",
      "Epoch: [48][156/390]\tTime 0.003 (0.003)\tLoss 1.2880 (1.1797)\tPrec@1 55.469 (59.106)\n",
      "Epoch: [48][234/390]\tTime 0.005 (0.003)\tLoss 1.1573 (1.1828)\tPrec@1 57.031 (58.886)\n",
      "Epoch: [48][312/390]\tTime 0.008 (0.003)\tLoss 1.1583 (1.1913)\tPrec@1 61.719 (58.451)\n",
      "Epoch: [48][390/390]\tTime 0.002 (0.003)\tLoss 1.3884 (1.1991)\tPrec@1 47.500 (58.234)\n",
      "EPOCH: 48 train Results: Prec@1 58.234 Loss: 1.1991\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1825 (1.1825)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5233 (1.2774)\tPrec@1 43.750 (54.740)\n",
      "EPOCH: 48 val Results: Prec@1 54.740 Loss: 1.2774\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [49][0/390]\tTime 0.005 (0.005)\tLoss 1.1302 (1.1302)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [49][78/390]\tTime 0.003 (0.003)\tLoss 1.2057 (1.1452)\tPrec@1 60.938 (60.750)\n",
      "Epoch: [49][156/390]\tTime 0.011 (0.003)\tLoss 1.2687 (1.1680)\tPrec@1 56.250 (59.748)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.003)\tLoss 1.2245 (1.1820)\tPrec@1 57.812 (59.092)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.1930 (1.1894)\tPrec@1 50.781 (58.671)\n",
      "Epoch: [49][390/390]\tTime 0.003 (0.003)\tLoss 1.3515 (1.1988)\tPrec@1 46.250 (58.290)\n",
      "EPOCH: 49 train Results: Prec@1 58.290 Loss: 1.1988\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1824 (1.1824)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2156 (1.2699)\tPrec@1 50.000 (54.980)\n",
      "EPOCH: 49 val Results: Prec@1 54.980 Loss: 1.2699\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [50][0/390]\tTime 0.002 (0.002)\tLoss 1.1672 (1.1672)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [50][78/390]\tTime 0.004 (0.004)\tLoss 1.1759 (1.1542)\tPrec@1 60.156 (60.206)\n",
      "Epoch: [50][156/390]\tTime 0.002 (0.003)\tLoss 1.1745 (1.1673)\tPrec@1 60.938 (59.619)\n",
      "Epoch: [50][234/390]\tTime 0.002 (0.003)\tLoss 1.2326 (1.1816)\tPrec@1 54.688 (59.003)\n",
      "Epoch: [50][312/390]\tTime 0.002 (0.003)\tLoss 1.1628 (1.1900)\tPrec@1 56.250 (58.614)\n",
      "Epoch: [50][390/390]\tTime 0.001 (0.003)\tLoss 1.1165 (1.1989)\tPrec@1 62.500 (58.258)\n",
      "EPOCH: 50 train Results: Prec@1 58.258 Loss: 1.1989\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1821 (1.1821)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4661 (1.2717)\tPrec@1 31.250 (55.030)\n",
      "EPOCH: 50 val Results: Prec@1 55.030 Loss: 1.2717\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [51][0/390]\tTime 0.002 (0.002)\tLoss 1.2405 (1.2405)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [51][78/390]\tTime 0.002 (0.003)\tLoss 1.1045 (1.1579)\tPrec@1 61.719 (59.919)\n",
      "Epoch: [51][156/390]\tTime 0.002 (0.003)\tLoss 1.1468 (1.1728)\tPrec@1 59.375 (59.062)\n",
      "Epoch: [51][234/390]\tTime 0.003 (0.003)\tLoss 1.2900 (1.1801)\tPrec@1 54.688 (58.893)\n",
      "Epoch: [51][312/390]\tTime 0.002 (0.003)\tLoss 1.1459 (1.1859)\tPrec@1 66.406 (58.734)\n",
      "Epoch: [51][390/390]\tTime 0.002 (0.003)\tLoss 1.0796 (1.1949)\tPrec@1 61.250 (58.394)\n",
      "EPOCH: 51 train Results: Prec@1 58.394 Loss: 1.1949\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2027 (1.2027)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4032 (1.2638)\tPrec@1 31.250 (55.560)\n",
      "EPOCH: 51 val Results: Prec@1 55.560 Loss: 1.2638\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [52][0/390]\tTime 0.005 (0.005)\tLoss 1.0995 (1.0995)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [52][78/390]\tTime 0.004 (0.003)\tLoss 1.2090 (1.1531)\tPrec@1 58.594 (60.562)\n",
      "Epoch: [52][156/390]\tTime 0.002 (0.003)\tLoss 1.1909 (1.1614)\tPrec@1 59.375 (59.878)\n",
      "Epoch: [52][234/390]\tTime 0.002 (0.003)\tLoss 1.1200 (1.1765)\tPrec@1 61.719 (59.119)\n",
      "Epoch: [52][312/390]\tTime 0.002 (0.003)\tLoss 1.2758 (1.1874)\tPrec@1 54.688 (58.556)\n",
      "Epoch: [52][390/390]\tTime 0.002 (0.003)\tLoss 1.4842 (1.1975)\tPrec@1 48.750 (58.230)\n",
      "EPOCH: 52 train Results: Prec@1 58.230 Loss: 1.1975\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1669 (1.1669)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2689 (1.2806)\tPrec@1 43.750 (54.340)\n",
      "EPOCH: 52 val Results: Prec@1 54.340 Loss: 1.2806\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [53][0/390]\tTime 0.004 (0.004)\tLoss 1.2712 (1.2712)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [53][78/390]\tTime 0.003 (0.003)\tLoss 1.1456 (1.1496)\tPrec@1 64.844 (60.364)\n",
      "Epoch: [53][156/390]\tTime 0.004 (0.003)\tLoss 1.1834 (1.1614)\tPrec@1 61.719 (59.853)\n",
      "Epoch: [53][234/390]\tTime 0.009 (0.003)\tLoss 1.1711 (1.1741)\tPrec@1 56.250 (59.352)\n",
      "Epoch: [53][312/390]\tTime 0.003 (0.003)\tLoss 1.4444 (1.1828)\tPrec@1 53.125 (59.058)\n",
      "Epoch: [53][390/390]\tTime 0.001 (0.003)\tLoss 1.2901 (1.1903)\tPrec@1 56.250 (58.652)\n",
      "EPOCH: 53 train Results: Prec@1 58.652 Loss: 1.1903\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1787 (1.1787)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4626 (1.2733)\tPrec@1 56.250 (54.770)\n",
      "EPOCH: 53 val Results: Prec@1 54.770 Loss: 1.2733\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [54][0/390]\tTime 0.002 (0.002)\tLoss 1.1175 (1.1175)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [54][78/390]\tTime 0.002 (0.003)\tLoss 1.1335 (1.1539)\tPrec@1 60.156 (60.433)\n",
      "Epoch: [54][156/390]\tTime 0.002 (0.003)\tLoss 1.1059 (1.1724)\tPrec@1 65.625 (59.206)\n",
      "Epoch: [54][234/390]\tTime 0.002 (0.003)\tLoss 1.2065 (1.1790)\tPrec@1 60.938 (58.943)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2856 (1.1873)\tPrec@1 57.031 (58.544)\n",
      "Epoch: [54][390/390]\tTime 0.002 (0.003)\tLoss 1.2911 (1.1971)\tPrec@1 48.750 (58.158)\n",
      "EPOCH: 54 train Results: Prec@1 58.158 Loss: 1.1971\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1608 (1.1608)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4760 (1.2682)\tPrec@1 50.000 (55.140)\n",
      "EPOCH: 54 val Results: Prec@1 55.140 Loss: 1.2682\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.2164 (1.2164)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.2549 (1.1491)\tPrec@1 61.719 (61.392)\n",
      "Epoch: [55][156/390]\tTime 0.003 (0.003)\tLoss 1.2884 (1.1704)\tPrec@1 53.906 (59.718)\n",
      "Epoch: [55][234/390]\tTime 0.003 (0.003)\tLoss 1.2090 (1.1820)\tPrec@1 57.031 (59.139)\n",
      "Epoch: [55][312/390]\tTime 0.004 (0.003)\tLoss 1.1111 (1.1846)\tPrec@1 64.062 (58.881)\n",
      "Epoch: [55][390/390]\tTime 0.003 (0.003)\tLoss 1.1312 (1.1923)\tPrec@1 62.500 (58.532)\n",
      "EPOCH: 55 train Results: Prec@1 58.532 Loss: 1.1923\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1572 (1.1572)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4509 (1.2675)\tPrec@1 31.250 (54.720)\n",
      "EPOCH: 55 val Results: Prec@1 54.720 Loss: 1.2675\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [56][0/390]\tTime 0.004 (0.004)\tLoss 1.0891 (1.0891)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [56][78/390]\tTime 0.003 (0.003)\tLoss 1.1325 (1.1641)\tPrec@1 60.938 (59.672)\n",
      "Epoch: [56][156/390]\tTime 0.002 (0.003)\tLoss 1.0989 (1.1671)\tPrec@1 65.625 (59.435)\n",
      "Epoch: [56][234/390]\tTime 0.004 (0.003)\tLoss 1.2376 (1.1740)\tPrec@1 60.156 (59.259)\n",
      "Epoch: [56][312/390]\tTime 0.002 (0.003)\tLoss 1.0720 (1.1827)\tPrec@1 64.062 (58.866)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.003)\tLoss 1.2713 (1.1901)\tPrec@1 61.250 (58.620)\n",
      "EPOCH: 56 train Results: Prec@1 58.620 Loss: 1.1901\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1037 (1.1037)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2749 (1.2699)\tPrec@1 50.000 (54.610)\n",
      "EPOCH: 56 val Results: Prec@1 54.610 Loss: 1.2699\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [57][0/390]\tTime 0.005 (0.005)\tLoss 1.2075 (1.2075)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [57][78/390]\tTime 0.003 (0.003)\tLoss 1.2237 (1.1457)\tPrec@1 54.688 (60.216)\n",
      "Epoch: [57][156/390]\tTime 0.003 (0.003)\tLoss 1.1806 (1.1592)\tPrec@1 60.156 (59.509)\n",
      "Epoch: [57][234/390]\tTime 0.002 (0.005)\tLoss 1.2492 (1.1683)\tPrec@1 56.250 (59.086)\n",
      "Epoch: [57][312/390]\tTime 0.003 (0.005)\tLoss 1.2272 (1.1773)\tPrec@1 56.250 (58.739)\n",
      "Epoch: [57][390/390]\tTime 0.001 (0.005)\tLoss 1.1773 (1.1893)\tPrec@1 56.250 (58.394)\n",
      "EPOCH: 57 train Results: Prec@1 58.394 Loss: 1.1893\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1848 (1.1848)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4239 (1.2675)\tPrec@1 31.250 (55.320)\n",
      "EPOCH: 57 val Results: Prec@1 55.320 Loss: 1.2675\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [58][0/390]\tTime 0.004 (0.004)\tLoss 1.1296 (1.1296)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [58][78/390]\tTime 0.002 (0.004)\tLoss 1.1760 (1.1493)\tPrec@1 57.031 (59.889)\n",
      "Epoch: [58][156/390]\tTime 0.002 (0.004)\tLoss 1.2286 (1.1624)\tPrec@1 55.469 (59.554)\n",
      "Epoch: [58][234/390]\tTime 0.002 (0.003)\tLoss 1.1697 (1.1711)\tPrec@1 60.938 (59.189)\n",
      "Epoch: [58][312/390]\tTime 0.002 (0.003)\tLoss 1.1045 (1.1784)\tPrec@1 60.156 (58.931)\n",
      "Epoch: [58][390/390]\tTime 0.001 (0.003)\tLoss 1.2563 (1.1865)\tPrec@1 57.500 (58.668)\n",
      "EPOCH: 58 train Results: Prec@1 58.668 Loss: 1.1865\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1691 (1.1691)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4091 (1.2639)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 58 val Results: Prec@1 55.370 Loss: 1.2639\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [59][0/390]\tTime 0.003 (0.003)\tLoss 1.2049 (1.2049)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [59][78/390]\tTime 0.003 (0.003)\tLoss 1.1305 (1.1426)\tPrec@1 53.906 (59.929)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.2231 (1.1556)\tPrec@1 53.125 (59.325)\n",
      "Epoch: [59][234/390]\tTime 0.004 (0.003)\tLoss 1.1794 (1.1732)\tPrec@1 60.938 (58.797)\n",
      "Epoch: [59][312/390]\tTime 0.002 (0.003)\tLoss 1.1900 (1.1803)\tPrec@1 59.375 (58.684)\n",
      "Epoch: [59][390/390]\tTime 0.001 (0.003)\tLoss 1.2000 (1.1894)\tPrec@1 61.250 (58.294)\n",
      "EPOCH: 59 train Results: Prec@1 58.294 Loss: 1.1894\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1958 (1.1958)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2071 (1.2713)\tPrec@1 56.250 (54.830)\n",
      "EPOCH: 59 val Results: Prec@1 54.830 Loss: 1.2713\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [60][0/390]\tTime 0.003 (0.003)\tLoss 1.1028 (1.1028)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [60][78/390]\tTime 0.002 (0.004)\tLoss 1.0910 (1.1544)\tPrec@1 60.156 (60.206)\n",
      "Epoch: [60][156/390]\tTime 0.003 (0.004)\tLoss 1.0982 (1.1657)\tPrec@1 61.719 (59.778)\n",
      "Epoch: [60][234/390]\tTime 0.006 (0.003)\tLoss 1.2534 (1.1749)\tPrec@1 55.469 (59.205)\n",
      "Epoch: [60][312/390]\tTime 0.002 (0.003)\tLoss 1.1214 (1.1847)\tPrec@1 63.281 (58.721)\n",
      "Epoch: [60][390/390]\tTime 0.001 (0.003)\tLoss 1.2907 (1.1898)\tPrec@1 51.250 (58.506)\n",
      "EPOCH: 60 train Results: Prec@1 58.506 Loss: 1.1898\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1242 (1.1242)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2383 (1.2716)\tPrec@1 56.250 (55.180)\n",
      "EPOCH: 60 val Results: Prec@1 55.180 Loss: 1.2716\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [61][0/390]\tTime 0.006 (0.006)\tLoss 1.0209 (1.0209)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [61][78/390]\tTime 0.002 (0.003)\tLoss 1.2506 (1.1478)\tPrec@1 53.125 (60.453)\n",
      "Epoch: [61][156/390]\tTime 0.002 (0.003)\tLoss 1.2996 (1.1605)\tPrec@1 54.688 (59.743)\n",
      "Epoch: [61][234/390]\tTime 0.004 (0.003)\tLoss 1.0832 (1.1717)\tPrec@1 63.281 (59.438)\n",
      "Epoch: [61][312/390]\tTime 0.002 (0.003)\tLoss 1.2501 (1.1816)\tPrec@1 54.688 (59.148)\n",
      "Epoch: [61][390/390]\tTime 0.002 (0.003)\tLoss 1.2907 (1.1879)\tPrec@1 52.500 (58.780)\n",
      "EPOCH: 61 train Results: Prec@1 58.780 Loss: 1.1879\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1423 (1.1423)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3111 (1.2627)\tPrec@1 37.500 (55.050)\n",
      "EPOCH: 61 val Results: Prec@1 55.050 Loss: 1.2627\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [62][0/390]\tTime 0.003 (0.003)\tLoss 1.1286 (1.1286)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.2353 (1.1395)\tPrec@1 57.031 (61.145)\n",
      "Epoch: [62][156/390]\tTime 0.002 (0.003)\tLoss 1.0598 (1.1549)\tPrec@1 67.188 (60.415)\n",
      "Epoch: [62][234/390]\tTime 0.002 (0.003)\tLoss 1.2535 (1.1669)\tPrec@1 56.250 (59.731)\n",
      "Epoch: [62][312/390]\tTime 0.005 (0.003)\tLoss 1.3084 (1.1744)\tPrec@1 53.906 (59.263)\n",
      "Epoch: [62][390/390]\tTime 0.001 (0.003)\tLoss 1.0272 (1.1842)\tPrec@1 66.250 (58.880)\n",
      "EPOCH: 62 train Results: Prec@1 58.880 Loss: 1.1842\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1682 (1.1682)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2613 (1.2656)\tPrec@1 37.500 (55.710)\n",
      "EPOCH: 62 val Results: Prec@1 55.710 Loss: 1.2656\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [63][0/390]\tTime 0.004 (0.004)\tLoss 1.1008 (1.1008)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [63][78/390]\tTime 0.002 (0.003)\tLoss 1.2121 (1.1370)\tPrec@1 59.375 (61.244)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.003)\tLoss 1.2240 (1.1577)\tPrec@1 59.375 (59.753)\n",
      "Epoch: [63][234/390]\tTime 0.002 (0.003)\tLoss 1.2426 (1.1700)\tPrec@1 57.031 (59.269)\n",
      "Epoch: [63][312/390]\tTime 0.007 (0.003)\tLoss 1.3428 (1.1791)\tPrec@1 54.688 (58.928)\n",
      "Epoch: [63][390/390]\tTime 0.002 (0.003)\tLoss 1.2438 (1.1833)\tPrec@1 62.500 (58.766)\n",
      "EPOCH: 63 train Results: Prec@1 58.766 Loss: 1.1833\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1483 (1.1483)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4740 (1.2784)\tPrec@1 37.500 (54.460)\n",
      "EPOCH: 63 val Results: Prec@1 54.460 Loss: 1.2784\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [64][0/390]\tTime 0.002 (0.002)\tLoss 1.1415 (1.1415)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [64][78/390]\tTime 0.005 (0.003)\tLoss 1.0494 (1.1319)\tPrec@1 61.719 (60.829)\n",
      "Epoch: [64][156/390]\tTime 0.003 (0.003)\tLoss 1.2573 (1.1572)\tPrec@1 51.562 (60.047)\n",
      "Epoch: [64][234/390]\tTime 0.002 (0.003)\tLoss 1.3226 (1.1712)\tPrec@1 55.469 (59.265)\n",
      "Epoch: [64][312/390]\tTime 0.002 (0.003)\tLoss 1.3380 (1.1822)\tPrec@1 52.344 (58.806)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.003)\tLoss 1.3344 (1.1896)\tPrec@1 50.000 (58.558)\n",
      "EPOCH: 64 train Results: Prec@1 58.558 Loss: 1.1896\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1879 (1.1879)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2101 (1.2691)\tPrec@1 56.250 (54.440)\n",
      "EPOCH: 64 val Results: Prec@1 54.440 Loss: 1.2691\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [65][0/390]\tTime 0.003 (0.003)\tLoss 1.1719 (1.1719)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.004)\tLoss 1.1748 (1.1383)\tPrec@1 57.031 (61.234)\n",
      "Epoch: [65][156/390]\tTime 0.006 (0.003)\tLoss 1.1206 (1.1492)\tPrec@1 64.844 (60.724)\n",
      "Epoch: [65][234/390]\tTime 0.003 (0.003)\tLoss 1.2497 (1.1666)\tPrec@1 53.125 (59.651)\n",
      "Epoch: [65][312/390]\tTime 0.002 (0.003)\tLoss 1.1494 (1.1781)\tPrec@1 59.375 (59.046)\n",
      "Epoch: [65][390/390]\tTime 0.004 (0.003)\tLoss 1.1306 (1.1841)\tPrec@1 57.500 (58.780)\n",
      "EPOCH: 65 train Results: Prec@1 58.780 Loss: 1.1841\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1777 (1.1777)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2552 (1.2608)\tPrec@1 50.000 (55.080)\n",
      "EPOCH: 65 val Results: Prec@1 55.080 Loss: 1.2608\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [66][0/390]\tTime 0.010 (0.010)\tLoss 1.1234 (1.1234)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [66][78/390]\tTime 0.003 (0.004)\tLoss 1.1585 (1.1482)\tPrec@1 57.812 (60.087)\n",
      "Epoch: [66][156/390]\tTime 0.002 (0.004)\tLoss 1.1428 (1.1582)\tPrec@1 60.938 (59.723)\n",
      "Epoch: [66][234/390]\tTime 0.002 (0.004)\tLoss 1.2583 (1.1677)\tPrec@1 57.031 (59.242)\n",
      "Epoch: [66][312/390]\tTime 0.003 (0.004)\tLoss 1.3251 (1.1794)\tPrec@1 57.812 (58.753)\n",
      "Epoch: [66][390/390]\tTime 0.013 (0.004)\tLoss 1.1607 (1.1846)\tPrec@1 61.250 (58.580)\n",
      "EPOCH: 66 train Results: Prec@1 58.580 Loss: 1.1846\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2054 (1.2054)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4612 (1.2643)\tPrec@1 31.250 (55.220)\n",
      "EPOCH: 66 val Results: Prec@1 55.220 Loss: 1.2643\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [67][0/390]\tTime 0.003 (0.003)\tLoss 1.2434 (1.2434)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [67][78/390]\tTime 0.003 (0.003)\tLoss 1.2037 (1.1390)\tPrec@1 58.594 (60.572)\n",
      "Epoch: [67][156/390]\tTime 0.002 (0.003)\tLoss 1.3599 (1.1526)\tPrec@1 53.906 (60.022)\n",
      "Epoch: [67][234/390]\tTime 0.004 (0.003)\tLoss 1.1554 (1.1668)\tPrec@1 60.156 (59.405)\n",
      "Epoch: [67][312/390]\tTime 0.002 (0.003)\tLoss 1.1784 (1.1722)\tPrec@1 58.594 (59.085)\n",
      "Epoch: [67][390/390]\tTime 0.001 (0.003)\tLoss 1.2751 (1.1814)\tPrec@1 55.000 (58.824)\n",
      "EPOCH: 67 train Results: Prec@1 58.824 Loss: 1.1814\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1776 (1.1776)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1278 (1.2603)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 67 val Results: Prec@1 55.230 Loss: 1.2603\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [68][0/390]\tTime 0.006 (0.006)\tLoss 1.1163 (1.1163)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [68][78/390]\tTime 0.003 (0.003)\tLoss 1.0997 (1.1236)\tPrec@1 62.500 (60.928)\n",
      "Epoch: [68][156/390]\tTime 0.003 (0.003)\tLoss 1.3056 (1.1559)\tPrec@1 56.250 (59.614)\n",
      "Epoch: [68][234/390]\tTime 0.002 (0.003)\tLoss 1.2520 (1.1657)\tPrec@1 57.812 (59.086)\n",
      "Epoch: [68][312/390]\tTime 0.004 (0.003)\tLoss 1.4106 (1.1767)\tPrec@1 52.344 (58.783)\n",
      "Epoch: [68][390/390]\tTime 0.003 (0.003)\tLoss 1.3610 (1.1831)\tPrec@1 56.250 (58.654)\n",
      "EPOCH: 68 train Results: Prec@1 58.654 Loss: 1.1831\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1274 (1.1274)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1946 (1.2621)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 68 val Results: Prec@1 55.230 Loss: 1.2621\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [69][0/390]\tTime 0.002 (0.002)\tLoss 1.1230 (1.1230)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.004)\tLoss 1.1516 (1.1282)\tPrec@1 57.031 (60.848)\n",
      "Epoch: [69][156/390]\tTime 0.002 (0.004)\tLoss 1.1588 (1.1459)\tPrec@1 60.938 (60.241)\n",
      "Epoch: [69][234/390]\tTime 0.003 (0.003)\tLoss 1.0474 (1.1607)\tPrec@1 63.281 (59.511)\n",
      "Epoch: [69][312/390]\tTime 0.002 (0.003)\tLoss 1.2280 (1.1726)\tPrec@1 60.156 (58.841)\n",
      "Epoch: [69][390/390]\tTime 0.001 (0.004)\tLoss 1.2777 (1.1806)\tPrec@1 52.500 (58.624)\n",
      "EPOCH: 69 train Results: Prec@1 58.624 Loss: 1.1806\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1526 (1.1526)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3472 (1.2626)\tPrec@1 37.500 (55.060)\n",
      "EPOCH: 69 val Results: Prec@1 55.060 Loss: 1.2626\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [70][0/390]\tTime 0.002 (0.002)\tLoss 1.0074 (1.0074)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [70][78/390]\tTime 0.003 (0.003)\tLoss 1.2882 (1.1309)\tPrec@1 52.344 (61.234)\n",
      "Epoch: [70][156/390]\tTime 0.007 (0.003)\tLoss 1.1996 (1.1492)\tPrec@1 54.688 (60.485)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.003)\tLoss 1.0880 (1.1599)\tPrec@1 64.062 (59.781)\n",
      "Epoch: [70][312/390]\tTime 0.003 (0.003)\tLoss 1.1827 (1.1755)\tPrec@1 62.500 (59.103)\n",
      "Epoch: [70][390/390]\tTime 0.005 (0.003)\tLoss 1.2280 (1.1802)\tPrec@1 57.500 (58.918)\n",
      "EPOCH: 70 train Results: Prec@1 58.918 Loss: 1.1802\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2170 (1.2170)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2085 (1.2601)\tPrec@1 43.750 (55.290)\n",
      "EPOCH: 70 val Results: Prec@1 55.290 Loss: 1.2601\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [71][0/390]\tTime 0.002 (0.002)\tLoss 1.1093 (1.1093)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [71][78/390]\tTime 0.003 (0.003)\tLoss 1.2181 (1.1306)\tPrec@1 60.156 (60.710)\n",
      "Epoch: [71][156/390]\tTime 0.002 (0.003)\tLoss 1.1722 (1.1482)\tPrec@1 55.469 (59.922)\n",
      "Epoch: [71][234/390]\tTime 0.002 (0.003)\tLoss 0.9769 (1.1645)\tPrec@1 67.969 (59.525)\n",
      "Epoch: [71][312/390]\tTime 0.002 (0.003)\tLoss 1.1490 (1.1687)\tPrec@1 63.281 (59.345)\n",
      "Epoch: [71][390/390]\tTime 0.005 (0.003)\tLoss 1.0855 (1.1798)\tPrec@1 58.750 (58.842)\n",
      "EPOCH: 71 train Results: Prec@1 58.842 Loss: 1.1798\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2037 (1.2037)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1893 (1.2679)\tPrec@1 43.750 (55.070)\n",
      "EPOCH: 71 val Results: Prec@1 55.070 Loss: 1.2679\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [72][0/390]\tTime 0.003 (0.003)\tLoss 1.0902 (1.0902)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [72][78/390]\tTime 0.002 (0.003)\tLoss 1.3339 (1.1418)\tPrec@1 50.781 (60.601)\n",
      "Epoch: [72][156/390]\tTime 0.003 (0.003)\tLoss 1.2161 (1.1490)\tPrec@1 59.375 (60.161)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.003)\tLoss 1.2007 (1.1611)\tPrec@1 54.688 (59.408)\n",
      "Epoch: [72][312/390]\tTime 0.002 (0.003)\tLoss 1.2017 (1.1715)\tPrec@1 58.594 (58.961)\n",
      "Epoch: [72][390/390]\tTime 0.001 (0.003)\tLoss 1.0218 (1.1776)\tPrec@1 68.750 (58.766)\n",
      "EPOCH: 72 train Results: Prec@1 58.766 Loss: 1.1776\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1327 (1.1327)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2597 (1.2582)\tPrec@1 62.500 (54.810)\n",
      "EPOCH: 72 val Results: Prec@1 54.810 Loss: 1.2582\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [73][0/390]\tTime 0.003 (0.003)\tLoss 1.0913 (1.0913)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.003)\tLoss 1.0527 (1.1339)\tPrec@1 61.719 (60.839)\n",
      "Epoch: [73][156/390]\tTime 0.007 (0.003)\tLoss 1.2818 (1.1458)\tPrec@1 53.125 (60.037)\n",
      "Epoch: [73][234/390]\tTime 0.002 (0.003)\tLoss 1.1605 (1.1594)\tPrec@1 60.938 (59.624)\n",
      "Epoch: [73][312/390]\tTime 0.002 (0.003)\tLoss 1.0447 (1.1725)\tPrec@1 64.062 (59.038)\n",
      "Epoch: [73][390/390]\tTime 0.002 (0.003)\tLoss 1.1866 (1.1785)\tPrec@1 57.500 (58.848)\n",
      "EPOCH: 73 train Results: Prec@1 58.848 Loss: 1.1785\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1582 (1.1582)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2145 (1.2564)\tPrec@1 50.000 (55.910)\n",
      "EPOCH: 73 val Results: Prec@1 55.910 Loss: 1.2564\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [74][0/390]\tTime 0.005 (0.005)\tLoss 1.2078 (1.2078)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [74][78/390]\tTime 0.002 (0.003)\tLoss 1.1833 (1.1379)\tPrec@1 63.281 (60.710)\n",
      "Epoch: [74][156/390]\tTime 0.003 (0.003)\tLoss 1.1253 (1.1520)\tPrec@1 60.156 (60.161)\n",
      "Epoch: [74][234/390]\tTime 0.004 (0.003)\tLoss 1.1876 (1.1612)\tPrec@1 52.344 (59.664)\n",
      "Epoch: [74][312/390]\tTime 0.021 (0.003)\tLoss 1.1014 (1.1670)\tPrec@1 64.062 (59.360)\n",
      "Epoch: [74][390/390]\tTime 0.003 (0.003)\tLoss 1.2447 (1.1758)\tPrec@1 57.500 (58.974)\n",
      "EPOCH: 74 train Results: Prec@1 58.974 Loss: 1.1758\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1858 (1.1858)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5115 (1.2663)\tPrec@1 31.250 (55.250)\n",
      "EPOCH: 74 val Results: Prec@1 55.250 Loss: 1.2663\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [75][0/390]\tTime 0.003 (0.003)\tLoss 1.1922 (1.1922)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [75][78/390]\tTime 0.002 (0.003)\tLoss 1.2618 (1.1286)\tPrec@1 57.812 (60.453)\n",
      "Epoch: [75][156/390]\tTime 0.002 (0.003)\tLoss 0.9152 (1.1534)\tPrec@1 67.969 (59.529)\n",
      "Epoch: [75][234/390]\tTime 0.002 (0.003)\tLoss 1.3059 (1.1636)\tPrec@1 51.562 (59.232)\n",
      "Epoch: [75][312/390]\tTime 0.002 (0.003)\tLoss 1.2593 (1.1697)\tPrec@1 56.250 (59.158)\n",
      "Epoch: [75][390/390]\tTime 0.010 (0.003)\tLoss 1.1872 (1.1781)\tPrec@1 60.000 (58.912)\n",
      "EPOCH: 75 train Results: Prec@1 58.912 Loss: 1.1781\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1837 (1.1837)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2485 (1.2665)\tPrec@1 50.000 (55.180)\n",
      "EPOCH: 75 val Results: Prec@1 55.180 Loss: 1.2665\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [76][0/390]\tTime 0.002 (0.002)\tLoss 1.2273 (1.2273)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.003)\tLoss 1.3169 (1.1395)\tPrec@1 56.250 (60.611)\n",
      "Epoch: [76][156/390]\tTime 0.008 (0.003)\tLoss 0.9831 (1.1522)\tPrec@1 68.750 (59.967)\n",
      "Epoch: [76][234/390]\tTime 0.005 (0.003)\tLoss 1.1100 (1.1626)\tPrec@1 64.844 (59.485)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 1.0373 (1.1671)\tPrec@1 62.500 (59.293)\n",
      "Epoch: [76][390/390]\tTime 0.001 (0.003)\tLoss 1.0499 (1.1732)\tPrec@1 66.250 (59.052)\n",
      "EPOCH: 76 train Results: Prec@1 59.052 Loss: 1.1732\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2039 (1.2039)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5484 (1.2635)\tPrec@1 25.000 (55.500)\n",
      "EPOCH: 76 val Results: Prec@1 55.500 Loss: 1.2635\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [77][0/390]\tTime 0.004 (0.004)\tLoss 1.1137 (1.1137)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [77][78/390]\tTime 0.003 (0.003)\tLoss 1.1964 (1.1324)\tPrec@1 61.719 (60.572)\n",
      "Epoch: [77][156/390]\tTime 0.002 (0.003)\tLoss 1.2276 (1.1493)\tPrec@1 56.250 (59.848)\n",
      "Epoch: [77][234/390]\tTime 0.004 (0.003)\tLoss 1.0763 (1.1627)\tPrec@1 64.844 (59.345)\n",
      "Epoch: [77][312/390]\tTime 0.003 (0.003)\tLoss 1.1849 (1.1711)\tPrec@1 57.812 (59.026)\n",
      "Epoch: [77][390/390]\tTime 0.001 (0.003)\tLoss 1.3006 (1.1792)\tPrec@1 53.750 (58.694)\n",
      "EPOCH: 77 train Results: Prec@1 58.694 Loss: 1.1792\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1447 (1.1447)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4159 (1.2652)\tPrec@1 43.750 (55.170)\n",
      "EPOCH: 77 val Results: Prec@1 55.170 Loss: 1.2652\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [78][0/390]\tTime 0.002 (0.002)\tLoss 1.0945 (1.0945)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [78][78/390]\tTime 0.003 (0.003)\tLoss 1.1446 (1.1194)\tPrec@1 59.375 (61.353)\n",
      "Epoch: [78][156/390]\tTime 0.003 (0.003)\tLoss 1.1951 (1.1444)\tPrec@1 54.688 (60.296)\n",
      "Epoch: [78][234/390]\tTime 0.010 (0.003)\tLoss 1.1483 (1.1600)\tPrec@1 60.938 (59.631)\n",
      "Epoch: [78][312/390]\tTime 0.004 (0.003)\tLoss 1.1734 (1.1715)\tPrec@1 60.938 (59.150)\n",
      "Epoch: [78][390/390]\tTime 0.002 (0.003)\tLoss 1.1431 (1.1763)\tPrec@1 56.250 (58.874)\n",
      "EPOCH: 78 train Results: Prec@1 58.874 Loss: 1.1763\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1079 (1.1079)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0915 (1.2431)\tPrec@1 43.750 (56.180)\n",
      "EPOCH: 78 val Results: Prec@1 56.180 Loss: 1.2431\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [79][0/390]\tTime 0.004 (0.004)\tLoss 1.0747 (1.0747)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [79][78/390]\tTime 0.002 (0.003)\tLoss 1.1223 (1.1044)\tPrec@1 59.375 (61.748)\n",
      "Epoch: [79][156/390]\tTime 0.003 (0.003)\tLoss 1.1076 (1.1278)\tPrec@1 64.062 (60.957)\n",
      "Epoch: [79][234/390]\tTime 0.005 (0.003)\tLoss 1.1935 (1.1454)\tPrec@1 59.375 (60.146)\n",
      "Epoch: [79][312/390]\tTime 0.003 (0.003)\tLoss 1.2168 (1.1609)\tPrec@1 55.469 (59.562)\n",
      "Epoch: [79][390/390]\tTime 0.005 (0.003)\tLoss 1.0503 (1.1724)\tPrec@1 62.500 (58.982)\n",
      "EPOCH: 79 train Results: Prec@1 58.982 Loss: 1.1724\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1472 (1.1472)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4113 (1.2512)\tPrec@1 43.750 (55.550)\n",
      "EPOCH: 79 val Results: Prec@1 55.550 Loss: 1.2512\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [80][0/390]\tTime 0.005 (0.005)\tLoss 1.1025 (1.1025)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [80][78/390]\tTime 0.006 (0.003)\tLoss 1.0898 (1.1306)\tPrec@1 62.500 (60.730)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.1042 (1.1493)\tPrec@1 63.281 (59.763)\n",
      "Epoch: [80][234/390]\tTime 0.009 (0.003)\tLoss 1.0873 (1.1606)\tPrec@1 64.062 (59.618)\n",
      "Epoch: [80][312/390]\tTime 0.002 (0.003)\tLoss 1.3647 (1.1706)\tPrec@1 51.562 (59.063)\n",
      "Epoch: [80][390/390]\tTime 0.004 (0.003)\tLoss 1.1829 (1.1746)\tPrec@1 53.750 (58.972)\n",
      "EPOCH: 80 train Results: Prec@1 58.972 Loss: 1.1746\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1855 (1.1855)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4518 (1.2546)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 80 val Results: Prec@1 55.150 Loss: 1.2546\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [81][0/390]\tTime 0.003 (0.003)\tLoss 1.0108 (1.0108)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [81][78/390]\tTime 0.002 (0.003)\tLoss 1.0775 (1.1209)\tPrec@1 57.031 (61.679)\n",
      "Epoch: [81][156/390]\tTime 0.003 (0.004)\tLoss 1.0646 (1.1428)\tPrec@1 64.062 (60.743)\n",
      "Epoch: [81][234/390]\tTime 0.005 (0.004)\tLoss 1.1104 (1.1501)\tPrec@1 61.719 (60.359)\n",
      "Epoch: [81][312/390]\tTime 0.003 (0.004)\tLoss 1.4085 (1.1635)\tPrec@1 48.438 (59.804)\n",
      "Epoch: [81][390/390]\tTime 0.010 (0.004)\tLoss 1.1096 (1.1731)\tPrec@1 65.000 (59.386)\n",
      "EPOCH: 81 train Results: Prec@1 59.386 Loss: 1.1731\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1748 (1.1748)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3500 (1.2590)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 81 val Results: Prec@1 55.300 Loss: 1.2590\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [82][0/390]\tTime 0.003 (0.003)\tLoss 1.1545 (1.1545)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [82][78/390]\tTime 0.002 (0.003)\tLoss 1.1122 (1.1301)\tPrec@1 62.500 (60.819)\n",
      "Epoch: [82][156/390]\tTime 0.002 (0.003)\tLoss 1.2218 (1.1431)\tPrec@1 63.281 (60.231)\n",
      "Epoch: [82][234/390]\tTime 0.003 (0.003)\tLoss 1.1268 (1.1534)\tPrec@1 61.719 (59.608)\n",
      "Epoch: [82][312/390]\tTime 0.004 (0.003)\tLoss 1.0939 (1.1580)\tPrec@1 59.375 (59.445)\n",
      "Epoch: [82][390/390]\tTime 0.002 (0.003)\tLoss 1.0388 (1.1732)\tPrec@1 61.250 (58.888)\n",
      "EPOCH: 82 train Results: Prec@1 58.888 Loss: 1.1732\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1649 (1.1649)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3947 (1.2554)\tPrec@1 37.500 (55.290)\n",
      "EPOCH: 82 val Results: Prec@1 55.290 Loss: 1.2554\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [83][0/390]\tTime 0.003 (0.003)\tLoss 1.1347 (1.1347)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [83][78/390]\tTime 0.004 (0.003)\tLoss 1.3550 (1.1268)\tPrec@1 58.594 (61.303)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.2594 (1.1454)\tPrec@1 53.125 (60.365)\n",
      "Epoch: [83][234/390]\tTime 0.003 (0.003)\tLoss 1.1009 (1.1566)\tPrec@1 61.719 (59.697)\n",
      "Epoch: [83][312/390]\tTime 0.002 (0.003)\tLoss 1.2362 (1.1663)\tPrec@1 57.031 (59.427)\n",
      "Epoch: [83][390/390]\tTime 0.003 (0.003)\tLoss 1.2709 (1.1730)\tPrec@1 56.250 (59.094)\n",
      "EPOCH: 83 train Results: Prec@1 59.094 Loss: 1.1730\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1082 (1.1082)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5019 (1.2612)\tPrec@1 25.000 (55.140)\n",
      "EPOCH: 83 val Results: Prec@1 55.140 Loss: 1.2612\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [84][0/390]\tTime 0.003 (0.003)\tLoss 0.9378 (0.9378)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [84][78/390]\tTime 0.002 (0.003)\tLoss 1.0043 (1.1298)\tPrec@1 68.750 (61.214)\n",
      "Epoch: [84][156/390]\tTime 0.005 (0.003)\tLoss 1.1682 (1.1476)\tPrec@1 64.062 (60.106)\n",
      "Epoch: [84][234/390]\tTime 0.003 (0.003)\tLoss 1.1147 (1.1610)\tPrec@1 60.156 (59.428)\n",
      "Epoch: [84][312/390]\tTime 0.002 (0.003)\tLoss 1.0786 (1.1648)\tPrec@1 61.719 (59.295)\n",
      "Epoch: [84][390/390]\tTime 0.007 (0.003)\tLoss 1.3547 (1.1684)\tPrec@1 46.250 (59.180)\n",
      "EPOCH: 84 train Results: Prec@1 59.180 Loss: 1.1684\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0878 (1.0878)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3468 (1.2666)\tPrec@1 50.000 (54.930)\n",
      "EPOCH: 84 val Results: Prec@1 54.930 Loss: 1.2666\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [85][0/390]\tTime 0.002 (0.002)\tLoss 1.0676 (1.0676)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [85][78/390]\tTime 0.004 (0.003)\tLoss 1.0503 (1.1188)\tPrec@1 65.625 (61.224)\n",
      "Epoch: [85][156/390]\tTime 0.003 (0.003)\tLoss 1.0092 (1.1390)\tPrec@1 66.406 (60.500)\n",
      "Epoch: [85][234/390]\tTime 0.002 (0.003)\tLoss 1.3166 (1.1515)\tPrec@1 53.125 (59.914)\n",
      "Epoch: [85][312/390]\tTime 0.003 (0.003)\tLoss 1.1315 (1.1632)\tPrec@1 63.281 (59.455)\n",
      "Epoch: [85][390/390]\tTime 0.007 (0.003)\tLoss 1.3046 (1.1708)\tPrec@1 52.500 (59.094)\n",
      "EPOCH: 85 train Results: Prec@1 59.094 Loss: 1.1708\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1530 (1.1530)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6249 (1.2545)\tPrec@1 37.500 (55.760)\n",
      "EPOCH: 85 val Results: Prec@1 55.760 Loss: 1.2545\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [86][0/390]\tTime 0.004 (0.004)\tLoss 1.0929 (1.0929)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [86][78/390]\tTime 0.005 (0.003)\tLoss 1.1914 (1.1481)\tPrec@1 62.500 (60.591)\n",
      "Epoch: [86][156/390]\tTime 0.003 (0.003)\tLoss 1.1077 (1.1464)\tPrec@1 59.375 (60.325)\n",
      "Epoch: [86][234/390]\tTime 0.003 (0.003)\tLoss 1.1283 (1.1562)\tPrec@1 59.375 (59.924)\n",
      "Epoch: [86][312/390]\tTime 0.005 (0.003)\tLoss 1.3260 (1.1657)\tPrec@1 53.906 (59.358)\n",
      "Epoch: [86][390/390]\tTime 0.001 (0.003)\tLoss 1.2855 (1.1752)\tPrec@1 53.750 (58.828)\n",
      "EPOCH: 86 train Results: Prec@1 58.828 Loss: 1.1752\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1681 (1.1681)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3391 (1.2503)\tPrec@1 37.500 (55.170)\n",
      "EPOCH: 86 val Results: Prec@1 55.170 Loss: 1.2503\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [87][0/390]\tTime 0.003 (0.003)\tLoss 1.0850 (1.0850)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [87][78/390]\tTime 0.002 (0.004)\tLoss 0.9426 (1.1081)\tPrec@1 67.969 (61.778)\n",
      "Epoch: [87][156/390]\tTime 0.003 (0.004)\tLoss 1.1558 (1.1364)\tPrec@1 58.594 (60.589)\n",
      "Epoch: [87][234/390]\tTime 0.004 (0.003)\tLoss 1.1298 (1.1518)\tPrec@1 60.156 (59.887)\n",
      "Epoch: [87][312/390]\tTime 0.004 (0.003)\tLoss 1.2644 (1.1655)\tPrec@1 55.469 (59.373)\n",
      "Epoch: [87][390/390]\tTime 0.060 (0.003)\tLoss 1.2813 (1.1715)\tPrec@1 47.500 (58.982)\n",
      "EPOCH: 87 train Results: Prec@1 58.982 Loss: 1.1715\n",
      "Test: [0/78]\tTime 0.016 (0.016)\tLoss 1.1017 (1.1017)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2996 (1.2536)\tPrec@1 43.750 (56.000)\n",
      "EPOCH: 87 val Results: Prec@1 56.000 Loss: 1.2536\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [88][0/390]\tTime 0.003 (0.003)\tLoss 1.0534 (1.0534)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [88][78/390]\tTime 0.002 (0.003)\tLoss 0.9917 (1.1101)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [88][156/390]\tTime 0.003 (0.003)\tLoss 1.1868 (1.1357)\tPrec@1 56.250 (60.778)\n",
      "Epoch: [88][234/390]\tTime 0.003 (0.003)\tLoss 1.1635 (1.1524)\tPrec@1 60.938 (60.050)\n",
      "Epoch: [88][312/390]\tTime 0.008 (0.003)\tLoss 1.1390 (1.1639)\tPrec@1 57.031 (59.330)\n",
      "Epoch: [88][390/390]\tTime 0.003 (0.003)\tLoss 1.5098 (1.1715)\tPrec@1 48.750 (59.052)\n",
      "EPOCH: 88 train Results: Prec@1 59.052 Loss: 1.1715\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1810 (1.1810)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2939 (1.2517)\tPrec@1 50.000 (55.520)\n",
      "EPOCH: 88 val Results: Prec@1 55.520 Loss: 1.2517\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [89][0/390]\tTime 0.004 (0.004)\tLoss 1.1325 (1.1325)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.003)\tLoss 1.0956 (1.1174)\tPrec@1 66.406 (61.155)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1902 (1.1373)\tPrec@1 58.594 (60.301)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 1.1494 (1.1481)\tPrec@1 61.719 (59.904)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.1583 (1.1619)\tPrec@1 63.281 (59.363)\n",
      "Epoch: [89][390/390]\tTime 0.001 (0.003)\tLoss 1.2988 (1.1683)\tPrec@1 57.500 (59.106)\n",
      "EPOCH: 89 train Results: Prec@1 59.106 Loss: 1.1683\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1533 (1.1533)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3914 (1.2604)\tPrec@1 43.750 (54.930)\n",
      "EPOCH: 89 val Results: Prec@1 54.930 Loss: 1.2604\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [90][0/390]\tTime 0.002 (0.002)\tLoss 1.1708 (1.1708)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [90][78/390]\tTime 0.003 (0.003)\tLoss 1.0643 (1.1193)\tPrec@1 64.062 (61.363)\n",
      "Epoch: [90][156/390]\tTime 0.002 (0.003)\tLoss 1.2336 (1.1405)\tPrec@1 55.469 (60.460)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.2358 (1.1524)\tPrec@1 50.781 (59.904)\n",
      "Epoch: [90][312/390]\tTime 0.004 (0.003)\tLoss 1.1836 (1.1582)\tPrec@1 58.594 (59.610)\n",
      "Epoch: [90][390/390]\tTime 0.003 (0.003)\tLoss 1.1445 (1.1672)\tPrec@1 56.250 (59.238)\n",
      "EPOCH: 90 train Results: Prec@1 59.238 Loss: 1.1672\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1213 (1.1213)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7331 (1.2548)\tPrec@1 25.000 (55.230)\n",
      "EPOCH: 90 val Results: Prec@1 55.230 Loss: 1.2548\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [91][0/390]\tTime 0.005 (0.005)\tLoss 1.2999 (1.2999)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [91][78/390]\tTime 0.002 (0.003)\tLoss 1.1429 (1.1134)\tPrec@1 58.594 (61.363)\n",
      "Epoch: [91][156/390]\tTime 0.006 (0.003)\tLoss 1.2280 (1.1344)\tPrec@1 57.812 (60.644)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.003)\tLoss 1.1512 (1.1459)\tPrec@1 58.594 (60.219)\n",
      "Epoch: [91][312/390]\tTime 0.004 (0.003)\tLoss 1.1172 (1.1579)\tPrec@1 59.375 (59.727)\n",
      "Epoch: [91][390/390]\tTime 0.003 (0.003)\tLoss 1.2097 (1.1693)\tPrec@1 55.000 (59.212)\n",
      "EPOCH: 91 train Results: Prec@1 59.212 Loss: 1.1693\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1399 (1.1399)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3508 (1.2530)\tPrec@1 37.500 (55.440)\n",
      "EPOCH: 91 val Results: Prec@1 55.440 Loss: 1.2530\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [92][0/390]\tTime 0.002 (0.002)\tLoss 1.1879 (1.1879)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.003)\tLoss 1.1679 (1.1193)\tPrec@1 57.031 (61.392)\n",
      "Epoch: [92][156/390]\tTime 0.013 (0.003)\tLoss 1.1429 (1.1404)\tPrec@1 64.062 (60.385)\n",
      "Epoch: [92][234/390]\tTime 0.002 (0.003)\tLoss 1.1364 (1.1499)\tPrec@1 60.156 (60.037)\n",
      "Epoch: [92][312/390]\tTime 0.003 (0.003)\tLoss 1.2271 (1.1591)\tPrec@1 53.125 (59.437)\n",
      "Epoch: [92][390/390]\tTime 0.003 (0.003)\tLoss 1.2388 (1.1663)\tPrec@1 60.000 (59.088)\n",
      "EPOCH: 92 train Results: Prec@1 59.088 Loss: 1.1663\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1283 (1.1283)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4066 (1.2560)\tPrec@1 50.000 (55.400)\n",
      "EPOCH: 92 val Results: Prec@1 55.400 Loss: 1.2560\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 1.2099 (1.2099)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [93][78/390]\tTime 0.003 (0.003)\tLoss 1.0982 (1.1030)\tPrec@1 65.625 (61.689)\n",
      "Epoch: [93][156/390]\tTime 0.002 (0.003)\tLoss 1.2040 (1.1269)\tPrec@1 60.156 (60.733)\n",
      "Epoch: [93][234/390]\tTime 0.015 (0.003)\tLoss 1.2940 (1.1462)\tPrec@1 57.812 (60.116)\n",
      "Epoch: [93][312/390]\tTime 0.002 (0.003)\tLoss 1.2617 (1.1553)\tPrec@1 57.031 (59.734)\n",
      "Epoch: [93][390/390]\tTime 0.003 (0.003)\tLoss 1.2021 (1.1639)\tPrec@1 55.000 (59.338)\n",
      "EPOCH: 93 train Results: Prec@1 59.338 Loss: 1.1639\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1017 (1.1017)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2938 (1.2616)\tPrec@1 50.000 (55.190)\n",
      "EPOCH: 93 val Results: Prec@1 55.190 Loss: 1.2616\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [94][0/390]\tTime 0.003 (0.003)\tLoss 0.9905 (0.9905)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [94][78/390]\tTime 0.009 (0.005)\tLoss 1.1120 (1.1077)\tPrec@1 60.938 (61.531)\n",
      "Epoch: [94][156/390]\tTime 0.006 (0.004)\tLoss 1.1145 (1.1298)\tPrec@1 64.844 (60.579)\n",
      "Epoch: [94][234/390]\tTime 0.002 (0.004)\tLoss 0.9897 (1.1497)\tPrec@1 64.062 (59.694)\n",
      "Epoch: [94][312/390]\tTime 0.002 (0.004)\tLoss 1.2345 (1.1597)\tPrec@1 57.031 (59.250)\n",
      "Epoch: [94][390/390]\tTime 0.002 (0.004)\tLoss 1.4270 (1.1677)\tPrec@1 55.000 (59.056)\n",
      "EPOCH: 94 train Results: Prec@1 59.056 Loss: 1.1677\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1847 (1.1847)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2998 (1.2552)\tPrec@1 56.250 (55.410)\n",
      "EPOCH: 94 val Results: Prec@1 55.410 Loss: 1.2552\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [95][0/390]\tTime 0.005 (0.005)\tLoss 1.1003 (1.1003)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [95][78/390]\tTime 0.002 (0.003)\tLoss 1.2000 (1.1067)\tPrec@1 51.562 (61.640)\n",
      "Epoch: [95][156/390]\tTime 0.006 (0.004)\tLoss 1.1721 (1.1272)\tPrec@1 55.469 (60.654)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.004)\tLoss 1.1269 (1.1404)\tPrec@1 57.812 (60.229)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.003)\tLoss 1.1024 (1.1532)\tPrec@1 66.406 (59.789)\n",
      "Epoch: [95][390/390]\tTime 0.003 (0.003)\tLoss 1.2754 (1.1641)\tPrec@1 57.500 (59.398)\n",
      "EPOCH: 95 train Results: Prec@1 59.398 Loss: 1.1641\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1276 (1.1276)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3529 (1.2528)\tPrec@1 43.750 (55.190)\n",
      "EPOCH: 95 val Results: Prec@1 55.190 Loss: 1.2528\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [96][0/390]\tTime 0.005 (0.005)\tLoss 1.1409 (1.1409)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.004)\tLoss 1.1534 (1.1200)\tPrec@1 57.031 (61.274)\n",
      "Epoch: [96][156/390]\tTime 0.003 (0.003)\tLoss 1.0143 (1.1388)\tPrec@1 66.406 (60.679)\n",
      "Epoch: [96][234/390]\tTime 0.004 (0.003)\tLoss 1.1599 (1.1545)\tPrec@1 60.938 (59.894)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 1.0964 (1.1574)\tPrec@1 62.500 (59.680)\n",
      "Epoch: [96][390/390]\tTime 0.001 (0.003)\tLoss 1.2850 (1.1645)\tPrec@1 53.750 (59.402)\n",
      "EPOCH: 96 train Results: Prec@1 59.402 Loss: 1.1645\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1428 (1.1428)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.3230 (1.2560)\tPrec@1 43.750 (55.660)\n",
      "EPOCH: 96 val Results: Prec@1 55.660 Loss: 1.2560\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [97][0/390]\tTime 0.005 (0.005)\tLoss 1.1736 (1.1736)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [97][78/390]\tTime 0.002 (0.003)\tLoss 1.1376 (1.1117)\tPrec@1 61.719 (61.343)\n",
      "Epoch: [97][156/390]\tTime 0.002 (0.003)\tLoss 1.3368 (1.1341)\tPrec@1 54.688 (60.554)\n",
      "Epoch: [97][234/390]\tTime 0.005 (0.003)\tLoss 1.2938 (1.1466)\tPrec@1 52.344 (60.017)\n",
      "Epoch: [97][312/390]\tTime 0.004 (0.003)\tLoss 1.2113 (1.1560)\tPrec@1 57.031 (59.622)\n",
      "Epoch: [97][390/390]\tTime 0.007 (0.003)\tLoss 0.9993 (1.1654)\tPrec@1 63.750 (59.268)\n",
      "EPOCH: 97 train Results: Prec@1 59.268 Loss: 1.1654\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1630 (1.1630)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5429 (1.2505)\tPrec@1 43.750 (56.000)\n",
      "EPOCH: 97 val Results: Prec@1 56.000 Loss: 1.2505\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [98][0/390]\tTime 0.015 (0.015)\tLoss 1.1468 (1.1468)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [98][78/390]\tTime 0.003 (0.003)\tLoss 0.9864 (1.1128)\tPrec@1 68.750 (61.027)\n",
      "Epoch: [98][156/390]\tTime 0.004 (0.003)\tLoss 1.2331 (1.1306)\tPrec@1 57.031 (60.425)\n",
      "Epoch: [98][234/390]\tTime 0.004 (0.004)\tLoss 1.1144 (1.1417)\tPrec@1 63.281 (60.013)\n",
      "Epoch: [98][312/390]\tTime 0.005 (0.004)\tLoss 1.3737 (1.1530)\tPrec@1 46.094 (59.472)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.004)\tLoss 1.1650 (1.1617)\tPrec@1 61.250 (59.254)\n",
      "EPOCH: 98 train Results: Prec@1 59.254 Loss: 1.1617\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1629 (1.1629)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6500 (1.2540)\tPrec@1 31.250 (55.190)\n",
      "EPOCH: 98 val Results: Prec@1 55.190 Loss: 1.2540\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [99][0/390]\tTime 0.003 (0.003)\tLoss 1.0752 (1.0752)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [99][78/390]\tTime 0.008 (0.003)\tLoss 1.0062 (1.1140)\tPrec@1 63.281 (61.412)\n",
      "Epoch: [99][156/390]\tTime 0.002 (0.003)\tLoss 1.2001 (1.1332)\tPrec@1 61.719 (60.584)\n",
      "Epoch: [99][234/390]\tTime 0.005 (0.003)\tLoss 1.3677 (1.1421)\tPrec@1 49.219 (60.083)\n",
      "Epoch: [99][312/390]\tTime 0.003 (0.003)\tLoss 1.0754 (1.1521)\tPrec@1 60.938 (59.602)\n",
      "Epoch: [99][390/390]\tTime 0.002 (0.003)\tLoss 1.0439 (1.1610)\tPrec@1 70.000 (59.296)\n",
      "EPOCH: 99 train Results: Prec@1 59.296 Loss: 1.1610\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1264 (1.1264)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5350 (1.2551)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 99 val Results: Prec@1 55.150 Loss: 1.2551\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [100][0/390]\tTime 0.005 (0.005)\tLoss 1.0674 (1.0674)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [100][78/390]\tTime 0.012 (0.003)\tLoss 1.2280 (1.1174)\tPrec@1 57.031 (60.730)\n",
      "Epoch: [100][156/390]\tTime 0.004 (0.003)\tLoss 1.1040 (1.1348)\tPrec@1 62.500 (60.186)\n",
      "Epoch: [100][234/390]\tTime 0.002 (0.003)\tLoss 1.1644 (1.1483)\tPrec@1 57.031 (59.535)\n",
      "Epoch: [100][312/390]\tTime 0.002 (0.003)\tLoss 1.1952 (1.1583)\tPrec@1 60.156 (59.228)\n",
      "Epoch: [100][390/390]\tTime 0.001 (0.003)\tLoss 1.2185 (1.1616)\tPrec@1 60.000 (59.196)\n",
      "EPOCH: 100 train Results: Prec@1 59.196 Loss: 1.1616\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1227 (1.1227)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5100 (1.2494)\tPrec@1 18.750 (55.800)\n",
      "EPOCH: 100 val Results: Prec@1 55.800 Loss: 1.2494\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [101][0/390]\tTime 0.004 (0.004)\tLoss 1.1172 (1.1172)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.003)\tLoss 1.1333 (1.1230)\tPrec@1 60.156 (60.690)\n",
      "Epoch: [101][156/390]\tTime 0.008 (0.003)\tLoss 1.2229 (1.1394)\tPrec@1 57.031 (60.032)\n",
      "Epoch: [101][234/390]\tTime 0.002 (0.003)\tLoss 1.2540 (1.1490)\tPrec@1 54.688 (59.890)\n",
      "Epoch: [101][312/390]\tTime 0.004 (0.003)\tLoss 1.1674 (1.1531)\tPrec@1 58.594 (59.647)\n",
      "Epoch: [101][390/390]\tTime 0.001 (0.003)\tLoss 1.1639 (1.1634)\tPrec@1 57.500 (59.240)\n",
      "EPOCH: 101 train Results: Prec@1 59.240 Loss: 1.1634\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1628 (1.1628)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3595 (1.2539)\tPrec@1 43.750 (55.250)\n",
      "EPOCH: 101 val Results: Prec@1 55.250 Loss: 1.2539\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [102][0/390]\tTime 0.004 (0.004)\tLoss 1.1053 (1.1053)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [102][78/390]\tTime 0.002 (0.004)\tLoss 1.1032 (1.1073)\tPrec@1 57.031 (61.570)\n",
      "Epoch: [102][156/390]\tTime 0.002 (0.004)\tLoss 1.1837 (1.1321)\tPrec@1 61.719 (60.738)\n",
      "Epoch: [102][234/390]\tTime 0.002 (0.003)\tLoss 1.2328 (1.1429)\tPrec@1 59.375 (60.153)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.003)\tLoss 1.2002 (1.1532)\tPrec@1 58.594 (59.754)\n",
      "Epoch: [102][390/390]\tTime 0.002 (0.003)\tLoss 1.2033 (1.1604)\tPrec@1 58.750 (59.540)\n",
      "EPOCH: 102 train Results: Prec@1 59.540 Loss: 1.1604\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0990 (1.0990)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2874 (1.2475)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 102 val Results: Prec@1 55.630 Loss: 1.2475\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [103][0/390]\tTime 0.002 (0.002)\tLoss 1.0731 (1.0731)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [103][78/390]\tTime 0.003 (0.004)\tLoss 1.0491 (1.1195)\tPrec@1 67.188 (61.333)\n",
      "Epoch: [103][156/390]\tTime 0.002 (0.004)\tLoss 1.2140 (1.1355)\tPrec@1 57.812 (60.370)\n",
      "Epoch: [103][234/390]\tTime 0.002 (0.004)\tLoss 1.1978 (1.1471)\tPrec@1 62.500 (59.884)\n",
      "Epoch: [103][312/390]\tTime 0.003 (0.003)\tLoss 1.2918 (1.1537)\tPrec@1 53.125 (59.764)\n",
      "Epoch: [103][390/390]\tTime 0.001 (0.003)\tLoss 1.2263 (1.1586)\tPrec@1 62.500 (59.594)\n",
      "EPOCH: 103 train Results: Prec@1 59.594 Loss: 1.1586\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1406 (1.1406)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3823 (1.2644)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 103 val Results: Prec@1 55.150 Loss: 1.2644\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [104][0/390]\tTime 0.005 (0.005)\tLoss 1.2194 (1.2194)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 1.1501 (1.1091)\tPrec@1 64.062 (61.758)\n",
      "Epoch: [104][156/390]\tTime 0.003 (0.003)\tLoss 1.1605 (1.1256)\tPrec@1 57.031 (60.853)\n",
      "Epoch: [104][234/390]\tTime 0.008 (0.003)\tLoss 1.1555 (1.1382)\tPrec@1 60.938 (60.422)\n",
      "Epoch: [104][312/390]\tTime 0.004 (0.003)\tLoss 1.1257 (1.1527)\tPrec@1 59.375 (59.847)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.003)\tLoss 1.0909 (1.1605)\tPrec@1 66.250 (59.530)\n",
      "EPOCH: 104 train Results: Prec@1 59.530 Loss: 1.1605\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1544 (1.1544)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3705 (1.2432)\tPrec@1 37.500 (55.750)\n",
      "EPOCH: 104 val Results: Prec@1 55.750 Loss: 1.2432\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [105][0/390]\tTime 0.003 (0.003)\tLoss 1.0700 (1.0700)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [105][78/390]\tTime 0.003 (0.003)\tLoss 1.1152 (1.1186)\tPrec@1 61.719 (61.363)\n",
      "Epoch: [105][156/390]\tTime 0.009 (0.003)\tLoss 1.1588 (1.1302)\tPrec@1 57.812 (60.559)\n",
      "Epoch: [105][234/390]\tTime 0.002 (0.003)\tLoss 1.2860 (1.1385)\tPrec@1 56.250 (59.937)\n",
      "Epoch: [105][312/390]\tTime 0.002 (0.003)\tLoss 1.1615 (1.1469)\tPrec@1 60.938 (59.759)\n",
      "Epoch: [105][390/390]\tTime 0.001 (0.003)\tLoss 1.1143 (1.1609)\tPrec@1 66.250 (59.280)\n",
      "EPOCH: 105 train Results: Prec@1 59.280 Loss: 1.1609\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1195 (1.1195)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2998 (1.2574)\tPrec@1 37.500 (55.220)\n",
      "EPOCH: 105 val Results: Prec@1 55.220 Loss: 1.2574\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [106][0/390]\tTime 0.003 (0.003)\tLoss 1.0140 (1.0140)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [106][78/390]\tTime 0.002 (0.004)\tLoss 0.9882 (1.0987)\tPrec@1 71.094 (62.025)\n",
      "Epoch: [106][156/390]\tTime 0.002 (0.003)\tLoss 1.2045 (1.1268)\tPrec@1 53.906 (60.549)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.003)\tLoss 1.0730 (1.1436)\tPrec@1 64.844 (60.060)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.003)\tLoss 1.1332 (1.1543)\tPrec@1 59.375 (59.595)\n",
      "Epoch: [106][390/390]\tTime 0.001 (0.003)\tLoss 1.2017 (1.1610)\tPrec@1 57.500 (59.258)\n",
      "EPOCH: 106 train Results: Prec@1 59.258 Loss: 1.1610\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1358 (1.1358)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6795 (1.2565)\tPrec@1 31.250 (55.420)\n",
      "EPOCH: 106 val Results: Prec@1 55.420 Loss: 1.2565\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [107][0/390]\tTime 0.007 (0.007)\tLoss 0.9820 (0.9820)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [107][78/390]\tTime 0.002 (0.003)\tLoss 1.2222 (1.1132)\tPrec@1 55.469 (61.472)\n",
      "Epoch: [107][156/390]\tTime 0.003 (0.003)\tLoss 1.2829 (1.1308)\tPrec@1 54.688 (60.584)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.003)\tLoss 1.1710 (1.1390)\tPrec@1 56.250 (60.193)\n",
      "Epoch: [107][312/390]\tTime 0.003 (0.003)\tLoss 1.1899 (1.1536)\tPrec@1 59.375 (59.615)\n",
      "Epoch: [107][390/390]\tTime 0.001 (0.003)\tLoss 1.0969 (1.1646)\tPrec@1 63.750 (59.166)\n",
      "EPOCH: 107 train Results: Prec@1 59.166 Loss: 1.1646\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1516 (1.1516)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5389 (1.2502)\tPrec@1 37.500 (55.330)\n",
      "EPOCH: 107 val Results: Prec@1 55.330 Loss: 1.2502\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [108][0/390]\tTime 0.004 (0.004)\tLoss 1.1104 (1.1104)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [108][78/390]\tTime 0.005 (0.003)\tLoss 1.1501 (1.1004)\tPrec@1 68.750 (62.085)\n",
      "Epoch: [108][156/390]\tTime 0.002 (0.003)\tLoss 1.1652 (1.1210)\tPrec@1 60.938 (61.151)\n",
      "Epoch: [108][234/390]\tTime 0.003 (0.003)\tLoss 1.2601 (1.1375)\tPrec@1 51.562 (60.442)\n",
      "Epoch: [108][312/390]\tTime 0.003 (0.003)\tLoss 1.0845 (1.1450)\tPrec@1 60.938 (60.104)\n",
      "Epoch: [108][390/390]\tTime 0.001 (0.003)\tLoss 1.1974 (1.1552)\tPrec@1 56.250 (59.748)\n",
      "EPOCH: 108 train Results: Prec@1 59.748 Loss: 1.1552\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1951 (1.1951)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6043 (1.2613)\tPrec@1 25.000 (54.870)\n",
      "EPOCH: 108 val Results: Prec@1 54.870 Loss: 1.2613\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [109][0/390]\tTime 0.004 (0.004)\tLoss 1.1584 (1.1584)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [109][78/390]\tTime 0.002 (0.003)\tLoss 1.1717 (1.1112)\tPrec@1 60.938 (61.580)\n",
      "Epoch: [109][156/390]\tTime 0.007 (0.003)\tLoss 1.1743 (1.1351)\tPrec@1 59.375 (60.669)\n",
      "Epoch: [109][234/390]\tTime 0.003 (0.003)\tLoss 1.0364 (1.1456)\tPrec@1 64.844 (59.930)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.003)\tLoss 1.1518 (1.1507)\tPrec@1 59.375 (59.754)\n",
      "Epoch: [109][390/390]\tTime 0.004 (0.003)\tLoss 1.2771 (1.1574)\tPrec@1 56.250 (59.472)\n",
      "EPOCH: 109 train Results: Prec@1 59.472 Loss: 1.1574\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1203 (1.1203)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3658 (1.2446)\tPrec@1 43.750 (55.930)\n",
      "EPOCH: 109 val Results: Prec@1 55.930 Loss: 1.2446\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [110][0/390]\tTime 0.003 (0.003)\tLoss 1.1548 (1.1548)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 1.1107 (1.1081)\tPrec@1 59.375 (61.284)\n",
      "Epoch: [110][156/390]\tTime 0.005 (0.004)\tLoss 1.0174 (1.1318)\tPrec@1 63.281 (60.375)\n",
      "Epoch: [110][234/390]\tTime 0.003 (0.004)\tLoss 1.2354 (1.1406)\tPrec@1 56.250 (60.126)\n",
      "Epoch: [110][312/390]\tTime 0.011 (0.004)\tLoss 1.2042 (1.1561)\tPrec@1 60.156 (59.697)\n",
      "Epoch: [110][390/390]\tTime 0.004 (0.004)\tLoss 1.2862 (1.1594)\tPrec@1 53.750 (59.472)\n",
      "EPOCH: 110 train Results: Prec@1 59.472 Loss: 1.1594\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1011 (1.1011)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6277 (1.2420)\tPrec@1 25.000 (55.760)\n",
      "EPOCH: 110 val Results: Prec@1 55.760 Loss: 1.2420\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [111][0/390]\tTime 0.004 (0.004)\tLoss 1.0992 (1.0992)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [111][78/390]\tTime 0.002 (0.003)\tLoss 1.0581 (1.1094)\tPrec@1 60.156 (61.590)\n",
      "Epoch: [111][156/390]\tTime 0.010 (0.003)\tLoss 1.1537 (1.1335)\tPrec@1 60.156 (60.544)\n",
      "Epoch: [111][234/390]\tTime 0.002 (0.003)\tLoss 1.3482 (1.1497)\tPrec@1 51.562 (60.086)\n",
      "Epoch: [111][312/390]\tTime 0.004 (0.003)\tLoss 1.1937 (1.1588)\tPrec@1 60.938 (59.600)\n",
      "Epoch: [111][390/390]\tTime 0.002 (0.003)\tLoss 1.0897 (1.1624)\tPrec@1 63.750 (59.412)\n",
      "EPOCH: 111 train Results: Prec@1 59.412 Loss: 1.1624\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1651 (1.1651)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3333 (1.2560)\tPrec@1 43.750 (55.210)\n",
      "EPOCH: 111 val Results: Prec@1 55.210 Loss: 1.2560\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [112][0/390]\tTime 0.005 (0.005)\tLoss 1.0849 (1.0849)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.0992 (1.1039)\tPrec@1 58.594 (61.788)\n",
      "Epoch: [112][156/390]\tTime 0.002 (0.003)\tLoss 1.2463 (1.1275)\tPrec@1 55.469 (60.823)\n",
      "Epoch: [112][234/390]\tTime 0.002 (0.003)\tLoss 1.1329 (1.1375)\tPrec@1 60.938 (60.572)\n",
      "Epoch: [112][312/390]\tTime 0.009 (0.003)\tLoss 1.0559 (1.1545)\tPrec@1 62.500 (59.857)\n",
      "Epoch: [112][390/390]\tTime 0.001 (0.003)\tLoss 1.3894 (1.1596)\tPrec@1 55.000 (59.618)\n",
      "EPOCH: 112 train Results: Prec@1 59.618 Loss: 1.1596\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1015 (1.1015)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5741 (1.2467)\tPrec@1 31.250 (55.740)\n",
      "EPOCH: 112 val Results: Prec@1 55.740 Loss: 1.2467\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 1.0990 (1.0990)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [113][78/390]\tTime 0.002 (0.003)\tLoss 1.1835 (1.1128)\tPrec@1 55.469 (61.343)\n",
      "Epoch: [113][156/390]\tTime 0.002 (0.003)\tLoss 1.1533 (1.1283)\tPrec@1 57.031 (60.793)\n",
      "Epoch: [113][234/390]\tTime 0.005 (0.003)\tLoss 0.9790 (1.1393)\tPrec@1 67.188 (60.316)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.003)\tLoss 1.0858 (1.1493)\tPrec@1 62.500 (59.927)\n",
      "Epoch: [113][390/390]\tTime 0.001 (0.003)\tLoss 1.2183 (1.1565)\tPrec@1 65.000 (59.640)\n",
      "EPOCH: 113 train Results: Prec@1 59.640 Loss: 1.1565\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2342 (1.2342)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3700 (1.2413)\tPrec@1 43.750 (55.730)\n",
      "EPOCH: 113 val Results: Prec@1 55.730 Loss: 1.2413\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [114][0/390]\tTime 0.008 (0.008)\tLoss 1.1404 (1.1404)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.003)\tLoss 0.9229 (1.1093)\tPrec@1 68.750 (61.729)\n",
      "Epoch: [114][156/390]\tTime 0.008 (0.004)\tLoss 1.1938 (1.1284)\tPrec@1 59.375 (61.037)\n",
      "Epoch: [114][234/390]\tTime 0.003 (0.004)\tLoss 1.1194 (1.1444)\tPrec@1 56.250 (60.316)\n",
      "Epoch: [114][312/390]\tTime 0.004 (0.003)\tLoss 1.0218 (1.1487)\tPrec@1 66.406 (60.206)\n",
      "Epoch: [114][390/390]\tTime 0.001 (0.003)\tLoss 1.2439 (1.1571)\tPrec@1 52.500 (59.756)\n",
      "EPOCH: 114 train Results: Prec@1 59.756 Loss: 1.1571\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1228 (1.1228)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6034 (1.2510)\tPrec@1 43.750 (55.490)\n",
      "EPOCH: 114 val Results: Prec@1 55.490 Loss: 1.2510\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [115][0/390]\tTime 0.004 (0.004)\tLoss 0.9699 (0.9699)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [115][78/390]\tTime 0.003 (0.003)\tLoss 1.0267 (1.0982)\tPrec@1 68.750 (62.223)\n",
      "Epoch: [115][156/390]\tTime 0.004 (0.003)\tLoss 1.3133 (1.1186)\tPrec@1 52.344 (61.137)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.004)\tLoss 1.2978 (1.1399)\tPrec@1 56.250 (60.133)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.004)\tLoss 1.3088 (1.1503)\tPrec@1 50.781 (59.844)\n",
      "Epoch: [115][390/390]\tTime 0.003 (0.004)\tLoss 1.3241 (1.1558)\tPrec@1 55.000 (59.566)\n",
      "EPOCH: 115 train Results: Prec@1 59.566 Loss: 1.1558\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1641 (1.1641)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5520 (1.2544)\tPrec@1 31.250 (55.560)\n",
      "EPOCH: 115 val Results: Prec@1 55.560 Loss: 1.2544\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [116][0/390]\tTime 0.002 (0.002)\tLoss 1.1826 (1.1826)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [116][78/390]\tTime 0.004 (0.003)\tLoss 1.1001 (1.1068)\tPrec@1 57.812 (62.154)\n",
      "Epoch: [116][156/390]\tTime 0.007 (0.003)\tLoss 1.2625 (1.1265)\tPrec@1 51.562 (60.813)\n",
      "Epoch: [116][234/390]\tTime 0.003 (0.003)\tLoss 1.1684 (1.1404)\tPrec@1 58.594 (60.249)\n",
      "Epoch: [116][312/390]\tTime 0.007 (0.003)\tLoss 1.2519 (1.1486)\tPrec@1 52.344 (59.907)\n",
      "Epoch: [116][390/390]\tTime 0.003 (0.003)\tLoss 1.2550 (1.1544)\tPrec@1 58.750 (59.690)\n",
      "EPOCH: 116 train Results: Prec@1 59.690 Loss: 1.1544\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1150 (1.1150)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3419 (1.2531)\tPrec@1 43.750 (55.390)\n",
      "EPOCH: 116 val Results: Prec@1 55.390 Loss: 1.2531\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [117][0/390]\tTime 0.004 (0.004)\tLoss 1.2014 (1.2014)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [117][78/390]\tTime 0.003 (0.003)\tLoss 1.1718 (1.1102)\tPrec@1 60.156 (61.650)\n",
      "Epoch: [117][156/390]\tTime 0.002 (0.004)\tLoss 1.1847 (1.1257)\tPrec@1 57.031 (60.923)\n",
      "Epoch: [117][234/390]\tTime 0.002 (0.004)\tLoss 1.2137 (1.1422)\tPrec@1 57.812 (60.243)\n",
      "Epoch: [117][312/390]\tTime 0.003 (0.004)\tLoss 1.1014 (1.1528)\tPrec@1 56.250 (59.769)\n",
      "Epoch: [117][390/390]\tTime 0.009 (0.004)\tLoss 1.1629 (1.1570)\tPrec@1 57.500 (59.576)\n",
      "EPOCH: 117 train Results: Prec@1 59.576 Loss: 1.1570\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0664 (1.0664)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.5636 (1.2429)\tPrec@1 31.250 (56.340)\n",
      "EPOCH: 117 val Results: Prec@1 56.340 Loss: 1.2429\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [118][0/390]\tTime 0.008 (0.008)\tLoss 1.0936 (1.0936)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [118][78/390]\tTime 0.014 (0.004)\tLoss 1.0596 (1.1198)\tPrec@1 60.938 (61.620)\n",
      "Epoch: [118][156/390]\tTime 0.002 (0.004)\tLoss 1.1126 (1.1223)\tPrec@1 60.156 (61.341)\n",
      "Epoch: [118][234/390]\tTime 0.004 (0.004)\tLoss 1.1953 (1.1308)\tPrec@1 51.562 (60.662)\n",
      "Epoch: [118][312/390]\tTime 0.004 (0.004)\tLoss 1.1852 (1.1452)\tPrec@1 64.844 (60.059)\n",
      "Epoch: [118][390/390]\tTime 0.004 (0.004)\tLoss 1.2045 (1.1570)\tPrec@1 63.750 (59.630)\n",
      "EPOCH: 118 train Results: Prec@1 59.630 Loss: 1.1570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1262 (1.1262)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2910 (1.2529)\tPrec@1 43.750 (55.640)\n",
      "EPOCH: 118 val Results: Prec@1 55.640 Loss: 1.2529\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [119][0/390]\tTime 0.006 (0.006)\tLoss 1.0976 (1.0976)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [119][78/390]\tTime 0.004 (0.004)\tLoss 1.2430 (1.0954)\tPrec@1 57.031 (62.816)\n",
      "Epoch: [119][156/390]\tTime 0.010 (0.004)\tLoss 1.2148 (1.1195)\tPrec@1 59.375 (61.321)\n",
      "Epoch: [119][234/390]\tTime 0.002 (0.004)\tLoss 1.1747 (1.1342)\tPrec@1 60.156 (60.588)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.003)\tLoss 1.1920 (1.1433)\tPrec@1 61.719 (60.074)\n",
      "Epoch: [119][390/390]\tTime 0.003 (0.003)\tLoss 1.2488 (1.1539)\tPrec@1 61.250 (59.710)\n",
      "EPOCH: 119 train Results: Prec@1 59.710 Loss: 1.1539\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1813 (1.1813)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4103 (1.2500)\tPrec@1 31.250 (55.720)\n",
      "EPOCH: 119 val Results: Prec@1 55.720 Loss: 1.2500\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [120][0/390]\tTime 0.002 (0.002)\tLoss 1.0023 (1.0023)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.003)\tLoss 1.1983 (1.1136)\tPrec@1 54.688 (61.501)\n",
      "Epoch: [120][156/390]\tTime 0.003 (0.003)\tLoss 1.0077 (1.1306)\tPrec@1 65.625 (60.738)\n",
      "Epoch: [120][234/390]\tTime 0.002 (0.003)\tLoss 1.1844 (1.1418)\tPrec@1 64.062 (60.346)\n",
      "Epoch: [120][312/390]\tTime 0.004 (0.003)\tLoss 1.2965 (1.1472)\tPrec@1 55.469 (60.016)\n",
      "Epoch: [120][390/390]\tTime 0.001 (0.003)\tLoss 1.0621 (1.1553)\tPrec@1 62.500 (59.640)\n",
      "EPOCH: 120 train Results: Prec@1 59.640 Loss: 1.1553\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1441 (1.1441)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1743 (1.2356)\tPrec@1 43.750 (56.140)\n",
      "EPOCH: 120 val Results: Prec@1 56.140 Loss: 1.2356\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [121][0/390]\tTime 0.002 (0.002)\tLoss 0.9757 (0.9757)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [121][78/390]\tTime 0.003 (0.003)\tLoss 1.0946 (1.0958)\tPrec@1 61.719 (61.353)\n",
      "Epoch: [121][156/390]\tTime 0.002 (0.003)\tLoss 1.1366 (1.1235)\tPrec@1 60.938 (60.704)\n",
      "Epoch: [121][234/390]\tTime 0.002 (0.003)\tLoss 1.2376 (1.1332)\tPrec@1 49.219 (60.436)\n",
      "Epoch: [121][312/390]\tTime 0.002 (0.003)\tLoss 1.0473 (1.1437)\tPrec@1 67.188 (60.024)\n",
      "Epoch: [121][390/390]\tTime 0.001 (0.003)\tLoss 0.9772 (1.1505)\tPrec@1 65.000 (59.786)\n",
      "EPOCH: 121 train Results: Prec@1 59.786 Loss: 1.1505\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1230 (1.1230)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4132 (1.2510)\tPrec@1 37.500 (55.070)\n",
      "EPOCH: 121 val Results: Prec@1 55.070 Loss: 1.2510\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [122][0/390]\tTime 0.002 (0.002)\tLoss 1.0414 (1.0414)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [122][78/390]\tTime 0.006 (0.005)\tLoss 0.9809 (1.0957)\tPrec@1 65.625 (62.391)\n",
      "Epoch: [122][156/390]\tTime 0.002 (0.004)\tLoss 1.0959 (1.1157)\tPrec@1 64.062 (61.664)\n",
      "Epoch: [122][234/390]\tTime 0.004 (0.004)\tLoss 1.2953 (1.1367)\tPrec@1 63.281 (60.768)\n",
      "Epoch: [122][312/390]\tTime 0.002 (0.003)\tLoss 1.1797 (1.1502)\tPrec@1 57.031 (60.111)\n",
      "Epoch: [122][390/390]\tTime 0.001 (0.003)\tLoss 1.3518 (1.1576)\tPrec@1 50.000 (59.784)\n",
      "EPOCH: 122 train Results: Prec@1 59.784 Loss: 1.1576\n",
      "Test: [0/78]\tTime 0.032 (0.032)\tLoss 1.1409 (1.1409)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3115 (1.2566)\tPrec@1 37.500 (55.970)\n",
      "EPOCH: 122 val Results: Prec@1 55.970 Loss: 1.2566\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [123][0/390]\tTime 0.003 (0.003)\tLoss 0.9790 (0.9790)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [123][78/390]\tTime 0.002 (0.003)\tLoss 1.1306 (1.0911)\tPrec@1 59.375 (62.500)\n",
      "Epoch: [123][156/390]\tTime 0.002 (0.003)\tLoss 1.2108 (1.1170)\tPrec@1 60.938 (61.390)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.003)\tLoss 1.3015 (1.1308)\tPrec@1 51.562 (60.898)\n",
      "Epoch: [123][312/390]\tTime 0.002 (0.003)\tLoss 1.2367 (1.1435)\tPrec@1 53.906 (60.378)\n",
      "Epoch: [123][390/390]\tTime 0.003 (0.003)\tLoss 1.1876 (1.1522)\tPrec@1 57.500 (59.896)\n",
      "EPOCH: 123 train Results: Prec@1 59.896 Loss: 1.1522\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1902 (1.1902)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4111 (1.2616)\tPrec@1 50.000 (55.460)\n",
      "EPOCH: 123 val Results: Prec@1 55.460 Loss: 1.2616\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [124][0/390]\tTime 0.003 (0.003)\tLoss 1.0534 (1.0534)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [124][78/390]\tTime 0.003 (0.003)\tLoss 1.1639 (1.1016)\tPrec@1 62.500 (61.788)\n",
      "Epoch: [124][156/390]\tTime 0.004 (0.003)\tLoss 1.1165 (1.1256)\tPrec@1 64.844 (61.052)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.2184 (1.1388)\tPrec@1 50.000 (60.419)\n",
      "Epoch: [124][312/390]\tTime 0.002 (0.003)\tLoss 1.1689 (1.1442)\tPrec@1 60.938 (60.109)\n",
      "Epoch: [124][390/390]\tTime 0.003 (0.003)\tLoss 1.2048 (1.1511)\tPrec@1 55.000 (59.834)\n",
      "EPOCH: 124 train Results: Prec@1 59.834 Loss: 1.1511\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1270 (1.1270)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4688 (1.2501)\tPrec@1 31.250 (55.060)\n",
      "EPOCH: 124 val Results: Prec@1 55.060 Loss: 1.2501\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [125][0/390]\tTime 0.004 (0.004)\tLoss 1.1011 (1.1011)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [125][78/390]\tTime 0.002 (0.003)\tLoss 0.9961 (1.1039)\tPrec@1 64.062 (61.521)\n",
      "Epoch: [125][156/390]\tTime 0.006 (0.003)\tLoss 1.1094 (1.1256)\tPrec@1 61.719 (60.828)\n",
      "Epoch: [125][234/390]\tTime 0.002 (0.003)\tLoss 1.1908 (1.1357)\tPrec@1 58.594 (60.585)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.2041 (1.1432)\tPrec@1 61.719 (60.318)\n",
      "Epoch: [125][390/390]\tTime 0.001 (0.003)\tLoss 1.0924 (1.1504)\tPrec@1 62.500 (59.998)\n",
      "EPOCH: 125 train Results: Prec@1 59.998 Loss: 1.1504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0948 (1.0948)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2835 (1.2593)\tPrec@1 43.750 (55.340)\n",
      "EPOCH: 125 val Results: Prec@1 55.340 Loss: 1.2593\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [126][0/390]\tTime 0.003 (0.003)\tLoss 0.9906 (0.9906)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [126][78/390]\tTime 0.004 (0.003)\tLoss 0.9394 (1.0932)\tPrec@1 67.188 (61.442)\n",
      "Epoch: [126][156/390]\tTime 0.005 (0.003)\tLoss 1.2332 (1.1186)\tPrec@1 57.812 (60.495)\n",
      "Epoch: [126][234/390]\tTime 0.002 (0.004)\tLoss 1.2466 (1.1298)\tPrec@1 53.906 (60.236)\n",
      "Epoch: [126][312/390]\tTime 0.006 (0.004)\tLoss 1.1222 (1.1413)\tPrec@1 55.469 (60.001)\n",
      "Epoch: [126][390/390]\tTime 0.001 (0.004)\tLoss 1.0944 (1.1491)\tPrec@1 62.500 (59.728)\n",
      "EPOCH: 126 train Results: Prec@1 59.728 Loss: 1.1491\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1385 (1.1385)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4670 (1.2498)\tPrec@1 37.500 (55.380)\n",
      "EPOCH: 126 val Results: Prec@1 55.380 Loss: 1.2498\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [127][0/390]\tTime 0.002 (0.002)\tLoss 1.1583 (1.1583)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.003)\tLoss 0.9643 (1.0910)\tPrec@1 70.312 (62.282)\n",
      "Epoch: [127][156/390]\tTime 0.004 (0.003)\tLoss 1.1667 (1.1134)\tPrec@1 54.688 (61.127)\n",
      "Epoch: [127][234/390]\tTime 0.009 (0.003)\tLoss 1.3443 (1.1282)\tPrec@1 50.781 (60.731)\n",
      "Epoch: [127][312/390]\tTime 0.002 (0.003)\tLoss 1.2595 (1.1434)\tPrec@1 55.469 (60.054)\n",
      "Epoch: [127][390/390]\tTime 0.004 (0.003)\tLoss 1.2107 (1.1504)\tPrec@1 57.500 (59.894)\n",
      "EPOCH: 127 train Results: Prec@1 59.894 Loss: 1.1504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1298 (1.1298)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5233 (1.2466)\tPrec@1 37.500 (56.000)\n",
      "EPOCH: 127 val Results: Prec@1 56.000 Loss: 1.2466\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [128][0/390]\tTime 0.004 (0.004)\tLoss 1.0780 (1.0780)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [128][78/390]\tTime 0.002 (0.003)\tLoss 1.1414 (1.1023)\tPrec@1 58.594 (62.263)\n",
      "Epoch: [128][156/390]\tTime 0.003 (0.003)\tLoss 1.0971 (1.1211)\tPrec@1 64.062 (61.092)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.003)\tLoss 1.0676 (1.1296)\tPrec@1 64.844 (60.625)\n",
      "Epoch: [128][312/390]\tTime 0.002 (0.003)\tLoss 1.0693 (1.1386)\tPrec@1 65.625 (60.348)\n",
      "Epoch: [128][390/390]\tTime 0.002 (0.003)\tLoss 1.3737 (1.1480)\tPrec@1 60.000 (59.984)\n",
      "EPOCH: 128 train Results: Prec@1 59.984 Loss: 1.1480\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0863 (1.0863)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2775 (1.2545)\tPrec@1 50.000 (54.980)\n",
      "EPOCH: 128 val Results: Prec@1 54.980 Loss: 1.2545\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [129][0/390]\tTime 0.004 (0.004)\tLoss 1.1941 (1.1941)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [129][78/390]\tTime 0.003 (0.003)\tLoss 1.0717 (1.0882)\tPrec@1 60.938 (62.470)\n",
      "Epoch: [129][156/390]\tTime 0.002 (0.003)\tLoss 1.1548 (1.1068)\tPrec@1 57.812 (61.843)\n",
      "Epoch: [129][234/390]\tTime 0.002 (0.003)\tLoss 1.2612 (1.1238)\tPrec@1 57.031 (61.120)\n",
      "Epoch: [129][312/390]\tTime 0.002 (0.003)\tLoss 1.0072 (1.1347)\tPrec@1 65.625 (60.481)\n",
      "Epoch: [129][390/390]\tTime 0.001 (0.003)\tLoss 1.1175 (1.1453)\tPrec@1 58.750 (60.076)\n",
      "EPOCH: 129 train Results: Prec@1 60.076 Loss: 1.1453\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1432 (1.1432)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4372 (1.2571)\tPrec@1 18.750 (55.310)\n",
      "EPOCH: 129 val Results: Prec@1 55.310 Loss: 1.2571\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [130][0/390]\tTime 0.002 (0.002)\tLoss 1.0536 (1.0536)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [130][78/390]\tTime 0.004 (0.006)\tLoss 0.9448 (1.0940)\tPrec@1 69.531 (62.184)\n",
      "Epoch: [130][156/390]\tTime 0.003 (0.005)\tLoss 1.2018 (1.1098)\tPrec@1 53.906 (61.331)\n",
      "Epoch: [130][234/390]\tTime 0.002 (0.005)\tLoss 1.3019 (1.1278)\tPrec@1 53.906 (60.688)\n",
      "Epoch: [130][312/390]\tTime 0.004 (0.005)\tLoss 1.1326 (1.1406)\tPrec@1 58.594 (60.156)\n",
      "Epoch: [130][390/390]\tTime 0.001 (0.005)\tLoss 1.3336 (1.1492)\tPrec@1 48.750 (59.832)\n",
      "EPOCH: 130 train Results: Prec@1 59.832 Loss: 1.1492\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1339 (1.1339)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1975 (1.2512)\tPrec@1 50.000 (55.920)\n",
      "EPOCH: 130 val Results: Prec@1 55.920 Loss: 1.2512\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [131][0/390]\tTime 0.010 (0.010)\tLoss 1.0103 (1.0103)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [131][78/390]\tTime 0.005 (0.003)\tLoss 1.1179 (1.1001)\tPrec@1 59.375 (61.798)\n",
      "Epoch: [131][156/390]\tTime 0.004 (0.003)\tLoss 1.1757 (1.1155)\tPrec@1 59.375 (61.027)\n",
      "Epoch: [131][234/390]\tTime 0.003 (0.003)\tLoss 1.1989 (1.1290)\tPrec@1 60.938 (60.499)\n",
      "Epoch: [131][312/390]\tTime 0.002 (0.003)\tLoss 1.3115 (1.1400)\tPrec@1 52.344 (60.076)\n",
      "Epoch: [131][390/390]\tTime 0.003 (0.003)\tLoss 1.0138 (1.1488)\tPrec@1 61.250 (59.736)\n",
      "EPOCH: 131 train Results: Prec@1 59.736 Loss: 1.1488\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1452 (1.1452)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6229 (1.2415)\tPrec@1 37.500 (55.770)\n",
      "EPOCH: 131 val Results: Prec@1 55.770 Loss: 1.2415\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [132][0/390]\tTime 0.003 (0.003)\tLoss 1.0855 (1.0855)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [132][78/390]\tTime 0.002 (0.003)\tLoss 1.1440 (1.0976)\tPrec@1 57.812 (62.144)\n",
      "Epoch: [132][156/390]\tTime 0.002 (0.003)\tLoss 1.2767 (1.1282)\tPrec@1 59.375 (60.868)\n",
      "Epoch: [132][234/390]\tTime 0.002 (0.003)\tLoss 1.0487 (1.1396)\tPrec@1 65.625 (60.306)\n",
      "Epoch: [132][312/390]\tTime 0.004 (0.003)\tLoss 1.4296 (1.1454)\tPrec@1 47.656 (59.939)\n",
      "Epoch: [132][390/390]\tTime 0.002 (0.003)\tLoss 1.1833 (1.1497)\tPrec@1 61.250 (59.720)\n",
      "EPOCH: 132 train Results: Prec@1 59.720 Loss: 1.1497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1070 (1.1070)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4059 (1.2402)\tPrec@1 43.750 (55.620)\n",
      "EPOCH: 132 val Results: Prec@1 55.620 Loss: 1.2402\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [133][0/390]\tTime 0.002 (0.002)\tLoss 0.9603 (0.9603)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [133][78/390]\tTime 0.003 (0.003)\tLoss 1.1173 (1.0968)\tPrec@1 57.031 (61.956)\n",
      "Epoch: [133][156/390]\tTime 0.002 (0.003)\tLoss 1.0990 (1.1099)\tPrec@1 62.500 (61.634)\n",
      "Epoch: [133][234/390]\tTime 0.004 (0.003)\tLoss 1.2531 (1.1242)\tPrec@1 55.469 (60.871)\n",
      "Epoch: [133][312/390]\tTime 0.009 (0.003)\tLoss 1.1994 (1.1376)\tPrec@1 60.938 (60.443)\n",
      "Epoch: [133][390/390]\tTime 0.001 (0.003)\tLoss 1.2176 (1.1466)\tPrec@1 50.000 (60.004)\n",
      "EPOCH: 133 train Results: Prec@1 60.004 Loss: 1.1466\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1612 (1.1612)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2432 (1.2553)\tPrec@1 56.250 (55.390)\n",
      "EPOCH: 133 val Results: Prec@1 55.390 Loss: 1.2553\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [134][0/390]\tTime 0.005 (0.005)\tLoss 1.1175 (1.1175)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [134][78/390]\tTime 0.005 (0.003)\tLoss 1.1281 (1.1076)\tPrec@1 64.062 (61.650)\n",
      "Epoch: [134][156/390]\tTime 0.009 (0.004)\tLoss 1.1467 (1.1290)\tPrec@1 58.594 (60.918)\n",
      "Epoch: [134][234/390]\tTime 0.005 (0.004)\tLoss 1.1290 (1.1384)\tPrec@1 55.469 (60.236)\n",
      "Epoch: [134][312/390]\tTime 0.004 (0.004)\tLoss 1.1206 (1.1442)\tPrec@1 63.281 (59.969)\n",
      "Epoch: [134][390/390]\tTime 0.001 (0.003)\tLoss 1.0960 (1.1492)\tPrec@1 63.750 (59.782)\n",
      "EPOCH: 134 train Results: Prec@1 59.782 Loss: 1.1492\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0845 (1.0845)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2780 (1.2495)\tPrec@1 31.250 (55.290)\n",
      "EPOCH: 134 val Results: Prec@1 55.290 Loss: 1.2495\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [135][0/390]\tTime 0.004 (0.004)\tLoss 1.0649 (1.0649)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [135][78/390]\tTime 0.002 (0.003)\tLoss 1.0546 (1.0966)\tPrec@1 64.844 (61.828)\n",
      "Epoch: [135][156/390]\tTime 0.005 (0.003)\tLoss 1.1629 (1.1128)\tPrec@1 60.156 (61.355)\n",
      "Epoch: [135][234/390]\tTime 0.002 (0.003)\tLoss 1.3060 (1.1270)\tPrec@1 53.125 (60.575)\n",
      "Epoch: [135][312/390]\tTime 0.003 (0.003)\tLoss 1.0905 (1.1399)\tPrec@1 56.250 (60.051)\n",
      "Epoch: [135][390/390]\tTime 0.011 (0.003)\tLoss 1.1912 (1.1502)\tPrec@1 63.750 (59.614)\n",
      "EPOCH: 135 train Results: Prec@1 59.614 Loss: 1.1502\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1646 (1.1646)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2105 (1.2404)\tPrec@1 43.750 (55.820)\n",
      "EPOCH: 135 val Results: Prec@1 55.820 Loss: 1.2404\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [136][0/390]\tTime 0.002 (0.002)\tLoss 0.8961 (0.8961)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [136][78/390]\tTime 0.002 (0.003)\tLoss 1.2695 (1.0830)\tPrec@1 53.125 (62.500)\n",
      "Epoch: [136][156/390]\tTime 0.003 (0.003)\tLoss 0.9731 (1.1084)\tPrec@1 67.188 (61.550)\n",
      "Epoch: [136][234/390]\tTime 0.002 (0.003)\tLoss 1.0884 (1.1259)\tPrec@1 67.188 (60.765)\n",
      "Epoch: [136][312/390]\tTime 0.003 (0.003)\tLoss 1.1901 (1.1354)\tPrec@1 57.031 (60.251)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.2459 (1.1440)\tPrec@1 56.250 (59.860)\n",
      "EPOCH: 136 train Results: Prec@1 59.860 Loss: 1.1440\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1110 (1.1110)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5573 (1.2399)\tPrec@1 37.500 (55.600)\n",
      "EPOCH: 136 val Results: Prec@1 55.600 Loss: 1.2399\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [137][0/390]\tTime 0.006 (0.006)\tLoss 1.1861 (1.1861)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [137][78/390]\tTime 0.002 (0.003)\tLoss 1.2124 (1.1054)\tPrec@1 56.250 (61.709)\n",
      "Epoch: [137][156/390]\tTime 0.004 (0.003)\tLoss 1.0470 (1.1172)\tPrec@1 64.844 (60.967)\n",
      "Epoch: [137][234/390]\tTime 0.008 (0.003)\tLoss 1.1154 (1.1249)\tPrec@1 59.375 (60.555)\n",
      "Epoch: [137][312/390]\tTime 0.002 (0.003)\tLoss 1.1874 (1.1337)\tPrec@1 60.156 (60.171)\n",
      "Epoch: [137][390/390]\tTime 0.001 (0.003)\tLoss 1.3120 (1.1458)\tPrec@1 61.250 (59.772)\n",
      "EPOCH: 137 train Results: Prec@1 59.772 Loss: 1.1458\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.0862 (1.0862)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5532 (1.2446)\tPrec@1 31.250 (55.690)\n",
      "EPOCH: 137 val Results: Prec@1 55.690 Loss: 1.2446\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [138][0/390]\tTime 0.004 (0.004)\tLoss 1.1835 (1.1835)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [138][78/390]\tTime 0.003 (0.003)\tLoss 1.0567 (1.1023)\tPrec@1 65.625 (61.778)\n",
      "Epoch: [138][156/390]\tTime 0.002 (0.003)\tLoss 1.0805 (1.1175)\tPrec@1 59.375 (61.171)\n",
      "Epoch: [138][234/390]\tTime 0.002 (0.003)\tLoss 1.2015 (1.1307)\tPrec@1 57.031 (60.559)\n",
      "Epoch: [138][312/390]\tTime 0.004 (0.003)\tLoss 1.3832 (1.1403)\tPrec@1 49.219 (60.181)\n",
      "Epoch: [138][390/390]\tTime 0.002 (0.004)\tLoss 1.2037 (1.1482)\tPrec@1 57.500 (59.794)\n",
      "EPOCH: 138 train Results: Prec@1 59.794 Loss: 1.1482\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0919 (1.0919)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3391 (1.2422)\tPrec@1 56.250 (55.830)\n",
      "EPOCH: 138 val Results: Prec@1 55.830 Loss: 1.2422\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [139][0/390]\tTime 0.003 (0.003)\tLoss 1.0929 (1.0929)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [139][78/390]\tTime 0.003 (0.004)\tLoss 1.0045 (1.0944)\tPrec@1 62.500 (61.402)\n",
      "Epoch: [139][156/390]\tTime 0.003 (0.004)\tLoss 1.0840 (1.1145)\tPrec@1 64.062 (61.002)\n",
      "Epoch: [139][234/390]\tTime 0.002 (0.004)\tLoss 1.1825 (1.1282)\tPrec@1 59.375 (60.578)\n",
      "Epoch: [139][312/390]\tTime 0.004 (0.003)\tLoss 1.1088 (1.1372)\tPrec@1 60.938 (60.271)\n",
      "Epoch: [139][390/390]\tTime 0.003 (0.003)\tLoss 1.1433 (1.1452)\tPrec@1 63.750 (59.928)\n",
      "EPOCH: 139 train Results: Prec@1 59.928 Loss: 1.1452\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2021 (1.2021)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3037 (1.2549)\tPrec@1 37.500 (54.950)\n",
      "EPOCH: 139 val Results: Prec@1 54.950 Loss: 1.2549\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [140][0/390]\tTime 0.003 (0.003)\tLoss 1.1463 (1.1463)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [140][78/390]\tTime 0.003 (0.003)\tLoss 1.2214 (1.0974)\tPrec@1 53.125 (62.233)\n",
      "Epoch: [140][156/390]\tTime 0.004 (0.003)\tLoss 1.1476 (1.1127)\tPrec@1 57.812 (61.291)\n",
      "Epoch: [140][234/390]\tTime 0.002 (0.003)\tLoss 0.9973 (1.1212)\tPrec@1 64.844 (61.004)\n",
      "Epoch: [140][312/390]\tTime 0.002 (0.003)\tLoss 1.1393 (1.1322)\tPrec@1 55.469 (60.473)\n",
      "Epoch: [140][390/390]\tTime 0.001 (0.003)\tLoss 1.3922 (1.1424)\tPrec@1 51.250 (59.998)\n",
      "EPOCH: 140 train Results: Prec@1 59.998 Loss: 1.1424\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1229 (1.1229)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4440 (1.2502)\tPrec@1 25.000 (55.320)\n",
      "EPOCH: 140 val Results: Prec@1 55.320 Loss: 1.2502\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [141][0/390]\tTime 0.003 (0.003)\tLoss 0.8732 (0.8732)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [141][78/390]\tTime 0.002 (0.003)\tLoss 1.2688 (1.0897)\tPrec@1 57.812 (62.045)\n",
      "Epoch: [141][156/390]\tTime 0.002 (0.003)\tLoss 1.1697 (1.1156)\tPrec@1 53.125 (60.962)\n",
      "Epoch: [141][234/390]\tTime 0.003 (0.003)\tLoss 1.2636 (1.1289)\tPrec@1 57.031 (60.346)\n",
      "Epoch: [141][312/390]\tTime 0.004 (0.003)\tLoss 1.2006 (1.1392)\tPrec@1 59.375 (59.969)\n",
      "Epoch: [141][390/390]\tTime 0.002 (0.003)\tLoss 1.2238 (1.1455)\tPrec@1 57.500 (59.806)\n",
      "EPOCH: 141 train Results: Prec@1 59.806 Loss: 1.1455\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1921 (1.1921)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3005 (1.2555)\tPrec@1 50.000 (54.990)\n",
      "EPOCH: 141 val Results: Prec@1 54.990 Loss: 1.2555\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [142][0/390]\tTime 0.004 (0.004)\tLoss 1.0938 (1.0938)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [142][78/390]\tTime 0.004 (0.004)\tLoss 1.0885 (1.0904)\tPrec@1 65.625 (62.708)\n",
      "Epoch: [142][156/390]\tTime 0.005 (0.004)\tLoss 1.0308 (1.1112)\tPrec@1 64.844 (61.510)\n",
      "Epoch: [142][234/390]\tTime 0.002 (0.004)\tLoss 1.2122 (1.1218)\tPrec@1 63.281 (61.114)\n",
      "Epoch: [142][312/390]\tTime 0.002 (0.004)\tLoss 1.1478 (1.1366)\tPrec@1 58.594 (60.513)\n",
      "Epoch: [142][390/390]\tTime 0.001 (0.004)\tLoss 1.2417 (1.1451)\tPrec@1 56.250 (60.090)\n",
      "EPOCH: 142 train Results: Prec@1 60.090 Loss: 1.1451\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1926 (1.1926)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2157 (1.2600)\tPrec@1 68.750 (55.360)\n",
      "EPOCH: 142 val Results: Prec@1 55.360 Loss: 1.2600\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [143][0/390]\tTime 0.005 (0.005)\tLoss 1.0383 (1.0383)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [143][78/390]\tTime 0.004 (0.003)\tLoss 1.0758 (1.0956)\tPrec@1 65.625 (62.302)\n",
      "Epoch: [143][156/390]\tTime 0.006 (0.003)\tLoss 1.1478 (1.1144)\tPrec@1 60.156 (61.654)\n",
      "Epoch: [143][234/390]\tTime 0.010 (0.003)\tLoss 1.0953 (1.1253)\tPrec@1 62.500 (61.150)\n",
      "Epoch: [143][312/390]\tTime 0.010 (0.003)\tLoss 1.2860 (1.1382)\tPrec@1 51.562 (60.446)\n",
      "Epoch: [143][390/390]\tTime 0.001 (0.003)\tLoss 1.1085 (1.1435)\tPrec@1 58.750 (60.280)\n",
      "EPOCH: 143 train Results: Prec@1 60.280 Loss: 1.1435\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1289 (1.1289)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2713 (1.2481)\tPrec@1 50.000 (56.140)\n",
      "EPOCH: 143 val Results: Prec@1 56.140 Loss: 1.2481\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [144][0/390]\tTime 0.004 (0.004)\tLoss 1.0751 (1.0751)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [144][78/390]\tTime 0.002 (0.003)\tLoss 1.1076 (1.1091)\tPrec@1 57.812 (61.195)\n",
      "Epoch: [144][156/390]\tTime 0.002 (0.004)\tLoss 1.3805 (1.1199)\tPrec@1 51.562 (60.843)\n",
      "Epoch: [144][234/390]\tTime 0.003 (0.003)\tLoss 1.1135 (1.1331)\tPrec@1 61.719 (60.512)\n",
      "Epoch: [144][312/390]\tTime 0.003 (0.003)\tLoss 1.1669 (1.1370)\tPrec@1 56.250 (60.473)\n",
      "Epoch: [144][390/390]\tTime 0.004 (0.003)\tLoss 1.1530 (1.1435)\tPrec@1 56.250 (60.182)\n",
      "EPOCH: 144 train Results: Prec@1 60.182 Loss: 1.1435\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0868 (1.0868)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5258 (1.2546)\tPrec@1 43.750 (55.350)\n",
      "EPOCH: 144 val Results: Prec@1 55.350 Loss: 1.2546\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [145][0/390]\tTime 0.002 (0.002)\tLoss 1.0388 (1.0388)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [145][78/390]\tTime 0.005 (0.004)\tLoss 0.9706 (1.1041)\tPrec@1 69.531 (61.303)\n",
      "Epoch: [145][156/390]\tTime 0.004 (0.003)\tLoss 1.1566 (1.1109)\tPrec@1 61.719 (61.465)\n",
      "Epoch: [145][234/390]\tTime 0.003 (0.003)\tLoss 1.1338 (1.1254)\tPrec@1 57.031 (60.901)\n",
      "Epoch: [145][312/390]\tTime 0.004 (0.003)\tLoss 1.1633 (1.1391)\tPrec@1 57.031 (60.201)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.003)\tLoss 1.1293 (1.1454)\tPrec@1 60.000 (60.044)\n",
      "EPOCH: 145 train Results: Prec@1 60.044 Loss: 1.1454\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1843 (1.1843)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6127 (1.2593)\tPrec@1 31.250 (55.310)\n",
      "EPOCH: 145 val Results: Prec@1 55.310 Loss: 1.2593\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [146][0/390]\tTime 0.007 (0.007)\tLoss 1.1269 (1.1269)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [146][78/390]\tTime 0.003 (0.004)\tLoss 1.0407 (1.0918)\tPrec@1 66.406 (62.362)\n",
      "Epoch: [146][156/390]\tTime 0.002 (0.003)\tLoss 1.0664 (1.1170)\tPrec@1 57.812 (61.087)\n",
      "Epoch: [146][234/390]\tTime 0.011 (0.004)\tLoss 1.0803 (1.1249)\tPrec@1 64.062 (60.888)\n",
      "Epoch: [146][312/390]\tTime 0.004 (0.004)\tLoss 1.0549 (1.1363)\tPrec@1 60.156 (60.348)\n",
      "Epoch: [146][390/390]\tTime 0.002 (0.004)\tLoss 0.9395 (1.1434)\tPrec@1 72.500 (60.074)\n",
      "EPOCH: 146 train Results: Prec@1 60.074 Loss: 1.1434\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1482 (1.1482)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2911 (1.2461)\tPrec@1 50.000 (56.060)\n",
      "EPOCH: 146 val Results: Prec@1 56.060 Loss: 1.2461\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [147][0/390]\tTime 0.003 (0.003)\tLoss 1.1092 (1.1092)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [147][78/390]\tTime 0.004 (0.003)\tLoss 1.0952 (1.0947)\tPrec@1 64.062 (62.203)\n",
      "Epoch: [147][156/390]\tTime 0.002 (0.003)\tLoss 1.1590 (1.1143)\tPrec@1 63.281 (61.221)\n",
      "Epoch: [147][234/390]\tTime 0.003 (0.004)\tLoss 1.1427 (1.1219)\tPrec@1 61.719 (60.911)\n",
      "Epoch: [147][312/390]\tTime 0.002 (0.003)\tLoss 1.1816 (1.1325)\tPrec@1 55.469 (60.518)\n",
      "Epoch: [147][390/390]\tTime 0.003 (0.003)\tLoss 1.1530 (1.1407)\tPrec@1 62.500 (60.170)\n",
      "EPOCH: 147 train Results: Prec@1 60.170 Loss: 1.1407\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1240 (1.1240)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6233 (1.2500)\tPrec@1 25.000 (55.530)\n",
      "EPOCH: 147 val Results: Prec@1 55.530 Loss: 1.2500\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [148][0/390]\tTime 0.005 (0.005)\tLoss 1.0665 (1.0665)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [148][78/390]\tTime 0.002 (0.003)\tLoss 1.1753 (1.0987)\tPrec@1 56.250 (62.391)\n",
      "Epoch: [148][156/390]\tTime 0.002 (0.003)\tLoss 1.2185 (1.1146)\tPrec@1 58.594 (61.540)\n",
      "Epoch: [148][234/390]\tTime 0.002 (0.003)\tLoss 1.2029 (1.1306)\tPrec@1 53.906 (60.615)\n",
      "Epoch: [148][312/390]\tTime 0.006 (0.003)\tLoss 1.0836 (1.1381)\tPrec@1 62.500 (60.284)\n",
      "Epoch: [148][390/390]\tTime 0.001 (0.003)\tLoss 1.0946 (1.1472)\tPrec@1 65.000 (59.988)\n",
      "EPOCH: 148 train Results: Prec@1 59.988 Loss: 1.1472\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.2157 (1.2157)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4765 (1.2592)\tPrec@1 31.250 (55.320)\n",
      "EPOCH: 148 val Results: Prec@1 55.320 Loss: 1.2592\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [149][0/390]\tTime 0.008 (0.008)\tLoss 0.9577 (0.9577)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [149][78/390]\tTime 0.002 (0.004)\tLoss 1.1148 (1.0861)\tPrec@1 57.031 (62.263)\n",
      "Epoch: [149][156/390]\tTime 0.007 (0.003)\tLoss 1.1931 (1.1034)\tPrec@1 57.031 (61.604)\n",
      "Epoch: [149][234/390]\tTime 0.004 (0.003)\tLoss 1.0228 (1.1203)\tPrec@1 67.188 (60.834)\n",
      "Epoch: [149][312/390]\tTime 0.003 (0.003)\tLoss 1.1448 (1.1309)\tPrec@1 60.938 (60.478)\n",
      "Epoch: [149][390/390]\tTime 0.004 (0.003)\tLoss 1.2735 (1.1415)\tPrec@1 55.000 (60.086)\n",
      "EPOCH: 149 train Results: Prec@1 60.086 Loss: 1.1415\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1343 (1.1343)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.2997 (1.2513)\tPrec@1 43.750 (55.160)\n",
      "EPOCH: 149 val Results: Prec@1 55.160 Loss: 1.2513\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [150][0/390]\tTime 0.005 (0.005)\tLoss 1.1374 (1.1374)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [150][78/390]\tTime 0.003 (0.004)\tLoss 1.2439 (1.0788)\tPrec@1 58.594 (62.896)\n",
      "Epoch: [150][156/390]\tTime 0.003 (0.004)\tLoss 1.2378 (1.0981)\tPrec@1 63.281 (62.032)\n",
      "Epoch: [150][234/390]\tTime 0.002 (0.004)\tLoss 1.3329 (1.1220)\tPrec@1 54.688 (60.798)\n",
      "Epoch: [150][312/390]\tTime 0.006 (0.003)\tLoss 1.1746 (1.1337)\tPrec@1 55.469 (60.321)\n",
      "Epoch: [150][390/390]\tTime 0.002 (0.003)\tLoss 1.1804 (1.1410)\tPrec@1 61.250 (59.978)\n",
      "EPOCH: 150 train Results: Prec@1 59.978 Loss: 1.1410\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1274 (1.1274)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2538 (1.2407)\tPrec@1 43.750 (55.610)\n",
      "EPOCH: 150 val Results: Prec@1 55.610 Loss: 1.2407\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [151][0/390]\tTime 0.002 (0.002)\tLoss 1.0541 (1.0541)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [151][78/390]\tTime 0.005 (0.003)\tLoss 1.1609 (1.0877)\tPrec@1 60.938 (61.659)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.003)\tLoss 1.1813 (1.1066)\tPrec@1 56.250 (61.067)\n",
      "Epoch: [151][234/390]\tTime 0.003 (0.003)\tLoss 1.2002 (1.1183)\tPrec@1 56.250 (60.642)\n",
      "Epoch: [151][312/390]\tTime 0.002 (0.003)\tLoss 1.2084 (1.1303)\tPrec@1 60.938 (60.134)\n",
      "Epoch: [151][390/390]\tTime 0.002 (0.003)\tLoss 1.2046 (1.1413)\tPrec@1 61.250 (59.774)\n",
      "EPOCH: 151 train Results: Prec@1 59.774 Loss: 1.1413\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1509 (1.1509)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3653 (1.2545)\tPrec@1 50.000 (55.490)\n",
      "EPOCH: 151 val Results: Prec@1 55.490 Loss: 1.2545\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [152][0/390]\tTime 0.005 (0.005)\tLoss 1.0240 (1.0240)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [152][78/390]\tTime 0.004 (0.003)\tLoss 1.0484 (1.0944)\tPrec@1 58.594 (62.095)\n",
      "Epoch: [152][156/390]\tTime 0.002 (0.004)\tLoss 1.0757 (1.1115)\tPrec@1 64.062 (61.321)\n",
      "Epoch: [152][234/390]\tTime 0.005 (0.003)\tLoss 1.2127 (1.1220)\tPrec@1 61.719 (60.981)\n",
      "Epoch: [152][312/390]\tTime 0.003 (0.003)\tLoss 1.2047 (1.1303)\tPrec@1 54.688 (60.518)\n",
      "Epoch: [152][390/390]\tTime 0.002 (0.003)\tLoss 1.0438 (1.1396)\tPrec@1 63.750 (60.260)\n",
      "EPOCH: 152 train Results: Prec@1 60.260 Loss: 1.1396\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1545 (1.1545)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3952 (1.2419)\tPrec@1 31.250 (55.750)\n",
      "EPOCH: 152 val Results: Prec@1 55.750 Loss: 1.2419\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [153][0/390]\tTime 0.003 (0.003)\tLoss 0.9424 (0.9424)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [153][78/390]\tTime 0.002 (0.004)\tLoss 1.0394 (1.0830)\tPrec@1 63.281 (63.054)\n",
      "Epoch: [153][156/390]\tTime 0.003 (0.003)\tLoss 1.1286 (1.1042)\tPrec@1 63.281 (61.903)\n",
      "Epoch: [153][234/390]\tTime 0.006 (0.003)\tLoss 1.0719 (1.1150)\tPrec@1 62.500 (61.230)\n",
      "Epoch: [153][312/390]\tTime 0.002 (0.003)\tLoss 1.0535 (1.1296)\tPrec@1 62.500 (60.770)\n",
      "Epoch: [153][390/390]\tTime 0.011 (0.003)\tLoss 1.2311 (1.1394)\tPrec@1 53.750 (60.298)\n",
      "EPOCH: 153 train Results: Prec@1 60.298 Loss: 1.1394\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1437 (1.1437)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4631 (1.2474)\tPrec@1 43.750 (55.450)\n",
      "EPOCH: 153 val Results: Prec@1 55.450 Loss: 1.2474\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [154][0/390]\tTime 0.004 (0.004)\tLoss 1.0334 (1.0334)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.003)\tLoss 1.0546 (1.0879)\tPrec@1 63.281 (62.540)\n",
      "Epoch: [154][156/390]\tTime 0.003 (0.003)\tLoss 1.1342 (1.1067)\tPrec@1 59.375 (61.729)\n",
      "Epoch: [154][234/390]\tTime 0.007 (0.003)\tLoss 1.2588 (1.1207)\tPrec@1 57.031 (60.931)\n",
      "Epoch: [154][312/390]\tTime 0.004 (0.003)\tLoss 1.2090 (1.1340)\tPrec@1 55.469 (60.473)\n",
      "Epoch: [154][390/390]\tTime 0.003 (0.003)\tLoss 1.2551 (1.1437)\tPrec@1 56.250 (60.054)\n",
      "EPOCH: 154 train Results: Prec@1 60.054 Loss: 1.1437\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1228 (1.1228)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6224 (1.2399)\tPrec@1 31.250 (55.500)\n",
      "EPOCH: 154 val Results: Prec@1 55.500 Loss: 1.2399\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [155][0/390]\tTime 0.002 (0.002)\tLoss 1.1409 (1.1409)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [155][78/390]\tTime 0.002 (0.003)\tLoss 1.0628 (1.0846)\tPrec@1 64.844 (62.045)\n",
      "Epoch: [155][156/390]\tTime 0.002 (0.003)\tLoss 1.0373 (1.1006)\tPrec@1 64.844 (61.574)\n",
      "Epoch: [155][234/390]\tTime 0.002 (0.003)\tLoss 1.2211 (1.1156)\tPrec@1 54.688 (60.934)\n",
      "Epoch: [155][312/390]\tTime 0.002 (0.003)\tLoss 1.2089 (1.1288)\tPrec@1 59.375 (60.483)\n",
      "Epoch: [155][390/390]\tTime 0.002 (0.003)\tLoss 1.0474 (1.1384)\tPrec@1 65.000 (60.138)\n",
      "EPOCH: 155 train Results: Prec@1 60.138 Loss: 1.1384\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1487 (1.1487)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2987 (1.2522)\tPrec@1 37.500 (55.440)\n",
      "EPOCH: 155 val Results: Prec@1 55.440 Loss: 1.2522\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [156][0/390]\tTime 0.003 (0.003)\tLoss 0.9841 (0.9841)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [156][78/390]\tTime 0.002 (0.003)\tLoss 1.1360 (1.0838)\tPrec@1 57.031 (62.391)\n",
      "Epoch: [156][156/390]\tTime 0.004 (0.004)\tLoss 1.3444 (1.1061)\tPrec@1 52.344 (61.629)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.004)\tLoss 1.1282 (1.1237)\tPrec@1 63.281 (60.695)\n",
      "Epoch: [156][312/390]\tTime 0.005 (0.005)\tLoss 1.2985 (1.1367)\tPrec@1 52.344 (60.224)\n",
      "Epoch: [156][390/390]\tTime 0.001 (0.005)\tLoss 1.2159 (1.1430)\tPrec@1 57.500 (60.016)\n",
      "EPOCH: 156 train Results: Prec@1 60.016 Loss: 1.1430\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1434 (1.1434)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4826 (1.2422)\tPrec@1 50.000 (56.270)\n",
      "EPOCH: 156 val Results: Prec@1 56.270 Loss: 1.2422\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [157][0/390]\tTime 0.005 (0.005)\tLoss 1.1553 (1.1553)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [157][78/390]\tTime 0.002 (0.003)\tLoss 1.0624 (1.0825)\tPrec@1 64.062 (62.688)\n",
      "Epoch: [157][156/390]\tTime 0.003 (0.004)\tLoss 1.0124 (1.1001)\tPrec@1 64.844 (61.803)\n",
      "Epoch: [157][234/390]\tTime 0.010 (0.004)\tLoss 1.0701 (1.1244)\tPrec@1 61.719 (60.602)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.004)\tLoss 1.2183 (1.1360)\tPrec@1 54.688 (60.244)\n",
      "Epoch: [157][390/390]\tTime 0.002 (0.004)\tLoss 1.1243 (1.1420)\tPrec@1 61.250 (60.030)\n",
      "EPOCH: 157 train Results: Prec@1 60.030 Loss: 1.1420\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1010 (1.1010)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3660 (1.2506)\tPrec@1 37.500 (55.240)\n",
      "EPOCH: 157 val Results: Prec@1 55.240 Loss: 1.2506\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [158][0/390]\tTime 0.004 (0.004)\tLoss 1.0280 (1.0280)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [158][78/390]\tTime 0.002 (0.003)\tLoss 1.2641 (1.0886)\tPrec@1 53.125 (61.857)\n",
      "Epoch: [158][156/390]\tTime 0.003 (0.003)\tLoss 1.1656 (1.1035)\tPrec@1 60.156 (61.002)\n",
      "Epoch: [158][234/390]\tTime 0.002 (0.004)\tLoss 1.1224 (1.1196)\tPrec@1 59.375 (60.386)\n",
      "Epoch: [158][312/390]\tTime 0.002 (0.004)\tLoss 1.1896 (1.1289)\tPrec@1 57.812 (60.144)\n",
      "Epoch: [158][390/390]\tTime 0.001 (0.004)\tLoss 1.2571 (1.1377)\tPrec@1 61.250 (59.818)\n",
      "EPOCH: 158 train Results: Prec@1 59.818 Loss: 1.1377\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1534 (1.1534)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4571 (1.2500)\tPrec@1 37.500 (55.310)\n",
      "EPOCH: 158 val Results: Prec@1 55.310 Loss: 1.2500\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [159][0/390]\tTime 0.003 (0.003)\tLoss 0.9971 (0.9971)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [159][78/390]\tTime 0.002 (0.004)\tLoss 1.1044 (1.1016)\tPrec@1 59.375 (61.511)\n",
      "Epoch: [159][156/390]\tTime 0.007 (0.005)\tLoss 1.0462 (1.1158)\tPrec@1 64.062 (60.833)\n",
      "Epoch: [159][234/390]\tTime 0.003 (0.005)\tLoss 0.9532 (1.1214)\tPrec@1 65.625 (60.731)\n",
      "Epoch: [159][312/390]\tTime 0.004 (0.005)\tLoss 1.1287 (1.1324)\tPrec@1 59.375 (60.316)\n",
      "Epoch: [159][390/390]\tTime 0.001 (0.005)\tLoss 1.2236 (1.1400)\tPrec@1 56.250 (59.928)\n",
      "EPOCH: 159 train Results: Prec@1 59.928 Loss: 1.1400\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0983 (1.0983)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5076 (1.2419)\tPrec@1 37.500 (55.960)\n",
      "EPOCH: 159 val Results: Prec@1 55.960 Loss: 1.2419\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [160][0/390]\tTime 0.002 (0.002)\tLoss 0.9532 (0.9532)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [160][78/390]\tTime 0.003 (0.003)\tLoss 1.1620 (1.1123)\tPrec@1 60.938 (61.521)\n",
      "Epoch: [160][156/390]\tTime 0.004 (0.003)\tLoss 1.1219 (1.1127)\tPrec@1 63.281 (61.072)\n",
      "Epoch: [160][234/390]\tTime 0.002 (0.004)\tLoss 1.0529 (1.1236)\tPrec@1 63.281 (60.615)\n",
      "Epoch: [160][312/390]\tTime 0.003 (0.004)\tLoss 1.1153 (1.1324)\tPrec@1 59.375 (60.229)\n",
      "Epoch: [160][390/390]\tTime 0.001 (0.004)\tLoss 1.0909 (1.1343)\tPrec@1 67.500 (60.250)\n",
      "EPOCH: 160 train Results: Prec@1 60.250 Loss: 1.1343\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1245 (1.1245)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3002 (1.2507)\tPrec@1 37.500 (55.670)\n",
      "EPOCH: 160 val Results: Prec@1 55.670 Loss: 1.2507\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [161][0/390]\tTime 0.013 (0.013)\tLoss 1.1373 (1.1373)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [161][78/390]\tTime 0.003 (0.003)\tLoss 1.0381 (1.0758)\tPrec@1 67.188 (62.520)\n",
      "Epoch: [161][156/390]\tTime 0.010 (0.003)\tLoss 1.1425 (1.1033)\tPrec@1 60.156 (61.480)\n",
      "Epoch: [161][234/390]\tTime 0.010 (0.004)\tLoss 1.1560 (1.1205)\tPrec@1 60.938 (60.921)\n",
      "Epoch: [161][312/390]\tTime 0.003 (0.004)\tLoss 1.1128 (1.1279)\tPrec@1 64.844 (60.630)\n",
      "Epoch: [161][390/390]\tTime 0.002 (0.004)\tLoss 1.4816 (1.1386)\tPrec@1 50.000 (60.152)\n",
      "EPOCH: 161 train Results: Prec@1 60.152 Loss: 1.1386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1168 (1.1168)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3402 (1.2391)\tPrec@1 50.000 (55.710)\n",
      "EPOCH: 161 val Results: Prec@1 55.710 Loss: 1.2391\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.9790 (0.9790)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [162][78/390]\tTime 0.003 (0.004)\tLoss 1.1750 (1.0847)\tPrec@1 60.938 (62.549)\n",
      "Epoch: [162][156/390]\tTime 0.002 (0.004)\tLoss 1.0534 (1.1048)\tPrec@1 68.750 (61.793)\n",
      "Epoch: [162][234/390]\tTime 0.004 (0.004)\tLoss 1.0764 (1.1145)\tPrec@1 64.844 (61.253)\n",
      "Epoch: [162][312/390]\tTime 0.002 (0.004)\tLoss 1.0691 (1.1258)\tPrec@1 62.500 (60.613)\n",
      "Epoch: [162][390/390]\tTime 0.003 (0.004)\tLoss 1.1647 (1.1336)\tPrec@1 61.250 (60.322)\n",
      "EPOCH: 162 train Results: Prec@1 60.322 Loss: 1.1336\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1593 (1.1593)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6435 (1.2494)\tPrec@1 25.000 (55.360)\n",
      "EPOCH: 162 val Results: Prec@1 55.360 Loss: 1.2494\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [163][0/390]\tTime 0.005 (0.005)\tLoss 1.2057 (1.2057)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [163][78/390]\tTime 0.002 (0.003)\tLoss 1.0366 (1.0997)\tPrec@1 61.719 (61.640)\n",
      "Epoch: [163][156/390]\tTime 0.004 (0.003)\tLoss 1.1070 (1.1024)\tPrec@1 53.906 (61.589)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.003)\tLoss 1.0859 (1.1214)\tPrec@1 63.281 (60.731)\n",
      "Epoch: [163][312/390]\tTime 0.012 (0.003)\tLoss 1.0853 (1.1315)\tPrec@1 66.406 (60.426)\n",
      "Epoch: [163][390/390]\tTime 0.006 (0.003)\tLoss 1.1357 (1.1389)\tPrec@1 62.500 (60.078)\n",
      "EPOCH: 163 train Results: Prec@1 60.078 Loss: 1.1389\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1585 (1.1585)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.2784 (1.2343)\tPrec@1 37.500 (56.050)\n",
      "EPOCH: 163 val Results: Prec@1 56.050 Loss: 1.2343\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [164][0/390]\tTime 0.004 (0.004)\tLoss 1.0890 (1.0890)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [164][78/390]\tTime 0.002 (0.003)\tLoss 1.1576 (1.0640)\tPrec@1 63.281 (63.212)\n",
      "Epoch: [164][156/390]\tTime 0.012 (0.006)\tLoss 1.1916 (1.0933)\tPrec@1 57.812 (62.261)\n",
      "Epoch: [164][234/390]\tTime 0.004 (0.005)\tLoss 1.1104 (1.1088)\tPrec@1 59.375 (61.569)\n",
      "Epoch: [164][312/390]\tTime 0.008 (0.005)\tLoss 1.2226 (1.1261)\tPrec@1 55.469 (60.810)\n",
      "Epoch: [164][390/390]\tTime 0.003 (0.005)\tLoss 1.1511 (1.1342)\tPrec@1 60.000 (60.374)\n",
      "EPOCH: 164 train Results: Prec@1 60.374 Loss: 1.1342\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1231 (1.1231)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4422 (1.2483)\tPrec@1 31.250 (55.610)\n",
      "EPOCH: 164 val Results: Prec@1 55.610 Loss: 1.2483\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [165][0/390]\tTime 0.003 (0.003)\tLoss 1.1592 (1.1592)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [165][78/390]\tTime 0.004 (0.004)\tLoss 0.9965 (1.0569)\tPrec@1 68.750 (63.588)\n",
      "Epoch: [165][156/390]\tTime 0.004 (0.004)\tLoss 1.0846 (1.0949)\tPrec@1 62.500 (62.137)\n",
      "Epoch: [165][234/390]\tTime 0.002 (0.004)\tLoss 1.3274 (1.1099)\tPrec@1 51.562 (61.566)\n",
      "Epoch: [165][312/390]\tTime 0.002 (0.004)\tLoss 1.1677 (1.1235)\tPrec@1 64.062 (60.972)\n",
      "Epoch: [165][390/390]\tTime 0.001 (0.004)\tLoss 1.3698 (1.1336)\tPrec@1 57.500 (60.454)\n",
      "EPOCH: 165 train Results: Prec@1 60.454 Loss: 1.1336\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1565 (1.1565)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4033 (1.2500)\tPrec@1 25.000 (55.380)\n",
      "EPOCH: 165 val Results: Prec@1 55.380 Loss: 1.2500\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [166][0/390]\tTime 0.004 (0.004)\tLoss 1.1219 (1.1219)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 1.0086 (1.0697)\tPrec@1 69.531 (63.004)\n",
      "Epoch: [166][156/390]\tTime 0.007 (0.003)\tLoss 1.2159 (1.0974)\tPrec@1 59.375 (61.709)\n",
      "Epoch: [166][234/390]\tTime 0.002 (0.003)\tLoss 1.1860 (1.1143)\tPrec@1 54.688 (61.077)\n",
      "Epoch: [166][312/390]\tTime 0.006 (0.003)\tLoss 1.2055 (1.1245)\tPrec@1 59.375 (60.640)\n",
      "Epoch: [166][390/390]\tTime 0.002 (0.004)\tLoss 1.1792 (1.1354)\tPrec@1 61.250 (60.286)\n",
      "EPOCH: 166 train Results: Prec@1 60.286 Loss: 1.1354\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1538 (1.1538)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3983 (1.2506)\tPrec@1 25.000 (55.350)\n",
      "EPOCH: 166 val Results: Prec@1 55.350 Loss: 1.2506\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [167][0/390]\tTime 0.008 (0.008)\tLoss 1.1284 (1.1284)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [167][78/390]\tTime 0.002 (0.005)\tLoss 1.2253 (1.0856)\tPrec@1 53.906 (62.460)\n",
      "Epoch: [167][156/390]\tTime 0.002 (0.004)\tLoss 1.0905 (1.1008)\tPrec@1 64.062 (61.629)\n",
      "Epoch: [167][234/390]\tTime 0.002 (0.004)\tLoss 1.1046 (1.1186)\tPrec@1 60.938 (60.974)\n",
      "Epoch: [167][312/390]\tTime 0.007 (0.003)\tLoss 1.2167 (1.1253)\tPrec@1 56.250 (60.635)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.003)\tLoss 1.2230 (1.1367)\tPrec@1 57.500 (60.332)\n",
      "EPOCH: 167 train Results: Prec@1 60.332 Loss: 1.1367\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0905 (1.0905)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2744 (1.2441)\tPrec@1 37.500 (55.170)\n",
      "EPOCH: 167 val Results: Prec@1 55.170 Loss: 1.2441\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [168][0/390]\tTime 0.004 (0.004)\tLoss 0.9574 (0.9574)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [168][78/390]\tTime 0.002 (0.003)\tLoss 0.9445 (1.0783)\tPrec@1 68.750 (62.638)\n",
      "Epoch: [168][156/390]\tTime 0.002 (0.003)\tLoss 1.0877 (1.1029)\tPrec@1 58.594 (61.525)\n",
      "Epoch: [168][234/390]\tTime 0.002 (0.003)\tLoss 1.1067 (1.1182)\tPrec@1 63.281 (60.934)\n",
      "Epoch: [168][312/390]\tTime 0.004 (0.003)\tLoss 1.1523 (1.1302)\tPrec@1 60.156 (60.518)\n",
      "Epoch: [168][390/390]\tTime 0.001 (0.003)\tLoss 1.1231 (1.1375)\tPrec@1 65.000 (60.316)\n",
      "EPOCH: 168 train Results: Prec@1 60.316 Loss: 1.1375\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1270 (1.1270)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6533 (1.2435)\tPrec@1 25.000 (55.530)\n",
      "EPOCH: 168 val Results: Prec@1 55.530 Loss: 1.2435\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [169][0/390]\tTime 0.003 (0.003)\tLoss 1.0360 (1.0360)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [169][78/390]\tTime 0.004 (0.003)\tLoss 1.0834 (1.0874)\tPrec@1 62.500 (61.877)\n",
      "Epoch: [169][156/390]\tTime 0.003 (0.003)\tLoss 0.9912 (1.0972)\tPrec@1 64.062 (61.739)\n",
      "Epoch: [169][234/390]\tTime 0.003 (0.003)\tLoss 1.0795 (1.1126)\tPrec@1 61.719 (61.230)\n",
      "Epoch: [169][312/390]\tTime 0.002 (0.003)\tLoss 1.1350 (1.1253)\tPrec@1 57.031 (60.728)\n",
      "Epoch: [169][390/390]\tTime 0.002 (0.003)\tLoss 1.1426 (1.1333)\tPrec@1 60.000 (60.522)\n",
      "EPOCH: 169 train Results: Prec@1 60.522 Loss: 1.1333\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1695 (1.1695)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4176 (1.2483)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 169 val Results: Prec@1 55.630 Loss: 1.2483\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [170][0/390]\tTime 0.006 (0.006)\tLoss 1.1244 (1.1244)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [170][78/390]\tTime 0.003 (0.003)\tLoss 1.1012 (1.0824)\tPrec@1 59.375 (61.986)\n",
      "Epoch: [170][156/390]\tTime 0.007 (0.003)\tLoss 0.9459 (1.0982)\tPrec@1 67.969 (61.530)\n",
      "Epoch: [170][234/390]\tTime 0.002 (0.003)\tLoss 1.1206 (1.1103)\tPrec@1 64.844 (61.074)\n",
      "Epoch: [170][312/390]\tTime 0.002 (0.003)\tLoss 1.0725 (1.1276)\tPrec@1 67.188 (60.453)\n",
      "Epoch: [170][390/390]\tTime 0.003 (0.003)\tLoss 1.0739 (1.1342)\tPrec@1 61.250 (60.282)\n",
      "EPOCH: 170 train Results: Prec@1 60.282 Loss: 1.1342\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1727 (1.1727)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3230 (1.2353)\tPrec@1 37.500 (56.260)\n",
      "EPOCH: 170 val Results: Prec@1 56.260 Loss: 1.2353\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [171][0/390]\tTime 0.006 (0.006)\tLoss 0.9696 (0.9696)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [171][78/390]\tTime 0.003 (0.003)\tLoss 1.0352 (1.0721)\tPrec@1 64.062 (63.182)\n",
      "Epoch: [171][156/390]\tTime 0.002 (0.003)\tLoss 1.2025 (1.1000)\tPrec@1 57.812 (61.943)\n",
      "Epoch: [171][234/390]\tTime 0.003 (0.003)\tLoss 1.0710 (1.1099)\tPrec@1 63.281 (61.523)\n",
      "Epoch: [171][312/390]\tTime 0.006 (0.003)\tLoss 1.0977 (1.1175)\tPrec@1 62.500 (61.274)\n",
      "Epoch: [171][390/390]\tTime 0.001 (0.003)\tLoss 1.3246 (1.1292)\tPrec@1 55.000 (60.682)\n",
      "EPOCH: 171 train Results: Prec@1 60.682 Loss: 1.1292\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1913 (1.1913)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4073 (1.2469)\tPrec@1 31.250 (55.880)\n",
      "EPOCH: 171 val Results: Prec@1 55.880 Loss: 1.2469\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [172][0/390]\tTime 0.012 (0.012)\tLoss 0.9326 (0.9326)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [172][78/390]\tTime 0.002 (0.003)\tLoss 1.0282 (1.0815)\tPrec@1 62.500 (62.332)\n",
      "Epoch: [172][156/390]\tTime 0.003 (0.003)\tLoss 1.0390 (1.0970)\tPrec@1 62.500 (61.848)\n",
      "Epoch: [172][234/390]\tTime 0.003 (0.003)\tLoss 1.1400 (1.1139)\tPrec@1 64.062 (61.190)\n",
      "Epoch: [172][312/390]\tTime 0.002 (0.003)\tLoss 1.2499 (1.1268)\tPrec@1 54.688 (60.421)\n",
      "Epoch: [172][390/390]\tTime 0.002 (0.003)\tLoss 1.1469 (1.1354)\tPrec@1 65.000 (60.226)\n",
      "EPOCH: 172 train Results: Prec@1 60.226 Loss: 1.1354\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2001 (1.2001)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6323 (1.2491)\tPrec@1 31.250 (55.530)\n",
      "EPOCH: 172 val Results: Prec@1 55.530 Loss: 1.2491\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [173][0/390]\tTime 0.004 (0.004)\tLoss 1.1283 (1.1283)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [173][78/390]\tTime 0.003 (0.003)\tLoss 1.2264 (1.1004)\tPrec@1 58.594 (61.778)\n",
      "Epoch: [173][156/390]\tTime 0.004 (0.003)\tLoss 1.1788 (1.1086)\tPrec@1 61.719 (61.171)\n",
      "Epoch: [173][234/390]\tTime 0.002 (0.003)\tLoss 1.0455 (1.1137)\tPrec@1 62.500 (61.213)\n",
      "Epoch: [173][312/390]\tTime 0.004 (0.003)\tLoss 1.1313 (1.1246)\tPrec@1 60.938 (60.725)\n",
      "Epoch: [173][390/390]\tTime 0.001 (0.003)\tLoss 1.0816 (1.1332)\tPrec@1 61.250 (60.466)\n",
      "EPOCH: 173 train Results: Prec@1 60.466 Loss: 1.1332\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1926 (1.1926)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5212 (1.2578)\tPrec@1 25.000 (55.030)\n",
      "EPOCH: 173 val Results: Prec@1 55.030 Loss: 1.2578\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [174][0/390]\tTime 0.002 (0.002)\tLoss 1.0521 (1.0521)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [174][78/390]\tTime 0.003 (0.003)\tLoss 0.9563 (1.0816)\tPrec@1 67.969 (62.134)\n",
      "Epoch: [174][156/390]\tTime 0.002 (0.003)\tLoss 1.0514 (1.1006)\tPrec@1 60.938 (61.445)\n",
      "Epoch: [174][234/390]\tTime 0.002 (0.003)\tLoss 1.0640 (1.1125)\tPrec@1 65.625 (61.124)\n",
      "Epoch: [174][312/390]\tTime 0.003 (0.003)\tLoss 1.1121 (1.1256)\tPrec@1 60.938 (60.660)\n",
      "Epoch: [174][390/390]\tTime 0.002 (0.003)\tLoss 1.1710 (1.1368)\tPrec@1 56.250 (60.130)\n",
      "EPOCH: 174 train Results: Prec@1 60.130 Loss: 1.1368\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1462 (1.1462)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4550 (1.2507)\tPrec@1 37.500 (55.850)\n",
      "EPOCH: 174 val Results: Prec@1 55.850 Loss: 1.2507\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [175][0/390]\tTime 0.002 (0.002)\tLoss 1.0375 (1.0375)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [175][78/390]\tTime 0.002 (0.002)\tLoss 1.1539 (1.0717)\tPrec@1 57.812 (63.400)\n",
      "Epoch: [175][156/390]\tTime 0.003 (0.002)\tLoss 1.0930 (1.0914)\tPrec@1 60.156 (62.316)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.003)\tLoss 1.1996 (1.1085)\tPrec@1 57.812 (61.695)\n",
      "Epoch: [175][312/390]\tTime 0.003 (0.003)\tLoss 1.3312 (1.1203)\tPrec@1 51.562 (61.045)\n",
      "Epoch: [175][390/390]\tTime 0.001 (0.003)\tLoss 1.2560 (1.1311)\tPrec@1 55.000 (60.554)\n",
      "EPOCH: 175 train Results: Prec@1 60.554 Loss: 1.1311\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1582 (1.1582)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4172 (1.2472)\tPrec@1 37.500 (55.560)\n",
      "EPOCH: 175 val Results: Prec@1 55.560 Loss: 1.2472\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [176][0/390]\tTime 0.007 (0.007)\tLoss 1.0630 (1.0630)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [176][78/390]\tTime 0.002 (0.003)\tLoss 1.0995 (1.0813)\tPrec@1 62.500 (62.876)\n",
      "Epoch: [176][156/390]\tTime 0.029 (0.003)\tLoss 1.1016 (1.0988)\tPrec@1 62.500 (61.694)\n",
      "Epoch: [176][234/390]\tTime 0.003 (0.006)\tLoss 1.1695 (1.1120)\tPrec@1 61.719 (61.270)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.006)\tLoss 0.9828 (1.1230)\tPrec@1 70.312 (60.808)\n",
      "Epoch: [176][390/390]\tTime 0.004 (0.005)\tLoss 1.0756 (1.1302)\tPrec@1 61.250 (60.450)\n",
      "EPOCH: 176 train Results: Prec@1 60.450 Loss: 1.1302\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1290 (1.1290)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3587 (1.2453)\tPrec@1 50.000 (55.840)\n",
      "EPOCH: 176 val Results: Prec@1 55.840 Loss: 1.2453\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [177][0/390]\tTime 0.004 (0.004)\tLoss 1.1288 (1.1288)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [177][78/390]\tTime 0.005 (0.003)\tLoss 0.9346 (1.0715)\tPrec@1 67.969 (63.212)\n",
      "Epoch: [177][156/390]\tTime 0.004 (0.003)\tLoss 1.1015 (1.1023)\tPrec@1 64.062 (61.973)\n",
      "Epoch: [177][234/390]\tTime 0.004 (0.003)\tLoss 1.0986 (1.1150)\tPrec@1 61.719 (61.410)\n",
      "Epoch: [177][312/390]\tTime 0.003 (0.003)\tLoss 1.1254 (1.1249)\tPrec@1 59.375 (60.940)\n",
      "Epoch: [177][390/390]\tTime 0.002 (0.003)\tLoss 1.0565 (1.1310)\tPrec@1 66.250 (60.604)\n",
      "EPOCH: 177 train Results: Prec@1 60.604 Loss: 1.1310\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1983 (1.1983)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6953 (1.2438)\tPrec@1 31.250 (56.050)\n",
      "EPOCH: 177 val Results: Prec@1 56.050 Loss: 1.2438\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [178][0/390]\tTime 0.002 (0.002)\tLoss 1.1724 (1.1724)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [178][78/390]\tTime 0.002 (0.003)\tLoss 1.1043 (1.0717)\tPrec@1 64.062 (62.767)\n",
      "Epoch: [178][156/390]\tTime 0.004 (0.003)\tLoss 1.1496 (1.0992)\tPrec@1 64.844 (61.868)\n",
      "Epoch: [178][234/390]\tTime 0.003 (0.003)\tLoss 1.2145 (1.1166)\tPrec@1 55.469 (60.891)\n",
      "Epoch: [178][312/390]\tTime 0.003 (0.003)\tLoss 1.1359 (1.1250)\tPrec@1 58.594 (60.566)\n",
      "Epoch: [178][390/390]\tTime 0.001 (0.003)\tLoss 1.1765 (1.1325)\tPrec@1 62.500 (60.258)\n",
      "EPOCH: 178 train Results: Prec@1 60.258 Loss: 1.1325\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1762 (1.1762)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3789 (1.2557)\tPrec@1 43.750 (55.240)\n",
      "EPOCH: 178 val Results: Prec@1 55.240 Loss: 1.2557\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [179][0/390]\tTime 0.005 (0.005)\tLoss 1.1918 (1.1918)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [179][78/390]\tTime 0.003 (0.003)\tLoss 1.2497 (1.0690)\tPrec@1 57.031 (62.836)\n",
      "Epoch: [179][156/390]\tTime 0.003 (0.003)\tLoss 1.2482 (1.0965)\tPrec@1 57.031 (61.754)\n",
      "Epoch: [179][234/390]\tTime 0.002 (0.003)\tLoss 1.2442 (1.1156)\tPrec@1 56.250 (61.024)\n",
      "Epoch: [179][312/390]\tTime 0.010 (0.003)\tLoss 0.9870 (1.1253)\tPrec@1 64.844 (60.511)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.3478 (1.1342)\tPrec@1 51.250 (60.244)\n",
      "EPOCH: 179 train Results: Prec@1 60.244 Loss: 1.1342\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1277 (1.1277)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3584 (1.2446)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 179 val Results: Prec@1 55.150 Loss: 1.2446\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [180][0/390]\tTime 0.005 (0.005)\tLoss 1.1772 (1.1772)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [180][78/390]\tTime 0.003 (0.002)\tLoss 1.1715 (1.0860)\tPrec@1 62.500 (62.164)\n",
      "Epoch: [180][156/390]\tTime 0.003 (0.003)\tLoss 1.0672 (1.1002)\tPrec@1 64.062 (61.589)\n",
      "Epoch: [180][234/390]\tTime 0.004 (0.003)\tLoss 1.0585 (1.1130)\tPrec@1 57.812 (61.127)\n",
      "Epoch: [180][312/390]\tTime 0.002 (0.003)\tLoss 1.0595 (1.1264)\tPrec@1 66.406 (60.621)\n",
      "Epoch: [180][390/390]\tTime 0.002 (0.003)\tLoss 1.2681 (1.1323)\tPrec@1 56.250 (60.380)\n",
      "EPOCH: 180 train Results: Prec@1 60.380 Loss: 1.1323\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1589 (1.1589)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4476 (1.2465)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 180 val Results: Prec@1 55.370 Loss: 1.2465\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [181][0/390]\tTime 0.003 (0.003)\tLoss 1.0664 (1.0664)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [181][78/390]\tTime 0.002 (0.003)\tLoss 1.0805 (1.0627)\tPrec@1 60.938 (63.420)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 0.9881 (1.0857)\tPrec@1 70.312 (62.201)\n",
      "Epoch: [181][234/390]\tTime 0.003 (0.003)\tLoss 1.0674 (1.1052)\tPrec@1 63.281 (61.406)\n",
      "Epoch: [181][312/390]\tTime 0.002 (0.003)\tLoss 1.2082 (1.1189)\tPrec@1 58.594 (60.813)\n",
      "Epoch: [181][390/390]\tTime 0.002 (0.003)\tLoss 1.3053 (1.1274)\tPrec@1 56.250 (60.494)\n",
      "EPOCH: 181 train Results: Prec@1 60.494 Loss: 1.1274\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0991 (1.0991)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5002 (1.2391)\tPrec@1 43.750 (56.210)\n",
      "EPOCH: 181 val Results: Prec@1 56.210 Loss: 1.2391\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [182][0/390]\tTime 0.006 (0.006)\tLoss 1.1188 (1.1188)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [182][78/390]\tTime 0.003 (0.003)\tLoss 1.2336 (1.0785)\tPrec@1 51.562 (62.154)\n",
      "Epoch: [182][156/390]\tTime 0.005 (0.003)\tLoss 0.9942 (1.1012)\tPrec@1 67.969 (61.365)\n",
      "Epoch: [182][234/390]\tTime 0.004 (0.003)\tLoss 1.2998 (1.1192)\tPrec@1 53.906 (60.708)\n",
      "Epoch: [182][312/390]\tTime 0.002 (0.003)\tLoss 1.1422 (1.1254)\tPrec@1 63.281 (60.521)\n",
      "Epoch: [182][390/390]\tTime 0.003 (0.003)\tLoss 1.2164 (1.1314)\tPrec@1 58.750 (60.190)\n",
      "EPOCH: 182 train Results: Prec@1 60.190 Loss: 1.1314\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1541 (1.1541)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5318 (1.2414)\tPrec@1 31.250 (55.630)\n",
      "EPOCH: 182 val Results: Prec@1 55.630 Loss: 1.2414\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [183][0/390]\tTime 0.004 (0.004)\tLoss 1.0237 (1.0237)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [183][78/390]\tTime 0.002 (0.003)\tLoss 1.0984 (1.0780)\tPrec@1 60.938 (62.233)\n",
      "Epoch: [183][156/390]\tTime 0.005 (0.003)\tLoss 1.1071 (1.0937)\tPrec@1 66.406 (61.679)\n",
      "Epoch: [183][234/390]\tTime 0.002 (0.003)\tLoss 1.1405 (1.1118)\tPrec@1 56.250 (61.167)\n",
      "Epoch: [183][312/390]\tTime 0.003 (0.003)\tLoss 1.0787 (1.1215)\tPrec@1 60.156 (60.478)\n",
      "Epoch: [183][390/390]\tTime 0.001 (0.003)\tLoss 1.1983 (1.1324)\tPrec@1 53.750 (60.158)\n",
      "EPOCH: 183 train Results: Prec@1 60.158 Loss: 1.1324\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1321 (1.1321)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6793 (1.2524)\tPrec@1 31.250 (54.630)\n",
      "EPOCH: 183 val Results: Prec@1 54.630 Loss: 1.2524\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [184][0/390]\tTime 0.004 (0.004)\tLoss 0.9947 (0.9947)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [184][78/390]\tTime 0.002 (0.003)\tLoss 1.1263 (1.0751)\tPrec@1 59.375 (62.530)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.003)\tLoss 1.2000 (1.0969)\tPrec@1 57.812 (61.331)\n",
      "Epoch: [184][234/390]\tTime 0.002 (0.003)\tLoss 1.2483 (1.1154)\tPrec@1 54.688 (60.974)\n",
      "Epoch: [184][312/390]\tTime 0.003 (0.003)\tLoss 1.1320 (1.1258)\tPrec@1 57.031 (60.581)\n",
      "Epoch: [184][390/390]\tTime 0.002 (0.003)\tLoss 1.3121 (1.1310)\tPrec@1 51.250 (60.246)\n",
      "EPOCH: 184 train Results: Prec@1 60.246 Loss: 1.1310\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0847 (1.0847)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3908 (1.2449)\tPrec@1 50.000 (55.520)\n",
      "EPOCH: 184 val Results: Prec@1 55.520 Loss: 1.2449\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [185][0/390]\tTime 0.006 (0.006)\tLoss 1.0660 (1.0660)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [185][78/390]\tTime 0.003 (0.003)\tLoss 1.2359 (1.0844)\tPrec@1 60.156 (62.184)\n",
      "Epoch: [185][156/390]\tTime 0.003 (0.003)\tLoss 1.0078 (1.0958)\tPrec@1 71.875 (61.818)\n",
      "Epoch: [185][234/390]\tTime 0.005 (0.003)\tLoss 1.2397 (1.1033)\tPrec@1 60.156 (61.759)\n",
      "Epoch: [185][312/390]\tTime 0.002 (0.003)\tLoss 1.0332 (1.1132)\tPrec@1 58.594 (61.200)\n",
      "Epoch: [185][390/390]\tTime 0.001 (0.003)\tLoss 1.2868 (1.1292)\tPrec@1 51.250 (60.530)\n",
      "EPOCH: 185 train Results: Prec@1 60.530 Loss: 1.1292\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1921 (1.1921)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2369 (1.2610)\tPrec@1 56.250 (55.190)\n",
      "EPOCH: 185 val Results: Prec@1 55.190 Loss: 1.2610\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [186][0/390]\tTime 0.005 (0.005)\tLoss 1.0427 (1.0427)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [186][78/390]\tTime 0.002 (0.003)\tLoss 1.0845 (1.0732)\tPrec@1 64.844 (63.390)\n",
      "Epoch: [186][156/390]\tTime 0.003 (0.003)\tLoss 1.1136 (1.0991)\tPrec@1 59.375 (61.803)\n",
      "Epoch: [186][234/390]\tTime 0.004 (0.003)\tLoss 1.1514 (1.1146)\tPrec@1 56.250 (61.090)\n",
      "Epoch: [186][312/390]\tTime 0.008 (0.003)\tLoss 1.0419 (1.1257)\tPrec@1 67.969 (60.538)\n",
      "Epoch: [186][390/390]\tTime 0.001 (0.003)\tLoss 0.9135 (1.1298)\tPrec@1 68.750 (60.348)\n",
      "EPOCH: 186 train Results: Prec@1 60.348 Loss: 1.1298\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0996 (1.0996)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4357 (1.2482)\tPrec@1 43.750 (55.730)\n",
      "EPOCH: 186 val Results: Prec@1 55.730 Loss: 1.2482\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [187][0/390]\tTime 0.008 (0.008)\tLoss 1.1523 (1.1523)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [187][78/390]\tTime 0.002 (0.002)\tLoss 1.0036 (1.0649)\tPrec@1 62.500 (63.123)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.2868 (1.0927)\tPrec@1 52.344 (62.231)\n",
      "Epoch: [187][234/390]\tTime 0.005 (0.003)\tLoss 1.1420 (1.1054)\tPrec@1 60.156 (61.639)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.2406 (1.1215)\tPrec@1 57.812 (60.920)\n",
      "Epoch: [187][390/390]\tTime 0.002 (0.003)\tLoss 1.2681 (1.1311)\tPrec@1 55.000 (60.528)\n",
      "EPOCH: 187 train Results: Prec@1 60.528 Loss: 1.1311\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1705 (1.1705)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4499 (1.2432)\tPrec@1 43.750 (55.420)\n",
      "EPOCH: 187 val Results: Prec@1 55.420 Loss: 1.2432\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [188][0/390]\tTime 0.003 (0.003)\tLoss 1.0278 (1.0278)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [188][78/390]\tTime 0.005 (0.003)\tLoss 1.3176 (1.0688)\tPrec@1 53.906 (63.153)\n",
      "Epoch: [188][156/390]\tTime 0.002 (0.003)\tLoss 1.1646 (1.0919)\tPrec@1 56.250 (62.102)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.003)\tLoss 1.2336 (1.1045)\tPrec@1 51.562 (61.543)\n",
      "Epoch: [188][312/390]\tTime 0.025 (0.003)\tLoss 1.2254 (1.1138)\tPrec@1 58.594 (61.180)\n",
      "Epoch: [188][390/390]\tTime 0.001 (0.003)\tLoss 1.2317 (1.1270)\tPrec@1 53.750 (60.624)\n",
      "EPOCH: 188 train Results: Prec@1 60.624 Loss: 1.1270\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1674 (1.1674)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3605 (1.2483)\tPrec@1 37.500 (55.280)\n",
      "EPOCH: 188 val Results: Prec@1 55.280 Loss: 1.2483\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [189][0/390]\tTime 0.003 (0.003)\tLoss 1.1810 (1.1810)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.003)\tLoss 0.8955 (1.0694)\tPrec@1 71.875 (63.232)\n",
      "Epoch: [189][156/390]\tTime 0.002 (0.003)\tLoss 1.2525 (1.0944)\tPrec@1 56.250 (62.007)\n",
      "Epoch: [189][234/390]\tTime 0.003 (0.003)\tLoss 1.1649 (1.1067)\tPrec@1 56.250 (61.513)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.003)\tLoss 1.1538 (1.1184)\tPrec@1 59.375 (60.987)\n",
      "Epoch: [189][390/390]\tTime 0.005 (0.003)\tLoss 1.3224 (1.1310)\tPrec@1 50.000 (60.508)\n",
      "EPOCH: 189 train Results: Prec@1 60.508 Loss: 1.1310\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1313 (1.1313)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3793 (1.2403)\tPrec@1 50.000 (55.830)\n",
      "EPOCH: 189 val Results: Prec@1 55.830 Loss: 1.2403\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [190][0/390]\tTime 0.002 (0.002)\tLoss 0.9621 (0.9621)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.003)\tLoss 1.0588 (1.0766)\tPrec@1 60.156 (62.757)\n",
      "Epoch: [190][156/390]\tTime 0.003 (0.003)\tLoss 1.1904 (1.0911)\tPrec@1 57.812 (62.052)\n",
      "Epoch: [190][234/390]\tTime 0.002 (0.003)\tLoss 1.1332 (1.1089)\tPrec@1 57.812 (61.170)\n",
      "Epoch: [190][312/390]\tTime 0.006 (0.003)\tLoss 1.0893 (1.1196)\tPrec@1 64.062 (60.910)\n",
      "Epoch: [190][390/390]\tTime 0.003 (0.003)\tLoss 1.2237 (1.1311)\tPrec@1 55.000 (60.450)\n",
      "EPOCH: 190 train Results: Prec@1 60.450 Loss: 1.1311\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1722 (1.1722)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5370 (1.2469)\tPrec@1 37.500 (55.690)\n",
      "EPOCH: 190 val Results: Prec@1 55.690 Loss: 1.2469\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [191][0/390]\tTime 0.004 (0.004)\tLoss 1.1476 (1.1476)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.003)\tLoss 1.0057 (1.0801)\tPrec@1 69.531 (62.648)\n",
      "Epoch: [191][156/390]\tTime 0.004 (0.003)\tLoss 1.1269 (1.1005)\tPrec@1 60.938 (61.654)\n",
      "Epoch: [191][234/390]\tTime 0.004 (0.003)\tLoss 1.0614 (1.1092)\tPrec@1 62.500 (61.253)\n",
      "Epoch: [191][312/390]\tTime 0.003 (0.003)\tLoss 1.0174 (1.1186)\tPrec@1 64.062 (60.977)\n",
      "Epoch: [191][390/390]\tTime 0.003 (0.003)\tLoss 1.2866 (1.1302)\tPrec@1 48.750 (60.512)\n",
      "EPOCH: 191 train Results: Prec@1 60.512 Loss: 1.1302\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1177 (1.1177)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5175 (1.2381)\tPrec@1 37.500 (56.090)\n",
      "EPOCH: 191 val Results: Prec@1 56.090 Loss: 1.2381\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [192][0/390]\tTime 0.005 (0.005)\tLoss 0.9207 (0.9207)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [192][78/390]\tTime 0.020 (0.003)\tLoss 1.1284 (1.0870)\tPrec@1 53.906 (62.441)\n",
      "Epoch: [192][156/390]\tTime 0.002 (0.004)\tLoss 1.1017 (1.0954)\tPrec@1 64.062 (61.734)\n",
      "Epoch: [192][234/390]\tTime 0.003 (0.004)\tLoss 1.0379 (1.1076)\tPrec@1 66.406 (61.200)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.004)\tLoss 1.2840 (1.1136)\tPrec@1 55.469 (60.875)\n",
      "Epoch: [192][390/390]\tTime 0.001 (0.004)\tLoss 1.1093 (1.1232)\tPrec@1 65.000 (60.608)\n",
      "EPOCH: 192 train Results: Prec@1 60.608 Loss: 1.1232\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0941 (1.0941)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4403 (1.2374)\tPrec@1 43.750 (56.490)\n",
      "EPOCH: 192 val Results: Prec@1 56.490 Loss: 1.2374\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [193][0/390]\tTime 0.004 (0.004)\tLoss 0.9597 (0.9597)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [193][78/390]\tTime 0.002 (0.003)\tLoss 0.9996 (1.0707)\tPrec@1 61.719 (62.856)\n",
      "Epoch: [193][156/390]\tTime 0.002 (0.004)\tLoss 1.1503 (1.0984)\tPrec@1 58.594 (61.679)\n",
      "Epoch: [193][234/390]\tTime 0.003 (0.003)\tLoss 1.1683 (1.1140)\tPrec@1 58.594 (61.160)\n",
      "Epoch: [193][312/390]\tTime 0.002 (0.003)\tLoss 1.2386 (1.1258)\tPrec@1 58.594 (60.601)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.003)\tLoss 1.1825 (1.1302)\tPrec@1 56.250 (60.388)\n",
      "EPOCH: 193 train Results: Prec@1 60.388 Loss: 1.1302\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1069 (1.1069)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6484 (1.2465)\tPrec@1 37.500 (55.480)\n",
      "EPOCH: 193 val Results: Prec@1 55.480 Loss: 1.2465\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [194][0/390]\tTime 0.003 (0.003)\tLoss 1.0798 (1.0798)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [194][78/390]\tTime 0.002 (0.003)\tLoss 1.0052 (1.0824)\tPrec@1 64.062 (62.164)\n",
      "Epoch: [194][156/390]\tTime 0.004 (0.003)\tLoss 1.0182 (1.0980)\tPrec@1 58.594 (61.754)\n",
      "Epoch: [194][234/390]\tTime 0.004 (0.003)\tLoss 1.1334 (1.1080)\tPrec@1 60.938 (61.077)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.003)\tLoss 1.1695 (1.1182)\tPrec@1 58.594 (60.738)\n",
      "Epoch: [194][390/390]\tTime 0.001 (0.003)\tLoss 1.2137 (1.1258)\tPrec@1 56.250 (60.462)\n",
      "EPOCH: 194 train Results: Prec@1 60.462 Loss: 1.1258\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1686 (1.1686)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3290 (1.2369)\tPrec@1 50.000 (55.620)\n",
      "EPOCH: 194 val Results: Prec@1 55.620 Loss: 1.2369\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [195][0/390]\tTime 0.005 (0.005)\tLoss 0.9902 (0.9902)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [195][78/390]\tTime 0.004 (0.003)\tLoss 1.0855 (1.0672)\tPrec@1 63.281 (62.757)\n",
      "Epoch: [195][156/390]\tTime 0.003 (0.003)\tLoss 1.0080 (1.0895)\tPrec@1 61.719 (61.624)\n",
      "Epoch: [195][234/390]\tTime 0.002 (0.003)\tLoss 1.1615 (1.1016)\tPrec@1 57.812 (61.310)\n",
      "Epoch: [195][312/390]\tTime 0.003 (0.003)\tLoss 1.3348 (1.1137)\tPrec@1 53.125 (60.775)\n",
      "Epoch: [195][390/390]\tTime 0.004 (0.003)\tLoss 1.1572 (1.1223)\tPrec@1 56.250 (60.536)\n",
      "EPOCH: 195 train Results: Prec@1 60.536 Loss: 1.1223\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1311 (1.1311)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4009 (1.2461)\tPrec@1 50.000 (56.060)\n",
      "EPOCH: 195 val Results: Prec@1 56.060 Loss: 1.2461\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [196][0/390]\tTime 0.003 (0.003)\tLoss 0.9402 (0.9402)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 1.1328 (1.0643)\tPrec@1 60.156 (62.807)\n",
      "Epoch: [196][156/390]\tTime 0.004 (0.003)\tLoss 1.2759 (1.0879)\tPrec@1 51.562 (61.943)\n",
      "Epoch: [196][234/390]\tTime 0.004 (0.003)\tLoss 1.1118 (1.1124)\tPrec@1 64.062 (61.124)\n",
      "Epoch: [196][312/390]\tTime 0.004 (0.003)\tLoss 1.1395 (1.1191)\tPrec@1 57.031 (60.860)\n",
      "Epoch: [196][390/390]\tTime 0.003 (0.003)\tLoss 1.1492 (1.1259)\tPrec@1 60.000 (60.546)\n",
      "EPOCH: 196 train Results: Prec@1 60.546 Loss: 1.1259\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0952 (1.0952)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4786 (1.2369)\tPrec@1 31.250 (55.600)\n",
      "EPOCH: 196 val Results: Prec@1 55.600 Loss: 1.2369\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [197][0/390]\tTime 0.002 (0.002)\tLoss 0.9081 (0.9081)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [197][78/390]\tTime 0.004 (0.003)\tLoss 1.1061 (1.0533)\tPrec@1 57.031 (63.627)\n",
      "Epoch: [197][156/390]\tTime 0.003 (0.003)\tLoss 1.1415 (1.0793)\tPrec@1 61.719 (62.555)\n",
      "Epoch: [197][234/390]\tTime 0.002 (0.003)\tLoss 1.0515 (1.1017)\tPrec@1 63.281 (61.669)\n",
      "Epoch: [197][312/390]\tTime 0.002 (0.003)\tLoss 1.0814 (1.1198)\tPrec@1 60.938 (60.905)\n",
      "Epoch: [197][390/390]\tTime 0.001 (0.003)\tLoss 0.9241 (1.1290)\tPrec@1 68.750 (60.486)\n",
      "EPOCH: 197 train Results: Prec@1 60.486 Loss: 1.1290\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1425 (1.1425)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2388 (1.2369)\tPrec@1 37.500 (55.640)\n",
      "EPOCH: 197 val Results: Prec@1 55.640 Loss: 1.2369\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [198][0/390]\tTime 0.007 (0.007)\tLoss 1.0496 (1.0496)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [198][78/390]\tTime 0.004 (0.003)\tLoss 1.1616 (1.0772)\tPrec@1 57.812 (62.490)\n",
      "Epoch: [198][156/390]\tTime 0.004 (0.003)\tLoss 1.0482 (1.0962)\tPrec@1 67.188 (61.669)\n",
      "Epoch: [198][234/390]\tTime 0.004 (0.003)\tLoss 1.3560 (1.1122)\tPrec@1 52.344 (61.074)\n",
      "Epoch: [198][312/390]\tTime 0.003 (0.003)\tLoss 0.9964 (1.1185)\tPrec@1 66.406 (60.833)\n",
      "Epoch: [198][390/390]\tTime 0.001 (0.003)\tLoss 1.0974 (1.1265)\tPrec@1 55.000 (60.502)\n",
      "EPOCH: 198 train Results: Prec@1 60.502 Loss: 1.1265\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1333 (1.1333)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4060 (1.2496)\tPrec@1 25.000 (55.240)\n",
      "EPOCH: 198 val Results: Prec@1 55.240 Loss: 1.2496\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [199][0/390]\tTime 0.005 (0.005)\tLoss 1.1477 (1.1477)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [199][78/390]\tTime 0.002 (0.003)\tLoss 1.0367 (1.0634)\tPrec@1 63.281 (63.182)\n",
      "Epoch: [199][156/390]\tTime 0.004 (0.003)\tLoss 1.1449 (1.0796)\tPrec@1 58.594 (62.570)\n",
      "Epoch: [199][234/390]\tTime 0.003 (0.003)\tLoss 1.1860 (1.0991)\tPrec@1 57.812 (61.582)\n",
      "Epoch: [199][312/390]\tTime 0.002 (0.003)\tLoss 1.1893 (1.1145)\tPrec@1 57.031 (61.077)\n",
      "Epoch: [199][390/390]\tTime 0.001 (0.003)\tLoss 1.1566 (1.1234)\tPrec@1 62.500 (60.780)\n",
      "EPOCH: 199 train Results: Prec@1 60.780 Loss: 1.1234\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1041 (1.1041)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2478 (1.2448)\tPrec@1 43.750 (55.380)\n",
      "EPOCH: 199 val Results: Prec@1 55.380 Loss: 1.2448\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [200][0/390]\tTime 0.003 (0.003)\tLoss 1.0072 (1.0072)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [200][78/390]\tTime 0.002 (0.003)\tLoss 1.1470 (1.0868)\tPrec@1 61.719 (62.184)\n",
      "Epoch: [200][156/390]\tTime 0.007 (0.003)\tLoss 0.8810 (1.1025)\tPrec@1 74.219 (61.689)\n",
      "Epoch: [200][234/390]\tTime 0.003 (0.003)\tLoss 1.0709 (1.1123)\tPrec@1 61.719 (61.193)\n",
      "Epoch: [200][312/390]\tTime 0.002 (0.003)\tLoss 1.1677 (1.1164)\tPrec@1 60.938 (61.040)\n",
      "Epoch: [200][390/390]\tTime 0.002 (0.003)\tLoss 1.4375 (1.1248)\tPrec@1 48.750 (60.520)\n",
      "EPOCH: 200 train Results: Prec@1 60.520 Loss: 1.1248\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1603 (1.1603)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1702 (1.2372)\tPrec@1 50.000 (55.740)\n",
      "EPOCH: 200 val Results: Prec@1 55.740 Loss: 1.2372\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "End time:  Thu Apr  4 23:04:39 2024\n",
      "train executed in 263.9041 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.001, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer3 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:04:39 2024\n",
      "current lr 5.00000e-03\n",
      "Epoch: [1][0/390]\tTime 0.005 (0.005)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.002 (0.004)\tLoss 2.8495 (3.7941)\tPrec@1 25.000 (18.839)\n",
      "Epoch: [1][156/390]\tTime 0.002 (0.004)\tLoss 2.8395 (3.1958)\tPrec@1 22.656 (23.238)\n",
      "Epoch: [1][234/390]\tTime 0.002 (0.004)\tLoss 2.1464 (2.8916)\tPrec@1 32.031 (25.967)\n",
      "Epoch: [1][312/390]\tTime 0.003 (0.004)\tLoss 2.1551 (2.6953)\tPrec@1 27.344 (27.611)\n",
      "Epoch: [1][390/390]\tTime 0.009 (0.004)\tLoss 1.8144 (2.5549)\tPrec@1 42.500 (28.946)\n",
      "EPOCH: 1 train Results: Prec@1 28.946 Loss: 2.5549\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.6903 (1.6903)\tPrec@1 39.062 (39.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6257 (1.7333)\tPrec@1 25.000 (39.680)\n",
      "EPOCH: 1 val Results: Prec@1 39.680 Loss: 1.7333\n",
      "Best Prec@1: 39.680\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [2][0/390]\tTime 0.002 (0.002)\tLoss 1.7873 (1.7873)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [2][78/390]\tTime 0.005 (0.004)\tLoss 1.6409 (1.7902)\tPrec@1 40.625 (38.687)\n",
      "Epoch: [2][156/390]\tTime 0.002 (0.003)\tLoss 1.7499 (1.7657)\tPrec@1 39.062 (39.237)\n",
      "Epoch: [2][234/390]\tTime 0.004 (0.003)\tLoss 1.9392 (1.7445)\tPrec@1 37.500 (39.681)\n",
      "Epoch: [2][312/390]\tTime 0.002 (0.003)\tLoss 1.8323 (1.7255)\tPrec@1 33.594 (40.151)\n",
      "Epoch: [2][390/390]\tTime 0.001 (0.003)\tLoss 1.5227 (1.7095)\tPrec@1 46.250 (40.494)\n",
      "EPOCH: 2 train Results: Prec@1 40.494 Loss: 1.7095\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4828 (1.4828)\tPrec@1 52.344 (52.344)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4950 (1.5752)\tPrec@1 25.000 (44.020)\n",
      "EPOCH: 2 val Results: Prec@1 44.020 Loss: 1.5752\n",
      "Best Prec@1: 44.020\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [3][0/390]\tTime 0.004 (0.004)\tLoss 1.5607 (1.5607)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [3][78/390]\tTime 0.002 (0.003)\tLoss 1.5141 (1.5747)\tPrec@1 43.750 (44.254)\n",
      "Epoch: [3][156/390]\tTime 0.002 (0.003)\tLoss 1.4676 (1.5616)\tPrec@1 49.219 (44.850)\n",
      "Epoch: [3][234/390]\tTime 0.002 (0.003)\tLoss 1.5768 (1.5578)\tPrec@1 37.500 (44.943)\n",
      "Epoch: [3][312/390]\tTime 0.003 (0.003)\tLoss 1.5774 (1.5544)\tPrec@1 43.750 (45.073)\n",
      "Epoch: [3][390/390]\tTime 0.001 (0.003)\tLoss 1.4644 (1.5493)\tPrec@1 51.250 (45.236)\n",
      "EPOCH: 3 train Results: Prec@1 45.236 Loss: 1.5493\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.4153 (1.4153)\tPrec@1 51.562 (51.562)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4978 (1.5016)\tPrec@1 25.000 (46.680)\n",
      "EPOCH: 3 val Results: Prec@1 46.680 Loss: 1.5016\n",
      "Best Prec@1: 46.680\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [4][0/390]\tTime 0.003 (0.003)\tLoss 1.5971 (1.5971)\tPrec@1 42.188 (42.188)\n",
      "Epoch: [4][78/390]\tTime 0.002 (0.004)\tLoss 1.5943 (1.4767)\tPrec@1 48.438 (47.894)\n",
      "Epoch: [4][156/390]\tTime 0.002 (0.003)\tLoss 1.5025 (1.4735)\tPrec@1 48.438 (47.935)\n",
      "Epoch: [4][234/390]\tTime 0.002 (0.003)\tLoss 1.5751 (1.4692)\tPrec@1 41.406 (48.019)\n",
      "Epoch: [4][312/390]\tTime 0.003 (0.003)\tLoss 1.3837 (1.4686)\tPrec@1 51.562 (48.121)\n",
      "Epoch: [4][390/390]\tTime 0.003 (0.003)\tLoss 1.3926 (1.4660)\tPrec@1 50.000 (48.208)\n",
      "EPOCH: 4 train Results: Prec@1 48.208 Loss: 1.4660\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3594 (1.3594)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4509 (1.4504)\tPrec@1 31.250 (48.630)\n",
      "EPOCH: 4 val Results: Prec@1 48.630 Loss: 1.4504\n",
      "Best Prec@1: 48.630\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [5][0/390]\tTime 0.003 (0.003)\tLoss 1.4619 (1.4619)\tPrec@1 47.656 (47.656)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.003)\tLoss 1.4717 (1.3846)\tPrec@1 47.656 (51.543)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.003)\tLoss 1.4495 (1.4065)\tPrec@1 47.656 (50.189)\n",
      "Epoch: [5][234/390]\tTime 0.003 (0.003)\tLoss 1.4093 (1.4037)\tPrec@1 55.469 (50.422)\n",
      "Epoch: [5][312/390]\tTime 0.002 (0.003)\tLoss 1.3782 (1.4040)\tPrec@1 53.125 (50.344)\n",
      "Epoch: [5][390/390]\tTime 0.002 (0.003)\tLoss 1.3891 (1.4050)\tPrec@1 51.250 (50.348)\n",
      "EPOCH: 5 train Results: Prec@1 50.348 Loss: 1.4050\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3071 (1.3071)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3425 (1.4056)\tPrec@1 43.750 (49.900)\n",
      "EPOCH: 5 val Results: Prec@1 49.900 Loss: 1.4056\n",
      "Best Prec@1: 49.900\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [6][0/390]\tTime 0.003 (0.003)\tLoss 1.3444 (1.3444)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [6][78/390]\tTime 0.003 (0.003)\tLoss 1.4717 (1.3373)\tPrec@1 39.062 (53.165)\n",
      "Epoch: [6][156/390]\tTime 0.003 (0.003)\tLoss 1.2266 (1.3434)\tPrec@1 50.000 (52.707)\n",
      "Epoch: [6][234/390]\tTime 0.010 (0.003)\tLoss 1.4006 (1.3498)\tPrec@1 52.344 (52.513)\n",
      "Epoch: [6][312/390]\tTime 0.003 (0.003)\tLoss 1.4502 (1.3558)\tPrec@1 44.531 (52.254)\n",
      "Epoch: [6][390/390]\tTime 0.001 (0.003)\tLoss 1.5659 (1.3576)\tPrec@1 43.750 (52.150)\n",
      "EPOCH: 6 train Results: Prec@1 52.150 Loss: 1.3576\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2831 (1.2831)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4295 (1.3733)\tPrec@1 37.500 (50.660)\n",
      "EPOCH: 6 val Results: Prec@1 50.660 Loss: 1.3733\n",
      "Best Prec@1: 50.660\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [7][0/390]\tTime 0.003 (0.003)\tLoss 1.3937 (1.3937)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [7][78/390]\tTime 0.003 (0.003)\tLoss 1.2876 (1.3021)\tPrec@1 54.688 (54.430)\n",
      "Epoch: [7][156/390]\tTime 0.005 (0.003)\tLoss 1.4036 (1.3105)\tPrec@1 46.094 (54.180)\n",
      "Epoch: [7][234/390]\tTime 0.002 (0.003)\tLoss 1.3640 (1.3149)\tPrec@1 46.094 (53.710)\n",
      "Epoch: [7][312/390]\tTime 0.003 (0.003)\tLoss 1.3659 (1.3164)\tPrec@1 51.562 (53.532)\n",
      "Epoch: [7][390/390]\tTime 0.003 (0.003)\tLoss 1.4069 (1.3222)\tPrec@1 48.750 (53.310)\n",
      "EPOCH: 7 train Results: Prec@1 53.310 Loss: 1.3222\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2567 (1.2567)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5106 (1.3442)\tPrec@1 25.000 (51.780)\n",
      "EPOCH: 7 val Results: Prec@1 51.780 Loss: 1.3442\n",
      "Best Prec@1: 51.780\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [8][0/390]\tTime 0.006 (0.006)\tLoss 1.2521 (1.2521)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [8][78/390]\tTime 0.002 (0.003)\tLoss 1.2677 (1.2753)\tPrec@1 56.250 (55.301)\n",
      "Epoch: [8][156/390]\tTime 0.004 (0.003)\tLoss 1.3657 (1.2896)\tPrec@1 48.438 (54.628)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.003)\tLoss 1.2914 (1.2893)\tPrec@1 57.812 (54.611)\n",
      "Epoch: [8][312/390]\tTime 0.002 (0.003)\tLoss 1.1786 (1.2906)\tPrec@1 58.594 (54.490)\n",
      "Epoch: [8][390/390]\tTime 0.002 (0.003)\tLoss 1.4216 (1.2954)\tPrec@1 46.250 (54.262)\n",
      "EPOCH: 8 train Results: Prec@1 54.262 Loss: 1.2954\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.2019 (1.2019)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.003)\tLoss 1.4144 (1.3269)\tPrec@1 31.250 (52.560)\n",
      "EPOCH: 8 val Results: Prec@1 52.560 Loss: 1.3269\n",
      "Best Prec@1: 52.560\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [9][0/390]\tTime 0.008 (0.008)\tLoss 1.2597 (1.2597)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [9][78/390]\tTime 0.002 (0.003)\tLoss 1.1861 (1.2413)\tPrec@1 62.500 (56.309)\n",
      "Epoch: [9][156/390]\tTime 0.004 (0.003)\tLoss 1.4311 (1.2519)\tPrec@1 49.219 (56.006)\n",
      "Epoch: [9][234/390]\tTime 0.005 (0.003)\tLoss 1.3548 (1.2669)\tPrec@1 42.188 (55.163)\n",
      "Epoch: [9][312/390]\tTime 0.003 (0.003)\tLoss 1.3723 (1.2733)\tPrec@1 54.688 (55.014)\n",
      "Epoch: [9][390/390]\tTime 0.002 (0.003)\tLoss 1.4765 (1.2808)\tPrec@1 48.750 (54.644)\n",
      "EPOCH: 9 train Results: Prec@1 54.644 Loss: 1.2808\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1798 (1.1798)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3817 (1.3140)\tPrec@1 37.500 (53.380)\n",
      "EPOCH: 9 val Results: Prec@1 53.380 Loss: 1.3140\n",
      "Best Prec@1: 53.380\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [10][0/390]\tTime 0.002 (0.002)\tLoss 1.2381 (1.2381)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [10][78/390]\tTime 0.004 (0.003)\tLoss 1.0791 (1.2236)\tPrec@1 62.500 (56.616)\n",
      "Epoch: [10][156/390]\tTime 0.007 (0.003)\tLoss 1.2796 (1.2442)\tPrec@1 57.812 (55.941)\n",
      "Epoch: [10][234/390]\tTime 0.002 (0.003)\tLoss 1.2975 (1.2551)\tPrec@1 56.250 (55.303)\n",
      "Epoch: [10][312/390]\tTime 0.005 (0.003)\tLoss 1.3320 (1.2607)\tPrec@1 51.562 (55.142)\n",
      "Epoch: [10][390/390]\tTime 0.001 (0.003)\tLoss 1.1078 (1.2659)\tPrec@1 63.750 (54.998)\n",
      "EPOCH: 10 train Results: Prec@1 54.998 Loss: 1.2659\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2036 (1.2036)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6117 (1.2958)\tPrec@1 25.000 (53.410)\n",
      "EPOCH: 10 val Results: Prec@1 53.410 Loss: 1.2958\n",
      "Best Prec@1: 53.410\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [11][0/390]\tTime 0.004 (0.004)\tLoss 1.1811 (1.1811)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [11][78/390]\tTime 0.002 (0.003)\tLoss 1.1507 (1.2217)\tPrec@1 59.375 (57.031)\n",
      "Epoch: [11][156/390]\tTime 0.002 (0.003)\tLoss 1.1143 (1.2330)\tPrec@1 60.156 (56.260)\n",
      "Epoch: [11][234/390]\tTime 0.004 (0.003)\tLoss 1.3425 (1.2403)\tPrec@1 52.344 (55.977)\n",
      "Epoch: [11][312/390]\tTime 0.002 (0.003)\tLoss 1.1065 (1.2456)\tPrec@1 61.719 (55.843)\n",
      "Epoch: [11][390/390]\tTime 0.002 (0.003)\tLoss 1.3126 (1.2531)\tPrec@1 55.000 (55.518)\n",
      "EPOCH: 11 train Results: Prec@1 55.518 Loss: 1.2531\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1333 (1.1333)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5294 (1.2960)\tPrec@1 25.000 (53.430)\n",
      "EPOCH: 11 val Results: Prec@1 53.430 Loss: 1.2960\n",
      "Best Prec@1: 53.430\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [12][0/390]\tTime 0.002 (0.002)\tLoss 1.1759 (1.1759)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [12][78/390]\tTime 0.002 (0.003)\tLoss 1.0957 (1.2001)\tPrec@1 61.719 (58.070)\n",
      "Epoch: [12][156/390]\tTime 0.002 (0.003)\tLoss 1.3876 (1.2142)\tPrec@1 51.562 (57.186)\n",
      "Epoch: [12][234/390]\tTime 0.004 (0.003)\tLoss 1.0957 (1.2265)\tPrec@1 66.406 (56.473)\n",
      "Epoch: [12][312/390]\tTime 0.003 (0.003)\tLoss 1.2848 (1.2350)\tPrec@1 54.688 (56.243)\n",
      "Epoch: [12][390/390]\tTime 0.003 (0.003)\tLoss 1.1339 (1.2418)\tPrec@1 58.750 (56.014)\n",
      "EPOCH: 12 train Results: Prec@1 56.014 Loss: 1.2418\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1276 (1.1276)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2622 (1.2977)\tPrec@1 50.000 (53.580)\n",
      "EPOCH: 12 val Results: Prec@1 53.580 Loss: 1.2977\n",
      "Best Prec@1: 53.580\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [13][0/390]\tTime 0.005 (0.005)\tLoss 1.2045 (1.2045)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.1051 (1.1913)\tPrec@1 59.375 (57.714)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.003)\tLoss 1.1128 (1.1970)\tPrec@1 64.062 (57.713)\n",
      "Epoch: [13][234/390]\tTime 0.006 (0.003)\tLoss 1.3036 (1.2135)\tPrec@1 51.562 (57.078)\n",
      "Epoch: [13][312/390]\tTime 0.003 (0.003)\tLoss 1.2044 (1.2249)\tPrec@1 56.250 (56.624)\n",
      "Epoch: [13][390/390]\tTime 0.007 (0.003)\tLoss 1.2259 (1.2308)\tPrec@1 52.500 (56.516)\n",
      "EPOCH: 13 train Results: Prec@1 56.516 Loss: 1.2308\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1564 (1.1564)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5085 (1.2821)\tPrec@1 25.000 (53.560)\n",
      "EPOCH: 13 val Results: Prec@1 53.560 Loss: 1.2821\n",
      "Best Prec@1: 53.580\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [14][0/390]\tTime 0.003 (0.003)\tLoss 1.0853 (1.0853)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [14][78/390]\tTime 0.002 (0.003)\tLoss 1.2653 (1.1843)\tPrec@1 56.250 (58.248)\n",
      "Epoch: [14][156/390]\tTime 0.011 (0.003)\tLoss 1.3218 (1.1941)\tPrec@1 54.688 (57.504)\n",
      "Epoch: [14][234/390]\tTime 0.003 (0.003)\tLoss 1.2504 (1.2075)\tPrec@1 55.469 (57.131)\n",
      "Epoch: [14][312/390]\tTime 0.002 (0.003)\tLoss 1.1624 (1.2134)\tPrec@1 51.562 (56.954)\n",
      "Epoch: [14][390/390]\tTime 0.004 (0.003)\tLoss 1.3774 (1.2198)\tPrec@1 42.500 (56.650)\n",
      "EPOCH: 14 train Results: Prec@1 56.650 Loss: 1.2198\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1397 (1.1397)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4574 (1.2760)\tPrec@1 31.250 (54.210)\n",
      "EPOCH: 14 val Results: Prec@1 54.210 Loss: 1.2760\n",
      "Best Prec@1: 54.210\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [15][0/390]\tTime 0.003 (0.003)\tLoss 1.0741 (1.0741)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [15][78/390]\tTime 0.005 (0.004)\tLoss 1.2751 (1.1667)\tPrec@1 54.688 (59.049)\n",
      "Epoch: [15][156/390]\tTime 0.003 (0.003)\tLoss 1.0538 (1.1817)\tPrec@1 65.625 (58.539)\n",
      "Epoch: [15][234/390]\tTime 0.008 (0.003)\tLoss 1.2180 (1.1929)\tPrec@1 57.031 (57.889)\n",
      "Epoch: [15][312/390]\tTime 0.002 (0.003)\tLoss 1.3535 (1.2026)\tPrec@1 53.906 (57.478)\n",
      "Epoch: [15][390/390]\tTime 0.003 (0.003)\tLoss 1.2095 (1.2083)\tPrec@1 50.000 (57.258)\n",
      "EPOCH: 15 train Results: Prec@1 57.258 Loss: 1.2083\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1067 (1.1067)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4245 (1.2746)\tPrec@1 31.250 (53.890)\n",
      "EPOCH: 15 val Results: Prec@1 53.890 Loss: 1.2746\n",
      "Best Prec@1: 54.210\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [16][0/390]\tTime 0.009 (0.009)\tLoss 1.0743 (1.0743)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [16][78/390]\tTime 0.005 (0.003)\tLoss 1.1126 (1.1383)\tPrec@1 64.844 (59.909)\n",
      "Epoch: [16][156/390]\tTime 0.002 (0.004)\tLoss 1.1793 (1.1640)\tPrec@1 57.812 (59.042)\n",
      "Epoch: [16][234/390]\tTime 0.005 (0.003)\tLoss 1.3140 (1.1823)\tPrec@1 53.906 (58.451)\n",
      "Epoch: [16][312/390]\tTime 0.002 (0.003)\tLoss 1.2074 (1.1947)\tPrec@1 60.156 (57.817)\n",
      "Epoch: [16][390/390]\tTime 0.001 (0.003)\tLoss 1.4048 (1.2029)\tPrec@1 53.750 (57.464)\n",
      "EPOCH: 16 train Results: Prec@1 57.464 Loss: 1.2029\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0526 (1.0526)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3694 (1.2652)\tPrec@1 43.750 (54.660)\n",
      "EPOCH: 16 val Results: Prec@1 54.660 Loss: 1.2652\n",
      "Best Prec@1: 54.660\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [17][0/390]\tTime 0.004 (0.004)\tLoss 1.1530 (1.1530)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [17][78/390]\tTime 0.002 (0.003)\tLoss 1.2210 (1.1490)\tPrec@1 57.812 (59.553)\n",
      "Epoch: [17][156/390]\tTime 0.004 (0.003)\tLoss 1.3452 (1.1609)\tPrec@1 54.688 (59.261)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.003)\tLoss 1.1725 (1.1761)\tPrec@1 55.469 (58.554)\n",
      "Epoch: [17][312/390]\tTime 0.004 (0.003)\tLoss 1.2177 (1.1831)\tPrec@1 59.375 (58.259)\n",
      "Epoch: [17][390/390]\tTime 0.002 (0.003)\tLoss 1.1847 (1.1917)\tPrec@1 65.000 (57.946)\n",
      "EPOCH: 17 train Results: Prec@1 57.946 Loss: 1.1917\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1621 (1.1621)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3653 (1.2608)\tPrec@1 31.250 (54.930)\n",
      "EPOCH: 17 val Results: Prec@1 54.930 Loss: 1.2608\n",
      "Best Prec@1: 54.930\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [18][0/390]\tTime 0.005 (0.005)\tLoss 0.9917 (0.9917)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [18][78/390]\tTime 0.002 (0.003)\tLoss 1.1807 (1.1414)\tPrec@1 57.812 (59.919)\n",
      "Epoch: [18][156/390]\tTime 0.002 (0.003)\tLoss 1.1015 (1.1594)\tPrec@1 67.188 (59.156)\n",
      "Epoch: [18][234/390]\tTime 0.002 (0.003)\tLoss 1.0437 (1.1805)\tPrec@1 62.500 (58.108)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.003)\tLoss 1.2282 (1.1855)\tPrec@1 58.594 (57.925)\n",
      "Epoch: [18][390/390]\tTime 0.001 (0.003)\tLoss 1.1907 (1.1913)\tPrec@1 58.750 (57.684)\n",
      "EPOCH: 18 train Results: Prec@1 57.684 Loss: 1.1913\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1219 (1.1219)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2898 (1.2579)\tPrec@1 56.250 (54.400)\n",
      "EPOCH: 18 val Results: Prec@1 54.400 Loss: 1.2579\n",
      "Best Prec@1: 54.930\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [19][0/390]\tTime 0.009 (0.009)\tLoss 1.1920 (1.1920)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [19][78/390]\tTime 0.002 (0.003)\tLoss 1.3000 (1.1200)\tPrec@1 57.812 (60.878)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.003)\tLoss 1.1093 (1.1499)\tPrec@1 60.156 (59.723)\n",
      "Epoch: [19][234/390]\tTime 0.002 (0.003)\tLoss 1.1471 (1.1606)\tPrec@1 59.375 (59.099)\n",
      "Epoch: [19][312/390]\tTime 0.003 (0.003)\tLoss 1.0211 (1.1731)\tPrec@1 61.719 (58.546)\n",
      "Epoch: [19][390/390]\tTime 0.001 (0.003)\tLoss 1.3700 (1.1808)\tPrec@1 47.500 (58.196)\n",
      "EPOCH: 19 train Results: Prec@1 58.196 Loss: 1.1808\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1898 (1.1898)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6806 (1.2561)\tPrec@1 31.250 (55.120)\n",
      "EPOCH: 19 val Results: Prec@1 55.120 Loss: 1.2561\n",
      "Best Prec@1: 55.120\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [20][0/390]\tTime 0.002 (0.002)\tLoss 1.0959 (1.0959)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [20][78/390]\tTime 0.005 (0.003)\tLoss 0.9877 (1.1346)\tPrec@1 65.625 (59.780)\n",
      "Epoch: [20][156/390]\tTime 0.002 (0.003)\tLoss 1.0324 (1.1468)\tPrec@1 64.062 (59.350)\n",
      "Epoch: [20][234/390]\tTime 0.003 (0.003)\tLoss 1.1109 (1.1667)\tPrec@1 59.375 (58.554)\n",
      "Epoch: [20][312/390]\tTime 0.002 (0.003)\tLoss 1.2375 (1.1712)\tPrec@1 55.469 (58.389)\n",
      "Epoch: [20][390/390]\tTime 0.006 (0.003)\tLoss 1.2961 (1.1760)\tPrec@1 46.250 (58.200)\n",
      "EPOCH: 20 train Results: Prec@1 58.200 Loss: 1.1760\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1113 (1.1113)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5844 (1.2454)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 20 val Results: Prec@1 55.610 Loss: 1.2454\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [21][0/390]\tTime 0.006 (0.006)\tLoss 1.2493 (1.2493)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [21][78/390]\tTime 0.002 (0.003)\tLoss 1.2604 (1.1140)\tPrec@1 51.562 (60.839)\n",
      "Epoch: [21][156/390]\tTime 0.002 (0.003)\tLoss 1.2338 (1.1381)\tPrec@1 50.781 (59.684)\n",
      "Epoch: [21][234/390]\tTime 0.004 (0.003)\tLoss 1.1532 (1.1492)\tPrec@1 61.719 (59.362)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.1588 (1.1631)\tPrec@1 59.375 (58.928)\n",
      "Epoch: [21][390/390]\tTime 0.003 (0.003)\tLoss 1.1099 (1.1726)\tPrec@1 61.250 (58.556)\n",
      "EPOCH: 21 train Results: Prec@1 58.556 Loss: 1.1726\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1381 (1.1381)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2190 (1.2416)\tPrec@1 43.750 (55.190)\n",
      "EPOCH: 21 val Results: Prec@1 55.190 Loss: 1.2416\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [22][0/390]\tTime 0.002 (0.002)\tLoss 1.1992 (1.1992)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [22][78/390]\tTime 0.004 (0.003)\tLoss 1.0870 (1.1286)\tPrec@1 57.812 (60.117)\n",
      "Epoch: [22][156/390]\tTime 0.005 (0.003)\tLoss 1.0664 (1.1400)\tPrec@1 59.375 (59.619)\n",
      "Epoch: [22][234/390]\tTime 0.002 (0.003)\tLoss 1.1539 (1.1497)\tPrec@1 61.719 (59.245)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.003)\tLoss 1.1234 (1.1607)\tPrec@1 60.938 (59.008)\n",
      "Epoch: [22][390/390]\tTime 0.002 (0.003)\tLoss 1.0674 (1.1685)\tPrec@1 70.000 (58.614)\n",
      "EPOCH: 22 train Results: Prec@1 58.614 Loss: 1.1685\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1167 (1.1167)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2703 (1.2517)\tPrec@1 56.250 (55.080)\n",
      "EPOCH: 22 val Results: Prec@1 55.080 Loss: 1.2517\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [23][0/390]\tTime 0.004 (0.004)\tLoss 1.1154 (1.1154)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [23][78/390]\tTime 0.002 (0.003)\tLoss 1.1565 (1.1141)\tPrec@1 65.625 (60.542)\n",
      "Epoch: [23][156/390]\tTime 0.003 (0.003)\tLoss 1.3532 (1.1387)\tPrec@1 49.219 (59.679)\n",
      "Epoch: [23][234/390]\tTime 0.002 (0.003)\tLoss 1.1430 (1.1452)\tPrec@1 60.938 (59.412)\n",
      "Epoch: [23][312/390]\tTime 0.007 (0.003)\tLoss 1.2615 (1.1569)\tPrec@1 54.688 (59.041)\n",
      "Epoch: [23][390/390]\tTime 0.001 (0.003)\tLoss 1.3571 (1.1668)\tPrec@1 51.250 (58.766)\n",
      "EPOCH: 23 train Results: Prec@1 58.766 Loss: 1.1668\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1269 (1.1269)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2848 (1.2481)\tPrec@1 50.000 (55.540)\n",
      "EPOCH: 23 val Results: Prec@1 55.540 Loss: 1.2481\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [24][0/390]\tTime 0.009 (0.009)\tLoss 1.1320 (1.1320)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.1888 (1.0970)\tPrec@1 58.594 (61.650)\n",
      "Epoch: [24][156/390]\tTime 0.002 (0.003)\tLoss 1.2143 (1.1244)\tPrec@1 58.594 (60.106)\n",
      "Epoch: [24][234/390]\tTime 0.004 (0.003)\tLoss 1.2408 (1.1448)\tPrec@1 58.594 (59.438)\n",
      "Epoch: [24][312/390]\tTime 0.002 (0.003)\tLoss 1.2849 (1.1524)\tPrec@1 57.812 (59.238)\n",
      "Epoch: [24][390/390]\tTime 0.001 (0.003)\tLoss 1.3164 (1.1592)\tPrec@1 56.250 (58.932)\n",
      "EPOCH: 24 train Results: Prec@1 58.932 Loss: 1.1592\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0830 (1.0830)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.2823 (1.2530)\tPrec@1 56.250 (55.010)\n",
      "EPOCH: 24 val Results: Prec@1 55.010 Loss: 1.2530\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [25][0/390]\tTime 0.002 (0.002)\tLoss 1.0732 (1.0732)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [25][78/390]\tTime 0.002 (0.003)\tLoss 1.0521 (1.1025)\tPrec@1 57.812 (61.046)\n",
      "Epoch: [25][156/390]\tTime 0.003 (0.003)\tLoss 1.2440 (1.1258)\tPrec@1 54.688 (60.206)\n",
      "Epoch: [25][234/390]\tTime 0.003 (0.003)\tLoss 1.2274 (1.1387)\tPrec@1 54.688 (59.588)\n",
      "Epoch: [25][312/390]\tTime 0.005 (0.003)\tLoss 1.2523 (1.1492)\tPrec@1 53.906 (59.215)\n",
      "Epoch: [25][390/390]\tTime 0.001 (0.003)\tLoss 1.3174 (1.1534)\tPrec@1 51.250 (59.074)\n",
      "EPOCH: 25 train Results: Prec@1 59.074 Loss: 1.1534\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0846 (1.0846)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4605 (1.2443)\tPrec@1 31.250 (55.310)\n",
      "EPOCH: 25 val Results: Prec@1 55.310 Loss: 1.2443\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [26][0/390]\tTime 0.004 (0.004)\tLoss 1.1093 (1.1093)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [26][78/390]\tTime 0.003 (0.003)\tLoss 1.1272 (1.0922)\tPrec@1 64.844 (61.561)\n",
      "Epoch: [26][156/390]\tTime 0.002 (0.003)\tLoss 1.1578 (1.1174)\tPrec@1 58.594 (60.793)\n",
      "Epoch: [26][234/390]\tTime 0.004 (0.003)\tLoss 1.0955 (1.1319)\tPrec@1 64.844 (60.116)\n",
      "Epoch: [26][312/390]\tTime 0.003 (0.003)\tLoss 1.1409 (1.1446)\tPrec@1 56.250 (59.535)\n",
      "Epoch: [26][390/390]\tTime 0.001 (0.003)\tLoss 1.1945 (1.1506)\tPrec@1 56.250 (59.272)\n",
      "EPOCH: 26 train Results: Prec@1 59.272 Loss: 1.1506\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1052 (1.1052)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5416 (1.2453)\tPrec@1 37.500 (55.350)\n",
      "EPOCH: 26 val Results: Prec@1 55.350 Loss: 1.2453\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [27][0/390]\tTime 0.003 (0.003)\tLoss 1.1186 (1.1186)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [27][78/390]\tTime 0.003 (0.003)\tLoss 0.9938 (1.0932)\tPrec@1 64.062 (61.343)\n",
      "Epoch: [27][156/390]\tTime 0.002 (0.003)\tLoss 0.9739 (1.1117)\tPrec@1 67.969 (60.495)\n",
      "Epoch: [27][234/390]\tTime 0.002 (0.003)\tLoss 0.9497 (1.1266)\tPrec@1 67.188 (59.930)\n",
      "Epoch: [27][312/390]\tTime 0.004 (0.003)\tLoss 1.0674 (1.1386)\tPrec@1 62.500 (59.592)\n",
      "Epoch: [27][390/390]\tTime 0.002 (0.003)\tLoss 1.5435 (1.1484)\tPrec@1 43.750 (59.242)\n",
      "EPOCH: 27 train Results: Prec@1 59.242 Loss: 1.1484\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0920 (1.0920)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4958 (1.2396)\tPrec@1 43.750 (55.560)\n",
      "EPOCH: 27 val Results: Prec@1 55.560 Loss: 1.2396\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [28][0/390]\tTime 0.004 (0.004)\tLoss 1.0635 (1.0635)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [28][78/390]\tTime 0.005 (0.003)\tLoss 0.9934 (1.0911)\tPrec@1 66.406 (61.669)\n",
      "Epoch: [28][156/390]\tTime 0.003 (0.003)\tLoss 1.1167 (1.1076)\tPrec@1 60.938 (61.082)\n",
      "Epoch: [28][234/390]\tTime 0.004 (0.003)\tLoss 1.1034 (1.1234)\tPrec@1 57.812 (60.416)\n",
      "Epoch: [28][312/390]\tTime 0.004 (0.003)\tLoss 1.2519 (1.1332)\tPrec@1 56.250 (60.059)\n",
      "Epoch: [28][390/390]\tTime 0.001 (0.003)\tLoss 1.2989 (1.1420)\tPrec@1 50.000 (59.740)\n",
      "EPOCH: 28 train Results: Prec@1 59.740 Loss: 1.1420\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0942 (1.0942)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4875 (1.2379)\tPrec@1 50.000 (56.030)\n",
      "EPOCH: 28 val Results: Prec@1 56.030 Loss: 1.2379\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [29][0/390]\tTime 0.002 (0.002)\tLoss 1.0146 (1.0146)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [29][78/390]\tTime 0.003 (0.003)\tLoss 1.1840 (1.1012)\tPrec@1 56.250 (60.809)\n",
      "Epoch: [29][156/390]\tTime 0.005 (0.003)\tLoss 1.1207 (1.1148)\tPrec@1 59.375 (60.594)\n",
      "Epoch: [29][234/390]\tTime 0.002 (0.003)\tLoss 0.9797 (1.1255)\tPrec@1 72.656 (60.223)\n",
      "Epoch: [29][312/390]\tTime 0.008 (0.003)\tLoss 1.1592 (1.1367)\tPrec@1 57.031 (59.802)\n",
      "Epoch: [29][390/390]\tTime 0.001 (0.003)\tLoss 1.2679 (1.1435)\tPrec@1 50.000 (59.612)\n",
      "EPOCH: 29 train Results: Prec@1 59.612 Loss: 1.1435\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0259 (1.0259)\tPrec@1 69.531 (69.531)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1773 (1.2469)\tPrec@1 56.250 (55.410)\n",
      "EPOCH: 29 val Results: Prec@1 55.410 Loss: 1.2469\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [30][0/390]\tTime 0.002 (0.002)\tLoss 1.0986 (1.0986)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.003)\tLoss 1.1054 (1.0869)\tPrec@1 58.594 (61.491)\n",
      "Epoch: [30][156/390]\tTime 0.006 (0.003)\tLoss 1.1313 (1.1052)\tPrec@1 63.281 (60.898)\n",
      "Epoch: [30][234/390]\tTime 0.003 (0.003)\tLoss 1.1090 (1.1189)\tPrec@1 56.250 (60.436)\n",
      "Epoch: [30][312/390]\tTime 0.003 (0.003)\tLoss 1.1544 (1.1302)\tPrec@1 59.375 (59.927)\n",
      "Epoch: [30][390/390]\tTime 0.003 (0.003)\tLoss 1.1632 (1.1399)\tPrec@1 57.500 (59.570)\n",
      "EPOCH: 30 train Results: Prec@1 59.570 Loss: 1.1399\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0440 (1.0440)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3766 (1.2475)\tPrec@1 37.500 (55.140)\n",
      "EPOCH: 30 val Results: Prec@1 55.140 Loss: 1.2475\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [31][0/390]\tTime 0.009 (0.009)\tLoss 1.1084 (1.1084)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [31][78/390]\tTime 0.002 (0.004)\tLoss 1.0360 (1.0886)\tPrec@1 60.156 (61.541)\n",
      "Epoch: [31][156/390]\tTime 0.002 (0.003)\tLoss 1.0970 (1.1084)\tPrec@1 57.812 (60.709)\n",
      "Epoch: [31][234/390]\tTime 0.003 (0.003)\tLoss 1.1676 (1.1207)\tPrec@1 60.156 (60.256)\n",
      "Epoch: [31][312/390]\tTime 0.003 (0.003)\tLoss 1.1111 (1.1287)\tPrec@1 62.500 (59.872)\n",
      "Epoch: [31][390/390]\tTime 0.002 (0.003)\tLoss 1.1985 (1.1387)\tPrec@1 58.750 (59.516)\n",
      "EPOCH: 31 train Results: Prec@1 59.516 Loss: 1.1387\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1271 (1.1271)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2666 (1.2445)\tPrec@1 37.500 (55.690)\n",
      "EPOCH: 31 val Results: Prec@1 55.690 Loss: 1.2445\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [32][0/390]\tTime 0.004 (0.004)\tLoss 1.0746 (1.0746)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [32][78/390]\tTime 0.005 (0.003)\tLoss 1.1052 (1.0882)\tPrec@1 58.594 (61.837)\n",
      "Epoch: [32][156/390]\tTime 0.002 (0.003)\tLoss 0.8864 (1.1008)\tPrec@1 66.406 (61.291)\n",
      "Epoch: [32][234/390]\tTime 0.003 (0.003)\tLoss 1.1043 (1.1154)\tPrec@1 60.156 (60.655)\n",
      "Epoch: [32][312/390]\tTime 0.002 (0.003)\tLoss 1.0891 (1.1241)\tPrec@1 60.156 (60.301)\n",
      "Epoch: [32][390/390]\tTime 0.006 (0.003)\tLoss 1.0233 (1.1346)\tPrec@1 68.750 (59.990)\n",
      "EPOCH: 32 train Results: Prec@1 59.990 Loss: 1.1346\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1170 (1.1170)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3565 (1.2444)\tPrec@1 50.000 (55.580)\n",
      "EPOCH: 32 val Results: Prec@1 55.580 Loss: 1.2444\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [33][0/390]\tTime 0.008 (0.008)\tLoss 1.1935 (1.1935)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [33][78/390]\tTime 0.002 (0.004)\tLoss 1.2894 (1.1008)\tPrec@1 55.469 (61.462)\n",
      "Epoch: [33][156/390]\tTime 0.002 (0.004)\tLoss 1.2633 (1.1108)\tPrec@1 54.688 (60.803)\n",
      "Epoch: [33][234/390]\tTime 0.005 (0.003)\tLoss 1.2622 (1.1154)\tPrec@1 56.250 (60.379)\n",
      "Epoch: [33][312/390]\tTime 0.003 (0.003)\tLoss 1.1789 (1.1253)\tPrec@1 57.812 (59.982)\n",
      "Epoch: [33][390/390]\tTime 0.001 (0.003)\tLoss 1.1605 (1.1321)\tPrec@1 56.250 (59.826)\n",
      "EPOCH: 33 train Results: Prec@1 59.826 Loss: 1.1321\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1105 (1.1105)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3731 (1.2408)\tPrec@1 43.750 (55.540)\n",
      "EPOCH: 33 val Results: Prec@1 55.540 Loss: 1.2408\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [34][0/390]\tTime 0.003 (0.003)\tLoss 1.0543 (1.0543)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [34][78/390]\tTime 0.002 (0.003)\tLoss 1.1212 (1.0719)\tPrec@1 62.500 (62.362)\n",
      "Epoch: [34][156/390]\tTime 0.003 (0.003)\tLoss 1.1158 (1.0990)\tPrec@1 61.719 (61.067)\n",
      "Epoch: [34][234/390]\tTime 0.009 (0.003)\tLoss 1.1210 (1.1090)\tPrec@1 62.500 (60.755)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.1343 (1.1204)\tPrec@1 64.844 (60.284)\n",
      "Epoch: [34][390/390]\tTime 0.002 (0.003)\tLoss 1.0579 (1.1304)\tPrec@1 65.000 (59.928)\n",
      "EPOCH: 34 train Results: Prec@1 59.928 Loss: 1.1304\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0616 (1.0616)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3219 (1.2385)\tPrec@1 43.750 (55.540)\n",
      "EPOCH: 34 val Results: Prec@1 55.540 Loss: 1.2385\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [35][0/390]\tTime 0.004 (0.004)\tLoss 1.1319 (1.1319)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [35][78/390]\tTime 0.002 (0.003)\tLoss 1.1950 (1.0746)\tPrec@1 57.031 (61.600)\n",
      "Epoch: [35][156/390]\tTime 0.002 (0.003)\tLoss 1.3534 (1.0933)\tPrec@1 48.438 (60.982)\n",
      "Epoch: [35][234/390]\tTime 0.002 (0.003)\tLoss 1.1661 (1.1127)\tPrec@1 63.281 (60.419)\n",
      "Epoch: [35][312/390]\tTime 0.004 (0.003)\tLoss 1.1302 (1.1223)\tPrec@1 61.719 (60.094)\n",
      "Epoch: [35][390/390]\tTime 0.001 (0.003)\tLoss 1.0637 (1.1279)\tPrec@1 60.000 (59.914)\n",
      "EPOCH: 35 train Results: Prec@1 59.914 Loss: 1.1279\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0851 (1.0851)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3533 (1.2411)\tPrec@1 43.750 (56.260)\n",
      "EPOCH: 35 val Results: Prec@1 56.260 Loss: 1.2411\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [36][0/390]\tTime 0.005 (0.005)\tLoss 1.2431 (1.2431)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [36][78/390]\tTime 0.006 (0.004)\tLoss 1.1088 (1.0737)\tPrec@1 64.062 (62.282)\n",
      "Epoch: [36][156/390]\tTime 0.007 (0.003)\tLoss 1.1542 (1.0923)\tPrec@1 60.938 (61.639)\n",
      "Epoch: [36][234/390]\tTime 0.004 (0.003)\tLoss 1.1186 (1.1033)\tPrec@1 65.625 (61.137)\n",
      "Epoch: [36][312/390]\tTime 0.005 (0.003)\tLoss 1.1454 (1.1131)\tPrec@1 62.500 (60.635)\n",
      "Epoch: [36][390/390]\tTime 0.003 (0.003)\tLoss 1.2185 (1.1229)\tPrec@1 60.000 (60.400)\n",
      "EPOCH: 36 train Results: Prec@1 60.400 Loss: 1.1229\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0986 (1.0986)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3459 (1.2395)\tPrec@1 43.750 (55.930)\n",
      "EPOCH: 36 val Results: Prec@1 55.930 Loss: 1.2395\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [37][0/390]\tTime 0.003 (0.003)\tLoss 0.9165 (0.9165)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 0.9462 (1.0602)\tPrec@1 69.531 (62.965)\n",
      "Epoch: [37][156/390]\tTime 0.002 (0.003)\tLoss 1.0731 (1.0908)\tPrec@1 58.594 (61.455)\n",
      "Epoch: [37][234/390]\tTime 0.002 (0.003)\tLoss 1.3014 (1.1074)\tPrec@1 52.344 (60.685)\n",
      "Epoch: [37][312/390]\tTime 0.002 (0.003)\tLoss 1.0386 (1.1216)\tPrec@1 62.500 (60.194)\n",
      "Epoch: [37][390/390]\tTime 0.003 (0.003)\tLoss 1.1650 (1.1273)\tPrec@1 57.500 (60.022)\n",
      "EPOCH: 37 train Results: Prec@1 60.022 Loss: 1.1273\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1171 (1.1171)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.004 (0.003)\tLoss 1.3028 (1.2452)\tPrec@1 50.000 (55.530)\n",
      "EPOCH: 37 val Results: Prec@1 55.530 Loss: 1.2452\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [38][0/390]\tTime 0.009 (0.009)\tLoss 1.0316 (1.0316)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.003)\tLoss 1.1614 (1.0794)\tPrec@1 60.938 (62.322)\n",
      "Epoch: [38][156/390]\tTime 0.003 (0.003)\tLoss 1.0105 (1.0925)\tPrec@1 61.719 (61.754)\n",
      "Epoch: [38][234/390]\tTime 0.004 (0.003)\tLoss 1.1016 (1.1047)\tPrec@1 60.938 (61.203)\n",
      "Epoch: [38][312/390]\tTime 0.003 (0.004)\tLoss 1.3448 (1.1113)\tPrec@1 49.219 (60.833)\n",
      "Epoch: [38][390/390]\tTime 0.001 (0.004)\tLoss 1.2034 (1.1200)\tPrec@1 60.000 (60.506)\n",
      "EPOCH: 38 train Results: Prec@1 60.506 Loss: 1.1200\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1075 (1.1075)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2866 (1.2310)\tPrec@1 50.000 (56.240)\n",
      "EPOCH: 38 val Results: Prec@1 56.240 Loss: 1.2310\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [39][0/390]\tTime 0.006 (0.006)\tLoss 1.0911 (1.0911)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [39][78/390]\tTime 0.004 (0.004)\tLoss 1.1914 (1.0532)\tPrec@1 60.156 (63.331)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.004)\tLoss 1.0883 (1.0823)\tPrec@1 59.375 (62.167)\n",
      "Epoch: [39][234/390]\tTime 0.005 (0.004)\tLoss 1.1433 (1.1004)\tPrec@1 60.156 (61.393)\n",
      "Epoch: [39][312/390]\tTime 0.003 (0.004)\tLoss 1.4549 (1.1087)\tPrec@1 46.094 (60.990)\n",
      "Epoch: [39][390/390]\tTime 0.004 (0.004)\tLoss 1.2186 (1.1202)\tPrec@1 52.500 (60.498)\n",
      "EPOCH: 39 train Results: Prec@1 60.498 Loss: 1.1202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0268 (1.0268)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5476 (1.2347)\tPrec@1 50.000 (55.660)\n",
      "EPOCH: 39 val Results: Prec@1 55.660 Loss: 1.2347\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [40][0/390]\tTime 0.004 (0.004)\tLoss 1.0170 (1.0170)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.003)\tLoss 1.0337 (1.0625)\tPrec@1 64.062 (62.698)\n",
      "Epoch: [40][156/390]\tTime 0.005 (0.003)\tLoss 1.0657 (1.0736)\tPrec@1 60.938 (62.361)\n",
      "Epoch: [40][234/390]\tTime 0.005 (0.003)\tLoss 1.3863 (1.0888)\tPrec@1 46.875 (61.765)\n",
      "Epoch: [40][312/390]\tTime 0.004 (0.003)\tLoss 1.3729 (1.1037)\tPrec@1 50.000 (61.195)\n",
      "Epoch: [40][390/390]\tTime 0.004 (0.003)\tLoss 1.3379 (1.1171)\tPrec@1 55.000 (60.556)\n",
      "EPOCH: 40 train Results: Prec@1 60.556 Loss: 1.1171\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1092 (1.1092)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5726 (1.2327)\tPrec@1 43.750 (55.870)\n",
      "EPOCH: 40 val Results: Prec@1 55.870 Loss: 1.2327\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [41][0/390]\tTime 0.002 (0.002)\tLoss 1.1889 (1.1889)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [41][78/390]\tTime 0.002 (0.003)\tLoss 1.0530 (1.0520)\tPrec@1 65.625 (62.975)\n",
      "Epoch: [41][156/390]\tTime 0.002 (0.003)\tLoss 1.2933 (1.0792)\tPrec@1 53.906 (61.968)\n",
      "Epoch: [41][234/390]\tTime 0.002 (0.003)\tLoss 1.1498 (1.0914)\tPrec@1 60.156 (61.516)\n",
      "Epoch: [41][312/390]\tTime 0.004 (0.003)\tLoss 1.1189 (1.1061)\tPrec@1 65.625 (60.933)\n",
      "Epoch: [41][390/390]\tTime 0.003 (0.003)\tLoss 1.0799 (1.1120)\tPrec@1 62.500 (60.592)\n",
      "EPOCH: 41 train Results: Prec@1 60.592 Loss: 1.1120\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0359 (1.0359)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2788 (1.2296)\tPrec@1 56.250 (56.100)\n",
      "EPOCH: 41 val Results: Prec@1 56.100 Loss: 1.2296\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [42][0/390]\tTime 0.002 (0.002)\tLoss 1.0957 (1.0957)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [42][78/390]\tTime 0.004 (0.003)\tLoss 0.9927 (1.0620)\tPrec@1 60.156 (62.876)\n",
      "Epoch: [42][156/390]\tTime 0.004 (0.003)\tLoss 1.0492 (1.0740)\tPrec@1 61.719 (62.331)\n",
      "Epoch: [42][234/390]\tTime 0.002 (0.003)\tLoss 1.1259 (1.0886)\tPrec@1 63.281 (61.420)\n",
      "Epoch: [42][312/390]\tTime 0.002 (0.003)\tLoss 1.2214 (1.1015)\tPrec@1 53.906 (60.933)\n",
      "Epoch: [42][390/390]\tTime 0.001 (0.003)\tLoss 1.2360 (1.1124)\tPrec@1 53.750 (60.518)\n",
      "EPOCH: 42 train Results: Prec@1 60.518 Loss: 1.1124\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0219 (1.0219)\tPrec@1 70.312 (70.312)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2728 (1.2332)\tPrec@1 37.500 (56.350)\n",
      "EPOCH: 42 val Results: Prec@1 56.350 Loss: 1.2332\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [43][0/390]\tTime 0.004 (0.004)\tLoss 1.0696 (1.0696)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [43][78/390]\tTime 0.003 (0.003)\tLoss 1.0778 (1.0569)\tPrec@1 61.719 (62.451)\n",
      "Epoch: [43][156/390]\tTime 0.002 (0.003)\tLoss 1.2046 (1.0806)\tPrec@1 55.469 (61.769)\n",
      "Epoch: [43][234/390]\tTime 0.003 (0.003)\tLoss 1.1199 (1.0878)\tPrec@1 58.594 (61.353)\n",
      "Epoch: [43][312/390]\tTime 0.007 (0.003)\tLoss 1.0187 (1.1022)\tPrec@1 61.719 (60.845)\n",
      "Epoch: [43][390/390]\tTime 0.004 (0.003)\tLoss 1.0087 (1.1114)\tPrec@1 58.750 (60.486)\n",
      "EPOCH: 43 train Results: Prec@1 60.486 Loss: 1.1114\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1239 (1.1239)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2751 (1.2362)\tPrec@1 37.500 (55.940)\n",
      "EPOCH: 43 val Results: Prec@1 55.940 Loss: 1.2362\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [44][0/390]\tTime 0.008 (0.008)\tLoss 1.0896 (1.0896)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.003)\tLoss 0.9578 (1.0592)\tPrec@1 64.844 (63.143)\n",
      "Epoch: [44][156/390]\tTime 0.002 (0.003)\tLoss 1.0253 (1.0716)\tPrec@1 61.719 (62.376)\n",
      "Epoch: [44][234/390]\tTime 0.003 (0.003)\tLoss 1.0702 (1.0869)\tPrec@1 59.375 (61.872)\n",
      "Epoch: [44][312/390]\tTime 0.004 (0.003)\tLoss 1.1389 (1.0987)\tPrec@1 55.469 (61.297)\n",
      "Epoch: [44][390/390]\tTime 0.002 (0.003)\tLoss 1.1196 (1.1083)\tPrec@1 56.250 (60.810)\n",
      "EPOCH: 44 train Results: Prec@1 60.810 Loss: 1.1083\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0369 (1.0369)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3291 (1.2319)\tPrec@1 56.250 (56.280)\n",
      "EPOCH: 44 val Results: Prec@1 56.280 Loss: 1.2319\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [45][0/390]\tTime 0.002 (0.002)\tLoss 1.1315 (1.1315)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [45][78/390]\tTime 0.002 (0.003)\tLoss 1.1972 (1.0548)\tPrec@1 59.375 (62.579)\n",
      "Epoch: [45][156/390]\tTime 0.003 (0.002)\tLoss 1.1068 (1.0660)\tPrec@1 57.031 (62.346)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.003)\tLoss 1.1699 (1.0825)\tPrec@1 60.156 (61.785)\n",
      "Epoch: [45][312/390]\tTime 0.002 (0.003)\tLoss 1.0127 (1.0960)\tPrec@1 62.500 (61.362)\n",
      "Epoch: [45][390/390]\tTime 0.001 (0.003)\tLoss 1.2370 (1.1045)\tPrec@1 50.000 (61.100)\n",
      "EPOCH: 45 train Results: Prec@1 61.100 Loss: 1.1045\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1024 (1.1024)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3949 (1.2324)\tPrec@1 43.750 (55.950)\n",
      "EPOCH: 45 val Results: Prec@1 55.950 Loss: 1.2324\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [46][0/390]\tTime 0.002 (0.002)\tLoss 1.0781 (1.0781)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [46][78/390]\tTime 0.003 (0.003)\tLoss 1.1216 (1.0565)\tPrec@1 62.500 (62.816)\n",
      "Epoch: [46][156/390]\tTime 0.006 (0.003)\tLoss 1.2054 (1.0659)\tPrec@1 54.688 (62.420)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.0797 (1.0829)\tPrec@1 64.062 (61.755)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.004)\tLoss 1.1435 (1.0934)\tPrec@1 60.156 (61.324)\n",
      "Epoch: [46][390/390]\tTime 0.005 (0.004)\tLoss 1.0570 (1.1054)\tPrec@1 61.250 (60.902)\n",
      "EPOCH: 46 train Results: Prec@1 60.902 Loss: 1.1054\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1002 (1.1002)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3881 (1.2367)\tPrec@1 37.500 (55.930)\n",
      "EPOCH: 46 val Results: Prec@1 55.930 Loss: 1.2367\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [47][0/390]\tTime 0.003 (0.003)\tLoss 1.3072 (1.3072)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [47][78/390]\tTime 0.002 (0.003)\tLoss 1.2435 (1.0478)\tPrec@1 56.250 (62.935)\n",
      "Epoch: [47][156/390]\tTime 0.009 (0.003)\tLoss 1.0516 (1.0675)\tPrec@1 64.844 (62.311)\n",
      "Epoch: [47][234/390]\tTime 0.071 (0.004)\tLoss 1.0144 (1.0859)\tPrec@1 61.719 (61.469)\n",
      "Epoch: [47][312/390]\tTime 0.002 (0.004)\tLoss 1.1420 (1.0919)\tPrec@1 53.125 (61.170)\n",
      "Epoch: [47][390/390]\tTime 0.003 (0.004)\tLoss 1.1035 (1.1013)\tPrec@1 53.750 (60.872)\n",
      "EPOCH: 47 train Results: Prec@1 60.872 Loss: 1.1013\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0236 (1.0236)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2713 (1.2342)\tPrec@1 50.000 (56.180)\n",
      "EPOCH: 47 val Results: Prec@1 56.180 Loss: 1.2342\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [48][0/390]\tTime 0.003 (0.003)\tLoss 1.0313 (1.0313)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [48][78/390]\tTime 0.003 (0.003)\tLoss 1.1448 (1.0513)\tPrec@1 55.469 (63.548)\n",
      "Epoch: [48][156/390]\tTime 0.005 (0.003)\tLoss 1.1415 (1.0697)\tPrec@1 60.156 (62.580)\n",
      "Epoch: [48][234/390]\tTime 0.002 (0.003)\tLoss 1.0314 (1.0791)\tPrec@1 62.500 (61.988)\n",
      "Epoch: [48][312/390]\tTime 0.002 (0.003)\tLoss 1.1223 (1.0930)\tPrec@1 61.719 (61.477)\n",
      "Epoch: [48][390/390]\tTime 0.001 (0.003)\tLoss 1.3848 (1.1034)\tPrec@1 52.500 (61.012)\n",
      "EPOCH: 48 train Results: Prec@1 61.012 Loss: 1.1034\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0834 (1.0834)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2664 (1.2405)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 48 val Results: Prec@1 55.610 Loss: 1.2405\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [49][0/390]\tTime 0.002 (0.002)\tLoss 1.0140 (1.0140)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [49][78/390]\tTime 0.004 (0.003)\tLoss 1.1803 (1.0312)\tPrec@1 58.594 (64.349)\n",
      "Epoch: [49][156/390]\tTime 0.002 (0.003)\tLoss 1.1827 (1.0613)\tPrec@1 57.812 (62.863)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.003)\tLoss 1.0539 (1.0794)\tPrec@1 58.594 (62.081)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.1552 (1.0898)\tPrec@1 56.250 (61.586)\n",
      "Epoch: [49][390/390]\tTime 0.001 (0.003)\tLoss 1.2602 (1.1020)\tPrec@1 57.500 (61.062)\n",
      "EPOCH: 49 train Results: Prec@1 61.062 Loss: 1.1020\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0766 (1.0766)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0305 (1.2342)\tPrec@1 50.000 (55.450)\n",
      "EPOCH: 49 val Results: Prec@1 55.450 Loss: 1.2342\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [50][0/390]\tTime 0.009 (0.009)\tLoss 0.9991 (0.9991)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [50][78/390]\tTime 0.002 (0.003)\tLoss 1.0411 (1.0431)\tPrec@1 64.844 (63.331)\n",
      "Epoch: [50][156/390]\tTime 0.002 (0.003)\tLoss 1.0951 (1.0587)\tPrec@1 56.250 (62.376)\n",
      "Epoch: [50][234/390]\tTime 0.003 (0.003)\tLoss 1.2334 (1.0780)\tPrec@1 54.688 (61.809)\n",
      "Epoch: [50][312/390]\tTime 0.004 (0.003)\tLoss 1.0302 (1.0894)\tPrec@1 58.594 (61.364)\n",
      "Epoch: [50][390/390]\tTime 0.004 (0.003)\tLoss 1.0230 (1.1003)\tPrec@1 65.000 (60.878)\n",
      "EPOCH: 50 train Results: Prec@1 60.878 Loss: 1.1003\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0744 (1.0744)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3533 (1.2394)\tPrec@1 50.000 (55.330)\n",
      "EPOCH: 50 val Results: Prec@1 55.330 Loss: 1.2394\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [51][0/390]\tTime 0.002 (0.002)\tLoss 1.0875 (1.0875)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [51][78/390]\tTime 0.002 (0.003)\tLoss 1.1042 (1.0511)\tPrec@1 58.594 (62.401)\n",
      "Epoch: [51][156/390]\tTime 0.004 (0.003)\tLoss 1.1185 (1.0677)\tPrec@1 65.625 (61.848)\n",
      "Epoch: [51][234/390]\tTime 0.002 (0.003)\tLoss 1.1774 (1.0784)\tPrec@1 55.469 (61.549)\n",
      "Epoch: [51][312/390]\tTime 0.004 (0.003)\tLoss 1.0610 (1.0883)\tPrec@1 60.938 (61.277)\n",
      "Epoch: [51][390/390]\tTime 0.004 (0.003)\tLoss 1.1077 (1.0990)\tPrec@1 57.500 (60.978)\n",
      "EPOCH: 51 train Results: Prec@1 60.978 Loss: 1.0990\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0972 (1.0972)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2760 (1.2414)\tPrec@1 31.250 (56.240)\n",
      "EPOCH: 51 val Results: Prec@1 56.240 Loss: 1.2414\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [52][0/390]\tTime 0.006 (0.006)\tLoss 1.0283 (1.0283)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [52][78/390]\tTime 0.004 (0.003)\tLoss 1.1272 (1.0395)\tPrec@1 53.906 (63.489)\n",
      "Epoch: [52][156/390]\tTime 0.002 (0.003)\tLoss 1.1983 (1.0511)\tPrec@1 54.688 (63.037)\n",
      "Epoch: [52][234/390]\tTime 0.003 (0.003)\tLoss 0.9455 (1.0696)\tPrec@1 68.750 (62.134)\n",
      "Epoch: [52][312/390]\tTime 0.005 (0.003)\tLoss 1.2442 (1.0863)\tPrec@1 56.250 (61.532)\n",
      "Epoch: [52][390/390]\tTime 0.003 (0.003)\tLoss 1.2180 (1.0975)\tPrec@1 56.250 (61.118)\n",
      "EPOCH: 52 train Results: Prec@1 61.118 Loss: 1.0975\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0850 (1.0850)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2981 (1.2413)\tPrec@1 62.500 (55.470)\n",
      "EPOCH: 52 val Results: Prec@1 55.470 Loss: 1.2413\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [53][0/390]\tTime 0.006 (0.006)\tLoss 1.1355 (1.1355)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [53][78/390]\tTime 0.002 (0.003)\tLoss 1.0428 (1.0328)\tPrec@1 65.625 (63.252)\n",
      "Epoch: [53][156/390]\tTime 0.002 (0.003)\tLoss 1.0206 (1.0514)\tPrec@1 66.406 (62.809)\n",
      "Epoch: [53][234/390]\tTime 0.002 (0.003)\tLoss 1.1020 (1.0704)\tPrec@1 56.250 (62.035)\n",
      "Epoch: [53][312/390]\tTime 0.004 (0.003)\tLoss 1.4058 (1.0835)\tPrec@1 53.125 (61.616)\n",
      "Epoch: [53][390/390]\tTime 0.002 (0.003)\tLoss 1.0275 (1.0952)\tPrec@1 63.750 (61.200)\n",
      "EPOCH: 53 train Results: Prec@1 61.200 Loss: 1.0952\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1602 (1.1602)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3560 (1.2338)\tPrec@1 37.500 (55.980)\n",
      "EPOCH: 53 val Results: Prec@1 55.980 Loss: 1.2338\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [54][0/390]\tTime 0.005 (0.005)\tLoss 0.9865 (0.9865)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [54][78/390]\tTime 0.003 (0.003)\tLoss 0.9839 (1.0348)\tPrec@1 65.625 (63.825)\n",
      "Epoch: [54][156/390]\tTime 0.003 (0.003)\tLoss 0.9527 (1.0531)\tPrec@1 61.719 (63.032)\n",
      "Epoch: [54][234/390]\tTime 0.004 (0.003)\tLoss 1.0113 (1.0686)\tPrec@1 64.062 (62.414)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2670 (1.0825)\tPrec@1 53.906 (61.701)\n",
      "Epoch: [54][390/390]\tTime 0.004 (0.003)\tLoss 1.1946 (1.0960)\tPrec@1 53.750 (61.248)\n",
      "EPOCH: 54 train Results: Prec@1 61.248 Loss: 1.0960\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0463 (1.0463)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2251 (1.2345)\tPrec@1 43.750 (56.290)\n",
      "EPOCH: 54 val Results: Prec@1 56.290 Loss: 1.2345\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.1065 (1.1065)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.1141 (1.0299)\tPrec@1 64.062 (63.459)\n",
      "Epoch: [55][156/390]\tTime 0.003 (0.003)\tLoss 1.1611 (1.0543)\tPrec@1 55.469 (62.619)\n",
      "Epoch: [55][234/390]\tTime 0.003 (0.003)\tLoss 1.1703 (1.0737)\tPrec@1 59.375 (62.035)\n",
      "Epoch: [55][312/390]\tTime 0.005 (0.003)\tLoss 0.9202 (1.0825)\tPrec@1 71.875 (61.629)\n",
      "Epoch: [55][390/390]\tTime 0.005 (0.003)\tLoss 1.0324 (1.0926)\tPrec@1 63.750 (61.248)\n",
      "EPOCH: 55 train Results: Prec@1 61.248 Loss: 1.0926\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1257 (1.1257)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4026 (1.2321)\tPrec@1 43.750 (56.020)\n",
      "EPOCH: 55 val Results: Prec@1 56.020 Loss: 1.2321\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [56][0/390]\tTime 0.004 (0.004)\tLoss 0.9296 (0.9296)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [56][78/390]\tTime 0.004 (0.003)\tLoss 1.1026 (1.0504)\tPrec@1 62.500 (62.688)\n",
      "Epoch: [56][156/390]\tTime 0.003 (0.003)\tLoss 0.9807 (1.0585)\tPrec@1 67.188 (62.435)\n",
      "Epoch: [56][234/390]\tTime 0.002 (0.003)\tLoss 1.1657 (1.0697)\tPrec@1 60.156 (62.045)\n",
      "Epoch: [56][312/390]\tTime 0.002 (0.003)\tLoss 1.0364 (1.0832)\tPrec@1 67.188 (61.539)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.003)\tLoss 1.1828 (1.0927)\tPrec@1 63.750 (61.266)\n",
      "EPOCH: 56 train Results: Prec@1 61.266 Loss: 1.0927\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1371 (1.1371)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2733 (1.2419)\tPrec@1 37.500 (55.780)\n",
      "EPOCH: 56 val Results: Prec@1 55.780 Loss: 1.2419\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [57][0/390]\tTime 0.004 (0.004)\tLoss 1.0124 (1.0124)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [57][78/390]\tTime 0.002 (0.004)\tLoss 1.0503 (1.0407)\tPrec@1 65.625 (63.271)\n",
      "Epoch: [57][156/390]\tTime 0.003 (0.004)\tLoss 1.0563 (1.0520)\tPrec@1 63.281 (62.719)\n",
      "Epoch: [57][234/390]\tTime 0.002 (0.003)\tLoss 1.1362 (1.0666)\tPrec@1 61.719 (62.048)\n",
      "Epoch: [57][312/390]\tTime 0.003 (0.003)\tLoss 1.1462 (1.0780)\tPrec@1 58.594 (61.611)\n",
      "Epoch: [57][390/390]\tTime 0.003 (0.003)\tLoss 1.1255 (1.0915)\tPrec@1 58.750 (61.144)\n",
      "EPOCH: 57 train Results: Prec@1 61.144 Loss: 1.0915\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2126 (1.2126)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3973 (1.2366)\tPrec@1 37.500 (55.580)\n",
      "EPOCH: 57 val Results: Prec@1 55.580 Loss: 1.2366\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [58][0/390]\tTime 0.003 (0.003)\tLoss 1.0216 (1.0216)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [58][78/390]\tTime 0.002 (0.003)\tLoss 1.0640 (1.0286)\tPrec@1 62.500 (64.062)\n",
      "Epoch: [58][156/390]\tTime 0.003 (0.003)\tLoss 1.0973 (1.0501)\tPrec@1 60.938 (63.301)\n",
      "Epoch: [58][234/390]\tTime 0.003 (0.003)\tLoss 1.0911 (1.0669)\tPrec@1 60.156 (62.583)\n",
      "Epoch: [58][312/390]\tTime 0.007 (0.003)\tLoss 1.0146 (1.0797)\tPrec@1 64.062 (62.043)\n",
      "Epoch: [58][390/390]\tTime 0.001 (0.003)\tLoss 1.1772 (1.0905)\tPrec@1 60.000 (61.548)\n",
      "EPOCH: 58 train Results: Prec@1 61.548 Loss: 1.0905\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1740 (1.1740)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2721 (1.2270)\tPrec@1 37.500 (56.060)\n",
      "EPOCH: 58 val Results: Prec@1 56.060 Loss: 1.2270\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [59][0/390]\tTime 0.002 (0.002)\tLoss 1.0426 (1.0426)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [59][78/390]\tTime 0.002 (0.003)\tLoss 1.0337 (1.0279)\tPrec@1 61.719 (63.776)\n",
      "Epoch: [59][156/390]\tTime 0.003 (0.003)\tLoss 1.1640 (1.0439)\tPrec@1 60.938 (63.182)\n",
      "Epoch: [59][234/390]\tTime 0.002 (0.003)\tLoss 1.0846 (1.0647)\tPrec@1 57.812 (62.284)\n",
      "Epoch: [59][312/390]\tTime 0.002 (0.003)\tLoss 1.0547 (1.0767)\tPrec@1 64.844 (61.916)\n",
      "Epoch: [59][390/390]\tTime 0.001 (0.003)\tLoss 1.0782 (1.0875)\tPrec@1 63.750 (61.524)\n",
      "EPOCH: 59 train Results: Prec@1 61.524 Loss: 1.0875\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1489 (1.1489)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.2032 (1.2305)\tPrec@1 37.500 (56.000)\n",
      "EPOCH: 59 val Results: Prec@1 56.000 Loss: 1.2305\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [60][0/390]\tTime 0.006 (0.006)\tLoss 0.8979 (0.8979)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [60][78/390]\tTime 0.004 (0.003)\tLoss 1.0319 (1.0355)\tPrec@1 58.594 (63.716)\n",
      "Epoch: [60][156/390]\tTime 0.003 (0.003)\tLoss 0.9109 (1.0547)\tPrec@1 73.438 (63.037)\n",
      "Epoch: [60][234/390]\tTime 0.002 (0.003)\tLoss 1.1139 (1.0715)\tPrec@1 60.156 (62.131)\n",
      "Epoch: [60][312/390]\tTime 0.003 (0.003)\tLoss 1.0180 (1.0827)\tPrec@1 62.500 (61.562)\n",
      "Epoch: [60][390/390]\tTime 0.001 (0.003)\tLoss 1.2097 (1.0896)\tPrec@1 53.750 (61.302)\n",
      "EPOCH: 60 train Results: Prec@1 61.302 Loss: 1.0896\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.0965 (1.0965)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0512 (1.2450)\tPrec@1 56.250 (55.790)\n",
      "EPOCH: 60 val Results: Prec@1 55.790 Loss: 1.2450\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [61][0/390]\tTime 0.004 (0.004)\tLoss 0.9360 (0.9360)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [61][78/390]\tTime 0.002 (0.003)\tLoss 1.0558 (1.0281)\tPrec@1 65.625 (64.033)\n",
      "Epoch: [61][156/390]\tTime 0.004 (0.004)\tLoss 1.1664 (1.0476)\tPrec@1 59.375 (63.077)\n",
      "Epoch: [61][234/390]\tTime 0.003 (0.003)\tLoss 1.0781 (1.0646)\tPrec@1 59.375 (62.231)\n",
      "Epoch: [61][312/390]\tTime 0.003 (0.003)\tLoss 1.1760 (1.0777)\tPrec@1 53.125 (61.699)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.003)\tLoss 1.1206 (1.0881)\tPrec@1 63.750 (61.388)\n",
      "EPOCH: 61 train Results: Prec@1 61.388 Loss: 1.0881\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1447 (1.1447)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3405 (1.2303)\tPrec@1 50.000 (56.270)\n",
      "EPOCH: 61 val Results: Prec@1 56.270 Loss: 1.2303\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [62][0/390]\tTime 0.003 (0.003)\tLoss 1.0378 (1.0378)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.0573 (1.0222)\tPrec@1 60.156 (64.349)\n",
      "Epoch: [62][156/390]\tTime 0.002 (0.004)\tLoss 0.9991 (1.0456)\tPrec@1 67.188 (63.207)\n",
      "Epoch: [62][234/390]\tTime 0.002 (0.004)\tLoss 1.1409 (1.0620)\tPrec@1 63.281 (62.613)\n",
      "Epoch: [62][312/390]\tTime 0.004 (0.004)\tLoss 1.1850 (1.0726)\tPrec@1 59.375 (62.163)\n",
      "Epoch: [62][390/390]\tTime 0.002 (0.004)\tLoss 0.9192 (1.0841)\tPrec@1 70.000 (61.710)\n",
      "EPOCH: 62 train Results: Prec@1 61.710 Loss: 1.0841\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1730 (1.1730)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2816 (1.2330)\tPrec@1 43.750 (56.100)\n",
      "EPOCH: 62 val Results: Prec@1 56.100 Loss: 1.2330\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [63][0/390]\tTime 0.003 (0.003)\tLoss 1.0487 (1.0487)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [63][78/390]\tTime 0.004 (0.003)\tLoss 1.1332 (1.0265)\tPrec@1 60.156 (63.647)\n",
      "Epoch: [63][156/390]\tTime 0.007 (0.003)\tLoss 1.0808 (1.0518)\tPrec@1 61.719 (62.500)\n",
      "Epoch: [63][234/390]\tTime 0.004 (0.003)\tLoss 1.2219 (1.0668)\tPrec@1 57.812 (62.124)\n",
      "Epoch: [63][312/390]\tTime 0.002 (0.003)\tLoss 1.2787 (1.0806)\tPrec@1 56.250 (61.654)\n",
      "Epoch: [63][390/390]\tTime 0.001 (0.003)\tLoss 1.2105 (1.0890)\tPrec@1 58.750 (61.430)\n",
      "EPOCH: 63 train Results: Prec@1 61.430 Loss: 1.0890\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1446 (1.1446)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1445 (1.2326)\tPrec@1 43.750 (56.210)\n",
      "EPOCH: 63 val Results: Prec@1 56.210 Loss: 1.2326\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [64][0/390]\tTime 0.009 (0.009)\tLoss 0.9187 (0.9187)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [64][78/390]\tTime 0.002 (0.003)\tLoss 1.0358 (1.0141)\tPrec@1 59.375 (64.062)\n",
      "Epoch: [64][156/390]\tTime 0.004 (0.003)\tLoss 1.1527 (1.0421)\tPrec@1 57.812 (63.222)\n",
      "Epoch: [64][234/390]\tTime 0.005 (0.003)\tLoss 1.2237 (1.0589)\tPrec@1 57.031 (62.497)\n",
      "Epoch: [64][312/390]\tTime 0.003 (0.003)\tLoss 1.1959 (1.0771)\tPrec@1 60.938 (61.869)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.003)\tLoss 1.3159 (1.0888)\tPrec@1 55.000 (61.464)\n",
      "EPOCH: 64 train Results: Prec@1 61.464 Loss: 1.0888\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1564 (1.1564)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0662 (1.2434)\tPrec@1 62.500 (55.460)\n",
      "EPOCH: 64 val Results: Prec@1 55.460 Loss: 1.2434\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [65][0/390]\tTime 0.004 (0.004)\tLoss 1.0095 (1.0095)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.003)\tLoss 1.0935 (1.0319)\tPrec@1 61.719 (63.410)\n",
      "Epoch: [65][156/390]\tTime 0.009 (0.003)\tLoss 1.0382 (1.0445)\tPrec@1 61.719 (63.042)\n",
      "Epoch: [65][234/390]\tTime 0.005 (0.003)\tLoss 1.1054 (1.0623)\tPrec@1 58.594 (62.264)\n",
      "Epoch: [65][312/390]\tTime 0.002 (0.003)\tLoss 1.0879 (1.0753)\tPrec@1 62.500 (61.779)\n",
      "Epoch: [65][390/390]\tTime 0.003 (0.003)\tLoss 1.1173 (1.0852)\tPrec@1 60.000 (61.308)\n",
      "EPOCH: 65 train Results: Prec@1 61.308 Loss: 1.0852\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0969 (1.0969)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2174 (1.2317)\tPrec@1 56.250 (55.970)\n",
      "EPOCH: 65 val Results: Prec@1 55.970 Loss: 1.2317\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [66][0/390]\tTime 0.003 (0.003)\tLoss 0.9867 (0.9867)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [66][78/390]\tTime 0.004 (0.003)\tLoss 1.1542 (1.0192)\tPrec@1 57.812 (63.736)\n",
      "Epoch: [66][156/390]\tTime 0.002 (0.003)\tLoss 1.0888 (1.0396)\tPrec@1 60.938 (63.371)\n",
      "Epoch: [66][234/390]\tTime 0.002 (0.004)\tLoss 1.1667 (1.0576)\tPrec@1 59.375 (62.580)\n",
      "Epoch: [66][312/390]\tTime 0.004 (0.004)\tLoss 1.2817 (1.0760)\tPrec@1 57.031 (61.918)\n",
      "Epoch: [66][390/390]\tTime 0.002 (0.004)\tLoss 1.0876 (1.0836)\tPrec@1 70.000 (61.740)\n",
      "EPOCH: 66 train Results: Prec@1 61.740 Loss: 1.0836\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1119 (1.1119)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2524 (1.2448)\tPrec@1 50.000 (55.760)\n",
      "EPOCH: 66 val Results: Prec@1 55.760 Loss: 1.2448\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [67][0/390]\tTime 0.002 (0.002)\tLoss 1.1526 (1.1526)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [67][78/390]\tTime 0.006 (0.004)\tLoss 1.0402 (1.0151)\tPrec@1 59.375 (64.211)\n",
      "Epoch: [67][156/390]\tTime 0.002 (0.003)\tLoss 1.2241 (1.0394)\tPrec@1 55.469 (63.067)\n",
      "Epoch: [67][234/390]\tTime 0.002 (0.003)\tLoss 1.1799 (1.0609)\tPrec@1 63.281 (62.267)\n",
      "Epoch: [67][312/390]\tTime 0.002 (0.003)\tLoss 1.0219 (1.0735)\tPrec@1 64.062 (61.729)\n",
      "Epoch: [67][390/390]\tTime 0.007 (0.003)\tLoss 1.1286 (1.0830)\tPrec@1 58.750 (61.428)\n",
      "EPOCH: 67 train Results: Prec@1 61.428 Loss: 1.0830\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1119 (1.1119)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0656 (1.2348)\tPrec@1 62.500 (55.940)\n",
      "EPOCH: 67 val Results: Prec@1 55.940 Loss: 1.2348\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [68][0/390]\tTime 0.002 (0.002)\tLoss 0.9049 (0.9049)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [68][78/390]\tTime 0.006 (0.003)\tLoss 0.9261 (1.0131)\tPrec@1 70.312 (64.379)\n",
      "Epoch: [68][156/390]\tTime 0.002 (0.003)\tLoss 1.2402 (1.0497)\tPrec@1 60.156 (62.948)\n",
      "Epoch: [68][234/390]\tTime 0.008 (0.003)\tLoss 1.1922 (1.0635)\tPrec@1 58.594 (62.330)\n",
      "Epoch: [68][312/390]\tTime 0.004 (0.003)\tLoss 1.2907 (1.0763)\tPrec@1 52.344 (62.046)\n",
      "Epoch: [68][390/390]\tTime 0.002 (0.003)\tLoss 1.3545 (1.0862)\tPrec@1 53.750 (61.738)\n",
      "EPOCH: 68 train Results: Prec@1 61.738 Loss: 1.0862\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0647 (1.0647)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1184 (1.2308)\tPrec@1 56.250 (56.190)\n",
      "EPOCH: 68 val Results: Prec@1 56.190 Loss: 1.2308\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [69][0/390]\tTime 0.004 (0.004)\tLoss 1.0045 (1.0045)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.003)\tLoss 1.0491 (0.9986)\tPrec@1 57.812 (64.636)\n",
      "Epoch: [69][156/390]\tTime 0.003 (0.003)\tLoss 1.0644 (1.0352)\tPrec@1 60.938 (63.436)\n",
      "Epoch: [69][234/390]\tTime 0.003 (0.003)\tLoss 0.9091 (1.0531)\tPrec@1 74.219 (62.779)\n",
      "Epoch: [69][312/390]\tTime 0.009 (0.003)\tLoss 1.1261 (1.0684)\tPrec@1 59.375 (62.088)\n",
      "Epoch: [69][390/390]\tTime 0.001 (0.003)\tLoss 1.1266 (1.0798)\tPrec@1 61.250 (61.666)\n",
      "EPOCH: 69 train Results: Prec@1 61.666 Loss: 1.0798\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1335 (1.1335)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1652 (1.2336)\tPrec@1 37.500 (56.310)\n",
      "EPOCH: 69 val Results: Prec@1 56.310 Loss: 1.2336\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [70][0/390]\tTime 0.002 (0.002)\tLoss 0.9863 (0.9863)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [70][78/390]\tTime 0.002 (0.003)\tLoss 1.1255 (1.0115)\tPrec@1 57.812 (64.616)\n",
      "Epoch: [70][156/390]\tTime 0.003 (0.003)\tLoss 1.1236 (1.0325)\tPrec@1 58.594 (63.654)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.003)\tLoss 1.0181 (1.0530)\tPrec@1 58.594 (62.763)\n",
      "Epoch: [70][312/390]\tTime 0.007 (0.003)\tLoss 1.0408 (1.0749)\tPrec@1 67.188 (61.953)\n",
      "Epoch: [70][390/390]\tTime 0.001 (0.003)\tLoss 1.1138 (1.0814)\tPrec@1 66.250 (61.660)\n",
      "EPOCH: 70 train Results: Prec@1 61.660 Loss: 1.0814\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1433 (1.1433)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0267 (1.2322)\tPrec@1 43.750 (55.700)\n",
      "EPOCH: 70 val Results: Prec@1 55.700 Loss: 1.2322\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [71][0/390]\tTime 0.002 (0.002)\tLoss 0.9564 (0.9564)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [71][78/390]\tTime 0.002 (0.003)\tLoss 1.0672 (1.0138)\tPrec@1 57.031 (64.686)\n",
      "Epoch: [71][156/390]\tTime 0.002 (0.003)\tLoss 1.0100 (1.0379)\tPrec@1 60.938 (63.595)\n",
      "Epoch: [71][234/390]\tTime 0.004 (0.003)\tLoss 0.9083 (1.0613)\tPrec@1 67.188 (62.610)\n",
      "Epoch: [71][312/390]\tTime 0.007 (0.003)\tLoss 1.0563 (1.0689)\tPrec@1 55.469 (62.225)\n",
      "Epoch: [71][390/390]\tTime 0.002 (0.003)\tLoss 0.9471 (1.0800)\tPrec@1 62.500 (61.792)\n",
      "EPOCH: 71 train Results: Prec@1 61.792 Loss: 1.0800\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1280 (1.1280)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2665 (1.2370)\tPrec@1 31.250 (56.180)\n",
      "EPOCH: 71 val Results: Prec@1 56.180 Loss: 1.2370\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [72][0/390]\tTime 0.005 (0.005)\tLoss 1.0654 (1.0654)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [72][78/390]\tTime 0.006 (0.003)\tLoss 1.1557 (1.0285)\tPrec@1 55.469 (63.865)\n",
      "Epoch: [72][156/390]\tTime 0.005 (0.003)\tLoss 1.0539 (1.0441)\tPrec@1 57.031 (63.202)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.003)\tLoss 1.1334 (1.0610)\tPrec@1 57.812 (62.320)\n",
      "Epoch: [72][312/390]\tTime 0.003 (0.003)\tLoss 1.1536 (1.0718)\tPrec@1 61.719 (61.963)\n",
      "Epoch: [72][390/390]\tTime 0.001 (0.003)\tLoss 0.9524 (1.0820)\tPrec@1 63.750 (61.592)\n",
      "EPOCH: 72 train Results: Prec@1 61.592 Loss: 1.0820\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0525 (1.0525)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3311 (1.2210)\tPrec@1 43.750 (56.460)\n",
      "EPOCH: 72 val Results: Prec@1 56.460 Loss: 1.2210\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [73][0/390]\tTime 0.005 (0.005)\tLoss 0.9201 (0.9201)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.003)\tLoss 0.9306 (1.0251)\tPrec@1 67.969 (64.181)\n",
      "Epoch: [73][156/390]\tTime 0.004 (0.003)\tLoss 1.1798 (1.0354)\tPrec@1 59.375 (63.251)\n",
      "Epoch: [73][234/390]\tTime 0.012 (0.003)\tLoss 1.0079 (1.0500)\tPrec@1 63.281 (62.686)\n",
      "Epoch: [73][312/390]\tTime 0.002 (0.003)\tLoss 0.9693 (1.0657)\tPrec@1 65.625 (62.263)\n",
      "Epoch: [73][390/390]\tTime 0.001 (0.003)\tLoss 1.0785 (1.0763)\tPrec@1 58.750 (61.978)\n",
      "EPOCH: 73 train Results: Prec@1 61.978 Loss: 1.0763\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0673 (1.0673)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3216 (1.2304)\tPrec@1 37.500 (56.100)\n",
      "EPOCH: 73 val Results: Prec@1 56.100 Loss: 1.2304\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [74][0/390]\tTime 0.004 (0.004)\tLoss 1.1425 (1.1425)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [74][78/390]\tTime 0.005 (0.003)\tLoss 1.0721 (1.0175)\tPrec@1 60.938 (64.181)\n",
      "Epoch: [74][156/390]\tTime 0.002 (0.003)\tLoss 1.0361 (1.0417)\tPrec@1 62.500 (63.346)\n",
      "Epoch: [74][234/390]\tTime 0.002 (0.003)\tLoss 1.0657 (1.0567)\tPrec@1 61.719 (62.683)\n",
      "Epoch: [74][312/390]\tTime 0.003 (0.003)\tLoss 1.0789 (1.0666)\tPrec@1 64.062 (62.255)\n",
      "Epoch: [74][390/390]\tTime 0.003 (0.003)\tLoss 1.0676 (1.0783)\tPrec@1 67.500 (61.750)\n",
      "EPOCH: 74 train Results: Prec@1 61.750 Loss: 1.0783\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1527 (1.1527)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4157 (1.2434)\tPrec@1 25.000 (55.940)\n",
      "EPOCH: 74 val Results: Prec@1 55.940 Loss: 1.2434\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [75][0/390]\tTime 0.004 (0.004)\tLoss 1.0387 (1.0387)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [75][78/390]\tTime 0.004 (0.004)\tLoss 1.1215 (1.0026)\tPrec@1 60.938 (64.478)\n",
      "Epoch: [75][156/390]\tTime 0.002 (0.003)\tLoss 0.8985 (1.0386)\tPrec@1 71.094 (63.003)\n",
      "Epoch: [75][234/390]\tTime 0.008 (0.003)\tLoss 1.2172 (1.0539)\tPrec@1 52.344 (62.606)\n",
      "Epoch: [75][312/390]\tTime 0.008 (0.003)\tLoss 1.1745 (1.0665)\tPrec@1 58.594 (62.106)\n",
      "Epoch: [75][390/390]\tTime 0.003 (0.003)\tLoss 1.1175 (1.0791)\tPrec@1 61.250 (61.742)\n",
      "EPOCH: 75 train Results: Prec@1 61.742 Loss: 1.0791\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0860 (1.0860)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5978 (1.2440)\tPrec@1 37.500 (55.600)\n",
      "EPOCH: 75 val Results: Prec@1 55.600 Loss: 1.2440\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [76][0/390]\tTime 0.009 (0.009)\tLoss 1.1240 (1.1240)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.004)\tLoss 1.1117 (1.0241)\tPrec@1 60.156 (64.211)\n",
      "Epoch: [76][156/390]\tTime 0.002 (0.004)\tLoss 0.9429 (1.0407)\tPrec@1 70.312 (63.177)\n",
      "Epoch: [76][234/390]\tTime 0.002 (0.004)\tLoss 0.9829 (1.0566)\tPrec@1 64.062 (62.547)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 0.9658 (1.0658)\tPrec@1 63.281 (62.303)\n",
      "Epoch: [76][390/390]\tTime 0.001 (0.003)\tLoss 0.8827 (1.0770)\tPrec@1 70.000 (61.852)\n",
      "EPOCH: 76 train Results: Prec@1 61.852 Loss: 1.0770\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1511 (1.1511)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3325 (1.2269)\tPrec@1 43.750 (56.130)\n",
      "EPOCH: 76 val Results: Prec@1 56.130 Loss: 1.2269\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [77][0/390]\tTime 0.002 (0.002)\tLoss 0.9932 (0.9932)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [77][78/390]\tTime 0.003 (0.003)\tLoss 1.1392 (1.0224)\tPrec@1 60.156 (64.072)\n",
      "Epoch: [77][156/390]\tTime 0.008 (0.003)\tLoss 1.0964 (1.0391)\tPrec@1 57.812 (63.241)\n",
      "Epoch: [77][234/390]\tTime 0.002 (0.003)\tLoss 0.9342 (1.0533)\tPrec@1 67.188 (62.566)\n",
      "Epoch: [77][312/390]\tTime 0.002 (0.003)\tLoss 1.1298 (1.0652)\tPrec@1 60.938 (62.071)\n",
      "Epoch: [77][390/390]\tTime 0.001 (0.003)\tLoss 1.1164 (1.0772)\tPrec@1 56.250 (61.656)\n",
      "EPOCH: 77 train Results: Prec@1 61.656 Loss: 1.0772\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0961 (1.0961)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3022 (1.2324)\tPrec@1 31.250 (56.050)\n",
      "EPOCH: 77 val Results: Prec@1 56.050 Loss: 1.2324\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [78][0/390]\tTime 0.002 (0.002)\tLoss 0.9781 (0.9781)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [78][78/390]\tTime 0.003 (0.003)\tLoss 1.0755 (0.9952)\tPrec@1 56.250 (65.002)\n",
      "Epoch: [78][156/390]\tTime 0.003 (0.003)\tLoss 1.0448 (1.0280)\tPrec@1 59.375 (63.560)\n",
      "Epoch: [78][234/390]\tTime 0.004 (0.003)\tLoss 1.0600 (1.0499)\tPrec@1 61.719 (62.799)\n",
      "Epoch: [78][312/390]\tTime 0.002 (0.003)\tLoss 1.0553 (1.0655)\tPrec@1 64.844 (62.061)\n",
      "Epoch: [78][390/390]\tTime 0.002 (0.003)\tLoss 1.0195 (1.0752)\tPrec@1 66.250 (61.658)\n",
      "EPOCH: 78 train Results: Prec@1 61.658 Loss: 1.0752\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0897 (1.0897)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0635 (1.2285)\tPrec@1 43.750 (56.120)\n",
      "EPOCH: 78 val Results: Prec@1 56.120 Loss: 1.2285\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [79][0/390]\tTime 0.006 (0.006)\tLoss 0.8830 (0.8830)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [79][78/390]\tTime 0.003 (0.004)\tLoss 1.0044 (0.9882)\tPrec@1 62.500 (65.121)\n",
      "Epoch: [79][156/390]\tTime 0.004 (0.003)\tLoss 0.9979 (1.0166)\tPrec@1 67.969 (64.137)\n",
      "Epoch: [79][234/390]\tTime 0.002 (0.003)\tLoss 1.1267 (1.0391)\tPrec@1 57.031 (63.201)\n",
      "Epoch: [79][312/390]\tTime 0.003 (0.003)\tLoss 1.1645 (1.0592)\tPrec@1 58.594 (62.480)\n",
      "Epoch: [79][390/390]\tTime 0.002 (0.003)\tLoss 1.0301 (1.0731)\tPrec@1 62.500 (61.838)\n",
      "EPOCH: 79 train Results: Prec@1 61.838 Loss: 1.0731\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1066 (1.1066)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2858 (1.2298)\tPrec@1 37.500 (55.960)\n",
      "EPOCH: 79 val Results: Prec@1 55.960 Loss: 1.2298\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [80][0/390]\tTime 0.003 (0.003)\tLoss 1.0215 (1.0215)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [80][78/390]\tTime 0.004 (0.003)\tLoss 1.0354 (0.9988)\tPrec@1 62.500 (65.427)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.1419 (1.0302)\tPrec@1 57.812 (63.530)\n",
      "Epoch: [80][234/390]\tTime 0.002 (0.003)\tLoss 1.0152 (1.0519)\tPrec@1 63.281 (62.869)\n",
      "Epoch: [80][312/390]\tTime 0.005 (0.003)\tLoss 1.3171 (1.0663)\tPrec@1 57.031 (62.293)\n",
      "Epoch: [80][390/390]\tTime 0.001 (0.003)\tLoss 1.1856 (1.0736)\tPrec@1 53.750 (61.926)\n",
      "EPOCH: 80 train Results: Prec@1 61.926 Loss: 1.0736\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1255 (1.1255)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4424 (1.2456)\tPrec@1 31.250 (55.850)\n",
      "EPOCH: 80 val Results: Prec@1 55.850 Loss: 1.2456\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [81][0/390]\tTime 0.002 (0.002)\tLoss 1.0009 (1.0009)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [81][78/390]\tTime 0.005 (0.003)\tLoss 1.0580 (1.0021)\tPrec@1 62.500 (64.676)\n",
      "Epoch: [81][156/390]\tTime 0.003 (0.003)\tLoss 0.9516 (1.0318)\tPrec@1 66.406 (63.510)\n",
      "Epoch: [81][234/390]\tTime 0.002 (0.003)\tLoss 1.0469 (1.0445)\tPrec@1 64.844 (62.985)\n",
      "Epoch: [81][312/390]\tTime 0.002 (0.003)\tLoss 1.3557 (1.0619)\tPrec@1 50.781 (62.385)\n",
      "Epoch: [81][390/390]\tTime 0.001 (0.003)\tLoss 1.0627 (1.0731)\tPrec@1 58.750 (61.978)\n",
      "EPOCH: 81 train Results: Prec@1 61.978 Loss: 1.0731\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1539 (1.1539)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1550 (1.2378)\tPrec@1 37.500 (56.060)\n",
      "EPOCH: 81 val Results: Prec@1 56.060 Loss: 1.2378\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [82][0/390]\tTime 0.006 (0.006)\tLoss 1.0540 (1.0540)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [82][78/390]\tTime 0.003 (0.003)\tLoss 0.8941 (1.0125)\tPrec@1 71.094 (64.181)\n",
      "Epoch: [82][156/390]\tTime 0.002 (0.003)\tLoss 1.1340 (1.0284)\tPrec@1 57.031 (63.356)\n",
      "Epoch: [82][234/390]\tTime 0.002 (0.003)\tLoss 1.1063 (1.0463)\tPrec@1 62.500 (62.709)\n",
      "Epoch: [82][312/390]\tTime 0.002 (0.003)\tLoss 1.0560 (1.0541)\tPrec@1 64.062 (62.465)\n",
      "Epoch: [82][390/390]\tTime 0.003 (0.003)\tLoss 1.0658 (1.0724)\tPrec@1 57.500 (61.862)\n",
      "EPOCH: 82 train Results: Prec@1 61.862 Loss: 1.0724\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1099 (1.1099)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1784 (1.2226)\tPrec@1 50.000 (56.390)\n",
      "EPOCH: 82 val Results: Prec@1 56.390 Loss: 1.2226\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [83][0/390]\tTime 0.004 (0.004)\tLoss 0.9328 (0.9328)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [83][78/390]\tTime 0.003 (0.003)\tLoss 1.2311 (1.0084)\tPrec@1 53.906 (64.686)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.1721 (1.0378)\tPrec@1 53.906 (63.361)\n",
      "Epoch: [83][234/390]\tTime 0.002 (0.003)\tLoss 0.9850 (1.0538)\tPrec@1 64.844 (62.736)\n",
      "Epoch: [83][312/390]\tTime 0.003 (0.003)\tLoss 1.1960 (1.0686)\tPrec@1 59.375 (62.268)\n",
      "Epoch: [83][390/390]\tTime 0.002 (0.003)\tLoss 1.1405 (1.0752)\tPrec@1 63.750 (61.874)\n",
      "EPOCH: 83 train Results: Prec@1 61.874 Loss: 1.0752\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1003 (1.1003)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1726 (1.2335)\tPrec@1 31.250 (56.020)\n",
      "EPOCH: 83 val Results: Prec@1 56.020 Loss: 1.2335\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [84][0/390]\tTime 0.003 (0.003)\tLoss 0.7884 (0.7884)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [84][78/390]\tTime 0.003 (0.003)\tLoss 0.8443 (1.0069)\tPrec@1 71.094 (64.913)\n",
      "Epoch: [84][156/390]\tTime 0.003 (0.003)\tLoss 1.0472 (1.0295)\tPrec@1 62.500 (63.729)\n",
      "Epoch: [84][234/390]\tTime 0.002 (0.003)\tLoss 1.0556 (1.0526)\tPrec@1 61.719 (62.856)\n",
      "Epoch: [84][312/390]\tTime 0.003 (0.003)\tLoss 1.0063 (1.0617)\tPrec@1 65.625 (62.453)\n",
      "Epoch: [84][390/390]\tTime 0.001 (0.003)\tLoss 1.3239 (1.0705)\tPrec@1 55.000 (62.150)\n",
      "EPOCH: 84 train Results: Prec@1 62.150 Loss: 1.0705\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0244 (1.0244)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1001 (1.2431)\tPrec@1 56.250 (56.010)\n",
      "EPOCH: 84 val Results: Prec@1 56.010 Loss: 1.2431\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [85][0/390]\tTime 0.002 (0.002)\tLoss 0.8598 (0.8598)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [85][78/390]\tTime 0.002 (0.004)\tLoss 0.9279 (1.0058)\tPrec@1 73.438 (64.943)\n",
      "Epoch: [85][156/390]\tTime 0.010 (0.004)\tLoss 0.9076 (1.0282)\tPrec@1 67.969 (63.913)\n",
      "Epoch: [85][234/390]\tTime 0.002 (0.004)\tLoss 1.2712 (1.0465)\tPrec@1 53.125 (63.165)\n",
      "Epoch: [85][312/390]\tTime 0.002 (0.004)\tLoss 1.0796 (1.0608)\tPrec@1 60.938 (62.617)\n",
      "Epoch: [85][390/390]\tTime 0.004 (0.003)\tLoss 1.1236 (1.0718)\tPrec@1 56.250 (62.006)\n",
      "EPOCH: 85 train Results: Prec@1 62.006 Loss: 1.0718\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0547 (1.0547)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5419 (1.2384)\tPrec@1 37.500 (55.470)\n",
      "EPOCH: 85 val Results: Prec@1 55.470 Loss: 1.2384\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [86][0/390]\tTime 0.002 (0.002)\tLoss 1.0032 (1.0032)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [86][78/390]\tTime 0.005 (0.003)\tLoss 1.0115 (1.0246)\tPrec@1 62.500 (63.657)\n",
      "Epoch: [86][156/390]\tTime 0.005 (0.003)\tLoss 0.9998 (1.0331)\tPrec@1 63.281 (63.326)\n",
      "Epoch: [86][234/390]\tTime 0.002 (0.003)\tLoss 1.1173 (1.0461)\tPrec@1 59.375 (63.032)\n",
      "Epoch: [86][312/390]\tTime 0.003 (0.003)\tLoss 1.2314 (1.0592)\tPrec@1 58.594 (62.512)\n",
      "Epoch: [86][390/390]\tTime 0.002 (0.003)\tLoss 1.1657 (1.0726)\tPrec@1 60.000 (61.986)\n",
      "EPOCH: 86 train Results: Prec@1 61.986 Loss: 1.0726\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1119 (1.1119)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2099 (1.2300)\tPrec@1 37.500 (55.850)\n",
      "EPOCH: 86 val Results: Prec@1 55.850 Loss: 1.2300\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [87][0/390]\tTime 0.002 (0.002)\tLoss 0.9718 (0.9718)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [87][78/390]\tTime 0.002 (0.003)\tLoss 0.9153 (0.9857)\tPrec@1 70.312 (65.239)\n",
      "Epoch: [87][156/390]\tTime 0.002 (0.003)\tLoss 1.1287 (1.0249)\tPrec@1 57.031 (63.669)\n",
      "Epoch: [87][234/390]\tTime 0.002 (0.003)\tLoss 1.0888 (1.0422)\tPrec@1 64.844 (62.936)\n",
      "Epoch: [87][312/390]\tTime 0.004 (0.003)\tLoss 1.2561 (1.0581)\tPrec@1 58.594 (62.542)\n",
      "Epoch: [87][390/390]\tTime 0.002 (0.003)\tLoss 1.2518 (1.0674)\tPrec@1 51.250 (62.178)\n",
      "EPOCH: 87 train Results: Prec@1 62.178 Loss: 1.0674\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0561 (1.0561)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0519 (1.2359)\tPrec@1 68.750 (56.340)\n",
      "EPOCH: 87 val Results: Prec@1 56.340 Loss: 1.2359\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [88][0/390]\tTime 0.002 (0.002)\tLoss 0.9976 (0.9976)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [88][78/390]\tTime 0.002 (0.003)\tLoss 0.8443 (0.9864)\tPrec@1 70.312 (65.783)\n",
      "Epoch: [88][156/390]\tTime 0.003 (0.003)\tLoss 1.1437 (1.0294)\tPrec@1 55.469 (63.844)\n",
      "Epoch: [88][234/390]\tTime 0.007 (0.003)\tLoss 1.0932 (1.0490)\tPrec@1 56.250 (62.959)\n",
      "Epoch: [88][312/390]\tTime 0.002 (0.003)\tLoss 1.0661 (1.0636)\tPrec@1 59.375 (62.450)\n",
      "Epoch: [88][390/390]\tTime 0.001 (0.003)\tLoss 1.3547 (1.0736)\tPrec@1 53.750 (62.052)\n",
      "EPOCH: 88 train Results: Prec@1 62.052 Loss: 1.0736\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1688 (1.1688)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0231 (1.2443)\tPrec@1 56.250 (56.230)\n",
      "EPOCH: 88 val Results: Prec@1 56.230 Loss: 1.2443\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [89][0/390]\tTime 0.004 (0.004)\tLoss 0.9632 (0.9632)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.003)\tLoss 1.0243 (1.0078)\tPrec@1 70.312 (64.458)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1083 (1.0242)\tPrec@1 60.938 (63.819)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 1.1079 (1.0459)\tPrec@1 62.500 (63.112)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.1021 (1.0631)\tPrec@1 63.281 (62.413)\n",
      "Epoch: [89][390/390]\tTime 0.003 (0.003)\tLoss 1.1967 (1.0739)\tPrec@1 57.500 (61.998)\n",
      "EPOCH: 89 train Results: Prec@1 61.998 Loss: 1.0739\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1094 (1.1094)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4829 (1.2416)\tPrec@1 50.000 (56.440)\n",
      "EPOCH: 89 val Results: Prec@1 56.440 Loss: 1.2416\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [90][0/390]\tTime 0.002 (0.002)\tLoss 1.0691 (1.0691)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [90][78/390]\tTime 0.002 (0.003)\tLoss 0.9765 (0.9965)\tPrec@1 64.062 (64.597)\n",
      "Epoch: [90][156/390]\tTime 0.004 (0.003)\tLoss 1.1726 (1.0235)\tPrec@1 54.688 (63.849)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.0814 (1.0453)\tPrec@1 61.719 (62.879)\n",
      "Epoch: [90][312/390]\tTime 0.009 (0.003)\tLoss 1.1460 (1.0551)\tPrec@1 59.375 (62.507)\n",
      "Epoch: [90][390/390]\tTime 0.003 (0.003)\tLoss 0.9204 (1.0689)\tPrec@1 67.500 (62.004)\n",
      "EPOCH: 90 train Results: Prec@1 62.004 Loss: 1.0689\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0234 (1.0234)\tPrec@1 71.094 (71.094)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5110 (1.2436)\tPrec@1 43.750 (55.430)\n",
      "EPOCH: 90 val Results: Prec@1 55.430 Loss: 1.2436\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [91][0/390]\tTime 0.003 (0.003)\tLoss 1.1458 (1.1458)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [91][78/390]\tTime 0.002 (0.003)\tLoss 0.9902 (1.0040)\tPrec@1 65.625 (64.992)\n",
      "Epoch: [91][156/390]\tTime 0.002 (0.003)\tLoss 1.1741 (1.0257)\tPrec@1 61.719 (63.893)\n",
      "Epoch: [91][234/390]\tTime 0.003 (0.003)\tLoss 1.1067 (1.0402)\tPrec@1 57.812 (63.285)\n",
      "Epoch: [91][312/390]\tTime 0.004 (0.003)\tLoss 1.1104 (1.0579)\tPrec@1 60.938 (62.627)\n",
      "Epoch: [91][390/390]\tTime 0.002 (0.003)\tLoss 1.0359 (1.0701)\tPrec@1 58.750 (62.098)\n",
      "EPOCH: 91 train Results: Prec@1 62.098 Loss: 1.0701\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1244 (1.1244)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2693 (1.2356)\tPrec@1 50.000 (55.930)\n",
      "EPOCH: 91 val Results: Prec@1 55.930 Loss: 1.2356\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [92][0/390]\tTime 0.002 (0.002)\tLoss 1.0518 (1.0518)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.003)\tLoss 1.0130 (1.0031)\tPrec@1 64.062 (64.201)\n",
      "Epoch: [92][156/390]\tTime 0.003 (0.003)\tLoss 1.0565 (1.0328)\tPrec@1 65.625 (63.361)\n",
      "Epoch: [92][234/390]\tTime 0.008 (0.003)\tLoss 1.0779 (1.0465)\tPrec@1 61.719 (62.723)\n",
      "Epoch: [92][312/390]\tTime 0.004 (0.003)\tLoss 1.1319 (1.0616)\tPrec@1 56.250 (62.081)\n",
      "Epoch: [92][390/390]\tTime 0.002 (0.003)\tLoss 1.0879 (1.0732)\tPrec@1 58.750 (61.662)\n",
      "EPOCH: 92 train Results: Prec@1 61.662 Loss: 1.0732\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0657 (1.0657)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3797 (1.2356)\tPrec@1 50.000 (56.170)\n",
      "EPOCH: 92 val Results: Prec@1 56.170 Loss: 1.2356\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 0.9675 (0.9675)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.003)\tLoss 0.9313 (0.9843)\tPrec@1 66.406 (65.131)\n",
      "Epoch: [93][156/390]\tTime 0.004 (0.003)\tLoss 1.0777 (1.0152)\tPrec@1 60.938 (64.167)\n",
      "Epoch: [93][234/390]\tTime 0.002 (0.003)\tLoss 1.1613 (1.0360)\tPrec@1 60.156 (63.308)\n",
      "Epoch: [93][312/390]\tTime 0.003 (0.003)\tLoss 1.2600 (1.0530)\tPrec@1 55.469 (62.527)\n",
      "Epoch: [93][390/390]\tTime 0.027 (0.004)\tLoss 0.9846 (1.0665)\tPrec@1 62.500 (62.060)\n",
      "EPOCH: 93 train Results: Prec@1 62.060 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1010 (1.1010)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5213 (1.2422)\tPrec@1 37.500 (56.070)\n",
      "EPOCH: 93 val Results: Prec@1 56.070 Loss: 1.2422\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [94][0/390]\tTime 0.002 (0.002)\tLoss 0.9380 (0.9380)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [94][78/390]\tTime 0.004 (0.003)\tLoss 0.9810 (0.9945)\tPrec@1 65.625 (65.042)\n",
      "Epoch: [94][156/390]\tTime 0.005 (0.003)\tLoss 1.0051 (1.0205)\tPrec@1 64.844 (63.873)\n",
      "Epoch: [94][234/390]\tTime 0.006 (0.003)\tLoss 0.9233 (1.0451)\tPrec@1 65.625 (62.972)\n",
      "Epoch: [94][312/390]\tTime 0.006 (0.003)\tLoss 1.1802 (1.0588)\tPrec@1 56.250 (62.410)\n",
      "Epoch: [94][390/390]\tTime 0.005 (0.003)\tLoss 1.4465 (1.0726)\tPrec@1 56.250 (61.934)\n",
      "EPOCH: 94 train Results: Prec@1 61.934 Loss: 1.0726\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1701 (1.1701)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4250 (1.2380)\tPrec@1 43.750 (56.030)\n",
      "EPOCH: 94 val Results: Prec@1 56.030 Loss: 1.2380\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [95][0/390]\tTime 0.004 (0.004)\tLoss 1.0202 (1.0202)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [95][78/390]\tTime 0.002 (0.003)\tLoss 1.0994 (1.0004)\tPrec@1 64.844 (64.962)\n",
      "Epoch: [95][156/390]\tTime 0.002 (0.003)\tLoss 0.9846 (1.0191)\tPrec@1 64.062 (64.157)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.003)\tLoss 1.1684 (1.0396)\tPrec@1 58.594 (63.198)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.003)\tLoss 1.0937 (1.0552)\tPrec@1 54.688 (62.587)\n",
      "Epoch: [95][390/390]\tTime 0.002 (0.003)\tLoss 1.1727 (1.0690)\tPrec@1 62.500 (62.140)\n",
      "EPOCH: 95 train Results: Prec@1 62.140 Loss: 1.0690\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0476 (1.0476)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3418 (1.2477)\tPrec@1 43.750 (55.890)\n",
      "EPOCH: 95 val Results: Prec@1 55.890 Loss: 1.2477\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [96][0/390]\tTime 0.002 (0.002)\tLoss 1.0343 (1.0343)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.003)\tLoss 1.1241 (0.9985)\tPrec@1 55.469 (64.656)\n",
      "Epoch: [96][156/390]\tTime 0.002 (0.003)\tLoss 0.9821 (1.0231)\tPrec@1 69.531 (63.659)\n",
      "Epoch: [96][234/390]\tTime 0.002 (0.003)\tLoss 1.0412 (1.0462)\tPrec@1 67.188 (62.889)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 1.0121 (1.0551)\tPrec@1 65.625 (62.478)\n",
      "Epoch: [96][390/390]\tTime 0.002 (0.003)\tLoss 1.1976 (1.0690)\tPrec@1 58.750 (61.964)\n",
      "EPOCH: 96 train Results: Prec@1 61.964 Loss: 1.0690\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1595 (1.1595)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9926 (1.2406)\tPrec@1 62.500 (55.740)\n",
      "EPOCH: 96 val Results: Prec@1 55.740 Loss: 1.2406\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [97][0/390]\tTime 0.002 (0.002)\tLoss 0.9961 (0.9961)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [97][78/390]\tTime 0.004 (0.003)\tLoss 0.9661 (0.9961)\tPrec@1 64.062 (64.804)\n",
      "Epoch: [97][156/390]\tTime 0.002 (0.003)\tLoss 1.1600 (1.0231)\tPrec@1 58.594 (63.809)\n",
      "Epoch: [97][234/390]\tTime 0.008 (0.003)\tLoss 1.2710 (1.0378)\tPrec@1 57.031 (63.238)\n",
      "Epoch: [97][312/390]\tTime 0.002 (0.003)\tLoss 1.1476 (1.0522)\tPrec@1 64.844 (62.640)\n",
      "Epoch: [97][390/390]\tTime 0.002 (0.003)\tLoss 0.9369 (1.0658)\tPrec@1 72.500 (62.064)\n",
      "EPOCH: 97 train Results: Prec@1 62.064 Loss: 1.0658\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0652 (1.0652)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.2427 (1.2365)\tPrec@1 43.750 (56.650)\n",
      "EPOCH: 97 val Results: Prec@1 56.650 Loss: 1.2365\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [98][0/390]\tTime 0.004 (0.004)\tLoss 1.0234 (1.0234)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [98][78/390]\tTime 0.003 (0.003)\tLoss 0.9024 (0.9933)\tPrec@1 67.969 (65.032)\n",
      "Epoch: [98][156/390]\tTime 0.002 (0.003)\tLoss 1.2301 (1.0199)\tPrec@1 60.156 (64.152)\n",
      "Epoch: [98][234/390]\tTime 0.002 (0.003)\tLoss 0.9610 (1.0364)\tPrec@1 64.844 (63.368)\n",
      "Epoch: [98][312/390]\tTime 0.002 (0.003)\tLoss 1.1810 (1.0526)\tPrec@1 55.469 (62.712)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.003)\tLoss 1.0507 (1.0643)\tPrec@1 66.250 (62.358)\n",
      "EPOCH: 98 train Results: Prec@1 62.358 Loss: 1.0643\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1457 (1.1457)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2746 (1.2463)\tPrec@1 50.000 (55.600)\n",
      "EPOCH: 98 val Results: Prec@1 55.600 Loss: 1.2463\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [99][0/390]\tTime 0.007 (0.007)\tLoss 1.0034 (1.0034)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.003)\tLoss 0.8944 (1.0087)\tPrec@1 71.875 (64.557)\n",
      "Epoch: [99][156/390]\tTime 0.003 (0.003)\tLoss 1.0947 (1.0343)\tPrec@1 62.500 (63.605)\n",
      "Epoch: [99][234/390]\tTime 0.002 (0.003)\tLoss 1.3753 (1.0452)\tPrec@1 50.781 (63.029)\n",
      "Epoch: [99][312/390]\tTime 0.006 (0.003)\tLoss 1.0716 (1.0574)\tPrec@1 63.281 (62.607)\n",
      "Epoch: [99][390/390]\tTime 0.001 (0.003)\tLoss 0.9349 (1.0665)\tPrec@1 63.750 (62.212)\n",
      "EPOCH: 99 train Results: Prec@1 62.212 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1006 (1.1006)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5218 (1.2456)\tPrec@1 37.500 (55.680)\n",
      "EPOCH: 99 val Results: Prec@1 55.680 Loss: 1.2456\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [100][0/390]\tTime 0.002 (0.002)\tLoss 0.9029 (0.9029)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [100][78/390]\tTime 0.008 (0.003)\tLoss 1.1736 (1.0041)\tPrec@1 60.938 (64.399)\n",
      "Epoch: [100][156/390]\tTime 0.002 (0.003)\tLoss 0.9884 (1.0244)\tPrec@1 64.062 (63.659)\n",
      "Epoch: [100][234/390]\tTime 0.005 (0.003)\tLoss 1.1698 (1.0431)\tPrec@1 54.688 (62.939)\n",
      "Epoch: [100][312/390]\tTime 0.003 (0.003)\tLoss 1.0912 (1.0602)\tPrec@1 60.938 (62.343)\n",
      "Epoch: [100][390/390]\tTime 0.001 (0.003)\tLoss 1.2717 (1.0698)\tPrec@1 56.250 (62.114)\n",
      "EPOCH: 100 train Results: Prec@1 62.114 Loss: 1.0698\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1236 (1.1236)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4116 (1.2382)\tPrec@1 25.000 (55.930)\n",
      "EPOCH: 100 val Results: Prec@1 55.930 Loss: 1.2382\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [101][0/390]\tTime 0.002 (0.002)\tLoss 0.9714 (0.9714)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.003)\tLoss 1.0562 (1.0028)\tPrec@1 62.500 (64.725)\n",
      "Epoch: [101][156/390]\tTime 0.002 (0.003)\tLoss 1.1385 (1.0264)\tPrec@1 59.375 (63.754)\n",
      "Epoch: [101][234/390]\tTime 0.003 (0.003)\tLoss 1.0788 (1.0432)\tPrec@1 60.156 (62.952)\n",
      "Epoch: [101][312/390]\tTime 0.002 (0.003)\tLoss 1.0254 (1.0504)\tPrec@1 63.281 (62.652)\n",
      "Epoch: [101][390/390]\tTime 0.005 (0.003)\tLoss 1.2320 (1.0636)\tPrec@1 56.250 (62.218)\n",
      "EPOCH: 101 train Results: Prec@1 62.218 Loss: 1.0636\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1440 (1.1440)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2290 (1.2465)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 101 val Results: Prec@1 55.300 Loss: 1.2465\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [102][0/390]\tTime 0.002 (0.002)\tLoss 0.9416 (0.9416)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [102][78/390]\tTime 0.002 (0.003)\tLoss 1.0704 (0.9804)\tPrec@1 60.938 (65.477)\n",
      "Epoch: [102][156/390]\tTime 0.002 (0.003)\tLoss 1.0360 (1.0177)\tPrec@1 63.281 (64.058)\n",
      "Epoch: [102][234/390]\tTime 0.007 (0.003)\tLoss 1.1863 (1.0409)\tPrec@1 59.375 (63.085)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.003)\tLoss 1.0747 (1.0545)\tPrec@1 67.188 (62.625)\n",
      "Epoch: [102][390/390]\tTime 0.003 (0.003)\tLoss 1.0379 (1.0645)\tPrec@1 67.500 (62.230)\n",
      "EPOCH: 102 train Results: Prec@1 62.230 Loss: 1.0645\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1221 (1.1221)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3839 (1.2505)\tPrec@1 56.250 (55.650)\n",
      "EPOCH: 102 val Results: Prec@1 55.650 Loss: 1.2505\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [103][0/390]\tTime 0.002 (0.002)\tLoss 0.9924 (0.9924)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [103][78/390]\tTime 0.002 (0.003)\tLoss 1.0441 (1.0105)\tPrec@1 66.406 (63.973)\n",
      "Epoch: [103][156/390]\tTime 0.003 (0.003)\tLoss 1.0423 (1.0252)\tPrec@1 60.156 (63.500)\n",
      "Epoch: [103][234/390]\tTime 0.003 (0.003)\tLoss 1.1826 (1.0419)\tPrec@1 58.594 (62.896)\n",
      "Epoch: [103][312/390]\tTime 0.002 (0.003)\tLoss 1.1998 (1.0533)\tPrec@1 57.812 (62.560)\n",
      "Epoch: [103][390/390]\tTime 0.003 (0.003)\tLoss 1.2175 (1.0629)\tPrec@1 65.000 (62.258)\n",
      "EPOCH: 103 train Results: Prec@1 62.258 Loss: 1.0629\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1297 (1.1297)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3953 (1.2553)\tPrec@1 56.250 (55.360)\n",
      "EPOCH: 103 val Results: Prec@1 55.360 Loss: 1.2553\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [104][0/390]\tTime 0.003 (0.003)\tLoss 0.9585 (0.9585)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 0.9698 (0.9917)\tPrec@1 66.406 (64.893)\n",
      "Epoch: [104][156/390]\tTime 0.005 (0.003)\tLoss 1.0079 (1.0196)\tPrec@1 64.062 (63.694)\n",
      "Epoch: [104][234/390]\tTime 0.002 (0.003)\tLoss 0.9436 (1.0365)\tPrec@1 65.625 (63.135)\n",
      "Epoch: [104][312/390]\tTime 0.002 (0.003)\tLoss 0.9249 (1.0542)\tPrec@1 70.312 (62.485)\n",
      "Epoch: [104][390/390]\tTime 0.002 (0.003)\tLoss 1.0010 (1.0634)\tPrec@1 63.750 (62.182)\n",
      "EPOCH: 104 train Results: Prec@1 62.182 Loss: 1.0634\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1226 (1.1226)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2282 (1.2421)\tPrec@1 43.750 (55.670)\n",
      "EPOCH: 104 val Results: Prec@1 55.670 Loss: 1.2421\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [105][0/390]\tTime 0.002 (0.002)\tLoss 0.9050 (0.9050)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [105][78/390]\tTime 0.004 (0.003)\tLoss 1.0170 (0.9970)\tPrec@1 59.375 (64.923)\n",
      "Epoch: [105][156/390]\tTime 0.003 (0.003)\tLoss 1.1799 (1.0192)\tPrec@1 57.031 (64.092)\n",
      "Epoch: [105][234/390]\tTime 0.003 (0.003)\tLoss 1.2517 (1.0339)\tPrec@1 57.812 (63.517)\n",
      "Epoch: [105][312/390]\tTime 0.002 (0.003)\tLoss 1.1661 (1.0468)\tPrec@1 62.500 (62.999)\n",
      "Epoch: [105][390/390]\tTime 0.003 (0.003)\tLoss 1.0781 (1.0616)\tPrec@1 62.500 (62.504)\n",
      "EPOCH: 105 train Results: Prec@1 62.504 Loss: 1.0616\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1137 (1.1137)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4205 (1.2446)\tPrec@1 43.750 (55.910)\n",
      "EPOCH: 105 val Results: Prec@1 55.910 Loss: 1.2446\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [106][0/390]\tTime 0.003 (0.003)\tLoss 0.9277 (0.9277)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [106][78/390]\tTime 0.002 (0.003)\tLoss 0.8805 (0.9828)\tPrec@1 67.188 (65.526)\n",
      "Epoch: [106][156/390]\tTime 0.002 (0.003)\tLoss 1.1310 (1.0149)\tPrec@1 64.062 (64.142)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.003)\tLoss 1.0324 (1.0370)\tPrec@1 63.281 (63.145)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.003)\tLoss 1.0201 (1.0533)\tPrec@1 65.625 (62.510)\n",
      "Epoch: [106][390/390]\tTime 0.002 (0.003)\tLoss 1.0652 (1.0634)\tPrec@1 71.250 (62.118)\n",
      "EPOCH: 106 train Results: Prec@1 62.118 Loss: 1.0634\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0977 (1.0977)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3285 (1.2433)\tPrec@1 56.250 (55.910)\n",
      "EPOCH: 106 val Results: Prec@1 55.910 Loss: 1.2433\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [107][0/390]\tTime 0.003 (0.003)\tLoss 0.8380 (0.8380)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [107][78/390]\tTime 0.002 (0.003)\tLoss 1.0704 (0.9894)\tPrec@1 64.844 (65.042)\n",
      "Epoch: [107][156/390]\tTime 0.002 (0.004)\tLoss 1.2216 (1.0141)\tPrec@1 57.031 (64.072)\n",
      "Epoch: [107][234/390]\tTime 0.063 (0.004)\tLoss 1.1562 (1.0305)\tPrec@1 55.469 (63.461)\n",
      "Epoch: [107][312/390]\tTime 0.002 (0.003)\tLoss 1.1094 (1.0509)\tPrec@1 62.500 (62.742)\n",
      "Epoch: [107][390/390]\tTime 0.003 (0.004)\tLoss 0.9462 (1.0665)\tPrec@1 63.750 (62.120)\n",
      "EPOCH: 107 train Results: Prec@1 62.120 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1234 (1.1234)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4880 (1.2456)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 107 val Results: Prec@1 55.370 Loss: 1.2456\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [108][0/390]\tTime 0.002 (0.002)\tLoss 1.0178 (1.0178)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [108][78/390]\tTime 0.003 (0.004)\tLoss 1.1324 (0.9808)\tPrec@1 61.719 (65.051)\n",
      "Epoch: [108][156/390]\tTime 0.002 (0.004)\tLoss 1.0524 (1.0135)\tPrec@1 64.844 (64.162)\n",
      "Epoch: [108][234/390]\tTime 0.002 (0.004)\tLoss 1.1259 (1.0340)\tPrec@1 60.156 (63.295)\n",
      "Epoch: [108][312/390]\tTime 0.005 (0.004)\tLoss 1.0437 (1.0469)\tPrec@1 63.281 (62.672)\n",
      "Epoch: [108][390/390]\tTime 0.002 (0.004)\tLoss 1.1297 (1.0599)\tPrec@1 58.750 (62.226)\n",
      "EPOCH: 108 train Results: Prec@1 62.226 Loss: 1.0599\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1188 (1.1188)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4841 (1.2510)\tPrec@1 43.750 (54.750)\n",
      "EPOCH: 108 val Results: Prec@1 54.750 Loss: 1.2510\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [109][0/390]\tTime 0.003 (0.003)\tLoss 1.0805 (1.0805)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [109][78/390]\tTime 0.004 (0.003)\tLoss 1.0636 (0.9981)\tPrec@1 64.844 (64.775)\n",
      "Epoch: [109][156/390]\tTime 0.003 (0.003)\tLoss 1.1074 (1.0269)\tPrec@1 60.156 (63.620)\n",
      "Epoch: [109][234/390]\tTime 0.004 (0.003)\tLoss 0.9154 (1.0415)\tPrec@1 67.188 (62.959)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.003)\tLoss 1.1608 (1.0531)\tPrec@1 51.562 (62.512)\n",
      "Epoch: [109][390/390]\tTime 0.002 (0.003)\tLoss 1.1821 (1.0607)\tPrec@1 58.750 (62.216)\n",
      "EPOCH: 109 train Results: Prec@1 62.216 Loss: 1.0607\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0958 (1.0958)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2776 (1.2323)\tPrec@1 68.750 (56.310)\n",
      "EPOCH: 109 val Results: Prec@1 56.310 Loss: 1.2323\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [110][0/390]\tTime 0.005 (0.005)\tLoss 1.0152 (1.0152)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 1.0749 (0.9839)\tPrec@1 61.719 (65.042)\n",
      "Epoch: [110][156/390]\tTime 0.002 (0.003)\tLoss 0.9514 (1.0140)\tPrec@1 71.875 (64.023)\n",
      "Epoch: [110][234/390]\tTime 0.004 (0.003)\tLoss 1.1555 (1.0283)\tPrec@1 60.156 (63.614)\n",
      "Epoch: [110][312/390]\tTime 0.002 (0.003)\tLoss 1.0881 (1.0489)\tPrec@1 67.188 (62.844)\n",
      "Epoch: [110][390/390]\tTime 0.001 (0.003)\tLoss 1.2480 (1.0567)\tPrec@1 56.250 (62.424)\n",
      "EPOCH: 110 train Results: Prec@1 62.424 Loss: 1.0567\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0659 (1.0659)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4452 (1.2331)\tPrec@1 43.750 (56.200)\n",
      "EPOCH: 110 val Results: Prec@1 56.200 Loss: 1.2331\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [111][0/390]\tTime 0.002 (0.002)\tLoss 0.9671 (0.9671)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [111][78/390]\tTime 0.005 (0.004)\tLoss 1.0009 (0.9900)\tPrec@1 66.406 (64.389)\n",
      "Epoch: [111][156/390]\tTime 0.055 (0.004)\tLoss 1.0119 (1.0214)\tPrec@1 58.594 (63.241)\n",
      "Epoch: [111][234/390]\tTime 0.003 (0.004)\tLoss 1.2277 (1.0437)\tPrec@1 57.812 (62.776)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.003)\tLoss 1.0830 (1.0562)\tPrec@1 65.625 (62.400)\n",
      "Epoch: [111][390/390]\tTime 0.001 (0.003)\tLoss 1.1222 (1.0662)\tPrec@1 63.750 (62.042)\n",
      "EPOCH: 111 train Results: Prec@1 62.042 Loss: 1.0662\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1375 (1.1375)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2907 (1.2502)\tPrec@1 50.000 (55.630)\n",
      "EPOCH: 111 val Results: Prec@1 55.630 Loss: 1.2502\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [112][0/390]\tTime 0.005 (0.005)\tLoss 0.9740 (0.9740)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.1124 (0.9887)\tPrec@1 60.156 (65.051)\n",
      "Epoch: [112][156/390]\tTime 0.004 (0.003)\tLoss 1.0977 (1.0139)\tPrec@1 64.062 (64.092)\n",
      "Epoch: [112][234/390]\tTime 0.002 (0.003)\tLoss 1.1656 (1.0295)\tPrec@1 57.812 (63.551)\n",
      "Epoch: [112][312/390]\tTime 0.003 (0.003)\tLoss 1.0951 (1.0524)\tPrec@1 62.500 (62.707)\n",
      "Epoch: [112][390/390]\tTime 0.004 (0.003)\tLoss 1.2321 (1.0601)\tPrec@1 52.500 (62.440)\n",
      "EPOCH: 112 train Results: Prec@1 62.440 Loss: 1.0601\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0884 (1.0884)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1358 (1.2429)\tPrec@1 50.000 (55.860)\n",
      "EPOCH: 112 val Results: Prec@1 55.860 Loss: 1.2429\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 1.0080 (1.0080)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [113][78/390]\tTime 0.002 (0.003)\tLoss 1.0585 (0.9902)\tPrec@1 60.156 (65.635)\n",
      "Epoch: [113][156/390]\tTime 0.003 (0.003)\tLoss 0.9940 (1.0150)\tPrec@1 64.844 (64.421)\n",
      "Epoch: [113][234/390]\tTime 0.004 (0.003)\tLoss 0.9539 (1.0334)\tPrec@1 65.625 (63.707)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.003)\tLoss 1.0174 (1.0466)\tPrec@1 65.625 (63.159)\n",
      "Epoch: [113][390/390]\tTime 0.002 (0.003)\tLoss 1.2093 (1.0610)\tPrec@1 60.000 (62.588)\n",
      "EPOCH: 113 train Results: Prec@1 62.588 Loss: 1.0610\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1081 (1.1081)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2380 (1.2330)\tPrec@1 56.250 (56.080)\n",
      "EPOCH: 113 val Results: Prec@1 56.080 Loss: 1.2330\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [114][0/390]\tTime 0.006 (0.006)\tLoss 1.0444 (1.0444)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.003)\tLoss 0.8303 (0.9943)\tPrec@1 72.656 (65.121)\n",
      "Epoch: [114][156/390]\tTime 0.002 (0.003)\tLoss 1.0545 (1.0177)\tPrec@1 62.500 (64.192)\n",
      "Epoch: [114][234/390]\tTime 0.003 (0.003)\tLoss 1.1229 (1.0379)\tPrec@1 60.938 (63.381)\n",
      "Epoch: [114][312/390]\tTime 0.003 (0.003)\tLoss 0.9619 (1.0468)\tPrec@1 68.750 (63.007)\n",
      "Epoch: [114][390/390]\tTime 0.003 (0.003)\tLoss 1.2093 (1.0581)\tPrec@1 60.000 (62.544)\n",
      "EPOCH: 114 train Results: Prec@1 62.544 Loss: 1.0581\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0983 (1.0983)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4331 (1.2490)\tPrec@1 43.750 (55.530)\n",
      "EPOCH: 114 val Results: Prec@1 55.530 Loss: 1.2490\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [115][0/390]\tTime 0.003 (0.003)\tLoss 0.8017 (0.8017)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [115][78/390]\tTime 0.002 (0.003)\tLoss 0.9386 (0.9937)\tPrec@1 71.875 (64.636)\n",
      "Epoch: [115][156/390]\tTime 0.003 (0.003)\tLoss 1.1182 (1.0073)\tPrec@1 59.375 (63.953)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.003)\tLoss 1.1019 (1.0295)\tPrec@1 57.812 (63.398)\n",
      "Epoch: [115][312/390]\tTime 0.004 (0.003)\tLoss 1.1989 (1.0459)\tPrec@1 56.250 (62.782)\n",
      "Epoch: [115][390/390]\tTime 0.003 (0.003)\tLoss 1.3911 (1.0581)\tPrec@1 57.500 (62.348)\n",
      "EPOCH: 115 train Results: Prec@1 62.348 Loss: 1.0581\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1227 (1.1227)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2953 (1.2361)\tPrec@1 50.000 (56.430)\n",
      "EPOCH: 115 val Results: Prec@1 56.430 Loss: 1.2361\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [116][0/390]\tTime 0.003 (0.003)\tLoss 1.0399 (1.0399)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [116][78/390]\tTime 0.002 (0.003)\tLoss 1.0919 (0.9959)\tPrec@1 63.281 (65.002)\n",
      "Epoch: [116][156/390]\tTime 0.002 (0.003)\tLoss 1.1828 (1.0197)\tPrec@1 61.719 (63.883)\n",
      "Epoch: [116][234/390]\tTime 0.007 (0.003)\tLoss 1.0187 (1.0375)\tPrec@1 65.625 (63.142)\n",
      "Epoch: [116][312/390]\tTime 0.003 (0.003)\tLoss 1.2106 (1.0542)\tPrec@1 60.156 (62.488)\n",
      "Epoch: [116][390/390]\tTime 0.003 (0.003)\tLoss 1.2266 (1.0622)\tPrec@1 57.500 (62.160)\n",
      "EPOCH: 116 train Results: Prec@1 62.160 Loss: 1.0622\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0919 (1.0919)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2540 (1.2424)\tPrec@1 50.000 (55.640)\n",
      "EPOCH: 116 val Results: Prec@1 55.640 Loss: 1.2424\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [117][0/390]\tTime 0.003 (0.003)\tLoss 0.9586 (0.9586)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [117][78/390]\tTime 0.002 (0.003)\tLoss 1.1554 (0.9848)\tPrec@1 61.719 (65.467)\n",
      "Epoch: [117][156/390]\tTime 0.002 (0.003)\tLoss 0.9953 (1.0136)\tPrec@1 71.875 (64.043)\n",
      "Epoch: [117][234/390]\tTime 0.002 (0.003)\tLoss 1.1499 (1.0360)\tPrec@1 58.594 (63.108)\n",
      "Epoch: [117][312/390]\tTime 0.002 (0.003)\tLoss 1.0699 (1.0499)\tPrec@1 60.156 (62.615)\n",
      "Epoch: [117][390/390]\tTime 0.011 (0.003)\tLoss 1.0161 (1.0592)\tPrec@1 60.000 (62.312)\n",
      "EPOCH: 117 train Results: Prec@1 62.312 Loss: 1.0592\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0306 (1.0306)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3932 (1.2401)\tPrec@1 50.000 (56.030)\n",
      "EPOCH: 117 val Results: Prec@1 56.030 Loss: 1.2401\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [118][0/390]\tTime 0.005 (0.005)\tLoss 1.0416 (1.0416)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [118][78/390]\tTime 0.002 (0.003)\tLoss 1.0004 (0.9981)\tPrec@1 62.500 (64.864)\n",
      "Epoch: [118][156/390]\tTime 0.006 (0.003)\tLoss 1.0591 (1.0130)\tPrec@1 60.938 (64.336)\n",
      "Epoch: [118][234/390]\tTime 0.004 (0.003)\tLoss 1.0659 (1.0283)\tPrec@1 59.375 (63.607)\n",
      "Epoch: [118][312/390]\tTime 0.002 (0.003)\tLoss 1.0204 (1.0457)\tPrec@1 66.406 (62.777)\n",
      "Epoch: [118][390/390]\tTime 0.002 (0.003)\tLoss 1.0732 (1.0598)\tPrec@1 63.750 (62.294)\n",
      "EPOCH: 118 train Results: Prec@1 62.294 Loss: 1.0598\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0885 (1.0885)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1110 (1.2570)\tPrec@1 50.000 (55.630)\n",
      "EPOCH: 118 val Results: Prec@1 55.630 Loss: 1.2570\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [119][0/390]\tTime 0.005 (0.005)\tLoss 1.0058 (1.0058)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [119][78/390]\tTime 0.002 (0.002)\tLoss 1.0333 (0.9760)\tPrec@1 67.188 (66.199)\n",
      "Epoch: [119][156/390]\tTime 0.003 (0.003)\tLoss 1.1316 (1.0007)\tPrec@1 61.719 (64.948)\n",
      "Epoch: [119][234/390]\tTime 0.002 (0.003)\tLoss 1.0707 (1.0248)\tPrec@1 70.312 (63.956)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.003)\tLoss 1.0681 (1.0434)\tPrec@1 61.719 (63.309)\n",
      "Epoch: [119][390/390]\tTime 0.002 (0.003)\tLoss 1.2620 (1.0583)\tPrec@1 52.500 (62.686)\n",
      "EPOCH: 119 train Results: Prec@1 62.686 Loss: 1.0583\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1592 (1.1592)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1900 (1.2414)\tPrec@1 62.500 (56.500)\n",
      "EPOCH: 119 val Results: Prec@1 56.500 Loss: 1.2414\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [120][0/390]\tTime 0.010 (0.010)\tLoss 0.8465 (0.8465)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.003)\tLoss 1.1015 (1.0034)\tPrec@1 63.281 (63.934)\n",
      "Epoch: [120][156/390]\tTime 0.002 (0.003)\tLoss 0.9712 (1.0230)\tPrec@1 66.406 (63.411)\n",
      "Epoch: [120][234/390]\tTime 0.002 (0.003)\tLoss 1.1273 (1.0370)\tPrec@1 62.500 (63.085)\n",
      "Epoch: [120][312/390]\tTime 0.002 (0.003)\tLoss 1.1796 (1.0435)\tPrec@1 60.156 (62.869)\n",
      "Epoch: [120][390/390]\tTime 0.001 (0.003)\tLoss 1.0826 (1.0572)\tPrec@1 60.000 (62.422)\n",
      "EPOCH: 120 train Results: Prec@1 62.422 Loss: 1.0572\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1410 (1.1410)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1463 (1.2381)\tPrec@1 56.250 (56.320)\n",
      "EPOCH: 120 val Results: Prec@1 56.320 Loss: 1.2381\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [121][0/390]\tTime 0.004 (0.004)\tLoss 0.9176 (0.9176)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [121][78/390]\tTime 0.003 (0.003)\tLoss 1.0083 (0.9769)\tPrec@1 64.844 (65.170)\n",
      "Epoch: [121][156/390]\tTime 0.005 (0.003)\tLoss 1.0512 (1.0154)\tPrec@1 64.062 (63.878)\n",
      "Epoch: [121][234/390]\tTime 0.008 (0.003)\tLoss 1.1725 (1.0300)\tPrec@1 60.156 (63.428)\n",
      "Epoch: [121][312/390]\tTime 0.003 (0.003)\tLoss 0.9272 (1.0414)\tPrec@1 71.875 (62.969)\n",
      "Epoch: [121][390/390]\tTime 0.002 (0.003)\tLoss 0.7719 (1.0543)\tPrec@1 77.500 (62.634)\n",
      "EPOCH: 121 train Results: Prec@1 62.634 Loss: 1.0543\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1053 (1.1053)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2653 (1.2404)\tPrec@1 50.000 (55.830)\n",
      "EPOCH: 121 val Results: Prec@1 55.830 Loss: 1.2404\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [122][0/390]\tTime 0.004 (0.004)\tLoss 0.8587 (0.8587)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [122][78/390]\tTime 0.004 (0.003)\tLoss 0.9714 (0.9718)\tPrec@1 68.750 (65.783)\n",
      "Epoch: [122][156/390]\tTime 0.002 (0.003)\tLoss 0.9016 (1.0034)\tPrec@1 67.188 (64.769)\n",
      "Epoch: [122][234/390]\tTime 0.002 (0.003)\tLoss 1.2045 (1.0298)\tPrec@1 59.375 (63.684)\n",
      "Epoch: [122][312/390]\tTime 0.002 (0.003)\tLoss 1.1164 (1.0479)\tPrec@1 60.938 (62.857)\n",
      "Epoch: [122][390/390]\tTime 0.002 (0.003)\tLoss 1.2171 (1.0587)\tPrec@1 52.500 (62.412)\n",
      "EPOCH: 122 train Results: Prec@1 62.412 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1278 (1.1278)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1542 (1.2508)\tPrec@1 43.750 (55.820)\n",
      "EPOCH: 122 val Results: Prec@1 55.820 Loss: 1.2508\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [123][0/390]\tTime 0.005 (0.005)\tLoss 0.8843 (0.8843)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [123][78/390]\tTime 0.002 (0.003)\tLoss 1.1059 (0.9863)\tPrec@1 56.250 (64.933)\n",
      "Epoch: [123][156/390]\tTime 0.002 (0.003)\tLoss 1.0581 (1.0112)\tPrec@1 63.281 (64.112)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.003)\tLoss 1.1821 (1.0292)\tPrec@1 60.156 (63.547)\n",
      "Epoch: [123][312/390]\tTime 0.003 (0.003)\tLoss 1.1624 (1.0445)\tPrec@1 56.250 (62.934)\n",
      "Epoch: [123][390/390]\tTime 0.002 (0.003)\tLoss 0.9861 (1.0555)\tPrec@1 57.500 (62.426)\n",
      "EPOCH: 123 train Results: Prec@1 62.426 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1520 (1.1520)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5616 (1.2442)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 123 val Results: Prec@1 55.630 Loss: 1.2442\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [124][0/390]\tTime 0.003 (0.003)\tLoss 0.9526 (0.9526)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [124][78/390]\tTime 0.003 (0.003)\tLoss 1.1315 (0.9898)\tPrec@1 64.844 (64.982)\n",
      "Epoch: [124][156/390]\tTime 0.002 (0.003)\tLoss 1.0142 (1.0192)\tPrec@1 63.281 (63.938)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.1207 (1.0347)\tPrec@1 53.125 (63.361)\n",
      "Epoch: [124][312/390]\tTime 0.003 (0.003)\tLoss 1.1423 (1.0452)\tPrec@1 64.062 (62.992)\n",
      "Epoch: [124][390/390]\tTime 0.001 (0.003)\tLoss 1.2559 (1.0563)\tPrec@1 56.250 (62.590)\n",
      "EPOCH: 124 train Results: Prec@1 62.590 Loss: 1.0563\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1495 (1.1495)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3113 (1.2439)\tPrec@1 50.000 (55.430)\n",
      "EPOCH: 124 val Results: Prec@1 55.430 Loss: 1.2439\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [125][0/390]\tTime 0.010 (0.010)\tLoss 0.9665 (0.9665)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [125][78/390]\tTime 0.002 (0.003)\tLoss 0.8627 (0.9852)\tPrec@1 68.750 (65.447)\n",
      "Epoch: [125][156/390]\tTime 0.003 (0.003)\tLoss 1.0006 (1.0151)\tPrec@1 63.281 (63.824)\n",
      "Epoch: [125][234/390]\tTime 0.003 (0.003)\tLoss 1.1689 (1.0330)\tPrec@1 58.594 (63.278)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.0903 (1.0444)\tPrec@1 66.406 (62.984)\n",
      "Epoch: [125][390/390]\tTime 0.004 (0.003)\tLoss 0.9902 (1.0553)\tPrec@1 62.500 (62.578)\n",
      "EPOCH: 125 train Results: Prec@1 62.578 Loss: 1.0553\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0995 (1.0995)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1445 (1.2464)\tPrec@1 62.500 (55.350)\n",
      "EPOCH: 125 val Results: Prec@1 55.350 Loss: 1.2464\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [126][0/390]\tTime 0.003 (0.003)\tLoss 0.8440 (0.8440)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [126][78/390]\tTime 0.005 (0.003)\tLoss 0.8701 (0.9746)\tPrec@1 70.312 (66.060)\n",
      "Epoch: [126][156/390]\tTime 0.006 (0.003)\tLoss 1.1099 (1.0097)\tPrec@1 60.156 (64.590)\n",
      "Epoch: [126][234/390]\tTime 0.008 (0.003)\tLoss 1.1703 (1.0274)\tPrec@1 55.469 (63.883)\n",
      "Epoch: [126][312/390]\tTime 0.003 (0.003)\tLoss 1.0385 (1.0451)\tPrec@1 59.375 (63.092)\n",
      "Epoch: [126][390/390]\tTime 0.001 (0.003)\tLoss 1.1139 (1.0566)\tPrec@1 57.500 (62.648)\n",
      "EPOCH: 126 train Results: Prec@1 62.648 Loss: 1.0566\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1683 (1.1683)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5607 (1.2612)\tPrec@1 37.500 (55.270)\n",
      "EPOCH: 126 val Results: Prec@1 55.270 Loss: 1.2612\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [127][0/390]\tTime 0.004 (0.004)\tLoss 1.1059 (1.1059)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.004)\tLoss 0.8879 (0.9820)\tPrec@1 68.750 (65.625)\n",
      "Epoch: [127][156/390]\tTime 0.003 (0.004)\tLoss 0.9412 (1.0070)\tPrec@1 62.500 (64.590)\n",
      "Epoch: [127][234/390]\tTime 0.002 (0.004)\tLoss 1.2215 (1.0233)\tPrec@1 53.125 (63.916)\n",
      "Epoch: [127][312/390]\tTime 0.003 (0.004)\tLoss 1.1398 (1.0433)\tPrec@1 60.156 (63.082)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.003)\tLoss 1.0749 (1.0548)\tPrec@1 58.750 (62.734)\n",
      "EPOCH: 127 train Results: Prec@1 62.734 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0907 (1.0907)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5188 (1.2488)\tPrec@1 25.000 (56.110)\n",
      "EPOCH: 127 val Results: Prec@1 56.110 Loss: 1.2488\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [128][0/390]\tTime 0.002 (0.002)\tLoss 1.0835 (1.0835)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [128][78/390]\tTime 0.002 (0.003)\tLoss 1.0944 (0.9799)\tPrec@1 59.375 (65.348)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.003)\tLoss 1.0504 (1.0108)\tPrec@1 59.375 (64.291)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.003)\tLoss 1.0993 (1.0257)\tPrec@1 63.281 (63.813)\n",
      "Epoch: [128][312/390]\tTime 0.004 (0.003)\tLoss 0.9750 (1.0414)\tPrec@1 65.625 (63.219)\n",
      "Epoch: [128][390/390]\tTime 0.006 (0.003)\tLoss 1.2167 (1.0561)\tPrec@1 62.500 (62.710)\n",
      "EPOCH: 128 train Results: Prec@1 62.710 Loss: 1.0561\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1464 (1.1464)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0429 (1.2534)\tPrec@1 62.500 (56.020)\n",
      "EPOCH: 128 val Results: Prec@1 56.020 Loss: 1.2534\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [129][0/390]\tTime 0.003 (0.003)\tLoss 0.9921 (0.9921)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 0.9038 (0.9669)\tPrec@1 67.188 (65.576)\n",
      "Epoch: [129][156/390]\tTime 0.002 (0.003)\tLoss 1.1639 (0.9948)\tPrec@1 53.906 (64.565)\n",
      "Epoch: [129][234/390]\tTime 0.002 (0.003)\tLoss 1.2257 (1.0203)\tPrec@1 51.562 (63.630)\n",
      "Epoch: [129][312/390]\tTime 0.003 (0.003)\tLoss 0.9724 (1.0389)\tPrec@1 65.625 (62.942)\n",
      "Epoch: [129][390/390]\tTime 0.004 (0.003)\tLoss 1.0107 (1.0530)\tPrec@1 61.250 (62.494)\n",
      "EPOCH: 129 train Results: Prec@1 62.494 Loss: 1.0530\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1518 (1.1518)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5558 (1.2488)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 129 val Results: Prec@1 55.630 Loss: 1.2488\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [130][0/390]\tTime 0.005 (0.005)\tLoss 0.9196 (0.9196)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [130][78/390]\tTime 0.003 (0.003)\tLoss 0.8568 (0.9780)\tPrec@1 67.969 (65.318)\n",
      "Epoch: [130][156/390]\tTime 0.002 (0.003)\tLoss 1.1205 (0.9997)\tPrec@1 57.031 (64.610)\n",
      "Epoch: [130][234/390]\tTime 0.002 (0.003)\tLoss 1.2181 (1.0277)\tPrec@1 50.781 (63.647)\n",
      "Epoch: [130][312/390]\tTime 0.002 (0.003)\tLoss 1.0215 (1.0419)\tPrec@1 63.281 (63.062)\n",
      "Epoch: [130][390/390]\tTime 0.001 (0.003)\tLoss 1.2963 (1.0548)\tPrec@1 57.500 (62.690)\n",
      "EPOCH: 130 train Results: Prec@1 62.690 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1774 (1.1774)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2627 (1.2432)\tPrec@1 50.000 (55.650)\n",
      "EPOCH: 130 val Results: Prec@1 55.650 Loss: 1.2432\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [131][0/390]\tTime 0.006 (0.006)\tLoss 0.8136 (0.8136)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [131][78/390]\tTime 0.005 (0.003)\tLoss 0.9095 (0.9846)\tPrec@1 67.969 (64.903)\n",
      "Epoch: [131][156/390]\tTime 0.003 (0.003)\tLoss 1.1519 (1.0140)\tPrec@1 57.812 (63.431)\n",
      "Epoch: [131][234/390]\tTime 0.002 (0.003)\tLoss 1.0859 (1.0344)\tPrec@1 60.156 (62.965)\n",
      "Epoch: [131][312/390]\tTime 0.003 (0.003)\tLoss 1.3291 (1.0490)\tPrec@1 52.344 (62.542)\n",
      "Epoch: [131][390/390]\tTime 0.003 (0.003)\tLoss 0.9885 (1.0614)\tPrec@1 56.250 (62.164)\n",
      "EPOCH: 131 train Results: Prec@1 62.164 Loss: 1.0614\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1521 (1.1521)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4112 (1.2416)\tPrec@1 43.750 (56.310)\n",
      "EPOCH: 131 val Results: Prec@1 56.310 Loss: 1.2416\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [132][0/390]\tTime 0.003 (0.003)\tLoss 0.9764 (0.9764)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [132][78/390]\tTime 0.003 (0.003)\tLoss 0.9670 (0.9841)\tPrec@1 70.312 (65.477)\n",
      "Epoch: [132][156/390]\tTime 0.004 (0.004)\tLoss 1.1561 (1.0174)\tPrec@1 56.250 (63.888)\n",
      "Epoch: [132][234/390]\tTime 0.006 (0.003)\tLoss 1.0350 (1.0365)\tPrec@1 60.938 (63.108)\n",
      "Epoch: [132][312/390]\tTime 0.002 (0.003)\tLoss 1.3245 (1.0470)\tPrec@1 50.000 (62.822)\n",
      "Epoch: [132][390/390]\tTime 0.001 (0.003)\tLoss 1.1155 (1.0529)\tPrec@1 60.000 (62.556)\n",
      "EPOCH: 132 train Results: Prec@1 62.556 Loss: 1.0529\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1525 (1.1525)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1410 (1.2420)\tPrec@1 50.000 (55.350)\n",
      "EPOCH: 132 val Results: Prec@1 55.350 Loss: 1.2420\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [133][0/390]\tTime 0.004 (0.004)\tLoss 0.9031 (0.9031)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [133][78/390]\tTime 0.002 (0.004)\tLoss 1.0601 (0.9873)\tPrec@1 57.812 (64.814)\n",
      "Epoch: [133][156/390]\tTime 0.002 (0.004)\tLoss 0.9121 (1.0023)\tPrec@1 73.438 (64.694)\n",
      "Epoch: [133][234/390]\tTime 0.002 (0.003)\tLoss 1.1981 (1.0238)\tPrec@1 58.594 (63.803)\n",
      "Epoch: [133][312/390]\tTime 0.007 (0.003)\tLoss 1.1081 (1.0429)\tPrec@1 65.625 (63.067)\n",
      "Epoch: [133][390/390]\tTime 0.002 (0.003)\tLoss 1.1656 (1.0555)\tPrec@1 55.000 (62.606)\n",
      "EPOCH: 133 train Results: Prec@1 62.606 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1554 (1.1554)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2486 (1.2487)\tPrec@1 62.500 (55.740)\n",
      "EPOCH: 133 val Results: Prec@1 55.740 Loss: 1.2487\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [134][0/390]\tTime 0.002 (0.002)\tLoss 0.9774 (0.9774)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [134][78/390]\tTime 0.002 (0.003)\tLoss 0.9520 (0.9884)\tPrec@1 70.312 (65.032)\n",
      "Epoch: [134][156/390]\tTime 0.002 (0.003)\tLoss 1.0292 (1.0189)\tPrec@1 62.500 (64.122)\n",
      "Epoch: [134][234/390]\tTime 0.003 (0.003)\tLoss 1.0544 (1.0295)\tPrec@1 65.625 (63.670)\n",
      "Epoch: [134][312/390]\tTime 0.002 (0.003)\tLoss 0.9994 (1.0422)\tPrec@1 64.844 (63.112)\n",
      "Epoch: [134][390/390]\tTime 0.001 (0.003)\tLoss 1.1031 (1.0541)\tPrec@1 57.500 (62.632)\n",
      "EPOCH: 134 train Results: Prec@1 62.632 Loss: 1.0541\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0985 (1.0985)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0846 (1.2384)\tPrec@1 68.750 (56.230)\n",
      "EPOCH: 134 val Results: Prec@1 56.230 Loss: 1.2384\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [135][0/390]\tTime 0.005 (0.005)\tLoss 0.9843 (0.9843)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [135][78/390]\tTime 0.005 (0.003)\tLoss 0.9682 (0.9819)\tPrec@1 65.625 (65.437)\n",
      "Epoch: [135][156/390]\tTime 0.002 (0.003)\tLoss 1.0184 (1.0116)\tPrec@1 64.062 (64.386)\n",
      "Epoch: [135][234/390]\tTime 0.003 (0.003)\tLoss 1.1733 (1.0331)\tPrec@1 60.156 (63.477)\n",
      "Epoch: [135][312/390]\tTime 0.002 (0.003)\tLoss 1.0581 (1.0466)\tPrec@1 60.156 (63.092)\n",
      "Epoch: [135][390/390]\tTime 0.002 (0.003)\tLoss 1.1130 (1.0606)\tPrec@1 63.750 (62.520)\n",
      "EPOCH: 135 train Results: Prec@1 62.520 Loss: 1.0606\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1276 (1.1276)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1416 (1.2412)\tPrec@1 62.500 (56.260)\n",
      "EPOCH: 135 val Results: Prec@1 56.260 Loss: 1.2412\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [136][0/390]\tTime 0.003 (0.003)\tLoss 0.8898 (0.8898)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [136][78/390]\tTime 0.004 (0.003)\tLoss 0.9741 (0.9828)\tPrec@1 63.281 (65.556)\n",
      "Epoch: [136][156/390]\tTime 0.004 (0.003)\tLoss 0.9854 (1.0118)\tPrec@1 65.625 (64.406)\n",
      "Epoch: [136][234/390]\tTime 0.003 (0.003)\tLoss 1.0737 (1.0322)\tPrec@1 67.188 (63.610)\n",
      "Epoch: [136][312/390]\tTime 0.002 (0.003)\tLoss 1.0903 (1.0442)\tPrec@1 63.281 (63.039)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.0519 (1.0537)\tPrec@1 65.000 (62.498)\n",
      "EPOCH: 136 train Results: Prec@1 62.498 Loss: 1.0537\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0629 (1.0629)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3957 (1.2518)\tPrec@1 43.750 (55.760)\n",
      "EPOCH: 136 val Results: Prec@1 55.760 Loss: 1.2518\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [137][0/390]\tTime 0.004 (0.004)\tLoss 0.9844 (0.9844)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [137][78/390]\tTime 0.003 (0.003)\tLoss 1.0180 (1.0001)\tPrec@1 63.281 (64.705)\n",
      "Epoch: [137][156/390]\tTime 0.002 (0.003)\tLoss 1.0029 (1.0149)\tPrec@1 67.188 (64.062)\n",
      "Epoch: [137][234/390]\tTime 0.003 (0.003)\tLoss 0.9981 (1.0258)\tPrec@1 63.281 (63.481)\n",
      "Epoch: [137][312/390]\tTime 0.002 (0.004)\tLoss 1.1435 (1.0404)\tPrec@1 58.594 (62.932)\n",
      "Epoch: [137][390/390]\tTime 0.002 (0.003)\tLoss 1.1828 (1.0557)\tPrec@1 62.500 (62.410)\n",
      "EPOCH: 137 train Results: Prec@1 62.410 Loss: 1.0557\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0793 (1.0793)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1897 (1.2468)\tPrec@1 50.000 (55.920)\n",
      "EPOCH: 137 val Results: Prec@1 55.920 Loss: 1.2468\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [138][0/390]\tTime 0.004 (0.004)\tLoss 1.0197 (1.0197)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [138][78/390]\tTime 0.002 (0.003)\tLoss 1.0023 (0.9910)\tPrec@1 64.062 (64.982)\n",
      "Epoch: [138][156/390]\tTime 0.011 (0.004)\tLoss 1.0524 (1.0146)\tPrec@1 63.281 (64.192)\n",
      "Epoch: [138][234/390]\tTime 0.003 (0.004)\tLoss 1.2363 (1.0344)\tPrec@1 57.812 (63.600)\n",
      "Epoch: [138][312/390]\tTime 0.004 (0.004)\tLoss 1.3238 (1.0490)\tPrec@1 54.688 (63.029)\n",
      "Epoch: [138][390/390]\tTime 0.001 (0.004)\tLoss 1.2542 (1.0599)\tPrec@1 51.250 (62.606)\n",
      "EPOCH: 138 train Results: Prec@1 62.606 Loss: 1.0599\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1396 (1.1396)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1525 (1.2438)\tPrec@1 68.750 (55.730)\n",
      "EPOCH: 138 val Results: Prec@1 55.730 Loss: 1.2438\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [139][0/390]\tTime 0.004 (0.004)\tLoss 0.9400 (0.9400)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [139][78/390]\tTime 0.002 (0.003)\tLoss 0.9549 (0.9800)\tPrec@1 66.406 (64.972)\n",
      "Epoch: [139][156/390]\tTime 0.003 (0.003)\tLoss 0.9865 (1.0099)\tPrec@1 65.625 (64.271)\n",
      "Epoch: [139][234/390]\tTime 0.007 (0.003)\tLoss 1.0976 (1.0304)\tPrec@1 62.500 (63.471)\n",
      "Epoch: [139][312/390]\tTime 0.003 (0.004)\tLoss 1.0325 (1.0436)\tPrec@1 61.719 (62.967)\n",
      "Epoch: [139][390/390]\tTime 0.001 (0.004)\tLoss 1.1408 (1.0545)\tPrec@1 61.250 (62.500)\n",
      "EPOCH: 139 train Results: Prec@1 62.500 Loss: 1.0545\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2566 (1.2566)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2131 (1.2424)\tPrec@1 43.750 (55.900)\n",
      "EPOCH: 139 val Results: Prec@1 55.900 Loss: 1.2424\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [140][0/390]\tTime 0.004 (0.004)\tLoss 0.9393 (0.9393)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [140][78/390]\tTime 0.003 (0.003)\tLoss 1.0934 (0.9813)\tPrec@1 57.031 (65.684)\n",
      "Epoch: [140][156/390]\tTime 0.002 (0.004)\tLoss 1.0635 (1.0050)\tPrec@1 64.062 (64.431)\n",
      "Epoch: [140][234/390]\tTime 0.002 (0.005)\tLoss 0.9282 (1.0262)\tPrec@1 64.844 (63.381)\n",
      "Epoch: [140][312/390]\tTime 0.002 (0.005)\tLoss 1.0334 (1.0378)\tPrec@1 57.031 (63.114)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.005)\tLoss 1.2326 (1.0531)\tPrec@1 52.500 (62.532)\n",
      "EPOCH: 140 train Results: Prec@1 62.532 Loss: 1.0531\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1588 (1.1588)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6436 (1.2515)\tPrec@1 31.250 (55.730)\n",
      "EPOCH: 140 val Results: Prec@1 55.730 Loss: 1.2515\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [141][0/390]\tTime 0.003 (0.003)\tLoss 0.7695 (0.7695)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [141][78/390]\tTime 0.003 (0.003)\tLoss 1.0693 (0.9881)\tPrec@1 65.625 (64.903)\n",
      "Epoch: [141][156/390]\tTime 0.002 (0.003)\tLoss 1.0198 (1.0139)\tPrec@1 61.719 (63.769)\n",
      "Epoch: [141][234/390]\tTime 0.002 (0.003)\tLoss 1.0810 (1.0331)\tPrec@1 64.844 (63.305)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.003)\tLoss 1.1711 (1.0491)\tPrec@1 64.062 (62.670)\n",
      "Epoch: [141][390/390]\tTime 0.003 (0.004)\tLoss 1.1927 (1.0582)\tPrec@1 55.000 (62.378)\n",
      "EPOCH: 141 train Results: Prec@1 62.378 Loss: 1.0582\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1646 (1.1646)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4236 (1.2419)\tPrec@1 43.750 (56.170)\n",
      "EPOCH: 141 val Results: Prec@1 56.170 Loss: 1.2419\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [142][0/390]\tTime 0.006 (0.006)\tLoss 0.9188 (0.9188)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [142][78/390]\tTime 0.015 (0.005)\tLoss 0.9566 (0.9794)\tPrec@1 66.406 (65.615)\n",
      "Epoch: [142][156/390]\tTime 0.002 (0.004)\tLoss 1.0058 (1.0048)\tPrec@1 62.500 (64.262)\n",
      "Epoch: [142][234/390]\tTime 0.002 (0.004)\tLoss 1.0705 (1.0225)\tPrec@1 66.406 (63.700)\n",
      "Epoch: [142][312/390]\tTime 0.010 (0.004)\tLoss 1.1555 (1.0428)\tPrec@1 59.375 (62.934)\n",
      "Epoch: [142][390/390]\tTime 0.006 (0.004)\tLoss 1.1153 (1.0560)\tPrec@1 61.250 (62.444)\n",
      "EPOCH: 142 train Results: Prec@1 62.444 Loss: 1.0560\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1941 (1.1941)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9589 (1.2618)\tPrec@1 68.750 (55.720)\n",
      "EPOCH: 142 val Results: Prec@1 55.720 Loss: 1.2618\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [143][0/390]\tTime 0.005 (0.005)\tLoss 0.8287 (0.8287)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [143][78/390]\tTime 0.005 (0.003)\tLoss 1.0337 (0.9805)\tPrec@1 68.750 (66.011)\n",
      "Epoch: [143][156/390]\tTime 0.004 (0.003)\tLoss 0.9681 (1.0051)\tPrec@1 61.719 (64.749)\n",
      "Epoch: [143][234/390]\tTime 0.009 (0.003)\tLoss 1.0126 (1.0195)\tPrec@1 60.938 (64.142)\n",
      "Epoch: [143][312/390]\tTime 0.003 (0.003)\tLoss 1.1045 (1.0400)\tPrec@1 65.625 (63.219)\n",
      "Epoch: [143][390/390]\tTime 0.003 (0.003)\tLoss 1.0400 (1.0523)\tPrec@1 65.000 (62.762)\n",
      "EPOCH: 143 train Results: Prec@1 62.762 Loss: 1.0523\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1829 (1.1829)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1903 (1.2546)\tPrec@1 62.500 (55.800)\n",
      "EPOCH: 143 val Results: Prec@1 55.800 Loss: 1.2546\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [144][0/390]\tTime 0.006 (0.006)\tLoss 0.9555 (0.9555)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [144][78/390]\tTime 0.002 (0.006)\tLoss 1.0410 (0.9933)\tPrec@1 60.938 (64.735)\n",
      "Epoch: [144][156/390]\tTime 0.002 (0.005)\tLoss 1.3186 (1.0088)\tPrec@1 55.469 (63.993)\n",
      "Epoch: [144][234/390]\tTime 0.012 (0.004)\tLoss 1.0546 (1.0287)\tPrec@1 62.500 (63.424)\n",
      "Epoch: [144][312/390]\tTime 0.011 (0.004)\tLoss 0.9977 (1.0420)\tPrec@1 60.938 (62.864)\n",
      "Epoch: [144][390/390]\tTime 0.016 (0.005)\tLoss 0.9956 (1.0536)\tPrec@1 61.250 (62.482)\n",
      "EPOCH: 144 train Results: Prec@1 62.482 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1016 (1.1016)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3678 (1.2443)\tPrec@1 37.500 (55.850)\n",
      "EPOCH: 144 val Results: Prec@1 55.850 Loss: 1.2443\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [145][0/390]\tTime 0.002 (0.002)\tLoss 1.0386 (1.0386)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [145][78/390]\tTime 0.002 (0.006)\tLoss 0.8599 (0.9963)\tPrec@1 71.875 (64.705)\n",
      "Epoch: [145][156/390]\tTime 0.003 (0.005)\tLoss 1.1530 (1.0008)\tPrec@1 62.500 (64.456)\n",
      "Epoch: [145][234/390]\tTime 0.003 (0.004)\tLoss 1.0696 (1.0245)\tPrec@1 63.281 (63.634)\n",
      "Epoch: [145][312/390]\tTime 0.007 (0.004)\tLoss 1.2211 (1.0401)\tPrec@1 52.344 (63.042)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.004)\tLoss 1.0382 (1.0496)\tPrec@1 67.500 (62.714)\n",
      "EPOCH: 145 train Results: Prec@1 62.714 Loss: 1.0496\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1231 (1.1231)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5981 (1.2594)\tPrec@1 50.000 (56.050)\n",
      "EPOCH: 145 val Results: Prec@1 56.050 Loss: 1.2594\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [146][0/390]\tTime 0.003 (0.003)\tLoss 1.0175 (1.0175)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [146][78/390]\tTime 0.002 (0.005)\tLoss 0.9075 (0.9832)\tPrec@1 68.750 (65.318)\n",
      "Epoch: [146][156/390]\tTime 0.013 (0.005)\tLoss 0.8845 (1.0173)\tPrec@1 67.188 (63.898)\n",
      "Epoch: [146][234/390]\tTime 0.004 (0.004)\tLoss 0.8852 (1.0314)\tPrec@1 69.531 (63.408)\n",
      "Epoch: [146][312/390]\tTime 0.004 (0.004)\tLoss 0.9099 (1.0454)\tPrec@1 68.750 (62.904)\n",
      "Epoch: [146][390/390]\tTime 0.002 (0.004)\tLoss 0.9654 (1.0580)\tPrec@1 63.750 (62.444)\n",
      "EPOCH: 146 train Results: Prec@1 62.444 Loss: 1.0580\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1334 (1.1334)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1523 (1.2455)\tPrec@1 50.000 (56.190)\n",
      "EPOCH: 146 val Results: Prec@1 56.190 Loss: 1.2455\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [147][0/390]\tTime 0.004 (0.004)\tLoss 0.9262 (0.9262)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [147][78/390]\tTime 0.003 (0.004)\tLoss 1.0787 (0.9879)\tPrec@1 64.844 (65.526)\n",
      "Epoch: [147][156/390]\tTime 0.002 (0.005)\tLoss 1.1022 (1.0083)\tPrec@1 60.938 (64.376)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.004)\tLoss 1.0891 (1.0252)\tPrec@1 62.500 (63.614)\n",
      "Epoch: [147][312/390]\tTime 0.012 (0.004)\tLoss 1.0196 (1.0384)\tPrec@1 65.625 (63.211)\n",
      "Epoch: [147][390/390]\tTime 0.003 (0.004)\tLoss 1.2217 (1.0521)\tPrec@1 58.750 (62.764)\n",
      "EPOCH: 147 train Results: Prec@1 62.764 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1374 (1.1374)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3970 (1.2449)\tPrec@1 43.750 (56.020)\n",
      "EPOCH: 147 val Results: Prec@1 56.020 Loss: 1.2449\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [148][0/390]\tTime 0.003 (0.003)\tLoss 1.0187 (1.0187)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [148][78/390]\tTime 0.009 (0.005)\tLoss 0.9959 (0.9859)\tPrec@1 64.844 (65.694)\n",
      "Epoch: [148][156/390]\tTime 0.009 (0.005)\tLoss 1.0054 (1.0093)\tPrec@1 71.094 (64.565)\n",
      "Epoch: [148][234/390]\tTime 0.003 (0.005)\tLoss 0.9351 (1.0307)\tPrec@1 71.094 (63.561)\n",
      "Epoch: [148][312/390]\tTime 0.004 (0.005)\tLoss 0.9715 (1.0431)\tPrec@1 66.406 (63.064)\n",
      "Epoch: [148][390/390]\tTime 0.003 (0.004)\tLoss 1.0868 (1.0535)\tPrec@1 58.750 (62.722)\n",
      "EPOCH: 148 train Results: Prec@1 62.722 Loss: 1.0535\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1692 (1.1692)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3045 (1.2574)\tPrec@1 37.500 (55.920)\n",
      "EPOCH: 148 val Results: Prec@1 55.920 Loss: 1.2574\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [149][0/390]\tTime 0.003 (0.003)\tLoss 0.8989 (0.8989)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [149][78/390]\tTime 0.002 (0.004)\tLoss 1.0823 (0.9783)\tPrec@1 58.594 (65.526)\n",
      "Epoch: [149][156/390]\tTime 0.006 (0.003)\tLoss 1.0015 (1.0000)\tPrec@1 64.844 (64.550)\n",
      "Epoch: [149][234/390]\tTime 0.004 (0.004)\tLoss 0.9860 (1.0214)\tPrec@1 65.625 (63.600)\n",
      "Epoch: [149][312/390]\tTime 0.002 (0.004)\tLoss 1.1331 (1.0388)\tPrec@1 62.500 (63.074)\n",
      "Epoch: [149][390/390]\tTime 0.002 (0.003)\tLoss 1.1586 (1.0529)\tPrec@1 56.250 (62.614)\n",
      "EPOCH: 149 train Results: Prec@1 62.614 Loss: 1.0529\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1581 (1.1581)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2205 (1.2395)\tPrec@1 56.250 (55.950)\n",
      "EPOCH: 149 val Results: Prec@1 55.950 Loss: 1.2395\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [150][0/390]\tTime 0.005 (0.005)\tLoss 0.9954 (0.9954)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [150][78/390]\tTime 0.002 (0.003)\tLoss 1.1860 (0.9600)\tPrec@1 63.281 (66.525)\n",
      "Epoch: [150][156/390]\tTime 0.003 (0.003)\tLoss 1.1368 (0.9932)\tPrec@1 65.625 (65.107)\n",
      "Epoch: [150][234/390]\tTime 0.002 (0.003)\tLoss 1.1294 (1.0257)\tPrec@1 60.938 (63.747)\n",
      "Epoch: [150][312/390]\tTime 0.004 (0.003)\tLoss 1.1213 (1.0417)\tPrec@1 57.812 (63.107)\n",
      "Epoch: [150][390/390]\tTime 0.001 (0.003)\tLoss 1.1039 (1.0547)\tPrec@1 66.250 (62.682)\n",
      "EPOCH: 150 train Results: Prec@1 62.682 Loss: 1.0547\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0899 (1.0899)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1477 (1.2386)\tPrec@1 50.000 (55.810)\n",
      "EPOCH: 150 val Results: Prec@1 55.810 Loss: 1.2386\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [151][0/390]\tTime 0.004 (0.004)\tLoss 0.8751 (0.8751)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [151][78/390]\tTime 0.002 (0.003)\tLoss 1.1739 (0.9745)\tPrec@1 58.594 (65.971)\n",
      "Epoch: [151][156/390]\tTime 0.003 (0.004)\tLoss 1.0461 (0.9960)\tPrec@1 64.844 (65.018)\n",
      "Epoch: [151][234/390]\tTime 0.003 (0.004)\tLoss 1.0950 (1.0184)\tPrec@1 54.688 (63.979)\n",
      "Epoch: [151][312/390]\tTime 0.002 (0.003)\tLoss 1.1098 (1.0343)\tPrec@1 62.500 (63.191)\n",
      "Epoch: [151][390/390]\tTime 0.001 (0.003)\tLoss 1.2381 (1.0494)\tPrec@1 58.750 (62.668)\n",
      "EPOCH: 151 train Results: Prec@1 62.668 Loss: 1.0494\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1274 (1.1274)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1526 (1.2491)\tPrec@1 56.250 (55.820)\n",
      "EPOCH: 151 val Results: Prec@1 55.820 Loss: 1.2491\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [152][0/390]\tTime 0.002 (0.002)\tLoss 0.8862 (0.8862)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [152][78/390]\tTime 0.013 (0.004)\tLoss 1.0201 (0.9859)\tPrec@1 64.844 (65.150)\n",
      "Epoch: [152][156/390]\tTime 0.003 (0.003)\tLoss 0.9114 (1.0118)\tPrec@1 71.094 (64.097)\n",
      "Epoch: [152][234/390]\tTime 0.002 (0.003)\tLoss 1.1243 (1.0270)\tPrec@1 65.625 (63.418)\n",
      "Epoch: [152][312/390]\tTime 0.002 (0.003)\tLoss 1.0971 (1.0390)\tPrec@1 60.156 (63.052)\n",
      "Epoch: [152][390/390]\tTime 0.001 (0.003)\tLoss 0.9769 (1.0504)\tPrec@1 67.500 (62.650)\n",
      "EPOCH: 152 train Results: Prec@1 62.650 Loss: 1.0504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0998 (1.0998)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1490 (1.2469)\tPrec@1 43.750 (55.460)\n",
      "EPOCH: 152 val Results: Prec@1 55.460 Loss: 1.2469\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [153][0/390]\tTime 0.004 (0.004)\tLoss 0.7881 (0.7881)\tPrec@1 78.125 (78.125)\n",
      "Epoch: [153][78/390]\tTime 0.003 (0.004)\tLoss 1.0317 (0.9816)\tPrec@1 63.281 (64.923)\n",
      "Epoch: [153][156/390]\tTime 0.003 (0.003)\tLoss 0.9838 (1.0099)\tPrec@1 66.406 (64.053)\n",
      "Epoch: [153][234/390]\tTime 0.003 (0.003)\tLoss 0.9992 (1.0240)\tPrec@1 67.188 (63.777)\n",
      "Epoch: [153][312/390]\tTime 0.002 (0.003)\tLoss 0.9122 (1.0358)\tPrec@1 66.406 (63.284)\n",
      "Epoch: [153][390/390]\tTime 0.006 (0.003)\tLoss 1.1043 (1.0488)\tPrec@1 61.250 (62.690)\n",
      "EPOCH: 153 train Results: Prec@1 62.690 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1536 (1.1536)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5127 (1.2395)\tPrec@1 31.250 (56.110)\n",
      "EPOCH: 153 val Results: Prec@1 56.110 Loss: 1.2395\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [154][0/390]\tTime 0.004 (0.004)\tLoss 0.8154 (0.8154)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.004)\tLoss 0.9076 (0.9809)\tPrec@1 66.406 (65.111)\n",
      "Epoch: [154][156/390]\tTime 0.002 (0.004)\tLoss 1.1076 (1.0031)\tPrec@1 58.594 (64.441)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.003)\tLoss 1.2239 (1.0206)\tPrec@1 53.125 (63.604)\n",
      "Epoch: [154][312/390]\tTime 0.003 (0.003)\tLoss 1.0830 (1.0397)\tPrec@1 54.688 (62.939)\n",
      "Epoch: [154][390/390]\tTime 0.001 (0.003)\tLoss 1.1593 (1.0523)\tPrec@1 57.500 (62.468)\n",
      "EPOCH: 154 train Results: Prec@1 62.468 Loss: 1.0523\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1460 (1.1460)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5095 (1.2499)\tPrec@1 62.500 (55.580)\n",
      "EPOCH: 154 val Results: Prec@1 55.580 Loss: 1.2499\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [155][0/390]\tTime 0.005 (0.005)\tLoss 0.9730 (0.9730)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [155][78/390]\tTime 0.002 (0.003)\tLoss 1.0117 (0.9669)\tPrec@1 62.500 (65.932)\n",
      "Epoch: [155][156/390]\tTime 0.003 (0.003)\tLoss 0.8956 (0.9903)\tPrec@1 65.625 (64.565)\n",
      "Epoch: [155][234/390]\tTime 0.004 (0.003)\tLoss 1.0822 (1.0114)\tPrec@1 57.031 (63.800)\n",
      "Epoch: [155][312/390]\tTime 0.005 (0.003)\tLoss 1.2484 (1.0324)\tPrec@1 55.469 (63.087)\n",
      "Epoch: [155][390/390]\tTime 0.001 (0.003)\tLoss 0.9424 (1.0482)\tPrec@1 67.500 (62.606)\n",
      "EPOCH: 155 train Results: Prec@1 62.606 Loss: 1.0482\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1167 (1.1167)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5456 (1.2592)\tPrec@1 31.250 (55.240)\n",
      "EPOCH: 155 val Results: Prec@1 55.240 Loss: 1.2592\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [156][0/390]\tTime 0.004 (0.004)\tLoss 0.9581 (0.9581)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [156][78/390]\tTime 0.002 (0.003)\tLoss 1.0681 (0.9710)\tPrec@1 60.938 (65.872)\n",
      "Epoch: [156][156/390]\tTime 0.004 (0.003)\tLoss 1.2052 (0.9985)\tPrec@1 56.250 (64.675)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.003)\tLoss 0.9933 (1.0244)\tPrec@1 65.625 (63.314)\n",
      "Epoch: [156][312/390]\tTime 0.002 (0.003)\tLoss 1.2215 (1.0414)\tPrec@1 57.812 (62.762)\n",
      "Epoch: [156][390/390]\tTime 0.002 (0.003)\tLoss 1.1265 (1.0527)\tPrec@1 60.000 (62.416)\n",
      "EPOCH: 156 train Results: Prec@1 62.416 Loss: 1.0527\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1416 (1.1416)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2897 (1.2408)\tPrec@1 50.000 (55.680)\n",
      "EPOCH: 156 val Results: Prec@1 55.680 Loss: 1.2408\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [157][0/390]\tTime 0.003 (0.003)\tLoss 1.0445 (1.0445)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [157][78/390]\tTime 0.002 (0.003)\tLoss 0.9362 (0.9707)\tPrec@1 61.719 (65.971)\n",
      "Epoch: [157][156/390]\tTime 0.002 (0.003)\tLoss 0.8165 (1.0000)\tPrec@1 67.969 (64.859)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.0113 (1.0236)\tPrec@1 61.719 (63.926)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.003)\tLoss 0.9982 (1.0383)\tPrec@1 70.312 (63.286)\n",
      "Epoch: [157][390/390]\tTime 0.002 (0.003)\tLoss 1.0817 (1.0474)\tPrec@1 57.500 (62.880)\n",
      "EPOCH: 157 train Results: Prec@1 62.880 Loss: 1.0474\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0465 (1.0465)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1724 (1.2409)\tPrec@1 43.750 (55.920)\n",
      "EPOCH: 157 val Results: Prec@1 55.920 Loss: 1.2409\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [158][0/390]\tTime 0.003 (0.003)\tLoss 0.8930 (0.8930)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [158][78/390]\tTime 0.003 (0.003)\tLoss 1.1330 (0.9784)\tPrec@1 57.031 (65.922)\n",
      "Epoch: [158][156/390]\tTime 0.002 (0.003)\tLoss 1.0802 (1.0012)\tPrec@1 64.844 (64.724)\n",
      "Epoch: [158][234/390]\tTime 0.002 (0.003)\tLoss 1.0516 (1.0261)\tPrec@1 64.844 (63.830)\n",
      "Epoch: [158][312/390]\tTime 0.003 (0.003)\tLoss 1.1098 (1.0367)\tPrec@1 57.812 (63.131)\n",
      "Epoch: [158][390/390]\tTime 0.003 (0.003)\tLoss 1.1138 (1.0510)\tPrec@1 56.250 (62.610)\n",
      "EPOCH: 158 train Results: Prec@1 62.610 Loss: 1.0510\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0859 (1.0859)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4610 (1.2544)\tPrec@1 56.250 (55.570)\n",
      "EPOCH: 158 val Results: Prec@1 55.570 Loss: 1.2544\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [159][0/390]\tTime 0.005 (0.005)\tLoss 0.9582 (0.9582)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [159][78/390]\tTime 0.003 (0.003)\tLoss 0.9578 (0.9768)\tPrec@1 64.844 (65.595)\n",
      "Epoch: [159][156/390]\tTime 0.005 (0.003)\tLoss 0.8983 (0.9998)\tPrec@1 68.750 (64.620)\n",
      "Epoch: [159][234/390]\tTime 0.002 (0.003)\tLoss 0.9113 (1.0191)\tPrec@1 66.406 (63.903)\n",
      "Epoch: [159][312/390]\tTime 0.002 (0.003)\tLoss 1.0120 (1.0364)\tPrec@1 59.375 (63.261)\n",
      "Epoch: [159][390/390]\tTime 0.003 (0.003)\tLoss 1.2200 (1.0489)\tPrec@1 56.250 (62.818)\n",
      "EPOCH: 159 train Results: Prec@1 62.818 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1780 (1.1780)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4234 (1.2566)\tPrec@1 31.250 (54.970)\n",
      "EPOCH: 159 val Results: Prec@1 54.970 Loss: 1.2566\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [160][0/390]\tTime 0.005 (0.005)\tLoss 0.9319 (0.9319)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [160][78/390]\tTime 0.002 (0.003)\tLoss 1.0406 (1.0033)\tPrec@1 65.625 (64.705)\n",
      "Epoch: [160][156/390]\tTime 0.002 (0.003)\tLoss 1.0621 (1.0104)\tPrec@1 63.281 (64.097)\n",
      "Epoch: [160][234/390]\tTime 0.005 (0.003)\tLoss 1.0014 (1.0297)\tPrec@1 65.625 (63.328)\n",
      "Epoch: [160][312/390]\tTime 0.009 (0.003)\tLoss 1.0168 (1.0400)\tPrec@1 63.281 (63.037)\n",
      "Epoch: [160][390/390]\tTime 0.003 (0.003)\tLoss 0.9658 (1.0441)\tPrec@1 70.000 (62.862)\n",
      "EPOCH: 160 train Results: Prec@1 62.862 Loss: 1.0441\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1265 (1.1265)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2179 (1.2561)\tPrec@1 75.000 (54.660)\n",
      "EPOCH: 160 val Results: Prec@1 54.660 Loss: 1.2561\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [161][0/390]\tTime 0.006 (0.006)\tLoss 0.9399 (0.9399)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [161][78/390]\tTime 0.004 (0.003)\tLoss 0.9124 (0.9657)\tPrec@1 66.406 (65.318)\n",
      "Epoch: [161][156/390]\tTime 0.004 (0.003)\tLoss 1.0368 (0.9966)\tPrec@1 65.625 (64.625)\n",
      "Epoch: [161][234/390]\tTime 0.005 (0.003)\tLoss 1.0072 (1.0178)\tPrec@1 62.500 (63.840)\n",
      "Epoch: [161][312/390]\tTime 0.003 (0.003)\tLoss 1.0369 (1.0302)\tPrec@1 64.844 (63.448)\n",
      "Epoch: [161][390/390]\tTime 0.002 (0.003)\tLoss 1.5501 (1.0466)\tPrec@1 42.500 (62.820)\n",
      "EPOCH: 161 train Results: Prec@1 62.820 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2091 (1.2091)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2627 (1.2418)\tPrec@1 43.750 (55.630)\n",
      "EPOCH: 161 val Results: Prec@1 55.630 Loss: 1.2418\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.8447 (0.8447)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [162][78/390]\tTime 0.003 (0.003)\tLoss 1.0451 (0.9804)\tPrec@1 64.844 (65.556)\n",
      "Epoch: [162][156/390]\tTime 0.003 (0.003)\tLoss 0.9330 (1.0099)\tPrec@1 67.188 (64.446)\n",
      "Epoch: [162][234/390]\tTime 0.002 (0.003)\tLoss 1.0354 (1.0200)\tPrec@1 65.625 (63.886)\n",
      "Epoch: [162][312/390]\tTime 0.003 (0.003)\tLoss 1.0808 (1.0350)\tPrec@1 62.500 (63.254)\n",
      "Epoch: [162][390/390]\tTime 0.001 (0.003)\tLoss 1.1795 (1.0460)\tPrec@1 56.250 (62.766)\n",
      "EPOCH: 162 train Results: Prec@1 62.766 Loss: 1.0460\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1169 (1.1169)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.8570 (1.2518)\tPrec@1 31.250 (55.910)\n",
      "EPOCH: 162 val Results: Prec@1 55.910 Loss: 1.2518\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [163][0/390]\tTime 0.005 (0.005)\tLoss 1.0921 (1.0921)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [163][78/390]\tTime 0.003 (0.003)\tLoss 0.8628 (0.9728)\tPrec@1 69.531 (65.239)\n",
      "Epoch: [163][156/390]\tTime 0.002 (0.003)\tLoss 1.0246 (0.9923)\tPrec@1 64.844 (64.585)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.003)\tLoss 1.0071 (1.0191)\tPrec@1 64.062 (63.607)\n",
      "Epoch: [163][312/390]\tTime 0.002 (0.003)\tLoss 1.1336 (1.0375)\tPrec@1 58.594 (62.974)\n",
      "Epoch: [163][390/390]\tTime 0.002 (0.003)\tLoss 1.0741 (1.0496)\tPrec@1 63.750 (62.448)\n",
      "EPOCH: 163 train Results: Prec@1 62.448 Loss: 1.0496\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1129 (1.1129)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3005 (1.2428)\tPrec@1 50.000 (56.220)\n",
      "EPOCH: 163 val Results: Prec@1 56.220 Loss: 1.2428\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [164][0/390]\tTime 0.002 (0.002)\tLoss 0.9328 (0.9328)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [164][78/390]\tTime 0.003 (0.003)\tLoss 1.0149 (0.9597)\tPrec@1 68.750 (66.100)\n",
      "Epoch: [164][156/390]\tTime 0.002 (0.003)\tLoss 1.0066 (0.9903)\tPrec@1 62.500 (65.098)\n",
      "Epoch: [164][234/390]\tTime 0.004 (0.003)\tLoss 1.0540 (1.0122)\tPrec@1 64.062 (64.139)\n",
      "Epoch: [164][312/390]\tTime 0.003 (0.003)\tLoss 1.2027 (1.0340)\tPrec@1 59.375 (63.349)\n",
      "Epoch: [164][390/390]\tTime 0.004 (0.003)\tLoss 1.0162 (1.0437)\tPrec@1 66.250 (63.088)\n",
      "EPOCH: 164 train Results: Prec@1 63.088 Loss: 1.0437\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1073 (1.1073)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3308 (1.2488)\tPrec@1 25.000 (55.550)\n",
      "EPOCH: 164 val Results: Prec@1 55.550 Loss: 1.2488\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [165][0/390]\tTime 0.004 (0.004)\tLoss 1.0223 (1.0223)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [165][78/390]\tTime 0.002 (0.003)\tLoss 0.9978 (0.9540)\tPrec@1 64.844 (66.199)\n",
      "Epoch: [165][156/390]\tTime 0.003 (0.003)\tLoss 1.0810 (0.9931)\tPrec@1 61.719 (64.699)\n",
      "Epoch: [165][234/390]\tTime 0.003 (0.003)\tLoss 1.2983 (1.0112)\tPrec@1 61.719 (64.176)\n",
      "Epoch: [165][312/390]\tTime 0.003 (0.003)\tLoss 1.1559 (1.0304)\tPrec@1 57.812 (63.294)\n",
      "Epoch: [165][390/390]\tTime 0.003 (0.003)\tLoss 1.3753 (1.0469)\tPrec@1 53.750 (62.726)\n",
      "EPOCH: 165 train Results: Prec@1 62.726 Loss: 1.0469\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1087 (1.1087)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4870 (1.2437)\tPrec@1 31.250 (55.870)\n",
      "EPOCH: 165 val Results: Prec@1 55.870 Loss: 1.2437\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [166][0/390]\tTime 0.004 (0.004)\tLoss 0.9328 (0.9328)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 0.9897 (0.9785)\tPrec@1 68.750 (65.615)\n",
      "Epoch: [166][156/390]\tTime 0.004 (0.003)\tLoss 1.1603 (1.0011)\tPrec@1 61.719 (64.724)\n",
      "Epoch: [166][234/390]\tTime 0.002 (0.003)\tLoss 1.1360 (1.0233)\tPrec@1 60.938 (64.059)\n",
      "Epoch: [166][312/390]\tTime 0.004 (0.003)\tLoss 1.0954 (1.0351)\tPrec@1 65.625 (63.359)\n",
      "Epoch: [166][390/390]\tTime 0.002 (0.003)\tLoss 1.1400 (1.0488)\tPrec@1 66.250 (62.938)\n",
      "EPOCH: 166 train Results: Prec@1 62.938 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1253 (1.1253)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2427 (1.2560)\tPrec@1 43.750 (55.710)\n",
      "EPOCH: 166 val Results: Prec@1 55.710 Loss: 1.2560\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [167][0/390]\tTime 0.002 (0.002)\tLoss 1.0941 (1.0941)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [167][78/390]\tTime 0.003 (0.003)\tLoss 1.0380 (0.9762)\tPrec@1 58.594 (65.843)\n",
      "Epoch: [167][156/390]\tTime 0.003 (0.003)\tLoss 1.0179 (0.9985)\tPrec@1 67.188 (64.779)\n",
      "Epoch: [167][234/390]\tTime 0.003 (0.003)\tLoss 1.0262 (1.0223)\tPrec@1 62.500 (63.777)\n",
      "Epoch: [167][312/390]\tTime 0.004 (0.003)\tLoss 1.1427 (1.0341)\tPrec@1 57.031 (63.331)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.003)\tLoss 1.1687 (1.0489)\tPrec@1 61.250 (62.776)\n",
      "EPOCH: 167 train Results: Prec@1 62.776 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1245 (1.1245)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2168 (1.2527)\tPrec@1 50.000 (55.610)\n",
      "EPOCH: 167 val Results: Prec@1 55.610 Loss: 1.2527\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [168][0/390]\tTime 0.004 (0.004)\tLoss 0.9400 (0.9400)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [168][78/390]\tTime 0.003 (0.003)\tLoss 0.8851 (0.9661)\tPrec@1 71.094 (66.159)\n",
      "Epoch: [168][156/390]\tTime 0.003 (0.003)\tLoss 0.9360 (1.0031)\tPrec@1 69.531 (64.545)\n",
      "Epoch: [168][234/390]\tTime 0.003 (0.003)\tLoss 1.1173 (1.0238)\tPrec@1 61.719 (63.610)\n",
      "Epoch: [168][312/390]\tTime 0.002 (0.003)\tLoss 1.0270 (1.0412)\tPrec@1 63.281 (62.984)\n",
      "Epoch: [168][390/390]\tTime 0.002 (0.003)\tLoss 1.0879 (1.0525)\tPrec@1 62.500 (62.482)\n",
      "EPOCH: 168 train Results: Prec@1 62.482 Loss: 1.0525\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0878 (1.0878)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6343 (1.2464)\tPrec@1 37.500 (55.460)\n",
      "EPOCH: 168 val Results: Prec@1 55.460 Loss: 1.2464\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [169][0/390]\tTime 0.006 (0.006)\tLoss 0.9461 (0.9461)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [169][78/390]\tTime 0.002 (0.003)\tLoss 0.9573 (0.9737)\tPrec@1 63.281 (65.694)\n",
      "Epoch: [169][156/390]\tTime 0.003 (0.003)\tLoss 0.9511 (0.9937)\tPrec@1 64.844 (65.033)\n",
      "Epoch: [169][234/390]\tTime 0.002 (0.003)\tLoss 0.9718 (1.0150)\tPrec@1 64.844 (64.003)\n",
      "Epoch: [169][312/390]\tTime 0.004 (0.003)\tLoss 1.0193 (1.0331)\tPrec@1 60.156 (63.239)\n",
      "Epoch: [169][390/390]\tTime 0.001 (0.003)\tLoss 1.0216 (1.0459)\tPrec@1 65.000 (62.794)\n",
      "EPOCH: 169 train Results: Prec@1 62.794 Loss: 1.0459\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1412 (1.1412)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1248 (1.2568)\tPrec@1 56.250 (55.200)\n",
      "EPOCH: 169 val Results: Prec@1 55.200 Loss: 1.2568\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [170][0/390]\tTime 0.003 (0.003)\tLoss 0.9778 (0.9778)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [170][78/390]\tTime 0.003 (0.003)\tLoss 1.0730 (0.9698)\tPrec@1 57.031 (65.882)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.003)\tLoss 0.8725 (0.9961)\tPrec@1 68.750 (64.903)\n",
      "Epoch: [170][234/390]\tTime 0.003 (0.003)\tLoss 1.1281 (1.0198)\tPrec@1 62.500 (63.906)\n",
      "Epoch: [170][312/390]\tTime 0.003 (0.003)\tLoss 1.0620 (1.0392)\tPrec@1 61.719 (63.179)\n",
      "Epoch: [170][390/390]\tTime 0.001 (0.003)\tLoss 1.0222 (1.0468)\tPrec@1 63.750 (62.900)\n",
      "EPOCH: 170 train Results: Prec@1 62.900 Loss: 1.0468\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1587 (1.1587)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1593 (1.2432)\tPrec@1 56.250 (55.700)\n",
      "EPOCH: 170 val Results: Prec@1 55.700 Loss: 1.2432\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [171][0/390]\tTime 0.006 (0.006)\tLoss 0.8857 (0.8857)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [171][78/390]\tTime 0.003 (0.003)\tLoss 0.9510 (0.9719)\tPrec@1 68.750 (66.139)\n",
      "Epoch: [171][156/390]\tTime 0.004 (0.003)\tLoss 1.1038 (1.0048)\tPrec@1 66.406 (64.898)\n",
      "Epoch: [171][234/390]\tTime 0.004 (0.003)\tLoss 0.9085 (1.0224)\tPrec@1 64.062 (63.946)\n",
      "Epoch: [171][312/390]\tTime 0.003 (0.003)\tLoss 0.9581 (1.0340)\tPrec@1 71.875 (63.666)\n",
      "Epoch: [171][390/390]\tTime 0.001 (0.003)\tLoss 1.2376 (1.0464)\tPrec@1 51.250 (63.026)\n",
      "EPOCH: 171 train Results: Prec@1 63.026 Loss: 1.0464\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1020 (1.1020)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.008 (0.001)\tLoss 1.6248 (1.2537)\tPrec@1 31.250 (55.890)\n",
      "EPOCH: 171 val Results: Prec@1 55.890 Loss: 1.2537\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [172][0/390]\tTime 0.005 (0.005)\tLoss 0.8201 (0.8201)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [172][78/390]\tTime 0.003 (0.004)\tLoss 0.8731 (0.9614)\tPrec@1 64.062 (66.634)\n",
      "Epoch: [172][156/390]\tTime 0.002 (0.003)\tLoss 1.0522 (0.9907)\tPrec@1 64.844 (65.038)\n",
      "Epoch: [172][234/390]\tTime 0.004 (0.003)\tLoss 1.1010 (1.0200)\tPrec@1 65.625 (64.049)\n",
      "Epoch: [172][312/390]\tTime 0.003 (0.003)\tLoss 1.1422 (1.0349)\tPrec@1 60.156 (63.379)\n",
      "Epoch: [172][390/390]\tTime 0.002 (0.003)\tLoss 1.1942 (1.0472)\tPrec@1 62.500 (62.898)\n",
      "EPOCH: 172 train Results: Prec@1 62.898 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1276 (1.1276)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5709 (1.2412)\tPrec@1 43.750 (55.700)\n",
      "EPOCH: 172 val Results: Prec@1 55.700 Loss: 1.2412\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [173][0/390]\tTime 0.004 (0.004)\tLoss 1.0277 (1.0277)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [173][78/390]\tTime 0.002 (0.003)\tLoss 1.1934 (0.9924)\tPrec@1 60.156 (64.775)\n",
      "Epoch: [173][156/390]\tTime 0.002 (0.003)\tLoss 1.0398 (1.0049)\tPrec@1 65.625 (64.416)\n",
      "Epoch: [173][234/390]\tTime 0.003 (0.003)\tLoss 0.9578 (1.0158)\tPrec@1 66.406 (63.840)\n",
      "Epoch: [173][312/390]\tTime 0.002 (0.003)\tLoss 1.0942 (1.0307)\tPrec@1 62.500 (63.384)\n",
      "Epoch: [173][390/390]\tTime 0.001 (0.003)\tLoss 1.0194 (1.0420)\tPrec@1 66.250 (62.966)\n",
      "EPOCH: 173 train Results: Prec@1 62.966 Loss: 1.0420\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1224 (1.1224)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3836 (1.2625)\tPrec@1 56.250 (55.200)\n",
      "EPOCH: 173 val Results: Prec@1 55.200 Loss: 1.2625\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [174][0/390]\tTime 0.003 (0.003)\tLoss 0.9796 (0.9796)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [174][78/390]\tTime 0.002 (0.003)\tLoss 0.9132 (0.9665)\tPrec@1 64.062 (66.268)\n",
      "Epoch: [174][156/390]\tTime 0.005 (0.003)\tLoss 0.9287 (0.9938)\tPrec@1 67.969 (65.093)\n",
      "Epoch: [174][234/390]\tTime 0.004 (0.003)\tLoss 1.0511 (1.0114)\tPrec@1 67.969 (64.432)\n",
      "Epoch: [174][312/390]\tTime 0.003 (0.003)\tLoss 1.0833 (1.0297)\tPrec@1 64.844 (63.718)\n",
      "Epoch: [174][390/390]\tTime 0.006 (0.003)\tLoss 1.2148 (1.0454)\tPrec@1 60.000 (63.070)\n",
      "EPOCH: 174 train Results: Prec@1 63.070 Loss: 1.0454\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1013 (1.1013)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5803 (1.2563)\tPrec@1 43.750 (56.160)\n",
      "EPOCH: 174 val Results: Prec@1 56.160 Loss: 1.2563\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [175][0/390]\tTime 0.003 (0.003)\tLoss 0.9380 (0.9380)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [175][78/390]\tTime 0.003 (0.003)\tLoss 1.1188 (0.9728)\tPrec@1 59.375 (65.902)\n",
      "Epoch: [175][156/390]\tTime 0.002 (0.003)\tLoss 1.0672 (0.9933)\tPrec@1 64.062 (64.794)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.003)\tLoss 1.0997 (1.0163)\tPrec@1 59.375 (64.116)\n",
      "Epoch: [175][312/390]\tTime 0.004 (0.003)\tLoss 1.1252 (1.0325)\tPrec@1 60.156 (63.626)\n",
      "Epoch: [175][390/390]\tTime 0.001 (0.003)\tLoss 1.0761 (1.0472)\tPrec@1 61.250 (62.920)\n",
      "EPOCH: 175 train Results: Prec@1 62.920 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1492 (1.1492)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3397 (1.2638)\tPrec@1 43.750 (55.310)\n",
      "EPOCH: 175 val Results: Prec@1 55.310 Loss: 1.2638\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [176][0/390]\tTime 0.002 (0.002)\tLoss 1.0720 (1.0720)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [176][78/390]\tTime 0.003 (0.003)\tLoss 1.1627 (0.9914)\tPrec@1 61.719 (65.269)\n",
      "Epoch: [176][156/390]\tTime 0.003 (0.003)\tLoss 1.0145 (1.0125)\tPrec@1 61.719 (64.326)\n",
      "Epoch: [176][234/390]\tTime 0.005 (0.003)\tLoss 1.0383 (1.0296)\tPrec@1 61.719 (63.660)\n",
      "Epoch: [176][312/390]\tTime 0.010 (0.003)\tLoss 1.0174 (1.0396)\tPrec@1 62.500 (63.191)\n",
      "Epoch: [176][390/390]\tTime 0.002 (0.003)\tLoss 1.0484 (1.0488)\tPrec@1 57.500 (62.848)\n",
      "EPOCH: 176 train Results: Prec@1 62.848 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0743 (1.0743)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4634 (1.2524)\tPrec@1 37.500 (55.870)\n",
      "EPOCH: 176 val Results: Prec@1 55.870 Loss: 1.2524\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [177][0/390]\tTime 0.007 (0.007)\tLoss 1.0310 (1.0310)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [177][78/390]\tTime 0.002 (0.004)\tLoss 0.8249 (0.9678)\tPrec@1 68.750 (66.426)\n",
      "Epoch: [177][156/390]\tTime 0.002 (0.004)\tLoss 1.1410 (1.0010)\tPrec@1 55.469 (64.759)\n",
      "Epoch: [177][234/390]\tTime 0.003 (0.003)\tLoss 1.0725 (1.0182)\tPrec@1 56.250 (63.906)\n",
      "Epoch: [177][312/390]\tTime 0.003 (0.003)\tLoss 1.0462 (1.0326)\tPrec@1 63.281 (63.351)\n",
      "Epoch: [177][390/390]\tTime 0.001 (0.003)\tLoss 1.0632 (1.0445)\tPrec@1 67.500 (63.078)\n",
      "EPOCH: 177 train Results: Prec@1 63.078 Loss: 1.0445\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1761 (1.1761)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5163 (1.2609)\tPrec@1 25.000 (55.580)\n",
      "EPOCH: 177 val Results: Prec@1 55.580 Loss: 1.2609\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [178][0/390]\tTime 0.004 (0.004)\tLoss 1.0442 (1.0442)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [178][78/390]\tTime 0.002 (0.003)\tLoss 0.9195 (0.9842)\tPrec@1 64.844 (65.249)\n",
      "Epoch: [178][156/390]\tTime 0.002 (0.003)\tLoss 1.1793 (1.0092)\tPrec@1 58.594 (64.023)\n",
      "Epoch: [178][234/390]\tTime 0.003 (0.003)\tLoss 1.1790 (1.0274)\tPrec@1 59.375 (63.241)\n",
      "Epoch: [178][312/390]\tTime 0.003 (0.003)\tLoss 1.0554 (1.0408)\tPrec@1 57.031 (62.814)\n",
      "Epoch: [178][390/390]\tTime 0.002 (0.003)\tLoss 1.1016 (1.0512)\tPrec@1 62.500 (62.500)\n",
      "EPOCH: 178 train Results: Prec@1 62.500 Loss: 1.0512\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1458 (1.1458)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2778 (1.2588)\tPrec@1 50.000 (55.320)\n",
      "EPOCH: 178 val Results: Prec@1 55.320 Loss: 1.2588\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [179][0/390]\tTime 0.006 (0.006)\tLoss 1.1130 (1.1130)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [179][78/390]\tTime 0.002 (0.003)\tLoss 1.0834 (0.9741)\tPrec@1 62.500 (65.783)\n",
      "Epoch: [179][156/390]\tTime 0.003 (0.003)\tLoss 1.1344 (1.0016)\tPrec@1 62.500 (64.665)\n",
      "Epoch: [179][234/390]\tTime 0.006 (0.003)\tLoss 1.2396 (1.0285)\tPrec@1 51.562 (63.587)\n",
      "Epoch: [179][312/390]\tTime 0.005 (0.003)\tLoss 0.9430 (1.0386)\tPrec@1 60.156 (63.129)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.2663 (1.0497)\tPrec@1 60.000 (62.744)\n",
      "EPOCH: 179 train Results: Prec@1 62.744 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0966 (1.0966)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4226 (1.2616)\tPrec@1 43.750 (55.190)\n",
      "EPOCH: 179 val Results: Prec@1 55.190 Loss: 1.2616\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [180][0/390]\tTime 0.006 (0.006)\tLoss 0.9571 (0.9571)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [180][78/390]\tTime 0.002 (0.003)\tLoss 0.9628 (0.9794)\tPrec@1 64.844 (65.269)\n",
      "Epoch: [180][156/390]\tTime 0.002 (0.003)\tLoss 1.0035 (0.9967)\tPrec@1 61.719 (64.635)\n",
      "Epoch: [180][234/390]\tTime 0.002 (0.003)\tLoss 1.0097 (1.0198)\tPrec@1 60.938 (63.836)\n",
      "Epoch: [180][312/390]\tTime 0.003 (0.003)\tLoss 0.9995 (1.0356)\tPrec@1 62.500 (63.246)\n",
      "Epoch: [180][390/390]\tTime 0.008 (0.003)\tLoss 1.2200 (1.0472)\tPrec@1 51.250 (62.786)\n",
      "EPOCH: 180 train Results: Prec@1 62.786 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1056 (1.1056)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3170 (1.2598)\tPrec@1 56.250 (55.170)\n",
      "EPOCH: 180 val Results: Prec@1 55.170 Loss: 1.2598\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [181][0/390]\tTime 0.003 (0.003)\tLoss 0.9139 (0.9139)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [181][78/390]\tTime 0.002 (0.003)\tLoss 0.9497 (0.9729)\tPrec@1 67.969 (65.398)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 0.8538 (0.9989)\tPrec@1 70.312 (64.053)\n",
      "Epoch: [181][234/390]\tTime 0.003 (0.003)\tLoss 1.0173 (1.0230)\tPrec@1 64.844 (63.314)\n",
      "Epoch: [181][312/390]\tTime 0.002 (0.003)\tLoss 1.1749 (1.0367)\tPrec@1 63.281 (62.827)\n",
      "Epoch: [181][390/390]\tTime 0.001 (0.003)\tLoss 1.1375 (1.0481)\tPrec@1 58.750 (62.540)\n",
      "EPOCH: 181 train Results: Prec@1 62.540 Loss: 1.0481\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0585 (1.0585)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2523 (1.2504)\tPrec@1 37.500 (55.400)\n",
      "EPOCH: 181 val Results: Prec@1 55.400 Loss: 1.2504\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [182][0/390]\tTime 0.002 (0.002)\tLoss 1.0420 (1.0420)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [182][78/390]\tTime 0.003 (0.003)\tLoss 1.1415 (0.9616)\tPrec@1 57.812 (66.070)\n",
      "Epoch: [182][156/390]\tTime 0.003 (0.003)\tLoss 1.0642 (0.9933)\tPrec@1 66.406 (64.849)\n",
      "Epoch: [182][234/390]\tTime 0.004 (0.003)\tLoss 1.1778 (1.0185)\tPrec@1 55.469 (63.836)\n",
      "Epoch: [182][312/390]\tTime 0.002 (0.003)\tLoss 1.0654 (1.0327)\tPrec@1 59.375 (63.344)\n",
      "Epoch: [182][390/390]\tTime 0.002 (0.003)\tLoss 1.2314 (1.0425)\tPrec@1 50.000 (62.918)\n",
      "EPOCH: 182 train Results: Prec@1 62.918 Loss: 1.0425\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1319 (1.1319)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4710 (1.2556)\tPrec@1 37.500 (55.530)\n",
      "EPOCH: 182 val Results: Prec@1 55.530 Loss: 1.2556\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [183][0/390]\tTime 0.006 (0.006)\tLoss 0.9138 (0.9138)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [183][78/390]\tTime 0.002 (0.003)\tLoss 1.0230 (0.9693)\tPrec@1 59.375 (65.655)\n",
      "Epoch: [183][156/390]\tTime 0.003 (0.003)\tLoss 0.9735 (0.9914)\tPrec@1 67.969 (64.779)\n",
      "Epoch: [183][234/390]\tTime 0.004 (0.003)\tLoss 1.0751 (1.0152)\tPrec@1 60.156 (63.933)\n",
      "Epoch: [183][312/390]\tTime 0.002 (0.003)\tLoss 1.0940 (1.0297)\tPrec@1 59.375 (63.294)\n",
      "Epoch: [183][390/390]\tTime 0.002 (0.003)\tLoss 1.1205 (1.0461)\tPrec@1 60.000 (62.844)\n",
      "EPOCH: 183 train Results: Prec@1 62.844 Loss: 1.0461\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1055 (1.1055)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6359 (1.2553)\tPrec@1 25.000 (55.550)\n",
      "EPOCH: 183 val Results: Prec@1 55.550 Loss: 1.2553\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [184][0/390]\tTime 0.004 (0.004)\tLoss 0.8062 (0.8062)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [184][78/390]\tTime 0.003 (0.003)\tLoss 1.0619 (0.9591)\tPrec@1 65.625 (66.525)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.003)\tLoss 1.0596 (0.9905)\tPrec@1 64.062 (64.993)\n",
      "Epoch: [184][234/390]\tTime 0.002 (0.003)\tLoss 1.1303 (1.0189)\tPrec@1 58.594 (63.989)\n",
      "Epoch: [184][312/390]\tTime 0.002 (0.003)\tLoss 1.1243 (1.0371)\tPrec@1 57.031 (63.279)\n",
      "Epoch: [184][390/390]\tTime 0.003 (0.003)\tLoss 1.3938 (1.0474)\tPrec@1 57.500 (62.824)\n",
      "EPOCH: 184 train Results: Prec@1 62.824 Loss: 1.0474\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0825 (1.0825)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2809 (1.2525)\tPrec@1 37.500 (55.940)\n",
      "EPOCH: 184 val Results: Prec@1 55.940 Loss: 1.2525\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [185][0/390]\tTime 0.003 (0.003)\tLoss 0.9649 (0.9649)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [185][78/390]\tTime 0.005 (0.003)\tLoss 1.0796 (0.9818)\tPrec@1 57.031 (65.091)\n",
      "Epoch: [185][156/390]\tTime 0.002 (0.003)\tLoss 0.9345 (1.0005)\tPrec@1 70.312 (64.371)\n",
      "Epoch: [185][234/390]\tTime 0.003 (0.003)\tLoss 1.0679 (1.0111)\tPrec@1 62.500 (64.059)\n",
      "Epoch: [185][312/390]\tTime 0.003 (0.003)\tLoss 0.9294 (1.0268)\tPrec@1 65.625 (63.521)\n",
      "Epoch: [185][390/390]\tTime 0.007 (0.003)\tLoss 1.1245 (1.0470)\tPrec@1 56.250 (62.688)\n",
      "EPOCH: 185 train Results: Prec@1 62.688 Loss: 1.0470\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1204 (1.1204)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2590 (1.2654)\tPrec@1 50.000 (55.290)\n",
      "EPOCH: 185 val Results: Prec@1 55.290 Loss: 1.2654\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [186][0/390]\tTime 0.003 (0.003)\tLoss 0.9816 (0.9816)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [186][78/390]\tTime 0.003 (0.003)\tLoss 0.9760 (0.9799)\tPrec@1 66.406 (65.674)\n",
      "Epoch: [186][156/390]\tTime 0.002 (0.003)\tLoss 0.9215 (1.0024)\tPrec@1 62.500 (64.620)\n",
      "Epoch: [186][234/390]\tTime 0.002 (0.003)\tLoss 1.0623 (1.0230)\tPrec@1 60.938 (63.816)\n",
      "Epoch: [186][312/390]\tTime 0.009 (0.003)\tLoss 1.0597 (1.0402)\tPrec@1 65.625 (63.014)\n",
      "Epoch: [186][390/390]\tTime 0.003 (0.003)\tLoss 0.9753 (1.0470)\tPrec@1 66.250 (62.686)\n",
      "EPOCH: 186 train Results: Prec@1 62.686 Loss: 1.0470\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0657 (1.0657)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5094 (1.2667)\tPrec@1 37.500 (55.110)\n",
      "EPOCH: 186 val Results: Prec@1 55.110 Loss: 1.2667\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [187][0/390]\tTime 0.004 (0.004)\tLoss 1.0221 (1.0221)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [187][78/390]\tTime 0.003 (0.003)\tLoss 0.9606 (0.9698)\tPrec@1 70.312 (66.021)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.2455 (0.9916)\tPrec@1 50.000 (64.928)\n",
      "Epoch: [187][234/390]\tTime 0.003 (0.003)\tLoss 1.0200 (1.0167)\tPrec@1 65.625 (63.906)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.1157 (1.0356)\tPrec@1 60.938 (63.154)\n",
      "Epoch: [187][390/390]\tTime 0.003 (0.003)\tLoss 1.1646 (1.0478)\tPrec@1 43.750 (62.742)\n",
      "EPOCH: 187 train Results: Prec@1 62.742 Loss: 1.0478\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1042 (1.1042)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3300 (1.2547)\tPrec@1 50.000 (55.520)\n",
      "EPOCH: 187 val Results: Prec@1 55.520 Loss: 1.2547\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [188][0/390]\tTime 0.004 (0.004)\tLoss 0.8969 (0.8969)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [188][78/390]\tTime 0.005 (0.003)\tLoss 1.1906 (0.9580)\tPrec@1 57.031 (66.495)\n",
      "Epoch: [188][156/390]\tTime 0.004 (0.003)\tLoss 1.0506 (0.9908)\tPrec@1 64.844 (65.152)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.003)\tLoss 1.1124 (1.0102)\tPrec@1 56.250 (64.355)\n",
      "Epoch: [188][312/390]\tTime 0.005 (0.003)\tLoss 1.2388 (1.0274)\tPrec@1 60.156 (63.626)\n",
      "Epoch: [188][390/390]\tTime 0.003 (0.003)\tLoss 1.0935 (1.0428)\tPrec@1 61.250 (62.920)\n",
      "EPOCH: 188 train Results: Prec@1 62.920 Loss: 1.0428\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1313 (1.1313)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2982 (1.2540)\tPrec@1 50.000 (55.790)\n",
      "EPOCH: 188 val Results: Prec@1 55.790 Loss: 1.2540\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [189][0/390]\tTime 0.004 (0.004)\tLoss 1.0129 (1.0129)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.004)\tLoss 0.9230 (0.9637)\tPrec@1 72.656 (65.862)\n",
      "Epoch: [189][156/390]\tTime 0.003 (0.003)\tLoss 1.2959 (0.9982)\tPrec@1 55.469 (64.585)\n",
      "Epoch: [189][234/390]\tTime 0.003 (0.003)\tLoss 1.0847 (1.0120)\tPrec@1 63.281 (63.936)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.003)\tLoss 1.1934 (1.0294)\tPrec@1 56.250 (63.216)\n",
      "Epoch: [189][390/390]\tTime 0.001 (0.003)\tLoss 1.2625 (1.0437)\tPrec@1 51.250 (62.784)\n",
      "EPOCH: 189 train Results: Prec@1 62.784 Loss: 1.0437\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1279 (1.1279)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5225 (1.2483)\tPrec@1 50.000 (55.990)\n",
      "EPOCH: 189 val Results: Prec@1 55.990 Loss: 1.2483\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [190][0/390]\tTime 0.005 (0.005)\tLoss 0.8330 (0.8330)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.003)\tLoss 0.9152 (0.9669)\tPrec@1 64.062 (66.050)\n",
      "Epoch: [190][156/390]\tTime 0.002 (0.003)\tLoss 1.1347 (0.9924)\tPrec@1 58.594 (64.903)\n",
      "Epoch: [190][234/390]\tTime 0.003 (0.003)\tLoss 0.9945 (1.0181)\tPrec@1 67.188 (63.896)\n",
      "Epoch: [190][312/390]\tTime 0.002 (0.003)\tLoss 0.9400 (1.0351)\tPrec@1 64.062 (63.341)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.003)\tLoss 1.2552 (1.0497)\tPrec@1 55.000 (62.694)\n",
      "EPOCH: 190 train Results: Prec@1 62.694 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1591 (1.1591)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6958 (1.2511)\tPrec@1 37.500 (55.280)\n",
      "EPOCH: 190 val Results: Prec@1 55.280 Loss: 1.2511\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [191][0/390]\tTime 0.002 (0.002)\tLoss 0.9930 (0.9930)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [191][78/390]\tTime 0.004 (0.003)\tLoss 0.9274 (0.9745)\tPrec@1 69.531 (65.813)\n",
      "Epoch: [191][156/390]\tTime 0.002 (0.003)\tLoss 0.9955 (0.9990)\tPrec@1 64.844 (64.864)\n",
      "Epoch: [191][234/390]\tTime 0.008 (0.003)\tLoss 0.9437 (1.0219)\tPrec@1 65.625 (63.959)\n",
      "Epoch: [191][312/390]\tTime 0.003 (0.003)\tLoss 0.9932 (1.0340)\tPrec@1 62.500 (63.354)\n",
      "Epoch: [191][390/390]\tTime 0.010 (0.003)\tLoss 1.1576 (1.0478)\tPrec@1 57.500 (62.730)\n",
      "EPOCH: 191 train Results: Prec@1 62.730 Loss: 1.0478\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0531 (1.0531)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3600 (1.2481)\tPrec@1 56.250 (55.860)\n",
      "EPOCH: 191 val Results: Prec@1 55.860 Loss: 1.2481\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [192][0/390]\tTime 0.002 (0.002)\tLoss 0.7903 (0.7903)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [192][78/390]\tTime 0.002 (0.003)\tLoss 1.0245 (0.9830)\tPrec@1 60.938 (64.992)\n",
      "Epoch: [192][156/390]\tTime 0.003 (0.003)\tLoss 0.9877 (1.0004)\tPrec@1 65.625 (64.341)\n",
      "Epoch: [192][234/390]\tTime 0.002 (0.003)\tLoss 1.0540 (1.0242)\tPrec@1 64.062 (63.590)\n",
      "Epoch: [192][312/390]\tTime 0.004 (0.003)\tLoss 1.1390 (1.0338)\tPrec@1 54.688 (63.216)\n",
      "Epoch: [192][390/390]\tTime 0.002 (0.003)\tLoss 0.9822 (1.0444)\tPrec@1 65.000 (62.898)\n",
      "EPOCH: 192 train Results: Prec@1 62.898 Loss: 1.0444\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1119 (1.1119)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9618 (1.2557)\tPrec@1 75.000 (55.700)\n",
      "EPOCH: 192 val Results: Prec@1 55.700 Loss: 1.2557\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [193][0/390]\tTime 0.005 (0.005)\tLoss 0.8345 (0.8345)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [193][78/390]\tTime 0.003 (0.003)\tLoss 0.9295 (0.9751)\tPrec@1 64.844 (65.556)\n",
      "Epoch: [193][156/390]\tTime 0.002 (0.003)\tLoss 0.9883 (1.0006)\tPrec@1 64.062 (64.545)\n",
      "Epoch: [193][234/390]\tTime 0.003 (0.003)\tLoss 1.0185 (1.0195)\tPrec@1 64.844 (63.753)\n",
      "Epoch: [193][312/390]\tTime 0.009 (0.003)\tLoss 1.2772 (1.0356)\tPrec@1 57.031 (63.236)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.003)\tLoss 1.0590 (1.0447)\tPrec@1 58.750 (62.936)\n",
      "EPOCH: 193 train Results: Prec@1 62.936 Loss: 1.0447\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1199 (1.1199)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1907 (1.2537)\tPrec@1 37.500 (55.890)\n",
      "EPOCH: 193 val Results: Prec@1 55.890 Loss: 1.2537\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [194][0/390]\tTime 0.003 (0.003)\tLoss 0.9055 (0.9055)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [194][78/390]\tTime 0.002 (0.003)\tLoss 0.9322 (0.9796)\tPrec@1 67.188 (65.526)\n",
      "Epoch: [194][156/390]\tTime 0.004 (0.003)\tLoss 0.9899 (0.9995)\tPrec@1 62.500 (64.505)\n",
      "Epoch: [194][234/390]\tTime 0.002 (0.003)\tLoss 1.0399 (1.0203)\tPrec@1 64.844 (63.604)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.003)\tLoss 1.0345 (1.0347)\tPrec@1 59.375 (63.174)\n",
      "Epoch: [194][390/390]\tTime 0.002 (0.003)\tLoss 1.1787 (1.0445)\tPrec@1 58.750 (62.766)\n",
      "EPOCH: 194 train Results: Prec@1 62.766 Loss: 1.0445\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1332 (1.1332)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2050 (1.2472)\tPrec@1 50.000 (55.560)\n",
      "EPOCH: 194 val Results: Prec@1 55.560 Loss: 1.2472\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [195][0/390]\tTime 0.002 (0.002)\tLoss 0.9004 (0.9004)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [195][78/390]\tTime 0.002 (0.003)\tLoss 0.9925 (0.9635)\tPrec@1 62.500 (66.011)\n",
      "Epoch: [195][156/390]\tTime 0.002 (0.003)\tLoss 0.9735 (0.9900)\tPrec@1 67.969 (64.894)\n",
      "Epoch: [195][234/390]\tTime 0.002 (0.003)\tLoss 1.1091 (1.0108)\tPrec@1 62.500 (64.245)\n",
      "Epoch: [195][312/390]\tTime 0.002 (0.003)\tLoss 1.2993 (1.0287)\tPrec@1 57.812 (63.706)\n",
      "Epoch: [195][390/390]\tTime 0.001 (0.003)\tLoss 1.0762 (1.0370)\tPrec@1 65.000 (63.392)\n",
      "EPOCH: 195 train Results: Prec@1 63.392 Loss: 1.0370\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1353 (1.1353)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4918 (1.2603)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 195 val Results: Prec@1 55.150 Loss: 1.2603\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [196][0/390]\tTime 0.005 (0.005)\tLoss 0.9135 (0.9135)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [196][78/390]\tTime 0.009 (0.003)\tLoss 1.0043 (0.9699)\tPrec@1 60.156 (65.002)\n",
      "Epoch: [196][156/390]\tTime 0.005 (0.003)\tLoss 1.2323 (0.9926)\tPrec@1 59.375 (64.675)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.003)\tLoss 0.9840 (1.0238)\tPrec@1 64.062 (63.570)\n",
      "Epoch: [196][312/390]\tTime 0.004 (0.003)\tLoss 1.1704 (1.0368)\tPrec@1 57.031 (63.059)\n",
      "Epoch: [196][390/390]\tTime 0.003 (0.003)\tLoss 0.9349 (1.0443)\tPrec@1 62.500 (62.736)\n",
      "EPOCH: 196 train Results: Prec@1 62.736 Loss: 1.0443\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0959 (1.0959)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3160 (1.2418)\tPrec@1 43.750 (56.180)\n",
      "EPOCH: 196 val Results: Prec@1 56.180 Loss: 1.2418\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [197][0/390]\tTime 0.003 (0.003)\tLoss 0.8713 (0.8713)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [197][78/390]\tTime 0.002 (0.003)\tLoss 0.9768 (0.9515)\tPrec@1 66.406 (66.070)\n",
      "Epoch: [197][156/390]\tTime 0.004 (0.003)\tLoss 0.9895 (0.9817)\tPrec@1 64.062 (65.381)\n",
      "Epoch: [197][234/390]\tTime 0.002 (0.003)\tLoss 1.0439 (1.0109)\tPrec@1 67.969 (64.192)\n",
      "Epoch: [197][312/390]\tTime 0.002 (0.003)\tLoss 1.0722 (1.0338)\tPrec@1 57.812 (63.429)\n",
      "Epoch: [197][390/390]\tTime 0.001 (0.003)\tLoss 1.0184 (1.0464)\tPrec@1 56.250 (62.916)\n",
      "EPOCH: 197 train Results: Prec@1 62.916 Loss: 1.0464\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2252 (1.2252)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.005 (0.001)\tLoss 1.2355 (1.2554)\tPrec@1 43.750 (55.610)\n",
      "EPOCH: 197 val Results: Prec@1 55.610 Loss: 1.2554\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [198][0/390]\tTime 0.006 (0.006)\tLoss 0.9913 (0.9913)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [198][78/390]\tTime 0.004 (0.004)\tLoss 0.9574 (0.9690)\tPrec@1 65.625 (65.309)\n",
      "Epoch: [198][156/390]\tTime 0.003 (0.003)\tLoss 0.8438 (0.9934)\tPrec@1 68.750 (64.640)\n",
      "Epoch: [198][234/390]\tTime 0.002 (0.003)\tLoss 1.3072 (1.0149)\tPrec@1 59.375 (64.089)\n",
      "Epoch: [198][312/390]\tTime 0.003 (0.003)\tLoss 0.9062 (1.0295)\tPrec@1 67.969 (63.516)\n",
      "Epoch: [198][390/390]\tTime 0.001 (0.003)\tLoss 1.0523 (1.0414)\tPrec@1 58.750 (63.150)\n",
      "EPOCH: 198 train Results: Prec@1 63.150 Loss: 1.0414\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1853 (1.1853)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2713 (1.2585)\tPrec@1 50.000 (54.750)\n",
      "EPOCH: 198 val Results: Prec@1 54.750 Loss: 1.2585\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [199][0/390]\tTime 0.003 (0.003)\tLoss 0.9385 (0.9385)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [199][78/390]\tTime 0.002 (0.003)\tLoss 0.9224 (0.9600)\tPrec@1 66.406 (66.337)\n",
      "Epoch: [199][156/390]\tTime 0.003 (0.003)\tLoss 1.0251 (0.9838)\tPrec@1 64.062 (65.312)\n",
      "Epoch: [199][234/390]\tTime 0.007 (0.003)\tLoss 1.0470 (1.0087)\tPrec@1 60.156 (64.232)\n",
      "Epoch: [199][312/390]\tTime 0.003 (0.003)\tLoss 1.1340 (1.0278)\tPrec@1 61.719 (63.503)\n",
      "Epoch: [199][390/390]\tTime 0.001 (0.003)\tLoss 1.0075 (1.0396)\tPrec@1 67.500 (63.102)\n",
      "EPOCH: 199 train Results: Prec@1 63.102 Loss: 1.0396\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1861 (1.1861)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2950 (1.2502)\tPrec@1 43.750 (55.730)\n",
      "EPOCH: 199 val Results: Prec@1 55.730 Loss: 1.2502\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [200][0/390]\tTime 0.004 (0.004)\tLoss 0.8784 (0.8784)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [200][78/390]\tTime 0.003 (0.003)\tLoss 0.9935 (0.9716)\tPrec@1 65.625 (65.724)\n",
      "Epoch: [200][156/390]\tTime 0.003 (0.003)\tLoss 0.8013 (1.0008)\tPrec@1 70.312 (64.366)\n",
      "Epoch: [200][234/390]\tTime 0.002 (0.003)\tLoss 1.0529 (1.0167)\tPrec@1 60.938 (63.747)\n",
      "Epoch: [200][312/390]\tTime 0.003 (0.003)\tLoss 1.2514 (1.0276)\tPrec@1 52.344 (63.321)\n",
      "Epoch: [200][390/390]\tTime 0.002 (0.003)\tLoss 1.2870 (1.0412)\tPrec@1 52.500 (62.850)\n",
      "EPOCH: 200 train Results: Prec@1 62.850 Loss: 1.0412\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1336 (1.1336)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0008 (1.2516)\tPrec@1 75.000 (55.740)\n",
      "EPOCH: 200 val Results: Prec@1 55.740 Loss: 1.2516\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "End time:  Thu Apr  4 23:08:58 2024\n",
      "train executed in 258.5027 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.005, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer4 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:08:58 2024\n",
      "current lr 5.00000e-02\n",
      "Epoch: [1][0/390]\tTime 0.007 (0.007)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.002 (0.003)\tLoss 1.8188 (2.5696)\tPrec@1 35.938 (29.114)\n",
      "Epoch: [1][156/390]\tTime 0.002 (0.003)\tLoss 1.9599 (2.1665)\tPrec@1 29.688 (33.435)\n",
      "Epoch: [1][234/390]\tTime 0.003 (0.003)\tLoss 1.6756 (2.0000)\tPrec@1 46.094 (35.881)\n",
      "Epoch: [1][312/390]\tTime 0.006 (0.003)\tLoss 1.6016 (1.9032)\tPrec@1 42.188 (37.468)\n",
      "Epoch: [1][390/390]\tTime 0.001 (0.003)\tLoss 1.5402 (1.8414)\tPrec@1 41.250 (38.444)\n",
      "EPOCH: 1 train Results: Prec@1 38.444 Loss: 1.8414\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.3736 (1.3736)\tPrec@1 48.438 (48.438)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4550 (1.4873)\tPrec@1 37.500 (46.810)\n",
      "EPOCH: 1 val Results: Prec@1 46.810 Loss: 1.4873\n",
      "Best Prec@1: 46.810\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [2][0/390]\tTime 0.003 (0.003)\tLoss 1.5012 (1.5012)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [2][78/390]\tTime 0.002 (0.004)\tLoss 1.3811 (1.4727)\tPrec@1 52.344 (47.330)\n",
      "Epoch: [2][156/390]\tTime 0.002 (0.004)\tLoss 1.5013 (1.4606)\tPrec@1 43.750 (47.666)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.003)\tLoss 1.5439 (1.4596)\tPrec@1 44.531 (47.779)\n",
      "Epoch: [2][312/390]\tTime 0.002 (0.003)\tLoss 1.6764 (1.4544)\tPrec@1 36.719 (47.816)\n",
      "Epoch: [2][390/390]\tTime 0.001 (0.003)\tLoss 1.3668 (1.4469)\tPrec@1 48.750 (48.134)\n",
      "EPOCH: 2 train Results: Prec@1 48.134 Loss: 1.4469\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2507 (1.2507)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5007 (1.3926)\tPrec@1 25.000 (49.840)\n",
      "EPOCH: 2 val Results: Prec@1 49.840 Loss: 1.3926\n",
      "Best Prec@1: 49.840\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [3][0/390]\tTime 0.002 (0.002)\tLoss 1.3419 (1.3419)\tPrec@1 45.312 (45.312)\n",
      "Epoch: [3][78/390]\tTime 0.002 (0.003)\tLoss 1.3687 (1.3578)\tPrec@1 51.562 (51.315)\n",
      "Epoch: [3][156/390]\tTime 0.004 (0.003)\tLoss 1.3243 (1.3509)\tPrec@1 50.781 (51.602)\n",
      "Epoch: [3][234/390]\tTime 0.003 (0.003)\tLoss 1.4300 (1.3571)\tPrec@1 46.875 (51.556)\n",
      "Epoch: [3][312/390]\tTime 0.003 (0.003)\tLoss 1.4384 (1.3595)\tPrec@1 46.875 (51.398)\n",
      "Epoch: [3][390/390]\tTime 0.001 (0.003)\tLoss 1.4307 (1.3622)\tPrec@1 52.500 (51.314)\n",
      "EPOCH: 3 train Results: Prec@1 51.314 Loss: 1.3622\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1742 (1.1742)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2618 (1.3461)\tPrec@1 37.500 (51.470)\n",
      "EPOCH: 3 val Results: Prec@1 51.470 Loss: 1.3461\n",
      "Best Prec@1: 51.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [4][0/390]\tTime 0.003 (0.003)\tLoss 1.3356 (1.3356)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [4][78/390]\tTime 0.002 (0.003)\tLoss 1.3319 (1.2904)\tPrec@1 56.250 (53.995)\n",
      "Epoch: [4][156/390]\tTime 0.003 (0.003)\tLoss 1.2949 (1.2981)\tPrec@1 50.781 (53.414)\n",
      "Epoch: [4][234/390]\tTime 0.002 (0.003)\tLoss 1.3315 (1.3025)\tPrec@1 45.312 (53.208)\n",
      "Epoch: [4][312/390]\tTime 0.004 (0.003)\tLoss 1.4059 (1.3114)\tPrec@1 48.438 (52.880)\n",
      "Epoch: [4][390/390]\tTime 0.001 (0.003)\tLoss 1.3346 (1.3170)\tPrec@1 47.500 (52.644)\n",
      "EPOCH: 4 train Results: Prec@1 52.644 Loss: 1.3170\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1858 (1.1858)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2150 (1.3214)\tPrec@1 37.500 (52.190)\n",
      "EPOCH: 4 val Results: Prec@1 52.190 Loss: 1.3214\n",
      "Best Prec@1: 52.190\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [5][0/390]\tTime 0.002 (0.002)\tLoss 1.2751 (1.2751)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [5][78/390]\tTime 0.004 (0.003)\tLoss 1.3299 (1.2261)\tPrec@1 51.562 (56.102)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.003)\tLoss 1.2906 (1.2594)\tPrec@1 54.688 (54.991)\n",
      "Epoch: [5][234/390]\tTime 0.003 (0.003)\tLoss 1.3224 (1.2656)\tPrec@1 57.031 (54.727)\n",
      "Epoch: [5][312/390]\tTime 0.003 (0.003)\tLoss 1.2890 (1.2735)\tPrec@1 49.219 (54.301)\n",
      "Epoch: [5][390/390]\tTime 0.002 (0.003)\tLoss 1.2639 (1.2802)\tPrec@1 58.750 (54.114)\n",
      "EPOCH: 5 train Results: Prec@1 54.114 Loss: 1.2802\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1894 (1.1894)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3859 (1.3306)\tPrec@1 37.500 (52.350)\n",
      "EPOCH: 5 val Results: Prec@1 52.350 Loss: 1.3306\n",
      "Best Prec@1: 52.350\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [6][0/390]\tTime 0.004 (0.004)\tLoss 1.2816 (1.2816)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [6][78/390]\tTime 0.002 (0.003)\tLoss 1.3086 (1.2018)\tPrec@1 53.125 (57.239)\n",
      "Epoch: [6][156/390]\tTime 0.003 (0.003)\tLoss 1.1388 (1.2187)\tPrec@1 57.812 (56.568)\n",
      "Epoch: [6][234/390]\tTime 0.003 (0.003)\tLoss 1.3002 (1.2352)\tPrec@1 54.688 (55.901)\n",
      "Epoch: [6][312/390]\tTime 0.002 (0.003)\tLoss 1.4017 (1.2510)\tPrec@1 50.781 (55.399)\n",
      "Epoch: [6][390/390]\tTime 0.001 (0.003)\tLoss 1.4604 (1.2573)\tPrec@1 48.750 (55.220)\n",
      "EPOCH: 6 train Results: Prec@1 55.220 Loss: 1.2573\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1685 (1.1685)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3487 (1.3056)\tPrec@1 50.000 (53.150)\n",
      "EPOCH: 6 val Results: Prec@1 53.150 Loss: 1.3056\n",
      "Best Prec@1: 53.150\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [7][0/390]\tTime 0.005 (0.005)\tLoss 1.2931 (1.2931)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [7][78/390]\tTime 0.005 (0.003)\tLoss 1.2229 (1.1855)\tPrec@1 54.688 (57.991)\n",
      "Epoch: [7][156/390]\tTime 0.002 (0.003)\tLoss 1.4374 (1.2069)\tPrec@1 47.656 (56.867)\n",
      "Epoch: [7][234/390]\tTime 0.003 (0.003)\tLoss 1.2747 (1.2205)\tPrec@1 57.812 (56.247)\n",
      "Epoch: [7][312/390]\tTime 0.003 (0.003)\tLoss 1.3267 (1.2257)\tPrec@1 53.125 (56.118)\n",
      "Epoch: [7][390/390]\tTime 0.002 (0.003)\tLoss 1.3731 (1.2350)\tPrec@1 53.750 (55.938)\n",
      "EPOCH: 7 train Results: Prec@1 55.938 Loss: 1.2350\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1648 (1.1648)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1925 (1.2930)\tPrec@1 50.000 (53.720)\n",
      "EPOCH: 7 val Results: Prec@1 53.720 Loss: 1.2930\n",
      "Best Prec@1: 53.720\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [8][0/390]\tTime 0.007 (0.007)\tLoss 1.1494 (1.1494)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [8][78/390]\tTime 0.003 (0.003)\tLoss 1.1556 (1.1648)\tPrec@1 58.594 (58.732)\n",
      "Epoch: [8][156/390]\tTime 0.003 (0.003)\tLoss 1.3564 (1.1940)\tPrec@1 50.000 (56.981)\n",
      "Epoch: [8][234/390]\tTime 0.010 (0.003)\tLoss 1.1316 (1.2043)\tPrec@1 63.281 (56.636)\n",
      "Epoch: [8][312/390]\tTime 0.006 (0.003)\tLoss 1.0779 (1.2112)\tPrec@1 62.500 (56.555)\n",
      "Epoch: [8][390/390]\tTime 0.004 (0.003)\tLoss 1.2901 (1.2170)\tPrec@1 52.500 (56.308)\n",
      "EPOCH: 8 train Results: Prec@1 56.308 Loss: 1.2170\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1455 (1.1455)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4124 (1.2908)\tPrec@1 31.250 (53.590)\n",
      "EPOCH: 8 val Results: Prec@1 53.590 Loss: 1.2908\n",
      "Best Prec@1: 53.720\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [9][0/390]\tTime 0.003 (0.003)\tLoss 1.1328 (1.1328)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [9][78/390]\tTime 0.006 (0.003)\tLoss 1.1703 (1.1364)\tPrec@1 57.812 (59.464)\n",
      "Epoch: [9][156/390]\tTime 0.002 (0.003)\tLoss 1.4414 (1.1644)\tPrec@1 48.438 (58.544)\n",
      "Epoch: [9][234/390]\tTime 0.004 (0.003)\tLoss 1.3301 (1.1831)\tPrec@1 52.344 (57.746)\n",
      "Epoch: [9][312/390]\tTime 0.002 (0.003)\tLoss 1.2342 (1.1926)\tPrec@1 52.344 (57.448)\n",
      "Epoch: [9][390/390]\tTime 0.001 (0.003)\tLoss 1.4747 (1.2018)\tPrec@1 48.750 (57.028)\n",
      "EPOCH: 9 train Results: Prec@1 57.028 Loss: 1.2018\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1676 (1.1676)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2356 (1.2877)\tPrec@1 31.250 (53.420)\n",
      "EPOCH: 9 val Results: Prec@1 53.420 Loss: 1.2877\n",
      "Best Prec@1: 53.720\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [10][0/390]\tTime 0.003 (0.003)\tLoss 1.1789 (1.1789)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [10][78/390]\tTime 0.002 (0.003)\tLoss 0.9829 (1.1228)\tPrec@1 60.156 (59.553)\n",
      "Epoch: [10][156/390]\tTime 0.003 (0.003)\tLoss 1.1185 (1.1525)\tPrec@1 60.938 (58.609)\n",
      "Epoch: [10][234/390]\tTime 0.006 (0.003)\tLoss 1.1962 (1.1705)\tPrec@1 56.250 (57.793)\n",
      "Epoch: [10][312/390]\tTime 0.002 (0.003)\tLoss 1.2337 (1.1825)\tPrec@1 53.906 (57.565)\n",
      "Epoch: [10][390/390]\tTime 0.008 (0.003)\tLoss 1.0216 (1.1893)\tPrec@1 63.750 (57.344)\n",
      "EPOCH: 10 train Results: Prec@1 57.344 Loss: 1.1893\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1882 (1.1882)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4069 (1.2833)\tPrec@1 43.750 (53.600)\n",
      "EPOCH: 10 val Results: Prec@1 53.600 Loss: 1.2833\n",
      "Best Prec@1: 53.720\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [11][0/390]\tTime 0.003 (0.003)\tLoss 1.0512 (1.0512)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [11][78/390]\tTime 0.004 (0.003)\tLoss 1.0874 (1.1190)\tPrec@1 64.062 (59.869)\n",
      "Epoch: [11][156/390]\tTime 0.008 (0.003)\tLoss 1.0058 (1.1404)\tPrec@1 63.281 (59.012)\n",
      "Epoch: [11][234/390]\tTime 0.002 (0.003)\tLoss 1.2664 (1.1588)\tPrec@1 57.812 (58.255)\n",
      "Epoch: [11][312/390]\tTime 0.006 (0.003)\tLoss 1.0864 (1.1696)\tPrec@1 64.844 (57.837)\n",
      "Epoch: [11][390/390]\tTime 0.002 (0.003)\tLoss 1.1246 (1.1797)\tPrec@1 58.750 (57.526)\n",
      "EPOCH: 11 train Results: Prec@1 57.526 Loss: 1.1797\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1324 (1.1324)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5375 (1.2760)\tPrec@1 43.750 (54.250)\n",
      "EPOCH: 11 val Results: Prec@1 54.250 Loss: 1.2760\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [12][0/390]\tTime 0.002 (0.002)\tLoss 1.0133 (1.0133)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [12][78/390]\tTime 0.002 (0.004)\tLoss 1.0254 (1.0958)\tPrec@1 64.844 (61.175)\n",
      "Epoch: [12][156/390]\tTime 0.004 (0.003)\tLoss 1.2558 (1.1229)\tPrec@1 58.594 (60.231)\n",
      "Epoch: [12][234/390]\tTime 0.007 (0.003)\tLoss 1.0696 (1.1439)\tPrec@1 66.406 (59.122)\n",
      "Epoch: [12][312/390]\tTime 0.008 (0.003)\tLoss 1.1382 (1.1583)\tPrec@1 56.250 (58.776)\n",
      "Epoch: [12][390/390]\tTime 0.001 (0.003)\tLoss 0.9858 (1.1681)\tPrec@1 65.000 (58.508)\n",
      "EPOCH: 12 train Results: Prec@1 58.508 Loss: 1.1681\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1276 (1.1276)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3409 (1.2875)\tPrec@1 43.750 (53.800)\n",
      "EPOCH: 12 val Results: Prec@1 53.800 Loss: 1.2875\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [13][0/390]\tTime 0.003 (0.003)\tLoss 1.0728 (1.0728)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.0344 (1.0967)\tPrec@1 63.281 (60.848)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.004)\tLoss 1.0571 (1.1176)\tPrec@1 67.969 (60.111)\n",
      "Epoch: [13][234/390]\tTime 0.005 (0.003)\tLoss 1.3708 (1.1390)\tPrec@1 53.125 (59.318)\n",
      "Epoch: [13][312/390]\tTime 0.005 (0.003)\tLoss 1.2263 (1.1535)\tPrec@1 58.594 (58.761)\n",
      "Epoch: [13][390/390]\tTime 0.001 (0.003)\tLoss 1.0859 (1.1599)\tPrec@1 58.750 (58.500)\n",
      "EPOCH: 13 train Results: Prec@1 58.500 Loss: 1.1599\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1954 (1.1954)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5231 (1.2771)\tPrec@1 37.500 (54.290)\n",
      "EPOCH: 13 val Results: Prec@1 54.290 Loss: 1.2771\n",
      "Best Prec@1: 54.290\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [14][0/390]\tTime 0.003 (0.003)\tLoss 0.9510 (0.9510)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [14][78/390]\tTime 0.002 (0.003)\tLoss 1.1203 (1.0943)\tPrec@1 55.469 (60.532)\n",
      "Epoch: [14][156/390]\tTime 0.002 (0.003)\tLoss 1.1453 (1.1107)\tPrec@1 62.500 (59.932)\n",
      "Epoch: [14][234/390]\tTime 0.003 (0.003)\tLoss 1.2051 (1.1316)\tPrec@1 56.250 (59.555)\n",
      "Epoch: [14][312/390]\tTime 0.002 (0.003)\tLoss 1.0573 (1.1440)\tPrec@1 60.156 (59.140)\n",
      "Epoch: [14][390/390]\tTime 0.003 (0.003)\tLoss 1.2936 (1.1521)\tPrec@1 47.500 (58.810)\n",
      "EPOCH: 14 train Results: Prec@1 58.810 Loss: 1.1521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1218 (1.1218)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4726 (1.2685)\tPrec@1 37.500 (54.780)\n",
      "EPOCH: 14 val Results: Prec@1 54.780 Loss: 1.2685\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [15][0/390]\tTime 0.004 (0.004)\tLoss 1.0137 (1.0137)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [15][78/390]\tTime 0.003 (0.003)\tLoss 1.1851 (1.0733)\tPrec@1 58.594 (61.798)\n",
      "Epoch: [15][156/390]\tTime 0.004 (0.003)\tLoss 1.0415 (1.1065)\tPrec@1 60.156 (60.599)\n",
      "Epoch: [15][234/390]\tTime 0.002 (0.003)\tLoss 1.1780 (1.1225)\tPrec@1 55.469 (60.146)\n",
      "Epoch: [15][312/390]\tTime 0.005 (0.003)\tLoss 1.2798 (1.1362)\tPrec@1 57.031 (59.635)\n",
      "Epoch: [15][390/390]\tTime 0.002 (0.003)\tLoss 1.1789 (1.1431)\tPrec@1 58.750 (59.332)\n",
      "EPOCH: 15 train Results: Prec@1 59.332 Loss: 1.1431\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1364 (1.1364)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4928 (1.2801)\tPrec@1 37.500 (54.600)\n",
      "EPOCH: 15 val Results: Prec@1 54.600 Loss: 1.2801\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [16][0/390]\tTime 0.004 (0.004)\tLoss 1.0279 (1.0279)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.003)\tLoss 1.0034 (1.0588)\tPrec@1 65.625 (61.828)\n",
      "Epoch: [16][156/390]\tTime 0.009 (0.003)\tLoss 1.0568 (1.0894)\tPrec@1 60.156 (60.798)\n",
      "Epoch: [16][234/390]\tTime 0.002 (0.003)\tLoss 1.2371 (1.1144)\tPrec@1 57.812 (59.943)\n",
      "Epoch: [16][312/390]\tTime 0.002 (0.003)\tLoss 1.1640 (1.1289)\tPrec@1 56.250 (59.333)\n",
      "Epoch: [16][390/390]\tTime 0.002 (0.003)\tLoss 1.3219 (1.1405)\tPrec@1 51.250 (59.024)\n",
      "EPOCH: 16 train Results: Prec@1 59.024 Loss: 1.1405\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0991 (1.0991)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0826 (1.2813)\tPrec@1 56.250 (54.240)\n",
      "EPOCH: 16 val Results: Prec@1 54.240 Loss: 1.2813\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [17][0/390]\tTime 0.005 (0.005)\tLoss 1.0846 (1.0846)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [17][78/390]\tTime 0.002 (0.003)\tLoss 1.1644 (1.0577)\tPrec@1 58.594 (62.381)\n",
      "Epoch: [17][156/390]\tTime 0.002 (0.003)\tLoss 1.2876 (1.0842)\tPrec@1 53.906 (61.445)\n",
      "Epoch: [17][234/390]\tTime 0.004 (0.003)\tLoss 1.1141 (1.1041)\tPrec@1 57.812 (60.409)\n",
      "Epoch: [17][312/390]\tTime 0.002 (0.003)\tLoss 1.1851 (1.1205)\tPrec@1 62.500 (59.917)\n",
      "Epoch: [17][390/390]\tTime 0.002 (0.003)\tLoss 1.1986 (1.1334)\tPrec@1 60.000 (59.402)\n",
      "EPOCH: 17 train Results: Prec@1 59.402 Loss: 1.1334\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1190 (1.1190)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2422 (1.2858)\tPrec@1 56.250 (53.750)\n",
      "EPOCH: 17 val Results: Prec@1 53.750 Loss: 1.2858\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [18][0/390]\tTime 0.008 (0.008)\tLoss 0.9528 (0.9528)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [18][78/390]\tTime 0.004 (0.003)\tLoss 1.0184 (1.0623)\tPrec@1 61.719 (62.203)\n",
      "Epoch: [18][156/390]\tTime 0.004 (0.003)\tLoss 1.0277 (1.0894)\tPrec@1 70.312 (61.341)\n",
      "Epoch: [18][234/390]\tTime 0.002 (0.003)\tLoss 0.9552 (1.1115)\tPrec@1 64.062 (60.469)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.003)\tLoss 1.1818 (1.1205)\tPrec@1 63.281 (60.084)\n",
      "Epoch: [18][390/390]\tTime 0.001 (0.003)\tLoss 1.1098 (1.1301)\tPrec@1 55.000 (59.732)\n",
      "EPOCH: 18 train Results: Prec@1 59.732 Loss: 1.1301\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1691 (1.1691)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1363 (1.2912)\tPrec@1 56.250 (53.380)\n",
      "EPOCH: 18 val Results: Prec@1 53.380 Loss: 1.2912\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [19][0/390]\tTime 0.005 (0.005)\tLoss 1.0479 (1.0479)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [19][78/390]\tTime 0.049 (0.003)\tLoss 1.2420 (1.0391)\tPrec@1 57.812 (62.698)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.003)\tLoss 1.2165 (1.0731)\tPrec@1 57.812 (61.560)\n",
      "Epoch: [19][234/390]\tTime 0.007 (0.003)\tLoss 1.1244 (1.0947)\tPrec@1 60.156 (60.957)\n",
      "Epoch: [19][312/390]\tTime 0.004 (0.003)\tLoss 1.0154 (1.1115)\tPrec@1 61.719 (60.358)\n",
      "Epoch: [19][390/390]\tTime 0.004 (0.003)\tLoss 1.3959 (1.1210)\tPrec@1 55.000 (59.978)\n",
      "EPOCH: 19 train Results: Prec@1 59.978 Loss: 1.1210\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1481 (1.1481)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.6266 (1.2794)\tPrec@1 37.500 (54.070)\n",
      "EPOCH: 19 val Results: Prec@1 54.070 Loss: 1.2794\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [20][0/390]\tTime 0.012 (0.012)\tLoss 1.0244 (1.0244)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [20][78/390]\tTime 0.002 (0.003)\tLoss 0.9763 (1.0609)\tPrec@1 63.281 (61.847)\n",
      "Epoch: [20][156/390]\tTime 0.006 (0.003)\tLoss 0.9938 (1.0772)\tPrec@1 63.281 (61.355)\n",
      "Epoch: [20][234/390]\tTime 0.002 (0.003)\tLoss 1.0891 (1.1000)\tPrec@1 64.062 (60.455)\n",
      "Epoch: [20][312/390]\tTime 0.004 (0.003)\tLoss 1.2540 (1.1088)\tPrec@1 51.562 (60.333)\n",
      "Epoch: [20][390/390]\tTime 0.002 (0.003)\tLoss 1.2734 (1.1189)\tPrec@1 55.000 (60.002)\n",
      "EPOCH: 20 train Results: Prec@1 60.002 Loss: 1.1189\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.1246 (1.1246)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2110 (1.2768)\tPrec@1 43.750 (54.660)\n",
      "EPOCH: 20 val Results: Prec@1 54.660 Loss: 1.2768\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [21][0/390]\tTime 0.002 (0.002)\tLoss 1.1326 (1.1326)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [21][78/390]\tTime 0.003 (0.003)\tLoss 1.0840 (1.0339)\tPrec@1 55.469 (62.945)\n",
      "Epoch: [21][156/390]\tTime 0.003 (0.003)\tLoss 1.0653 (1.0685)\tPrec@1 60.156 (61.893)\n",
      "Epoch: [21][234/390]\tTime 0.002 (0.003)\tLoss 1.1605 (1.0831)\tPrec@1 62.500 (61.576)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.0295 (1.0990)\tPrec@1 67.969 (60.833)\n",
      "Epoch: [21][390/390]\tTime 0.006 (0.003)\tLoss 1.3250 (1.1116)\tPrec@1 53.750 (60.328)\n",
      "EPOCH: 21 train Results: Prec@1 60.328 Loss: 1.1116\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1789 (1.1789)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2911 (1.2747)\tPrec@1 43.750 (54.670)\n",
      "EPOCH: 21 val Results: Prec@1 54.670 Loss: 1.2747\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [22][0/390]\tTime 0.005 (0.005)\tLoss 1.1545 (1.1545)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [22][78/390]\tTime 0.005 (0.003)\tLoss 1.0840 (1.0626)\tPrec@1 62.500 (62.144)\n",
      "Epoch: [22][156/390]\tTime 0.006 (0.004)\tLoss 1.0955 (1.0781)\tPrec@1 60.938 (61.375)\n",
      "Epoch: [22][234/390]\tTime 0.002 (0.003)\tLoss 1.0644 (1.0902)\tPrec@1 59.375 (60.798)\n",
      "Epoch: [22][312/390]\tTime 0.003 (0.003)\tLoss 1.1485 (1.1052)\tPrec@1 59.375 (60.191)\n",
      "Epoch: [22][390/390]\tTime 0.005 (0.003)\tLoss 1.1045 (1.1184)\tPrec@1 58.750 (59.746)\n",
      "EPOCH: 22 train Results: Prec@1 59.746 Loss: 1.1184\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1487 (1.1487)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2054 (1.2813)\tPrec@1 31.250 (54.300)\n",
      "EPOCH: 22 val Results: Prec@1 54.300 Loss: 1.2813\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [23][0/390]\tTime 0.002 (0.002)\tLoss 1.0619 (1.0619)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [23][78/390]\tTime 0.003 (0.003)\tLoss 1.1433 (1.0278)\tPrec@1 60.156 (63.608)\n",
      "Epoch: [23][156/390]\tTime 0.003 (0.003)\tLoss 1.2536 (1.0632)\tPrec@1 52.344 (62.206)\n",
      "Epoch: [23][234/390]\tTime 0.003 (0.003)\tLoss 1.2205 (1.0785)\tPrec@1 57.031 (61.499)\n",
      "Epoch: [23][312/390]\tTime 0.002 (0.003)\tLoss 1.3087 (1.0979)\tPrec@1 53.906 (60.868)\n",
      "Epoch: [23][390/390]\tTime 0.003 (0.003)\tLoss 1.3752 (1.1109)\tPrec@1 58.750 (60.372)\n",
      "EPOCH: 23 train Results: Prec@1 60.372 Loss: 1.1109\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1800 (1.1800)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5958 (1.2707)\tPrec@1 31.250 (54.420)\n",
      "EPOCH: 23 val Results: Prec@1 54.420 Loss: 1.2707\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [24][0/390]\tTime 0.005 (0.005)\tLoss 1.0435 (1.0435)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.1727 (1.0231)\tPrec@1 67.188 (63.944)\n",
      "Epoch: [24][156/390]\tTime 0.008 (0.003)\tLoss 1.1401 (1.0584)\tPrec@1 57.031 (62.246)\n",
      "Epoch: [24][234/390]\tTime 0.002 (0.003)\tLoss 1.2878 (1.0848)\tPrec@1 54.688 (61.237)\n",
      "Epoch: [24][312/390]\tTime 0.002 (0.004)\tLoss 1.3018 (1.0990)\tPrec@1 54.688 (60.693)\n",
      "Epoch: [24][390/390]\tTime 0.002 (0.004)\tLoss 1.3255 (1.1088)\tPrec@1 55.000 (60.296)\n",
      "EPOCH: 24 train Results: Prec@1 60.296 Loss: 1.1088\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2120 (1.2120)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5150 (1.2944)\tPrec@1 31.250 (54.010)\n",
      "EPOCH: 24 val Results: Prec@1 54.010 Loss: 1.2944\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [25][0/390]\tTime 0.008 (0.008)\tLoss 0.9278 (0.9278)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [25][78/390]\tTime 0.002 (0.003)\tLoss 1.0294 (1.0267)\tPrec@1 64.844 (63.271)\n",
      "Epoch: [25][156/390]\tTime 0.002 (0.004)\tLoss 1.1922 (1.0558)\tPrec@1 60.156 (62.361)\n",
      "Epoch: [25][234/390]\tTime 0.002 (0.004)\tLoss 1.2191 (1.0809)\tPrec@1 54.688 (61.406)\n",
      "Epoch: [25][312/390]\tTime 0.003 (0.004)\tLoss 1.2276 (1.0965)\tPrec@1 60.156 (60.945)\n",
      "Epoch: [25][390/390]\tTime 0.004 (0.003)\tLoss 1.3127 (1.1050)\tPrec@1 51.250 (60.598)\n",
      "EPOCH: 25 train Results: Prec@1 60.598 Loss: 1.1050\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1484 (1.1484)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3425 (1.2799)\tPrec@1 56.250 (54.290)\n",
      "EPOCH: 25 val Results: Prec@1 54.290 Loss: 1.2799\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [26][0/390]\tTime 0.006 (0.006)\tLoss 1.0847 (1.0847)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [26][78/390]\tTime 0.002 (0.003)\tLoss 1.0236 (1.0130)\tPrec@1 60.938 (63.973)\n",
      "Epoch: [26][156/390]\tTime 0.003 (0.003)\tLoss 1.1405 (1.0530)\tPrec@1 62.500 (62.794)\n",
      "Epoch: [26][234/390]\tTime 0.002 (0.003)\tLoss 1.0530 (1.0739)\tPrec@1 60.156 (61.941)\n",
      "Epoch: [26][312/390]\tTime 0.004 (0.003)\tLoss 1.0509 (1.0908)\tPrec@1 60.156 (61.324)\n",
      "Epoch: [26][390/390]\tTime 0.001 (0.003)\tLoss 1.2073 (1.1010)\tPrec@1 55.000 (60.860)\n",
      "EPOCH: 26 train Results: Prec@1 60.860 Loss: 1.1010\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1917 (1.1917)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2608 (1.2842)\tPrec@1 37.500 (54.460)\n",
      "EPOCH: 26 val Results: Prec@1 54.460 Loss: 1.2842\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [27][0/390]\tTime 0.004 (0.004)\tLoss 1.0757 (1.0757)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [27][78/390]\tTime 0.004 (0.003)\tLoss 0.9873 (1.0215)\tPrec@1 64.062 (63.617)\n",
      "Epoch: [27][156/390]\tTime 0.003 (0.003)\tLoss 0.8946 (1.0512)\tPrec@1 65.625 (62.475)\n",
      "Epoch: [27][234/390]\tTime 0.002 (0.003)\tLoss 0.9891 (1.0688)\tPrec@1 65.625 (61.729)\n",
      "Epoch: [27][312/390]\tTime 0.004 (0.003)\tLoss 1.1280 (1.0881)\tPrec@1 60.156 (60.972)\n",
      "Epoch: [27][390/390]\tTime 0.005 (0.003)\tLoss 1.3398 (1.1025)\tPrec@1 52.500 (60.594)\n",
      "EPOCH: 27 train Results: Prec@1 60.594 Loss: 1.1025\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1170 (1.1170)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4828 (1.2813)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 27 val Results: Prec@1 54.630 Loss: 1.2813\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [28][0/390]\tTime 0.004 (0.004)\tLoss 1.0308 (1.0308)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [28][78/390]\tTime 0.008 (0.003)\tLoss 1.0196 (1.0335)\tPrec@1 70.312 (63.311)\n",
      "Epoch: [28][156/390]\tTime 0.002 (0.004)\tLoss 1.1482 (1.0532)\tPrec@1 60.938 (62.296)\n",
      "Epoch: [28][234/390]\tTime 0.002 (0.003)\tLoss 1.0352 (1.0734)\tPrec@1 63.281 (61.589)\n",
      "Epoch: [28][312/390]\tTime 0.004 (0.003)\tLoss 1.1826 (1.0856)\tPrec@1 61.719 (61.097)\n",
      "Epoch: [28][390/390]\tTime 0.003 (0.003)\tLoss 1.2334 (1.0984)\tPrec@1 57.500 (60.688)\n",
      "EPOCH: 28 train Results: Prec@1 60.688 Loss: 1.0984\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2030 (1.2030)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6278 (1.2732)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 28 val Results: Prec@1 55.230 Loss: 1.2732\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [29][0/390]\tTime 0.003 (0.003)\tLoss 0.9857 (0.9857)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [29][78/390]\tTime 0.002 (0.003)\tLoss 1.0549 (1.0210)\tPrec@1 66.406 (63.261)\n",
      "Epoch: [29][156/390]\tTime 0.003 (0.003)\tLoss 1.0663 (1.0482)\tPrec@1 58.594 (62.530)\n",
      "Epoch: [29][234/390]\tTime 0.002 (0.003)\tLoss 1.0148 (1.0718)\tPrec@1 65.625 (61.692)\n",
      "Epoch: [29][312/390]\tTime 0.002 (0.003)\tLoss 1.0641 (1.0907)\tPrec@1 60.156 (60.967)\n",
      "Epoch: [29][390/390]\tTime 0.003 (0.003)\tLoss 1.2661 (1.1007)\tPrec@1 57.500 (60.612)\n",
      "EPOCH: 29 train Results: Prec@1 60.612 Loss: 1.1007\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1110 (1.1110)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1953 (1.2742)\tPrec@1 56.250 (54.450)\n",
      "EPOCH: 29 val Results: Prec@1 54.450 Loss: 1.2742\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [30][0/390]\tTime 0.005 (0.005)\tLoss 0.9370 (0.9370)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.003)\tLoss 1.0583 (1.0285)\tPrec@1 64.062 (63.360)\n",
      "Epoch: [30][156/390]\tTime 0.002 (0.003)\tLoss 1.0981 (1.0568)\tPrec@1 55.469 (62.371)\n",
      "Epoch: [30][234/390]\tTime 0.005 (0.003)\tLoss 1.1814 (1.0722)\tPrec@1 59.375 (61.872)\n",
      "Epoch: [30][312/390]\tTime 0.005 (0.003)\tLoss 1.0904 (1.0856)\tPrec@1 64.844 (61.392)\n",
      "Epoch: [30][390/390]\tTime 0.001 (0.003)\tLoss 1.0602 (1.0988)\tPrec@1 60.000 (60.984)\n",
      "EPOCH: 30 train Results: Prec@1 60.984 Loss: 1.0988\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1279 (1.1279)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2511 (1.2791)\tPrec@1 43.750 (54.790)\n",
      "EPOCH: 30 val Results: Prec@1 54.790 Loss: 1.2791\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [31][0/390]\tTime 0.002 (0.002)\tLoss 0.8981 (0.8981)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [31][78/390]\tTime 0.002 (0.003)\tLoss 0.9234 (1.0233)\tPrec@1 66.406 (63.281)\n",
      "Epoch: [31][156/390]\tTime 0.003 (0.003)\tLoss 1.0218 (1.0422)\tPrec@1 60.156 (62.545)\n",
      "Epoch: [31][234/390]\tTime 0.007 (0.003)\tLoss 1.1494 (1.0618)\tPrec@1 60.938 (61.885)\n",
      "Epoch: [31][312/390]\tTime 0.006 (0.003)\tLoss 1.0893 (1.0766)\tPrec@1 60.938 (61.479)\n",
      "Epoch: [31][390/390]\tTime 0.002 (0.003)\tLoss 1.2468 (1.0924)\tPrec@1 62.500 (60.964)\n",
      "EPOCH: 31 train Results: Prec@1 60.964 Loss: 1.0924\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1692 (1.1692)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5650 (1.2817)\tPrec@1 18.750 (54.700)\n",
      "EPOCH: 31 val Results: Prec@1 54.700 Loss: 1.2817\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [32][0/390]\tTime 0.004 (0.004)\tLoss 0.9850 (0.9850)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [32][78/390]\tTime 0.002 (0.004)\tLoss 0.9557 (1.0077)\tPrec@1 68.750 (64.359)\n",
      "Epoch: [32][156/390]\tTime 0.003 (0.004)\tLoss 0.9941 (1.0446)\tPrec@1 67.969 (62.898)\n",
      "Epoch: [32][234/390]\tTime 0.003 (0.003)\tLoss 1.1219 (1.0616)\tPrec@1 60.156 (62.031)\n",
      "Epoch: [32][312/390]\tTime 0.005 (0.003)\tLoss 0.9866 (1.0756)\tPrec@1 63.281 (61.502)\n",
      "Epoch: [32][390/390]\tTime 0.004 (0.003)\tLoss 0.9577 (1.0900)\tPrec@1 68.750 (61.034)\n",
      "EPOCH: 32 train Results: Prec@1 61.034 Loss: 1.0900\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2563 (1.2563)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6028 (1.2856)\tPrec@1 50.000 (54.580)\n",
      "EPOCH: 32 val Results: Prec@1 54.580 Loss: 1.2856\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [33][0/390]\tTime 0.004 (0.004)\tLoss 1.1115 (1.1115)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [33][78/390]\tTime 0.003 (0.003)\tLoss 1.1822 (1.0259)\tPrec@1 54.688 (62.994)\n",
      "Epoch: [33][156/390]\tTime 0.004 (0.003)\tLoss 1.1119 (1.0493)\tPrec@1 60.156 (62.316)\n",
      "Epoch: [33][234/390]\tTime 0.002 (0.003)\tLoss 1.1700 (1.0643)\tPrec@1 57.031 (61.765)\n",
      "Epoch: [33][312/390]\tTime 0.008 (0.003)\tLoss 1.1552 (1.0769)\tPrec@1 60.938 (61.389)\n",
      "Epoch: [33][390/390]\tTime 0.002 (0.003)\tLoss 1.2877 (1.0919)\tPrec@1 57.500 (60.950)\n",
      "EPOCH: 33 train Results: Prec@1 60.950 Loss: 1.0919\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1515 (1.1515)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6077 (1.2760)\tPrec@1 37.500 (54.950)\n",
      "EPOCH: 33 val Results: Prec@1 54.950 Loss: 1.2760\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [34][0/390]\tTime 0.002 (0.002)\tLoss 0.8998 (0.8998)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [34][78/390]\tTime 0.002 (0.003)\tLoss 1.0872 (1.0087)\tPrec@1 62.500 (64.715)\n",
      "Epoch: [34][156/390]\tTime 0.002 (0.003)\tLoss 1.1636 (1.0389)\tPrec@1 64.062 (63.391)\n",
      "Epoch: [34][234/390]\tTime 0.002 (0.003)\tLoss 1.1098 (1.0558)\tPrec@1 57.031 (62.497)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.1302 (1.0741)\tPrec@1 62.500 (61.903)\n",
      "Epoch: [34][390/390]\tTime 0.004 (0.003)\tLoss 1.0717 (1.0873)\tPrec@1 55.000 (61.324)\n",
      "EPOCH: 34 train Results: Prec@1 61.324 Loss: 1.0873\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1182 (1.1182)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2825 (1.2813)\tPrec@1 56.250 (54.040)\n",
      "EPOCH: 34 val Results: Prec@1 54.040 Loss: 1.2813\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [35][0/390]\tTime 0.002 (0.002)\tLoss 1.0572 (1.0572)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [35][78/390]\tTime 0.003 (0.004)\tLoss 1.1519 (1.0013)\tPrec@1 54.688 (64.508)\n",
      "Epoch: [35][156/390]\tTime 0.007 (0.004)\tLoss 1.3191 (1.0299)\tPrec@1 57.812 (63.251)\n",
      "Epoch: [35][234/390]\tTime 0.002 (0.003)\tLoss 1.1560 (1.0603)\tPrec@1 60.938 (62.098)\n",
      "Epoch: [35][312/390]\tTime 0.002 (0.003)\tLoss 1.1823 (1.0776)\tPrec@1 53.906 (61.552)\n",
      "Epoch: [35][390/390]\tTime 0.009 (0.003)\tLoss 1.0559 (1.0882)\tPrec@1 67.500 (61.202)\n",
      "EPOCH: 35 train Results: Prec@1 61.202 Loss: 1.0882\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1425 (1.1425)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.8031 (1.2783)\tPrec@1 25.000 (54.240)\n",
      "EPOCH: 35 val Results: Prec@1 54.240 Loss: 1.2783\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [36][0/390]\tTime 0.002 (0.002)\tLoss 1.1035 (1.1035)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [36][78/390]\tTime 0.004 (0.003)\tLoss 1.1714 (1.0058)\tPrec@1 56.250 (64.330)\n",
      "Epoch: [36][156/390]\tTime 0.007 (0.003)\tLoss 1.1166 (1.0426)\tPrec@1 60.938 (62.749)\n",
      "Epoch: [36][234/390]\tTime 0.003 (0.003)\tLoss 1.1041 (1.0589)\tPrec@1 64.062 (62.108)\n",
      "Epoch: [36][312/390]\tTime 0.003 (0.003)\tLoss 1.2327 (1.0718)\tPrec@1 60.156 (61.681)\n",
      "Epoch: [36][390/390]\tTime 0.001 (0.003)\tLoss 1.2852 (1.0842)\tPrec@1 48.750 (61.272)\n",
      "EPOCH: 36 train Results: Prec@1 61.272 Loss: 1.0842\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1404 (1.1404)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3955 (1.2735)\tPrec@1 43.750 (55.130)\n",
      "EPOCH: 36 val Results: Prec@1 55.130 Loss: 1.2735\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [37][0/390]\tTime 0.005 (0.005)\tLoss 0.8278 (0.8278)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 0.9678 (0.9957)\tPrec@1 65.625 (65.071)\n",
      "Epoch: [37][156/390]\tTime 0.002 (0.003)\tLoss 0.9042 (1.0326)\tPrec@1 67.969 (63.246)\n",
      "Epoch: [37][234/390]\tTime 0.002 (0.003)\tLoss 1.1706 (1.0590)\tPrec@1 56.250 (62.380)\n",
      "Epoch: [37][312/390]\tTime 0.003 (0.003)\tLoss 1.0533 (1.0817)\tPrec@1 69.531 (61.432)\n",
      "Epoch: [37][390/390]\tTime 0.005 (0.003)\tLoss 1.1111 (1.0890)\tPrec@1 56.250 (61.224)\n",
      "EPOCH: 37 train Results: Prec@1 61.224 Loss: 1.0890\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2157 (1.2157)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5334 (1.2759)\tPrec@1 37.500 (54.960)\n",
      "EPOCH: 37 val Results: Prec@1 54.960 Loss: 1.2759\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [38][0/390]\tTime 0.005 (0.005)\tLoss 0.9967 (0.9967)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [38][78/390]\tTime 0.003 (0.003)\tLoss 1.2499 (1.0245)\tPrec@1 57.031 (63.558)\n",
      "Epoch: [38][156/390]\tTime 0.012 (0.003)\tLoss 0.9926 (1.0437)\tPrec@1 67.188 (62.883)\n",
      "Epoch: [38][234/390]\tTime 0.003 (0.003)\tLoss 1.0457 (1.0668)\tPrec@1 65.625 (62.058)\n",
      "Epoch: [38][312/390]\tTime 0.002 (0.003)\tLoss 1.2319 (1.0769)\tPrec@1 56.250 (61.686)\n",
      "Epoch: [38][390/390]\tTime 0.002 (0.003)\tLoss 1.3319 (1.0873)\tPrec@1 60.000 (61.230)\n",
      "EPOCH: 38 train Results: Prec@1 61.230 Loss: 1.0873\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1426 (1.1426)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5026 (1.2743)\tPrec@1 50.000 (54.860)\n",
      "EPOCH: 38 val Results: Prec@1 54.860 Loss: 1.2743\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [39][0/390]\tTime 0.002 (0.002)\tLoss 0.9770 (0.9770)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.003)\tLoss 1.1584 (0.9991)\tPrec@1 61.719 (64.557)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.003)\tLoss 1.0875 (1.0332)\tPrec@1 63.281 (63.640)\n",
      "Epoch: [39][234/390]\tTime 0.004 (0.003)\tLoss 1.1842 (1.0573)\tPrec@1 59.375 (62.689)\n",
      "Epoch: [39][312/390]\tTime 0.002 (0.003)\tLoss 1.4436 (1.0705)\tPrec@1 38.281 (62.078)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2792 (1.0856)\tPrec@1 50.000 (61.524)\n",
      "EPOCH: 39 train Results: Prec@1 61.524 Loss: 1.0856\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1100 (1.1100)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7733 (1.2825)\tPrec@1 50.000 (54.740)\n",
      "EPOCH: 39 val Results: Prec@1 54.740 Loss: 1.2825\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [40][0/390]\tTime 0.002 (0.002)\tLoss 0.9824 (0.9824)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.003)\tLoss 1.0025 (1.0058)\tPrec@1 62.500 (64.517)\n",
      "Epoch: [40][156/390]\tTime 0.004 (0.003)\tLoss 1.0671 (1.0232)\tPrec@1 61.719 (63.565)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.003)\tLoss 1.1570 (1.0434)\tPrec@1 57.031 (62.766)\n",
      "Epoch: [40][312/390]\tTime 0.006 (0.003)\tLoss 1.2956 (1.0661)\tPrec@1 57.031 (61.948)\n",
      "Epoch: [40][390/390]\tTime 0.002 (0.003)\tLoss 1.3083 (1.0833)\tPrec@1 50.000 (61.230)\n",
      "EPOCH: 40 train Results: Prec@1 61.230 Loss: 1.0833\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1386 (1.1386)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6661 (1.2744)\tPrec@1 50.000 (54.440)\n",
      "EPOCH: 40 val Results: Prec@1 54.440 Loss: 1.2744\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [41][0/390]\tTime 0.003 (0.003)\tLoss 1.1466 (1.1466)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [41][78/390]\tTime 0.003 (0.003)\tLoss 0.9220 (0.9893)\tPrec@1 69.531 (64.666)\n",
      "Epoch: [41][156/390]\tTime 0.004 (0.003)\tLoss 1.1869 (1.0318)\tPrec@1 58.594 (63.087)\n",
      "Epoch: [41][234/390]\tTime 0.002 (0.003)\tLoss 1.1511 (1.0541)\tPrec@1 60.156 (62.204)\n",
      "Epoch: [41][312/390]\tTime 0.003 (0.003)\tLoss 1.2960 (1.0710)\tPrec@1 56.250 (61.487)\n",
      "Epoch: [41][390/390]\tTime 0.001 (0.003)\tLoss 1.3165 (1.0826)\tPrec@1 52.500 (61.164)\n",
      "EPOCH: 41 train Results: Prec@1 61.164 Loss: 1.0826\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1811 (1.1811)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1503 (1.2666)\tPrec@1 56.250 (55.210)\n",
      "EPOCH: 41 val Results: Prec@1 55.210 Loss: 1.2666\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [42][0/390]\tTime 0.007 (0.007)\tLoss 0.9582 (0.9582)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [42][78/390]\tTime 0.003 (0.003)\tLoss 0.9995 (1.0072)\tPrec@1 62.500 (64.300)\n",
      "Epoch: [42][156/390]\tTime 0.002 (0.003)\tLoss 1.0475 (1.0276)\tPrec@1 63.281 (63.630)\n",
      "Epoch: [42][234/390]\tTime 0.002 (0.003)\tLoss 1.1259 (1.0500)\tPrec@1 60.156 (62.726)\n",
      "Epoch: [42][312/390]\tTime 0.004 (0.003)\tLoss 1.1452 (1.0668)\tPrec@1 59.375 (62.013)\n",
      "Epoch: [42][390/390]\tTime 0.004 (0.003)\tLoss 1.2064 (1.0828)\tPrec@1 60.000 (61.388)\n",
      "EPOCH: 42 train Results: Prec@1 61.388 Loss: 1.0828\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2009 (1.2009)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2749 (1.2797)\tPrec@1 43.750 (54.790)\n",
      "EPOCH: 42 val Results: Prec@1 54.790 Loss: 1.2797\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [43][0/390]\tTime 0.002 (0.002)\tLoss 1.0460 (1.0460)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [43][78/390]\tTime 0.002 (0.003)\tLoss 1.0962 (1.0096)\tPrec@1 61.719 (63.726)\n",
      "Epoch: [43][156/390]\tTime 0.004 (0.003)\tLoss 1.2716 (1.0372)\tPrec@1 54.688 (62.809)\n",
      "Epoch: [43][234/390]\tTime 0.003 (0.003)\tLoss 1.1639 (1.0514)\tPrec@1 60.156 (62.337)\n",
      "Epoch: [43][312/390]\tTime 0.010 (0.003)\tLoss 1.0386 (1.0707)\tPrec@1 64.062 (61.751)\n",
      "Epoch: [43][390/390]\tTime 0.004 (0.003)\tLoss 0.9912 (1.0805)\tPrec@1 61.250 (61.426)\n",
      "EPOCH: 43 train Results: Prec@1 61.426 Loss: 1.0805\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1542 (1.1542)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4859 (1.2732)\tPrec@1 43.750 (55.470)\n",
      "EPOCH: 43 val Results: Prec@1 55.470 Loss: 1.2732\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [44][0/390]\tTime 0.002 (0.002)\tLoss 0.9624 (0.9624)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.003)\tLoss 0.9852 (1.0151)\tPrec@1 60.938 (63.934)\n",
      "Epoch: [44][156/390]\tTime 0.005 (0.003)\tLoss 1.0796 (1.0364)\tPrec@1 60.156 (63.231)\n",
      "Epoch: [44][234/390]\tTime 0.002 (0.003)\tLoss 0.9295 (1.0551)\tPrec@1 66.406 (62.560)\n",
      "Epoch: [44][312/390]\tTime 0.003 (0.003)\tLoss 1.0767 (1.0709)\tPrec@1 57.812 (61.791)\n",
      "Epoch: [44][390/390]\tTime 0.002 (0.003)\tLoss 1.1669 (1.0822)\tPrec@1 65.000 (61.286)\n",
      "EPOCH: 44 train Results: Prec@1 61.286 Loss: 1.0822\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2492 (1.2492)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5318 (1.2907)\tPrec@1 25.000 (53.950)\n",
      "EPOCH: 44 val Results: Prec@1 53.950 Loss: 1.2907\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [45][0/390]\tTime 0.002 (0.002)\tLoss 1.0854 (1.0854)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [45][78/390]\tTime 0.003 (0.003)\tLoss 1.1013 (1.0050)\tPrec@1 60.938 (64.033)\n",
      "Epoch: [45][156/390]\tTime 0.003 (0.003)\tLoss 1.0999 (1.0303)\tPrec@1 57.812 (63.182)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.003)\tLoss 1.1068 (1.0481)\tPrec@1 60.938 (62.407)\n",
      "Epoch: [45][312/390]\tTime 0.002 (0.003)\tLoss 1.1178 (1.0667)\tPrec@1 59.375 (61.886)\n",
      "Epoch: [45][390/390]\tTime 0.004 (0.003)\tLoss 1.1554 (1.0776)\tPrec@1 57.500 (61.524)\n",
      "EPOCH: 45 train Results: Prec@1 61.524 Loss: 1.0776\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2332 (1.2332)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4293 (1.2866)\tPrec@1 56.250 (53.750)\n",
      "EPOCH: 45 val Results: Prec@1 53.750 Loss: 1.2866\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [46][0/390]\tTime 0.005 (0.005)\tLoss 1.0349 (1.0349)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [46][78/390]\tTime 0.002 (0.003)\tLoss 0.9916 (1.0064)\tPrec@1 64.062 (64.092)\n",
      "Epoch: [46][156/390]\tTime 0.003 (0.003)\tLoss 1.0570 (1.0236)\tPrec@1 60.938 (63.321)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.0947 (1.0453)\tPrec@1 58.594 (62.520)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.003)\tLoss 1.1043 (1.0574)\tPrec@1 60.156 (62.203)\n",
      "Epoch: [46][390/390]\tTime 0.001 (0.003)\tLoss 1.1682 (1.0740)\tPrec@1 60.000 (61.682)\n",
      "EPOCH: 46 train Results: Prec@1 61.682 Loss: 1.0740\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1684 (1.1684)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5721 (1.2974)\tPrec@1 31.250 (54.350)\n",
      "EPOCH: 46 val Results: Prec@1 54.350 Loss: 1.2974\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [47][0/390]\tTime 0.002 (0.002)\tLoss 1.2586 (1.2586)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [47][78/390]\tTime 0.003 (0.004)\tLoss 1.2039 (0.9863)\tPrec@1 59.375 (65.279)\n",
      "Epoch: [47][156/390]\tTime 0.005 (0.003)\tLoss 1.0902 (1.0176)\tPrec@1 62.500 (63.988)\n",
      "Epoch: [47][234/390]\tTime 0.004 (0.003)\tLoss 1.0539 (1.0473)\tPrec@1 64.844 (62.945)\n",
      "Epoch: [47][312/390]\tTime 0.005 (0.003)\tLoss 1.0108 (1.0612)\tPrec@1 65.625 (62.398)\n",
      "Epoch: [47][390/390]\tTime 0.001 (0.003)\tLoss 1.2024 (1.0744)\tPrec@1 51.250 (61.922)\n",
      "EPOCH: 47 train Results: Prec@1 61.922 Loss: 1.0744\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1427 (1.1427)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2208 (1.2789)\tPrec@1 56.250 (54.500)\n",
      "EPOCH: 47 val Results: Prec@1 54.500 Loss: 1.2789\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [48][0/390]\tTime 0.002 (0.002)\tLoss 0.9331 (0.9331)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [48][78/390]\tTime 0.002 (0.003)\tLoss 0.9649 (1.0086)\tPrec@1 67.969 (63.875)\n",
      "Epoch: [48][156/390]\tTime 0.002 (0.003)\tLoss 1.0866 (1.0295)\tPrec@1 60.938 (63.236)\n",
      "Epoch: [48][234/390]\tTime 0.003 (0.003)\tLoss 1.0060 (1.0442)\tPrec@1 65.625 (62.706)\n",
      "Epoch: [48][312/390]\tTime 0.003 (0.003)\tLoss 1.1125 (1.0597)\tPrec@1 59.375 (62.203)\n",
      "Epoch: [48][390/390]\tTime 0.001 (0.003)\tLoss 1.4226 (1.0718)\tPrec@1 56.250 (61.916)\n",
      "EPOCH: 48 train Results: Prec@1 61.916 Loss: 1.0718\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1313 (1.1313)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5982 (1.2819)\tPrec@1 43.750 (54.850)\n",
      "EPOCH: 48 val Results: Prec@1 54.850 Loss: 1.2819\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [49][0/390]\tTime 0.002 (0.002)\tLoss 0.9880 (0.9880)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [49][78/390]\tTime 0.002 (0.003)\tLoss 1.1662 (0.9812)\tPrec@1 58.594 (65.447)\n",
      "Epoch: [49][156/390]\tTime 0.005 (0.003)\tLoss 1.1849 (1.0238)\tPrec@1 62.500 (63.654)\n",
      "Epoch: [49][234/390]\tTime 0.008 (0.003)\tLoss 1.0452 (1.0426)\tPrec@1 67.188 (62.995)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.0764 (1.0575)\tPrec@1 62.500 (62.363)\n",
      "Epoch: [49][390/390]\tTime 0.003 (0.003)\tLoss 1.2710 (1.0731)\tPrec@1 53.750 (61.818)\n",
      "EPOCH: 49 train Results: Prec@1 61.818 Loss: 1.0731\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2430 (1.2430)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3481 (1.2727)\tPrec@1 50.000 (55.270)\n",
      "EPOCH: 49 val Results: Prec@1 55.270 Loss: 1.2727\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [50][0/390]\tTime 0.005 (0.005)\tLoss 1.0523 (1.0523)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [50][78/390]\tTime 0.002 (0.003)\tLoss 1.0600 (1.0076)\tPrec@1 64.844 (64.003)\n",
      "Epoch: [50][156/390]\tTime 0.005 (0.003)\tLoss 1.0913 (1.0293)\tPrec@1 58.594 (63.356)\n",
      "Epoch: [50][234/390]\tTime 0.002 (0.003)\tLoss 1.1260 (1.0528)\tPrec@1 59.375 (62.643)\n",
      "Epoch: [50][312/390]\tTime 0.004 (0.003)\tLoss 1.0610 (1.0659)\tPrec@1 64.844 (62.283)\n",
      "Epoch: [50][390/390]\tTime 0.001 (0.003)\tLoss 1.0818 (1.0796)\tPrec@1 62.500 (61.752)\n",
      "EPOCH: 50 train Results: Prec@1 61.752 Loss: 1.0796\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1587 (1.1587)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5379 (1.2765)\tPrec@1 31.250 (54.830)\n",
      "EPOCH: 50 val Results: Prec@1 54.830 Loss: 1.2765\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [51][0/390]\tTime 0.002 (0.002)\tLoss 1.0543 (1.0543)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [51][78/390]\tTime 0.005 (0.003)\tLoss 1.0220 (1.0064)\tPrec@1 63.281 (63.825)\n",
      "Epoch: [51][156/390]\tTime 0.007 (0.003)\tLoss 1.1263 (1.0293)\tPrec@1 61.719 (63.172)\n",
      "Epoch: [51][234/390]\tTime 0.002 (0.003)\tLoss 1.1837 (1.0437)\tPrec@1 61.719 (62.763)\n",
      "Epoch: [51][312/390]\tTime 0.008 (0.003)\tLoss 0.9957 (1.0586)\tPrec@1 68.750 (62.395)\n",
      "Epoch: [51][390/390]\tTime 0.003 (0.003)\tLoss 1.1301 (1.0730)\tPrec@1 53.750 (61.796)\n",
      "EPOCH: 51 train Results: Prec@1 61.796 Loss: 1.0730\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2996 (1.2996)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3418 (1.2808)\tPrec@1 50.000 (54.790)\n",
      "EPOCH: 51 val Results: Prec@1 54.790 Loss: 1.2808\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [52][0/390]\tTime 0.003 (0.003)\tLoss 0.9591 (0.9591)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [52][78/390]\tTime 0.003 (0.004)\tLoss 1.0336 (0.9907)\tPrec@1 61.719 (64.695)\n",
      "Epoch: [52][156/390]\tTime 0.005 (0.003)\tLoss 1.0796 (1.0160)\tPrec@1 57.031 (63.988)\n",
      "Epoch: [52][234/390]\tTime 0.008 (0.003)\tLoss 0.9910 (1.0397)\tPrec@1 65.625 (62.909)\n",
      "Epoch: [52][312/390]\tTime 0.005 (0.003)\tLoss 1.3333 (1.0592)\tPrec@1 46.875 (62.156)\n",
      "Epoch: [52][390/390]\tTime 0.001 (0.003)\tLoss 1.2540 (1.0713)\tPrec@1 60.000 (61.828)\n",
      "EPOCH: 52 train Results: Prec@1 61.828 Loss: 1.0713\n",
      "Test: [0/78]\tTime 0.014 (0.014)\tLoss 1.1736 (1.1736)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2819 (1.2800)\tPrec@1 43.750 (54.470)\n",
      "EPOCH: 52 val Results: Prec@1 54.470 Loss: 1.2800\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [53][0/390]\tTime 0.004 (0.004)\tLoss 1.0487 (1.0487)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [53][78/390]\tTime 0.002 (0.003)\tLoss 1.0103 (0.9852)\tPrec@1 62.500 (65.140)\n",
      "Epoch: [53][156/390]\tTime 0.002 (0.003)\tLoss 1.0360 (1.0180)\tPrec@1 59.375 (63.525)\n",
      "Epoch: [53][234/390]\tTime 0.004 (0.003)\tLoss 1.1756 (1.0397)\tPrec@1 60.156 (62.836)\n",
      "Epoch: [53][312/390]\tTime 0.004 (0.003)\tLoss 1.4020 (1.0564)\tPrec@1 50.781 (62.178)\n",
      "Epoch: [53][390/390]\tTime 0.001 (0.003)\tLoss 1.1508 (1.0723)\tPrec@1 65.000 (61.646)\n",
      "EPOCH: 53 train Results: Prec@1 61.646 Loss: 1.0723\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2483 (1.2483)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4761 (1.2736)\tPrec@1 31.250 (54.560)\n",
      "EPOCH: 53 val Results: Prec@1 54.560 Loss: 1.2736\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [54][0/390]\tTime 0.004 (0.004)\tLoss 0.9295 (0.9295)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [54][78/390]\tTime 0.002 (0.003)\tLoss 1.0369 (0.9938)\tPrec@1 64.844 (64.577)\n",
      "Epoch: [54][156/390]\tTime 0.004 (0.003)\tLoss 0.9917 (1.0212)\tPrec@1 63.281 (63.769)\n",
      "Epoch: [54][234/390]\tTime 0.002 (0.003)\tLoss 0.9871 (1.0446)\tPrec@1 67.188 (62.803)\n",
      "Epoch: [54][312/390]\tTime 0.004 (0.003)\tLoss 1.3116 (1.0605)\tPrec@1 63.281 (62.181)\n",
      "Epoch: [54][390/390]\tTime 0.001 (0.003)\tLoss 1.2400 (1.0751)\tPrec@1 60.000 (61.710)\n",
      "EPOCH: 54 train Results: Prec@1 61.710 Loss: 1.0751\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1561 (1.1561)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4297 (1.2817)\tPrec@1 56.250 (54.470)\n",
      "EPOCH: 54 val Results: Prec@1 54.470 Loss: 1.2817\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.0466 (1.0466)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.1490 (0.9927)\tPrec@1 62.500 (64.903)\n",
      "Epoch: [55][156/390]\tTime 0.002 (0.003)\tLoss 1.0967 (1.0176)\tPrec@1 61.719 (63.769)\n",
      "Epoch: [55][234/390]\tTime 0.002 (0.003)\tLoss 1.2043 (1.0430)\tPrec@1 58.594 (62.866)\n",
      "Epoch: [55][312/390]\tTime 0.003 (0.003)\tLoss 0.8842 (1.0578)\tPrec@1 70.312 (62.113)\n",
      "Epoch: [55][390/390]\tTime 0.001 (0.003)\tLoss 1.0986 (1.0723)\tPrec@1 55.000 (61.596)\n",
      "EPOCH: 55 train Results: Prec@1 61.596 Loss: 1.0723\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2139 (1.2139)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5488 (1.2769)\tPrec@1 43.750 (54.840)\n",
      "EPOCH: 55 val Results: Prec@1 54.840 Loss: 1.2769\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [56][0/390]\tTime 0.003 (0.003)\tLoss 0.8653 (0.8653)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [56][78/390]\tTime 0.004 (0.003)\tLoss 0.9948 (1.0162)\tPrec@1 60.938 (63.528)\n",
      "Epoch: [56][156/390]\tTime 0.004 (0.003)\tLoss 0.9221 (1.0204)\tPrec@1 71.094 (63.445)\n",
      "Epoch: [56][234/390]\tTime 0.002 (0.003)\tLoss 1.0643 (1.0386)\tPrec@1 59.375 (62.739)\n",
      "Epoch: [56][312/390]\tTime 0.008 (0.003)\tLoss 0.9309 (1.0538)\tPrec@1 66.406 (62.200)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.003)\tLoss 1.2578 (1.0701)\tPrec@1 53.750 (61.738)\n",
      "EPOCH: 56 train Results: Prec@1 61.738 Loss: 1.0701\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1455 (1.1455)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3831 (1.2717)\tPrec@1 50.000 (55.660)\n",
      "EPOCH: 56 val Results: Prec@1 55.660 Loss: 1.2717\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [57][0/390]\tTime 0.005 (0.005)\tLoss 1.0013 (1.0013)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [57][78/390]\tTime 0.002 (0.005)\tLoss 0.9728 (0.9912)\tPrec@1 65.625 (64.893)\n",
      "Epoch: [57][156/390]\tTime 0.004 (0.004)\tLoss 0.9530 (1.0264)\tPrec@1 67.188 (63.550)\n",
      "Epoch: [57][234/390]\tTime 0.037 (0.008)\tLoss 1.1064 (1.0438)\tPrec@1 63.281 (62.812)\n",
      "Epoch: [57][312/390]\tTime 0.022 (0.011)\tLoss 1.1458 (1.0574)\tPrec@1 57.812 (62.273)\n",
      "Epoch: [57][390/390]\tTime 0.021 (0.013)\tLoss 1.1211 (1.0720)\tPrec@1 58.750 (61.786)\n",
      "EPOCH: 57 train Results: Prec@1 61.786 Loss: 1.0720\n",
      "Test: [0/78]\tTime 0.018 (0.018)\tLoss 1.1589 (1.1589)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.011 (0.012)\tLoss 1.4586 (1.2799)\tPrec@1 37.500 (54.760)\n",
      "EPOCH: 57 val Results: Prec@1 54.760 Loss: 1.2799\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [58][0/390]\tTime 0.039 (0.039)\tLoss 0.9631 (0.9631)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [58][78/390]\tTime 0.040 (0.028)\tLoss 1.0176 (0.9865)\tPrec@1 67.969 (64.854)\n",
      "Epoch: [58][156/390]\tTime 0.029 (0.026)\tLoss 1.0685 (1.0209)\tPrec@1 62.500 (63.560)\n",
      "Epoch: [58][234/390]\tTime 0.005 (0.019)\tLoss 0.9947 (1.0411)\tPrec@1 61.719 (62.856)\n",
      "Epoch: [58][312/390]\tTime 0.008 (0.015)\tLoss 0.9573 (1.0528)\tPrec@1 66.406 (62.403)\n",
      "Epoch: [58][390/390]\tTime 0.001 (0.013)\tLoss 1.1471 (1.0662)\tPrec@1 61.250 (61.876)\n",
      "EPOCH: 58 train Results: Prec@1 61.876 Loss: 1.0662\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1580 (1.1580)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2817 (1.2697)\tPrec@1 43.750 (55.050)\n",
      "EPOCH: 58 val Results: Prec@1 55.050 Loss: 1.2697\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [59][0/390]\tTime 0.004 (0.004)\tLoss 0.9925 (0.9925)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [59][78/390]\tTime 0.002 (0.003)\tLoss 1.1198 (1.0002)\tPrec@1 61.719 (64.873)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.1455 (1.0194)\tPrec@1 59.375 (63.809)\n",
      "Epoch: [59][234/390]\tTime 0.004 (0.003)\tLoss 1.0412 (1.0432)\tPrec@1 61.719 (62.729)\n",
      "Epoch: [59][312/390]\tTime 0.002 (0.004)\tLoss 1.1666 (1.0599)\tPrec@1 60.156 (62.166)\n",
      "Epoch: [59][390/390]\tTime 0.003 (0.003)\tLoss 1.1380 (1.0706)\tPrec@1 56.250 (61.796)\n",
      "EPOCH: 59 train Results: Prec@1 61.796 Loss: 1.0706\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2382 (1.2382)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2687 (1.2976)\tPrec@1 50.000 (54.570)\n",
      "EPOCH: 59 val Results: Prec@1 54.570 Loss: 1.2976\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [60][0/390]\tTime 0.004 (0.004)\tLoss 0.9158 (0.9158)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [60][78/390]\tTime 0.004 (0.003)\tLoss 1.0133 (0.9948)\tPrec@1 58.594 (64.765)\n",
      "Epoch: [60][156/390]\tTime 0.005 (0.003)\tLoss 0.9862 (1.0248)\tPrec@1 60.156 (63.809)\n",
      "Epoch: [60][234/390]\tTime 0.003 (0.003)\tLoss 1.0858 (1.0432)\tPrec@1 60.938 (62.889)\n",
      "Epoch: [60][312/390]\tTime 0.002 (0.003)\tLoss 1.1822 (1.0595)\tPrec@1 56.250 (62.380)\n",
      "Epoch: [60][390/390]\tTime 0.002 (0.003)\tLoss 1.3171 (1.0704)\tPrec@1 50.000 (62.002)\n",
      "EPOCH: 60 train Results: Prec@1 62.002 Loss: 1.0704\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1514 (1.1514)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1446 (1.2835)\tPrec@1 43.750 (54.910)\n",
      "EPOCH: 60 val Results: Prec@1 54.910 Loss: 1.2835\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [61][0/390]\tTime 0.004 (0.004)\tLoss 0.9072 (0.9072)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [61][78/390]\tTime 0.011 (0.003)\tLoss 0.9910 (0.9858)\tPrec@1 63.281 (64.260)\n",
      "Epoch: [61][156/390]\tTime 0.002 (0.003)\tLoss 1.0420 (1.0129)\tPrec@1 65.625 (63.565)\n",
      "Epoch: [61][234/390]\tTime 0.003 (0.003)\tLoss 1.0848 (1.0374)\tPrec@1 55.469 (62.653)\n",
      "Epoch: [61][312/390]\tTime 0.004 (0.003)\tLoss 1.2083 (1.0551)\tPrec@1 59.375 (62.156)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.003)\tLoss 1.1005 (1.0655)\tPrec@1 63.750 (61.770)\n",
      "EPOCH: 61 train Results: Prec@1 61.770 Loss: 1.0655\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1895 (1.1895)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7674 (1.2631)\tPrec@1 25.000 (54.920)\n",
      "EPOCH: 61 val Results: Prec@1 54.920 Loss: 1.2631\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [62][0/390]\tTime 0.003 (0.003)\tLoss 0.9968 (0.9968)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.0617 (0.9917)\tPrec@1 66.406 (64.982)\n",
      "Epoch: [62][156/390]\tTime 0.003 (0.003)\tLoss 0.9058 (1.0198)\tPrec@1 67.969 (63.714)\n",
      "Epoch: [62][234/390]\tTime 0.002 (0.003)\tLoss 1.2886 (1.0417)\tPrec@1 56.250 (62.759)\n",
      "Epoch: [62][312/390]\tTime 0.002 (0.003)\tLoss 1.1485 (1.0577)\tPrec@1 55.469 (62.058)\n",
      "Epoch: [62][390/390]\tTime 0.002 (0.003)\tLoss 0.8766 (1.0707)\tPrec@1 67.500 (61.634)\n",
      "EPOCH: 62 train Results: Prec@1 61.634 Loss: 1.0707\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1791 (1.1791)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2939 (1.2685)\tPrec@1 37.500 (54.910)\n",
      "EPOCH: 62 val Results: Prec@1 54.910 Loss: 1.2685\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [63][0/390]\tTime 0.003 (0.003)\tLoss 0.9571 (0.9571)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [63][78/390]\tTime 0.004 (0.003)\tLoss 1.1089 (0.9909)\tPrec@1 58.594 (64.953)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.003)\tLoss 1.1768 (1.0210)\tPrec@1 61.719 (63.600)\n",
      "Epoch: [63][234/390]\tTime 0.003 (0.003)\tLoss 1.1468 (1.0466)\tPrec@1 57.031 (62.689)\n",
      "Epoch: [63][312/390]\tTime 0.003 (0.003)\tLoss 1.2555 (1.0625)\tPrec@1 53.125 (62.051)\n",
      "Epoch: [63][390/390]\tTime 0.001 (0.003)\tLoss 1.2378 (1.0736)\tPrec@1 58.750 (61.690)\n",
      "EPOCH: 63 train Results: Prec@1 61.690 Loss: 1.0736\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2051 (1.2051)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4459 (1.2618)\tPrec@1 31.250 (55.260)\n",
      "EPOCH: 63 val Results: Prec@1 55.260 Loss: 1.2618\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [64][0/390]\tTime 0.002 (0.002)\tLoss 0.9491 (0.9491)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [64][78/390]\tTime 0.002 (0.003)\tLoss 0.9531 (0.9738)\tPrec@1 70.312 (65.526)\n",
      "Epoch: [64][156/390]\tTime 0.003 (0.003)\tLoss 1.0994 (1.0128)\tPrec@1 60.938 (64.087)\n",
      "Epoch: [64][234/390]\tTime 0.002 (0.003)\tLoss 1.1740 (1.0373)\tPrec@1 59.375 (62.902)\n",
      "Epoch: [64][312/390]\tTime 0.003 (0.003)\tLoss 1.1752 (1.0548)\tPrec@1 56.250 (62.368)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.003)\tLoss 1.2526 (1.0703)\tPrec@1 52.500 (61.868)\n",
      "EPOCH: 64 train Results: Prec@1 61.868 Loss: 1.0703\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1988 (1.1988)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0134 (1.2810)\tPrec@1 43.750 (55.120)\n",
      "EPOCH: 64 val Results: Prec@1 55.120 Loss: 1.2810\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [65][0/390]\tTime 0.010 (0.010)\tLoss 0.9545 (0.9545)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.004)\tLoss 1.0078 (0.9953)\tPrec@1 61.719 (64.765)\n",
      "Epoch: [65][156/390]\tTime 0.004 (0.004)\tLoss 1.0430 (1.0184)\tPrec@1 64.062 (63.804)\n",
      "Epoch: [65][234/390]\tTime 0.002 (0.004)\tLoss 1.1034 (1.0412)\tPrec@1 59.375 (62.955)\n",
      "Epoch: [65][312/390]\tTime 0.002 (0.004)\tLoss 1.0684 (1.0565)\tPrec@1 60.156 (62.323)\n",
      "Epoch: [65][390/390]\tTime 0.003 (0.004)\tLoss 0.9935 (1.0668)\tPrec@1 68.750 (61.888)\n",
      "EPOCH: 65 train Results: Prec@1 61.888 Loss: 1.0668\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1839 (1.1839)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3617 (1.2647)\tPrec@1 37.500 (55.140)\n",
      "EPOCH: 65 val Results: Prec@1 55.140 Loss: 1.2647\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [66][0/390]\tTime 0.003 (0.003)\tLoss 0.9819 (0.9819)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [66][78/390]\tTime 0.002 (0.003)\tLoss 1.1325 (0.9878)\tPrec@1 55.469 (65.061)\n",
      "Epoch: [66][156/390]\tTime 0.004 (0.003)\tLoss 1.0181 (1.0181)\tPrec@1 66.406 (63.913)\n",
      "Epoch: [66][234/390]\tTime 0.003 (0.003)\tLoss 1.3402 (1.0407)\tPrec@1 54.688 (63.009)\n",
      "Epoch: [66][312/390]\tTime 0.005 (0.003)\tLoss 1.1578 (1.0603)\tPrec@1 62.500 (62.323)\n",
      "Epoch: [66][390/390]\tTime 0.002 (0.003)\tLoss 1.0183 (1.0706)\tPrec@1 67.500 (61.866)\n",
      "EPOCH: 66 train Results: Prec@1 61.866 Loss: 1.0706\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1992 (1.1992)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4401 (1.2696)\tPrec@1 43.750 (54.610)\n",
      "EPOCH: 66 val Results: Prec@1 54.610 Loss: 1.2696\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [67][0/390]\tTime 0.002 (0.002)\tLoss 1.0221 (1.0221)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [67][78/390]\tTime 0.004 (0.003)\tLoss 1.1872 (0.9884)\tPrec@1 57.812 (65.645)\n",
      "Epoch: [67][156/390]\tTime 0.002 (0.003)\tLoss 1.1171 (1.0127)\tPrec@1 61.719 (64.296)\n",
      "Epoch: [67][234/390]\tTime 0.002 (0.003)\tLoss 1.2195 (1.0395)\tPrec@1 55.469 (63.195)\n",
      "Epoch: [67][312/390]\tTime 0.005 (0.003)\tLoss 1.1266 (1.0542)\tPrec@1 60.938 (62.530)\n",
      "Epoch: [67][390/390]\tTime 0.002 (0.003)\tLoss 1.0659 (1.0661)\tPrec@1 66.250 (61.966)\n",
      "EPOCH: 67 train Results: Prec@1 61.966 Loss: 1.0661\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2016 (1.2016)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0030 (1.2718)\tPrec@1 62.500 (54.640)\n",
      "EPOCH: 67 val Results: Prec@1 54.640 Loss: 1.2718\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [68][0/390]\tTime 0.005 (0.005)\tLoss 0.8798 (0.8798)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [68][78/390]\tTime 0.002 (0.003)\tLoss 0.9083 (0.9747)\tPrec@1 67.188 (65.398)\n",
      "Epoch: [68][156/390]\tTime 0.002 (0.003)\tLoss 1.2222 (1.0190)\tPrec@1 60.156 (63.570)\n",
      "Epoch: [68][234/390]\tTime 0.005 (0.003)\tLoss 1.0991 (1.0375)\tPrec@1 61.719 (62.862)\n",
      "Epoch: [68][312/390]\tTime 0.002 (0.003)\tLoss 1.2106 (1.0541)\tPrec@1 55.469 (62.460)\n",
      "Epoch: [68][390/390]\tTime 0.003 (0.003)\tLoss 1.3580 (1.0659)\tPrec@1 53.750 (62.004)\n",
      "EPOCH: 68 train Results: Prec@1 62.004 Loss: 1.0659\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1050 (1.1050)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8262 (1.2645)\tPrec@1 62.500 (54.920)\n",
      "EPOCH: 68 val Results: Prec@1 54.920 Loss: 1.2645\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [69][0/390]\tTime 0.004 (0.004)\tLoss 0.9525 (0.9525)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.003)\tLoss 1.0198 (0.9777)\tPrec@1 60.156 (65.833)\n",
      "Epoch: [69][156/390]\tTime 0.002 (0.003)\tLoss 1.0128 (1.0094)\tPrec@1 70.312 (64.476)\n",
      "Epoch: [69][234/390]\tTime 0.002 (0.004)\tLoss 0.9818 (1.0323)\tPrec@1 61.719 (63.561)\n",
      "Epoch: [69][312/390]\tTime 0.002 (0.004)\tLoss 1.1322 (1.0498)\tPrec@1 62.500 (62.690)\n",
      "Epoch: [69][390/390]\tTime 0.003 (0.003)\tLoss 1.2601 (1.0654)\tPrec@1 56.250 (62.114)\n",
      "EPOCH: 69 train Results: Prec@1 62.114 Loss: 1.0654\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2196 (1.2196)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3725 (1.2743)\tPrec@1 50.000 (55.430)\n",
      "EPOCH: 69 val Results: Prec@1 55.430 Loss: 1.2743\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [70][0/390]\tTime 0.003 (0.003)\tLoss 0.7541 (0.7541)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [70][78/390]\tTime 0.003 (0.003)\tLoss 1.2142 (0.9845)\tPrec@1 51.562 (64.893)\n",
      "Epoch: [70][156/390]\tTime 0.002 (0.003)\tLoss 1.1103 (1.0141)\tPrec@1 63.281 (63.709)\n",
      "Epoch: [70][234/390]\tTime 0.003 (0.003)\tLoss 1.0225 (1.0340)\tPrec@1 67.969 (63.331)\n",
      "Epoch: [70][312/390]\tTime 0.004 (0.004)\tLoss 1.0710 (1.0564)\tPrec@1 60.938 (62.507)\n",
      "Epoch: [70][390/390]\tTime 0.003 (0.004)\tLoss 1.1094 (1.0667)\tPrec@1 55.000 (62.032)\n",
      "EPOCH: 70 train Results: Prec@1 62.032 Loss: 1.0667\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2125 (1.2125)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4177 (1.2719)\tPrec@1 50.000 (54.550)\n",
      "EPOCH: 70 val Results: Prec@1 54.550 Loss: 1.2719\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [71][0/390]\tTime 0.002 (0.002)\tLoss 1.0286 (1.0286)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [71][78/390]\tTime 0.004 (0.003)\tLoss 1.0175 (0.9819)\tPrec@1 62.500 (65.378)\n",
      "Epoch: [71][156/390]\tTime 0.005 (0.003)\tLoss 1.0056 (1.0152)\tPrec@1 63.281 (64.107)\n",
      "Epoch: [71][234/390]\tTime 0.002 (0.003)\tLoss 0.9194 (1.0388)\tPrec@1 62.500 (63.108)\n",
      "Epoch: [71][312/390]\tTime 0.002 (0.003)\tLoss 1.0313 (1.0508)\tPrec@1 60.156 (62.632)\n",
      "Epoch: [71][390/390]\tTime 0.001 (0.003)\tLoss 0.9820 (1.0651)\tPrec@1 67.500 (61.920)\n",
      "EPOCH: 71 train Results: Prec@1 61.920 Loss: 1.0651\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2738 (1.2738)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2815 (1.2779)\tPrec@1 37.500 (54.970)\n",
      "EPOCH: 71 val Results: Prec@1 54.970 Loss: 1.2779\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [72][0/390]\tTime 0.004 (0.004)\tLoss 1.0390 (1.0390)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [72][78/390]\tTime 0.002 (0.003)\tLoss 1.2249 (0.9866)\tPrec@1 54.688 (65.487)\n",
      "Epoch: [72][156/390]\tTime 0.004 (0.003)\tLoss 1.1287 (1.0140)\tPrec@1 60.938 (64.157)\n",
      "Epoch: [72][234/390]\tTime 0.005 (0.003)\tLoss 1.1005 (1.0336)\tPrec@1 64.062 (63.314)\n",
      "Epoch: [72][312/390]\tTime 0.007 (0.003)\tLoss 1.2333 (1.0517)\tPrec@1 58.594 (62.680)\n",
      "Epoch: [72][390/390]\tTime 0.001 (0.003)\tLoss 0.8444 (1.0626)\tPrec@1 65.000 (62.222)\n",
      "EPOCH: 72 train Results: Prec@1 62.222 Loss: 1.0626\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1258 (1.1258)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4061 (1.2753)\tPrec@1 56.250 (54.810)\n",
      "EPOCH: 72 val Results: Prec@1 54.810 Loss: 1.2753\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [73][0/390]\tTime 0.003 (0.003)\tLoss 0.9689 (0.9689)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.003)\tLoss 0.9899 (0.9910)\tPrec@1 64.062 (64.784)\n",
      "Epoch: [73][156/390]\tTime 0.002 (0.004)\tLoss 1.0963 (1.0106)\tPrec@1 63.281 (63.993)\n",
      "Epoch: [73][234/390]\tTime 0.003 (0.004)\tLoss 0.9948 (1.0305)\tPrec@1 63.281 (63.075)\n",
      "Epoch: [73][312/390]\tTime 0.002 (0.004)\tLoss 1.0090 (1.0519)\tPrec@1 61.719 (62.288)\n",
      "Epoch: [73][390/390]\tTime 0.007 (0.004)\tLoss 1.1782 (1.0619)\tPrec@1 53.750 (61.948)\n",
      "EPOCH: 73 train Results: Prec@1 61.948 Loss: 1.0619\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1705 (1.1705)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 0.9942 (1.2790)\tPrec@1 68.750 (55.110)\n",
      "EPOCH: 73 val Results: Prec@1 55.110 Loss: 1.2790\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [74][0/390]\tTime 0.002 (0.002)\tLoss 1.0388 (1.0388)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [74][78/390]\tTime 0.002 (0.003)\tLoss 0.9061 (0.9887)\tPrec@1 66.406 (65.220)\n",
      "Epoch: [74][156/390]\tTime 0.004 (0.003)\tLoss 0.9261 (1.0207)\tPrec@1 64.844 (63.819)\n",
      "Epoch: [74][234/390]\tTime 0.003 (0.003)\tLoss 1.0667 (1.0404)\tPrec@1 66.406 (63.049)\n",
      "Epoch: [74][312/390]\tTime 0.002 (0.003)\tLoss 1.0523 (1.0523)\tPrec@1 60.156 (62.410)\n",
      "Epoch: [74][390/390]\tTime 0.001 (0.003)\tLoss 1.1253 (1.0637)\tPrec@1 63.750 (62.070)\n",
      "EPOCH: 74 train Results: Prec@1 62.070 Loss: 1.0637\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2022 (1.2022)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4141 (1.2841)\tPrec@1 43.750 (54.530)\n",
      "EPOCH: 74 val Results: Prec@1 54.530 Loss: 1.2841\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [75][0/390]\tTime 0.004 (0.004)\tLoss 0.9669 (0.9669)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [75][78/390]\tTime 0.003 (0.004)\tLoss 1.2110 (0.9807)\tPrec@1 60.938 (64.893)\n",
      "Epoch: [75][156/390]\tTime 0.002 (0.004)\tLoss 0.8740 (1.0158)\tPrec@1 69.531 (63.286)\n",
      "Epoch: [75][234/390]\tTime 0.004 (0.004)\tLoss 1.2140 (1.0411)\tPrec@1 55.469 (62.663)\n",
      "Epoch: [75][312/390]\tTime 0.004 (0.004)\tLoss 1.2055 (1.0575)\tPrec@1 55.469 (62.148)\n",
      "Epoch: [75][390/390]\tTime 0.002 (0.003)\tLoss 1.1497 (1.0701)\tPrec@1 57.500 (61.730)\n",
      "EPOCH: 75 train Results: Prec@1 61.730 Loss: 1.0701\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1655 (1.1655)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2079 (1.2671)\tPrec@1 43.750 (55.190)\n",
      "EPOCH: 75 val Results: Prec@1 55.190 Loss: 1.2671\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [76][0/390]\tTime 0.002 (0.002)\tLoss 1.1168 (1.1168)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.003)\tLoss 1.0235 (0.9857)\tPrec@1 61.719 (64.735)\n",
      "Epoch: [76][156/390]\tTime 0.002 (0.003)\tLoss 1.0222 (1.0168)\tPrec@1 67.969 (63.774)\n",
      "Epoch: [76][234/390]\tTime 0.002 (0.003)\tLoss 1.0875 (1.0409)\tPrec@1 65.625 (62.779)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 0.9003 (1.0536)\tPrec@1 65.625 (62.270)\n",
      "Epoch: [76][390/390]\tTime 0.006 (0.003)\tLoss 0.8558 (1.0664)\tPrec@1 66.250 (61.786)\n",
      "EPOCH: 76 train Results: Prec@1 61.786 Loss: 1.0664\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1925 (1.1925)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6263 (1.2688)\tPrec@1 43.750 (55.060)\n",
      "EPOCH: 76 val Results: Prec@1 55.060 Loss: 1.2688\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [77][0/390]\tTime 0.006 (0.006)\tLoss 0.9726 (0.9726)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [77][78/390]\tTime 0.002 (0.003)\tLoss 1.0122 (0.9954)\tPrec@1 64.844 (64.241)\n",
      "Epoch: [77][156/390]\tTime 0.003 (0.003)\tLoss 1.1409 (1.0157)\tPrec@1 57.812 (63.416)\n",
      "Epoch: [77][234/390]\tTime 0.002 (0.003)\tLoss 1.0163 (1.0361)\tPrec@1 65.625 (62.680)\n",
      "Epoch: [77][312/390]\tTime 0.007 (0.003)\tLoss 1.1320 (1.0508)\tPrec@1 57.812 (62.188)\n",
      "Epoch: [77][390/390]\tTime 0.001 (0.003)\tLoss 1.1676 (1.0652)\tPrec@1 60.000 (61.732)\n",
      "EPOCH: 77 train Results: Prec@1 61.732 Loss: 1.0652\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1290 (1.1290)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4022 (1.2615)\tPrec@1 50.000 (55.220)\n",
      "EPOCH: 77 val Results: Prec@1 55.220 Loss: 1.2615\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [78][0/390]\tTime 0.003 (0.003)\tLoss 0.8747 (0.8747)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [78][78/390]\tTime 0.002 (0.004)\tLoss 1.0990 (0.9601)\tPrec@1 61.719 (66.228)\n",
      "Epoch: [78][156/390]\tTime 0.002 (0.003)\tLoss 1.1035 (1.0067)\tPrec@1 60.156 (64.545)\n",
      "Epoch: [78][234/390]\tTime 0.003 (0.003)\tLoss 1.0249 (1.0348)\tPrec@1 60.156 (63.487)\n",
      "Epoch: [78][312/390]\tTime 0.005 (0.003)\tLoss 1.0723 (1.0536)\tPrec@1 65.625 (62.657)\n",
      "Epoch: [78][390/390]\tTime 0.001 (0.003)\tLoss 0.9720 (1.0640)\tPrec@1 65.000 (62.104)\n",
      "EPOCH: 78 train Results: Prec@1 62.104 Loss: 1.0640\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1391 (1.1391)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1357 (1.2646)\tPrec@1 56.250 (54.710)\n",
      "EPOCH: 78 val Results: Prec@1 54.710 Loss: 1.2646\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [79][0/390]\tTime 0.003 (0.003)\tLoss 0.8318 (0.8318)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [79][78/390]\tTime 0.003 (0.004)\tLoss 0.9445 (0.9632)\tPrec@1 60.938 (65.348)\n",
      "Epoch: [79][156/390]\tTime 0.002 (0.003)\tLoss 0.9798 (0.9932)\tPrec@1 65.625 (64.436)\n",
      "Epoch: [79][234/390]\tTime 0.006 (0.003)\tLoss 1.0303 (1.0233)\tPrec@1 64.844 (63.344)\n",
      "Epoch: [79][312/390]\tTime 0.002 (0.003)\tLoss 1.3173 (1.0500)\tPrec@1 53.125 (62.657)\n",
      "Epoch: [79][390/390]\tTime 0.001 (0.003)\tLoss 1.0160 (1.0650)\tPrec@1 61.250 (62.110)\n",
      "EPOCH: 79 train Results: Prec@1 62.110 Loss: 1.0650\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2207 (1.2207)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1655 (1.2620)\tPrec@1 62.500 (54.820)\n",
      "EPOCH: 79 val Results: Prec@1 54.820 Loss: 1.2620\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [80][0/390]\tTime 0.004 (0.004)\tLoss 1.0161 (1.0161)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [80][78/390]\tTime 0.004 (0.003)\tLoss 0.9587 (0.9852)\tPrec@1 64.062 (65.140)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.0728 (1.0156)\tPrec@1 57.031 (63.814)\n",
      "Epoch: [80][234/390]\tTime 0.003 (0.003)\tLoss 1.0965 (1.0367)\tPrec@1 55.469 (63.175)\n",
      "Epoch: [80][312/390]\tTime 0.002 (0.003)\tLoss 1.2717 (1.0535)\tPrec@1 53.125 (62.512)\n",
      "Epoch: [80][390/390]\tTime 0.002 (0.003)\tLoss 1.1460 (1.0649)\tPrec@1 58.750 (62.100)\n",
      "EPOCH: 80 train Results: Prec@1 62.100 Loss: 1.0649\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1631 (1.1631)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4021 (1.2693)\tPrec@1 43.750 (55.580)\n",
      "EPOCH: 80 val Results: Prec@1 55.580 Loss: 1.2693\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [81][0/390]\tTime 0.005 (0.005)\tLoss 0.9148 (0.9148)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [81][78/390]\tTime 0.002 (0.003)\tLoss 0.9205 (0.9692)\tPrec@1 64.062 (65.526)\n",
      "Epoch: [81][156/390]\tTime 0.007 (0.003)\tLoss 0.8922 (1.0061)\tPrec@1 66.406 (64.346)\n",
      "Epoch: [81][234/390]\tTime 0.002 (0.003)\tLoss 0.9361 (1.0326)\tPrec@1 68.750 (63.408)\n",
      "Epoch: [81][312/390]\tTime 0.002 (0.003)\tLoss 1.3069 (1.0521)\tPrec@1 52.344 (62.710)\n",
      "Epoch: [81][390/390]\tTime 0.001 (0.003)\tLoss 1.0195 (1.0663)\tPrec@1 66.250 (62.158)\n",
      "EPOCH: 81 train Results: Prec@1 62.158 Loss: 1.0663\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2828 (1.2828)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4324 (1.2698)\tPrec@1 37.500 (54.580)\n",
      "EPOCH: 81 val Results: Prec@1 54.580 Loss: 1.2698\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [82][0/390]\tTime 0.005 (0.005)\tLoss 0.9351 (0.9351)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [82][78/390]\tTime 0.002 (0.004)\tLoss 0.9028 (0.9757)\tPrec@1 66.406 (65.259)\n",
      "Epoch: [82][156/390]\tTime 0.003 (0.004)\tLoss 1.1004 (1.0059)\tPrec@1 64.062 (63.968)\n",
      "Epoch: [82][234/390]\tTime 0.002 (0.004)\tLoss 1.1099 (1.0302)\tPrec@1 57.812 (63.059)\n",
      "Epoch: [82][312/390]\tTime 0.002 (0.003)\tLoss 0.9926 (1.0446)\tPrec@1 63.281 (62.562)\n",
      "Epoch: [82][390/390]\tTime 0.001 (0.003)\tLoss 1.0918 (1.0622)\tPrec@1 56.250 (62.004)\n",
      "EPOCH: 82 train Results: Prec@1 62.004 Loss: 1.0622\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1971 (1.1971)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3234 (1.2552)\tPrec@1 25.000 (55.790)\n",
      "EPOCH: 82 val Results: Prec@1 55.790 Loss: 1.2552\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [83][0/390]\tTime 0.002 (0.002)\tLoss 0.9606 (0.9606)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [83][78/390]\tTime 0.002 (0.003)\tLoss 1.1387 (0.9895)\tPrec@1 60.938 (64.933)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.1759 (1.0202)\tPrec@1 56.250 (63.699)\n",
      "Epoch: [83][234/390]\tTime 0.002 (0.004)\tLoss 1.0148 (1.0391)\tPrec@1 64.062 (62.965)\n",
      "Epoch: [83][312/390]\tTime 0.003 (0.003)\tLoss 1.2163 (1.0593)\tPrec@1 56.250 (62.255)\n",
      "Epoch: [83][390/390]\tTime 0.003 (0.003)\tLoss 1.1949 (1.0683)\tPrec@1 56.250 (61.828)\n",
      "EPOCH: 83 train Results: Prec@1 61.828 Loss: 1.0683\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1398 (1.1398)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1878 (1.2673)\tPrec@1 43.750 (55.760)\n",
      "EPOCH: 83 val Results: Prec@1 55.760 Loss: 1.2673\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [84][0/390]\tTime 0.011 (0.011)\tLoss 0.8526 (0.8526)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [84][78/390]\tTime 0.002 (0.004)\tLoss 0.8975 (1.0057)\tPrec@1 70.312 (64.597)\n",
      "Epoch: [84][156/390]\tTime 0.002 (0.004)\tLoss 1.1596 (1.0194)\tPrec@1 59.375 (63.834)\n",
      "Epoch: [84][234/390]\tTime 0.004 (0.004)\tLoss 1.0848 (1.0425)\tPrec@1 67.188 (62.866)\n",
      "Epoch: [84][312/390]\tTime 0.003 (0.003)\tLoss 1.0593 (1.0540)\tPrec@1 61.719 (62.480)\n",
      "Epoch: [84][390/390]\tTime 0.008 (0.003)\tLoss 1.3506 (1.0591)\tPrec@1 55.000 (62.264)\n",
      "EPOCH: 84 train Results: Prec@1 62.264 Loss: 1.0591\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1658 (1.1658)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2707 (1.2713)\tPrec@1 37.500 (55.200)\n",
      "EPOCH: 84 val Results: Prec@1 55.200 Loss: 1.2713\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [85][0/390]\tTime 0.002 (0.002)\tLoss 0.8915 (0.8915)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [85][78/390]\tTime 0.003 (0.003)\tLoss 0.8670 (0.9719)\tPrec@1 67.969 (65.239)\n",
      "Epoch: [85][156/390]\tTime 0.004 (0.003)\tLoss 0.8077 (1.0031)\tPrec@1 71.875 (64.167)\n",
      "Epoch: [85][234/390]\tTime 0.007 (0.003)\tLoss 1.2353 (1.0308)\tPrec@1 57.031 (63.231)\n",
      "Epoch: [85][312/390]\tTime 0.002 (0.003)\tLoss 0.9834 (1.0491)\tPrec@1 67.188 (62.555)\n",
      "Epoch: [85][390/390]\tTime 0.003 (0.003)\tLoss 1.1295 (1.0632)\tPrec@1 52.500 (62.062)\n",
      "EPOCH: 85 train Results: Prec@1 62.062 Loss: 1.0632\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1307 (1.1307)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2019 (1.2602)\tPrec@1 56.250 (55.560)\n",
      "EPOCH: 85 val Results: Prec@1 55.560 Loss: 1.2602\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [86][0/390]\tTime 0.003 (0.003)\tLoss 0.9748 (0.9748)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [86][78/390]\tTime 0.002 (0.004)\tLoss 0.9400 (0.9892)\tPrec@1 65.625 (65.210)\n",
      "Epoch: [86][156/390]\tTime 0.003 (0.004)\tLoss 0.8549 (1.0071)\tPrec@1 65.625 (64.127)\n",
      "Epoch: [86][234/390]\tTime 0.002 (0.004)\tLoss 1.0029 (1.0281)\tPrec@1 64.062 (63.471)\n",
      "Epoch: [86][312/390]\tTime 0.003 (0.004)\tLoss 1.3038 (1.0443)\tPrec@1 53.906 (62.907)\n",
      "Epoch: [86][390/390]\tTime 0.002 (0.004)\tLoss 1.1256 (1.0595)\tPrec@1 60.000 (62.234)\n",
      "EPOCH: 86 train Results: Prec@1 62.234 Loss: 1.0595\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1927 (1.1927)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2393 (1.2746)\tPrec@1 43.750 (55.200)\n",
      "EPOCH: 86 val Results: Prec@1 55.200 Loss: 1.2746\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [87][0/390]\tTime 0.006 (0.006)\tLoss 1.0315 (1.0315)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [87][78/390]\tTime 0.004 (0.003)\tLoss 0.8143 (0.9665)\tPrec@1 71.875 (65.576)\n",
      "Epoch: [87][156/390]\tTime 0.006 (0.003)\tLoss 1.0230 (1.0046)\tPrec@1 60.156 (64.281)\n",
      "Epoch: [87][234/390]\tTime 0.006 (0.003)\tLoss 1.0919 (1.0286)\tPrec@1 65.625 (63.401)\n",
      "Epoch: [87][312/390]\tTime 0.002 (0.003)\tLoss 1.1148 (1.0491)\tPrec@1 58.594 (62.775)\n",
      "Epoch: [87][390/390]\tTime 0.002 (0.003)\tLoss 1.2686 (1.0586)\tPrec@1 51.250 (62.448)\n",
      "EPOCH: 87 train Results: Prec@1 62.448 Loss: 1.0586\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1693 (1.1693)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9613 (1.2784)\tPrec@1 50.000 (55.200)\n",
      "EPOCH: 87 val Results: Prec@1 55.200 Loss: 1.2784\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [88][0/390]\tTime 0.006 (0.006)\tLoss 0.9419 (0.9419)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [88][78/390]\tTime 0.003 (0.003)\tLoss 0.9799 (0.9628)\tPrec@1 65.625 (65.843)\n",
      "Epoch: [88][156/390]\tTime 0.002 (0.003)\tLoss 1.1192 (1.0097)\tPrec@1 60.938 (63.983)\n",
      "Epoch: [88][234/390]\tTime 0.003 (0.003)\tLoss 1.1366 (1.0305)\tPrec@1 59.375 (63.138)\n",
      "Epoch: [88][312/390]\tTime 0.003 (0.003)\tLoss 1.1030 (1.0516)\tPrec@1 58.594 (62.318)\n",
      "Epoch: [88][390/390]\tTime 0.002 (0.003)\tLoss 1.3147 (1.0635)\tPrec@1 58.750 (61.956)\n",
      "EPOCH: 88 train Results: Prec@1 61.956 Loss: 1.0635\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1932 (1.1932)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1640 (1.2678)\tPrec@1 43.750 (55.320)\n",
      "EPOCH: 88 val Results: Prec@1 55.320 Loss: 1.2678\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [89][0/390]\tTime 0.002 (0.002)\tLoss 0.8929 (0.8929)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.003)\tLoss 1.0076 (0.9927)\tPrec@1 65.625 (64.171)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1859 (1.0159)\tPrec@1 57.031 (63.436)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 0.9890 (1.0385)\tPrec@1 67.969 (62.643)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.0659 (1.0542)\tPrec@1 63.281 (62.116)\n",
      "Epoch: [89][390/390]\tTime 0.006 (0.003)\tLoss 1.2881 (1.0670)\tPrec@1 52.500 (61.688)\n",
      "EPOCH: 89 train Results: Prec@1 61.688 Loss: 1.0670\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1691 (1.1691)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4950 (1.2795)\tPrec@1 43.750 (55.420)\n",
      "EPOCH: 89 val Results: Prec@1 55.420 Loss: 1.2795\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [90][0/390]\tTime 0.005 (0.005)\tLoss 0.9755 (0.9755)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [90][78/390]\tTime 0.003 (0.003)\tLoss 1.0221 (0.9705)\tPrec@1 66.406 (65.724)\n",
      "Epoch: [90][156/390]\tTime 0.004 (0.003)\tLoss 1.1651 (1.0061)\tPrec@1 60.938 (64.461)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.0531 (1.0315)\tPrec@1 60.938 (63.361)\n",
      "Epoch: [90][312/390]\tTime 0.003 (0.003)\tLoss 1.2831 (1.0453)\tPrec@1 57.812 (62.765)\n",
      "Epoch: [90][390/390]\tTime 0.004 (0.003)\tLoss 0.9745 (1.0591)\tPrec@1 62.500 (62.330)\n",
      "EPOCH: 90 train Results: Prec@1 62.330 Loss: 1.0591\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1122 (1.1122)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6762 (1.2926)\tPrec@1 37.500 (54.310)\n",
      "EPOCH: 90 val Results: Prec@1 54.310 Loss: 1.2926\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [91][0/390]\tTime 0.004 (0.004)\tLoss 1.0534 (1.0534)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [91][78/390]\tTime 0.002 (0.003)\tLoss 1.0326 (0.9701)\tPrec@1 64.844 (65.625)\n",
      "Epoch: [91][156/390]\tTime 0.011 (0.003)\tLoss 1.3308 (1.0072)\tPrec@1 53.125 (64.087)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.003)\tLoss 1.1132 (1.0250)\tPrec@1 60.938 (63.351)\n",
      "Epoch: [91][312/390]\tTime 0.002 (0.003)\tLoss 1.1395 (1.0462)\tPrec@1 59.375 (62.587)\n",
      "Epoch: [91][390/390]\tTime 0.002 (0.003)\tLoss 1.1996 (1.0616)\tPrec@1 56.250 (61.952)\n",
      "EPOCH: 91 train Results: Prec@1 61.952 Loss: 1.0616\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1136 (1.1136)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3611 (1.2696)\tPrec@1 56.250 (55.330)\n",
      "EPOCH: 91 val Results: Prec@1 55.330 Loss: 1.2696\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [92][0/390]\tTime 0.002 (0.002)\tLoss 1.1540 (1.1540)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [92][78/390]\tTime 0.005 (0.003)\tLoss 0.9639 (0.9795)\tPrec@1 63.281 (64.715)\n",
      "Epoch: [92][156/390]\tTime 0.005 (0.003)\tLoss 1.0628 (1.0138)\tPrec@1 63.281 (63.664)\n",
      "Epoch: [92][234/390]\tTime 0.002 (0.003)\tLoss 1.0614 (1.0341)\tPrec@1 65.625 (62.783)\n",
      "Epoch: [92][312/390]\tTime 0.002 (0.003)\tLoss 1.1652 (1.0501)\tPrec@1 55.469 (62.360)\n",
      "Epoch: [92][390/390]\tTime 0.001 (0.003)\tLoss 1.0475 (1.0648)\tPrec@1 61.250 (61.852)\n",
      "EPOCH: 92 train Results: Prec@1 61.852 Loss: 1.0648\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1955 (1.1955)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2089 (1.2803)\tPrec@1 50.000 (55.210)\n",
      "EPOCH: 92 val Results: Prec@1 55.210 Loss: 1.2803\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 0.9554 (0.9554)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.004)\tLoss 0.9655 (0.9542)\tPrec@1 64.844 (66.149)\n",
      "Epoch: [93][156/390]\tTime 0.005 (0.004)\tLoss 1.0139 (0.9930)\tPrec@1 64.062 (64.600)\n",
      "Epoch: [93][234/390]\tTime 0.002 (0.003)\tLoss 1.0462 (1.0198)\tPrec@1 62.500 (63.507)\n",
      "Epoch: [93][312/390]\tTime 0.002 (0.003)\tLoss 1.2201 (1.0435)\tPrec@1 60.156 (62.478)\n",
      "Epoch: [93][390/390]\tTime 0.001 (0.003)\tLoss 1.0452 (1.0592)\tPrec@1 65.000 (61.932)\n",
      "EPOCH: 93 train Results: Prec@1 61.932 Loss: 1.0592\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1378 (1.1378)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3352 (1.2745)\tPrec@1 25.000 (55.110)\n",
      "EPOCH: 93 val Results: Prec@1 55.110 Loss: 1.2745\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [94][0/390]\tTime 0.002 (0.002)\tLoss 0.8318 (0.8318)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [94][78/390]\tTime 0.003 (0.003)\tLoss 0.9812 (0.9638)\tPrec@1 68.750 (66.149)\n",
      "Epoch: [94][156/390]\tTime 0.002 (0.003)\tLoss 1.0180 (0.9973)\tPrec@1 63.281 (64.739)\n",
      "Epoch: [94][234/390]\tTime 0.002 (0.003)\tLoss 0.9694 (1.0314)\tPrec@1 67.969 (63.331)\n",
      "Epoch: [94][312/390]\tTime 0.009 (0.003)\tLoss 1.2382 (1.0503)\tPrec@1 59.375 (62.595)\n",
      "Epoch: [94][390/390]\tTime 0.001 (0.003)\tLoss 1.2668 (1.0658)\tPrec@1 61.250 (62.030)\n",
      "EPOCH: 94 train Results: Prec@1 62.030 Loss: 1.0658\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1911 (1.1911)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1627 (1.2737)\tPrec@1 37.500 (54.790)\n",
      "EPOCH: 94 val Results: Prec@1 54.790 Loss: 1.2737\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [95][0/390]\tTime 0.002 (0.002)\tLoss 1.0341 (1.0341)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [95][78/390]\tTime 0.002 (0.003)\tLoss 1.0791 (0.9717)\tPrec@1 66.406 (65.051)\n",
      "Epoch: [95][156/390]\tTime 0.004 (0.003)\tLoss 0.9830 (1.0079)\tPrec@1 63.281 (63.878)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.003)\tLoss 1.1847 (1.0332)\tPrec@1 57.031 (63.112)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.003)\tLoss 1.1329 (1.0490)\tPrec@1 59.375 (62.468)\n",
      "Epoch: [95][390/390]\tTime 0.006 (0.003)\tLoss 1.1293 (1.0626)\tPrec@1 62.500 (62.072)\n",
      "EPOCH: 95 train Results: Prec@1 62.072 Loss: 1.0626\n",
      "Test: [0/78]\tTime 0.040 (0.040)\tLoss 1.1364 (1.1364)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.002)\tLoss 1.1954 (1.2746)\tPrec@1 37.500 (55.200)\n",
      "EPOCH: 95 val Results: Prec@1 55.200 Loss: 1.2746\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [96][0/390]\tTime 0.006 (0.006)\tLoss 0.9241 (0.9241)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.003)\tLoss 0.9960 (0.9711)\tPrec@1 64.844 (65.378)\n",
      "Epoch: [96][156/390]\tTime 0.002 (0.003)\tLoss 0.8921 (1.0062)\tPrec@1 66.406 (64.321)\n",
      "Epoch: [96][234/390]\tTime 0.002 (0.003)\tLoss 1.0160 (1.0308)\tPrec@1 63.281 (63.288)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 0.9326 (1.0420)\tPrec@1 66.406 (62.867)\n",
      "Epoch: [96][390/390]\tTime 0.007 (0.003)\tLoss 1.1494 (1.0571)\tPrec@1 61.250 (62.254)\n",
      "EPOCH: 96 train Results: Prec@1 62.254 Loss: 1.0571\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.2040 (1.2040)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0675 (1.2755)\tPrec@1 50.000 (54.450)\n",
      "EPOCH: 96 val Results: Prec@1 54.450 Loss: 1.2755\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [97][0/390]\tTime 0.006 (0.006)\tLoss 0.9709 (0.9709)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [97][78/390]\tTime 0.005 (0.003)\tLoss 0.9844 (0.9898)\tPrec@1 60.156 (64.537)\n",
      "Epoch: [97][156/390]\tTime 0.002 (0.003)\tLoss 1.2118 (1.0171)\tPrec@1 59.375 (63.844)\n",
      "Epoch: [97][234/390]\tTime 0.002 (0.003)\tLoss 1.2107 (1.0367)\tPrec@1 56.250 (62.975)\n",
      "Epoch: [97][312/390]\tTime 0.003 (0.003)\tLoss 1.0523 (1.0517)\tPrec@1 61.719 (62.450)\n",
      "Epoch: [97][390/390]\tTime 0.001 (0.003)\tLoss 1.0625 (1.0650)\tPrec@1 65.000 (61.992)\n",
      "EPOCH: 97 train Results: Prec@1 61.992 Loss: 1.0650\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1975 (1.1975)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.0948 (1.2903)\tPrec@1 43.750 (54.680)\n",
      "EPOCH: 97 val Results: Prec@1 54.680 Loss: 1.2903\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [98][0/390]\tTime 0.004 (0.004)\tLoss 1.0418 (1.0418)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [98][78/390]\tTime 0.004 (0.003)\tLoss 0.8694 (0.9697)\tPrec@1 69.531 (65.309)\n",
      "Epoch: [98][156/390]\tTime 0.002 (0.003)\tLoss 1.2069 (1.0060)\tPrec@1 60.156 (64.013)\n",
      "Epoch: [98][234/390]\tTime 0.002 (0.003)\tLoss 0.9936 (1.0280)\tPrec@1 68.750 (63.195)\n",
      "Epoch: [98][312/390]\tTime 0.003 (0.003)\tLoss 1.2980 (1.0461)\tPrec@1 52.344 (62.595)\n",
      "Epoch: [98][390/390]\tTime 0.003 (0.003)\tLoss 1.1057 (1.0620)\tPrec@1 60.000 (62.048)\n",
      "EPOCH: 98 train Results: Prec@1 62.048 Loss: 1.0620\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2034 (1.2034)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6856 (1.2892)\tPrec@1 18.750 (54.260)\n",
      "EPOCH: 98 val Results: Prec@1 54.260 Loss: 1.2892\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [99][0/390]\tTime 0.002 (0.002)\tLoss 0.9254 (0.9254)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.003)\tLoss 0.9087 (0.9962)\tPrec@1 70.312 (65.111)\n",
      "Epoch: [99][156/390]\tTime 0.006 (0.003)\tLoss 1.0571 (1.0168)\tPrec@1 64.062 (64.147)\n",
      "Epoch: [99][234/390]\tTime 0.003 (0.003)\tLoss 1.3252 (1.0309)\tPrec@1 53.906 (63.491)\n",
      "Epoch: [99][312/390]\tTime 0.006 (0.003)\tLoss 1.0726 (1.0487)\tPrec@1 61.719 (62.720)\n",
      "Epoch: [99][390/390]\tTime 0.003 (0.003)\tLoss 0.8657 (1.0613)\tPrec@1 72.500 (62.138)\n",
      "EPOCH: 99 train Results: Prec@1 62.138 Loss: 1.0613\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1817 (1.1817)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6488 (1.2657)\tPrec@1 31.250 (55.650)\n",
      "EPOCH: 99 val Results: Prec@1 55.650 Loss: 1.2657\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [100][0/390]\tTime 0.008 (0.008)\tLoss 0.8834 (0.8834)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [100][78/390]\tTime 0.002 (0.003)\tLoss 1.0262 (0.9761)\tPrec@1 64.062 (65.170)\n",
      "Epoch: [100][156/390]\tTime 0.002 (0.003)\tLoss 1.0628 (1.0028)\tPrec@1 61.719 (64.331)\n",
      "Epoch: [100][234/390]\tTime 0.003 (0.003)\tLoss 1.0907 (1.0306)\tPrec@1 60.938 (63.401)\n",
      "Epoch: [100][312/390]\tTime 0.013 (0.003)\tLoss 1.0557 (1.0516)\tPrec@1 65.625 (62.730)\n",
      "Epoch: [100][390/390]\tTime 0.003 (0.003)\tLoss 1.3214 (1.0626)\tPrec@1 55.000 (62.342)\n",
      "EPOCH: 100 train Results: Prec@1 62.342 Loss: 1.0626\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1823 (1.1823)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5954 (1.2666)\tPrec@1 31.250 (55.300)\n",
      "EPOCH: 100 val Results: Prec@1 55.300 Loss: 1.2666\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [101][0/390]\tTime 0.004 (0.004)\tLoss 1.0176 (1.0176)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.003)\tLoss 0.9925 (0.9918)\tPrec@1 62.500 (64.953)\n",
      "Epoch: [101][156/390]\tTime 0.007 (0.003)\tLoss 1.1441 (1.0132)\tPrec@1 57.031 (64.087)\n",
      "Epoch: [101][234/390]\tTime 0.002 (0.003)\tLoss 1.0471 (1.0356)\tPrec@1 61.719 (63.115)\n",
      "Epoch: [101][312/390]\tTime 0.002 (0.003)\tLoss 1.0731 (1.0459)\tPrec@1 60.156 (62.590)\n",
      "Epoch: [101][390/390]\tTime 0.002 (0.003)\tLoss 1.0754 (1.0613)\tPrec@1 57.500 (62.104)\n",
      "EPOCH: 101 train Results: Prec@1 62.104 Loss: 1.0613\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2287 (1.2287)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3841 (1.2959)\tPrec@1 43.750 (54.150)\n",
      "EPOCH: 101 val Results: Prec@1 54.150 Loss: 1.2959\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [102][0/390]\tTime 0.006 (0.006)\tLoss 1.0303 (1.0303)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [102][78/390]\tTime 0.002 (0.003)\tLoss 1.0467 (0.9781)\tPrec@1 57.031 (64.814)\n",
      "Epoch: [102][156/390]\tTime 0.003 (0.003)\tLoss 0.9211 (1.0124)\tPrec@1 70.312 (63.898)\n",
      "Epoch: [102][234/390]\tTime 0.003 (0.003)\tLoss 1.0540 (1.0272)\tPrec@1 66.406 (63.441)\n",
      "Epoch: [102][312/390]\tTime 0.003 (0.003)\tLoss 1.1175 (1.0453)\tPrec@1 60.156 (62.872)\n",
      "Epoch: [102][390/390]\tTime 0.004 (0.003)\tLoss 1.0492 (1.0575)\tPrec@1 56.250 (62.446)\n",
      "EPOCH: 102 train Results: Prec@1 62.446 Loss: 1.0575\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2126 (1.2126)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5630 (1.2717)\tPrec@1 50.000 (54.870)\n",
      "EPOCH: 102 val Results: Prec@1 54.870 Loss: 1.2717\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [103][0/390]\tTime 0.002 (0.002)\tLoss 0.9580 (0.9580)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [103][78/390]\tTime 0.006 (0.003)\tLoss 1.0571 (0.9869)\tPrec@1 63.281 (64.597)\n",
      "Epoch: [103][156/390]\tTime 0.003 (0.003)\tLoss 1.1439 (1.0170)\tPrec@1 55.469 (63.649)\n",
      "Epoch: [103][234/390]\tTime 0.002 (0.003)\tLoss 1.1372 (1.0344)\tPrec@1 59.375 (63.019)\n",
      "Epoch: [103][312/390]\tTime 0.003 (0.003)\tLoss 1.2933 (1.0484)\tPrec@1 57.031 (62.597)\n",
      "Epoch: [103][390/390]\tTime 0.002 (0.003)\tLoss 1.2758 (1.0572)\tPrec@1 61.250 (62.324)\n",
      "EPOCH: 103 train Results: Prec@1 62.324 Loss: 1.0572\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1071 (1.1071)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3865 (1.2853)\tPrec@1 56.250 (54.880)\n",
      "EPOCH: 103 val Results: Prec@1 54.880 Loss: 1.2853\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [104][0/390]\tTime 0.002 (0.002)\tLoss 1.0036 (1.0036)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 0.9430 (0.9773)\tPrec@1 68.750 (65.526)\n",
      "Epoch: [104][156/390]\tTime 0.003 (0.003)\tLoss 0.9602 (1.0062)\tPrec@1 66.406 (64.092)\n",
      "Epoch: [104][234/390]\tTime 0.006 (0.003)\tLoss 0.9605 (1.0226)\tPrec@1 65.625 (63.547)\n",
      "Epoch: [104][312/390]\tTime 0.003 (0.003)\tLoss 1.0733 (1.0459)\tPrec@1 63.281 (62.745)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.003)\tLoss 0.9493 (1.0579)\tPrec@1 68.750 (62.364)\n",
      "EPOCH: 104 train Results: Prec@1 62.364 Loss: 1.0579\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1840 (1.1840)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0926 (1.2637)\tPrec@1 50.000 (55.160)\n",
      "EPOCH: 104 val Results: Prec@1 55.160 Loss: 1.2637\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [105][0/390]\tTime 0.006 (0.006)\tLoss 0.9371 (0.9371)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [105][78/390]\tTime 0.006 (0.003)\tLoss 1.0111 (0.9932)\tPrec@1 66.406 (64.043)\n",
      "Epoch: [105][156/390]\tTime 0.002 (0.003)\tLoss 1.1201 (1.0129)\tPrec@1 53.125 (63.495)\n",
      "Epoch: [105][234/390]\tTime 0.006 (0.003)\tLoss 1.2016 (1.0286)\tPrec@1 63.281 (62.919)\n",
      "Epoch: [105][312/390]\tTime 0.002 (0.003)\tLoss 1.1179 (1.0452)\tPrec@1 58.594 (62.340)\n",
      "Epoch: [105][390/390]\tTime 0.003 (0.003)\tLoss 1.0560 (1.0612)\tPrec@1 63.750 (61.994)\n",
      "EPOCH: 105 train Results: Prec@1 61.994 Loss: 1.0612\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1052 (1.1052)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3546 (1.2881)\tPrec@1 43.750 (55.220)\n",
      "EPOCH: 105 val Results: Prec@1 55.220 Loss: 1.2881\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [106][0/390]\tTime 0.002 (0.002)\tLoss 0.9487 (0.9487)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [106][78/390]\tTime 0.003 (0.003)\tLoss 0.9786 (0.9678)\tPrec@1 58.594 (65.427)\n",
      "Epoch: [106][156/390]\tTime 0.002 (0.003)\tLoss 1.0089 (1.0019)\tPrec@1 64.844 (64.092)\n",
      "Epoch: [106][234/390]\tTime 0.007 (0.003)\tLoss 0.9840 (1.0263)\tPrec@1 64.062 (63.351)\n",
      "Epoch: [106][312/390]\tTime 0.004 (0.003)\tLoss 1.0701 (1.0474)\tPrec@1 56.250 (62.605)\n",
      "Epoch: [106][390/390]\tTime 0.002 (0.003)\tLoss 1.0552 (1.0587)\tPrec@1 60.000 (62.278)\n",
      "EPOCH: 106 train Results: Prec@1 62.278 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0953 (1.0953)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3044 (1.2732)\tPrec@1 43.750 (54.850)\n",
      "EPOCH: 106 val Results: Prec@1 54.850 Loss: 1.2732\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [107][0/390]\tTime 0.003 (0.003)\tLoss 0.8581 (0.8581)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [107][78/390]\tTime 0.005 (0.004)\tLoss 1.1520 (0.9746)\tPrec@1 60.156 (65.150)\n",
      "Epoch: [107][156/390]\tTime 0.003 (0.003)\tLoss 1.0159 (1.0058)\tPrec@1 64.062 (63.903)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.003)\tLoss 1.1200 (1.0200)\tPrec@1 62.500 (63.401)\n",
      "Epoch: [107][312/390]\tTime 0.004 (0.003)\tLoss 1.1613 (1.0436)\tPrec@1 59.375 (62.705)\n",
      "Epoch: [107][390/390]\tTime 0.001 (0.003)\tLoss 0.9627 (1.0586)\tPrec@1 66.250 (62.230)\n",
      "EPOCH: 107 train Results: Prec@1 62.230 Loss: 1.0586\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2309 (1.2309)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3505 (1.2829)\tPrec@1 50.000 (54.590)\n",
      "EPOCH: 107 val Results: Prec@1 54.590 Loss: 1.2829\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [108][0/390]\tTime 0.002 (0.002)\tLoss 1.0024 (1.0024)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [108][78/390]\tTime 0.008 (0.003)\tLoss 1.0461 (0.9675)\tPrec@1 66.406 (65.566)\n",
      "Epoch: [108][156/390]\tTime 0.002 (0.003)\tLoss 0.8661 (1.0039)\tPrec@1 72.656 (64.072)\n",
      "Epoch: [108][234/390]\tTime 0.002 (0.003)\tLoss 1.2185 (1.0285)\tPrec@1 50.000 (63.298)\n",
      "Epoch: [108][312/390]\tTime 0.002 (0.003)\tLoss 0.9968 (1.0443)\tPrec@1 62.500 (62.732)\n",
      "Epoch: [108][390/390]\tTime 0.001 (0.003)\tLoss 1.1056 (1.0579)\tPrec@1 62.500 (62.292)\n",
      "EPOCH: 108 train Results: Prec@1 62.292 Loss: 1.0579\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2168 (1.2168)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6365 (1.2773)\tPrec@1 50.000 (54.870)\n",
      "EPOCH: 108 val Results: Prec@1 54.870 Loss: 1.2773\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [109][0/390]\tTime 0.002 (0.002)\tLoss 1.0235 (1.0235)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [109][78/390]\tTime 0.002 (0.003)\tLoss 1.0247 (0.9876)\tPrec@1 62.500 (64.695)\n",
      "Epoch: [109][156/390]\tTime 0.002 (0.003)\tLoss 1.0659 (1.0140)\tPrec@1 63.281 (63.709)\n",
      "Epoch: [109][234/390]\tTime 0.002 (0.003)\tLoss 0.9203 (1.0314)\tPrec@1 63.281 (63.132)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.003)\tLoss 1.0573 (1.0441)\tPrec@1 57.812 (62.535)\n",
      "Epoch: [109][390/390]\tTime 0.002 (0.003)\tLoss 1.1416 (1.0564)\tPrec@1 60.000 (62.088)\n",
      "EPOCH: 109 train Results: Prec@1 62.088 Loss: 1.0564\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1210 (1.1210)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6092 (1.2575)\tPrec@1 37.500 (55.880)\n",
      "EPOCH: 109 val Results: Prec@1 55.880 Loss: 1.2575\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [110][0/390]\tTime 0.002 (0.002)\tLoss 1.0046 (1.0046)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 0.9736 (0.9835)\tPrec@1 65.625 (64.903)\n",
      "Epoch: [110][156/390]\tTime 0.002 (0.003)\tLoss 1.0018 (1.0114)\tPrec@1 64.062 (63.625)\n",
      "Epoch: [110][234/390]\tTime 0.002 (0.003)\tLoss 1.1512 (1.0261)\tPrec@1 58.594 (63.381)\n",
      "Epoch: [110][312/390]\tTime 0.004 (0.003)\tLoss 1.0916 (1.0474)\tPrec@1 60.156 (62.635)\n",
      "Epoch: [110][390/390]\tTime 0.001 (0.003)\tLoss 1.1858 (1.0569)\tPrec@1 62.500 (62.152)\n",
      "EPOCH: 110 train Results: Prec@1 62.152 Loss: 1.0569\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0670 (1.0670)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.3937 (1.2703)\tPrec@1 43.750 (55.550)\n",
      "EPOCH: 110 val Results: Prec@1 55.550 Loss: 1.2703\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [111][0/390]\tTime 0.012 (0.012)\tLoss 0.9272 (0.9272)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [111][78/390]\tTime 0.002 (0.003)\tLoss 1.0260 (0.9681)\tPrec@1 63.281 (65.279)\n",
      "Epoch: [111][156/390]\tTime 0.002 (0.003)\tLoss 1.0470 (1.0159)\tPrec@1 60.156 (63.515)\n",
      "Epoch: [111][234/390]\tTime 0.003 (0.003)\tLoss 1.2730 (1.0414)\tPrec@1 59.375 (62.653)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.003)\tLoss 1.0421 (1.0539)\tPrec@1 64.844 (62.116)\n",
      "Epoch: [111][390/390]\tTime 0.002 (0.003)\tLoss 1.1862 (1.0618)\tPrec@1 67.500 (61.836)\n",
      "EPOCH: 111 train Results: Prec@1 61.836 Loss: 1.0618\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1972 (1.1972)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4879 (1.2844)\tPrec@1 43.750 (55.130)\n",
      "EPOCH: 111 val Results: Prec@1 55.130 Loss: 1.2844\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [112][0/390]\tTime 0.004 (0.004)\tLoss 0.9057 (0.9057)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [112][78/390]\tTime 0.003 (0.003)\tLoss 1.0465 (0.9725)\tPrec@1 60.156 (65.378)\n",
      "Epoch: [112][156/390]\tTime 0.004 (0.003)\tLoss 1.2159 (1.0000)\tPrec@1 57.031 (64.331)\n",
      "Epoch: [112][234/390]\tTime 0.003 (0.003)\tLoss 1.2305 (1.0268)\tPrec@1 58.594 (63.451)\n",
      "Epoch: [112][312/390]\tTime 0.003 (0.003)\tLoss 0.9786 (1.0514)\tPrec@1 64.844 (62.455)\n",
      "Epoch: [112][390/390]\tTime 0.001 (0.003)\tLoss 1.4319 (1.0603)\tPrec@1 57.500 (62.260)\n",
      "EPOCH: 112 train Results: Prec@1 62.260 Loss: 1.0603\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0975 (1.0975)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5973 (1.2794)\tPrec@1 31.250 (55.220)\n",
      "EPOCH: 112 val Results: Prec@1 55.220 Loss: 1.2794\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 0.9061 (0.9061)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [113][78/390]\tTime 0.003 (0.003)\tLoss 0.9264 (0.9776)\tPrec@1 65.625 (65.160)\n",
      "Epoch: [113][156/390]\tTime 0.003 (0.003)\tLoss 1.0399 (1.0006)\tPrec@1 61.719 (63.933)\n",
      "Epoch: [113][234/390]\tTime 0.002 (0.003)\tLoss 0.9718 (1.0236)\tPrec@1 66.406 (63.152)\n",
      "Epoch: [113][312/390]\tTime 0.003 (0.003)\tLoss 1.0770 (1.0410)\tPrec@1 60.938 (62.582)\n",
      "Epoch: [113][390/390]\tTime 0.001 (0.003)\tLoss 1.0925 (1.0556)\tPrec@1 63.750 (62.198)\n",
      "EPOCH: 113 train Results: Prec@1 62.198 Loss: 1.0556\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1543 (1.1543)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4604 (1.2559)\tPrec@1 43.750 (55.920)\n",
      "EPOCH: 113 val Results: Prec@1 55.920 Loss: 1.2559\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [114][0/390]\tTime 0.002 (0.002)\tLoss 1.0181 (1.0181)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [114][78/390]\tTime 0.003 (0.002)\tLoss 0.6639 (0.9674)\tPrec@1 78.906 (65.724)\n",
      "Epoch: [114][156/390]\tTime 0.002 (0.003)\tLoss 1.1183 (1.0000)\tPrec@1 59.375 (64.500)\n",
      "Epoch: [114][234/390]\tTime 0.010 (0.003)\tLoss 1.1297 (1.0300)\tPrec@1 59.375 (63.321)\n",
      "Epoch: [114][312/390]\tTime 0.002 (0.003)\tLoss 0.9720 (1.0438)\tPrec@1 70.312 (62.887)\n",
      "Epoch: [114][390/390]\tTime 0.005 (0.003)\tLoss 1.1674 (1.0596)\tPrec@1 53.750 (62.268)\n",
      "EPOCH: 114 train Results: Prec@1 62.268 Loss: 1.0596\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1211 (1.1211)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7723 (1.2880)\tPrec@1 25.000 (54.090)\n",
      "EPOCH: 114 val Results: Prec@1 54.090 Loss: 1.2880\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [115][0/390]\tTime 0.003 (0.003)\tLoss 0.7921 (0.7921)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [115][78/390]\tTime 0.002 (0.003)\tLoss 0.8813 (0.9750)\tPrec@1 66.406 (65.101)\n",
      "Epoch: [115][156/390]\tTime 0.002 (0.003)\tLoss 1.1162 (0.9951)\tPrec@1 60.938 (64.222)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.003)\tLoss 1.2006 (1.0242)\tPrec@1 54.688 (63.185)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.003)\tLoss 1.3279 (1.0426)\tPrec@1 52.344 (62.615)\n",
      "Epoch: [115][390/390]\tTime 0.001 (0.003)\tLoss 1.2542 (1.0565)\tPrec@1 57.500 (62.088)\n",
      "EPOCH: 115 train Results: Prec@1 62.088 Loss: 1.0565\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1686 (1.1686)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3198 (1.2715)\tPrec@1 43.750 (55.580)\n",
      "EPOCH: 115 val Results: Prec@1 55.580 Loss: 1.2715\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [116][0/390]\tTime 0.002 (0.002)\tLoss 1.0630 (1.0630)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [116][78/390]\tTime 0.003 (0.003)\tLoss 0.8889 (0.9791)\tPrec@1 67.969 (65.042)\n",
      "Epoch: [116][156/390]\tTime 0.003 (0.003)\tLoss 1.1596 (1.0116)\tPrec@1 61.719 (63.804)\n",
      "Epoch: [116][234/390]\tTime 0.002 (0.004)\tLoss 1.0472 (1.0345)\tPrec@1 62.500 (63.025)\n",
      "Epoch: [116][312/390]\tTime 0.005 (0.003)\tLoss 1.1260 (1.0491)\tPrec@1 62.500 (62.595)\n",
      "Epoch: [116][390/390]\tTime 0.003 (0.003)\tLoss 1.2652 (1.0585)\tPrec@1 60.000 (62.230)\n",
      "EPOCH: 116 train Results: Prec@1 62.230 Loss: 1.0585\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1470 (1.1470)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4220 (1.2663)\tPrec@1 50.000 (55.590)\n",
      "EPOCH: 116 val Results: Prec@1 55.590 Loss: 1.2663\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [117][0/390]\tTime 0.002 (0.002)\tLoss 0.9047 (0.9047)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [117][78/390]\tTime 0.002 (0.003)\tLoss 1.1406 (0.9695)\tPrec@1 59.375 (65.427)\n",
      "Epoch: [117][156/390]\tTime 0.002 (0.003)\tLoss 1.1875 (1.0029)\tPrec@1 61.719 (64.391)\n",
      "Epoch: [117][234/390]\tTime 0.003 (0.003)\tLoss 1.0517 (1.0264)\tPrec@1 67.969 (63.511)\n",
      "Epoch: [117][312/390]\tTime 0.006 (0.003)\tLoss 1.0378 (1.0461)\tPrec@1 61.719 (62.829)\n",
      "Epoch: [117][390/390]\tTime 0.002 (0.003)\tLoss 1.0624 (1.0564)\tPrec@1 57.500 (62.480)\n",
      "EPOCH: 117 train Results: Prec@1 62.480 Loss: 1.0564\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0790 (1.0790)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4085 (1.2639)\tPrec@1 37.500 (55.310)\n",
      "EPOCH: 117 val Results: Prec@1 55.310 Loss: 1.2639\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [118][0/390]\tTime 0.004 (0.004)\tLoss 0.9370 (0.9370)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [118][78/390]\tTime 0.005 (0.003)\tLoss 1.0378 (0.9881)\tPrec@1 60.938 (64.735)\n",
      "Epoch: [118][156/390]\tTime 0.003 (0.003)\tLoss 0.9752 (1.0106)\tPrec@1 62.500 (64.038)\n",
      "Epoch: [118][234/390]\tTime 0.002 (0.003)\tLoss 1.0124 (1.0247)\tPrec@1 57.812 (63.541)\n",
      "Epoch: [118][312/390]\tTime 0.014 (0.003)\tLoss 1.0739 (1.0415)\tPrec@1 64.062 (62.902)\n",
      "Epoch: [118][390/390]\tTime 0.002 (0.003)\tLoss 1.2203 (1.0587)\tPrec@1 57.500 (62.310)\n",
      "EPOCH: 118 train Results: Prec@1 62.310 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1874 (1.1874)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2505 (1.2829)\tPrec@1 25.000 (54.350)\n",
      "EPOCH: 118 val Results: Prec@1 54.350 Loss: 1.2829\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [119][0/390]\tTime 0.002 (0.002)\tLoss 0.8914 (0.8914)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [119][78/390]\tTime 0.005 (0.003)\tLoss 1.1185 (0.9589)\tPrec@1 60.938 (65.961)\n",
      "Epoch: [119][156/390]\tTime 0.002 (0.003)\tLoss 1.1781 (0.9981)\tPrec@1 59.375 (64.615)\n",
      "Epoch: [119][234/390]\tTime 0.003 (0.003)\tLoss 0.9921 (1.0224)\tPrec@1 62.500 (63.597)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.004)\tLoss 1.1443 (1.0411)\tPrec@1 57.812 (62.889)\n",
      "Epoch: [119][390/390]\tTime 0.005 (0.004)\tLoss 1.1475 (1.0557)\tPrec@1 65.000 (62.434)\n",
      "EPOCH: 119 train Results: Prec@1 62.434 Loss: 1.0557\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1863 (1.1863)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4040 (1.2804)\tPrec@1 50.000 (55.160)\n",
      "EPOCH: 119 val Results: Prec@1 55.160 Loss: 1.2804\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [120][0/390]\tTime 0.003 (0.003)\tLoss 0.7583 (0.7583)\tPrec@1 78.125 (78.125)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.003)\tLoss 1.0494 (0.9773)\tPrec@1 55.469 (65.032)\n",
      "Epoch: [120][156/390]\tTime 0.002 (0.004)\tLoss 0.9370 (1.0095)\tPrec@1 64.062 (63.938)\n",
      "Epoch: [120][234/390]\tTime 0.005 (0.003)\tLoss 1.0619 (1.0297)\tPrec@1 61.719 (63.208)\n",
      "Epoch: [120][312/390]\tTime 0.002 (0.003)\tLoss 1.2868 (1.0448)\tPrec@1 53.125 (62.590)\n",
      "Epoch: [120][390/390]\tTime 0.002 (0.003)\tLoss 1.0729 (1.0587)\tPrec@1 65.000 (62.026)\n",
      "EPOCH: 120 train Results: Prec@1 62.026 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1100 (1.1100)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2774 (1.2656)\tPrec@1 43.750 (55.460)\n",
      "EPOCH: 120 val Results: Prec@1 55.460 Loss: 1.2656\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [121][0/390]\tTime 0.005 (0.005)\tLoss 0.8959 (0.8959)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [121][78/390]\tTime 0.002 (0.003)\tLoss 0.9726 (0.9562)\tPrec@1 59.375 (65.625)\n",
      "Epoch: [121][156/390]\tTime 0.002 (0.003)\tLoss 0.9311 (1.0080)\tPrec@1 70.312 (63.814)\n",
      "Epoch: [121][234/390]\tTime 0.003 (0.003)\tLoss 1.1400 (1.0284)\tPrec@1 60.156 (63.271)\n",
      "Epoch: [121][312/390]\tTime 0.003 (0.003)\tLoss 0.9345 (1.0405)\tPrec@1 67.188 (62.814)\n",
      "Epoch: [121][390/390]\tTime 0.001 (0.003)\tLoss 0.9419 (1.0548)\tPrec@1 61.250 (62.314)\n",
      "EPOCH: 121 train Results: Prec@1 62.314 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1802 (1.1802)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5896 (1.2822)\tPrec@1 43.750 (55.090)\n",
      "EPOCH: 121 val Results: Prec@1 55.090 Loss: 1.2822\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [122][0/390]\tTime 0.004 (0.004)\tLoss 0.9090 (0.9090)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [122][78/390]\tTime 0.002 (0.003)\tLoss 1.0022 (0.9731)\tPrec@1 61.719 (65.506)\n",
      "Epoch: [122][156/390]\tTime 0.004 (0.003)\tLoss 1.0020 (1.0078)\tPrec@1 64.844 (64.436)\n",
      "Epoch: [122][234/390]\tTime 0.007 (0.003)\tLoss 1.1966 (1.0291)\tPrec@1 57.812 (63.398)\n",
      "Epoch: [122][312/390]\tTime 0.003 (0.003)\tLoss 1.0570 (1.0457)\tPrec@1 58.594 (62.692)\n",
      "Epoch: [122][390/390]\tTime 0.001 (0.003)\tLoss 1.1433 (1.0587)\tPrec@1 57.500 (62.310)\n",
      "EPOCH: 122 train Results: Prec@1 62.310 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1627 (1.1627)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5027 (1.2815)\tPrec@1 43.750 (55.660)\n",
      "EPOCH: 122 val Results: Prec@1 55.660 Loss: 1.2815\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [123][0/390]\tTime 0.002 (0.002)\tLoss 0.9790 (0.9790)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [123][78/390]\tTime 0.007 (0.003)\tLoss 0.9149 (0.9817)\tPrec@1 67.969 (64.636)\n",
      "Epoch: [123][156/390]\tTime 0.002 (0.003)\tLoss 1.1353 (1.0076)\tPrec@1 61.719 (63.560)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.003)\tLoss 1.2845 (1.0295)\tPrec@1 61.719 (62.982)\n",
      "Epoch: [123][312/390]\tTime 0.005 (0.003)\tLoss 1.1044 (1.0447)\tPrec@1 61.719 (62.572)\n",
      "Epoch: [123][390/390]\tTime 0.001 (0.003)\tLoss 1.0842 (1.0589)\tPrec@1 60.000 (62.072)\n",
      "EPOCH: 123 train Results: Prec@1 62.072 Loss: 1.0589\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2142 (1.2142)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7815 (1.2734)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 123 val Results: Prec@1 55.370 Loss: 1.2734\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [124][0/390]\tTime 0.002 (0.002)\tLoss 0.8392 (0.8392)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [124][78/390]\tTime 0.004 (0.003)\tLoss 0.9400 (0.9751)\tPrec@1 67.188 (65.259)\n",
      "Epoch: [124][156/390]\tTime 0.008 (0.003)\tLoss 1.0112 (1.0143)\tPrec@1 64.844 (63.878)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.0604 (1.0300)\tPrec@1 57.812 (63.281)\n",
      "Epoch: [124][312/390]\tTime 0.002 (0.003)\tLoss 1.1325 (1.0407)\tPrec@1 62.500 (62.977)\n",
      "Epoch: [124][390/390]\tTime 0.001 (0.003)\tLoss 1.1941 (1.0550)\tPrec@1 53.750 (62.520)\n",
      "EPOCH: 124 train Results: Prec@1 62.520 Loss: 1.0550\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1685 (1.1685)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4352 (1.2757)\tPrec@1 50.000 (55.270)\n",
      "EPOCH: 124 val Results: Prec@1 55.270 Loss: 1.2757\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [125][0/390]\tTime 0.004 (0.004)\tLoss 1.0182 (1.0182)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [125][78/390]\tTime 0.003 (0.004)\tLoss 0.9252 (0.9738)\tPrec@1 67.188 (65.200)\n",
      "Epoch: [125][156/390]\tTime 0.002 (0.003)\tLoss 0.9221 (0.9993)\tPrec@1 68.750 (64.431)\n",
      "Epoch: [125][234/390]\tTime 0.002 (0.003)\tLoss 1.1496 (1.0252)\tPrec@1 59.375 (63.514)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.0632 (1.0392)\tPrec@1 60.938 (62.904)\n",
      "Epoch: [125][390/390]\tTime 0.001 (0.003)\tLoss 1.0824 (1.0525)\tPrec@1 60.000 (62.406)\n",
      "EPOCH: 125 train Results: Prec@1 62.406 Loss: 1.0525\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1412 (1.1412)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0219 (1.2709)\tPrec@1 56.250 (55.580)\n",
      "EPOCH: 125 val Results: Prec@1 55.580 Loss: 1.2709\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [126][0/390]\tTime 0.002 (0.002)\tLoss 0.8516 (0.8516)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [126][78/390]\tTime 0.003 (0.003)\tLoss 0.9495 (0.9678)\tPrec@1 66.406 (65.348)\n",
      "Epoch: [126][156/390]\tTime 0.004 (0.004)\tLoss 1.0913 (1.0080)\tPrec@1 64.844 (64.152)\n",
      "Epoch: [126][234/390]\tTime 0.016 (0.004)\tLoss 1.2227 (1.0262)\tPrec@1 57.031 (63.371)\n",
      "Epoch: [126][312/390]\tTime 0.006 (0.003)\tLoss 1.0892 (1.0451)\tPrec@1 56.250 (62.782)\n",
      "Epoch: [126][390/390]\tTime 0.001 (0.003)\tLoss 1.0182 (1.0570)\tPrec@1 58.750 (62.506)\n",
      "EPOCH: 126 train Results: Prec@1 62.506 Loss: 1.0570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1650 (1.1650)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5126 (1.2862)\tPrec@1 43.750 (54.960)\n",
      "EPOCH: 126 val Results: Prec@1 54.960 Loss: 1.2862\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [127][0/390]\tTime 0.002 (0.002)\tLoss 1.0068 (1.0068)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.003)\tLoss 0.8789 (0.9775)\tPrec@1 69.531 (65.210)\n",
      "Epoch: [127][156/390]\tTime 0.003 (0.003)\tLoss 0.9612 (0.9985)\tPrec@1 62.500 (64.406)\n",
      "Epoch: [127][234/390]\tTime 0.002 (0.004)\tLoss 1.2703 (1.0235)\tPrec@1 55.469 (63.544)\n",
      "Epoch: [127][312/390]\tTime 0.002 (0.004)\tLoss 1.1859 (1.0448)\tPrec@1 55.469 (62.672)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.004)\tLoss 1.1352 (1.0575)\tPrec@1 62.500 (62.292)\n",
      "EPOCH: 127 train Results: Prec@1 62.292 Loss: 1.0575\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1415 (1.1415)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0750 (1.2656)\tPrec@1 56.250 (55.670)\n",
      "EPOCH: 127 val Results: Prec@1 55.670 Loss: 1.2656\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [128][0/390]\tTime 0.002 (0.002)\tLoss 0.9762 (0.9762)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [128][78/390]\tTime 0.004 (0.003)\tLoss 1.0724 (0.9754)\tPrec@1 63.281 (65.299)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.003)\tLoss 0.9882 (1.0082)\tPrec@1 67.969 (64.072)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.003)\tLoss 1.0450 (1.0252)\tPrec@1 62.500 (63.467)\n",
      "Epoch: [128][312/390]\tTime 0.003 (0.003)\tLoss 1.0905 (1.0415)\tPrec@1 60.938 (62.834)\n",
      "Epoch: [128][390/390]\tTime 0.001 (0.003)\tLoss 1.2652 (1.0555)\tPrec@1 55.000 (62.472)\n",
      "EPOCH: 128 train Results: Prec@1 62.472 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0864 (1.0864)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3620 (1.2803)\tPrec@1 43.750 (55.280)\n",
      "EPOCH: 128 val Results: Prec@1 55.280 Loss: 1.2803\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [129][0/390]\tTime 0.004 (0.004)\tLoss 1.1178 (1.1178)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 0.9524 (0.9793)\tPrec@1 69.531 (64.962)\n",
      "Epoch: [129][156/390]\tTime 0.017 (0.003)\tLoss 1.0972 (0.9995)\tPrec@1 58.594 (64.286)\n",
      "Epoch: [129][234/390]\tTime 0.002 (0.004)\tLoss 1.1651 (1.0234)\tPrec@1 59.375 (63.457)\n",
      "Epoch: [129][312/390]\tTime 0.002 (0.004)\tLoss 0.9217 (1.0412)\tPrec@1 64.062 (62.857)\n",
      "Epoch: [129][390/390]\tTime 0.002 (0.004)\tLoss 1.1019 (1.0559)\tPrec@1 56.250 (62.276)\n",
      "EPOCH: 129 train Results: Prec@1 62.276 Loss: 1.0559\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1469 (1.1469)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2512 (1.2620)\tPrec@1 31.250 (55.440)\n",
      "EPOCH: 129 val Results: Prec@1 55.440 Loss: 1.2620\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [130][0/390]\tTime 0.002 (0.002)\tLoss 0.8411 (0.8411)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [130][78/390]\tTime 0.002 (0.003)\tLoss 0.8896 (0.9731)\tPrec@1 64.062 (65.417)\n",
      "Epoch: [130][156/390]\tTime 0.002 (0.003)\tLoss 1.1477 (1.0005)\tPrec@1 59.375 (64.316)\n",
      "Epoch: [130][234/390]\tTime 0.004 (0.003)\tLoss 1.1516 (1.0262)\tPrec@1 59.375 (63.298)\n",
      "Epoch: [130][312/390]\tTime 0.002 (0.003)\tLoss 1.1629 (1.0440)\tPrec@1 58.594 (62.800)\n",
      "Epoch: [130][390/390]\tTime 0.001 (0.003)\tLoss 1.3576 (1.0586)\tPrec@1 57.500 (62.286)\n",
      "EPOCH: 130 train Results: Prec@1 62.286 Loss: 1.0586\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2686 (1.2686)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2197 (1.2847)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 130 val Results: Prec@1 54.630 Loss: 1.2847\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [131][0/390]\tTime 0.005 (0.005)\tLoss 0.8687 (0.8687)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [131][78/390]\tTime 0.002 (0.004)\tLoss 0.9167 (0.9755)\tPrec@1 67.188 (65.131)\n",
      "Epoch: [131][156/390]\tTime 0.019 (0.004)\tLoss 1.1968 (1.0022)\tPrec@1 57.031 (64.013)\n",
      "Epoch: [131][234/390]\tTime 0.008 (0.004)\tLoss 1.1939 (1.0250)\tPrec@1 57.031 (63.321)\n",
      "Epoch: [131][312/390]\tTime 0.005 (0.004)\tLoss 1.3674 (1.0440)\tPrec@1 50.781 (62.712)\n",
      "Epoch: [131][390/390]\tTime 0.003 (0.004)\tLoss 1.0260 (1.0576)\tPrec@1 60.000 (62.340)\n",
      "EPOCH: 131 train Results: Prec@1 62.340 Loss: 1.0576\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2590 (1.2590)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4127 (1.2783)\tPrec@1 43.750 (55.660)\n",
      "EPOCH: 131 val Results: Prec@1 55.660 Loss: 1.2783\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [132][0/390]\tTime 0.006 (0.006)\tLoss 0.9295 (0.9295)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [132][78/390]\tTime 0.002 (0.003)\tLoss 0.9584 (0.9716)\tPrec@1 66.406 (65.773)\n",
      "Epoch: [132][156/390]\tTime 0.002 (0.003)\tLoss 1.0773 (1.0175)\tPrec@1 60.156 (63.913)\n",
      "Epoch: [132][234/390]\tTime 0.007 (0.004)\tLoss 1.0384 (1.0396)\tPrec@1 67.969 (63.198)\n",
      "Epoch: [132][312/390]\tTime 0.004 (0.003)\tLoss 1.2507 (1.0496)\tPrec@1 52.344 (62.745)\n",
      "Epoch: [132][390/390]\tTime 0.002 (0.004)\tLoss 1.0426 (1.0567)\tPrec@1 63.750 (62.416)\n",
      "EPOCH: 132 train Results: Prec@1 62.416 Loss: 1.0567\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1601 (1.1601)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1963 (1.2712)\tPrec@1 43.750 (55.680)\n",
      "EPOCH: 132 val Results: Prec@1 55.680 Loss: 1.2712\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [133][0/390]\tTime 0.004 (0.004)\tLoss 0.9084 (0.9084)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [133][78/390]\tTime 0.002 (0.004)\tLoss 0.9303 (0.9719)\tPrec@1 66.406 (65.309)\n",
      "Epoch: [133][156/390]\tTime 0.004 (0.004)\tLoss 0.9852 (0.9931)\tPrec@1 63.281 (64.525)\n",
      "Epoch: [133][234/390]\tTime 0.003 (0.004)\tLoss 1.1468 (1.0221)\tPrec@1 59.375 (63.318)\n",
      "Epoch: [133][312/390]\tTime 0.002 (0.003)\tLoss 1.0194 (1.0436)\tPrec@1 66.406 (62.530)\n",
      "Epoch: [133][390/390]\tTime 0.006 (0.003)\tLoss 1.0452 (1.0539)\tPrec@1 62.500 (62.218)\n",
      "EPOCH: 133 train Results: Prec@1 62.218 Loss: 1.0539\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2785 (1.2785)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4063 (1.2824)\tPrec@1 43.750 (55.200)\n",
      "EPOCH: 133 val Results: Prec@1 55.200 Loss: 1.2824\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [134][0/390]\tTime 0.002 (0.002)\tLoss 0.9994 (0.9994)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [134][78/390]\tTime 0.002 (0.004)\tLoss 1.0019 (0.9734)\tPrec@1 67.188 (66.110)\n",
      "Epoch: [134][156/390]\tTime 0.002 (0.004)\tLoss 1.0233 (1.0144)\tPrec@1 64.844 (64.291)\n",
      "Epoch: [134][234/390]\tTime 0.005 (0.004)\tLoss 1.1444 (1.0318)\tPrec@1 61.719 (63.501)\n",
      "Epoch: [134][312/390]\tTime 0.008 (0.003)\tLoss 1.0489 (1.0438)\tPrec@1 63.281 (63.044)\n",
      "Epoch: [134][390/390]\tTime 0.014 (0.003)\tLoss 1.1543 (1.0562)\tPrec@1 55.000 (62.580)\n",
      "EPOCH: 134 train Results: Prec@1 62.580 Loss: 1.0562\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1828 (1.1828)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2819 (1.2673)\tPrec@1 43.750 (55.810)\n",
      "EPOCH: 134 val Results: Prec@1 55.810 Loss: 1.2673\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [135][0/390]\tTime 0.005 (0.005)\tLoss 0.9706 (0.9706)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [135][78/390]\tTime 0.007 (0.003)\tLoss 1.0065 (0.9840)\tPrec@1 67.969 (64.715)\n",
      "Epoch: [135][156/390]\tTime 0.008 (0.003)\tLoss 1.0629 (1.0066)\tPrec@1 64.844 (64.008)\n",
      "Epoch: [135][234/390]\tTime 0.002 (0.003)\tLoss 1.2625 (1.0292)\tPrec@1 53.906 (63.075)\n",
      "Epoch: [135][312/390]\tTime 0.003 (0.003)\tLoss 0.9436 (1.0465)\tPrec@1 64.062 (62.527)\n",
      "Epoch: [135][390/390]\tTime 0.001 (0.003)\tLoss 1.1159 (1.0594)\tPrec@1 60.000 (62.188)\n",
      "EPOCH: 135 train Results: Prec@1 62.188 Loss: 1.0594\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2432 (1.2432)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1610 (1.2677)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 135 val Results: Prec@1 55.230 Loss: 1.2677\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [136][0/390]\tTime 0.003 (0.003)\tLoss 0.8243 (0.8243)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [136][78/390]\tTime 0.002 (0.003)\tLoss 1.1068 (0.9801)\tPrec@1 62.500 (65.546)\n",
      "Epoch: [136][156/390]\tTime 0.002 (0.003)\tLoss 0.8672 (1.0030)\tPrec@1 71.875 (64.739)\n",
      "Epoch: [136][234/390]\tTime 0.003 (0.003)\tLoss 1.0238 (1.0240)\tPrec@1 62.500 (63.687)\n",
      "Epoch: [136][312/390]\tTime 0.002 (0.003)\tLoss 1.1397 (1.0386)\tPrec@1 62.500 (63.084)\n",
      "Epoch: [136][390/390]\tTime 0.003 (0.003)\tLoss 1.1839 (1.0492)\tPrec@1 57.500 (62.758)\n",
      "EPOCH: 136 train Results: Prec@1 62.758 Loss: 1.0492\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2107 (1.2107)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1389 (1.2795)\tPrec@1 68.750 (55.500)\n",
      "EPOCH: 136 val Results: Prec@1 55.500 Loss: 1.2795\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [137][0/390]\tTime 0.005 (0.005)\tLoss 1.0124 (1.0124)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [137][78/390]\tTime 0.002 (0.003)\tLoss 1.0741 (0.9864)\tPrec@1 60.156 (64.903)\n",
      "Epoch: [137][156/390]\tTime 0.002 (0.003)\tLoss 0.9216 (1.0034)\tPrec@1 68.750 (64.510)\n",
      "Epoch: [137][234/390]\tTime 0.006 (0.003)\tLoss 0.9174 (1.0216)\tPrec@1 66.406 (63.481)\n",
      "Epoch: [137][312/390]\tTime 0.002 (0.004)\tLoss 0.9866 (1.0385)\tPrec@1 68.750 (62.879)\n",
      "Epoch: [137][390/390]\tTime 0.009 (0.005)\tLoss 1.2039 (1.0551)\tPrec@1 60.000 (62.384)\n",
      "EPOCH: 137 train Results: Prec@1 62.384 Loss: 1.0551\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1189 (1.1189)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3363 (1.2738)\tPrec@1 43.750 (55.560)\n",
      "EPOCH: 137 val Results: Prec@1 55.560 Loss: 1.2738\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [138][0/390]\tTime 0.005 (0.005)\tLoss 1.0291 (1.0291)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [138][78/390]\tTime 0.002 (0.005)\tLoss 0.9423 (0.9646)\tPrec@1 64.844 (65.724)\n",
      "Epoch: [138][156/390]\tTime 0.009 (0.004)\tLoss 0.9918 (1.0057)\tPrec@1 60.938 (64.117)\n",
      "Epoch: [138][234/390]\tTime 0.004 (0.005)\tLoss 1.1888 (1.0271)\tPrec@1 57.812 (63.374)\n",
      "Epoch: [138][312/390]\tTime 0.002 (0.005)\tLoss 1.1460 (1.0424)\tPrec@1 63.281 (62.750)\n",
      "Epoch: [138][390/390]\tTime 0.002 (0.005)\tLoss 1.1539 (1.0557)\tPrec@1 57.500 (62.274)\n",
      "EPOCH: 138 train Results: Prec@1 62.274 Loss: 1.0557\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1866 (1.1866)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5186 (1.2796)\tPrec@1 37.500 (55.140)\n",
      "EPOCH: 138 val Results: Prec@1 55.140 Loss: 1.2796\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [139][0/390]\tTime 0.003 (0.003)\tLoss 0.9915 (0.9915)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [139][78/390]\tTime 0.004 (0.003)\tLoss 0.9201 (0.9818)\tPrec@1 68.750 (64.854)\n",
      "Epoch: [139][156/390]\tTime 0.002 (0.003)\tLoss 1.0426 (1.0087)\tPrec@1 64.844 (64.222)\n",
      "Epoch: [139][234/390]\tTime 0.004 (0.003)\tLoss 0.9975 (1.0276)\tPrec@1 66.406 (63.411)\n",
      "Epoch: [139][312/390]\tTime 0.002 (0.003)\tLoss 1.0904 (1.0424)\tPrec@1 58.594 (62.797)\n",
      "Epoch: [139][390/390]\tTime 0.005 (0.003)\tLoss 1.0182 (1.0546)\tPrec@1 63.750 (62.312)\n",
      "EPOCH: 139 train Results: Prec@1 62.312 Loss: 1.0546\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1983 (1.1983)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.3045 (1.2905)\tPrec@1 37.500 (54.740)\n",
      "EPOCH: 139 val Results: Prec@1 54.740 Loss: 1.2905\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [140][0/390]\tTime 0.003 (0.003)\tLoss 1.0560 (1.0560)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [140][78/390]\tTime 0.002 (0.003)\tLoss 0.9958 (0.9716)\tPrec@1 67.969 (66.446)\n",
      "Epoch: [140][156/390]\tTime 0.003 (0.003)\tLoss 0.9786 (0.9985)\tPrec@1 64.844 (64.675)\n",
      "Epoch: [140][234/390]\tTime 0.004 (0.003)\tLoss 0.9594 (1.0215)\tPrec@1 62.500 (63.783)\n",
      "Epoch: [140][312/390]\tTime 0.002 (0.003)\tLoss 0.9797 (1.0382)\tPrec@1 60.938 (63.024)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.003)\tLoss 1.2719 (1.0552)\tPrec@1 52.500 (62.416)\n",
      "EPOCH: 140 train Results: Prec@1 62.416 Loss: 1.0552\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2126 (1.2126)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.4619 (1.2797)\tPrec@1 37.500 (54.960)\n",
      "EPOCH: 140 val Results: Prec@1 54.960 Loss: 1.2797\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [141][0/390]\tTime 0.005 (0.005)\tLoss 0.6640 (0.6640)\tPrec@1 78.906 (78.906)\n",
      "Epoch: [141][78/390]\tTime 0.002 (0.006)\tLoss 1.1954 (0.9732)\tPrec@1 57.812 (65.338)\n",
      "Epoch: [141][156/390]\tTime 0.002 (0.006)\tLoss 1.0877 (1.0072)\tPrec@1 57.031 (63.858)\n",
      "Epoch: [141][234/390]\tTime 0.004 (0.006)\tLoss 1.1971 (1.0296)\tPrec@1 61.719 (63.098)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.006)\tLoss 1.1613 (1.0437)\tPrec@1 62.500 (62.517)\n",
      "Epoch: [141][390/390]\tTime 0.002 (0.005)\tLoss 1.2001 (1.0540)\tPrec@1 56.250 (62.234)\n",
      "EPOCH: 141 train Results: Prec@1 62.234 Loss: 1.0540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2176 (1.2176)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2271 (1.2754)\tPrec@1 37.500 (55.000)\n",
      "EPOCH: 141 val Results: Prec@1 55.000 Loss: 1.2754\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [142][0/390]\tTime 0.004 (0.004)\tLoss 0.9739 (0.9739)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [142][78/390]\tTime 0.002 (0.003)\tLoss 0.9589 (0.9787)\tPrec@1 63.281 (65.279)\n",
      "Epoch: [142][156/390]\tTime 0.003 (0.003)\tLoss 0.9702 (1.0037)\tPrec@1 70.312 (64.356)\n",
      "Epoch: [142][234/390]\tTime 0.009 (0.003)\tLoss 1.0196 (1.0175)\tPrec@1 64.062 (63.750)\n",
      "Epoch: [142][312/390]\tTime 0.004 (0.004)\tLoss 1.2466 (1.0357)\tPrec@1 54.688 (62.994)\n",
      "Epoch: [142][390/390]\tTime 0.002 (0.004)\tLoss 1.0765 (1.0498)\tPrec@1 63.750 (62.438)\n",
      "EPOCH: 142 train Results: Prec@1 62.438 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3533 (1.3533)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2340 (1.2858)\tPrec@1 37.500 (55.080)\n",
      "EPOCH: 142 val Results: Prec@1 55.080 Loss: 1.2858\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [143][0/390]\tTime 0.003 (0.003)\tLoss 0.8759 (0.8759)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [143][78/390]\tTime 0.002 (0.003)\tLoss 1.0551 (0.9653)\tPrec@1 63.281 (65.961)\n",
      "Epoch: [143][156/390]\tTime 0.002 (0.004)\tLoss 1.0174 (1.0025)\tPrec@1 63.281 (64.267)\n",
      "Epoch: [143][234/390]\tTime 0.017 (0.003)\tLoss 1.0090 (1.0229)\tPrec@1 64.844 (63.620)\n",
      "Epoch: [143][312/390]\tTime 0.003 (0.003)\tLoss 1.1809 (1.0379)\tPrec@1 60.156 (63.007)\n",
      "Epoch: [143][390/390]\tTime 0.001 (0.004)\tLoss 1.0902 (1.0520)\tPrec@1 58.750 (62.446)\n",
      "EPOCH: 143 train Results: Prec@1 62.446 Loss: 1.0520\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1666 (1.1666)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5304 (1.2861)\tPrec@1 43.750 (55.070)\n",
      "EPOCH: 143 val Results: Prec@1 55.070 Loss: 1.2861\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [144][0/390]\tTime 0.002 (0.002)\tLoss 0.8754 (0.8754)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [144][78/390]\tTime 0.002 (0.005)\tLoss 1.0114 (0.9766)\tPrec@1 67.969 (64.953)\n",
      "Epoch: [144][156/390]\tTime 0.002 (0.006)\tLoss 1.2154 (1.0054)\tPrec@1 55.469 (64.152)\n",
      "Epoch: [144][234/390]\tTime 0.002 (0.006)\tLoss 1.1210 (1.0291)\tPrec@1 60.156 (63.275)\n",
      "Epoch: [144][312/390]\tTime 0.003 (0.005)\tLoss 1.1045 (1.0406)\tPrec@1 62.500 (62.802)\n",
      "Epoch: [144][390/390]\tTime 0.005 (0.005)\tLoss 1.1109 (1.0540)\tPrec@1 56.250 (62.280)\n",
      "EPOCH: 144 train Results: Prec@1 62.280 Loss: 1.0540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0783 (1.0783)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3885 (1.2691)\tPrec@1 50.000 (55.190)\n",
      "EPOCH: 144 val Results: Prec@1 55.190 Loss: 1.2691\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [145][0/390]\tTime 0.005 (0.005)\tLoss 0.9592 (0.9592)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [145][78/390]\tTime 0.002 (0.004)\tLoss 0.9436 (0.9803)\tPrec@1 64.844 (64.715)\n",
      "Epoch: [145][156/390]\tTime 0.009 (0.004)\tLoss 1.1899 (1.0002)\tPrec@1 57.812 (64.341)\n",
      "Epoch: [145][234/390]\tTime 0.004 (0.005)\tLoss 0.9033 (1.0264)\tPrec@1 66.406 (63.404)\n",
      "Epoch: [145][312/390]\tTime 0.003 (0.004)\tLoss 1.2107 (1.0461)\tPrec@1 56.250 (62.615)\n",
      "Epoch: [145][390/390]\tTime 0.002 (0.004)\tLoss 1.1190 (1.0570)\tPrec@1 62.500 (62.316)\n",
      "EPOCH: 145 train Results: Prec@1 62.316 Loss: 1.0570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1698 (1.1698)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4618 (1.2749)\tPrec@1 43.750 (55.090)\n",
      "EPOCH: 145 val Results: Prec@1 55.090 Loss: 1.2749\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [146][0/390]\tTime 0.002 (0.002)\tLoss 1.0142 (1.0142)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [146][78/390]\tTime 0.003 (0.003)\tLoss 0.9875 (0.9765)\tPrec@1 64.062 (65.091)\n",
      "Epoch: [146][156/390]\tTime 0.003 (0.004)\tLoss 0.9657 (1.0124)\tPrec@1 67.188 (63.829)\n",
      "Epoch: [146][234/390]\tTime 0.003 (0.005)\tLoss 0.9167 (1.0300)\tPrec@1 68.750 (63.228)\n",
      "Epoch: [146][312/390]\tTime 0.003 (0.006)\tLoss 0.9189 (1.0453)\tPrec@1 66.406 (62.715)\n",
      "Epoch: [146][390/390]\tTime 0.006 (0.006)\tLoss 0.9842 (1.0583)\tPrec@1 62.500 (62.204)\n",
      "EPOCH: 146 train Results: Prec@1 62.204 Loss: 1.0583\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1554 (1.1554)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2210 (1.2823)\tPrec@1 50.000 (54.690)\n",
      "EPOCH: 146 val Results: Prec@1 54.690 Loss: 1.2823\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [147][0/390]\tTime 0.010 (0.010)\tLoss 0.8892 (0.8892)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [147][78/390]\tTime 0.008 (0.006)\tLoss 0.9920 (0.9799)\tPrec@1 64.844 (65.150)\n",
      "Epoch: [147][156/390]\tTime 0.005 (0.006)\tLoss 1.1061 (1.0057)\tPrec@1 62.500 (64.097)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.006)\tLoss 1.1256 (1.0273)\tPrec@1 62.500 (63.338)\n",
      "Epoch: [147][312/390]\tTime 0.003 (0.005)\tLoss 1.1484 (1.0442)\tPrec@1 64.844 (62.620)\n",
      "Epoch: [147][390/390]\tTime 0.002 (0.005)\tLoss 1.1600 (1.0557)\tPrec@1 56.250 (62.284)\n",
      "EPOCH: 147 train Results: Prec@1 62.284 Loss: 1.0557\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1474 (1.1474)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5218 (1.2847)\tPrec@1 37.500 (54.490)\n",
      "EPOCH: 147 val Results: Prec@1 54.490 Loss: 1.2847\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [148][0/390]\tTime 0.003 (0.003)\tLoss 0.9431 (0.9431)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [148][78/390]\tTime 0.004 (0.004)\tLoss 1.0363 (0.9796)\tPrec@1 60.938 (65.140)\n",
      "Epoch: [148][156/390]\tTime 0.003 (0.004)\tLoss 1.0889 (1.0081)\tPrec@1 58.594 (63.973)\n",
      "Epoch: [148][234/390]\tTime 0.002 (0.005)\tLoss 1.0857 (1.0325)\tPrec@1 55.469 (63.112)\n",
      "Epoch: [148][312/390]\tTime 0.005 (0.004)\tLoss 1.1014 (1.0435)\tPrec@1 63.281 (62.837)\n",
      "Epoch: [148][390/390]\tTime 0.001 (0.004)\tLoss 0.8781 (1.0540)\tPrec@1 65.000 (62.338)\n",
      "EPOCH: 148 train Results: Prec@1 62.338 Loss: 1.0540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1848 (1.1848)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4664 (1.2815)\tPrec@1 50.000 (55.370)\n",
      "EPOCH: 148 val Results: Prec@1 55.370 Loss: 1.2815\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [149][0/390]\tTime 0.004 (0.004)\tLoss 0.8937 (0.8937)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [149][78/390]\tTime 0.004 (0.003)\tLoss 0.9870 (0.9639)\tPrec@1 67.969 (65.922)\n",
      "Epoch: [149][156/390]\tTime 0.008 (0.003)\tLoss 0.9871 (0.9974)\tPrec@1 64.844 (64.436)\n",
      "Epoch: [149][234/390]\tTime 0.004 (0.003)\tLoss 1.0101 (1.0245)\tPrec@1 64.844 (63.441)\n",
      "Epoch: [149][312/390]\tTime 0.002 (0.003)\tLoss 1.1388 (1.0391)\tPrec@1 55.469 (62.792)\n",
      "Epoch: [149][390/390]\tTime 0.003 (0.003)\tLoss 1.1773 (1.0537)\tPrec@1 57.500 (62.218)\n",
      "EPOCH: 149 train Results: Prec@1 62.218 Loss: 1.0537\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1329 (1.1329)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3726 (1.2654)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 149 val Results: Prec@1 55.150 Loss: 1.2654\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [150][0/390]\tTime 0.002 (0.002)\tLoss 1.0329 (1.0329)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [150][78/390]\tTime 0.005 (0.004)\tLoss 1.2202 (0.9547)\tPrec@1 57.812 (66.208)\n",
      "Epoch: [150][156/390]\tTime 0.002 (0.004)\tLoss 1.1726 (0.9919)\tPrec@1 59.375 (64.734)\n",
      "Epoch: [150][234/390]\tTime 0.002 (0.004)\tLoss 1.2929 (1.0245)\tPrec@1 54.688 (63.344)\n",
      "Epoch: [150][312/390]\tTime 0.002 (0.004)\tLoss 1.1972 (1.0413)\tPrec@1 57.812 (62.797)\n",
      "Epoch: [150][390/390]\tTime 0.006 (0.004)\tLoss 1.0898 (1.0517)\tPrec@1 62.500 (62.364)\n",
      "EPOCH: 150 train Results: Prec@1 62.364 Loss: 1.0517\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1993 (1.1993)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3348 (1.2688)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 150 val Results: Prec@1 55.630 Loss: 1.2688\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [151][0/390]\tTime 0.005 (0.005)\tLoss 0.9282 (0.9282)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [151][78/390]\tTime 0.003 (0.004)\tLoss 1.1566 (0.9652)\tPrec@1 53.906 (65.477)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.005)\tLoss 0.9782 (0.9939)\tPrec@1 66.406 (64.456)\n",
      "Epoch: [151][234/390]\tTime 0.003 (0.004)\tLoss 1.1344 (1.0222)\tPrec@1 54.688 (63.295)\n",
      "Epoch: [151][312/390]\tTime 0.004 (0.004)\tLoss 1.1533 (1.0376)\tPrec@1 59.375 (62.705)\n",
      "Epoch: [151][390/390]\tTime 0.002 (0.004)\tLoss 1.2783 (1.0563)\tPrec@1 57.500 (62.182)\n",
      "EPOCH: 151 train Results: Prec@1 62.182 Loss: 1.0563\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2193 (1.2193)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3854 (1.2657)\tPrec@1 50.000 (55.300)\n",
      "EPOCH: 151 val Results: Prec@1 55.300 Loss: 1.2657\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [152][0/390]\tTime 0.004 (0.004)\tLoss 0.8986 (0.8986)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [152][78/390]\tTime 0.003 (0.003)\tLoss 1.0596 (0.9748)\tPrec@1 57.812 (65.338)\n",
      "Epoch: [152][156/390]\tTime 0.002 (0.003)\tLoss 1.0129 (1.0089)\tPrec@1 62.500 (64.003)\n",
      "Epoch: [152][234/390]\tTime 0.002 (0.003)\tLoss 1.0813 (1.0229)\tPrec@1 64.844 (63.627)\n",
      "Epoch: [152][312/390]\tTime 0.002 (0.003)\tLoss 1.2338 (1.0372)\tPrec@1 48.438 (63.064)\n",
      "Epoch: [152][390/390]\tTime 0.003 (0.003)\tLoss 0.9794 (1.0489)\tPrec@1 71.250 (62.574)\n",
      "EPOCH: 152 train Results: Prec@1 62.574 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1441 (1.1441)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4594 (1.2726)\tPrec@1 37.500 (55.070)\n",
      "EPOCH: 152 val Results: Prec@1 55.070 Loss: 1.2726\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [153][0/390]\tTime 0.002 (0.002)\tLoss 0.8083 (0.8083)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [153][78/390]\tTime 0.002 (0.004)\tLoss 0.9372 (0.9632)\tPrec@1 67.969 (65.734)\n",
      "Epoch: [153][156/390]\tTime 0.004 (0.003)\tLoss 0.9652 (0.9964)\tPrec@1 67.188 (64.675)\n",
      "Epoch: [153][234/390]\tTime 0.002 (0.004)\tLoss 1.0995 (1.0186)\tPrec@1 61.719 (63.890)\n",
      "Epoch: [153][312/390]\tTime 0.008 (0.004)\tLoss 0.9770 (1.0360)\tPrec@1 67.188 (63.264)\n",
      "Epoch: [153][390/390]\tTime 0.010 (0.003)\tLoss 1.1738 (1.0512)\tPrec@1 58.750 (62.586)\n",
      "EPOCH: 153 train Results: Prec@1 62.586 Loss: 1.0512\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1974 (1.1974)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2853 (1.2674)\tPrec@1 50.000 (55.400)\n",
      "EPOCH: 153 val Results: Prec@1 55.400 Loss: 1.2674\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [154][0/390]\tTime 0.006 (0.006)\tLoss 0.8441 (0.8441)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.003)\tLoss 0.9333 (0.9666)\tPrec@1 64.062 (65.655)\n",
      "Epoch: [154][156/390]\tTime 0.002 (0.004)\tLoss 1.1208 (0.9972)\tPrec@1 59.375 (64.281)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.004)\tLoss 1.2625 (1.0172)\tPrec@1 56.250 (63.614)\n",
      "Epoch: [154][312/390]\tTime 0.002 (0.004)\tLoss 1.1499 (1.0392)\tPrec@1 55.469 (62.919)\n",
      "Epoch: [154][390/390]\tTime 0.001 (0.004)\tLoss 1.2449 (1.0553)\tPrec@1 57.500 (62.268)\n",
      "EPOCH: 154 train Results: Prec@1 62.268 Loss: 1.0553\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2100 (1.2100)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1720 (1.2699)\tPrec@1 43.750 (54.990)\n",
      "EPOCH: 154 val Results: Prec@1 54.990 Loss: 1.2699\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [155][0/390]\tTime 0.004 (0.004)\tLoss 1.0149 (1.0149)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [155][78/390]\tTime 0.005 (0.003)\tLoss 1.0837 (0.9690)\tPrec@1 64.844 (65.783)\n",
      "Epoch: [155][156/390]\tTime 0.004 (0.003)\tLoss 0.9518 (0.9906)\tPrec@1 64.062 (64.953)\n",
      "Epoch: [155][234/390]\tTime 0.004 (0.003)\tLoss 1.2940 (1.0159)\tPrec@1 50.781 (64.146)\n",
      "Epoch: [155][312/390]\tTime 0.008 (0.003)\tLoss 1.0599 (1.0312)\tPrec@1 67.969 (63.673)\n",
      "Epoch: [155][390/390]\tTime 0.003 (0.003)\tLoss 1.0135 (1.0500)\tPrec@1 62.500 (62.756)\n",
      "EPOCH: 155 train Results: Prec@1 62.756 Loss: 1.0500\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1842 (1.1842)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3321 (1.2830)\tPrec@1 43.750 (54.740)\n",
      "EPOCH: 155 val Results: Prec@1 54.740 Loss: 1.2830\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [156][0/390]\tTime 0.002 (0.002)\tLoss 0.9788 (0.9788)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [156][78/390]\tTime 0.003 (0.003)\tLoss 0.9788 (0.9516)\tPrec@1 60.156 (66.159)\n",
      "Epoch: [156][156/390]\tTime 0.004 (0.003)\tLoss 1.2629 (0.9963)\tPrec@1 53.906 (64.257)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.003)\tLoss 1.1077 (1.0231)\tPrec@1 60.938 (63.351)\n",
      "Epoch: [156][312/390]\tTime 0.002 (0.003)\tLoss 1.2801 (1.0412)\tPrec@1 53.125 (62.707)\n",
      "Epoch: [156][390/390]\tTime 0.013 (0.003)\tLoss 1.1941 (1.0509)\tPrec@1 56.250 (62.368)\n",
      "EPOCH: 156 train Results: Prec@1 62.368 Loss: 1.0509\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2574 (1.2574)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7317 (1.2787)\tPrec@1 31.250 (55.130)\n",
      "EPOCH: 156 val Results: Prec@1 55.130 Loss: 1.2787\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [157][0/390]\tTime 0.006 (0.006)\tLoss 1.0983 (1.0983)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [157][78/390]\tTime 0.002 (0.003)\tLoss 0.9867 (0.9814)\tPrec@1 61.719 (64.814)\n",
      "Epoch: [157][156/390]\tTime 0.065 (0.003)\tLoss 0.9547 (1.0041)\tPrec@1 67.188 (63.699)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.0239 (1.0278)\tPrec@1 67.969 (63.042)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.003)\tLoss 1.1458 (1.0441)\tPrec@1 60.156 (62.665)\n",
      "Epoch: [157][390/390]\tTime 0.001 (0.003)\tLoss 1.1487 (1.0528)\tPrec@1 61.250 (62.408)\n",
      "EPOCH: 157 train Results: Prec@1 62.408 Loss: 1.0528\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.1763 (1.1763)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9522 (1.2784)\tPrec@1 50.000 (55.160)\n",
      "EPOCH: 157 val Results: Prec@1 55.160 Loss: 1.2784\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [158][0/390]\tTime 0.002 (0.002)\tLoss 0.9057 (0.9057)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [158][78/390]\tTime 0.003 (0.003)\tLoss 1.1486 (0.9622)\tPrec@1 60.938 (65.991)\n",
      "Epoch: [158][156/390]\tTime 0.002 (0.004)\tLoss 1.0785 (0.9894)\tPrec@1 57.812 (64.560)\n",
      "Epoch: [158][234/390]\tTime 0.002 (0.003)\tLoss 1.0586 (1.0153)\tPrec@1 59.375 (63.733)\n",
      "Epoch: [158][312/390]\tTime 0.003 (0.003)\tLoss 1.1659 (1.0328)\tPrec@1 60.938 (63.032)\n",
      "Epoch: [158][390/390]\tTime 0.002 (0.004)\tLoss 1.1654 (1.0476)\tPrec@1 60.000 (62.466)\n",
      "EPOCH: 158 train Results: Prec@1 62.466 Loss: 1.0476\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1554 (1.1554)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1961 (1.2789)\tPrec@1 56.250 (55.450)\n",
      "EPOCH: 158 val Results: Prec@1 55.450 Loss: 1.2789\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [159][0/390]\tTime 0.003 (0.003)\tLoss 0.8849 (0.8849)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [159][78/390]\tTime 0.003 (0.003)\tLoss 0.9523 (0.9701)\tPrec@1 67.188 (65.773)\n",
      "Epoch: [159][156/390]\tTime 0.002 (0.004)\tLoss 0.9254 (1.0064)\tPrec@1 66.406 (64.217)\n",
      "Epoch: [159][234/390]\tTime 0.002 (0.004)\tLoss 0.9947 (1.0276)\tPrec@1 66.406 (63.467)\n",
      "Epoch: [159][312/390]\tTime 0.002 (0.004)\tLoss 0.9721 (1.0434)\tPrec@1 63.281 (62.735)\n",
      "Epoch: [159][390/390]\tTime 0.001 (0.004)\tLoss 1.2444 (1.0551)\tPrec@1 51.250 (62.258)\n",
      "EPOCH: 159 train Results: Prec@1 62.258 Loss: 1.0551\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1506 (1.1506)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5919 (1.2747)\tPrec@1 31.250 (55.250)\n",
      "EPOCH: 159 val Results: Prec@1 55.250 Loss: 1.2747\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [160][0/390]\tTime 0.006 (0.006)\tLoss 0.9350 (0.9350)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [160][78/390]\tTime 0.003 (0.003)\tLoss 1.0617 (1.0034)\tPrec@1 64.844 (64.428)\n",
      "Epoch: [160][156/390]\tTime 0.002 (0.003)\tLoss 1.0270 (1.0134)\tPrec@1 64.062 (63.928)\n",
      "Epoch: [160][234/390]\tTime 0.004 (0.004)\tLoss 0.9794 (1.0292)\tPrec@1 67.969 (63.351)\n",
      "Epoch: [160][312/390]\tTime 0.003 (0.004)\tLoss 1.0046 (1.0415)\tPrec@1 67.188 (63.014)\n",
      "Epoch: [160][390/390]\tTime 0.002 (0.003)\tLoss 0.9708 (1.0470)\tPrec@1 70.000 (62.790)\n",
      "EPOCH: 160 train Results: Prec@1 62.790 Loss: 1.0470\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1477 (1.1477)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1484 (1.2714)\tPrec@1 50.000 (55.340)\n",
      "EPOCH: 160 val Results: Prec@1 55.340 Loss: 1.2714\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [161][0/390]\tTime 0.007 (0.007)\tLoss 0.9777 (0.9777)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [161][78/390]\tTime 0.003 (0.003)\tLoss 0.9600 (0.9564)\tPrec@1 68.750 (65.971)\n",
      "Epoch: [161][156/390]\tTime 0.004 (0.003)\tLoss 1.0059 (0.9920)\tPrec@1 63.281 (64.854)\n",
      "Epoch: [161][234/390]\tTime 0.003 (0.003)\tLoss 0.9870 (1.0186)\tPrec@1 66.406 (63.813)\n",
      "Epoch: [161][312/390]\tTime 0.002 (0.003)\tLoss 0.9858 (1.0341)\tPrec@1 67.969 (63.284)\n",
      "Epoch: [161][390/390]\tTime 0.002 (0.003)\tLoss 1.2690 (1.0500)\tPrec@1 51.250 (62.740)\n",
      "EPOCH: 161 train Results: Prec@1 62.740 Loss: 1.0500\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2367 (1.2367)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2709 (1.2649)\tPrec@1 50.000 (55.130)\n",
      "EPOCH: 161 val Results: Prec@1 55.130 Loss: 1.2649\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.8036 (0.8036)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [162][78/390]\tTime 0.002 (0.003)\tLoss 1.0772 (0.9716)\tPrec@1 63.281 (65.220)\n",
      "Epoch: [162][156/390]\tTime 0.005 (0.003)\tLoss 0.9170 (0.9998)\tPrec@1 67.188 (64.466)\n",
      "Epoch: [162][234/390]\tTime 0.002 (0.003)\tLoss 0.9683 (1.0200)\tPrec@1 67.969 (63.664)\n",
      "Epoch: [162][312/390]\tTime 0.005 (0.003)\tLoss 1.1521 (1.0380)\tPrec@1 61.719 (63.059)\n",
      "Epoch: [162][390/390]\tTime 0.007 (0.003)\tLoss 1.0920 (1.0516)\tPrec@1 66.250 (62.646)\n",
      "EPOCH: 162 train Results: Prec@1 62.646 Loss: 1.0516\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2669 (1.2669)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6285 (1.2859)\tPrec@1 43.750 (54.750)\n",
      "EPOCH: 162 val Results: Prec@1 54.750 Loss: 1.2859\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [163][0/390]\tTime 0.004 (0.004)\tLoss 1.0660 (1.0660)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [163][78/390]\tTime 0.002 (0.004)\tLoss 1.0678 (0.9728)\tPrec@1 58.594 (65.210)\n",
      "Epoch: [163][156/390]\tTime 0.002 (0.004)\tLoss 1.0335 (0.9971)\tPrec@1 63.281 (64.550)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.004)\tLoss 1.0391 (1.0233)\tPrec@1 65.625 (63.674)\n",
      "Epoch: [163][312/390]\tTime 0.012 (0.003)\tLoss 1.0980 (1.0383)\tPrec@1 56.250 (63.236)\n",
      "Epoch: [163][390/390]\tTime 0.003 (0.003)\tLoss 1.0405 (1.0512)\tPrec@1 62.500 (62.664)\n",
      "EPOCH: 163 train Results: Prec@1 62.664 Loss: 1.0512\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1462 (1.1462)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1379 (1.2701)\tPrec@1 56.250 (55.660)\n",
      "EPOCH: 163 val Results: Prec@1 55.660 Loss: 1.2701\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [164][0/390]\tTime 0.003 (0.003)\tLoss 0.9560 (0.9560)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [164][78/390]\tTime 0.002 (0.003)\tLoss 1.1102 (0.9647)\tPrec@1 61.719 (65.398)\n",
      "Epoch: [164][156/390]\tTime 0.003 (0.003)\tLoss 1.0740 (1.0015)\tPrec@1 57.031 (64.142)\n",
      "Epoch: [164][234/390]\tTime 0.011 (0.003)\tLoss 1.0299 (1.0263)\tPrec@1 60.938 (63.082)\n",
      "Epoch: [164][312/390]\tTime 0.003 (0.004)\tLoss 1.2488 (1.0449)\tPrec@1 57.812 (62.460)\n",
      "Epoch: [164][390/390]\tTime 0.001 (0.003)\tLoss 1.0010 (1.0548)\tPrec@1 61.250 (62.062)\n",
      "EPOCH: 164 train Results: Prec@1 62.062 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.1130 (1.1130)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0342 (1.2695)\tPrec@1 50.000 (55.530)\n",
      "EPOCH: 164 val Results: Prec@1 55.530 Loss: 1.2695\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [165][0/390]\tTime 0.004 (0.004)\tLoss 1.0631 (1.0631)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [165][78/390]\tTime 0.002 (0.003)\tLoss 0.9483 (0.9485)\tPrec@1 65.625 (66.584)\n",
      "Epoch: [165][156/390]\tTime 0.003 (0.003)\tLoss 1.0360 (0.9939)\tPrec@1 64.844 (64.719)\n",
      "Epoch: [165][234/390]\tTime 0.006 (0.003)\tLoss 1.2476 (1.0186)\tPrec@1 60.156 (63.813)\n",
      "Epoch: [165][312/390]\tTime 0.003 (0.003)\tLoss 1.1353 (1.0381)\tPrec@1 60.156 (63.079)\n",
      "Epoch: [165][390/390]\tTime 0.003 (0.003)\tLoss 1.1594 (1.0497)\tPrec@1 63.750 (62.564)\n",
      "EPOCH: 165 train Results: Prec@1 62.564 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1508 (1.1508)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0310 (1.2886)\tPrec@1 43.750 (54.810)\n",
      "EPOCH: 165 val Results: Prec@1 54.810 Loss: 1.2886\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [166][0/390]\tTime 0.005 (0.005)\tLoss 1.0151 (1.0151)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.004)\tLoss 0.9569 (0.9599)\tPrec@1 67.188 (65.605)\n",
      "Epoch: [166][156/390]\tTime 0.009 (0.003)\tLoss 1.0884 (0.9916)\tPrec@1 63.281 (64.510)\n",
      "Epoch: [166][234/390]\tTime 0.003 (0.003)\tLoss 1.2448 (1.0263)\tPrec@1 53.906 (63.025)\n",
      "Epoch: [166][312/390]\tTime 0.002 (0.003)\tLoss 1.0379 (1.0391)\tPrec@1 63.281 (62.657)\n",
      "Epoch: [166][390/390]\tTime 0.002 (0.004)\tLoss 0.9054 (1.0531)\tPrec@1 70.000 (62.324)\n",
      "EPOCH: 166 train Results: Prec@1 62.324 Loss: 1.0531\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1791 (1.1791)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2267 (1.2762)\tPrec@1 43.750 (55.560)\n",
      "EPOCH: 166 val Results: Prec@1 55.560 Loss: 1.2762\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [167][0/390]\tTime 0.004 (0.004)\tLoss 1.0987 (1.0987)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [167][78/390]\tTime 0.005 (0.004)\tLoss 1.0524 (0.9703)\tPrec@1 60.938 (65.407)\n",
      "Epoch: [167][156/390]\tTime 0.004 (0.004)\tLoss 0.9838 (0.9944)\tPrec@1 66.406 (64.217)\n",
      "Epoch: [167][234/390]\tTime 0.002 (0.003)\tLoss 1.0791 (1.0224)\tPrec@1 60.156 (63.447)\n",
      "Epoch: [167][312/390]\tTime 0.003 (0.003)\tLoss 1.2113 (1.0356)\tPrec@1 61.719 (62.954)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.003)\tLoss 1.1419 (1.0494)\tPrec@1 58.750 (62.482)\n",
      "EPOCH: 167 train Results: Prec@1 62.482 Loss: 1.0494\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1709 (1.1709)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2743 (1.2681)\tPrec@1 56.250 (55.670)\n",
      "EPOCH: 167 val Results: Prec@1 55.670 Loss: 1.2681\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [168][0/390]\tTime 0.004 (0.004)\tLoss 0.8502 (0.8502)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [168][78/390]\tTime 0.002 (0.003)\tLoss 0.9077 (0.9581)\tPrec@1 69.531 (65.813)\n",
      "Epoch: [168][156/390]\tTime 0.003 (0.003)\tLoss 1.0789 (0.9923)\tPrec@1 64.844 (64.719)\n",
      "Epoch: [168][234/390]\tTime 0.002 (0.003)\tLoss 1.0492 (1.0146)\tPrec@1 57.812 (63.707)\n",
      "Epoch: [168][312/390]\tTime 0.002 (0.003)\tLoss 1.0290 (1.0358)\tPrec@1 60.156 (62.892)\n",
      "Epoch: [168][390/390]\tTime 0.001 (0.003)\tLoss 1.0820 (1.0489)\tPrec@1 63.750 (62.444)\n",
      "EPOCH: 168 train Results: Prec@1 62.444 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.2360 (1.2360)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4459 (1.2775)\tPrec@1 37.500 (55.010)\n",
      "EPOCH: 168 val Results: Prec@1 55.010 Loss: 1.2775\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [169][0/390]\tTime 0.002 (0.002)\tLoss 0.9207 (0.9207)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [169][78/390]\tTime 0.006 (0.004)\tLoss 1.0043 (0.9770)\tPrec@1 62.500 (65.793)\n",
      "Epoch: [169][156/390]\tTime 0.005 (0.004)\tLoss 0.9166 (1.0012)\tPrec@1 66.406 (64.530)\n",
      "Epoch: [169][234/390]\tTime 0.002 (0.003)\tLoss 0.9935 (1.0273)\tPrec@1 67.188 (63.481)\n",
      "Epoch: [169][312/390]\tTime 0.003 (0.003)\tLoss 1.0345 (1.0435)\tPrec@1 62.500 (62.879)\n",
      "Epoch: [169][390/390]\tTime 0.001 (0.003)\tLoss 1.0339 (1.0570)\tPrec@1 58.750 (62.330)\n",
      "EPOCH: 169 train Results: Prec@1 62.330 Loss: 1.0570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2630 (1.2630)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1171 (1.2872)\tPrec@1 43.750 (54.680)\n",
      "EPOCH: 169 val Results: Prec@1 54.680 Loss: 1.2872\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [170][0/390]\tTime 0.003 (0.003)\tLoss 0.9438 (0.9438)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [170][78/390]\tTime 0.002 (0.003)\tLoss 1.1096 (0.9542)\tPrec@1 61.719 (66.377)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.003)\tLoss 0.9324 (0.9865)\tPrec@1 67.969 (64.809)\n",
      "Epoch: [170][234/390]\tTime 0.005 (0.003)\tLoss 1.1591 (1.0117)\tPrec@1 63.281 (63.770)\n",
      "Epoch: [170][312/390]\tTime 0.002 (0.003)\tLoss 1.1080 (1.0352)\tPrec@1 55.469 (62.844)\n",
      "Epoch: [170][390/390]\tTime 0.001 (0.003)\tLoss 0.9915 (1.0476)\tPrec@1 63.750 (62.454)\n",
      "EPOCH: 170 train Results: Prec@1 62.454 Loss: 1.0476\n",
      "Test: [0/78]\tTime 0.015 (0.015)\tLoss 1.1559 (1.1559)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0393 (1.2681)\tPrec@1 43.750 (55.490)\n",
      "EPOCH: 170 val Results: Prec@1 55.490 Loss: 1.2681\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [171][0/390]\tTime 0.002 (0.002)\tLoss 0.8451 (0.8451)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [171][78/390]\tTime 0.002 (0.003)\tLoss 1.0146 (0.9829)\tPrec@1 66.406 (65.091)\n",
      "Epoch: [171][156/390]\tTime 0.002 (0.003)\tLoss 1.1900 (1.0102)\tPrec@1 60.938 (63.983)\n",
      "Epoch: [171][234/390]\tTime 0.003 (0.003)\tLoss 0.8682 (1.0235)\tPrec@1 68.750 (63.624)\n",
      "Epoch: [171][312/390]\tTime 0.004 (0.003)\tLoss 0.9438 (1.0369)\tPrec@1 70.312 (63.191)\n",
      "Epoch: [171][390/390]\tTime 0.002 (0.003)\tLoss 1.3729 (1.0525)\tPrec@1 57.500 (62.562)\n",
      "EPOCH: 171 train Results: Prec@1 62.562 Loss: 1.0525\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2874 (1.2860)\tPrec@1 31.250 (54.550)\n",
      "EPOCH: 171 val Results: Prec@1 54.550 Loss: 1.2860\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [172][0/390]\tTime 0.002 (0.002)\tLoss 0.8504 (0.8504)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [172][78/390]\tTime 0.003 (0.003)\tLoss 0.9169 (0.9549)\tPrec@1 66.406 (66.070)\n",
      "Epoch: [172][156/390]\tTime 0.003 (0.003)\tLoss 1.0534 (0.9905)\tPrec@1 61.719 (64.689)\n",
      "Epoch: [172][234/390]\tTime 0.003 (0.004)\tLoss 0.9967 (1.0206)\tPrec@1 67.188 (63.557)\n",
      "Epoch: [172][312/390]\tTime 0.005 (0.004)\tLoss 1.1219 (1.0378)\tPrec@1 60.938 (62.992)\n",
      "Epoch: [172][390/390]\tTime 0.013 (0.004)\tLoss 1.1309 (1.0498)\tPrec@1 58.750 (62.604)\n",
      "EPOCH: 172 train Results: Prec@1 62.604 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1841 (1.1841)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3541 (1.2742)\tPrec@1 50.000 (55.480)\n",
      "EPOCH: 172 val Results: Prec@1 55.480 Loss: 1.2742\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [173][0/390]\tTime 0.005 (0.005)\tLoss 0.9739 (0.9739)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [173][78/390]\tTime 0.002 (0.003)\tLoss 1.2207 (0.9692)\tPrec@1 56.250 (65.071)\n",
      "Epoch: [173][156/390]\tTime 0.004 (0.003)\tLoss 1.0249 (1.0032)\tPrec@1 63.281 (63.694)\n",
      "Epoch: [173][234/390]\tTime 0.002 (0.003)\tLoss 0.9059 (1.0179)\tPrec@1 67.969 (63.258)\n",
      "Epoch: [173][312/390]\tTime 0.008 (0.003)\tLoss 1.0716 (1.0385)\tPrec@1 64.062 (62.587)\n",
      "Epoch: [173][390/390]\tTime 0.001 (0.003)\tLoss 1.0667 (1.0540)\tPrec@1 56.250 (62.232)\n",
      "EPOCH: 173 train Results: Prec@1 62.232 Loss: 1.0540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2192 (1.2192)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4511 (1.2900)\tPrec@1 31.250 (55.180)\n",
      "EPOCH: 173 val Results: Prec@1 55.180 Loss: 1.2900\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [174][0/390]\tTime 0.002 (0.002)\tLoss 1.0283 (1.0283)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [174][78/390]\tTime 0.002 (0.003)\tLoss 0.8644 (0.9530)\tPrec@1 62.500 (66.139)\n",
      "Epoch: [174][156/390]\tTime 0.003 (0.003)\tLoss 1.0219 (0.9935)\tPrec@1 60.938 (64.630)\n",
      "Epoch: [174][234/390]\tTime 0.004 (0.003)\tLoss 1.0697 (1.0160)\tPrec@1 60.938 (63.767)\n",
      "Epoch: [174][312/390]\tTime 0.002 (0.003)\tLoss 0.9763 (1.0339)\tPrec@1 65.625 (63.112)\n",
      "Epoch: [174][390/390]\tTime 0.003 (0.003)\tLoss 0.9747 (1.0496)\tPrec@1 58.750 (62.514)\n",
      "EPOCH: 174 train Results: Prec@1 62.514 Loss: 1.0496\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1622 (1.1622)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2613 (1.2788)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 174 val Results: Prec@1 55.230 Loss: 1.2788\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [175][0/390]\tTime 0.003 (0.003)\tLoss 0.9966 (0.9966)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [175][78/390]\tTime 0.004 (0.005)\tLoss 1.1348 (0.9680)\tPrec@1 59.375 (65.655)\n",
      "Epoch: [175][156/390]\tTime 0.003 (0.004)\tLoss 0.9629 (0.9917)\tPrec@1 67.969 (64.739)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.004)\tLoss 1.1205 (1.0144)\tPrec@1 58.594 (63.963)\n",
      "Epoch: [175][312/390]\tTime 0.002 (0.004)\tLoss 1.1321 (1.0337)\tPrec@1 59.375 (63.274)\n",
      "Epoch: [175][390/390]\tTime 0.004 (0.003)\tLoss 1.2169 (1.0486)\tPrec@1 48.750 (62.672)\n",
      "EPOCH: 175 train Results: Prec@1 62.672 Loss: 1.0486\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2115 (1.2115)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4381 (1.2762)\tPrec@1 43.750 (54.730)\n",
      "EPOCH: 175 val Results: Prec@1 54.730 Loss: 1.2762\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [176][0/390]\tTime 0.003 (0.003)\tLoss 1.0673 (1.0673)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [176][78/390]\tTime 0.003 (0.003)\tLoss 1.1339 (0.9829)\tPrec@1 62.500 (65.378)\n",
      "Epoch: [176][156/390]\tTime 0.002 (0.004)\tLoss 1.0696 (1.0050)\tPrec@1 53.125 (64.097)\n",
      "Epoch: [176][234/390]\tTime 0.004 (0.004)\tLoss 1.2216 (1.0265)\tPrec@1 58.594 (63.418)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.004)\tLoss 1.0274 (1.0392)\tPrec@1 63.281 (62.917)\n",
      "Epoch: [176][390/390]\tTime 0.005 (0.004)\tLoss 0.9526 (1.0504)\tPrec@1 68.750 (62.472)\n",
      "EPOCH: 176 train Results: Prec@1 62.472 Loss: 1.0504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1260 (1.1260)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3751 (1.2726)\tPrec@1 43.750 (55.400)\n",
      "EPOCH: 176 val Results: Prec@1 55.400 Loss: 1.2726\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [177][0/390]\tTime 0.004 (0.004)\tLoss 1.0030 (1.0030)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [177][78/390]\tTime 0.002 (0.004)\tLoss 0.9079 (0.9681)\tPrec@1 67.969 (66.129)\n",
      "Epoch: [177][156/390]\tTime 0.002 (0.003)\tLoss 1.1086 (1.0067)\tPrec@1 59.375 (64.107)\n",
      "Epoch: [177][234/390]\tTime 0.003 (0.004)\tLoss 1.0480 (1.0223)\tPrec@1 58.594 (63.434)\n",
      "Epoch: [177][312/390]\tTime 0.019 (0.004)\tLoss 1.0877 (1.0398)\tPrec@1 57.031 (63.007)\n",
      "Epoch: [177][390/390]\tTime 0.003 (0.004)\tLoss 1.1559 (1.0493)\tPrec@1 61.250 (62.588)\n",
      "EPOCH: 177 train Results: Prec@1 62.588 Loss: 1.0493\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2586 (1.2586)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4494 (1.2935)\tPrec@1 31.250 (54.920)\n",
      "EPOCH: 177 val Results: Prec@1 54.920 Loss: 1.2935\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [178][0/390]\tTime 0.002 (0.002)\tLoss 1.0602 (1.0602)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [178][78/390]\tTime 0.003 (0.003)\tLoss 1.0035 (0.9746)\tPrec@1 68.750 (65.200)\n",
      "Epoch: [178][156/390]\tTime 0.002 (0.003)\tLoss 1.0493 (1.0026)\tPrec@1 66.406 (64.257)\n",
      "Epoch: [178][234/390]\tTime 0.004 (0.003)\tLoss 1.1247 (1.0237)\tPrec@1 60.938 (63.408)\n",
      "Epoch: [178][312/390]\tTime 0.002 (0.003)\tLoss 1.0997 (1.0382)\tPrec@1 58.594 (62.947)\n",
      "Epoch: [178][390/390]\tTime 0.001 (0.003)\tLoss 1.0498 (1.0507)\tPrec@1 62.500 (62.548)\n",
      "EPOCH: 178 train Results: Prec@1 62.548 Loss: 1.0507\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1768 (1.1768)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2206 (1.2954)\tPrec@1 37.500 (54.900)\n",
      "EPOCH: 178 val Results: Prec@1 54.900 Loss: 1.2954\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [179][0/390]\tTime 0.002 (0.002)\tLoss 1.0416 (1.0416)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [179][78/390]\tTime 0.003 (0.003)\tLoss 1.0088 (0.9530)\tPrec@1 69.531 (66.159)\n",
      "Epoch: [179][156/390]\tTime 0.024 (0.003)\tLoss 1.0910 (0.9951)\tPrec@1 67.969 (64.476)\n",
      "Epoch: [179][234/390]\tTime 0.002 (0.003)\tLoss 1.2298 (1.0261)\tPrec@1 57.031 (63.341)\n",
      "Epoch: [179][312/390]\tTime 0.005 (0.003)\tLoss 1.0640 (1.0413)\tPrec@1 60.938 (62.785)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.3608 (1.0546)\tPrec@1 52.500 (62.344)\n",
      "EPOCH: 179 train Results: Prec@1 62.344 Loss: 1.0546\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0921 (1.0921)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1258 (1.2954)\tPrec@1 50.000 (54.430)\n",
      "EPOCH: 179 val Results: Prec@1 54.430 Loss: 1.2954\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [180][0/390]\tTime 0.010 (0.010)\tLoss 0.9902 (0.9902)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [180][78/390]\tTime 0.003 (0.003)\tLoss 1.0207 (0.9731)\tPrec@1 62.500 (65.309)\n",
      "Epoch: [180][156/390]\tTime 0.004 (0.003)\tLoss 1.1116 (0.9972)\tPrec@1 66.406 (64.262)\n",
      "Epoch: [180][234/390]\tTime 0.003 (0.003)\tLoss 1.0380 (1.0253)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [180][312/390]\tTime 0.005 (0.003)\tLoss 1.0292 (1.0434)\tPrec@1 67.188 (62.597)\n",
      "Epoch: [180][390/390]\tTime 0.005 (0.003)\tLoss 1.2511 (1.0521)\tPrec@1 51.250 (62.300)\n",
      "EPOCH: 180 train Results: Prec@1 62.300 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1538 (1.1538)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2570 (1.2942)\tPrec@1 43.750 (54.490)\n",
      "EPOCH: 180 val Results: Prec@1 54.490 Loss: 1.2942\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [181][0/390]\tTime 0.012 (0.012)\tLoss 1.0288 (1.0288)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [181][78/390]\tTime 0.003 (0.004)\tLoss 0.8773 (0.9618)\tPrec@1 66.406 (65.220)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 0.9303 (0.9909)\tPrec@1 67.188 (64.276)\n",
      "Epoch: [181][234/390]\tTime 0.002 (0.003)\tLoss 1.0420 (1.0204)\tPrec@1 63.281 (63.331)\n",
      "Epoch: [181][312/390]\tTime 0.002 (0.003)\tLoss 1.0994 (1.0383)\tPrec@1 63.281 (62.660)\n",
      "Epoch: [181][390/390]\tTime 0.001 (0.003)\tLoss 1.2904 (1.0500)\tPrec@1 50.000 (62.268)\n",
      "EPOCH: 181 train Results: Prec@1 62.268 Loss: 1.0500\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1270 (1.1270)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9782 (1.2892)\tPrec@1 56.250 (55.120)\n",
      "EPOCH: 181 val Results: Prec@1 55.120 Loss: 1.2892\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [182][0/390]\tTime 0.002 (0.002)\tLoss 1.0553 (1.0553)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [182][78/390]\tTime 0.003 (0.003)\tLoss 1.0987 (0.9636)\tPrec@1 58.594 (65.417)\n",
      "Epoch: [182][156/390]\tTime 0.002 (0.003)\tLoss 0.9091 (0.9967)\tPrec@1 73.438 (64.172)\n",
      "Epoch: [182][234/390]\tTime 0.004 (0.004)\tLoss 1.2803 (1.0257)\tPrec@1 57.031 (63.175)\n",
      "Epoch: [182][312/390]\tTime 0.009 (0.003)\tLoss 1.0573 (1.0405)\tPrec@1 65.625 (62.710)\n",
      "Epoch: [182][390/390]\tTime 0.004 (0.003)\tLoss 1.2152 (1.0498)\tPrec@1 56.250 (62.414)\n",
      "EPOCH: 182 train Results: Prec@1 62.414 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2434 (1.2906)\tPrec@1 56.250 (54.920)\n",
      "EPOCH: 182 val Results: Prec@1 54.920 Loss: 1.2906\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [183][0/390]\tTime 0.003 (0.003)\tLoss 0.8174 (0.8174)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [183][78/390]\tTime 0.002 (0.003)\tLoss 0.9482 (0.9501)\tPrec@1 66.406 (66.485)\n",
      "Epoch: [183][156/390]\tTime 0.003 (0.003)\tLoss 0.9455 (0.9917)\tPrec@1 70.312 (64.764)\n",
      "Epoch: [183][234/390]\tTime 0.002 (0.003)\tLoss 1.0783 (1.0210)\tPrec@1 65.625 (63.541)\n",
      "Epoch: [183][312/390]\tTime 0.006 (0.004)\tLoss 1.0477 (1.0408)\tPrec@1 64.844 (62.877)\n",
      "Epoch: [183][390/390]\tTime 0.002 (0.004)\tLoss 1.2305 (1.0545)\tPrec@1 58.750 (62.358)\n",
      "EPOCH: 183 train Results: Prec@1 62.358 Loss: 1.0545\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1769 (1.1769)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6685 (1.2790)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 183 val Results: Prec@1 55.150 Loss: 1.2790\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [184][0/390]\tTime 0.003 (0.003)\tLoss 0.8851 (0.8851)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [184][78/390]\tTime 0.004 (0.005)\tLoss 1.0891 (0.9615)\tPrec@1 60.938 (66.040)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.004)\tLoss 1.1144 (0.9929)\tPrec@1 63.281 (64.774)\n",
      "Epoch: [184][234/390]\tTime 0.014 (0.004)\tLoss 1.2643 (1.0205)\tPrec@1 57.031 (63.634)\n",
      "Epoch: [184][312/390]\tTime 0.002 (0.004)\tLoss 1.0575 (1.0384)\tPrec@1 58.594 (62.869)\n",
      "Epoch: [184][390/390]\tTime 0.003 (0.004)\tLoss 1.3520 (1.0516)\tPrec@1 53.750 (62.402)\n",
      "EPOCH: 184 train Results: Prec@1 62.402 Loss: 1.0516\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0542 (1.0542)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2534 (1.2766)\tPrec@1 50.000 (55.770)\n",
      "EPOCH: 184 val Results: Prec@1 55.770 Loss: 1.2766\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [185][0/390]\tTime 0.002 (0.002)\tLoss 1.0624 (1.0624)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [185][78/390]\tTime 0.005 (0.003)\tLoss 1.1761 (0.9844)\tPrec@1 51.562 (64.695)\n",
      "Epoch: [185][156/390]\tTime 0.002 (0.003)\tLoss 0.9582 (1.0000)\tPrec@1 64.062 (64.048)\n",
      "Epoch: [185][234/390]\tTime 0.003 (0.003)\tLoss 1.1197 (1.0152)\tPrec@1 60.156 (63.693)\n",
      "Epoch: [185][312/390]\tTime 0.002 (0.003)\tLoss 1.0257 (1.0292)\tPrec@1 62.500 (63.161)\n",
      "Epoch: [185][390/390]\tTime 0.001 (0.003)\tLoss 1.1298 (1.0486)\tPrec@1 56.250 (62.508)\n",
      "EPOCH: 185 train Results: Prec@1 62.508 Loss: 1.0486\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0972 (1.0972)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.4375 (1.2890)\tPrec@1 31.250 (54.760)\n",
      "EPOCH: 185 val Results: Prec@1 54.760 Loss: 1.2890\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [186][0/390]\tTime 0.005 (0.005)\tLoss 0.9534 (0.9534)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [186][78/390]\tTime 0.003 (0.003)\tLoss 1.0136 (0.9700)\tPrec@1 60.938 (65.655)\n",
      "Epoch: [186][156/390]\tTime 0.085 (0.004)\tLoss 1.0166 (0.9999)\tPrec@1 64.844 (64.485)\n",
      "Epoch: [186][234/390]\tTime 0.017 (0.004)\tLoss 1.1401 (1.0243)\tPrec@1 56.250 (63.551)\n",
      "Epoch: [186][312/390]\tTime 0.002 (0.004)\tLoss 1.0591 (1.0443)\tPrec@1 64.844 (62.899)\n",
      "Epoch: [186][390/390]\tTime 0.008 (0.004)\tLoss 0.9597 (1.0526)\tPrec@1 66.250 (62.542)\n",
      "EPOCH: 186 train Results: Prec@1 62.542 Loss: 1.0526\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1185 (1.1185)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4281 (1.2962)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 186 val Results: Prec@1 54.630 Loss: 1.2962\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [187][0/390]\tTime 0.002 (0.002)\tLoss 1.0670 (1.0670)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [187][78/390]\tTime 0.004 (0.004)\tLoss 0.8741 (0.9634)\tPrec@1 70.312 (65.348)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.2432 (0.9884)\tPrec@1 53.125 (64.565)\n",
      "Epoch: [187][234/390]\tTime 0.002 (0.003)\tLoss 1.0166 (1.0142)\tPrec@1 67.188 (63.720)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.0702 (1.0342)\tPrec@1 66.406 (63.094)\n",
      "Epoch: [187][390/390]\tTime 0.001 (0.003)\tLoss 1.1837 (1.0484)\tPrec@1 57.500 (62.588)\n",
      "EPOCH: 187 train Results: Prec@1 62.588 Loss: 1.0484\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1446 (1.1446)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6352 (1.2930)\tPrec@1 43.750 (54.760)\n",
      "EPOCH: 187 val Results: Prec@1 54.760 Loss: 1.2930\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [188][0/390]\tTime 0.002 (0.002)\tLoss 0.8107 (0.8107)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [188][78/390]\tTime 0.002 (0.003)\tLoss 1.1783 (0.9644)\tPrec@1 62.500 (66.208)\n",
      "Epoch: [188][156/390]\tTime 0.007 (0.003)\tLoss 1.0560 (0.9882)\tPrec@1 54.688 (65.088)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.003)\tLoss 1.3150 (1.0127)\tPrec@1 50.781 (64.116)\n",
      "Epoch: [188][312/390]\tTime 0.002 (0.003)\tLoss 1.3795 (1.0313)\tPrec@1 60.156 (63.463)\n",
      "Epoch: [188][390/390]\tTime 0.002 (0.003)\tLoss 1.3469 (1.0489)\tPrec@1 48.750 (62.620)\n",
      "EPOCH: 188 train Results: Prec@1 62.620 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1901 (1.1901)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2590 (1.2907)\tPrec@1 37.500 (54.820)\n",
      "EPOCH: 188 val Results: Prec@1 54.820 Loss: 1.2907\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [189][0/390]\tTime 0.006 (0.006)\tLoss 0.9439 (0.9439)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.002)\tLoss 0.9524 (0.9596)\tPrec@1 68.750 (65.595)\n",
      "Epoch: [189][156/390]\tTime 0.002 (0.003)\tLoss 1.2134 (0.9966)\tPrec@1 57.812 (64.356)\n",
      "Epoch: [189][234/390]\tTime 0.006 (0.003)\tLoss 1.2292 (1.0164)\tPrec@1 53.906 (63.504)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.003)\tLoss 1.1375 (1.0358)\tPrec@1 58.594 (62.635)\n",
      "Epoch: [189][390/390]\tTime 0.002 (0.003)\tLoss 1.3582 (1.0506)\tPrec@1 48.750 (62.200)\n",
      "EPOCH: 189 train Results: Prec@1 62.200 Loss: 1.0506\n",
      "Test: [0/78]\tTime 0.013 (0.013)\tLoss 1.1495 (1.1495)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4607 (1.2835)\tPrec@1 43.750 (54.460)\n",
      "EPOCH: 189 val Results: Prec@1 54.460 Loss: 1.2835\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [190][0/390]\tTime 0.005 (0.005)\tLoss 0.9696 (0.9696)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.003)\tLoss 1.0165 (0.9770)\tPrec@1 63.281 (65.022)\n",
      "Epoch: [190][156/390]\tTime 0.010 (0.003)\tLoss 0.9380 (1.0007)\tPrec@1 70.312 (64.067)\n",
      "Epoch: [190][234/390]\tTime 0.002 (0.003)\tLoss 1.0972 (1.0272)\tPrec@1 60.156 (62.975)\n",
      "Epoch: [190][312/390]\tTime 0.003 (0.004)\tLoss 0.9632 (1.0446)\tPrec@1 64.844 (62.455)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.003)\tLoss 1.1989 (1.0577)\tPrec@1 60.000 (62.094)\n",
      "EPOCH: 190 train Results: Prec@1 62.094 Loss: 1.0577\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1271 (1.1271)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4742 (1.2877)\tPrec@1 50.000 (54.700)\n",
      "EPOCH: 190 val Results: Prec@1 54.700 Loss: 1.2877\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [191][0/390]\tTime 0.004 (0.004)\tLoss 0.8493 (0.8493)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.003)\tLoss 0.8819 (0.9654)\tPrec@1 75.000 (65.912)\n",
      "Epoch: [191][156/390]\tTime 0.005 (0.004)\tLoss 1.0677 (1.0006)\tPrec@1 61.719 (64.192)\n",
      "Epoch: [191][234/390]\tTime 0.004 (0.004)\tLoss 0.9720 (1.0255)\tPrec@1 67.969 (63.354)\n",
      "Epoch: [191][312/390]\tTime 0.004 (0.004)\tLoss 1.0280 (1.0404)\tPrec@1 60.156 (62.829)\n",
      "Epoch: [191][390/390]\tTime 0.003 (0.004)\tLoss 1.2119 (1.0548)\tPrec@1 55.000 (62.282)\n",
      "EPOCH: 191 train Results: Prec@1 62.282 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1684 (1.1684)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4545 (1.2672)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 191 val Results: Prec@1 55.610 Loss: 1.2672\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [192][0/390]\tTime 0.003 (0.003)\tLoss 0.8439 (0.8439)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [192][78/390]\tTime 0.002 (0.004)\tLoss 0.9842 (0.9679)\tPrec@1 63.281 (66.090)\n",
      "Epoch: [192][156/390]\tTime 0.002 (0.004)\tLoss 1.1003 (0.9996)\tPrec@1 64.844 (64.570)\n",
      "Epoch: [192][234/390]\tTime 0.002 (0.004)\tLoss 0.9874 (1.0194)\tPrec@1 59.375 (63.886)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.003)\tLoss 1.1154 (1.0343)\tPrec@1 52.344 (63.384)\n",
      "Epoch: [192][390/390]\tTime 0.010 (0.004)\tLoss 1.0439 (1.0487)\tPrec@1 60.000 (62.846)\n",
      "EPOCH: 192 train Results: Prec@1 62.846 Loss: 1.0487\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1298 (1.1298)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1542 (1.2683)\tPrec@1 56.250 (55.670)\n",
      "EPOCH: 192 val Results: Prec@1 55.670 Loss: 1.2683\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [193][0/390]\tTime 0.003 (0.003)\tLoss 0.8510 (0.8510)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [193][78/390]\tTime 0.006 (0.003)\tLoss 0.9306 (0.9661)\tPrec@1 66.406 (66.228)\n",
      "Epoch: [193][156/390]\tTime 0.015 (0.003)\tLoss 1.0519 (0.9997)\tPrec@1 59.375 (64.471)\n",
      "Epoch: [193][234/390]\tTime 0.002 (0.003)\tLoss 1.0842 (1.0181)\tPrec@1 66.406 (63.873)\n",
      "Epoch: [193][312/390]\tTime 0.003 (0.004)\tLoss 1.1010 (1.0358)\tPrec@1 62.500 (63.304)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.004)\tLoss 1.0003 (1.0465)\tPrec@1 63.750 (62.832)\n",
      "EPOCH: 193 train Results: Prec@1 62.832 Loss: 1.0465\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1363 (1.1363)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3392 (1.2797)\tPrec@1 31.250 (54.730)\n",
      "EPOCH: 193 val Results: Prec@1 54.730 Loss: 1.2797\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [194][0/390]\tTime 0.004 (0.004)\tLoss 0.8793 (0.8793)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [194][78/390]\tTime 0.002 (0.004)\tLoss 1.0017 (0.9867)\tPrec@1 60.938 (64.577)\n",
      "Epoch: [194][156/390]\tTime 0.002 (0.004)\tLoss 0.9124 (1.0059)\tPrec@1 64.062 (63.938)\n",
      "Epoch: [194][234/390]\tTime 0.007 (0.003)\tLoss 1.1256 (1.0232)\tPrec@1 59.375 (63.211)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.003)\tLoss 1.2009 (1.0378)\tPrec@1 57.812 (62.755)\n",
      "Epoch: [194][390/390]\tTime 0.002 (0.003)\tLoss 1.2495 (1.0536)\tPrec@1 52.500 (62.270)\n",
      "EPOCH: 194 train Results: Prec@1 62.270 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2613 (1.2613)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9177 (1.2736)\tPrec@1 62.500 (55.260)\n",
      "EPOCH: 194 val Results: Prec@1 55.260 Loss: 1.2736\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [195][0/390]\tTime 0.005 (0.005)\tLoss 0.7983 (0.7983)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [195][78/390]\tTime 0.002 (0.003)\tLoss 0.9430 (0.9736)\tPrec@1 68.750 (65.388)\n",
      "Epoch: [195][156/390]\tTime 0.003 (0.003)\tLoss 1.0029 (0.9975)\tPrec@1 61.719 (64.097)\n",
      "Epoch: [195][234/390]\tTime 0.002 (0.003)\tLoss 1.0986 (1.0180)\tPrec@1 58.594 (63.584)\n",
      "Epoch: [195][312/390]\tTime 0.002 (0.003)\tLoss 1.2609 (1.0374)\tPrec@1 52.344 (62.944)\n",
      "Epoch: [195][390/390]\tTime 0.002 (0.003)\tLoss 1.1798 (1.0475)\tPrec@1 57.500 (62.514)\n",
      "EPOCH: 195 train Results: Prec@1 62.514 Loss: 1.0475\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1814 (1.1814)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3546 (1.2995)\tPrec@1 56.250 (54.630)\n",
      "EPOCH: 195 val Results: Prec@1 54.630 Loss: 1.2995\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [196][0/390]\tTime 0.004 (0.004)\tLoss 0.8933 (0.8933)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 1.0879 (0.9626)\tPrec@1 65.625 (65.704)\n",
      "Epoch: [196][156/390]\tTime 0.002 (0.003)\tLoss 1.0811 (0.9926)\tPrec@1 59.375 (64.441)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.004)\tLoss 1.0134 (1.0256)\tPrec@1 67.969 (63.191)\n",
      "Epoch: [196][312/390]\tTime 0.003 (0.003)\tLoss 1.1686 (1.0396)\tPrec@1 57.812 (62.710)\n",
      "Epoch: [196][390/390]\tTime 0.002 (0.003)\tLoss 0.9945 (1.0508)\tPrec@1 63.750 (62.288)\n",
      "EPOCH: 196 train Results: Prec@1 62.288 Loss: 1.0508\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2030 (1.2030)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.1823 (1.2815)\tPrec@1 50.000 (55.060)\n",
      "EPOCH: 196 val Results: Prec@1 55.060 Loss: 1.2815\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [197][0/390]\tTime 0.005 (0.005)\tLoss 0.8775 (0.8775)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [197][78/390]\tTime 0.003 (0.004)\tLoss 1.0880 (0.9498)\tPrec@1 65.625 (66.297)\n",
      "Epoch: [197][156/390]\tTime 0.009 (0.004)\tLoss 1.0555 (0.9869)\tPrec@1 64.844 (64.874)\n",
      "Epoch: [197][234/390]\tTime 0.003 (0.004)\tLoss 0.9627 (1.0194)\tPrec@1 67.188 (63.697)\n",
      "Epoch: [197][312/390]\tTime 0.006 (0.004)\tLoss 0.9931 (1.0408)\tPrec@1 63.281 (62.805)\n",
      "Epoch: [197][390/390]\tTime 0.006 (0.004)\tLoss 0.8275 (1.0544)\tPrec@1 80.000 (62.412)\n",
      "EPOCH: 197 train Results: Prec@1 62.412 Loss: 1.0544\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1605 (1.1605)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5509 (1.2846)\tPrec@1 56.250 (54.890)\n",
      "EPOCH: 197 val Results: Prec@1 54.890 Loss: 1.2846\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [198][0/390]\tTime 0.003 (0.003)\tLoss 1.0150 (1.0150)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [198][78/390]\tTime 0.002 (0.004)\tLoss 1.0224 (0.9739)\tPrec@1 65.625 (65.180)\n",
      "Epoch: [198][156/390]\tTime 0.006 (0.004)\tLoss 1.0505 (0.9959)\tPrec@1 60.156 (64.476)\n",
      "Epoch: [198][234/390]\tTime 0.006 (0.004)\tLoss 1.2706 (1.0219)\tPrec@1 53.906 (63.418)\n",
      "Epoch: [198][312/390]\tTime 0.004 (0.003)\tLoss 1.0281 (1.0372)\tPrec@1 62.500 (62.814)\n",
      "Epoch: [198][390/390]\tTime 0.003 (0.004)\tLoss 1.0056 (1.0512)\tPrec@1 58.750 (62.348)\n",
      "EPOCH: 198 train Results: Prec@1 62.348 Loss: 1.0512\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1018 (1.1018)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4544 (1.2988)\tPrec@1 50.000 (54.130)\n",
      "EPOCH: 198 val Results: Prec@1 54.130 Loss: 1.2988\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [199][0/390]\tTime 0.002 (0.002)\tLoss 1.1413 (1.1413)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [199][78/390]\tTime 0.005 (0.003)\tLoss 1.0338 (0.9565)\tPrec@1 65.625 (66.208)\n",
      "Epoch: [199][156/390]\tTime 0.003 (0.003)\tLoss 1.0599 (0.9834)\tPrec@1 60.156 (65.028)\n",
      "Epoch: [199][234/390]\tTime 0.002 (0.003)\tLoss 0.9818 (1.0138)\tPrec@1 68.750 (63.866)\n",
      "Epoch: [199][312/390]\tTime 0.003 (0.003)\tLoss 1.1839 (1.0316)\tPrec@1 53.125 (62.917)\n",
      "Epoch: [199][390/390]\tTime 0.003 (0.003)\tLoss 1.1163 (1.0477)\tPrec@1 57.500 (62.402)\n",
      "EPOCH: 199 train Results: Prec@1 62.402 Loss: 1.0477\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1605 (1.1605)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1917 (1.2977)\tPrec@1 50.000 (54.400)\n",
      "EPOCH: 199 val Results: Prec@1 54.400 Loss: 1.2977\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [200][0/390]\tTime 0.004 (0.004)\tLoss 0.9240 (0.9240)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [200][78/390]\tTime 0.003 (0.003)\tLoss 1.0649 (0.9824)\tPrec@1 57.812 (65.002)\n",
      "Epoch: [200][156/390]\tTime 0.002 (0.003)\tLoss 0.7854 (1.0077)\tPrec@1 71.875 (64.092)\n",
      "Epoch: [200][234/390]\tTime 0.004 (0.003)\tLoss 1.0138 (1.0293)\tPrec@1 65.625 (63.338)\n",
      "Epoch: [200][312/390]\tTime 0.002 (0.003)\tLoss 1.2991 (1.0392)\tPrec@1 53.125 (63.039)\n",
      "Epoch: [200][390/390]\tTime 0.003 (0.003)\tLoss 1.2498 (1.0537)\tPrec@1 53.750 (62.384)\n",
      "EPOCH: 200 train Results: Prec@1 62.384 Loss: 1.0537\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.0851 (1.0851)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1298 (1.2711)\tPrec@1 56.250 (55.420)\n",
      "EPOCH: 200 val Results: Prec@1 55.420 Loss: 1.2711\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "End time:  Thu Apr  4 23:13:39 2024\n",
      "train executed in 281.4510 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.05, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer5 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:13:40 2024\n",
      "current lr 5.00000e-04\n",
      "Epoch: [1][0/390]\tTime 0.006 (0.006)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.003 (0.004)\tLoss 4.8175 (5.1444)\tPrec@1 11.719 (10.987)\n",
      "Epoch: [1][156/390]\tTime 0.002 (0.003)\tLoss 4.2685 (4.6881)\tPrec@1 11.719 (12.371)\n",
      "Epoch: [1][234/390]\tTime 0.002 (0.003)\tLoss 3.3635 (4.3636)\tPrec@1 19.531 (13.461)\n",
      "Epoch: [1][312/390]\tTime 0.002 (0.003)\tLoss 3.3746 (4.1154)\tPrec@1 19.531 (14.569)\n",
      "Epoch: [1][390/390]\tTime 0.001 (0.003)\tLoss 2.7901 (3.9102)\tPrec@1 21.250 (15.806)\n",
      "EPOCH: 1 train Results: Prec@1 15.806 Loss: 3.9102\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 2.6787 (2.6787)\tPrec@1 20.312 (20.312)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 2.6365 (2.5733)\tPrec@1 25.000 (23.550)\n",
      "EPOCH: 1 val Results: Prec@1 23.550 Loss: 2.5733\n",
      "Best Prec@1: 23.550\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [2][0/390]\tTime 0.004 (0.004)\tLoss 2.9218 (2.9218)\tPrec@1 19.531 (19.531)\n",
      "Epoch: [2][78/390]\tTime 0.004 (0.003)\tLoss 2.4449 (2.8344)\tPrec@1 23.438 (21.727)\n",
      "Epoch: [2][156/390]\tTime 0.008 (0.003)\tLoss 2.6524 (2.7534)\tPrec@1 19.531 (22.930)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.003)\tLoss 2.8675 (2.6834)\tPrec@1 17.969 (23.594)\n",
      "Epoch: [2][312/390]\tTime 0.009 (0.003)\tLoss 2.6599 (2.6218)\tPrec@1 21.875 (24.321)\n",
      "Epoch: [2][390/390]\tTime 0.007 (0.003)\tLoss 2.2045 (2.5664)\tPrec@1 25.000 (24.878)\n",
      "EPOCH: 2 train Results: Prec@1 24.878 Loss: 2.5664\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 2.1176 (2.1176)\tPrec@1 30.469 (30.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.9654 (2.0576)\tPrec@1 18.750 (31.040)\n",
      "EPOCH: 2 val Results: Prec@1 31.040 Loss: 2.0576\n",
      "Best Prec@1: 31.040\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [3][0/390]\tTime 0.002 (0.002)\tLoss 2.1558 (2.1558)\tPrec@1 28.125 (28.125)\n",
      "Epoch: [3][78/390]\tTime 0.002 (0.003)\tLoss 2.1571 (2.2477)\tPrec@1 32.031 (28.708)\n",
      "Epoch: [3][156/390]\tTime 0.003 (0.004)\tLoss 2.0899 (2.2072)\tPrec@1 25.781 (29.265)\n",
      "Epoch: [3][234/390]\tTime 0.008 (0.003)\tLoss 2.0944 (2.1789)\tPrec@1 27.344 (29.644)\n",
      "Epoch: [3][312/390]\tTime 0.003 (0.004)\tLoss 2.0893 (2.1553)\tPrec@1 30.469 (30.034)\n",
      "Epoch: [3][390/390]\tTime 0.001 (0.003)\tLoss 1.8981 (2.1285)\tPrec@1 30.000 (30.380)\n",
      "EPOCH: 3 train Results: Prec@1 30.380 Loss: 2.1285\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.8987 (1.8987)\tPrec@1 32.031 (32.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7110 (1.8509)\tPrec@1 18.750 (35.400)\n",
      "EPOCH: 3 val Results: Prec@1 35.400 Loss: 1.8509\n",
      "Best Prec@1: 35.400\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [4][0/390]\tTime 0.002 (0.002)\tLoss 2.0733 (2.0733)\tPrec@1 21.875 (21.875)\n",
      "Epoch: [4][78/390]\tTime 0.003 (0.003)\tLoss 2.0473 (1.9675)\tPrec@1 35.156 (32.605)\n",
      "Epoch: [4][156/390]\tTime 0.002 (0.003)\tLoss 1.9737 (1.9443)\tPrec@1 34.375 (33.181)\n",
      "Epoch: [4][234/390]\tTime 0.003 (0.003)\tLoss 2.0308 (1.9294)\tPrec@1 28.125 (33.358)\n",
      "Epoch: [4][312/390]\tTime 0.007 (0.003)\tLoss 1.7379 (1.9162)\tPrec@1 37.500 (33.681)\n",
      "Epoch: [4][390/390]\tTime 0.005 (0.003)\tLoss 1.7338 (1.9012)\tPrec@1 35.000 (34.008)\n",
      "EPOCH: 4 train Results: Prec@1 34.008 Loss: 1.9012\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.7558 (1.7558)\tPrec@1 35.156 (35.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6075 (1.7430)\tPrec@1 31.250 (38.480)\n",
      "EPOCH: 4 val Results: Prec@1 38.480 Loss: 1.7430\n",
      "Best Prec@1: 38.480\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [5][0/390]\tTime 0.004 (0.004)\tLoss 1.9966 (1.9966)\tPrec@1 25.781 (25.781)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.003)\tLoss 1.9753 (1.7861)\tPrec@1 36.719 (36.689)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.003)\tLoss 1.8870 (1.7971)\tPrec@1 34.375 (36.385)\n",
      "Epoch: [5][234/390]\tTime 0.004 (0.003)\tLoss 1.7252 (1.7840)\tPrec@1 36.719 (36.905)\n",
      "Epoch: [5][312/390]\tTime 0.003 (0.003)\tLoss 1.6975 (1.7773)\tPrec@1 34.375 (37.245)\n",
      "Epoch: [5][390/390]\tTime 0.001 (0.003)\tLoss 1.7003 (1.7683)\tPrec@1 42.500 (37.586)\n",
      "EPOCH: 5 train Results: Prec@1 37.586 Loss: 1.7683\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.6671 (1.6671)\tPrec@1 38.281 (38.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5541 (1.6777)\tPrec@1 31.250 (40.820)\n",
      "EPOCH: 5 val Results: Prec@1 40.820 Loss: 1.6777\n",
      "Best Prec@1: 40.820\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [6][0/390]\tTime 0.002 (0.002)\tLoss 1.7795 (1.7795)\tPrec@1 40.625 (40.625)\n",
      "Epoch: [6][78/390]\tTime 0.004 (0.003)\tLoss 1.7975 (1.6895)\tPrec@1 35.938 (40.882)\n",
      "Epoch: [6][156/390]\tTime 0.007 (0.003)\tLoss 1.5600 (1.6888)\tPrec@1 40.625 (40.779)\n",
      "Epoch: [6][234/390]\tTime 0.006 (0.003)\tLoss 1.7876 (1.6873)\tPrec@1 37.500 (40.682)\n",
      "Epoch: [6][312/390]\tTime 0.004 (0.003)\tLoss 1.7899 (1.6860)\tPrec@1 35.938 (40.695)\n",
      "Epoch: [6][390/390]\tTime 0.001 (0.004)\tLoss 1.8947 (1.6824)\tPrec@1 27.500 (40.832)\n",
      "EPOCH: 6 train Results: Prec@1 40.832 Loss: 1.6824\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.6071 (1.6071)\tPrec@1 46.875 (46.875)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5382 (1.6356)\tPrec@1 31.250 (42.810)\n",
      "EPOCH: 6 val Results: Prec@1 42.810 Loss: 1.6356\n",
      "Best Prec@1: 42.810\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [7][0/390]\tTime 0.004 (0.004)\tLoss 1.6896 (1.6896)\tPrec@1 39.062 (39.062)\n",
      "Epoch: [7][78/390]\tTime 0.002 (0.004)\tLoss 1.6855 (1.6414)\tPrec@1 35.938 (42.534)\n",
      "Epoch: [7][156/390]\tTime 0.004 (0.004)\tLoss 1.6619 (1.6369)\tPrec@1 43.750 (43.004)\n",
      "Epoch: [7][234/390]\tTime 0.005 (0.004)\tLoss 1.6211 (1.6315)\tPrec@1 43.750 (42.759)\n",
      "Epoch: [7][312/390]\tTime 0.003 (0.004)\tLoss 1.5375 (1.6275)\tPrec@1 48.438 (42.789)\n",
      "Epoch: [7][390/390]\tTime 0.003 (0.004)\tLoss 1.6448 (1.6271)\tPrec@1 51.250 (42.850)\n",
      "EPOCH: 7 train Results: Prec@1 42.850 Loss: 1.6271\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.5640 (1.5640)\tPrec@1 46.875 (46.875)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5290 (1.6051)\tPrec@1 31.250 (43.980)\n",
      "EPOCH: 7 val Results: Prec@1 43.980 Loss: 1.6051\n",
      "Best Prec@1: 43.980\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [8][0/390]\tTime 0.006 (0.006)\tLoss 1.6069 (1.6069)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [8][78/390]\tTime 0.003 (0.003)\tLoss 1.6141 (1.6008)\tPrec@1 44.531 (44.610)\n",
      "Epoch: [8][156/390]\tTime 0.004 (0.004)\tLoss 1.6429 (1.6006)\tPrec@1 36.719 (44.357)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.004)\tLoss 1.5426 (1.5956)\tPrec@1 44.531 (44.561)\n",
      "Epoch: [8][312/390]\tTime 0.009 (0.004)\tLoss 1.4690 (1.5909)\tPrec@1 49.219 (44.893)\n",
      "Epoch: [8][390/390]\tTime 0.004 (0.003)\tLoss 1.7086 (1.5901)\tPrec@1 36.250 (44.962)\n",
      "EPOCH: 8 train Results: Prec@1 44.962 Loss: 1.5901\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.5364 (1.5364)\tPrec@1 46.875 (46.875)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5017 (1.5803)\tPrec@1 31.250 (45.360)\n",
      "EPOCH: 8 val Results: Prec@1 45.360 Loss: 1.5803\n",
      "Best Prec@1: 45.360\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [9][0/390]\tTime 0.003 (0.003)\tLoss 1.5893 (1.5893)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [9][78/390]\tTime 0.011 (0.004)\tLoss 1.5062 (1.5544)\tPrec@1 50.000 (46.855)\n",
      "Epoch: [9][156/390]\tTime 0.002 (0.004)\tLoss 1.7130 (1.5528)\tPrec@1 39.844 (46.835)\n",
      "Epoch: [9][234/390]\tTime 0.003 (0.004)\tLoss 1.6062 (1.5595)\tPrec@1 43.750 (46.366)\n",
      "Epoch: [9][312/390]\tTime 0.002 (0.004)\tLoss 1.5954 (1.5586)\tPrec@1 44.531 (46.413)\n",
      "Epoch: [9][390/390]\tTime 0.006 (0.004)\tLoss 1.7099 (1.5593)\tPrec@1 45.000 (46.332)\n",
      "EPOCH: 9 train Results: Prec@1 46.332 Loss: 1.5593\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.5118 (1.5118)\tPrec@1 51.562 (51.562)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5166 (1.5599)\tPrec@1 18.750 (46.420)\n",
      "EPOCH: 9 val Results: Prec@1 46.420 Loss: 1.5599\n",
      "Best Prec@1: 46.420\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [10][0/390]\tTime 0.005 (0.005)\tLoss 1.5762 (1.5762)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [10][78/390]\tTime 0.004 (0.003)\tLoss 1.5180 (1.5301)\tPrec@1 52.344 (48.042)\n",
      "Epoch: [10][156/390]\tTime 0.004 (0.003)\tLoss 1.5919 (1.5417)\tPrec@1 46.875 (47.169)\n",
      "Epoch: [10][234/390]\tTime 0.002 (0.003)\tLoss 1.5746 (1.5414)\tPrec@1 39.844 (47.071)\n",
      "Epoch: [10][312/390]\tTime 0.003 (0.003)\tLoss 1.5687 (1.5385)\tPrec@1 50.000 (47.267)\n",
      "Epoch: [10][390/390]\tTime 0.003 (0.003)\tLoss 1.3883 (1.5375)\tPrec@1 51.250 (47.384)\n",
      "EPOCH: 10 train Results: Prec@1 47.384 Loss: 1.5375\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4993 (1.4993)\tPrec@1 47.656 (47.656)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5652 (1.5337)\tPrec@1 18.750 (46.920)\n",
      "EPOCH: 10 val Results: Prec@1 46.920 Loss: 1.5337\n",
      "Best Prec@1: 46.920\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [11][0/390]\tTime 0.002 (0.002)\tLoss 1.4812 (1.4812)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [11][78/390]\tTime 0.002 (0.003)\tLoss 1.4978 (1.5062)\tPrec@1 48.438 (48.655)\n",
      "Epoch: [11][156/390]\tTime 0.002 (0.003)\tLoss 1.4570 (1.5085)\tPrec@1 52.344 (48.423)\n",
      "Epoch: [11][234/390]\tTime 0.003 (0.003)\tLoss 1.5654 (1.5117)\tPrec@1 47.656 (48.258)\n",
      "Epoch: [11][312/390]\tTime 0.003 (0.003)\tLoss 1.4224 (1.5097)\tPrec@1 54.688 (48.333)\n",
      "Epoch: [11][390/390]\tTime 0.001 (0.003)\tLoss 1.5814 (1.5098)\tPrec@1 47.500 (48.240)\n",
      "EPOCH: 11 train Results: Prec@1 48.240 Loss: 1.5098\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4702 (1.4702)\tPrec@1 49.219 (49.219)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5406 (1.5104)\tPrec@1 18.750 (47.950)\n",
      "EPOCH: 11 val Results: Prec@1 47.950 Loss: 1.5104\n",
      "Best Prec@1: 47.950\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [12][0/390]\tTime 0.002 (0.002)\tLoss 1.4779 (1.4779)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [12][78/390]\tTime 0.003 (0.003)\tLoss 1.4438 (1.4816)\tPrec@1 49.219 (50.020)\n",
      "Epoch: [12][156/390]\tTime 0.002 (0.003)\tLoss 1.5597 (1.4816)\tPrec@1 42.188 (49.746)\n",
      "Epoch: [12][234/390]\tTime 0.002 (0.003)\tLoss 1.3459 (1.4792)\tPrec@1 55.469 (49.727)\n",
      "Epoch: [12][312/390]\tTime 0.002 (0.003)\tLoss 1.4837 (1.4789)\tPrec@1 52.344 (49.643)\n",
      "Epoch: [12][390/390]\tTime 0.005 (0.003)\tLoss 1.3271 (1.4788)\tPrec@1 56.250 (49.672)\n",
      "EPOCH: 12 train Results: Prec@1 49.672 Loss: 1.4788\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4223 (1.4223)\tPrec@1 51.562 (51.562)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4382 (1.4756)\tPrec@1 37.500 (48.560)\n",
      "EPOCH: 12 val Results: Prec@1 48.560 Loss: 1.4756\n",
      "Best Prec@1: 48.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [13][0/390]\tTime 0.002 (0.002)\tLoss 1.4517 (1.4517)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.4005 (1.4485)\tPrec@1 55.469 (50.455)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.003)\tLoss 1.3852 (1.4433)\tPrec@1 52.344 (50.995)\n",
      "Epoch: [13][234/390]\tTime 0.003 (0.003)\tLoss 1.5293 (1.4463)\tPrec@1 49.219 (50.878)\n",
      "Epoch: [13][312/390]\tTime 0.002 (0.003)\tLoss 1.4393 (1.4484)\tPrec@1 48.438 (50.599)\n",
      "Epoch: [13][390/390]\tTime 0.002 (0.003)\tLoss 1.4006 (1.4477)\tPrec@1 47.500 (50.696)\n",
      "EPOCH: 13 train Results: Prec@1 50.696 Loss: 1.4477\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3893 (1.3893)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5100 (1.4463)\tPrec@1 31.250 (49.610)\n",
      "EPOCH: 13 val Results: Prec@1 49.610 Loss: 1.4463\n",
      "Best Prec@1: 49.610\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [14][0/390]\tTime 0.002 (0.002)\tLoss 1.3367 (1.3367)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [14][78/390]\tTime 0.002 (0.003)\tLoss 1.3904 (1.4139)\tPrec@1 55.469 (51.998)\n",
      "Epoch: [14][156/390]\tTime 0.002 (0.003)\tLoss 1.4554 (1.4135)\tPrec@1 51.562 (51.876)\n",
      "Epoch: [14][234/390]\tTime 0.002 (0.003)\tLoss 1.4012 (1.4177)\tPrec@1 52.344 (51.679)\n",
      "Epoch: [14][312/390]\tTime 0.002 (0.003)\tLoss 1.3118 (1.4171)\tPrec@1 50.781 (51.655)\n",
      "Epoch: [14][390/390]\tTime 0.001 (0.003)\tLoss 1.5682 (1.4174)\tPrec@1 45.000 (51.600)\n",
      "EPOCH: 14 train Results: Prec@1 51.600 Loss: 1.4174\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.3433 (1.3433)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4291 (1.4204)\tPrec@1 37.500 (51.140)\n",
      "EPOCH: 14 val Results: Prec@1 51.140 Loss: 1.4204\n",
      "Best Prec@1: 51.140\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [15][0/390]\tTime 0.009 (0.009)\tLoss 1.3124 (1.3124)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [15][78/390]\tTime 0.002 (0.003)\tLoss 1.4701 (1.3763)\tPrec@1 49.219 (53.204)\n",
      "Epoch: [15][156/390]\tTime 0.002 (0.003)\tLoss 1.2889 (1.3829)\tPrec@1 52.344 (53.110)\n",
      "Epoch: [15][234/390]\tTime 0.003 (0.003)\tLoss 1.3548 (1.3867)\tPrec@1 57.812 (52.590)\n",
      "Epoch: [15][312/390]\tTime 0.010 (0.003)\tLoss 1.5340 (1.3884)\tPrec@1 50.781 (52.441)\n",
      "Epoch: [15][390/390]\tTime 0.001 (0.003)\tLoss 1.3691 (1.3882)\tPrec@1 53.750 (52.478)\n",
      "EPOCH: 15 train Results: Prec@1 52.478 Loss: 1.3882\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2691 (1.2691)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2573 (1.3908)\tPrec@1 31.250 (51.630)\n",
      "EPOCH: 15 val Results: Prec@1 51.630 Loss: 1.3908\n",
      "Best Prec@1: 51.630\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [16][0/390]\tTime 0.005 (0.005)\tLoss 1.2812 (1.2812)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.003)\tLoss 1.2880 (1.3322)\tPrec@1 56.250 (54.420)\n",
      "Epoch: [16][156/390]\tTime 0.003 (0.003)\tLoss 1.3715 (1.3490)\tPrec@1 50.781 (53.608)\n",
      "Epoch: [16][234/390]\tTime 0.006 (0.003)\tLoss 1.5338 (1.3583)\tPrec@1 48.438 (53.444)\n",
      "Epoch: [16][312/390]\tTime 0.004 (0.003)\tLoss 1.3443 (1.3638)\tPrec@1 52.344 (53.225)\n",
      "Epoch: [16][390/390]\tTime 0.001 (0.003)\tLoss 1.5525 (1.3681)\tPrec@1 47.500 (53.046)\n",
      "EPOCH: 16 train Results: Prec@1 53.046 Loss: 1.3681\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2525 (1.2525)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2814 (1.3807)\tPrec@1 50.000 (52.080)\n",
      "EPOCH: 16 val Results: Prec@1 52.080 Loss: 1.3807\n",
      "Best Prec@1: 52.080\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [17][0/390]\tTime 0.005 (0.005)\tLoss 1.3545 (1.3545)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [17][78/390]\tTime 0.004 (0.003)\tLoss 1.4050 (1.3284)\tPrec@1 47.656 (54.509)\n",
      "Epoch: [17][156/390]\tTime 0.003 (0.003)\tLoss 1.4489 (1.3356)\tPrec@1 48.438 (54.260)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.003)\tLoss 1.3742 (1.3423)\tPrec@1 50.781 (53.896)\n",
      "Epoch: [17][312/390]\tTime 0.004 (0.003)\tLoss 1.3717 (1.3448)\tPrec@1 52.344 (53.869)\n",
      "Epoch: [17][390/390]\tTime 0.004 (0.003)\tLoss 1.2755 (1.3497)\tPrec@1 60.000 (53.726)\n",
      "EPOCH: 17 train Results: Prec@1 53.726 Loss: 1.3497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2557 (1.2557)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2048 (1.3666)\tPrec@1 62.500 (52.360)\n",
      "EPOCH: 17 val Results: Prec@1 52.360 Loss: 1.3666\n",
      "Best Prec@1: 52.360\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [18][0/390]\tTime 0.003 (0.003)\tLoss 1.2693 (1.2693)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [18][78/390]\tTime 0.005 (0.003)\tLoss 1.3496 (1.3132)\tPrec@1 52.344 (55.014)\n",
      "Epoch: [18][156/390]\tTime 0.003 (0.003)\tLoss 1.2631 (1.3224)\tPrec@1 57.812 (54.747)\n",
      "Epoch: [18][234/390]\tTime 0.003 (0.004)\tLoss 1.2049 (1.3354)\tPrec@1 57.812 (54.092)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.004)\tLoss 1.3427 (1.3367)\tPrec@1 55.469 (54.044)\n",
      "Epoch: [18][390/390]\tTime 0.001 (0.003)\tLoss 1.2166 (1.3379)\tPrec@1 58.750 (53.956)\n",
      "EPOCH: 18 train Results: Prec@1 53.956 Loss: 1.3379\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2245 (1.2245)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2927 (1.3532)\tPrec@1 50.000 (53.160)\n",
      "EPOCH: 18 val Results: Prec@1 53.160 Loss: 1.3532\n",
      "Best Prec@1: 53.160\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [19][0/390]\tTime 0.003 (0.003)\tLoss 1.3007 (1.3007)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [19][78/390]\tTime 0.003 (0.003)\tLoss 1.4261 (1.2807)\tPrec@1 50.781 (56.804)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.003)\tLoss 1.2704 (1.3020)\tPrec@1 56.250 (55.797)\n",
      "Epoch: [19][234/390]\tTime 0.005 (0.003)\tLoss 1.3132 (1.3064)\tPrec@1 53.125 (55.436)\n",
      "Epoch: [19][312/390]\tTime 0.005 (0.003)\tLoss 1.2001 (1.3178)\tPrec@1 62.500 (54.762)\n",
      "Epoch: [19][390/390]\tTime 0.002 (0.003)\tLoss 1.5276 (1.3216)\tPrec@1 45.000 (54.578)\n",
      "EPOCH: 19 train Results: Prec@1 54.578 Loss: 1.3216\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2494 (1.2494)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5206 (1.3451)\tPrec@1 25.000 (53.310)\n",
      "EPOCH: 19 val Results: Prec@1 53.310 Loss: 1.3451\n",
      "Best Prec@1: 53.310\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [20][0/390]\tTime 0.004 (0.004)\tLoss 1.2568 (1.2568)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [20][78/390]\tTime 0.005 (0.003)\tLoss 1.1280 (1.2944)\tPrec@1 59.375 (56.013)\n",
      "Epoch: [20][156/390]\tTime 0.003 (0.003)\tLoss 1.2142 (1.2967)\tPrec@1 54.688 (55.862)\n",
      "Epoch: [20][234/390]\tTime 0.002 (0.003)\tLoss 1.3250 (1.3078)\tPrec@1 56.250 (55.083)\n",
      "Epoch: [20][312/390]\tTime 0.002 (0.003)\tLoss 1.3619 (1.3099)\tPrec@1 55.469 (54.957)\n",
      "Epoch: [20][390/390]\tTime 0.002 (0.003)\tLoss 1.3924 (1.3121)\tPrec@1 51.250 (54.858)\n",
      "EPOCH: 20 train Results: Prec@1 54.858 Loss: 1.3121\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2322 (1.2322)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3543 (1.3368)\tPrec@1 50.000 (53.340)\n",
      "EPOCH: 20 val Results: Prec@1 53.340 Loss: 1.3368\n",
      "Best Prec@1: 53.340\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [21][0/390]\tTime 0.003 (0.003)\tLoss 1.3590 (1.3590)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [21][78/390]\tTime 0.002 (0.003)\tLoss 1.4001 (1.2653)\tPrec@1 52.344 (56.883)\n",
      "Epoch: [21][156/390]\tTime 0.003 (0.003)\tLoss 1.4362 (1.2808)\tPrec@1 48.438 (56.220)\n",
      "Epoch: [21][234/390]\tTime 0.002 (0.003)\tLoss 1.2988 (1.2902)\tPrec@1 54.688 (55.924)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.2414 (1.3001)\tPrec@1 54.688 (55.549)\n",
      "Epoch: [21][390/390]\tTime 0.003 (0.003)\tLoss 1.3374 (1.3066)\tPrec@1 56.250 (55.172)\n",
      "EPOCH: 21 train Results: Prec@1 55.172 Loss: 1.3066\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2689 (1.2689)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2487 (1.3327)\tPrec@1 43.750 (54.030)\n",
      "EPOCH: 21 val Results: Prec@1 54.030 Loss: 1.3327\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [22][0/390]\tTime 0.002 (0.002)\tLoss 1.2814 (1.2814)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [22][78/390]\tTime 0.003 (0.003)\tLoss 1.2571 (1.2760)\tPrec@1 53.906 (56.794)\n",
      "Epoch: [22][156/390]\tTime 0.002 (0.003)\tLoss 1.2092 (1.2824)\tPrec@1 57.812 (56.369)\n",
      "Epoch: [22][234/390]\tTime 0.003 (0.003)\tLoss 1.2733 (1.2872)\tPrec@1 54.688 (55.997)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.003)\tLoss 1.2672 (1.2947)\tPrec@1 53.906 (55.576)\n",
      "Epoch: [22][390/390]\tTime 0.007 (0.003)\tLoss 1.2212 (1.3009)\tPrec@1 65.000 (55.330)\n",
      "EPOCH: 22 train Results: Prec@1 55.330 Loss: 1.3009\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2343 (1.2343)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1572 (1.3286)\tPrec@1 56.250 (53.950)\n",
      "EPOCH: 22 val Results: Prec@1 53.950 Loss: 1.3286\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [23][0/390]\tTime 0.004 (0.004)\tLoss 1.2254 (1.2254)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [23][78/390]\tTime 0.004 (0.003)\tLoss 1.4177 (1.2693)\tPrec@1 51.562 (57.002)\n",
      "Epoch: [23][156/390]\tTime 0.002 (0.003)\tLoss 1.4447 (1.2819)\tPrec@1 54.688 (56.160)\n",
      "Epoch: [23][234/390]\tTime 0.002 (0.003)\tLoss 1.2347 (1.2818)\tPrec@1 60.156 (56.107)\n",
      "Epoch: [23][312/390]\tTime 0.002 (0.003)\tLoss 1.3901 (1.2888)\tPrec@1 50.000 (55.858)\n",
      "Epoch: [23][390/390]\tTime 0.002 (0.003)\tLoss 1.4754 (1.2957)\tPrec@1 53.750 (55.618)\n",
      "EPOCH: 23 train Results: Prec@1 55.618 Loss: 1.2957\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2027 (1.2027)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3861 (1.3374)\tPrec@1 37.500 (53.320)\n",
      "EPOCH: 23 val Results: Prec@1 53.320 Loss: 1.3374\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [24][0/390]\tTime 0.003 (0.003)\tLoss 1.2541 (1.2541)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.2974 (1.2398)\tPrec@1 58.594 (58.722)\n",
      "Epoch: [24][156/390]\tTime 0.002 (0.003)\tLoss 1.3168 (1.2620)\tPrec@1 57.031 (57.305)\n",
      "Epoch: [24][234/390]\tTime 0.004 (0.003)\tLoss 1.3782 (1.2786)\tPrec@1 51.562 (56.579)\n",
      "Epoch: [24][312/390]\tTime 0.004 (0.003)\tLoss 1.3844 (1.2841)\tPrec@1 50.000 (56.377)\n",
      "Epoch: [24][390/390]\tTime 0.001 (0.003)\tLoss 1.3160 (1.2875)\tPrec@1 52.500 (56.156)\n",
      "EPOCH: 24 train Results: Prec@1 56.156 Loss: 1.2875\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2241 (1.2241)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2286 (1.3253)\tPrec@1 43.750 (53.610)\n",
      "EPOCH: 24 val Results: Prec@1 53.610 Loss: 1.3253\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [25][0/390]\tTime 0.002 (0.002)\tLoss 1.2641 (1.2641)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [25][78/390]\tTime 0.003 (0.003)\tLoss 1.2123 (1.2577)\tPrec@1 57.812 (57.387)\n",
      "Epoch: [25][156/390]\tTime 0.003 (0.003)\tLoss 1.2666 (1.2697)\tPrec@1 57.031 (56.932)\n",
      "Epoch: [25][234/390]\tTime 0.011 (0.003)\tLoss 1.3335 (1.2774)\tPrec@1 52.344 (56.287)\n",
      "Epoch: [25][312/390]\tTime 0.002 (0.003)\tLoss 1.3970 (1.2833)\tPrec@1 50.000 (55.960)\n",
      "Epoch: [25][390/390]\tTime 0.004 (0.003)\tLoss 1.4233 (1.2861)\tPrec@1 45.000 (55.860)\n",
      "EPOCH: 25 train Results: Prec@1 55.860 Loss: 1.2861\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2205 (1.2205)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4438 (1.3246)\tPrec@1 31.250 (53.510)\n",
      "EPOCH: 25 val Results: Prec@1 53.510 Loss: 1.3246\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [26][0/390]\tTime 0.003 (0.003)\tLoss 1.2810 (1.2810)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [26][78/390]\tTime 0.002 (0.003)\tLoss 1.2420 (1.2373)\tPrec@1 61.719 (58.366)\n",
      "Epoch: [26][156/390]\tTime 0.007 (0.003)\tLoss 1.2752 (1.2572)\tPrec@1 53.906 (57.718)\n",
      "Epoch: [26][234/390]\tTime 0.003 (0.003)\tLoss 1.2180 (1.2658)\tPrec@1 57.812 (56.995)\n",
      "Epoch: [26][312/390]\tTime 0.002 (0.003)\tLoss 1.2585 (1.2754)\tPrec@1 57.031 (56.382)\n",
      "Epoch: [26][390/390]\tTime 0.010 (0.003)\tLoss 1.3419 (1.2796)\tPrec@1 50.000 (56.212)\n",
      "EPOCH: 26 train Results: Prec@1 56.212 Loss: 1.2796\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2149 (1.2149)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3660 (1.3258)\tPrec@1 25.000 (54.030)\n",
      "EPOCH: 26 val Results: Prec@1 54.030 Loss: 1.3258\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [27][0/390]\tTime 0.002 (0.002)\tLoss 1.3688 (1.3688)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [27][78/390]\tTime 0.003 (0.003)\tLoss 1.2031 (1.2418)\tPrec@1 60.156 (57.694)\n",
      "Epoch: [27][156/390]\tTime 0.003 (0.003)\tLoss 1.1492 (1.2541)\tPrec@1 59.375 (57.171)\n",
      "Epoch: [27][234/390]\tTime 0.003 (0.003)\tLoss 1.1679 (1.2626)\tPrec@1 61.719 (56.699)\n",
      "Epoch: [27][312/390]\tTime 0.002 (0.003)\tLoss 1.2486 (1.2718)\tPrec@1 60.938 (56.355)\n",
      "Epoch: [27][390/390]\tTime 0.003 (0.003)\tLoss 1.5213 (1.2792)\tPrec@1 48.750 (56.128)\n",
      "EPOCH: 27 train Results: Prec@1 56.128 Loss: 1.2792\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1781 (1.1781)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.3718 (1.3234)\tPrec@1 50.000 (53.910)\n",
      "EPOCH: 27 val Results: Prec@1 53.910 Loss: 1.3234\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [28][0/390]\tTime 0.005 (0.005)\tLoss 1.2218 (1.2218)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [28][78/390]\tTime 0.002 (0.003)\tLoss 1.1611 (1.2433)\tPrec@1 66.406 (57.981)\n",
      "Epoch: [28][156/390]\tTime 0.002 (0.003)\tLoss 1.2403 (1.2505)\tPrec@1 60.938 (57.618)\n",
      "Epoch: [28][234/390]\tTime 0.004 (0.003)\tLoss 1.2478 (1.2596)\tPrec@1 56.250 (57.221)\n",
      "Epoch: [28][312/390]\tTime 0.006 (0.003)\tLoss 1.3227 (1.2666)\tPrec@1 51.562 (56.904)\n",
      "Epoch: [28][390/390]\tTime 0.002 (0.003)\tLoss 1.4296 (1.2733)\tPrec@1 42.500 (56.570)\n",
      "EPOCH: 28 train Results: Prec@1 56.570 Loss: 1.2733\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2230 (1.2230)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3409 (1.3152)\tPrec@1 43.750 (54.310)\n",
      "EPOCH: 28 val Results: Prec@1 54.310 Loss: 1.3152\n",
      "Best Prec@1: 54.310\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [29][0/390]\tTime 0.006 (0.006)\tLoss 1.1836 (1.1836)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [29][78/390]\tTime 0.004 (0.003)\tLoss 1.3463 (1.2502)\tPrec@1 52.344 (57.358)\n",
      "Epoch: [29][156/390]\tTime 0.002 (0.003)\tLoss 1.2039 (1.2564)\tPrec@1 59.375 (57.404)\n",
      "Epoch: [29][234/390]\tTime 0.005 (0.003)\tLoss 1.1700 (1.2646)\tPrec@1 65.625 (57.118)\n",
      "Epoch: [29][312/390]\tTime 0.002 (0.003)\tLoss 1.2730 (1.2711)\tPrec@1 53.906 (56.804)\n",
      "Epoch: [29][390/390]\tTime 0.004 (0.003)\tLoss 1.3716 (1.2747)\tPrec@1 48.750 (56.462)\n",
      "EPOCH: 29 train Results: Prec@1 56.462 Loss: 1.2747\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1526 (1.1526)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1597 (1.3167)\tPrec@1 50.000 (54.700)\n",
      "EPOCH: 29 val Results: Prec@1 54.700 Loss: 1.3167\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [30][0/390]\tTime 0.010 (0.010)\tLoss 1.2591 (1.2591)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.003)\tLoss 1.2796 (1.2343)\tPrec@1 56.250 (58.258)\n",
      "Epoch: [30][156/390]\tTime 0.004 (0.003)\tLoss 1.2549 (1.2494)\tPrec@1 57.031 (57.972)\n",
      "Epoch: [30][234/390]\tTime 0.003 (0.003)\tLoss 1.2572 (1.2552)\tPrec@1 53.906 (57.350)\n",
      "Epoch: [30][312/390]\tTime 0.004 (0.003)\tLoss 1.2226 (1.2607)\tPrec@1 58.594 (56.916)\n",
      "Epoch: [30][390/390]\tTime 0.001 (0.003)\tLoss 1.2421 (1.2677)\tPrec@1 61.250 (56.642)\n",
      "EPOCH: 30 train Results: Prec@1 56.642 Loss: 1.2677\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1910 (1.1910)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2805 (1.3166)\tPrec@1 43.750 (53.950)\n",
      "EPOCH: 30 val Results: Prec@1 53.950 Loss: 1.3166\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [31][0/390]\tTime 0.002 (0.002)\tLoss 1.2185 (1.2185)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [31][78/390]\tTime 0.003 (0.003)\tLoss 1.2112 (1.2376)\tPrec@1 56.250 (58.139)\n",
      "Epoch: [31][156/390]\tTime 0.003 (0.003)\tLoss 1.2326 (1.2475)\tPrec@1 58.594 (57.947)\n",
      "Epoch: [31][234/390]\tTime 0.002 (0.003)\tLoss 1.2922 (1.2549)\tPrec@1 55.469 (57.550)\n",
      "Epoch: [31][312/390]\tTime 0.007 (0.003)\tLoss 1.2123 (1.2610)\tPrec@1 61.719 (57.116)\n",
      "Epoch: [31][390/390]\tTime 0.003 (0.003)\tLoss 1.2920 (1.2676)\tPrec@1 58.750 (56.722)\n",
      "EPOCH: 31 train Results: Prec@1 56.722 Loss: 1.2676\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2578 (1.2578)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4832 (1.3220)\tPrec@1 31.250 (53.870)\n",
      "EPOCH: 31 val Results: Prec@1 53.870 Loss: 1.3220\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [32][0/390]\tTime 0.005 (0.005)\tLoss 1.2493 (1.2493)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [32][78/390]\tTime 0.004 (0.003)\tLoss 1.1590 (1.2386)\tPrec@1 60.156 (58.030)\n",
      "Epoch: [32][156/390]\tTime 0.002 (0.003)\tLoss 1.0421 (1.2432)\tPrec@1 69.531 (57.444)\n",
      "Epoch: [32][234/390]\tTime 0.004 (0.003)\tLoss 1.2497 (1.2533)\tPrec@1 57.031 (57.064)\n",
      "Epoch: [32][312/390]\tTime 0.002 (0.003)\tLoss 1.2187 (1.2581)\tPrec@1 59.375 (56.931)\n",
      "Epoch: [32][390/390]\tTime 0.002 (0.003)\tLoss 1.2152 (1.2659)\tPrec@1 55.000 (56.648)\n",
      "EPOCH: 32 train Results: Prec@1 56.648 Loss: 1.2659\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.2129 (1.2129)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3419 (1.3118)\tPrec@1 37.500 (54.440)\n",
      "EPOCH: 32 val Results: Prec@1 54.440 Loss: 1.3118\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [33][0/390]\tTime 0.005 (0.005)\tLoss 1.3007 (1.3007)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [33][78/390]\tTime 0.002 (0.003)\tLoss 1.3719 (1.2385)\tPrec@1 53.125 (57.902)\n",
      "Epoch: [33][156/390]\tTime 0.002 (0.003)\tLoss 1.3224 (1.2489)\tPrec@1 50.781 (57.275)\n",
      "Epoch: [33][234/390]\tTime 0.002 (0.003)\tLoss 1.3035 (1.2507)\tPrec@1 54.688 (57.251)\n",
      "Epoch: [33][312/390]\tTime 0.002 (0.003)\tLoss 1.3085 (1.2598)\tPrec@1 54.688 (56.896)\n",
      "Epoch: [33][390/390]\tTime 0.001 (0.003)\tLoss 1.2818 (1.2638)\tPrec@1 53.750 (56.788)\n",
      "EPOCH: 33 train Results: Prec@1 56.788 Loss: 1.2638\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2086 (1.2086)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4200 (1.3122)\tPrec@1 37.500 (54.410)\n",
      "EPOCH: 33 val Results: Prec@1 54.410 Loss: 1.3122\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [34][0/390]\tTime 0.002 (0.002)\tLoss 1.1757 (1.1757)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [34][78/390]\tTime 0.005 (0.003)\tLoss 1.3173 (1.2209)\tPrec@1 50.000 (59.326)\n",
      "Epoch: [34][156/390]\tTime 0.002 (0.003)\tLoss 1.2840 (1.2357)\tPrec@1 55.469 (58.260)\n",
      "Epoch: [34][234/390]\tTime 0.002 (0.003)\tLoss 1.2302 (1.2416)\tPrec@1 56.250 (57.846)\n",
      "Epoch: [34][312/390]\tTime 0.003 (0.003)\tLoss 1.2348 (1.2508)\tPrec@1 62.500 (57.403)\n",
      "Epoch: [34][390/390]\tTime 0.003 (0.003)\tLoss 1.2295 (1.2582)\tPrec@1 57.500 (57.008)\n",
      "EPOCH: 34 train Results: Prec@1 57.008 Loss: 1.2582\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1591 (1.1591)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3237 (1.3084)\tPrec@1 37.500 (55.110)\n",
      "EPOCH: 34 val Results: Prec@1 55.110 Loss: 1.3084\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [35][0/390]\tTime 0.004 (0.004)\tLoss 1.2261 (1.2261)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [35][78/390]\tTime 0.002 (0.004)\tLoss 1.2894 (1.2255)\tPrec@1 56.250 (58.960)\n",
      "Epoch: [35][156/390]\tTime 0.002 (0.004)\tLoss 1.4749 (1.2321)\tPrec@1 52.344 (58.181)\n",
      "Epoch: [35][234/390]\tTime 0.003 (0.003)\tLoss 1.2239 (1.2455)\tPrec@1 61.719 (57.593)\n",
      "Epoch: [35][312/390]\tTime 0.003 (0.003)\tLoss 1.3121 (1.2516)\tPrec@1 54.688 (57.311)\n",
      "Epoch: [35][390/390]\tTime 0.001 (0.003)\tLoss 1.2571 (1.2566)\tPrec@1 55.000 (57.086)\n",
      "EPOCH: 35 train Results: Prec@1 57.086 Loss: 1.2566\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1990 (1.1990)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2926 (1.3124)\tPrec@1 37.500 (54.570)\n",
      "EPOCH: 35 val Results: Prec@1 54.570 Loss: 1.3124\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [36][0/390]\tTime 0.003 (0.003)\tLoss 1.3486 (1.3486)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [36][78/390]\tTime 0.002 (0.003)\tLoss 1.2624 (1.2262)\tPrec@1 53.125 (58.465)\n",
      "Epoch: [36][156/390]\tTime 0.003 (0.004)\tLoss 1.3448 (1.2390)\tPrec@1 51.562 (58.076)\n",
      "Epoch: [36][234/390]\tTime 0.002 (0.003)\tLoss 1.2939 (1.2442)\tPrec@1 56.250 (57.563)\n",
      "Epoch: [36][312/390]\tTime 0.002 (0.003)\tLoss 1.2865 (1.2506)\tPrec@1 61.719 (57.353)\n",
      "Epoch: [36][390/390]\tTime 0.001 (0.003)\tLoss 1.3571 (1.2565)\tPrec@1 52.500 (56.976)\n",
      "EPOCH: 36 train Results: Prec@1 56.976 Loss: 1.2565\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2064 (1.2064)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3555 (1.3022)\tPrec@1 37.500 (54.070)\n",
      "EPOCH: 36 val Results: Prec@1 54.070 Loss: 1.3022\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [37][0/390]\tTime 0.002 (0.002)\tLoss 1.1116 (1.1116)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 1.1197 (1.2052)\tPrec@1 64.844 (59.424)\n",
      "Epoch: [37][156/390]\tTime 0.004 (0.003)\tLoss 1.1836 (1.2278)\tPrec@1 55.469 (58.370)\n",
      "Epoch: [37][234/390]\tTime 0.003 (0.003)\tLoss 1.4228 (1.2449)\tPrec@1 45.312 (57.370)\n",
      "Epoch: [37][312/390]\tTime 0.002 (0.003)\tLoss 1.2434 (1.2552)\tPrec@1 58.594 (56.884)\n",
      "Epoch: [37][390/390]\tTime 0.002 (0.003)\tLoss 1.3003 (1.2578)\tPrec@1 56.250 (56.886)\n",
      "EPOCH: 37 train Results: Prec@1 56.886 Loss: 1.2578\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2314 (1.2314)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.001 (0.002)\tLoss 1.3739 (1.3093)\tPrec@1 43.750 (54.420)\n",
      "EPOCH: 37 val Results: Prec@1 54.420 Loss: 1.3093\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [38][0/390]\tTime 0.015 (0.015)\tLoss 1.1985 (1.1985)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.004)\tLoss 1.2787 (1.2355)\tPrec@1 54.688 (57.981)\n",
      "Epoch: [38][156/390]\tTime 0.002 (0.004)\tLoss 1.2223 (1.2414)\tPrec@1 56.250 (57.703)\n",
      "Epoch: [38][234/390]\tTime 0.002 (0.003)\tLoss 1.2186 (1.2459)\tPrec@1 56.250 (57.367)\n",
      "Epoch: [38][312/390]\tTime 0.020 (0.003)\tLoss 1.3667 (1.2490)\tPrec@1 47.656 (57.139)\n",
      "Epoch: [38][390/390]\tTime 0.001 (0.004)\tLoss 1.2024 (1.2555)\tPrec@1 61.250 (56.928)\n",
      "EPOCH: 38 train Results: Prec@1 56.928 Loss: 1.2555\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.2325 (1.2325)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2935 (1.3104)\tPrec@1 56.250 (54.130)\n",
      "EPOCH: 38 val Results: Prec@1 54.130 Loss: 1.3104\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [39][0/390]\tTime 0.007 (0.007)\tLoss 1.2960 (1.2960)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.004)\tLoss 1.2939 (1.2049)\tPrec@1 55.469 (59.612)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.003)\tLoss 1.2399 (1.2251)\tPrec@1 54.688 (58.484)\n",
      "Epoch: [39][234/390]\tTime 0.002 (0.003)\tLoss 1.2925 (1.2368)\tPrec@1 53.906 (57.975)\n",
      "Epoch: [39][312/390]\tTime 0.006 (0.003)\tLoss 1.5444 (1.2426)\tPrec@1 46.094 (57.790)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2783 (1.2511)\tPrec@1 50.000 (57.442)\n",
      "EPOCH: 39 train Results: Prec@1 57.442 Loss: 1.2511\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1769 (1.1769)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5843 (1.3060)\tPrec@1 43.750 (54.270)\n",
      "EPOCH: 39 val Results: Prec@1 54.270 Loss: 1.3060\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [40][0/390]\tTime 0.002 (0.002)\tLoss 1.2156 (1.2156)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.003)\tLoss 1.2453 (1.2178)\tPrec@1 56.250 (59.009)\n",
      "Epoch: [40][156/390]\tTime 0.002 (0.003)\tLoss 1.3282 (1.2225)\tPrec@1 57.812 (58.549)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.003)\tLoss 1.3875 (1.2320)\tPrec@1 53.125 (58.231)\n",
      "Epoch: [40][312/390]\tTime 0.002 (0.003)\tLoss 1.4029 (1.2435)\tPrec@1 50.781 (57.610)\n",
      "Epoch: [40][390/390]\tTime 0.002 (0.003)\tLoss 1.3277 (1.2522)\tPrec@1 56.250 (57.244)\n",
      "EPOCH: 40 train Results: Prec@1 57.244 Loss: 1.2522\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2519 (1.2519)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5657 (1.3002)\tPrec@1 31.250 (55.130)\n",
      "EPOCH: 40 val Results: Prec@1 55.130 Loss: 1.3002\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [41][0/390]\tTime 0.005 (0.005)\tLoss 1.3309 (1.3309)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [41][78/390]\tTime 0.002 (0.003)\tLoss 1.1843 (1.2062)\tPrec@1 62.500 (59.217)\n",
      "Epoch: [41][156/390]\tTime 0.004 (0.003)\tLoss 1.3480 (1.2282)\tPrec@1 53.906 (58.370)\n",
      "Epoch: [41][234/390]\tTime 0.002 (0.003)\tLoss 1.3492 (1.2354)\tPrec@1 52.344 (58.115)\n",
      "Epoch: [41][312/390]\tTime 0.002 (0.004)\tLoss 1.2867 (1.2437)\tPrec@1 57.031 (57.713)\n",
      "Epoch: [41][390/390]\tTime 0.002 (0.003)\tLoss 1.2076 (1.2502)\tPrec@1 62.500 (57.366)\n",
      "EPOCH: 41 train Results: Prec@1 57.366 Loss: 1.2502\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1424 (1.1424)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3816 (1.3001)\tPrec@1 43.750 (54.690)\n",
      "EPOCH: 41 val Results: Prec@1 54.690 Loss: 1.3001\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [42][0/390]\tTime 0.004 (0.004)\tLoss 1.2853 (1.2853)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [42][78/390]\tTime 0.002 (0.003)\tLoss 1.2669 (1.2091)\tPrec@1 57.812 (59.227)\n",
      "Epoch: [42][156/390]\tTime 0.002 (0.003)\tLoss 1.1931 (1.2167)\tPrec@1 63.281 (58.783)\n",
      "Epoch: [42][234/390]\tTime 0.002 (0.004)\tLoss 1.2398 (1.2344)\tPrec@1 57.812 (57.995)\n",
      "Epoch: [42][312/390]\tTime 0.002 (0.004)\tLoss 1.3484 (1.2437)\tPrec@1 57.031 (57.770)\n",
      "Epoch: [42][390/390]\tTime 0.008 (0.004)\tLoss 1.1955 (1.2506)\tPrec@1 61.250 (57.356)\n",
      "EPOCH: 42 train Results: Prec@1 57.356 Loss: 1.2506\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.2000 (1.2000)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3955 (1.3004)\tPrec@1 37.500 (55.100)\n",
      "EPOCH: 42 val Results: Prec@1 55.100 Loss: 1.3004\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [43][0/390]\tTime 0.005 (0.005)\tLoss 1.2683 (1.2683)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [43][78/390]\tTime 0.005 (0.003)\tLoss 1.2526 (1.2194)\tPrec@1 54.688 (58.455)\n",
      "Epoch: [43][156/390]\tTime 0.002 (0.003)\tLoss 1.2656 (1.2307)\tPrec@1 55.469 (57.837)\n",
      "Epoch: [43][234/390]\tTime 0.002 (0.003)\tLoss 1.2134 (1.2356)\tPrec@1 59.375 (57.693)\n",
      "Epoch: [43][312/390]\tTime 0.003 (0.003)\tLoss 1.1297 (1.2459)\tPrec@1 62.500 (57.268)\n",
      "Epoch: [43][390/390]\tTime 0.002 (0.003)\tLoss 1.0839 (1.2496)\tPrec@1 71.250 (57.064)\n",
      "EPOCH: 43 train Results: Prec@1 57.064 Loss: 1.2496\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2376 (1.2376)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3304 (1.3063)\tPrec@1 37.500 (54.840)\n",
      "EPOCH: 43 val Results: Prec@1 54.840 Loss: 1.3063\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [44][0/390]\tTime 0.005 (0.005)\tLoss 1.2049 (1.2049)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.004)\tLoss 1.1138 (1.2117)\tPrec@1 57.812 (59.217)\n",
      "Epoch: [44][156/390]\tTime 0.004 (0.004)\tLoss 1.1252 (1.2208)\tPrec@1 59.375 (58.549)\n",
      "Epoch: [44][234/390]\tTime 0.003 (0.004)\tLoss 1.1956 (1.2349)\tPrec@1 57.031 (57.985)\n",
      "Epoch: [44][312/390]\tTime 0.002 (0.004)\tLoss 1.1947 (1.2425)\tPrec@1 60.156 (57.481)\n",
      "Epoch: [44][390/390]\tTime 0.001 (0.004)\tLoss 1.2281 (1.2479)\tPrec@1 55.000 (57.168)\n",
      "EPOCH: 44 train Results: Prec@1 57.168 Loss: 1.2479\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1712 (1.1712)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4385 (1.3010)\tPrec@1 43.750 (55.130)\n",
      "EPOCH: 44 val Results: Prec@1 55.130 Loss: 1.3010\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [45][0/390]\tTime 0.002 (0.002)\tLoss 1.2343 (1.2343)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [45][78/390]\tTime 0.003 (0.003)\tLoss 1.2803 (1.2121)\tPrec@1 57.812 (58.564)\n",
      "Epoch: [45][156/390]\tTime 0.003 (0.003)\tLoss 1.2652 (1.2182)\tPrec@1 60.938 (58.529)\n",
      "Epoch: [45][234/390]\tTime 0.010 (0.003)\tLoss 1.3072 (1.2289)\tPrec@1 58.594 (58.052)\n",
      "Epoch: [45][312/390]\tTime 0.007 (0.003)\tLoss 1.2091 (1.2379)\tPrec@1 57.031 (57.718)\n",
      "Epoch: [45][390/390]\tTime 0.001 (0.003)\tLoss 1.3963 (1.2431)\tPrec@1 51.250 (57.516)\n",
      "EPOCH: 45 train Results: Prec@1 57.516 Loss: 1.2431\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1776 (1.1776)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3150 (1.2958)\tPrec@1 37.500 (55.120)\n",
      "EPOCH: 45 val Results: Prec@1 55.120 Loss: 1.2958\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [46][0/390]\tTime 0.002 (0.002)\tLoss 1.2737 (1.2737)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [46][78/390]\tTime 0.002 (0.003)\tLoss 1.2852 (1.2190)\tPrec@1 52.344 (58.604)\n",
      "Epoch: [46][156/390]\tTime 0.002 (0.003)\tLoss 1.2893 (1.2230)\tPrec@1 50.000 (58.305)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.2730 (1.2330)\tPrec@1 52.344 (57.869)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.003)\tLoss 1.2577 (1.2400)\tPrec@1 56.250 (57.593)\n",
      "Epoch: [46][390/390]\tTime 0.003 (0.003)\tLoss 1.2135 (1.2479)\tPrec@1 53.750 (57.280)\n",
      "EPOCH: 46 train Results: Prec@1 57.280 Loss: 1.2479\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2283 (1.2283)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5113 (1.3019)\tPrec@1 37.500 (54.780)\n",
      "EPOCH: 46 val Results: Prec@1 54.780 Loss: 1.3019\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [47][0/390]\tTime 0.004 (0.004)\tLoss 1.4856 (1.4856)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [47][78/390]\tTime 0.003 (0.003)\tLoss 1.3421 (1.2046)\tPrec@1 57.031 (59.167)\n",
      "Epoch: [47][156/390]\tTime 0.003 (0.003)\tLoss 1.2535 (1.2215)\tPrec@1 57.812 (58.474)\n",
      "Epoch: [47][234/390]\tTime 0.002 (0.003)\tLoss 1.1153 (1.2298)\tPrec@1 61.719 (57.945)\n",
      "Epoch: [47][312/390]\tTime 0.004 (0.003)\tLoss 1.2903 (1.2326)\tPrec@1 50.000 (57.745)\n",
      "Epoch: [47][390/390]\tTime 0.002 (0.003)\tLoss 1.2556 (1.2407)\tPrec@1 55.000 (57.448)\n",
      "EPOCH: 47 train Results: Prec@1 57.448 Loss: 1.2407\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1360 (1.1360)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3283 (1.2954)\tPrec@1 43.750 (54.930)\n",
      "EPOCH: 47 val Results: Prec@1 54.930 Loss: 1.2954\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [48][0/390]\tTime 0.003 (0.003)\tLoss 1.1786 (1.1786)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [48][78/390]\tTime 0.005 (0.003)\tLoss 1.2821 (1.2217)\tPrec@1 53.125 (58.495)\n",
      "Epoch: [48][156/390]\tTime 0.002 (0.003)\tLoss 1.3176 (1.2280)\tPrec@1 54.688 (58.136)\n",
      "Epoch: [48][234/390]\tTime 0.002 (0.003)\tLoss 1.2665 (1.2318)\tPrec@1 57.031 (57.892)\n",
      "Epoch: [48][312/390]\tTime 0.003 (0.003)\tLoss 1.2216 (1.2394)\tPrec@1 63.281 (57.488)\n",
      "Epoch: [48][390/390]\tTime 0.004 (0.003)\tLoss 1.3957 (1.2458)\tPrec@1 47.500 (57.142)\n",
      "EPOCH: 48 train Results: Prec@1 57.142 Loss: 1.2458\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2483 (1.2483)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6881 (1.3058)\tPrec@1 31.250 (54.260)\n",
      "EPOCH: 48 val Results: Prec@1 54.260 Loss: 1.3058\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [49][0/390]\tTime 0.004 (0.004)\tLoss 1.1757 (1.1757)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [49][78/390]\tTime 0.003 (0.003)\tLoss 1.2363 (1.1959)\tPrec@1 59.375 (59.355)\n",
      "Epoch: [49][156/390]\tTime 0.004 (0.004)\tLoss 1.2959 (1.2184)\tPrec@1 52.344 (58.549)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.004)\tLoss 1.2653 (1.2310)\tPrec@1 60.938 (57.919)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.2610 (1.2371)\tPrec@1 50.000 (57.725)\n",
      "Epoch: [49][390/390]\tTime 0.001 (0.003)\tLoss 1.4012 (1.2451)\tPrec@1 51.250 (57.342)\n",
      "EPOCH: 49 train Results: Prec@1 57.342 Loss: 1.2451\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2078 (1.2078)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3580 (1.2999)\tPrec@1 31.250 (54.860)\n",
      "EPOCH: 49 val Results: Prec@1 54.860 Loss: 1.2999\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [50][0/390]\tTime 0.002 (0.002)\tLoss 1.2196 (1.2196)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [50][78/390]\tTime 0.003 (0.003)\tLoss 1.2498 (1.2052)\tPrec@1 60.156 (59.405)\n",
      "Epoch: [50][156/390]\tTime 0.003 (0.003)\tLoss 1.2259 (1.2174)\tPrec@1 52.344 (58.514)\n",
      "Epoch: [50][234/390]\tTime 0.003 (0.004)\tLoss 1.2719 (1.2284)\tPrec@1 53.125 (58.165)\n",
      "Epoch: [50][312/390]\tTime 0.004 (0.004)\tLoss 1.2362 (1.2355)\tPrec@1 53.906 (57.773)\n",
      "Epoch: [50][390/390]\tTime 0.003 (0.004)\tLoss 1.2123 (1.2443)\tPrec@1 57.500 (57.460)\n",
      "EPOCH: 50 train Results: Prec@1 57.460 Loss: 1.2443\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2172 (1.2172)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4909 (1.3046)\tPrec@1 43.750 (54.430)\n",
      "EPOCH: 50 val Results: Prec@1 54.430 Loss: 1.3046\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [51][0/390]\tTime 0.002 (0.002)\tLoss 1.2474 (1.2474)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [51][78/390]\tTime 0.009 (0.004)\tLoss 1.1722 (1.2064)\tPrec@1 63.281 (59.049)\n",
      "Epoch: [51][156/390]\tTime 0.002 (0.004)\tLoss 1.2227 (1.2199)\tPrec@1 60.156 (58.464)\n",
      "Epoch: [51][234/390]\tTime 0.002 (0.004)\tLoss 1.3278 (1.2257)\tPrec@1 51.562 (58.175)\n",
      "Epoch: [51][312/390]\tTime 0.004 (0.004)\tLoss 1.2109 (1.2322)\tPrec@1 60.156 (57.920)\n",
      "Epoch: [51][390/390]\tTime 0.002 (0.004)\tLoss 1.1640 (1.2396)\tPrec@1 66.250 (57.644)\n",
      "EPOCH: 51 train Results: Prec@1 57.644 Loss: 1.2396\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2163 (1.2163)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3444 (1.2975)\tPrec@1 43.750 (54.870)\n",
      "EPOCH: 51 val Results: Prec@1 54.870 Loss: 1.2975\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [52][0/390]\tTime 0.004 (0.004)\tLoss 1.1486 (1.1486)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [52][78/390]\tTime 0.002 (0.003)\tLoss 1.2147 (1.1983)\tPrec@1 62.500 (59.593)\n",
      "Epoch: [52][156/390]\tTime 0.003 (0.003)\tLoss 1.2564 (1.2112)\tPrec@1 52.344 (58.803)\n",
      "Epoch: [52][234/390]\tTime 0.002 (0.003)\tLoss 1.1174 (1.2240)\tPrec@1 62.500 (58.125)\n",
      "Epoch: [52][312/390]\tTime 0.003 (0.003)\tLoss 1.3444 (1.2349)\tPrec@1 56.250 (57.673)\n",
      "Epoch: [52][390/390]\tTime 0.003 (0.004)\tLoss 1.4885 (1.2432)\tPrec@1 48.750 (57.292)\n",
      "EPOCH: 52 train Results: Prec@1 57.292 Loss: 1.2432\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2205 (1.2205)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3592 (1.3127)\tPrec@1 50.000 (53.780)\n",
      "EPOCH: 52 val Results: Prec@1 53.780 Loss: 1.3127\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [53][0/390]\tTime 0.007 (0.007)\tLoss 1.2875 (1.2875)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [53][78/390]\tTime 0.004 (0.004)\tLoss 1.2073 (1.1971)\tPrec@1 64.062 (59.909)\n",
      "Epoch: [53][156/390]\tTime 0.003 (0.004)\tLoss 1.2353 (1.2065)\tPrec@1 60.938 (59.365)\n",
      "Epoch: [53][234/390]\tTime 0.003 (0.004)\tLoss 1.2369 (1.2196)\tPrec@1 59.375 (58.703)\n",
      "Epoch: [53][312/390]\tTime 0.002 (0.004)\tLoss 1.5088 (1.2275)\tPrec@1 43.750 (58.384)\n",
      "Epoch: [53][390/390]\tTime 0.008 (0.004)\tLoss 1.2529 (1.2353)\tPrec@1 62.500 (57.920)\n",
      "EPOCH: 53 train Results: Prec@1 57.920 Loss: 1.2353\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2395 (1.2395)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4323 (1.3009)\tPrec@1 37.500 (54.130)\n",
      "EPOCH: 53 val Results: Prec@1 54.130 Loss: 1.3009\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [54][0/390]\tTime 0.004 (0.004)\tLoss 1.1302 (1.1302)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [54][78/390]\tTime 0.003 (0.003)\tLoss 1.2045 (1.2037)\tPrec@1 59.375 (59.286)\n",
      "Epoch: [54][156/390]\tTime 0.003 (0.003)\tLoss 1.1747 (1.2182)\tPrec@1 60.156 (58.290)\n",
      "Epoch: [54][234/390]\tTime 0.002 (0.003)\tLoss 1.2229 (1.2246)\tPrec@1 64.062 (58.148)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2752 (1.2312)\tPrec@1 58.594 (57.820)\n",
      "Epoch: [54][390/390]\tTime 0.002 (0.003)\tLoss 1.3251 (1.2406)\tPrec@1 52.500 (57.484)\n",
      "EPOCH: 54 train Results: Prec@1 57.484 Loss: 1.2406\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1841 (1.1841)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3184 (1.3012)\tPrec@1 50.000 (54.440)\n",
      "EPOCH: 54 val Results: Prec@1 54.440 Loss: 1.3012\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.2667 (1.2667)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.2766 (1.1953)\tPrec@1 58.594 (59.998)\n",
      "Epoch: [55][156/390]\tTime 0.002 (0.003)\tLoss 1.2912 (1.2156)\tPrec@1 52.344 (58.614)\n",
      "Epoch: [55][234/390]\tTime 0.003 (0.003)\tLoss 1.2508 (1.2278)\tPrec@1 58.594 (58.042)\n",
      "Epoch: [55][312/390]\tTime 0.004 (0.003)\tLoss 1.1350 (1.2313)\tPrec@1 65.625 (57.780)\n",
      "Epoch: [55][390/390]\tTime 0.003 (0.003)\tLoss 1.2304 (1.2376)\tPrec@1 63.750 (57.426)\n",
      "EPOCH: 55 train Results: Prec@1 57.426 Loss: 1.2376\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1853 (1.1853)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4375 (1.2977)\tPrec@1 31.250 (54.980)\n",
      "EPOCH: 55 val Results: Prec@1 54.980 Loss: 1.2977\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [56][0/390]\tTime 0.005 (0.005)\tLoss 1.1342 (1.1342)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [56][78/390]\tTime 0.004 (0.003)\tLoss 1.1990 (1.2183)\tPrec@1 58.594 (58.406)\n",
      "Epoch: [56][156/390]\tTime 0.008 (0.003)\tLoss 1.1407 (1.2189)\tPrec@1 65.625 (58.390)\n",
      "Epoch: [56][234/390]\tTime 0.005 (0.003)\tLoss 1.2267 (1.2226)\tPrec@1 57.812 (58.268)\n",
      "Epoch: [56][312/390]\tTime 0.004 (0.003)\tLoss 1.1483 (1.2299)\tPrec@1 60.156 (58.042)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.003)\tLoss 1.3605 (1.2357)\tPrec@1 55.000 (57.782)\n",
      "EPOCH: 56 train Results: Prec@1 57.782 Loss: 1.2357\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1291 (1.1291)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3535 (1.3008)\tPrec@1 37.500 (54.280)\n",
      "EPOCH: 56 val Results: Prec@1 54.280 Loss: 1.3008\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [57][0/390]\tTime 0.002 (0.002)\tLoss 1.2079 (1.2079)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [57][78/390]\tTime 0.003 (0.003)\tLoss 1.2382 (1.1907)\tPrec@1 54.688 (59.365)\n",
      "Epoch: [57][156/390]\tTime 0.002 (0.003)\tLoss 1.3007 (1.2082)\tPrec@1 57.812 (58.758)\n",
      "Epoch: [57][234/390]\tTime 0.007 (0.003)\tLoss 1.3300 (1.2183)\tPrec@1 54.688 (58.218)\n",
      "Epoch: [57][312/390]\tTime 0.004 (0.003)\tLoss 1.2608 (1.2250)\tPrec@1 57.031 (57.897)\n",
      "Epoch: [57][390/390]\tTime 0.005 (0.003)\tLoss 1.2431 (1.2366)\tPrec@1 53.750 (57.584)\n",
      "EPOCH: 57 train Results: Prec@1 57.584 Loss: 1.2366\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2114 (1.2114)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4071 (1.3064)\tPrec@1 12.500 (54.410)\n",
      "EPOCH: 57 val Results: Prec@1 54.410 Loss: 1.3064\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [58][0/390]\tTime 0.002 (0.002)\tLoss 1.1656 (1.1656)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [58][78/390]\tTime 0.004 (0.003)\tLoss 1.2329 (1.1952)\tPrec@1 56.250 (59.207)\n",
      "Epoch: [58][156/390]\tTime 0.002 (0.003)\tLoss 1.2349 (1.2108)\tPrec@1 56.250 (58.474)\n",
      "Epoch: [58][234/390]\tTime 0.002 (0.003)\tLoss 1.2213 (1.2207)\tPrec@1 61.719 (58.195)\n",
      "Epoch: [58][312/390]\tTime 0.002 (0.003)\tLoss 1.2051 (1.2266)\tPrec@1 60.156 (58.022)\n",
      "Epoch: [58][390/390]\tTime 0.003 (0.003)\tLoss 1.2753 (1.2332)\tPrec@1 57.500 (57.656)\n",
      "EPOCH: 58 train Results: Prec@1 57.656 Loss: 1.2332\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2225 (1.2225)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4099 (1.2922)\tPrec@1 25.000 (54.770)\n",
      "EPOCH: 58 val Results: Prec@1 54.770 Loss: 1.2922\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [59][0/390]\tTime 0.002 (0.002)\tLoss 1.2743 (1.2743)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [59][78/390]\tTime 0.002 (0.003)\tLoss 1.1848 (1.1966)\tPrec@1 53.125 (58.979)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.3003 (1.2018)\tPrec@1 52.344 (58.947)\n",
      "Epoch: [59][234/390]\tTime 0.007 (0.003)\tLoss 1.2039 (1.2191)\tPrec@1 57.812 (58.301)\n",
      "Epoch: [59][312/390]\tTime 0.004 (0.003)\tLoss 1.2583 (1.2265)\tPrec@1 58.594 (58.045)\n",
      "Epoch: [59][390/390]\tTime 0.001 (0.003)\tLoss 1.1802 (1.2349)\tPrec@1 62.500 (57.654)\n",
      "EPOCH: 59 train Results: Prec@1 57.654 Loss: 1.2349\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2302 (1.2302)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2846 (1.3040)\tPrec@1 37.500 (54.510)\n",
      "EPOCH: 59 val Results: Prec@1 54.510 Loss: 1.3040\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [60][0/390]\tTime 0.002 (0.002)\tLoss 1.1739 (1.1739)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [60][78/390]\tTime 0.002 (0.003)\tLoss 1.2054 (1.2094)\tPrec@1 57.812 (59.078)\n",
      "Epoch: [60][156/390]\tTime 0.007 (0.003)\tLoss 1.1671 (1.2186)\tPrec@1 60.938 (58.698)\n",
      "Epoch: [60][234/390]\tTime 0.002 (0.003)\tLoss 1.2821 (1.2251)\tPrec@1 57.031 (58.188)\n",
      "Epoch: [60][312/390]\tTime 0.003 (0.003)\tLoss 1.1856 (1.2323)\tPrec@1 60.156 (57.763)\n",
      "Epoch: [60][390/390]\tTime 0.002 (0.004)\tLoss 1.3013 (1.2364)\tPrec@1 55.000 (57.546)\n",
      "EPOCH: 60 train Results: Prec@1 57.546 Loss: 1.2364\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1743 (1.1743)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2830 (1.3027)\tPrec@1 43.750 (54.820)\n",
      "EPOCH: 60 val Results: Prec@1 54.820 Loss: 1.3027\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [61][0/390]\tTime 0.005 (0.005)\tLoss 1.0819 (1.0819)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [61][78/390]\tTime 0.004 (0.004)\tLoss 1.2872 (1.2056)\tPrec@1 56.250 (58.841)\n",
      "Epoch: [61][156/390]\tTime 0.002 (0.004)\tLoss 1.3365 (1.2100)\tPrec@1 50.781 (58.828)\n",
      "Epoch: [61][234/390]\tTime 0.002 (0.003)\tLoss 1.1244 (1.2187)\tPrec@1 60.938 (58.305)\n",
      "Epoch: [61][312/390]\tTime 0.004 (0.003)\tLoss 1.2611 (1.2276)\tPrec@1 59.375 (58.007)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.003)\tLoss 1.2613 (1.2341)\tPrec@1 56.250 (57.694)\n",
      "EPOCH: 61 train Results: Prec@1 57.694 Loss: 1.2341\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1703 (1.1703)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3856 (1.2952)\tPrec@1 31.250 (54.990)\n",
      "EPOCH: 61 val Results: Prec@1 54.990 Loss: 1.2952\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [62][0/390]\tTime 0.002 (0.002)\tLoss 1.1888 (1.1888)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.2240 (1.1945)\tPrec@1 58.594 (59.652)\n",
      "Epoch: [62][156/390]\tTime 0.005 (0.003)\tLoss 1.1187 (1.2063)\tPrec@1 64.062 (58.813)\n",
      "Epoch: [62][234/390]\tTime 0.003 (0.003)\tLoss 1.3004 (1.2168)\tPrec@1 57.031 (58.348)\n",
      "Epoch: [62][312/390]\tTime 0.006 (0.003)\tLoss 1.3652 (1.2222)\tPrec@1 52.344 (57.995)\n",
      "Epoch: [62][390/390]\tTime 0.002 (0.003)\tLoss 1.0522 (1.2297)\tPrec@1 68.750 (57.748)\n",
      "EPOCH: 62 train Results: Prec@1 57.748 Loss: 1.2297\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1856 (1.1856)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4506 (1.2911)\tPrec@1 37.500 (54.940)\n",
      "EPOCH: 62 val Results: Prec@1 54.940 Loss: 1.2911\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [63][0/390]\tTime 0.005 (0.005)\tLoss 1.1443 (1.1443)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [63][78/390]\tTime 0.002 (0.003)\tLoss 1.2842 (1.1943)\tPrec@1 53.906 (59.484)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.003)\tLoss 1.2345 (1.2091)\tPrec@1 55.469 (58.823)\n",
      "Epoch: [63][234/390]\tTime 0.003 (0.003)\tLoss 1.2657 (1.2198)\tPrec@1 56.250 (58.308)\n",
      "Epoch: [63][312/390]\tTime 0.003 (0.003)\tLoss 1.3789 (1.2266)\tPrec@1 53.906 (58.187)\n",
      "Epoch: [63][390/390]\tTime 0.003 (0.003)\tLoss 1.2939 (1.2308)\tPrec@1 57.500 (58.040)\n",
      "EPOCH: 63 train Results: Prec@1 58.040 Loss: 1.2308\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2087 (1.2087)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3659 (1.3068)\tPrec@1 37.500 (54.270)\n",
      "EPOCH: 63 val Results: Prec@1 54.270 Loss: 1.3068\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [64][0/390]\tTime 0.003 (0.003)\tLoss 1.1599 (1.1599)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [64][78/390]\tTime 0.002 (0.004)\tLoss 1.1325 (1.1753)\tPrec@1 57.812 (59.662)\n",
      "Epoch: [64][156/390]\tTime 0.004 (0.003)\tLoss 1.3963 (1.1988)\tPrec@1 53.125 (59.136)\n",
      "Epoch: [64][234/390]\tTime 0.002 (0.004)\tLoss 1.3711 (1.2129)\tPrec@1 54.688 (58.477)\n",
      "Epoch: [64][312/390]\tTime 0.003 (0.005)\tLoss 1.3275 (1.2240)\tPrec@1 49.219 (57.965)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.005)\tLoss 1.3455 (1.2319)\tPrec@1 50.000 (57.632)\n",
      "EPOCH: 64 train Results: Prec@1 57.632 Loss: 1.2319\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1967 (1.1967)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2339 (1.2975)\tPrec@1 43.750 (54.300)\n",
      "EPOCH: 64 val Results: Prec@1 54.300 Loss: 1.2975\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [65][0/390]\tTime 0.005 (0.005)\tLoss 1.2075 (1.2075)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.004)\tLoss 1.2381 (1.1884)\tPrec@1 57.031 (59.642)\n",
      "Epoch: [65][156/390]\tTime 0.003 (0.004)\tLoss 1.1908 (1.1971)\tPrec@1 62.500 (59.549)\n",
      "Epoch: [65][234/390]\tTime 0.006 (0.004)\tLoss 1.2720 (1.2125)\tPrec@1 55.469 (58.753)\n",
      "Epoch: [65][312/390]\tTime 0.003 (0.003)\tLoss 1.2390 (1.2227)\tPrec@1 53.125 (58.284)\n",
      "Epoch: [65][390/390]\tTime 0.004 (0.003)\tLoss 1.1067 (1.2285)\tPrec@1 65.000 (57.984)\n",
      "EPOCH: 65 train Results: Prec@1 57.984 Loss: 1.2285\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2234 (1.2234)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1642 (1.2930)\tPrec@1 50.000 (55.220)\n",
      "EPOCH: 65 val Results: Prec@1 55.220 Loss: 1.2930\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [66][0/390]\tTime 0.004 (0.004)\tLoss 1.1706 (1.1706)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [66][78/390]\tTime 0.003 (0.003)\tLoss 1.2490 (1.1937)\tPrec@1 53.906 (59.573)\n",
      "Epoch: [66][156/390]\tTime 0.004 (0.003)\tLoss 1.1865 (1.2042)\tPrec@1 61.719 (58.962)\n",
      "Epoch: [66][234/390]\tTime 0.002 (0.003)\tLoss 1.3049 (1.2135)\tPrec@1 53.906 (58.477)\n",
      "Epoch: [66][312/390]\tTime 0.005 (0.003)\tLoss 1.3521 (1.2245)\tPrec@1 56.250 (57.852)\n",
      "Epoch: [66][390/390]\tTime 0.001 (0.003)\tLoss 1.1626 (1.2299)\tPrec@1 66.250 (57.698)\n",
      "EPOCH: 66 train Results: Prec@1 57.698 Loss: 1.2299\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.2218 (1.2218)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4992 (1.2954)\tPrec@1 31.250 (54.670)\n",
      "EPOCH: 66 val Results: Prec@1 54.670 Loss: 1.2954\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [67][0/390]\tTime 0.002 (0.002)\tLoss 1.2580 (1.2580)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [67][78/390]\tTime 0.002 (0.003)\tLoss 1.2986 (1.1943)\tPrec@1 53.906 (58.970)\n",
      "Epoch: [67][156/390]\tTime 0.003 (0.003)\tLoss 1.4069 (1.2024)\tPrec@1 51.562 (58.733)\n",
      "Epoch: [67][234/390]\tTime 0.003 (0.004)\tLoss 1.2333 (1.2163)\tPrec@1 60.156 (58.341)\n",
      "Epoch: [67][312/390]\tTime 0.008 (0.004)\tLoss 1.2516 (1.2223)\tPrec@1 54.688 (58.164)\n",
      "Epoch: [67][390/390]\tTime 0.001 (0.003)\tLoss 1.3073 (1.2292)\tPrec@1 53.750 (57.882)\n",
      "EPOCH: 67 train Results: Prec@1 57.882 Loss: 1.2292\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2240 (1.2240)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2378 (1.2961)\tPrec@1 50.000 (54.720)\n",
      "EPOCH: 67 val Results: Prec@1 54.720 Loss: 1.2961\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [68][0/390]\tTime 0.002 (0.002)\tLoss 1.1511 (1.1511)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [68][78/390]\tTime 0.025 (0.004)\tLoss 1.1591 (1.1807)\tPrec@1 58.594 (60.047)\n",
      "Epoch: [68][156/390]\tTime 0.002 (0.004)\tLoss 1.3251 (1.2069)\tPrec@1 53.906 (58.738)\n",
      "Epoch: [68][234/390]\tTime 0.002 (0.004)\tLoss 1.3166 (1.2175)\tPrec@1 53.906 (58.118)\n",
      "Epoch: [68][312/390]\tTime 0.010 (0.004)\tLoss 1.4600 (1.2268)\tPrec@1 49.219 (57.733)\n",
      "Epoch: [68][390/390]\tTime 0.001 (0.004)\tLoss 1.4884 (1.2314)\tPrec@1 43.750 (57.668)\n",
      "EPOCH: 68 train Results: Prec@1 57.668 Loss: 1.2314\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1416 (1.1416)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1554 (1.2914)\tPrec@1 50.000 (54.560)\n",
      "EPOCH: 68 val Results: Prec@1 54.560 Loss: 1.2914\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [69][0/390]\tTime 0.002 (0.002)\tLoss 1.1648 (1.1648)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [69][78/390]\tTime 0.003 (0.003)\tLoss 1.1946 (1.1756)\tPrec@1 56.250 (60.087)\n",
      "Epoch: [69][156/390]\tTime 0.007 (0.003)\tLoss 1.2722 (1.1940)\tPrec@1 56.250 (59.330)\n",
      "Epoch: [69][234/390]\tTime 0.002 (0.003)\tLoss 1.0600 (1.2078)\tPrec@1 64.844 (58.797)\n",
      "Epoch: [69][312/390]\tTime 0.002 (0.003)\tLoss 1.2904 (1.2172)\tPrec@1 57.031 (58.142)\n",
      "Epoch: [69][390/390]\tTime 0.002 (0.003)\tLoss 1.2346 (1.2252)\tPrec@1 57.500 (57.824)\n",
      "EPOCH: 69 train Results: Prec@1 57.824 Loss: 1.2252\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2064 (1.2064)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4380 (1.2938)\tPrec@1 31.250 (54.600)\n",
      "EPOCH: 69 val Results: Prec@1 54.600 Loss: 1.2938\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [70][0/390]\tTime 0.004 (0.004)\tLoss 1.0495 (1.0495)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [70][78/390]\tTime 0.002 (0.003)\tLoss 1.3439 (1.1864)\tPrec@1 48.438 (59.385)\n",
      "Epoch: [70][156/390]\tTime 0.002 (0.003)\tLoss 1.2567 (1.2036)\tPrec@1 57.031 (59.017)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.003)\tLoss 1.1594 (1.2101)\tPrec@1 59.375 (58.700)\n",
      "Epoch: [70][312/390]\tTime 0.005 (0.003)\tLoss 1.1276 (1.2233)\tPrec@1 67.188 (58.007)\n",
      "Epoch: [70][390/390]\tTime 0.004 (0.003)\tLoss 1.2858 (1.2278)\tPrec@1 53.750 (57.738)\n",
      "EPOCH: 70 train Results: Prec@1 57.738 Loss: 1.2278\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2214 (1.2214)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2905 (1.2918)\tPrec@1 43.750 (54.690)\n",
      "EPOCH: 70 val Results: Prec@1 54.690 Loss: 1.2918\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [71][0/390]\tTime 0.004 (0.004)\tLoss 1.1782 (1.1782)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [71][78/390]\tTime 0.003 (0.003)\tLoss 1.1873 (1.1858)\tPrec@1 62.500 (59.840)\n",
      "Epoch: [71][156/390]\tTime 0.004 (0.004)\tLoss 1.2378 (1.2019)\tPrec@1 55.469 (59.151)\n",
      "Epoch: [71][234/390]\tTime 0.003 (0.004)\tLoss 1.0527 (1.2145)\tPrec@1 64.844 (58.547)\n",
      "Epoch: [71][312/390]\tTime 0.007 (0.004)\tLoss 1.2451 (1.2195)\tPrec@1 55.469 (58.234)\n",
      "Epoch: [71][390/390]\tTime 0.003 (0.004)\tLoss 1.1555 (1.2288)\tPrec@1 60.000 (57.770)\n",
      "EPOCH: 71 train Results: Prec@1 57.770 Loss: 1.2288\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2302 (1.2302)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2502 (1.3033)\tPrec@1 43.750 (54.550)\n",
      "EPOCH: 71 val Results: Prec@1 54.550 Loss: 1.3033\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [72][0/390]\tTime 0.002 (0.002)\tLoss 1.1687 (1.1687)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [72][78/390]\tTime 0.002 (0.004)\tLoss 1.3478 (1.1881)\tPrec@1 50.781 (60.097)\n",
      "Epoch: [72][156/390]\tTime 0.002 (0.004)\tLoss 1.2222 (1.1955)\tPrec@1 55.469 (59.280)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.004)\tLoss 1.3382 (1.2093)\tPrec@1 53.125 (58.531)\n",
      "Epoch: [72][312/390]\tTime 0.011 (0.005)\tLoss 1.2511 (1.2189)\tPrec@1 50.781 (58.194)\n",
      "Epoch: [72][390/390]\tTime 0.001 (0.005)\tLoss 1.0940 (1.2238)\tPrec@1 61.250 (57.996)\n",
      "EPOCH: 72 train Results: Prec@1 57.996 Loss: 1.2238\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1702 (1.1702)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2689 (1.2881)\tPrec@1 50.000 (54.780)\n",
      "EPOCH: 72 val Results: Prec@1 54.780 Loss: 1.2881\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [73][0/390]\tTime 0.003 (0.003)\tLoss 1.1047 (1.1047)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.004)\tLoss 1.1179 (1.1967)\tPrec@1 58.594 (59.434)\n",
      "Epoch: [73][156/390]\tTime 0.007 (0.003)\tLoss 1.2971 (1.2023)\tPrec@1 55.469 (58.544)\n",
      "Epoch: [73][234/390]\tTime 0.003 (0.004)\tLoss 1.2115 (1.2095)\tPrec@1 55.469 (58.404)\n",
      "Epoch: [73][312/390]\tTime 0.003 (0.004)\tLoss 1.1030 (1.2215)\tPrec@1 63.281 (57.945)\n",
      "Epoch: [73][390/390]\tTime 0.024 (0.004)\tLoss 1.1738 (1.2262)\tPrec@1 62.500 (57.774)\n",
      "EPOCH: 73 train Results: Prec@1 57.774 Loss: 1.2262\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1664 (1.1664)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3387 (1.2839)\tPrec@1 31.250 (55.540)\n",
      "EPOCH: 73 val Results: Prec@1 55.540 Loss: 1.2839\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [74][0/390]\tTime 0.012 (0.012)\tLoss 1.3350 (1.3350)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [74][78/390]\tTime 0.008 (0.005)\tLoss 1.1513 (1.1892)\tPrec@1 66.406 (59.909)\n",
      "Epoch: [74][156/390]\tTime 0.006 (0.005)\tLoss 1.2174 (1.2033)\tPrec@1 53.125 (59.310)\n",
      "Epoch: [74][234/390]\tTime 0.002 (0.005)\tLoss 1.2450 (1.2099)\tPrec@1 55.469 (58.880)\n",
      "Epoch: [74][312/390]\tTime 0.003 (0.004)\tLoss 1.1970 (1.2148)\tPrec@1 60.156 (58.494)\n",
      "Epoch: [74][390/390]\tTime 0.001 (0.004)\tLoss 1.2838 (1.2216)\tPrec@1 55.000 (58.164)\n",
      "EPOCH: 74 train Results: Prec@1 58.164 Loss: 1.2216\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2021 (1.2021)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6074 (1.2964)\tPrec@1 25.000 (54.790)\n",
      "EPOCH: 74 val Results: Prec@1 54.790 Loss: 1.2964\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [75][0/390]\tTime 0.002 (0.002)\tLoss 1.2052 (1.2052)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [75][78/390]\tTime 0.004 (0.003)\tLoss 1.3277 (1.1762)\tPrec@1 57.031 (59.474)\n",
      "Epoch: [75][156/390]\tTime 0.008 (0.003)\tLoss 0.9793 (1.2011)\tPrec@1 70.312 (58.624)\n",
      "Epoch: [75][234/390]\tTime 0.004 (0.003)\tLoss 1.2931 (1.2096)\tPrec@1 57.031 (58.481)\n",
      "Epoch: [75][312/390]\tTime 0.005 (0.003)\tLoss 1.2736 (1.2140)\tPrec@1 53.125 (58.347)\n",
      "Epoch: [75][390/390]\tTime 0.007 (0.003)\tLoss 1.2670 (1.2227)\tPrec@1 52.500 (58.048)\n",
      "EPOCH: 75 train Results: Prec@1 58.048 Loss: 1.2227\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2122 (1.2122)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4500 (1.3020)\tPrec@1 43.750 (54.440)\n",
      "EPOCH: 75 val Results: Prec@1 54.440 Loss: 1.3020\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [76][0/390]\tTime 0.004 (0.004)\tLoss 1.1677 (1.1677)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.003)\tLoss 1.3865 (1.1932)\tPrec@1 53.906 (59.563)\n",
      "Epoch: [76][156/390]\tTime 0.003 (0.003)\tLoss 1.0837 (1.2046)\tPrec@1 63.281 (58.877)\n",
      "Epoch: [76][234/390]\tTime 0.002 (0.003)\tLoss 1.1929 (1.2124)\tPrec@1 57.812 (58.394)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 1.0945 (1.2157)\tPrec@1 64.062 (58.334)\n",
      "Epoch: [76][390/390]\tTime 0.003 (0.003)\tLoss 1.0114 (1.2206)\tPrec@1 70.000 (58.116)\n",
      "EPOCH: 76 train Results: Prec@1 58.116 Loss: 1.2206\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2375 (1.2375)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4898 (1.2979)\tPrec@1 31.250 (54.750)\n",
      "EPOCH: 76 val Results: Prec@1 54.750 Loss: 1.2979\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [77][0/390]\tTime 0.002 (0.002)\tLoss 1.1870 (1.1870)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [77][78/390]\tTime 0.002 (0.004)\tLoss 1.2368 (1.1859)\tPrec@1 57.031 (59.494)\n",
      "Epoch: [77][156/390]\tTime 0.011 (0.004)\tLoss 1.2279 (1.2013)\tPrec@1 57.812 (58.808)\n",
      "Epoch: [77][234/390]\tTime 0.005 (0.005)\tLoss 1.1296 (1.2109)\tPrec@1 60.938 (58.461)\n",
      "Epoch: [77][312/390]\tTime 0.002 (0.005)\tLoss 1.2171 (1.2190)\tPrec@1 57.031 (58.122)\n",
      "Epoch: [77][390/390]\tTime 0.003 (0.005)\tLoss 1.3326 (1.2261)\tPrec@1 57.500 (57.864)\n",
      "EPOCH: 77 train Results: Prec@1 57.864 Loss: 1.2261\n",
      "Test: [0/78]\tTime 0.019 (0.019)\tLoss 1.1571 (1.1571)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4196 (1.2940)\tPrec@1 43.750 (54.440)\n",
      "EPOCH: 77 val Results: Prec@1 54.440 Loss: 1.2940\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [78][0/390]\tTime 0.006 (0.006)\tLoss 1.1271 (1.1271)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [78][78/390]\tTime 0.008 (0.006)\tLoss 1.2221 (1.1773)\tPrec@1 55.469 (60.364)\n",
      "Epoch: [78][156/390]\tTime 0.004 (0.005)\tLoss 1.2387 (1.1961)\tPrec@1 60.156 (59.410)\n",
      "Epoch: [78][234/390]\tTime 0.002 (0.004)\tLoss 1.2183 (1.2099)\tPrec@1 60.156 (58.816)\n",
      "Epoch: [78][312/390]\tTime 0.003 (0.004)\tLoss 1.2467 (1.2202)\tPrec@1 53.125 (58.122)\n",
      "Epoch: [78][390/390]\tTime 0.002 (0.004)\tLoss 1.2231 (1.2247)\tPrec@1 57.500 (57.890)\n",
      "EPOCH: 78 train Results: Prec@1 57.890 Loss: 1.2247\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1574 (1.1574)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2957 (1.2783)\tPrec@1 50.000 (55.330)\n",
      "EPOCH: 78 val Results: Prec@1 55.330 Loss: 1.2783\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [79][0/390]\tTime 0.004 (0.004)\tLoss 1.1086 (1.1086)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [79][78/390]\tTime 0.005 (0.003)\tLoss 1.0988 (1.1570)\tPrec@1 64.844 (61.402)\n",
      "Epoch: [79][156/390]\tTime 0.002 (0.003)\tLoss 1.1651 (1.1828)\tPrec@1 61.719 (60.231)\n",
      "Epoch: [79][234/390]\tTime 0.002 (0.003)\tLoss 1.2097 (1.1970)\tPrec@1 63.281 (59.495)\n",
      "Epoch: [79][312/390]\tTime 0.003 (0.003)\tLoss 1.2736 (1.2104)\tPrec@1 57.031 (58.893)\n",
      "Epoch: [79][390/390]\tTime 0.003 (0.003)\tLoss 1.2353 (1.2213)\tPrec@1 60.000 (58.220)\n",
      "EPOCH: 79 train Results: Prec@1 58.220 Loss: 1.2213\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1906 (1.1906)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4219 (1.2851)\tPrec@1 31.250 (55.030)\n",
      "EPOCH: 79 val Results: Prec@1 55.030 Loss: 1.2851\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [80][0/390]\tTime 0.004 (0.004)\tLoss 1.1269 (1.1269)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [80][78/390]\tTime 0.002 (0.003)\tLoss 1.1436 (1.1810)\tPrec@1 60.938 (60.018)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.1525 (1.1984)\tPrec@1 60.156 (58.952)\n",
      "Epoch: [80][234/390]\tTime 0.003 (0.003)\tLoss 1.1254 (1.2095)\tPrec@1 60.156 (58.703)\n",
      "Epoch: [80][312/390]\tTime 0.005 (0.004)\tLoss 1.3319 (1.2173)\tPrec@1 52.344 (58.432)\n",
      "Epoch: [80][390/390]\tTime 0.001 (0.004)\tLoss 1.2650 (1.2223)\tPrec@1 57.500 (58.222)\n",
      "EPOCH: 80 train Results: Prec@1 58.222 Loss: 1.2223\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2100 (1.2100)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4843 (1.2966)\tPrec@1 37.500 (54.600)\n",
      "EPOCH: 80 val Results: Prec@1 54.600 Loss: 1.2966\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [81][0/390]\tTime 0.003 (0.003)\tLoss 1.1159 (1.1159)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [81][78/390]\tTime 0.005 (0.004)\tLoss 1.1413 (1.1767)\tPrec@1 60.156 (60.127)\n",
      "Epoch: [81][156/390]\tTime 0.002 (0.004)\tLoss 1.0764 (1.1920)\tPrec@1 66.406 (59.733)\n",
      "Epoch: [81][234/390]\tTime 0.006 (0.004)\tLoss 1.1674 (1.1997)\tPrec@1 63.281 (59.302)\n",
      "Epoch: [81][312/390]\tTime 0.002 (0.004)\tLoss 1.3966 (1.2135)\tPrec@1 48.438 (58.579)\n",
      "Epoch: [81][390/390]\tTime 0.003 (0.004)\tLoss 1.1650 (1.2210)\tPrec@1 61.250 (58.150)\n",
      "EPOCH: 81 train Results: Prec@1 58.150 Loss: 1.2210\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1670 (1.1670)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4011 (1.2910)\tPrec@1 25.000 (54.880)\n",
      "EPOCH: 81 val Results: Prec@1 54.880 Loss: 1.2910\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [82][0/390]\tTime 0.005 (0.005)\tLoss 1.2545 (1.2545)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [82][78/390]\tTime 0.003 (0.003)\tLoss 1.1658 (1.1773)\tPrec@1 64.844 (60.443)\n",
      "Epoch: [82][156/390]\tTime 0.003 (0.003)\tLoss 1.3088 (1.1890)\tPrec@1 57.031 (59.559)\n",
      "Epoch: [82][234/390]\tTime 0.004 (0.003)\tLoss 1.2613 (1.2013)\tPrec@1 55.469 (58.893)\n",
      "Epoch: [82][312/390]\tTime 0.003 (0.003)\tLoss 1.1494 (1.2077)\tPrec@1 62.500 (58.751)\n",
      "Epoch: [82][390/390]\tTime 0.001 (0.003)\tLoss 1.1570 (1.2202)\tPrec@1 58.750 (58.260)\n",
      "EPOCH: 82 train Results: Prec@1 58.260 Loss: 1.2202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1984 (1.1984)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5185 (1.2835)\tPrec@1 31.250 (54.630)\n",
      "EPOCH: 82 val Results: Prec@1 54.630 Loss: 1.2835\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [83][0/390]\tTime 0.003 (0.003)\tLoss 1.1720 (1.1720)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [83][78/390]\tTime 0.002 (0.004)\tLoss 1.3529 (1.1829)\tPrec@1 46.094 (60.216)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.3733 (1.1985)\tPrec@1 48.438 (59.171)\n",
      "Epoch: [83][234/390]\tTime 0.002 (0.003)\tLoss 1.1533 (1.2067)\tPrec@1 61.719 (58.607)\n",
      "Epoch: [83][312/390]\tTime 0.002 (0.003)\tLoss 1.2086 (1.2136)\tPrec@1 58.594 (58.382)\n",
      "Epoch: [83][390/390]\tTime 0.002 (0.004)\tLoss 1.3812 (1.2198)\tPrec@1 47.500 (58.044)\n",
      "EPOCH: 83 train Results: Prec@1 58.044 Loss: 1.2198\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1565 (1.1565)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6991 (1.2897)\tPrec@1 31.250 (55.010)\n",
      "EPOCH: 83 val Results: Prec@1 55.010 Loss: 1.2897\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [84][0/390]\tTime 0.002 (0.002)\tLoss 1.0178 (1.0178)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [84][78/390]\tTime 0.011 (0.005)\tLoss 1.0432 (1.1856)\tPrec@1 71.094 (60.403)\n",
      "Epoch: [84][156/390]\tTime 0.004 (0.007)\tLoss 1.2356 (1.2021)\tPrec@1 58.594 (59.261)\n",
      "Epoch: [84][234/390]\tTime 0.012 (0.006)\tLoss 1.1898 (1.2134)\tPrec@1 59.375 (58.507)\n",
      "Epoch: [84][312/390]\tTime 0.006 (0.006)\tLoss 1.1620 (1.2166)\tPrec@1 60.938 (58.419)\n",
      "Epoch: [84][390/390]\tTime 0.001 (0.006)\tLoss 1.4053 (1.2185)\tPrec@1 51.250 (58.360)\n",
      "EPOCH: 84 train Results: Prec@1 58.360 Loss: 1.2185\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1384 (1.1384)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3985 (1.2960)\tPrec@1 31.250 (55.000)\n",
      "EPOCH: 84 val Results: Prec@1 55.000 Loss: 1.2960\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [85][0/390]\tTime 0.005 (0.005)\tLoss 1.1238 (1.1238)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [85][78/390]\tTime 0.002 (0.003)\tLoss 1.0400 (1.1745)\tPrec@1 68.750 (60.038)\n",
      "Epoch: [85][156/390]\tTime 0.004 (0.003)\tLoss 1.0419 (1.1937)\tPrec@1 64.062 (59.012)\n",
      "Epoch: [85][234/390]\tTime 0.002 (0.003)\tLoss 1.3643 (1.2030)\tPrec@1 50.000 (58.620)\n",
      "Epoch: [85][312/390]\tTime 0.003 (0.003)\tLoss 1.1963 (1.2140)\tPrec@1 59.375 (58.232)\n",
      "Epoch: [85][390/390]\tTime 0.001 (0.003)\tLoss 1.3064 (1.2215)\tPrec@1 60.000 (57.906)\n",
      "EPOCH: 85 train Results: Prec@1 57.906 Loss: 1.2215\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1801 (1.1801)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6422 (1.2856)\tPrec@1 18.750 (55.060)\n",
      "EPOCH: 85 val Results: Prec@1 55.060 Loss: 1.2856\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [86][0/390]\tTime 0.004 (0.004)\tLoss 1.1246 (1.1246)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [86][78/390]\tTime 0.004 (0.003)\tLoss 1.1671 (1.1889)\tPrec@1 63.281 (60.107)\n",
      "Epoch: [86][156/390]\tTime 0.003 (0.003)\tLoss 1.1395 (1.1938)\tPrec@1 63.281 (59.465)\n",
      "Epoch: [86][234/390]\tTime 0.002 (0.003)\tLoss 1.1793 (1.2037)\tPrec@1 57.031 (58.823)\n",
      "Epoch: [86][312/390]\tTime 0.002 (0.003)\tLoss 1.3778 (1.2123)\tPrec@1 50.000 (58.362)\n",
      "Epoch: [86][390/390]\tTime 0.001 (0.003)\tLoss 1.3843 (1.2217)\tPrec@1 52.500 (57.844)\n",
      "EPOCH: 86 train Results: Prec@1 57.844 Loss: 1.2217\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2027 (1.2027)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4265 (1.2825)\tPrec@1 37.500 (54.800)\n",
      "EPOCH: 86 val Results: Prec@1 54.800 Loss: 1.2825\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [87][0/390]\tTime 0.002 (0.002)\tLoss 1.0880 (1.0880)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [87][78/390]\tTime 0.003 (0.003)\tLoss 1.0650 (1.1710)\tPrec@1 66.406 (60.779)\n",
      "Epoch: [87][156/390]\tTime 0.004 (0.004)\tLoss 1.1953 (1.1949)\tPrec@1 64.844 (59.400)\n",
      "Epoch: [87][234/390]\tTime 0.002 (0.004)\tLoss 1.1286 (1.2071)\tPrec@1 62.500 (58.703)\n",
      "Epoch: [87][312/390]\tTime 0.004 (0.003)\tLoss 1.2432 (1.2156)\tPrec@1 56.250 (58.449)\n",
      "Epoch: [87][390/390]\tTime 0.003 (0.003)\tLoss 1.3201 (1.2208)\tPrec@1 51.250 (58.134)\n",
      "EPOCH: 87 train Results: Prec@1 58.134 Loss: 1.2208\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1682 (1.1682)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3881 (1.2864)\tPrec@1 50.000 (54.980)\n",
      "EPOCH: 87 val Results: Prec@1 54.980 Loss: 1.2864\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [88][0/390]\tTime 0.002 (0.002)\tLoss 1.1467 (1.1467)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [88][78/390]\tTime 0.005 (0.003)\tLoss 1.0675 (1.1653)\tPrec@1 64.062 (61.155)\n",
      "Epoch: [88][156/390]\tTime 0.002 (0.004)\tLoss 1.2534 (1.1900)\tPrec@1 54.688 (59.758)\n",
      "Epoch: [88][234/390]\tTime 0.007 (0.004)\tLoss 1.2550 (1.2028)\tPrec@1 59.375 (59.053)\n",
      "Epoch: [88][312/390]\tTime 0.008 (0.005)\tLoss 1.1637 (1.2134)\tPrec@1 57.812 (58.491)\n",
      "Epoch: [88][390/390]\tTime 0.002 (0.005)\tLoss 1.5171 (1.2200)\tPrec@1 47.500 (58.186)\n",
      "EPOCH: 88 train Results: Prec@1 58.186 Loss: 1.2200\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1984 (1.1984)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.003)\tLoss 1.3685 (1.2830)\tPrec@1 50.000 (55.130)\n",
      "EPOCH: 88 val Results: Prec@1 55.130 Loss: 1.2830\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [89][0/390]\tTime 0.015 (0.015)\tLoss 1.1554 (1.1554)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.006)\tLoss 1.1667 (1.1796)\tPrec@1 66.406 (59.840)\n",
      "Epoch: [89][156/390]\tTime 0.005 (0.006)\tLoss 1.2060 (1.1919)\tPrec@1 60.938 (59.285)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.006)\tLoss 1.2310 (1.2033)\tPrec@1 60.938 (58.850)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.006)\tLoss 1.2539 (1.2134)\tPrec@1 59.375 (58.484)\n",
      "Epoch: [89][390/390]\tTime 0.003 (0.005)\tLoss 1.3624 (1.2202)\tPrec@1 52.500 (58.188)\n",
      "EPOCH: 89 train Results: Prec@1 58.188 Loss: 1.2202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1947 (1.1947)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5457 (1.2910)\tPrec@1 31.250 (54.570)\n",
      "EPOCH: 89 val Results: Prec@1 54.570 Loss: 1.2910\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [90][0/390]\tTime 0.002 (0.002)\tLoss 1.1688 (1.1688)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [90][78/390]\tTime 0.002 (0.004)\tLoss 1.1404 (1.1681)\tPrec@1 62.500 (60.117)\n",
      "Epoch: [90][156/390]\tTime 0.002 (0.003)\tLoss 1.2879 (1.1881)\tPrec@1 56.250 (59.400)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.2679 (1.2014)\tPrec@1 57.031 (58.624)\n",
      "Epoch: [90][312/390]\tTime 0.003 (0.003)\tLoss 1.2457 (1.2071)\tPrec@1 56.250 (58.499)\n",
      "Epoch: [90][390/390]\tTime 0.002 (0.003)\tLoss 1.1781 (1.2161)\tPrec@1 58.750 (58.164)\n",
      "EPOCH: 90 train Results: Prec@1 58.164 Loss: 1.2161\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1212 (1.1212)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.7320 (1.2916)\tPrec@1 37.500 (54.850)\n",
      "EPOCH: 90 val Results: Prec@1 54.850 Loss: 1.2916\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [91][0/390]\tTime 0.002 (0.002)\tLoss 1.3601 (1.3601)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [91][78/390]\tTime 0.006 (0.003)\tLoss 1.1413 (1.1704)\tPrec@1 63.281 (60.225)\n",
      "Epoch: [91][156/390]\tTime 0.003 (0.003)\tLoss 1.3410 (1.1874)\tPrec@1 52.344 (59.420)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.004)\tLoss 1.1894 (1.1957)\tPrec@1 53.906 (58.969)\n",
      "Epoch: [91][312/390]\tTime 0.002 (0.004)\tLoss 1.1773 (1.2065)\tPrec@1 57.812 (58.664)\n",
      "Epoch: [91][390/390]\tTime 0.002 (0.004)\tLoss 1.2531 (1.2166)\tPrec@1 57.500 (58.144)\n",
      "EPOCH: 91 train Results: Prec@1 58.144 Loss: 1.2166\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1439 (1.1439)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3233 (1.2897)\tPrec@1 37.500 (54.690)\n",
      "EPOCH: 91 val Results: Prec@1 54.690 Loss: 1.2897\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [92][0/390]\tTime 0.002 (0.002)\tLoss 1.2117 (1.2117)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.005)\tLoss 1.2140 (1.1739)\tPrec@1 57.031 (59.593)\n",
      "Epoch: [92][156/390]\tTime 0.003 (0.005)\tLoss 1.1708 (1.1937)\tPrec@1 63.281 (59.221)\n",
      "Epoch: [92][234/390]\tTime 0.006 (0.004)\tLoss 1.2140 (1.2047)\tPrec@1 55.469 (58.627)\n",
      "Epoch: [92][312/390]\tTime 0.004 (0.005)\tLoss 1.2680 (1.2114)\tPrec@1 55.469 (58.164)\n",
      "Epoch: [92][390/390]\tTime 0.002 (0.004)\tLoss 1.2565 (1.2186)\tPrec@1 52.500 (57.902)\n",
      "EPOCH: 92 train Results: Prec@1 57.902 Loss: 1.2186\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1620 (1.1620)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4954 (1.2877)\tPrec@1 43.750 (54.790)\n",
      "EPOCH: 92 val Results: Prec@1 54.790 Loss: 1.2877\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [93][0/390]\tTime 0.002 (0.002)\tLoss 1.2096 (1.2096)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.004)\tLoss 1.1519 (1.1561)\tPrec@1 61.719 (60.581)\n",
      "Epoch: [93][156/390]\tTime 0.002 (0.004)\tLoss 1.1538 (1.1768)\tPrec@1 58.594 (59.688)\n",
      "Epoch: [93][234/390]\tTime 0.002 (0.005)\tLoss 1.3539 (1.1960)\tPrec@1 53.125 (58.916)\n",
      "Epoch: [93][312/390]\tTime 0.020 (0.006)\tLoss 1.3825 (1.2039)\tPrec@1 53.125 (58.432)\n",
      "Epoch: [93][390/390]\tTime 0.004 (0.006)\tLoss 1.2267 (1.2139)\tPrec@1 61.250 (57.976)\n",
      "EPOCH: 93 train Results: Prec@1 57.976 Loss: 1.2139\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1565 (1.1565)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.004)\tLoss 1.3425 (1.2910)\tPrec@1 37.500 (54.610)\n",
      "EPOCH: 93 val Results: Prec@1 54.610 Loss: 1.2910\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [94][0/390]\tTime 0.007 (0.007)\tLoss 1.0561 (1.0561)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [94][78/390]\tTime 0.007 (0.008)\tLoss 1.2078 (1.1661)\tPrec@1 62.500 (60.235)\n",
      "Epoch: [94][156/390]\tTime 0.004 (0.007)\tLoss 1.1455 (1.1855)\tPrec@1 64.062 (59.435)\n",
      "Epoch: [94][234/390]\tTime 0.002 (0.007)\tLoss 1.1099 (1.2012)\tPrec@1 61.719 (58.680)\n",
      "Epoch: [94][312/390]\tTime 0.005 (0.007)\tLoss 1.2954 (1.2103)\tPrec@1 55.469 (58.302)\n",
      "Epoch: [94][390/390]\tTime 0.047 (0.007)\tLoss 1.4397 (1.2179)\tPrec@1 56.250 (58.100)\n",
      "EPOCH: 94 train Results: Prec@1 58.100 Loss: 1.2179\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1726 (1.1726)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3337 (1.2802)\tPrec@1 50.000 (55.560)\n",
      "EPOCH: 94 val Results: Prec@1 55.560 Loss: 1.2802\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [95][0/390]\tTime 0.005 (0.005)\tLoss 1.1395 (1.1395)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [95][78/390]\tTime 0.002 (0.005)\tLoss 1.2526 (1.1622)\tPrec@1 50.781 (60.651)\n",
      "Epoch: [95][156/390]\tTime 0.002 (0.006)\tLoss 1.0843 (1.1821)\tPrec@1 64.062 (59.619)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.006)\tLoss 1.2068 (1.1939)\tPrec@1 59.375 (59.212)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.007)\tLoss 1.1470 (1.2064)\tPrec@1 63.281 (58.631)\n",
      "Epoch: [95][390/390]\tTime 0.003 (0.007)\tLoss 1.2551 (1.2160)\tPrec@1 58.750 (58.178)\n",
      "EPOCH: 95 train Results: Prec@1 58.178 Loss: 1.2160\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1500 (1.1500)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4995 (1.2897)\tPrec@1 37.500 (54.860)\n",
      "EPOCH: 95 val Results: Prec@1 54.860 Loss: 1.2897\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [96][0/390]\tTime 0.003 (0.003)\tLoss 1.1806 (1.1806)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [96][78/390]\tTime 0.004 (0.005)\tLoss 1.2334 (1.1792)\tPrec@1 60.156 (59.919)\n",
      "Epoch: [96][156/390]\tTime 0.011 (0.005)\tLoss 1.1243 (1.1963)\tPrec@1 63.281 (59.440)\n",
      "Epoch: [96][234/390]\tTime 0.009 (0.006)\tLoss 1.2533 (1.2092)\tPrec@1 52.344 (58.693)\n",
      "Epoch: [96][312/390]\tTime 0.008 (0.005)\tLoss 1.1252 (1.2113)\tPrec@1 61.719 (58.594)\n",
      "Epoch: [96][390/390]\tTime 0.003 (0.005)\tLoss 1.2761 (1.2168)\tPrec@1 57.500 (58.288)\n",
      "EPOCH: 96 train Results: Prec@1 58.288 Loss: 1.2168\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1495 (1.1495)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2610 (1.2847)\tPrec@1 56.250 (55.250)\n",
      "EPOCH: 96 val Results: Prec@1 55.250 Loss: 1.2847\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [97][0/390]\tTime 0.006 (0.006)\tLoss 1.1519 (1.1519)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [97][78/390]\tTime 0.009 (0.004)\tLoss 1.2309 (1.1729)\tPrec@1 57.812 (60.156)\n",
      "Epoch: [97][156/390]\tTime 0.008 (0.005)\tLoss 1.3189 (1.1907)\tPrec@1 58.594 (59.390)\n",
      "Epoch: [97][234/390]\tTime 0.002 (0.005)\tLoss 1.3471 (1.2006)\tPrec@1 50.781 (58.936)\n",
      "Epoch: [97][312/390]\tTime 0.008 (0.005)\tLoss 1.2999 (1.2081)\tPrec@1 52.344 (58.519)\n",
      "Epoch: [97][390/390]\tTime 0.001 (0.005)\tLoss 1.0910 (1.2160)\tPrec@1 63.750 (58.142)\n",
      "EPOCH: 97 train Results: Prec@1 58.142 Loss: 1.2160\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1978 (1.1978)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3354 (1.2855)\tPrec@1 37.500 (55.250)\n",
      "EPOCH: 97 val Results: Prec@1 55.250 Loss: 1.2855\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [98][0/390]\tTime 0.011 (0.011)\tLoss 1.1680 (1.1680)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [98][78/390]\tTime 0.005 (0.005)\tLoss 1.0599 (1.1732)\tPrec@1 66.406 (59.939)\n",
      "Epoch: [98][156/390]\tTime 0.004 (0.006)\tLoss 1.2767 (1.1893)\tPrec@1 57.812 (59.479)\n",
      "Epoch: [98][234/390]\tTime 0.005 (0.005)\tLoss 1.1609 (1.1964)\tPrec@1 58.594 (59.069)\n",
      "Epoch: [98][312/390]\tTime 0.003 (0.005)\tLoss 1.4437 (1.2049)\tPrec@1 43.750 (58.691)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.004)\tLoss 1.2146 (1.2120)\tPrec@1 58.750 (58.408)\n",
      "EPOCH: 98 train Results: Prec@1 58.408 Loss: 1.2120\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1382 (1.1382)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5755 (1.2884)\tPrec@1 31.250 (54.830)\n",
      "EPOCH: 98 val Results: Prec@1 54.830 Loss: 1.2884\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [99][0/390]\tTime 0.004 (0.004)\tLoss 1.1804 (1.1804)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.005)\tLoss 1.0946 (1.1746)\tPrec@1 61.719 (60.750)\n",
      "Epoch: [99][156/390]\tTime 0.003 (0.004)\tLoss 1.1861 (1.1891)\tPrec@1 61.719 (59.564)\n",
      "Epoch: [99][234/390]\tTime 0.003 (0.004)\tLoss 1.4044 (1.1942)\tPrec@1 45.312 (59.099)\n",
      "Epoch: [99][312/390]\tTime 0.004 (0.004)\tLoss 1.2363 (1.2044)\tPrec@1 53.906 (58.654)\n",
      "Epoch: [99][390/390]\tTime 0.010 (0.004)\tLoss 1.0471 (1.2122)\tPrec@1 62.500 (58.210)\n",
      "EPOCH: 99 train Results: Prec@1 58.210 Loss: 1.2122\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1731 (1.1731)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.006 (0.002)\tLoss 1.5863 (1.2824)\tPrec@1 37.500 (54.820)\n",
      "EPOCH: 99 val Results: Prec@1 54.820 Loss: 1.2824\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [100][0/390]\tTime 0.005 (0.005)\tLoss 1.0897 (1.0897)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [100][78/390]\tTime 0.015 (0.007)\tLoss 1.2757 (1.1747)\tPrec@1 58.594 (59.504)\n",
      "Epoch: [100][156/390]\tTime 0.003 (0.007)\tLoss 1.2149 (1.1887)\tPrec@1 60.156 (59.256)\n",
      "Epoch: [100][234/390]\tTime 0.002 (0.006)\tLoss 1.2852 (1.2008)\tPrec@1 55.469 (58.743)\n",
      "Epoch: [100][312/390]\tTime 0.015 (0.006)\tLoss 1.1761 (1.2096)\tPrec@1 58.594 (58.364)\n",
      "Epoch: [100][390/390]\tTime 0.001 (0.006)\tLoss 1.2319 (1.2123)\tPrec@1 62.500 (58.268)\n",
      "EPOCH: 100 train Results: Prec@1 58.268 Loss: 1.2123\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1338 (1.1338)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4831 (1.2759)\tPrec@1 31.250 (55.140)\n",
      "EPOCH: 100 val Results: Prec@1 55.140 Loss: 1.2759\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [101][0/390]\tTime 0.003 (0.003)\tLoss 1.1786 (1.1786)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [101][78/390]\tTime 0.008 (0.006)\tLoss 1.1918 (1.1785)\tPrec@1 54.688 (59.553)\n",
      "Epoch: [101][156/390]\tTime 0.002 (0.005)\tLoss 1.2465 (1.1945)\tPrec@1 58.594 (58.644)\n",
      "Epoch: [101][234/390]\tTime 0.003 (0.005)\tLoss 1.2875 (1.2030)\tPrec@1 57.812 (58.441)\n",
      "Epoch: [101][312/390]\tTime 0.004 (0.005)\tLoss 1.1731 (1.2049)\tPrec@1 60.938 (58.227)\n",
      "Epoch: [101][390/390]\tTime 0.003 (0.005)\tLoss 1.2245 (1.2135)\tPrec@1 61.250 (58.034)\n",
      "EPOCH: 101 train Results: Prec@1 58.034 Loss: 1.2135\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2265 (1.2265)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2440 (1.2928)\tPrec@1 43.750 (54.540)\n",
      "EPOCH: 101 val Results: Prec@1 54.540 Loss: 1.2928\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [102][0/390]\tTime 0.008 (0.008)\tLoss 1.1109 (1.1109)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [102][78/390]\tTime 0.004 (0.007)\tLoss 1.1554 (1.1608)\tPrec@1 54.688 (59.939)\n",
      "Epoch: [102][156/390]\tTime 0.004 (0.006)\tLoss 1.1650 (1.1805)\tPrec@1 61.719 (59.275)\n",
      "Epoch: [102][234/390]\tTime 0.007 (0.006)\tLoss 1.2473 (1.1908)\tPrec@1 60.156 (58.916)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.006)\tLoss 1.2196 (1.2017)\tPrec@1 57.812 (58.686)\n",
      "Epoch: [102][390/390]\tTime 0.002 (0.005)\tLoss 1.1497 (1.2074)\tPrec@1 62.500 (58.426)\n",
      "EPOCH: 102 train Results: Prec@1 58.426 Loss: 1.2074\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1640 (1.1640)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4851 (1.2788)\tPrec@1 37.500 (55.200)\n",
      "EPOCH: 102 val Results: Prec@1 55.200 Loss: 1.2788\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [103][0/390]\tTime 0.006 (0.006)\tLoss 1.1306 (1.1306)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [103][78/390]\tTime 0.008 (0.007)\tLoss 1.1751 (1.1753)\tPrec@1 58.594 (60.176)\n",
      "Epoch: [103][156/390]\tTime 0.005 (0.006)\tLoss 1.2385 (1.1881)\tPrec@1 57.812 (59.410)\n",
      "Epoch: [103][234/390]\tTime 0.006 (0.006)\tLoss 1.3004 (1.1993)\tPrec@1 60.156 (58.840)\n",
      "Epoch: [103][312/390]\tTime 0.016 (0.006)\tLoss 1.4081 (1.2064)\tPrec@1 50.000 (58.476)\n",
      "Epoch: [103][390/390]\tTime 0.002 (0.006)\tLoss 1.3046 (1.2112)\tPrec@1 57.500 (58.222)\n",
      "EPOCH: 103 train Results: Prec@1 58.222 Loss: 1.2112\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1998 (1.1998)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4556 (1.2911)\tPrec@1 43.750 (54.640)\n",
      "EPOCH: 103 val Results: Prec@1 54.640 Loss: 1.2911\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [104][0/390]\tTime 0.022 (0.022)\tLoss 1.1953 (1.1953)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [104][78/390]\tTime 0.003 (0.005)\tLoss 1.2286 (1.1608)\tPrec@1 61.719 (60.591)\n",
      "Epoch: [104][156/390]\tTime 0.002 (0.004)\tLoss 1.2484 (1.1812)\tPrec@1 54.688 (59.440)\n",
      "Epoch: [104][234/390]\tTime 0.003 (0.004)\tLoss 1.2059 (1.1917)\tPrec@1 59.375 (59.109)\n",
      "Epoch: [104][312/390]\tTime 0.003 (0.004)\tLoss 1.1870 (1.2045)\tPrec@1 59.375 (58.494)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.004)\tLoss 1.1691 (1.2117)\tPrec@1 55.000 (58.224)\n",
      "EPOCH: 104 train Results: Prec@1 58.224 Loss: 1.2117\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1930 (1.1930)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3628 (1.2780)\tPrec@1 31.250 (55.490)\n",
      "EPOCH: 104 val Results: Prec@1 55.490 Loss: 1.2780\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [105][0/390]\tTime 0.007 (0.007)\tLoss 1.1170 (1.1170)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [105][78/390]\tTime 0.007 (0.005)\tLoss 1.2164 (1.1766)\tPrec@1 53.125 (59.830)\n",
      "Epoch: [105][156/390]\tTime 0.007 (0.005)\tLoss 1.2105 (1.1818)\tPrec@1 53.125 (59.415)\n",
      "Epoch: [105][234/390]\tTime 0.006 (0.005)\tLoss 1.3626 (1.1914)\tPrec@1 52.344 (58.860)\n",
      "Epoch: [105][312/390]\tTime 0.003 (0.004)\tLoss 1.2260 (1.1985)\tPrec@1 55.469 (58.546)\n",
      "Epoch: [105][390/390]\tTime 0.002 (0.004)\tLoss 1.2246 (1.2107)\tPrec@1 60.000 (58.118)\n",
      "EPOCH: 105 train Results: Prec@1 58.118 Loss: 1.2107\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1446 (1.1446)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.006 (0.001)\tLoss 1.3497 (1.2858)\tPrec@1 43.750 (54.510)\n",
      "EPOCH: 105 val Results: Prec@1 54.510 Loss: 1.2858\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [106][0/390]\tTime 0.021 (0.021)\tLoss 1.1035 (1.1035)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [106][78/390]\tTime 0.002 (0.004)\tLoss 1.0591 (1.1582)\tPrec@1 67.969 (61.086)\n",
      "Epoch: [106][156/390]\tTime 0.004 (0.004)\tLoss 1.2177 (1.1782)\tPrec@1 57.031 (59.788)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.003)\tLoss 1.1462 (1.1954)\tPrec@1 60.938 (58.926)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.004)\tLoss 1.1887 (1.2071)\tPrec@1 60.938 (58.456)\n",
      "Epoch: [106][390/390]\tTime 0.013 (0.004)\tLoss 1.2763 (1.2135)\tPrec@1 56.250 (58.208)\n",
      "EPOCH: 106 train Results: Prec@1 58.208 Loss: 1.2135\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1758 (1.1758)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.002)\tLoss 1.5214 (1.2892)\tPrec@1 25.000 (54.650)\n",
      "EPOCH: 106 val Results: Prec@1 54.650 Loss: 1.2892\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [107][0/390]\tTime 0.005 (0.005)\tLoss 1.0744 (1.0744)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [107][78/390]\tTime 0.008 (0.004)\tLoss 1.2539 (1.1682)\tPrec@1 57.812 (60.502)\n",
      "Epoch: [107][156/390]\tTime 0.009 (0.005)\tLoss 1.3784 (1.1767)\tPrec@1 50.000 (59.748)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.004)\tLoss 1.2151 (1.1863)\tPrec@1 60.938 (59.418)\n",
      "Epoch: [107][312/390]\tTime 0.002 (0.004)\tLoss 1.3098 (1.2020)\tPrec@1 55.469 (58.823)\n",
      "Epoch: [107][390/390]\tTime 0.001 (0.004)\tLoss 1.0614 (1.2128)\tPrec@1 66.250 (58.252)\n",
      "EPOCH: 107 train Results: Prec@1 58.252 Loss: 1.2128\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1393 (1.1393)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6151 (1.2814)\tPrec@1 31.250 (55.130)\n",
      "EPOCH: 107 val Results: Prec@1 55.130 Loss: 1.2814\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [108][0/390]\tTime 0.003 (0.003)\tLoss 1.2009 (1.2009)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [108][78/390]\tTime 0.004 (0.004)\tLoss 1.2021 (1.1520)\tPrec@1 60.938 (61.116)\n",
      "Epoch: [108][156/390]\tTime 0.004 (0.004)\tLoss 1.2443 (1.1737)\tPrec@1 59.375 (60.186)\n",
      "Epoch: [108][234/390]\tTime 0.010 (0.004)\tLoss 1.2571 (1.1873)\tPrec@1 52.344 (59.661)\n",
      "Epoch: [108][312/390]\tTime 0.002 (0.004)\tLoss 1.2378 (1.1971)\tPrec@1 55.469 (59.053)\n",
      "Epoch: [108][390/390]\tTime 0.002 (0.004)\tLoss 1.1940 (1.2078)\tPrec@1 62.500 (58.622)\n",
      "EPOCH: 108 train Results: Prec@1 58.622 Loss: 1.2078\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1840 (1.1840)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5774 (1.2979)\tPrec@1 31.250 (54.150)\n",
      "EPOCH: 108 val Results: Prec@1 54.150 Loss: 1.2979\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [109][0/390]\tTime 0.002 (0.002)\tLoss 1.2194 (1.2194)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [109][78/390]\tTime 0.002 (0.004)\tLoss 1.2487 (1.1745)\tPrec@1 55.469 (60.275)\n",
      "Epoch: [109][156/390]\tTime 0.002 (0.003)\tLoss 1.2402 (1.1928)\tPrec@1 61.719 (59.440)\n",
      "Epoch: [109][234/390]\tTime 0.007 (0.004)\tLoss 1.0633 (1.1989)\tPrec@1 65.625 (58.993)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.004)\tLoss 1.1703 (1.2043)\tPrec@1 61.719 (58.589)\n",
      "Epoch: [109][390/390]\tTime 0.014 (0.006)\tLoss 1.2947 (1.2092)\tPrec@1 51.250 (58.374)\n",
      "EPOCH: 109 train Results: Prec@1 58.374 Loss: 1.2092\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1520 (1.1520)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3713 (1.2788)\tPrec@1 37.500 (55.190)\n",
      "EPOCH: 109 val Results: Prec@1 55.190 Loss: 1.2788\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [110][0/390]\tTime 0.004 (0.004)\tLoss 1.2609 (1.2609)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.014)\tLoss 1.0998 (1.1624)\tPrec@1 58.594 (60.433)\n",
      "Epoch: [110][156/390]\tTime 0.002 (0.010)\tLoss 1.0840 (1.1805)\tPrec@1 61.719 (59.693)\n",
      "Epoch: [110][234/390]\tTime 0.002 (0.009)\tLoss 1.2838 (1.1875)\tPrec@1 59.375 (59.345)\n",
      "Epoch: [110][312/390]\tTime 0.011 (0.007)\tLoss 1.2851 (1.2025)\tPrec@1 55.469 (58.758)\n",
      "Epoch: [110][390/390]\tTime 0.006 (0.007)\tLoss 1.2996 (1.2063)\tPrec@1 58.750 (58.446)\n",
      "EPOCH: 110 train Results: Prec@1 58.446 Loss: 1.2063\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1593 (1.1593)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5975 (1.2762)\tPrec@1 37.500 (55.290)\n",
      "EPOCH: 110 val Results: Prec@1 55.290 Loss: 1.2762\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [111][0/390]\tTime 0.002 (0.002)\tLoss 1.1834 (1.1834)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [111][78/390]\tTime 0.004 (0.005)\tLoss 1.2223 (1.1708)\tPrec@1 56.250 (60.265)\n",
      "Epoch: [111][156/390]\tTime 0.002 (0.005)\tLoss 1.2315 (1.1891)\tPrec@1 59.375 (59.186)\n",
      "Epoch: [111][234/390]\tTime 0.004 (0.005)\tLoss 1.4086 (1.2031)\tPrec@1 53.906 (58.787)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.005)\tLoss 1.2178 (1.2103)\tPrec@1 57.812 (58.402)\n",
      "Epoch: [111][390/390]\tTime 0.002 (0.005)\tLoss 1.1089 (1.2134)\tPrec@1 66.250 (58.282)\n",
      "EPOCH: 111 train Results: Prec@1 58.282 Loss: 1.2134\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1972 (1.1972)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2500 (1.2900)\tPrec@1 62.500 (54.300)\n",
      "EPOCH: 111 val Results: Prec@1 54.300 Loss: 1.2900\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [112][0/390]\tTime 0.002 (0.002)\tLoss 1.1485 (1.1485)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.1659 (1.1684)\tPrec@1 57.031 (59.939)\n",
      "Epoch: [112][156/390]\tTime 0.007 (0.004)\tLoss 1.2339 (1.1837)\tPrec@1 58.594 (59.534)\n",
      "Epoch: [112][234/390]\tTime 0.003 (0.004)\tLoss 1.1911 (1.1896)\tPrec@1 60.156 (59.325)\n",
      "Epoch: [112][312/390]\tTime 0.002 (0.004)\tLoss 1.1596 (1.2051)\tPrec@1 57.812 (58.669)\n",
      "Epoch: [112][390/390]\tTime 0.002 (0.004)\tLoss 1.4530 (1.2098)\tPrec@1 50.000 (58.402)\n",
      "EPOCH: 112 train Results: Prec@1 58.402 Loss: 1.2098\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1140 (1.1140)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3902 (1.2767)\tPrec@1 25.000 (54.990)\n",
      "EPOCH: 112 val Results: Prec@1 54.990 Loss: 1.2767\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 1.0978 (1.0978)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [113][78/390]\tTime 0.003 (0.004)\tLoss 1.2095 (1.1621)\tPrec@1 60.938 (60.453)\n",
      "Epoch: [113][156/390]\tTime 0.006 (0.004)\tLoss 1.1946 (1.1809)\tPrec@1 57.031 (59.698)\n",
      "Epoch: [113][234/390]\tTime 0.008 (0.004)\tLoss 1.1084 (1.1924)\tPrec@1 64.062 (59.382)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.004)\tLoss 1.1468 (1.2000)\tPrec@1 59.375 (58.983)\n",
      "Epoch: [113][390/390]\tTime 0.002 (0.004)\tLoss 1.2126 (1.2067)\tPrec@1 58.750 (58.650)\n",
      "EPOCH: 113 train Results: Prec@1 58.650 Loss: 1.2067\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2039 (1.2039)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4391 (1.2719)\tPrec@1 43.750 (55.340)\n",
      "EPOCH: 113 val Results: Prec@1 55.340 Loss: 1.2719\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [114][0/390]\tTime 0.002 (0.002)\tLoss 1.2575 (1.2575)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.005)\tLoss 1.0029 (1.1680)\tPrec@1 71.875 (60.463)\n",
      "Epoch: [114][156/390]\tTime 0.003 (0.005)\tLoss 1.1632 (1.1794)\tPrec@1 59.375 (59.947)\n",
      "Epoch: [114][234/390]\tTime 0.002 (0.005)\tLoss 1.1550 (1.1943)\tPrec@1 53.906 (59.249)\n",
      "Epoch: [114][312/390]\tTime 0.002 (0.005)\tLoss 1.1038 (1.1999)\tPrec@1 66.406 (59.031)\n",
      "Epoch: [114][390/390]\tTime 0.011 (0.005)\tLoss 1.2363 (1.2079)\tPrec@1 51.250 (58.604)\n",
      "EPOCH: 114 train Results: Prec@1 58.604 Loss: 1.2079\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1261 (1.1261)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4950 (1.2791)\tPrec@1 43.750 (55.350)\n",
      "EPOCH: 114 val Results: Prec@1 55.350 Loss: 1.2791\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [115][0/390]\tTime 0.004 (0.004)\tLoss 0.9876 (0.9876)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [115][78/390]\tTime 0.003 (0.004)\tLoss 1.1082 (1.1586)\tPrec@1 61.719 (60.799)\n",
      "Epoch: [115][156/390]\tTime 0.005 (0.003)\tLoss 1.3632 (1.1817)\tPrec@1 49.219 (59.564)\n",
      "Epoch: [115][234/390]\tTime 0.004 (0.003)\tLoss 1.3343 (1.1962)\tPrec@1 58.594 (58.996)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.004)\tLoss 1.3628 (1.2037)\tPrec@1 51.562 (58.621)\n",
      "Epoch: [115][390/390]\tTime 0.002 (0.004)\tLoss 1.3926 (1.2077)\tPrec@1 56.250 (58.472)\n",
      "EPOCH: 115 train Results: Prec@1 58.472 Loss: 1.2077\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1793 (1.1793)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5571 (1.2825)\tPrec@1 43.750 (55.040)\n",
      "EPOCH: 115 val Results: Prec@1 55.040 Loss: 1.2825\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [116][0/390]\tTime 0.003 (0.003)\tLoss 1.1699 (1.1699)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [116][78/390]\tTime 0.002 (0.003)\tLoss 1.1726 (1.1681)\tPrec@1 58.594 (59.810)\n",
      "Epoch: [116][156/390]\tTime 0.008 (0.004)\tLoss 1.3291 (1.1841)\tPrec@1 54.688 (59.086)\n",
      "Epoch: [116][234/390]\tTime 0.003 (0.004)\tLoss 1.1984 (1.1937)\tPrec@1 59.375 (58.697)\n",
      "Epoch: [116][312/390]\tTime 0.003 (0.004)\tLoss 1.3400 (1.2037)\tPrec@1 47.656 (58.389)\n",
      "Epoch: [116][390/390]\tTime 0.001 (0.004)\tLoss 1.3752 (1.2097)\tPrec@1 52.500 (58.118)\n",
      "EPOCH: 116 train Results: Prec@1 58.118 Loss: 1.2097\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.1179 (1.1179)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3485 (1.2825)\tPrec@1 25.000 (54.920)\n",
      "EPOCH: 116 val Results: Prec@1 54.920 Loss: 1.2825\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [117][0/390]\tTime 0.003 (0.003)\tLoss 1.2371 (1.2371)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [117][78/390]\tTime 0.003 (0.005)\tLoss 1.2362 (1.1739)\tPrec@1 53.906 (59.800)\n",
      "Epoch: [117][156/390]\tTime 0.007 (0.004)\tLoss 1.2479 (1.1837)\tPrec@1 60.156 (59.340)\n",
      "Epoch: [117][234/390]\tTime 0.004 (0.004)\tLoss 1.2697 (1.1946)\tPrec@1 57.031 (59.023)\n",
      "Epoch: [117][312/390]\tTime 0.002 (0.004)\tLoss 1.1833 (1.2044)\tPrec@1 53.906 (58.549)\n",
      "Epoch: [117][390/390]\tTime 0.001 (0.004)\tLoss 1.2284 (1.2078)\tPrec@1 53.750 (58.436)\n",
      "EPOCH: 117 train Results: Prec@1 58.436 Loss: 1.2078\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1309 (1.1309)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4488 (1.2777)\tPrec@1 37.500 (55.530)\n",
      "EPOCH: 117 val Results: Prec@1 55.530 Loss: 1.2777\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [118][0/390]\tTime 0.004 (0.004)\tLoss 1.1159 (1.1159)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [118][78/390]\tTime 0.012 (0.005)\tLoss 1.1409 (1.1724)\tPrec@1 57.812 (59.840)\n",
      "Epoch: [118][156/390]\tTime 0.010 (0.004)\tLoss 1.1670 (1.1787)\tPrec@1 60.156 (59.698)\n",
      "Epoch: [118][234/390]\tTime 0.003 (0.004)\tLoss 1.2340 (1.1868)\tPrec@1 57.812 (59.219)\n",
      "Epoch: [118][312/390]\tTime 0.004 (0.004)\tLoss 1.2299 (1.1965)\tPrec@1 61.719 (58.766)\n",
      "Epoch: [118][390/390]\tTime 0.002 (0.004)\tLoss 1.3262 (1.2062)\tPrec@1 52.500 (58.334)\n",
      "EPOCH: 118 train Results: Prec@1 58.334 Loss: 1.2062\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1318 (1.1318)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4823 (1.2931)\tPrec@1 43.750 (54.530)\n",
      "EPOCH: 118 val Results: Prec@1 54.530 Loss: 1.2931\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [119][0/390]\tTime 0.002 (0.002)\tLoss 1.2227 (1.2227)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [119][78/390]\tTime 0.003 (0.004)\tLoss 1.2470 (1.1567)\tPrec@1 57.031 (60.710)\n",
      "Epoch: [119][156/390]\tTime 0.003 (0.003)\tLoss 1.2946 (1.1780)\tPrec@1 57.031 (59.460)\n",
      "Epoch: [119][234/390]\tTime 0.002 (0.004)\tLoss 1.2063 (1.1887)\tPrec@1 62.500 (59.082)\n",
      "Epoch: [119][312/390]\tTime 0.003 (0.004)\tLoss 1.2392 (1.1966)\tPrec@1 59.375 (58.686)\n",
      "Epoch: [119][390/390]\tTime 0.002 (0.004)\tLoss 1.3096 (1.2061)\tPrec@1 47.500 (58.338)\n",
      "EPOCH: 119 train Results: Prec@1 58.338 Loss: 1.2061\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1622 (1.1622)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3971 (1.2812)\tPrec@1 37.500 (55.000)\n",
      "EPOCH: 119 val Results: Prec@1 55.000 Loss: 1.2812\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [120][0/390]\tTime 0.003 (0.003)\tLoss 1.0997 (1.0997)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [120][78/390]\tTime 0.010 (0.004)\tLoss 1.2504 (1.1716)\tPrec@1 52.344 (59.968)\n",
      "Epoch: [120][156/390]\tTime 0.002 (0.003)\tLoss 1.0959 (1.1884)\tPrec@1 58.594 (59.350)\n",
      "Epoch: [120][234/390]\tTime 0.004 (0.004)\tLoss 1.2703 (1.1956)\tPrec@1 57.031 (59.082)\n",
      "Epoch: [120][312/390]\tTime 0.004 (0.004)\tLoss 1.3086 (1.1995)\tPrec@1 51.562 (58.858)\n",
      "Epoch: [120][390/390]\tTime 0.003 (0.005)\tLoss 1.1433 (1.2066)\tPrec@1 63.750 (58.516)\n",
      "EPOCH: 120 train Results: Prec@1 58.516 Loss: 1.2066\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1259 (1.1259)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4125 (1.2674)\tPrec@1 43.750 (55.360)\n",
      "EPOCH: 120 val Results: Prec@1 55.360 Loss: 1.2674\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [121][0/390]\tTime 0.003 (0.003)\tLoss 1.0720 (1.0720)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [121][78/390]\tTime 0.004 (0.007)\tLoss 1.1824 (1.1519)\tPrec@1 57.031 (60.601)\n",
      "Epoch: [121][156/390]\tTime 0.018 (0.007)\tLoss 1.1406 (1.1807)\tPrec@1 61.719 (59.544)\n",
      "Epoch: [121][234/390]\tTime 0.005 (0.008)\tLoss 1.2377 (1.1890)\tPrec@1 48.438 (59.016)\n",
      "Epoch: [121][312/390]\tTime 0.003 (0.011)\tLoss 1.1130 (1.1972)\tPrec@1 65.625 (58.739)\n",
      "Epoch: [121][390/390]\tTime 0.001 (0.009)\tLoss 1.0292 (1.2040)\tPrec@1 60.000 (58.486)\n",
      "EPOCH: 121 train Results: Prec@1 58.486 Loss: 1.2040\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1004 (1.1004)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4177 (1.2794)\tPrec@1 31.250 (54.910)\n",
      "EPOCH: 121 val Results: Prec@1 54.910 Loss: 1.2794\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [122][0/390]\tTime 0.010 (0.010)\tLoss 1.0978 (1.0978)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [122][78/390]\tTime 0.002 (0.006)\tLoss 1.0575 (1.1467)\tPrec@1 61.719 (61.116)\n",
      "Epoch: [122][156/390]\tTime 0.006 (0.006)\tLoss 1.1319 (1.1648)\tPrec@1 59.375 (60.390)\n",
      "Epoch: [122][234/390]\tTime 0.004 (0.006)\tLoss 1.3618 (1.1831)\tPrec@1 55.469 (59.648)\n",
      "Epoch: [122][312/390]\tTime 0.003 (0.006)\tLoss 1.1904 (1.1953)\tPrec@1 64.062 (59.023)\n",
      "Epoch: [122][390/390]\tTime 0.008 (0.006)\tLoss 1.3973 (1.2047)\tPrec@1 48.750 (58.684)\n",
      "EPOCH: 122 train Results: Prec@1 58.684 Loss: 1.2047\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1437 (1.1437)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4233 (1.2885)\tPrec@1 31.250 (54.960)\n",
      "EPOCH: 122 val Results: Prec@1 54.960 Loss: 1.2885\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [123][0/390]\tTime 0.003 (0.003)\tLoss 1.0802 (1.0802)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [123][78/390]\tTime 0.003 (0.005)\tLoss 1.1892 (1.1560)\tPrec@1 53.125 (60.443)\n",
      "Epoch: [123][156/390]\tTime 0.007 (0.004)\tLoss 1.2306 (1.1741)\tPrec@1 62.500 (59.808)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.005)\tLoss 1.3480 (1.1876)\tPrec@1 56.250 (59.299)\n",
      "Epoch: [123][312/390]\tTime 0.009 (0.005)\tLoss 1.2681 (1.1986)\tPrec@1 53.125 (58.936)\n",
      "Epoch: [123][390/390]\tTime 0.003 (0.005)\tLoss 1.2630 (1.2064)\tPrec@1 53.750 (58.614)\n",
      "EPOCH: 123 train Results: Prec@1 58.614 Loss: 1.2064\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2283 (1.2283)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4307 (1.2891)\tPrec@1 43.750 (54.680)\n",
      "EPOCH: 123 val Results: Prec@1 54.680 Loss: 1.2891\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [124][0/390]\tTime 0.003 (0.003)\tLoss 1.1104 (1.1104)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [124][78/390]\tTime 0.003 (0.004)\tLoss 1.2378 (1.1758)\tPrec@1 55.469 (59.583)\n",
      "Epoch: [124][156/390]\tTime 0.010 (0.004)\tLoss 1.1826 (1.1868)\tPrec@1 60.938 (59.141)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.004)\tLoss 1.2262 (1.1972)\tPrec@1 55.469 (58.607)\n",
      "Epoch: [124][312/390]\tTime 0.002 (0.005)\tLoss 1.2127 (1.1990)\tPrec@1 62.500 (58.459)\n",
      "Epoch: [124][390/390]\tTime 0.001 (0.005)\tLoss 1.2418 (1.2039)\tPrec@1 51.250 (58.378)\n",
      "EPOCH: 124 train Results: Prec@1 58.378 Loss: 1.2039\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2144 (1.2144)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5948 (1.2807)\tPrec@1 31.250 (54.740)\n",
      "EPOCH: 124 val Results: Prec@1 54.740 Loss: 1.2807\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [125][0/390]\tTime 0.005 (0.005)\tLoss 1.1709 (1.1709)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [125][78/390]\tTime 0.010 (0.005)\tLoss 1.0086 (1.1648)\tPrec@1 64.844 (60.285)\n",
      "Epoch: [125][156/390]\tTime 0.002 (0.007)\tLoss 1.1372 (1.1825)\tPrec@1 60.938 (59.484)\n",
      "Epoch: [125][234/390]\tTime 0.002 (0.006)\tLoss 1.2629 (1.1916)\tPrec@1 53.906 (59.139)\n",
      "Epoch: [125][312/390]\tTime 0.009 (0.005)\tLoss 1.2893 (1.1983)\tPrec@1 57.812 (58.881)\n",
      "Epoch: [125][390/390]\tTime 0.004 (0.006)\tLoss 1.0765 (1.2036)\tPrec@1 66.250 (58.598)\n",
      "EPOCH: 125 train Results: Prec@1 58.598 Loss: 1.2036\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.1603 (1.1603)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5389 (1.2895)\tPrec@1 25.000 (54.260)\n",
      "EPOCH: 125 val Results: Prec@1 54.260 Loss: 1.2895\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [126][0/390]\tTime 0.003 (0.003)\tLoss 1.0430 (1.0430)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [126][78/390]\tTime 0.002 (0.005)\tLoss 1.0104 (1.1499)\tPrec@1 67.188 (60.967)\n",
      "Epoch: [126][156/390]\tTime 0.002 (0.005)\tLoss 1.3279 (1.1744)\tPrec@1 53.906 (59.907)\n",
      "Epoch: [126][234/390]\tTime 0.003 (0.004)\tLoss 1.3180 (1.1859)\tPrec@1 53.906 (59.222)\n",
      "Epoch: [126][312/390]\tTime 0.002 (0.004)\tLoss 1.2186 (1.1971)\tPrec@1 52.344 (58.679)\n",
      "Epoch: [126][390/390]\tTime 0.005 (0.004)\tLoss 1.1459 (1.2047)\tPrec@1 61.250 (58.378)\n",
      "EPOCH: 126 train Results: Prec@1 58.378 Loss: 1.2047\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1900 (1.1900)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5380 (1.2813)\tPrec@1 31.250 (55.010)\n",
      "EPOCH: 126 val Results: Prec@1 55.010 Loss: 1.2813\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [127][0/390]\tTime 0.004 (0.004)\tLoss 1.2302 (1.2302)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [127][78/390]\tTime 0.003 (0.003)\tLoss 0.9891 (1.1586)\tPrec@1 71.875 (60.581)\n",
      "Epoch: [127][156/390]\tTime 0.002 (0.003)\tLoss 1.1796 (1.1763)\tPrec@1 57.031 (59.390)\n",
      "Epoch: [127][234/390]\tTime 0.003 (0.003)\tLoss 1.4046 (1.1863)\tPrec@1 50.000 (59.179)\n",
      "Epoch: [127][312/390]\tTime 0.012 (0.003)\tLoss 1.3060 (1.2002)\tPrec@1 53.125 (58.669)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.004)\tLoss 1.2304 (1.2049)\tPrec@1 57.500 (58.426)\n",
      "EPOCH: 127 train Results: Prec@1 58.426 Loss: 1.2049\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1471 (1.1471)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4881 (1.2794)\tPrec@1 31.250 (55.430)\n",
      "EPOCH: 127 val Results: Prec@1 55.430 Loss: 1.2794\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [128][0/390]\tTime 0.003 (0.003)\tLoss 1.1506 (1.1506)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [128][78/390]\tTime 0.002 (0.003)\tLoss 1.1967 (1.1560)\tPrec@1 59.375 (60.305)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.004)\tLoss 1.1544 (1.1790)\tPrec@1 60.156 (59.136)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.004)\tLoss 1.1417 (1.1852)\tPrec@1 65.625 (58.983)\n",
      "Epoch: [128][312/390]\tTime 0.004 (0.004)\tLoss 1.1670 (1.1938)\tPrec@1 61.719 (58.664)\n",
      "Epoch: [128][390/390]\tTime 0.001 (0.004)\tLoss 1.4488 (1.2027)\tPrec@1 56.250 (58.368)\n",
      "EPOCH: 128 train Results: Prec@1 58.368 Loss: 1.2027\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1637 (1.1637)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3958 (1.2886)\tPrec@1 37.500 (54.140)\n",
      "EPOCH: 128 val Results: Prec@1 54.140 Loss: 1.2886\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [129][0/390]\tTime 0.002 (0.002)\tLoss 1.2304 (1.2304)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 1.1180 (1.1470)\tPrec@1 60.156 (60.690)\n",
      "Epoch: [129][156/390]\tTime 0.004 (0.003)\tLoss 1.2023 (1.1626)\tPrec@1 59.375 (60.062)\n",
      "Epoch: [129][234/390]\tTime 0.003 (0.003)\tLoss 1.3615 (1.1794)\tPrec@1 50.000 (59.511)\n",
      "Epoch: [129][312/390]\tTime 0.002 (0.003)\tLoss 1.1328 (1.1904)\tPrec@1 61.719 (58.996)\n",
      "Epoch: [129][390/390]\tTime 0.001 (0.003)\tLoss 1.1776 (1.1989)\tPrec@1 57.500 (58.610)\n",
      "EPOCH: 129 train Results: Prec@1 58.610 Loss: 1.1989\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.2080 (1.2080)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4877 (1.2953)\tPrec@1 31.250 (54.620)\n",
      "EPOCH: 129 val Results: Prec@1 54.620 Loss: 1.2953\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [130][0/390]\tTime 0.005 (0.005)\tLoss 1.1054 (1.1054)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [130][78/390]\tTime 0.002 (0.003)\tLoss 1.0423 (1.1457)\tPrec@1 66.406 (61.284)\n",
      "Epoch: [130][156/390]\tTime 0.002 (0.003)\tLoss 1.3085 (1.1616)\tPrec@1 53.125 (60.156)\n",
      "Epoch: [130][234/390]\tTime 0.002 (0.003)\tLoss 1.3526 (1.1811)\tPrec@1 51.562 (59.405)\n",
      "Epoch: [130][312/390]\tTime 0.002 (0.003)\tLoss 1.2336 (1.1931)\tPrec@1 57.812 (58.893)\n",
      "Epoch: [130][390/390]\tTime 0.002 (0.003)\tLoss 1.4229 (1.2006)\tPrec@1 43.750 (58.612)\n",
      "EPOCH: 130 train Results: Prec@1 58.612 Loss: 1.2006\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1865 (1.1865)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2256 (1.2847)\tPrec@1 43.750 (55.040)\n",
      "EPOCH: 130 val Results: Prec@1 55.040 Loss: 1.2847\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [131][0/390]\tTime 0.002 (0.002)\tLoss 1.0178 (1.0178)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [131][78/390]\tTime 0.002 (0.003)\tLoss 1.1810 (1.1618)\tPrec@1 57.812 (59.919)\n",
      "Epoch: [131][156/390]\tTime 0.005 (0.003)\tLoss 1.2269 (1.1772)\tPrec@1 54.688 (59.290)\n",
      "Epoch: [131][234/390]\tTime 0.004 (0.003)\tLoss 1.2223 (1.1866)\tPrec@1 55.469 (58.886)\n",
      "Epoch: [131][312/390]\tTime 0.007 (0.003)\tLoss 1.3933 (1.1936)\tPrec@1 51.562 (58.753)\n",
      "Epoch: [131][390/390]\tTime 0.002 (0.004)\tLoss 1.1228 (1.2009)\tPrec@1 62.500 (58.488)\n",
      "EPOCH: 131 train Results: Prec@1 58.488 Loss: 1.2009\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1653 (1.1653)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6300 (1.2721)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 131 val Results: Prec@1 55.300 Loss: 1.2721\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [132][0/390]\tTime 0.002 (0.002)\tLoss 1.1634 (1.1634)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [132][78/390]\tTime 0.006 (0.003)\tLoss 1.1639 (1.1588)\tPrec@1 57.812 (60.384)\n",
      "Epoch: [132][156/390]\tTime 0.003 (0.003)\tLoss 1.2185 (1.1824)\tPrec@1 59.375 (59.340)\n",
      "Epoch: [132][234/390]\tTime 0.008 (0.003)\tLoss 1.1305 (1.1952)\tPrec@1 61.719 (58.793)\n",
      "Epoch: [132][312/390]\tTime 0.004 (0.003)\tLoss 1.4145 (1.2002)\tPrec@1 49.219 (58.654)\n",
      "Epoch: [132][390/390]\tTime 0.003 (0.003)\tLoss 1.1756 (1.2005)\tPrec@1 66.250 (58.628)\n",
      "EPOCH: 132 train Results: Prec@1 58.628 Loss: 1.2005\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1456 (1.1456)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.4911 (1.2799)\tPrec@1 37.500 (54.660)\n",
      "EPOCH: 132 val Results: Prec@1 54.660 Loss: 1.2799\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [133][0/390]\tTime 0.005 (0.005)\tLoss 1.0308 (1.0308)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [133][78/390]\tTime 0.003 (0.003)\tLoss 1.1455 (1.1579)\tPrec@1 60.938 (60.977)\n",
      "Epoch: [133][156/390]\tTime 0.003 (0.003)\tLoss 1.0751 (1.1661)\tPrec@1 65.625 (60.266)\n",
      "Epoch: [133][234/390]\tTime 0.006 (0.003)\tLoss 1.3449 (1.1813)\tPrec@1 52.344 (59.714)\n",
      "Epoch: [133][312/390]\tTime 0.004 (0.003)\tLoss 1.2420 (1.1937)\tPrec@1 55.469 (59.225)\n",
      "Epoch: [133][390/390]\tTime 0.003 (0.003)\tLoss 1.2279 (1.1988)\tPrec@1 58.750 (58.908)\n",
      "EPOCH: 133 train Results: Prec@1 58.908 Loss: 1.1988\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1803 (1.1803)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3064 (1.2878)\tPrec@1 56.250 (54.760)\n",
      "EPOCH: 133 val Results: Prec@1 54.760 Loss: 1.2878\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [134][0/390]\tTime 0.004 (0.004)\tLoss 1.1936 (1.1936)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [134][78/390]\tTime 0.002 (0.003)\tLoss 1.1917 (1.1695)\tPrec@1 59.375 (60.028)\n",
      "Epoch: [134][156/390]\tTime 0.002 (0.003)\tLoss 1.1679 (1.1874)\tPrec@1 58.594 (59.231)\n",
      "Epoch: [134][234/390]\tTime 0.003 (0.003)\tLoss 1.1688 (1.1943)\tPrec@1 54.688 (58.667)\n",
      "Epoch: [134][312/390]\tTime 0.004 (0.003)\tLoss 1.1910 (1.1999)\tPrec@1 60.938 (58.491)\n",
      "Epoch: [134][390/390]\tTime 0.005 (0.003)\tLoss 1.2577 (1.2046)\tPrec@1 55.000 (58.300)\n",
      "EPOCH: 134 train Results: Prec@1 58.300 Loss: 1.2046\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1164 (1.1164)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4784 (1.2759)\tPrec@1 37.500 (55.130)\n",
      "EPOCH: 134 val Results: Prec@1 55.130 Loss: 1.2759\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [135][0/390]\tTime 0.003 (0.003)\tLoss 1.1557 (1.1557)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [135][78/390]\tTime 0.002 (0.003)\tLoss 1.1030 (1.1559)\tPrec@1 67.969 (60.502)\n",
      "Epoch: [135][156/390]\tTime 0.006 (0.003)\tLoss 1.1661 (1.1705)\tPrec@1 59.375 (59.982)\n",
      "Epoch: [135][234/390]\tTime 0.003 (0.003)\tLoss 1.3214 (1.1829)\tPrec@1 54.688 (59.242)\n",
      "Epoch: [135][312/390]\tTime 0.002 (0.003)\tLoss 1.1623 (1.1923)\tPrec@1 59.375 (58.936)\n",
      "Epoch: [135][390/390]\tTime 0.003 (0.003)\tLoss 1.2521 (1.2012)\tPrec@1 55.000 (58.620)\n",
      "EPOCH: 135 train Results: Prec@1 58.620 Loss: 1.2012\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1857 (1.1857)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2644 (1.2787)\tPrec@1 56.250 (54.900)\n",
      "EPOCH: 135 val Results: Prec@1 54.900 Loss: 1.2787\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [136][0/390]\tTime 0.005 (0.005)\tLoss 0.9731 (0.9731)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [136][78/390]\tTime 0.004 (0.004)\tLoss 1.3694 (1.1545)\tPrec@1 48.438 (60.572)\n",
      "Epoch: [136][156/390]\tTime 0.004 (0.004)\tLoss 1.0757 (1.1666)\tPrec@1 63.281 (59.992)\n",
      "Epoch: [136][234/390]\tTime 0.002 (0.003)\tLoss 1.1348 (1.1828)\tPrec@1 65.625 (59.352)\n",
      "Epoch: [136][312/390]\tTime 0.002 (0.003)\tLoss 1.2614 (1.1915)\tPrec@1 54.688 (58.961)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.2922 (1.1990)\tPrec@1 52.500 (58.674)\n",
      "EPOCH: 136 train Results: Prec@1 58.674 Loss: 1.1990\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1156 (1.1156)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4554 (1.2746)\tPrec@1 37.500 (55.380)\n",
      "EPOCH: 136 val Results: Prec@1 55.380 Loss: 1.2746\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [137][0/390]\tTime 0.002 (0.002)\tLoss 1.1450 (1.1450)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [137][78/390]\tTime 0.003 (0.003)\tLoss 1.3352 (1.1663)\tPrec@1 56.250 (60.047)\n",
      "Epoch: [137][156/390]\tTime 0.007 (0.003)\tLoss 1.0721 (1.1755)\tPrec@1 66.406 (59.574)\n",
      "Epoch: [137][234/390]\tTime 0.004 (0.003)\tLoss 1.1404 (1.1817)\tPrec@1 60.156 (59.172)\n",
      "Epoch: [137][312/390]\tTime 0.003 (0.003)\tLoss 1.2110 (1.1909)\tPrec@1 60.156 (58.856)\n",
      "Epoch: [137][390/390]\tTime 0.003 (0.003)\tLoss 1.3640 (1.2016)\tPrec@1 53.750 (58.452)\n",
      "EPOCH: 137 train Results: Prec@1 58.452 Loss: 1.2016\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1436 (1.1436)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5461 (1.2751)\tPrec@1 37.500 (55.570)\n",
      "EPOCH: 137 val Results: Prec@1 55.570 Loss: 1.2751\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [138][0/390]\tTime 0.003 (0.003)\tLoss 1.2414 (1.2414)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [138][78/390]\tTime 0.004 (0.004)\tLoss 1.0855 (1.1618)\tPrec@1 67.188 (60.067)\n",
      "Epoch: [138][156/390]\tTime 0.004 (0.004)\tLoss 1.1934 (1.1787)\tPrec@1 58.594 (59.614)\n",
      "Epoch: [138][234/390]\tTime 0.007 (0.003)\tLoss 1.2739 (1.1892)\tPrec@1 58.594 (59.129)\n",
      "Epoch: [138][312/390]\tTime 0.006 (0.004)\tLoss 1.3400 (1.1972)\tPrec@1 50.781 (58.828)\n",
      "Epoch: [138][390/390]\tTime 0.001 (0.004)\tLoss 1.3267 (1.2027)\tPrec@1 57.500 (58.616)\n",
      "EPOCH: 138 train Results: Prec@1 58.616 Loss: 1.2027\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1210 (1.1210)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3186 (1.2709)\tPrec@1 43.750 (54.970)\n",
      "EPOCH: 138 val Results: Prec@1 54.970 Loss: 1.2709\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [139][0/390]\tTime 0.003 (0.003)\tLoss 1.1908 (1.1908)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [139][78/390]\tTime 0.002 (0.003)\tLoss 1.0084 (1.1504)\tPrec@1 67.188 (60.779)\n",
      "Epoch: [139][156/390]\tTime 0.003 (0.003)\tLoss 1.1461 (1.1738)\tPrec@1 64.062 (59.888)\n",
      "Epoch: [139][234/390]\tTime 0.006 (0.003)\tLoss 1.2245 (1.1853)\tPrec@1 59.375 (59.428)\n",
      "Epoch: [139][312/390]\tTime 0.013 (0.003)\tLoss 1.1918 (1.1925)\tPrec@1 57.031 (58.963)\n",
      "Epoch: [139][390/390]\tTime 0.003 (0.004)\tLoss 1.2161 (1.1992)\tPrec@1 58.750 (58.808)\n",
      "EPOCH: 139 train Results: Prec@1 58.808 Loss: 1.1992\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2067 (1.2067)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4348 (1.2769)\tPrec@1 37.500 (54.640)\n",
      "EPOCH: 139 val Results: Prec@1 54.640 Loss: 1.2769\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [140][0/390]\tTime 0.004 (0.004)\tLoss 1.1531 (1.1531)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [140][78/390]\tTime 0.004 (0.003)\tLoss 1.2319 (1.1481)\tPrec@1 55.469 (61.432)\n",
      "Epoch: [140][156/390]\tTime 0.002 (0.003)\tLoss 1.1432 (1.1664)\tPrec@1 57.031 (60.052)\n",
      "Epoch: [140][234/390]\tTime 0.004 (0.003)\tLoss 1.1078 (1.1767)\tPrec@1 59.375 (59.628)\n",
      "Epoch: [140][312/390]\tTime 0.004 (0.003)\tLoss 1.2915 (1.1870)\tPrec@1 53.125 (59.238)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.004)\tLoss 1.3592 (1.1969)\tPrec@1 51.250 (58.784)\n",
      "EPOCH: 140 train Results: Prec@1 58.784 Loss: 1.1969\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1624 (1.1624)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4059 (1.2808)\tPrec@1 37.500 (55.360)\n",
      "EPOCH: 140 val Results: Prec@1 55.360 Loss: 1.2808\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [141][0/390]\tTime 0.008 (0.008)\tLoss 0.9233 (0.9233)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [141][78/390]\tTime 0.003 (0.004)\tLoss 1.3136 (1.1546)\tPrec@1 56.250 (60.473)\n",
      "Epoch: [141][156/390]\tTime 0.009 (0.004)\tLoss 1.2111 (1.1758)\tPrec@1 53.906 (59.584)\n",
      "Epoch: [141][234/390]\tTime 0.004 (0.003)\tLoss 1.3117 (1.1855)\tPrec@1 52.344 (59.209)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.004)\tLoss 1.2038 (1.1942)\tPrec@1 59.375 (58.726)\n",
      "Epoch: [141][390/390]\tTime 0.003 (0.003)\tLoss 1.3467 (1.1976)\tPrec@1 52.500 (58.616)\n",
      "EPOCH: 141 train Results: Prec@1 58.616 Loss: 1.1976\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1825 (1.1825)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2663 (1.2805)\tPrec@1 62.500 (54.700)\n",
      "EPOCH: 141 val Results: Prec@1 54.700 Loss: 1.2805\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [142][0/390]\tTime 0.005 (0.005)\tLoss 1.1429 (1.1429)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [142][78/390]\tTime 0.002 (0.003)\tLoss 1.1858 (1.1500)\tPrec@1 61.719 (60.829)\n",
      "Epoch: [142][156/390]\tTime 0.002 (0.003)\tLoss 1.1325 (1.1707)\tPrec@1 59.375 (59.808)\n",
      "Epoch: [142][234/390]\tTime 0.002 (0.003)\tLoss 1.1850 (1.1776)\tPrec@1 60.156 (59.545)\n",
      "Epoch: [142][312/390]\tTime 0.002 (0.003)\tLoss 1.1936 (1.1897)\tPrec@1 59.375 (59.128)\n",
      "Epoch: [142][390/390]\tTime 0.002 (0.003)\tLoss 1.3134 (1.1980)\tPrec@1 57.500 (58.634)\n",
      "EPOCH: 142 train Results: Prec@1 58.634 Loss: 1.1980\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2107 (1.2107)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1373 (1.2862)\tPrec@1 56.250 (55.210)\n",
      "EPOCH: 142 val Results: Prec@1 55.210 Loss: 1.2862\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [143][0/390]\tTime 0.003 (0.003)\tLoss 1.0445 (1.0445)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [143][78/390]\tTime 0.002 (0.003)\tLoss 1.1752 (1.1614)\tPrec@1 60.156 (61.056)\n",
      "Epoch: [143][156/390]\tTime 0.002 (0.003)\tLoss 1.2243 (1.1760)\tPrec@1 53.906 (60.281)\n",
      "Epoch: [143][234/390]\tTime 0.002 (0.003)\tLoss 1.1617 (1.1830)\tPrec@1 61.719 (59.774)\n",
      "Epoch: [143][312/390]\tTime 0.003 (0.003)\tLoss 1.2191 (1.1928)\tPrec@1 57.031 (59.258)\n",
      "Epoch: [143][390/390]\tTime 0.002 (0.003)\tLoss 1.1902 (1.1976)\tPrec@1 60.000 (59.002)\n",
      "EPOCH: 143 train Results: Prec@1 59.002 Loss: 1.1976\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1855 (1.1855)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1966 (1.2835)\tPrec@1 43.750 (55.100)\n",
      "EPOCH: 143 val Results: Prec@1 55.100 Loss: 1.2835\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [144][0/390]\tTime 0.002 (0.002)\tLoss 1.0974 (1.0974)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [144][78/390]\tTime 0.007 (0.003)\tLoss 1.2297 (1.1651)\tPrec@1 59.375 (60.275)\n",
      "Epoch: [144][156/390]\tTime 0.004 (0.003)\tLoss 1.3750 (1.1724)\tPrec@1 54.688 (60.017)\n",
      "Epoch: [144][234/390]\tTime 0.002 (0.003)\tLoss 1.1804 (1.1872)\tPrec@1 65.625 (59.435)\n",
      "Epoch: [144][312/390]\tTime 0.003 (0.003)\tLoss 1.1905 (1.1910)\tPrec@1 55.469 (59.170)\n",
      "Epoch: [144][390/390]\tTime 0.001 (0.003)\tLoss 1.2115 (1.1966)\tPrec@1 56.250 (58.924)\n",
      "EPOCH: 144 train Results: Prec@1 58.924 Loss: 1.1966\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1090 (1.1090)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6583 (1.2920)\tPrec@1 37.500 (54.010)\n",
      "EPOCH: 144 val Results: Prec@1 54.010 Loss: 1.2920\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [145][0/390]\tTime 0.003 (0.003)\tLoss 1.0880 (1.0880)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [145][78/390]\tTime 0.010 (0.003)\tLoss 1.0933 (1.1646)\tPrec@1 66.406 (60.235)\n",
      "Epoch: [145][156/390]\tTime 0.003 (0.003)\tLoss 1.1689 (1.1672)\tPrec@1 59.375 (60.121)\n",
      "Epoch: [145][234/390]\tTime 0.002 (0.003)\tLoss 1.1718 (1.1793)\tPrec@1 57.812 (59.694)\n",
      "Epoch: [145][312/390]\tTime 0.005 (0.003)\tLoss 1.2047 (1.1907)\tPrec@1 58.594 (58.996)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.003)\tLoss 1.1826 (1.1958)\tPrec@1 60.000 (58.834)\n",
      "EPOCH: 145 train Results: Prec@1 58.834 Loss: 1.1958\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1639 (1.1639)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5587 (1.2913)\tPrec@1 31.250 (54.570)\n",
      "EPOCH: 145 val Results: Prec@1 54.570 Loss: 1.2913\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [146][0/390]\tTime 0.006 (0.006)\tLoss 1.1676 (1.1676)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [146][78/390]\tTime 0.002 (0.003)\tLoss 1.1188 (1.1554)\tPrec@1 59.375 (60.324)\n",
      "Epoch: [146][156/390]\tTime 0.002 (0.003)\tLoss 1.0965 (1.1750)\tPrec@1 60.156 (59.644)\n",
      "Epoch: [146][234/390]\tTime 0.007 (0.003)\tLoss 1.0913 (1.1821)\tPrec@1 66.406 (59.385)\n",
      "Epoch: [146][312/390]\tTime 0.011 (0.003)\tLoss 1.1315 (1.1926)\tPrec@1 63.281 (59.080)\n",
      "Epoch: [146][390/390]\tTime 0.003 (0.003)\tLoss 1.0876 (1.1996)\tPrec@1 66.250 (58.666)\n",
      "EPOCH: 146 train Results: Prec@1 58.666 Loss: 1.1996\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1426 (1.1426)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2742 (1.2834)\tPrec@1 37.500 (55.030)\n",
      "EPOCH: 146 val Results: Prec@1 55.030 Loss: 1.2834\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [147][0/390]\tTime 0.003 (0.003)\tLoss 1.0627 (1.0627)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [147][78/390]\tTime 0.002 (0.003)\tLoss 1.1312 (1.1506)\tPrec@1 64.062 (60.502)\n",
      "Epoch: [147][156/390]\tTime 0.005 (0.006)\tLoss 1.2188 (1.1652)\tPrec@1 54.688 (59.763)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.006)\tLoss 1.2009 (1.1729)\tPrec@1 60.938 (59.491)\n",
      "Epoch: [147][312/390]\tTime 0.003 (0.005)\tLoss 1.2676 (1.1844)\tPrec@1 59.375 (59.093)\n",
      "Epoch: [147][390/390]\tTime 0.004 (0.005)\tLoss 1.2928 (1.1935)\tPrec@1 57.500 (58.766)\n",
      "EPOCH: 147 train Results: Prec@1 58.766 Loss: 1.1935\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1723 (1.1723)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.6793 (1.2864)\tPrec@1 25.000 (54.680)\n",
      "EPOCH: 147 val Results: Prec@1 54.680 Loss: 1.2864\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [148][0/390]\tTime 0.003 (0.003)\tLoss 1.1767 (1.1767)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [148][78/390]\tTime 0.002 (0.003)\tLoss 1.2187 (1.1536)\tPrec@1 52.344 (60.700)\n",
      "Epoch: [148][156/390]\tTime 0.002 (0.003)\tLoss 1.2390 (1.1671)\tPrec@1 64.844 (60.057)\n",
      "Epoch: [148][234/390]\tTime 0.002 (0.004)\tLoss 1.2401 (1.1815)\tPrec@1 54.688 (59.348)\n",
      "Epoch: [148][312/390]\tTime 0.004 (0.004)\tLoss 1.0799 (1.1896)\tPrec@1 64.844 (59.008)\n",
      "Epoch: [148][390/390]\tTime 0.009 (0.004)\tLoss 1.1132 (1.1968)\tPrec@1 62.500 (58.700)\n",
      "EPOCH: 148 train Results: Prec@1 58.700 Loss: 1.1968\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2214 (1.2214)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4028 (1.2843)\tPrec@1 31.250 (54.990)\n",
      "EPOCH: 148 val Results: Prec@1 54.990 Loss: 1.2843\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [149][0/390]\tTime 0.004 (0.004)\tLoss 0.9784 (0.9784)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [149][78/390]\tTime 0.003 (0.004)\tLoss 1.2011 (1.1402)\tPrec@1 53.125 (61.333)\n",
      "Epoch: [149][156/390]\tTime 0.011 (0.004)\tLoss 1.1792 (1.1574)\tPrec@1 57.031 (60.181)\n",
      "Epoch: [149][234/390]\tTime 0.002 (0.004)\tLoss 1.0975 (1.1735)\tPrec@1 64.844 (59.428)\n",
      "Epoch: [149][312/390]\tTime 0.003 (0.004)\tLoss 1.1643 (1.1845)\tPrec@1 59.375 (59.083)\n",
      "Epoch: [149][390/390]\tTime 0.001 (0.004)\tLoss 1.2776 (1.1945)\tPrec@1 55.000 (58.656)\n",
      "EPOCH: 149 train Results: Prec@1 58.656 Loss: 1.1945\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1421 (1.1421)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4084 (1.2876)\tPrec@1 43.750 (54.450)\n",
      "EPOCH: 149 val Results: Prec@1 54.450 Loss: 1.2876\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [150][0/390]\tTime 0.003 (0.003)\tLoss 1.1954 (1.1954)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [150][78/390]\tTime 0.004 (0.005)\tLoss 1.2941 (1.1362)\tPrec@1 60.938 (61.600)\n",
      "Epoch: [150][156/390]\tTime 0.021 (0.018)\tLoss 1.2596 (1.1529)\tPrec@1 60.156 (60.410)\n",
      "Epoch: [150][234/390]\tTime 0.010 (0.018)\tLoss 1.3604 (1.1776)\tPrec@1 55.469 (59.368)\n",
      "Epoch: [150][312/390]\tTime 0.002 (0.019)\tLoss 1.2458 (1.1888)\tPrec@1 58.594 (58.891)\n",
      "Epoch: [150][390/390]\tTime 0.019 (0.022)\tLoss 1.2237 (1.1953)\tPrec@1 61.250 (58.572)\n",
      "EPOCH: 150 train Results: Prec@1 58.572 Loss: 1.1953\n",
      "Test: [0/78]\tTime 0.043 (0.043)\tLoss 1.1436 (1.1436)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.008)\tLoss 1.4120 (1.2739)\tPrec@1 37.500 (55.110)\n",
      "EPOCH: 150 val Results: Prec@1 55.110 Loss: 1.2739\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [151][0/390]\tTime 0.030 (0.030)\tLoss 1.1144 (1.1144)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [151][78/390]\tTime 0.007 (0.024)\tLoss 1.2432 (1.1570)\tPrec@1 56.250 (60.473)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.016)\tLoss 1.2648 (1.1685)\tPrec@1 53.125 (59.838)\n",
      "Epoch: [151][234/390]\tTime 0.004 (0.012)\tLoss 1.1938 (1.1759)\tPrec@1 56.250 (59.422)\n",
      "Epoch: [151][312/390]\tTime 0.005 (0.011)\tLoss 1.2513 (1.1851)\tPrec@1 55.469 (59.038)\n",
      "Epoch: [151][390/390]\tTime 0.003 (0.009)\tLoss 1.2975 (1.1959)\tPrec@1 56.250 (58.576)\n",
      "EPOCH: 151 train Results: Prec@1 58.576 Loss: 1.1959\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1488 (1.1488)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3697 (1.2874)\tPrec@1 50.000 (54.760)\n",
      "EPOCH: 151 val Results: Prec@1 54.760 Loss: 1.2874\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [152][0/390]\tTime 0.006 (0.006)\tLoss 1.0341 (1.0341)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [152][78/390]\tTime 0.002 (0.006)\tLoss 1.1079 (1.1612)\tPrec@1 56.250 (60.107)\n",
      "Epoch: [152][156/390]\tTime 0.002 (0.005)\tLoss 1.1686 (1.1721)\tPrec@1 60.156 (60.022)\n",
      "Epoch: [152][234/390]\tTime 0.002 (0.005)\tLoss 1.2189 (1.1801)\tPrec@1 60.938 (59.621)\n",
      "Epoch: [152][312/390]\tTime 0.003 (0.005)\tLoss 1.2638 (1.1879)\tPrec@1 55.469 (59.240)\n",
      "Epoch: [152][390/390]\tTime 0.001 (0.005)\tLoss 1.0643 (1.1935)\tPrec@1 65.000 (59.004)\n",
      "EPOCH: 152 train Results: Prec@1 59.004 Loss: 1.1935\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1611 (1.1611)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3419 (1.2765)\tPrec@1 37.500 (54.870)\n",
      "EPOCH: 152 val Results: Prec@1 54.870 Loss: 1.2765\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [153][0/390]\tTime 0.004 (0.004)\tLoss 1.0755 (1.0755)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [153][78/390]\tTime 0.004 (0.004)\tLoss 1.0755 (1.1565)\tPrec@1 62.500 (60.572)\n",
      "Epoch: [153][156/390]\tTime 0.002 (0.004)\tLoss 1.1644 (1.1692)\tPrec@1 61.719 (59.977)\n",
      "Epoch: [153][234/390]\tTime 0.010 (0.004)\tLoss 1.0985 (1.1758)\tPrec@1 66.406 (59.697)\n",
      "Epoch: [153][312/390]\tTime 0.008 (0.005)\tLoss 1.1197 (1.1858)\tPrec@1 62.500 (59.165)\n",
      "Epoch: [153][390/390]\tTime 0.007 (0.004)\tLoss 1.2896 (1.1938)\tPrec@1 56.250 (58.714)\n",
      "EPOCH: 153 train Results: Prec@1 58.714 Loss: 1.1938\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1555 (1.1555)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4694 (1.2714)\tPrec@1 37.500 (55.090)\n",
      "EPOCH: 153 val Results: Prec@1 55.090 Loss: 1.2714\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [154][0/390]\tTime 0.002 (0.002)\tLoss 1.0731 (1.0731)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.003)\tLoss 1.1341 (1.1491)\tPrec@1 58.594 (61.017)\n",
      "Epoch: [154][156/390]\tTime 0.005 (0.003)\tLoss 1.2353 (1.1670)\tPrec@1 55.469 (60.121)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.003)\tLoss 1.2792 (1.1771)\tPrec@1 56.250 (59.485)\n",
      "Epoch: [154][312/390]\tTime 0.004 (0.003)\tLoss 1.2572 (1.1882)\tPrec@1 53.125 (59.073)\n",
      "Epoch: [154][390/390]\tTime 0.005 (0.004)\tLoss 1.3194 (1.1966)\tPrec@1 53.750 (58.690)\n",
      "EPOCH: 154 train Results: Prec@1 58.690 Loss: 1.1966\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1737 (1.1737)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4013 (1.2754)\tPrec@1 43.750 (54.870)\n",
      "EPOCH: 154 val Results: Prec@1 54.870 Loss: 1.2754\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [155][0/390]\tTime 0.002 (0.002)\tLoss 1.2421 (1.2421)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [155][78/390]\tTime 0.003 (0.004)\tLoss 1.2202 (1.1372)\tPrec@1 57.812 (61.214)\n",
      "Epoch: [155][156/390]\tTime 0.002 (0.004)\tLoss 1.1100 (1.1536)\tPrec@1 59.375 (60.340)\n",
      "Epoch: [155][234/390]\tTime 0.009 (0.004)\tLoss 1.2778 (1.1705)\tPrec@1 53.125 (59.432)\n",
      "Epoch: [155][312/390]\tTime 0.003 (0.004)\tLoss 1.2402 (1.1831)\tPrec@1 62.500 (59.048)\n",
      "Epoch: [155][390/390]\tTime 0.001 (0.004)\tLoss 1.0426 (1.1909)\tPrec@1 62.500 (58.742)\n",
      "EPOCH: 155 train Results: Prec@1 58.742 Loss: 1.1909\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1523 (1.1523)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5072 (1.2854)\tPrec@1 31.250 (54.500)\n",
      "EPOCH: 155 val Results: Prec@1 54.500 Loss: 1.2854\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [156][0/390]\tTime 0.003 (0.003)\tLoss 1.0753 (1.0753)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [156][78/390]\tTime 0.004 (0.003)\tLoss 1.1632 (1.1511)\tPrec@1 57.031 (60.463)\n",
      "Epoch: [156][156/390]\tTime 0.002 (0.003)\tLoss 1.3203 (1.1686)\tPrec@1 56.250 (59.957)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.003)\tLoss 1.1613 (1.1815)\tPrec@1 66.406 (59.342)\n",
      "Epoch: [156][312/390]\tTime 0.009 (0.003)\tLoss 1.3358 (1.1914)\tPrec@1 50.000 (59.058)\n",
      "Epoch: [156][390/390]\tTime 0.001 (0.003)\tLoss 1.3548 (1.1972)\tPrec@1 47.500 (58.764)\n",
      "EPOCH: 156 train Results: Prec@1 58.764 Loss: 1.1972\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1676 (1.1676)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6096 (1.2768)\tPrec@1 43.750 (55.280)\n",
      "EPOCH: 156 val Results: Prec@1 55.280 Loss: 1.2768\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [157][0/390]\tTime 0.003 (0.003)\tLoss 1.2566 (1.2566)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [157][78/390]\tTime 0.003 (0.003)\tLoss 1.0681 (1.1456)\tPrec@1 60.938 (61.224)\n",
      "Epoch: [157][156/390]\tTime 0.004 (0.003)\tLoss 1.1191 (1.1653)\tPrec@1 65.625 (60.276)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.1377 (1.1803)\tPrec@1 61.719 (59.408)\n",
      "Epoch: [157][312/390]\tTime 0.003 (0.003)\tLoss 1.2156 (1.1879)\tPrec@1 57.031 (59.001)\n",
      "Epoch: [157][390/390]\tTime 0.001 (0.003)\tLoss 1.2021 (1.1919)\tPrec@1 57.500 (58.852)\n",
      "EPOCH: 157 train Results: Prec@1 58.852 Loss: 1.1919\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1186 (1.1186)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3655 (1.2742)\tPrec@1 31.250 (55.370)\n",
      "EPOCH: 157 val Results: Prec@1 55.370 Loss: 1.2742\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [158][0/390]\tTime 0.004 (0.004)\tLoss 1.0466 (1.0466)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [158][78/390]\tTime 0.004 (0.003)\tLoss 1.2793 (1.1454)\tPrec@1 47.656 (60.690)\n",
      "Epoch: [158][156/390]\tTime 0.002 (0.003)\tLoss 1.2375 (1.1590)\tPrec@1 55.469 (59.987)\n",
      "Epoch: [158][234/390]\tTime 0.004 (0.003)\tLoss 1.2116 (1.1742)\tPrec@1 53.125 (59.422)\n",
      "Epoch: [158][312/390]\tTime 0.003 (0.004)\tLoss 1.1782 (1.1818)\tPrec@1 58.594 (59.185)\n",
      "Epoch: [158][390/390]\tTime 0.008 (0.004)\tLoss 1.1886 (1.1893)\tPrec@1 63.750 (58.938)\n",
      "EPOCH: 158 train Results: Prec@1 58.938 Loss: 1.1893\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1480 (1.1480)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4765 (1.2761)\tPrec@1 43.750 (54.920)\n",
      "EPOCH: 158 val Results: Prec@1 54.920 Loss: 1.2761\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [159][0/390]\tTime 0.005 (0.005)\tLoss 1.0335 (1.0335)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [159][78/390]\tTime 0.004 (0.004)\tLoss 1.1898 (1.1564)\tPrec@1 57.812 (60.255)\n",
      "Epoch: [159][156/390]\tTime 0.005 (0.003)\tLoss 1.0913 (1.1681)\tPrec@1 67.188 (59.728)\n",
      "Epoch: [159][234/390]\tTime 0.004 (0.003)\tLoss 1.0618 (1.1761)\tPrec@1 66.406 (59.448)\n",
      "Epoch: [159][312/390]\tTime 0.004 (0.003)\tLoss 1.1702 (1.1855)\tPrec@1 60.156 (59.120)\n",
      "Epoch: [159][390/390]\tTime 0.001 (0.003)\tLoss 1.2693 (1.1928)\tPrec@1 55.000 (58.806)\n",
      "EPOCH: 159 train Results: Prec@1 58.806 Loss: 1.1928\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1558 (1.1558)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4804 (1.2822)\tPrec@1 37.500 (54.930)\n",
      "EPOCH: 159 val Results: Prec@1 54.930 Loss: 1.2822\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [160][0/390]\tTime 0.004 (0.004)\tLoss 1.0515 (1.0515)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [160][78/390]\tTime 0.002 (0.005)\tLoss 1.2302 (1.1810)\tPrec@1 57.031 (59.632)\n",
      "Epoch: [160][156/390]\tTime 0.002 (0.004)\tLoss 1.2323 (1.1770)\tPrec@1 57.812 (59.460)\n",
      "Epoch: [160][234/390]\tTime 0.003 (0.004)\tLoss 1.1382 (1.1867)\tPrec@1 59.375 (58.973)\n",
      "Epoch: [160][312/390]\tTime 0.002 (0.004)\tLoss 1.1986 (1.1922)\tPrec@1 63.281 (58.791)\n",
      "Epoch: [160][390/390]\tTime 0.004 (0.004)\tLoss 1.1276 (1.1932)\tPrec@1 61.250 (58.780)\n",
      "EPOCH: 160 train Results: Prec@1 58.780 Loss: 1.1932\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1667 (1.1667)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2689 (1.2825)\tPrec@1 43.750 (55.350)\n",
      "EPOCH: 160 val Results: Prec@1 55.350 Loss: 1.2825\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [161][0/390]\tTime 0.003 (0.003)\tLoss 1.1817 (1.1817)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [161][78/390]\tTime 0.002 (0.003)\tLoss 1.0645 (1.1477)\tPrec@1 66.406 (60.384)\n",
      "Epoch: [161][156/390]\tTime 0.017 (0.003)\tLoss 1.1450 (1.1649)\tPrec@1 60.156 (59.733)\n",
      "Epoch: [161][234/390]\tTime 0.007 (0.003)\tLoss 1.2205 (1.1771)\tPrec@1 54.688 (59.275)\n",
      "Epoch: [161][312/390]\tTime 0.002 (0.003)\tLoss 1.2117 (1.1843)\tPrec@1 57.031 (59.073)\n",
      "Epoch: [161][390/390]\tTime 0.001 (0.003)\tLoss 1.4431 (1.1926)\tPrec@1 50.000 (58.640)\n",
      "EPOCH: 161 train Results: Prec@1 58.640 Loss: 1.1926\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1483 (1.1483)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4200 (1.2660)\tPrec@1 37.500 (55.410)\n",
      "EPOCH: 161 val Results: Prec@1 55.410 Loss: 1.2660\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 1.0160 (1.0160)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [162][78/390]\tTime 0.003 (0.004)\tLoss 1.1970 (1.1378)\tPrec@1 61.719 (60.750)\n",
      "Epoch: [162][156/390]\tTime 0.004 (0.004)\tLoss 1.0549 (1.1659)\tPrec@1 64.844 (59.743)\n",
      "Epoch: [162][234/390]\tTime 0.004 (0.004)\tLoss 1.1652 (1.1742)\tPrec@1 57.031 (59.528)\n",
      "Epoch: [162][312/390]\tTime 0.002 (0.004)\tLoss 1.1639 (1.1814)\tPrec@1 60.156 (59.285)\n",
      "Epoch: [162][390/390]\tTime 0.005 (0.004)\tLoss 1.2235 (1.1888)\tPrec@1 62.500 (58.930)\n",
      "EPOCH: 162 train Results: Prec@1 58.930 Loss: 1.1888\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1794 (1.1794)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6512 (1.2870)\tPrec@1 31.250 (54.340)\n",
      "EPOCH: 162 val Results: Prec@1 54.340 Loss: 1.2870\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [163][0/390]\tTime 0.005 (0.005)\tLoss 1.2740 (1.2740)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [163][78/390]\tTime 0.007 (0.003)\tLoss 1.0837 (1.1555)\tPrec@1 61.719 (60.572)\n",
      "Epoch: [163][156/390]\tTime 0.002 (0.003)\tLoss 1.1445 (1.1623)\tPrec@1 60.938 (60.221)\n",
      "Epoch: [163][234/390]\tTime 0.003 (0.003)\tLoss 1.1437 (1.1789)\tPrec@1 63.281 (59.508)\n",
      "Epoch: [163][312/390]\tTime 0.004 (0.004)\tLoss 1.1777 (1.1895)\tPrec@1 64.062 (59.008)\n",
      "Epoch: [163][390/390]\tTime 0.001 (0.003)\tLoss 1.2645 (1.1946)\tPrec@1 52.500 (58.840)\n",
      "EPOCH: 163 train Results: Prec@1 58.840 Loss: 1.1946\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1625 (1.1625)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3672 (1.2654)\tPrec@1 43.750 (55.400)\n",
      "EPOCH: 163 val Results: Prec@1 55.400 Loss: 1.2654\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [164][0/390]\tTime 0.002 (0.002)\tLoss 1.1930 (1.1930)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [164][78/390]\tTime 0.002 (0.004)\tLoss 1.1718 (1.1264)\tPrec@1 60.938 (61.679)\n",
      "Epoch: [164][156/390]\tTime 0.002 (0.003)\tLoss 1.2481 (1.1551)\tPrec@1 50.781 (60.534)\n",
      "Epoch: [164][234/390]\tTime 0.004 (0.003)\tLoss 1.2326 (1.1660)\tPrec@1 52.344 (60.013)\n",
      "Epoch: [164][312/390]\tTime 0.005 (0.003)\tLoss 1.2996 (1.1800)\tPrec@1 53.906 (59.330)\n",
      "Epoch: [164][390/390]\tTime 0.003 (0.003)\tLoss 1.1835 (1.1868)\tPrec@1 61.250 (59.000)\n",
      "EPOCH: 164 train Results: Prec@1 59.000 Loss: 1.1868\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1114 (1.1114)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4176 (1.2739)\tPrec@1 25.000 (55.570)\n",
      "EPOCH: 164 val Results: Prec@1 55.570 Loss: 1.2739\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [165][0/390]\tTime 0.004 (0.004)\tLoss 1.2400 (1.2400)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [165][78/390]\tTime 0.004 (0.003)\tLoss 1.0661 (1.1225)\tPrec@1 62.500 (61.808)\n",
      "Epoch: [165][156/390]\tTime 0.002 (0.003)\tLoss 1.1721 (1.1532)\tPrec@1 56.250 (60.460)\n",
      "Epoch: [165][234/390]\tTime 0.004 (0.003)\tLoss 1.3053 (1.1653)\tPrec@1 57.812 (60.106)\n",
      "Epoch: [165][312/390]\tTime 0.003 (0.003)\tLoss 1.2192 (1.1782)\tPrec@1 61.719 (59.432)\n",
      "Epoch: [165][390/390]\tTime 0.007 (0.003)\tLoss 1.4007 (1.1866)\tPrec@1 52.500 (59.018)\n",
      "EPOCH: 165 train Results: Prec@1 59.018 Loss: 1.1866\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1838 (1.1838)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3316 (1.2758)\tPrec@1 50.000 (54.990)\n",
      "EPOCH: 165 val Results: Prec@1 54.990 Loss: 1.2758\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [166][0/390]\tTime 0.011 (0.011)\tLoss 1.1995 (1.1995)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 1.0545 (1.1315)\tPrec@1 70.312 (61.689)\n",
      "Epoch: [166][156/390]\tTime 0.029 (0.004)\tLoss 1.2369 (1.1542)\tPrec@1 57.812 (60.435)\n",
      "Epoch: [166][234/390]\tTime 0.003 (0.004)\tLoss 1.2350 (1.1696)\tPrec@1 58.594 (59.814)\n",
      "Epoch: [166][312/390]\tTime 0.005 (0.004)\tLoss 1.2414 (1.1765)\tPrec@1 55.469 (59.560)\n",
      "Epoch: [166][390/390]\tTime 0.003 (0.004)\tLoss 1.1800 (1.1866)\tPrec@1 55.000 (59.136)\n",
      "EPOCH: 166 train Results: Prec@1 59.136 Loss: 1.1866\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1790 (1.1790)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3142 (1.2884)\tPrec@1 43.750 (54.670)\n",
      "EPOCH: 166 val Results: Prec@1 54.670 Loss: 1.2884\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [167][0/390]\tTime 0.004 (0.004)\tLoss 1.2531 (1.2531)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [167][78/390]\tTime 0.003 (0.004)\tLoss 1.2523 (1.1505)\tPrec@1 53.906 (61.135)\n",
      "Epoch: [167][156/390]\tTime 0.002 (0.003)\tLoss 1.1063 (1.1648)\tPrec@1 64.062 (60.057)\n",
      "Epoch: [167][234/390]\tTime 0.003 (0.003)\tLoss 1.1361 (1.1798)\tPrec@1 57.031 (59.305)\n",
      "Epoch: [167][312/390]\tTime 0.014 (0.003)\tLoss 1.3370 (1.1830)\tPrec@1 53.125 (59.213)\n",
      "Epoch: [167][390/390]\tTime 0.002 (0.003)\tLoss 1.2811 (1.1909)\tPrec@1 57.500 (58.886)\n",
      "EPOCH: 167 train Results: Prec@1 58.886 Loss: 1.1909\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1374 (1.1374)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3367 (1.2785)\tPrec@1 37.500 (54.930)\n",
      "EPOCH: 167 val Results: Prec@1 54.930 Loss: 1.2785\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [168][0/390]\tTime 0.002 (0.002)\tLoss 1.0900 (1.0900)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [168][78/390]\tTime 0.002 (0.003)\tLoss 1.0113 (1.1345)\tPrec@1 72.656 (61.185)\n",
      "Epoch: [168][156/390]\tTime 0.006 (0.003)\tLoss 1.1099 (1.1629)\tPrec@1 61.719 (59.922)\n",
      "Epoch: [168][234/390]\tTime 0.002 (0.003)\tLoss 1.1578 (1.1756)\tPrec@1 60.156 (59.511)\n",
      "Epoch: [168][312/390]\tTime 0.002 (0.003)\tLoss 1.1125 (1.1850)\tPrec@1 60.156 (59.113)\n",
      "Epoch: [168][390/390]\tTime 0.004 (0.003)\tLoss 1.1935 (1.1918)\tPrec@1 52.500 (58.888)\n",
      "EPOCH: 168 train Results: Prec@1 58.888 Loss: 1.1918\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1294 (1.1294)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7179 (1.2767)\tPrec@1 25.000 (55.140)\n",
      "EPOCH: 168 val Results: Prec@1 55.140 Loss: 1.2767\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [169][0/390]\tTime 0.002 (0.002)\tLoss 1.1293 (1.1293)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [169][78/390]\tTime 0.002 (0.003)\tLoss 1.1488 (1.1508)\tPrec@1 62.500 (60.730)\n",
      "Epoch: [169][156/390]\tTime 0.004 (0.003)\tLoss 1.0910 (1.1581)\tPrec@1 60.156 (60.241)\n",
      "Epoch: [169][234/390]\tTime 0.002 (0.004)\tLoss 1.1356 (1.1697)\tPrec@1 62.500 (59.674)\n",
      "Epoch: [169][312/390]\tTime 0.006 (0.004)\tLoss 1.1932 (1.1792)\tPrec@1 60.938 (59.260)\n",
      "Epoch: [169][390/390]\tTime 0.001 (0.004)\tLoss 1.2392 (1.1854)\tPrec@1 63.750 (59.070)\n",
      "EPOCH: 169 train Results: Prec@1 59.070 Loss: 1.1854\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1510 (1.1510)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3839 (1.2817)\tPrec@1 43.750 (54.990)\n",
      "EPOCH: 169 val Results: Prec@1 54.990 Loss: 1.2817\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [170][0/390]\tTime 0.004 (0.004)\tLoss 1.1337 (1.1337)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [170][78/390]\tTime 0.002 (0.003)\tLoss 1.1711 (1.1463)\tPrec@1 57.812 (60.997)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.003)\tLoss 1.0328 (1.1620)\tPrec@1 66.406 (59.843)\n",
      "Epoch: [170][234/390]\tTime 0.005 (0.003)\tLoss 1.1765 (1.1743)\tPrec@1 57.031 (59.295)\n",
      "Epoch: [170][312/390]\tTime 0.002 (0.003)\tLoss 1.1460 (1.1848)\tPrec@1 62.500 (58.903)\n",
      "Epoch: [170][390/390]\tTime 0.001 (0.003)\tLoss 1.2034 (1.1887)\tPrec@1 55.000 (58.770)\n",
      "EPOCH: 170 train Results: Prec@1 58.770 Loss: 1.1887\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1366 (1.1366)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2424 (1.2698)\tPrec@1 37.500 (55.620)\n",
      "EPOCH: 170 val Results: Prec@1 55.620 Loss: 1.2698\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [171][0/390]\tTime 0.005 (0.005)\tLoss 1.0710 (1.0710)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [171][78/390]\tTime 0.007 (0.003)\tLoss 1.1086 (1.1417)\tPrec@1 65.625 (60.789)\n",
      "Epoch: [171][156/390]\tTime 0.002 (0.003)\tLoss 1.2235 (1.1607)\tPrec@1 59.375 (60.062)\n",
      "Epoch: [171][234/390]\tTime 0.002 (0.003)\tLoss 1.1266 (1.1710)\tPrec@1 65.625 (59.787)\n",
      "Epoch: [171][312/390]\tTime 0.002 (0.003)\tLoss 1.1562 (1.1767)\tPrec@1 63.281 (59.512)\n",
      "Epoch: [171][390/390]\tTime 0.003 (0.003)\tLoss 1.3606 (1.1849)\tPrec@1 52.500 (58.966)\n",
      "EPOCH: 171 train Results: Prec@1 58.966 Loss: 1.1849\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1902 (1.1902)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3257 (1.2744)\tPrec@1 31.250 (54.970)\n",
      "EPOCH: 171 val Results: Prec@1 54.970 Loss: 1.2744\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [172][0/390]\tTime 0.003 (0.003)\tLoss 0.9601 (0.9601)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [172][78/390]\tTime 0.002 (0.004)\tLoss 1.1697 (1.1404)\tPrec@1 57.812 (61.027)\n",
      "Epoch: [172][156/390]\tTime 0.002 (0.004)\tLoss 1.1050 (1.1572)\tPrec@1 58.594 (60.320)\n",
      "Epoch: [172][234/390]\tTime 0.002 (0.003)\tLoss 1.1808 (1.1710)\tPrec@1 57.812 (59.894)\n",
      "Epoch: [172][312/390]\tTime 0.002 (0.003)\tLoss 1.3384 (1.1827)\tPrec@1 53.125 (59.250)\n",
      "Epoch: [172][390/390]\tTime 0.003 (0.003)\tLoss 1.2105 (1.1892)\tPrec@1 61.250 (59.094)\n",
      "EPOCH: 172 train Results: Prec@1 59.094 Loss: 1.1892\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1956 (1.1956)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5681 (1.2741)\tPrec@1 43.750 (55.080)\n",
      "EPOCH: 172 val Results: Prec@1 55.080 Loss: 1.2741\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [173][0/390]\tTime 0.005 (0.005)\tLoss 1.1366 (1.1366)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [173][78/390]\tTime 0.004 (0.003)\tLoss 1.2871 (1.1532)\tPrec@1 51.562 (60.522)\n",
      "Epoch: [173][156/390]\tTime 0.002 (0.004)\tLoss 1.2466 (1.1660)\tPrec@1 58.594 (59.763)\n",
      "Epoch: [173][234/390]\tTime 0.002 (0.005)\tLoss 1.1574 (1.1698)\tPrec@1 63.281 (59.751)\n",
      "Epoch: [173][312/390]\tTime 0.002 (0.004)\tLoss 1.1480 (1.1791)\tPrec@1 60.156 (59.290)\n",
      "Epoch: [173][390/390]\tTime 0.004 (0.004)\tLoss 1.1233 (1.1870)\tPrec@1 58.750 (59.000)\n",
      "EPOCH: 173 train Results: Prec@1 59.000 Loss: 1.1870\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1353 (1.1353)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4368 (1.2756)\tPrec@1 43.750 (54.780)\n",
      "EPOCH: 173 val Results: Prec@1 54.780 Loss: 1.2756\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [174][0/390]\tTime 0.004 (0.004)\tLoss 1.1733 (1.1733)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [174][78/390]\tTime 0.004 (0.004)\tLoss 1.0670 (1.1437)\tPrec@1 63.281 (60.473)\n",
      "Epoch: [174][156/390]\tTime 0.002 (0.004)\tLoss 1.0978 (1.1643)\tPrec@1 63.281 (59.708)\n",
      "Epoch: [174][234/390]\tTime 0.002 (0.004)\tLoss 1.1848 (1.1712)\tPrec@1 64.844 (59.654)\n",
      "Epoch: [174][312/390]\tTime 0.003 (0.004)\tLoss 1.2141 (1.1836)\tPrec@1 54.688 (59.163)\n",
      "Epoch: [174][390/390]\tTime 0.002 (0.004)\tLoss 1.1937 (1.1925)\tPrec@1 56.250 (58.700)\n",
      "EPOCH: 174 train Results: Prec@1 58.700 Loss: 1.1925\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1529 (1.1529)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4807 (1.2747)\tPrec@1 37.500 (55.050)\n",
      "EPOCH: 174 val Results: Prec@1 55.050 Loss: 1.2747\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [175][0/390]\tTime 0.003 (0.003)\tLoss 1.1025 (1.1025)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [175][78/390]\tTime 0.005 (0.003)\tLoss 1.1936 (1.1363)\tPrec@1 55.469 (61.313)\n",
      "Epoch: [175][156/390]\tTime 0.002 (0.003)\tLoss 1.1947 (1.1536)\tPrec@1 60.156 (60.380)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.004)\tLoss 1.2693 (1.1667)\tPrec@1 57.031 (59.830)\n",
      "Epoch: [175][312/390]\tTime 0.002 (0.003)\tLoss 1.3023 (1.1769)\tPrec@1 55.469 (59.540)\n",
      "Epoch: [175][390/390]\tTime 0.001 (0.003)\tLoss 1.2722 (1.1872)\tPrec@1 52.500 (59.084)\n",
      "EPOCH: 175 train Results: Prec@1 59.084 Loss: 1.1872\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2127 (1.2127)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3512 (1.2782)\tPrec@1 50.000 (55.100)\n",
      "EPOCH: 175 val Results: Prec@1 55.100 Loss: 1.2782\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [176][0/390]\tTime 0.004 (0.004)\tLoss 1.1329 (1.1329)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [176][78/390]\tTime 0.004 (0.003)\tLoss 1.3300 (1.1538)\tPrec@1 54.688 (60.522)\n",
      "Epoch: [176][156/390]\tTime 0.002 (0.003)\tLoss 1.1466 (1.1666)\tPrec@1 60.156 (60.062)\n",
      "Epoch: [176][234/390]\tTime 0.008 (0.003)\tLoss 1.1640 (1.1731)\tPrec@1 59.375 (59.601)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.003)\tLoss 1.1271 (1.1797)\tPrec@1 65.625 (59.365)\n",
      "Epoch: [176][390/390]\tTime 0.006 (0.003)\tLoss 1.2139 (1.1865)\tPrec@1 51.250 (59.130)\n",
      "EPOCH: 176 train Results: Prec@1 59.130 Loss: 1.1865\n",
      "Test: [0/78]\tTime 0.020 (0.020)\tLoss 1.1466 (1.1466)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3977 (1.2783)\tPrec@1 43.750 (54.850)\n",
      "EPOCH: 176 val Results: Prec@1 54.850 Loss: 1.2783\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [177][0/390]\tTime 0.005 (0.005)\tLoss 1.2694 (1.2694)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [177][78/390]\tTime 0.002 (0.003)\tLoss 1.0336 (1.1415)\tPrec@1 64.844 (61.135)\n",
      "Epoch: [177][156/390]\tTime 0.003 (0.004)\tLoss 1.1800 (1.1645)\tPrec@1 60.938 (60.047)\n",
      "Epoch: [177][234/390]\tTime 0.002 (0.004)\tLoss 1.2029 (1.1726)\tPrec@1 59.375 (59.684)\n",
      "Epoch: [177][312/390]\tTime 0.002 (0.004)\tLoss 1.1664 (1.1795)\tPrec@1 60.156 (59.415)\n",
      "Epoch: [177][390/390]\tTime 0.002 (0.003)\tLoss 1.2101 (1.1847)\tPrec@1 62.500 (59.182)\n",
      "EPOCH: 177 train Results: Prec@1 59.182 Loss: 1.1847\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2016 (1.2016)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6524 (1.2705)\tPrec@1 31.250 (55.580)\n",
      "EPOCH: 177 val Results: Prec@1 55.580 Loss: 1.2705\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [178][0/390]\tTime 0.004 (0.004)\tLoss 1.2041 (1.2041)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [178][78/390]\tTime 0.002 (0.003)\tLoss 1.0893 (1.1432)\tPrec@1 63.281 (60.740)\n",
      "Epoch: [178][156/390]\tTime 0.002 (0.003)\tLoss 1.1617 (1.1632)\tPrec@1 62.500 (60.136)\n",
      "Epoch: [178][234/390]\tTime 0.005 (0.003)\tLoss 1.2647 (1.1767)\tPrec@1 56.250 (59.239)\n",
      "Epoch: [178][312/390]\tTime 0.004 (0.003)\tLoss 1.2585 (1.1827)\tPrec@1 53.125 (59.155)\n",
      "Epoch: [178][390/390]\tTime 0.001 (0.003)\tLoss 1.2672 (1.1881)\tPrec@1 55.000 (58.912)\n",
      "EPOCH: 178 train Results: Prec@1 58.912 Loss: 1.1881\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1578 (1.1578)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3895 (1.2777)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 178 val Results: Prec@1 55.150 Loss: 1.2777\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [179][0/390]\tTime 0.004 (0.004)\tLoss 1.2557 (1.2557)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [179][78/390]\tTime 0.008 (0.003)\tLoss 1.3219 (1.1349)\tPrec@1 51.562 (61.531)\n",
      "Epoch: [179][156/390]\tTime 0.012 (0.004)\tLoss 1.2202 (1.1589)\tPrec@1 58.594 (60.226)\n",
      "Epoch: [179][234/390]\tTime 0.023 (0.004)\tLoss 1.2422 (1.1761)\tPrec@1 56.250 (59.564)\n",
      "Epoch: [179][312/390]\tTime 0.003 (0.004)\tLoss 1.0418 (1.1824)\tPrec@1 63.281 (59.275)\n",
      "Epoch: [179][390/390]\tTime 0.003 (0.004)\tLoss 1.3483 (1.1901)\tPrec@1 57.500 (58.912)\n",
      "EPOCH: 179 train Results: Prec@1 58.912 Loss: 1.1901\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1777 (1.1777)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3544 (1.2788)\tPrec@1 43.750 (54.710)\n",
      "EPOCH: 179 val Results: Prec@1 54.710 Loss: 1.2788\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [180][0/390]\tTime 0.003 (0.003)\tLoss 1.2058 (1.2058)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [180][78/390]\tTime 0.002 (0.003)\tLoss 1.1382 (1.1525)\tPrec@1 65.625 (60.127)\n",
      "Epoch: [180][156/390]\tTime 0.004 (0.003)\tLoss 1.1497 (1.1606)\tPrec@1 59.375 (60.126)\n",
      "Epoch: [180][234/390]\tTime 0.002 (0.003)\tLoss 1.1735 (1.1727)\tPrec@1 56.250 (59.581)\n",
      "Epoch: [180][312/390]\tTime 0.004 (0.003)\tLoss 1.0777 (1.1823)\tPrec@1 61.719 (59.233)\n",
      "Epoch: [180][390/390]\tTime 0.001 (0.003)\tLoss 1.2923 (1.1888)\tPrec@1 52.500 (58.884)\n",
      "EPOCH: 180 train Results: Prec@1 58.884 Loss: 1.1888\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1527 (1.1527)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3929 (1.2741)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 180 val Results: Prec@1 54.630 Loss: 1.2741\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [181][0/390]\tTime 0.004 (0.004)\tLoss 1.1340 (1.1340)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [181][78/390]\tTime 0.004 (0.004)\tLoss 1.1405 (1.1289)\tPrec@1 63.281 (60.918)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 1.0958 (1.1540)\tPrec@1 60.156 (59.888)\n",
      "Epoch: [181][234/390]\tTime 0.002 (0.003)\tLoss 1.1981 (1.1680)\tPrec@1 64.062 (59.485)\n",
      "Epoch: [181][312/390]\tTime 0.004 (0.003)\tLoss 1.2110 (1.1765)\tPrec@1 57.812 (59.210)\n",
      "Epoch: [181][390/390]\tTime 0.003 (0.003)\tLoss 1.3059 (1.1830)\tPrec@1 48.750 (58.988)\n",
      "EPOCH: 181 train Results: Prec@1 58.988 Loss: 1.1830\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1341 (1.1341)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4692 (1.2746)\tPrec@1 31.250 (55.250)\n",
      "EPOCH: 181 val Results: Prec@1 55.250 Loss: 1.2746\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [182][0/390]\tTime 0.002 (0.002)\tLoss 1.1806 (1.1806)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [182][78/390]\tTime 0.002 (0.004)\tLoss 1.2954 (1.1409)\tPrec@1 50.781 (60.769)\n",
      "Epoch: [182][156/390]\tTime 0.002 (0.004)\tLoss 1.0875 (1.1616)\tPrec@1 68.750 (59.932)\n",
      "Epoch: [182][234/390]\tTime 0.002 (0.004)\tLoss 1.3600 (1.1768)\tPrec@1 50.781 (59.348)\n",
      "Epoch: [182][312/390]\tTime 0.004 (0.003)\tLoss 1.1777 (1.1821)\tPrec@1 60.156 (59.265)\n",
      "Epoch: [182][390/390]\tTime 0.004 (0.003)\tLoss 1.2742 (1.1878)\tPrec@1 57.500 (59.002)\n",
      "EPOCH: 182 train Results: Prec@1 59.002 Loss: 1.1878\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1682 (1.1682)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5756 (1.2670)\tPrec@1 37.500 (55.090)\n",
      "EPOCH: 182 val Results: Prec@1 55.090 Loss: 1.2670\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [183][0/390]\tTime 0.004 (0.004)\tLoss 1.0906 (1.0906)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [183][78/390]\tTime 0.003 (0.003)\tLoss 1.0995 (1.1354)\tPrec@1 57.812 (60.799)\n",
      "Epoch: [183][156/390]\tTime 0.002 (0.004)\tLoss 1.1438 (1.1526)\tPrec@1 61.719 (60.216)\n",
      "Epoch: [183][234/390]\tTime 0.005 (0.004)\tLoss 1.1596 (1.1663)\tPrec@1 60.156 (59.688)\n",
      "Epoch: [183][312/390]\tTime 0.003 (0.004)\tLoss 1.1696 (1.1752)\tPrec@1 55.469 (59.163)\n",
      "Epoch: [183][390/390]\tTime 0.001 (0.004)\tLoss 1.3079 (1.1866)\tPrec@1 55.000 (58.774)\n",
      "EPOCH: 183 train Results: Prec@1 58.774 Loss: 1.1866\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1939 (1.1939)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5760 (1.2778)\tPrec@1 25.000 (54.980)\n",
      "EPOCH: 183 val Results: Prec@1 54.980 Loss: 1.2778\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [184][0/390]\tTime 0.008 (0.008)\tLoss 1.0511 (1.0511)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [184][78/390]\tTime 0.003 (0.004)\tLoss 1.1843 (1.1337)\tPrec@1 60.156 (61.066)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.004)\tLoss 1.2084 (1.1511)\tPrec@1 56.250 (60.425)\n",
      "Epoch: [184][234/390]\tTime 0.004 (0.004)\tLoss 1.2891 (1.1701)\tPrec@1 51.562 (59.767)\n",
      "Epoch: [184][312/390]\tTime 0.002 (0.005)\tLoss 1.2418 (1.1811)\tPrec@1 53.906 (59.220)\n",
      "Epoch: [184][390/390]\tTime 0.001 (0.004)\tLoss 1.3133 (1.1863)\tPrec@1 53.750 (59.068)\n",
      "EPOCH: 184 train Results: Prec@1 59.068 Loss: 1.1863\n",
      "Test: [0/78]\tTime 0.016 (0.016)\tLoss 1.1310 (1.1310)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.3143 (1.2731)\tPrec@1 43.750 (54.700)\n",
      "EPOCH: 184 val Results: Prec@1 54.700 Loss: 1.2731\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [185][0/390]\tTime 0.003 (0.003)\tLoss 1.0912 (1.0912)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [185][78/390]\tTime 0.003 (0.004)\tLoss 1.2724 (1.1447)\tPrec@1 57.812 (60.799)\n",
      "Epoch: [185][156/390]\tTime 0.006 (0.004)\tLoss 1.1114 (1.1550)\tPrec@1 66.406 (60.012)\n",
      "Epoch: [185][234/390]\tTime 0.002 (0.004)\tLoss 1.3293 (1.1603)\tPrec@1 56.250 (60.020)\n",
      "Epoch: [185][312/390]\tTime 0.002 (0.004)\tLoss 1.0659 (1.1692)\tPrec@1 64.062 (59.565)\n",
      "Epoch: [185][390/390]\tTime 0.003 (0.004)\tLoss 1.3208 (1.1844)\tPrec@1 50.000 (58.924)\n",
      "EPOCH: 185 train Results: Prec@1 58.924 Loss: 1.1844\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1570 (1.1570)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3113 (1.2853)\tPrec@1 37.500 (54.880)\n",
      "EPOCH: 185 val Results: Prec@1 54.880 Loss: 1.2853\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [186][0/390]\tTime 0.004 (0.004)\tLoss 1.1774 (1.1774)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [186][78/390]\tTime 0.002 (0.004)\tLoss 1.1306 (1.1412)\tPrec@1 63.281 (61.116)\n",
      "Epoch: [186][156/390]\tTime 0.003 (0.004)\tLoss 1.1596 (1.1604)\tPrec@1 57.812 (60.047)\n",
      "Epoch: [186][234/390]\tTime 0.002 (0.004)\tLoss 1.2635 (1.1711)\tPrec@1 57.812 (59.658)\n",
      "Epoch: [186][312/390]\tTime 0.002 (0.004)\tLoss 1.1492 (1.1810)\tPrec@1 59.375 (59.193)\n",
      "Epoch: [186][390/390]\tTime 0.003 (0.004)\tLoss 1.0818 (1.1853)\tPrec@1 65.000 (58.970)\n",
      "EPOCH: 186 train Results: Prec@1 58.970 Loss: 1.1853\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1206 (1.1206)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3919 (1.2799)\tPrec@1 56.250 (54.910)\n",
      "EPOCH: 186 val Results: Prec@1 54.910 Loss: 1.2799\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [187][0/390]\tTime 0.003 (0.003)\tLoss 1.2402 (1.2402)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [187][78/390]\tTime 0.003 (0.003)\tLoss 1.0992 (1.1297)\tPrec@1 63.281 (61.363)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.3337 (1.1508)\tPrec@1 55.469 (60.524)\n",
      "Epoch: [187][234/390]\tTime 0.008 (0.003)\tLoss 1.2727 (1.1643)\tPrec@1 53.906 (59.860)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.2941 (1.1780)\tPrec@1 60.156 (59.185)\n",
      "Epoch: [187][390/390]\tTime 0.003 (0.003)\tLoss 1.3519 (1.1872)\tPrec@1 60.000 (58.824)\n",
      "EPOCH: 187 train Results: Prec@1 58.824 Loss: 1.1872\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2061 (1.2061)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4775 (1.2677)\tPrec@1 43.750 (55.250)\n",
      "EPOCH: 187 val Results: Prec@1 55.250 Loss: 1.2677\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [188][0/390]\tTime 0.008 (0.008)\tLoss 1.0771 (1.0771)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [188][78/390]\tTime 0.007 (0.004)\tLoss 1.3635 (1.1380)\tPrec@1 53.125 (61.096)\n",
      "Epoch: [188][156/390]\tTime 0.009 (0.006)\tLoss 1.1611 (1.1540)\tPrec@1 57.812 (60.216)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.005)\tLoss 1.2497 (1.1641)\tPrec@1 58.594 (59.827)\n",
      "Epoch: [188][312/390]\tTime 0.005 (0.005)\tLoss 1.2466 (1.1740)\tPrec@1 57.031 (59.425)\n",
      "Epoch: [188][390/390]\tTime 0.002 (0.005)\tLoss 1.2856 (1.1848)\tPrec@1 53.750 (59.020)\n",
      "EPOCH: 188 train Results: Prec@1 59.020 Loss: 1.1848\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1777 (1.1777)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3740 (1.2697)\tPrec@1 43.750 (55.320)\n",
      "EPOCH: 188 val Results: Prec@1 55.320 Loss: 1.2697\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [189][0/390]\tTime 0.012 (0.012)\tLoss 1.2466 (1.2466)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.005)\tLoss 1.0638 (1.1288)\tPrec@1 64.062 (61.343)\n",
      "Epoch: [189][156/390]\tTime 0.002 (0.005)\tLoss 1.3282 (1.1527)\tPrec@1 57.812 (60.395)\n",
      "Epoch: [189][234/390]\tTime 0.005 (0.004)\tLoss 1.2160 (1.1628)\tPrec@1 53.906 (59.900)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.004)\tLoss 1.2448 (1.1743)\tPrec@1 54.688 (59.235)\n",
      "Epoch: [189][390/390]\tTime 0.015 (0.004)\tLoss 1.3231 (1.1841)\tPrec@1 53.750 (58.962)\n",
      "EPOCH: 189 train Results: Prec@1 58.962 Loss: 1.1841\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1686 (1.1686)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3144 (1.2713)\tPrec@1 43.750 (54.880)\n",
      "EPOCH: 189 val Results: Prec@1 54.880 Loss: 1.2713\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [190][0/390]\tTime 0.003 (0.003)\tLoss 1.0743 (1.0743)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.007)\tLoss 1.1130 (1.1385)\tPrec@1 61.719 (61.165)\n",
      "Epoch: [190][156/390]\tTime 0.002 (0.007)\tLoss 1.2271 (1.1504)\tPrec@1 56.250 (60.340)\n",
      "Epoch: [190][234/390]\tTime 0.004 (0.006)\tLoss 1.1442 (1.1661)\tPrec@1 59.375 (59.591)\n",
      "Epoch: [190][312/390]\tTime 0.002 (0.005)\tLoss 1.2125 (1.1766)\tPrec@1 59.375 (59.382)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.005)\tLoss 1.2334 (1.1866)\tPrec@1 52.500 (59.028)\n",
      "EPOCH: 190 train Results: Prec@1 59.028 Loss: 1.1866\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1747 (1.1747)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5665 (1.2705)\tPrec@1 43.750 (55.160)\n",
      "EPOCH: 190 val Results: Prec@1 55.160 Loss: 1.2705\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [191][0/390]\tTime 0.009 (0.009)\tLoss 1.1637 (1.1637)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.005)\tLoss 1.0557 (1.1414)\tPrec@1 67.969 (61.145)\n",
      "Epoch: [191][156/390]\tTime 0.005 (0.004)\tLoss 1.1607 (1.1596)\tPrec@1 57.031 (60.301)\n",
      "Epoch: [191][234/390]\tTime 0.008 (0.005)\tLoss 1.1294 (1.1682)\tPrec@1 60.156 (59.910)\n",
      "Epoch: [191][312/390]\tTime 0.002 (0.005)\tLoss 1.0790 (1.1757)\tPrec@1 64.844 (59.495)\n",
      "Epoch: [191][390/390]\tTime 0.002 (0.005)\tLoss 1.3157 (1.1876)\tPrec@1 50.000 (58.996)\n",
      "EPOCH: 191 train Results: Prec@1 58.996 Loss: 1.1876\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1470 (1.1470)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3913 (1.2620)\tPrec@1 43.750 (55.740)\n",
      "EPOCH: 191 val Results: Prec@1 55.740 Loss: 1.2620\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [192][0/390]\tTime 0.012 (0.012)\tLoss 1.0552 (1.0552)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [192][78/390]\tTime 0.004 (0.007)\tLoss 1.1254 (1.1486)\tPrec@1 57.031 (61.056)\n",
      "Epoch: [192][156/390]\tTime 0.010 (0.006)\tLoss 1.1876 (1.1596)\tPrec@1 60.938 (60.241)\n",
      "Epoch: [192][234/390]\tTime 0.002 (0.005)\tLoss 1.1339 (1.1715)\tPrec@1 61.719 (59.621)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.006)\tLoss 1.3129 (1.1743)\tPrec@1 53.125 (59.412)\n",
      "Epoch: [192][390/390]\tTime 0.001 (0.006)\tLoss 1.1490 (1.1821)\tPrec@1 62.500 (59.054)\n",
      "EPOCH: 192 train Results: Prec@1 59.054 Loss: 1.1821\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1368 (1.1368)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4099 (1.2647)\tPrec@1 43.750 (55.240)\n",
      "EPOCH: 192 val Results: Prec@1 55.240 Loss: 1.2647\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [193][0/390]\tTime 0.004 (0.004)\tLoss 1.0520 (1.0520)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [193][78/390]\tTime 0.002 (0.004)\tLoss 1.1079 (1.1411)\tPrec@1 59.375 (60.858)\n",
      "Epoch: [193][156/390]\tTime 0.007 (0.004)\tLoss 1.1184 (1.1593)\tPrec@1 61.719 (60.311)\n",
      "Epoch: [193][234/390]\tTime 0.002 (0.004)\tLoss 1.1957 (1.1739)\tPrec@1 55.469 (59.475)\n",
      "Epoch: [193][312/390]\tTime 0.004 (0.004)\tLoss 1.2933 (1.1825)\tPrec@1 59.375 (59.188)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.004)\tLoss 1.2150 (1.1859)\tPrec@1 61.250 (59.052)\n",
      "EPOCH: 193 train Results: Prec@1 59.052 Loss: 1.1859\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1487 (1.1487)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4895 (1.2706)\tPrec@1 43.750 (55.270)\n",
      "EPOCH: 193 val Results: Prec@1 55.270 Loss: 1.2706\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [194][0/390]\tTime 0.005 (0.005)\tLoss 1.0935 (1.0935)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [194][78/390]\tTime 0.004 (0.003)\tLoss 1.0841 (1.1480)\tPrec@1 60.156 (61.007)\n",
      "Epoch: [194][156/390]\tTime 0.002 (0.003)\tLoss 1.0319 (1.1617)\tPrec@1 61.719 (60.106)\n",
      "Epoch: [194][234/390]\tTime 0.005 (0.004)\tLoss 1.1712 (1.1708)\tPrec@1 56.250 (59.754)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.004)\tLoss 1.2796 (1.1783)\tPrec@1 52.344 (59.477)\n",
      "Epoch: [194][390/390]\tTime 0.003 (0.004)\tLoss 1.3376 (1.1828)\tPrec@1 52.500 (59.176)\n",
      "EPOCH: 194 train Results: Prec@1 59.176 Loss: 1.1828\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1656 (1.1656)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3816 (1.2666)\tPrec@1 50.000 (55.100)\n",
      "EPOCH: 194 val Results: Prec@1 55.100 Loss: 1.2666\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [195][0/390]\tTime 0.012 (0.012)\tLoss 1.0714 (1.0714)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [195][78/390]\tTime 0.002 (0.004)\tLoss 1.1404 (1.1296)\tPrec@1 64.844 (61.521)\n",
      "Epoch: [195][156/390]\tTime 0.002 (0.003)\tLoss 1.0928 (1.1497)\tPrec@1 63.281 (60.465)\n",
      "Epoch: [195][234/390]\tTime 0.002 (0.003)\tLoss 1.1996 (1.1603)\tPrec@1 53.906 (60.083)\n",
      "Epoch: [195][312/390]\tTime 0.002 (0.003)\tLoss 1.3859 (1.1739)\tPrec@1 52.344 (59.555)\n",
      "Epoch: [195][390/390]\tTime 0.002 (0.003)\tLoss 1.2962 (1.1805)\tPrec@1 56.250 (59.340)\n",
      "EPOCH: 195 train Results: Prec@1 59.340 Loss: 1.1805\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1712 (1.1712)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4911 (1.2695)\tPrec@1 31.250 (55.100)\n",
      "EPOCH: 195 val Results: Prec@1 55.100 Loss: 1.2695\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [196][0/390]\tTime 0.003 (0.003)\tLoss 1.0672 (1.0672)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 1.1500 (1.1313)\tPrec@1 60.938 (61.640)\n",
      "Epoch: [196][156/390]\tTime 0.003 (0.003)\tLoss 1.2373 (1.1481)\tPrec@1 59.375 (60.729)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.003)\tLoss 1.1112 (1.1721)\tPrec@1 60.938 (59.707)\n",
      "Epoch: [196][312/390]\tTime 0.002 (0.003)\tLoss 1.2204 (1.1764)\tPrec@1 51.562 (59.472)\n",
      "Epoch: [196][390/390]\tTime 0.005 (0.003)\tLoss 1.1674 (1.1824)\tPrec@1 62.500 (59.098)\n",
      "EPOCH: 196 train Results: Prec@1 59.098 Loss: 1.1824\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1347 (1.1347)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4848 (1.2634)\tPrec@1 31.250 (55.490)\n",
      "EPOCH: 196 val Results: Prec@1 55.490 Loss: 1.2634\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [197][0/390]\tTime 0.003 (0.003)\tLoss 1.0441 (1.0441)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [197][78/390]\tTime 0.002 (0.003)\tLoss 1.1960 (1.1165)\tPrec@1 52.344 (61.818)\n",
      "Epoch: [197][156/390]\tTime 0.002 (0.003)\tLoss 1.1993 (1.1396)\tPrec@1 59.375 (61.047)\n",
      "Epoch: [197][234/390]\tTime 0.002 (0.003)\tLoss 1.1067 (1.1566)\tPrec@1 66.406 (60.339)\n",
      "Epoch: [197][312/390]\tTime 0.003 (0.003)\tLoss 1.1900 (1.1739)\tPrec@1 58.594 (59.532)\n",
      "Epoch: [197][390/390]\tTime 0.007 (0.003)\tLoss 0.8923 (1.1819)\tPrec@1 71.250 (59.296)\n",
      "EPOCH: 197 train Results: Prec@1 59.296 Loss: 1.1819\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1929 (1.1929)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1942 (1.2682)\tPrec@1 37.500 (54.970)\n",
      "EPOCH: 197 val Results: Prec@1 54.970 Loss: 1.2682\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [198][0/390]\tTime 0.002 (0.002)\tLoss 1.0860 (1.0860)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [198][78/390]\tTime 0.003 (0.003)\tLoss 1.1750 (1.1267)\tPrec@1 59.375 (61.323)\n",
      "Epoch: [198][156/390]\tTime 0.002 (0.003)\tLoss 1.1748 (1.1526)\tPrec@1 60.156 (60.236)\n",
      "Epoch: [198][234/390]\tTime 0.003 (0.003)\tLoss 1.3476 (1.1659)\tPrec@1 53.906 (59.781)\n",
      "Epoch: [198][312/390]\tTime 0.004 (0.004)\tLoss 1.0905 (1.1739)\tPrec@1 61.719 (59.462)\n",
      "Epoch: [198][390/390]\tTime 0.003 (0.004)\tLoss 1.3181 (1.1818)\tPrec@1 50.000 (59.190)\n",
      "EPOCH: 198 train Results: Prec@1 59.190 Loss: 1.1818\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2053 (1.2053)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5579 (1.2769)\tPrec@1 37.500 (54.670)\n",
      "EPOCH: 198 val Results: Prec@1 54.670 Loss: 1.2769\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [199][0/390]\tTime 0.003 (0.003)\tLoss 1.2040 (1.2040)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [199][78/390]\tTime 0.002 (0.003)\tLoss 1.1427 (1.1223)\tPrec@1 60.156 (61.729)\n",
      "Epoch: [199][156/390]\tTime 0.002 (0.003)\tLoss 1.2468 (1.1451)\tPrec@1 53.906 (60.793)\n",
      "Epoch: [199][234/390]\tTime 0.002 (0.003)\tLoss 1.1699 (1.1625)\tPrec@1 58.594 (60.033)\n",
      "Epoch: [199][312/390]\tTime 0.007 (0.003)\tLoss 1.2219 (1.1740)\tPrec@1 61.719 (59.557)\n",
      "Epoch: [199][390/390]\tTime 0.002 (0.003)\tLoss 1.2168 (1.1795)\tPrec@1 57.500 (59.424)\n",
      "EPOCH: 199 train Results: Prec@1 59.424 Loss: 1.1795\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1458 (1.1458)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3556 (1.2712)\tPrec@1 43.750 (54.930)\n",
      "EPOCH: 199 val Results: Prec@1 54.930 Loss: 1.2712\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [200][0/390]\tTime 0.002 (0.002)\tLoss 1.0840 (1.0840)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [200][78/390]\tTime 0.002 (0.003)\tLoss 1.1696 (1.1471)\tPrec@1 58.594 (60.661)\n",
      "Epoch: [200][156/390]\tTime 0.008 (0.003)\tLoss 0.9888 (1.1620)\tPrec@1 71.875 (60.216)\n",
      "Epoch: [200][234/390]\tTime 0.005 (0.003)\tLoss 1.1119 (1.1677)\tPrec@1 61.719 (59.794)\n",
      "Epoch: [200][312/390]\tTime 0.003 (0.003)\tLoss 1.2753 (1.1729)\tPrec@1 57.031 (59.625)\n",
      "Epoch: [200][390/390]\tTime 0.001 (0.003)\tLoss 1.4209 (1.1816)\tPrec@1 50.000 (59.182)\n",
      "EPOCH: 200 train Results: Prec@1 59.182 Loss: 1.1816\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1840 (1.1840)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1576 (1.2666)\tPrec@1 43.750 (55.100)\n",
      "EPOCH: 200 val Results: Prec@1 55.100 Loss: 1.2666\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "End time:  Thu Apr  4 23:18:58 2024\n",
      "train executed in 318.0764 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.0005, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer6 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer6.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainers):\n",
    "    train_accu = [trainer.train_precs for trainer in trainers]\n",
    "    test_accu = [trainer.test_precs for trainer in trainers]\n",
    "    lrs = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005]\n",
    "    c = ['r-', 'b-', 'g-', 'y-', 'k-', 'm-']\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "\n",
    "    for i, y1 in enumerate(train_accu):\n",
    "        ax1.plot(x, y1, c[i], label='lr ' + str(lrs[i])) \n",
    "    \n",
    "    for i, y2 in enumerate(test_accu):\n",
    "        ax2.plot(x, y2, c[i], label='lr ' + str(lrs[i])) \n",
    "\n",
    "    ax1.set_title('Training Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "\n",
    "    ax2.set_title('Testing Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax2.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy([trainer, trainer5, trainer2, trainer4, trainer3, trainer6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainers):\n",
    "    train_losses = [trainer.train_losses for trainer in trainers]\n",
    "    test_losses = [trainer.test_losses for trainer in trainers]\n",
    "    lrs = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005]\n",
    "    c = ['r-', 'b-', 'g-', 'y-', 'k-', 'm-']\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "\n",
    "    for i, y1 in enumerate(train_losses):\n",
    "        ax1.plot(x, y1, c[i], label='lr ' + str(lrs[i])) \n",
    "    \n",
    "    for i, y2 in enumerate(test_losses):\n",
    "        ax2.plot(x, y2, c[i], label='lr ' + str(lrs[i])) \n",
    "\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    ax2.set_title('Testing Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy([trainer, trainer5, trainer2, trainer4, trainer3, trainer6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调bs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
