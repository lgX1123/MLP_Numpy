{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 520556528 + 530101303"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Assignment1-Dataset/'\n",
    "\n",
    "train_X = np.load(file_path + 'train_data.npy')\n",
    "train_y = np.load(file_path + 'train_label.npy')\n",
    "test_X = np.load(file_path + 'test_data.npy')\n",
    "test_y = np.load(file_path + 'test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2626576 , -0.07343279,  0.19805761, -1.51868025],\n",
       "       [ 0.39725082, -0.7781142 ,  0.4378592 ,  1.24392688],\n",
       "       [ 1.81033706, -1.26092481, -1.66679203,  1.05324767]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array = np.random.randn(3, 4)\n",
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:36:19 2024\n",
      "End time:  Thu Apr  4 23:36:20 2024\n",
      "test_fun executed in 1.0051 seconds\n"
     ]
    }
   ],
   "source": [
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print('Start time: ', time.ctime())\n",
    "        start_time = time.time()  # start time\n",
    "\n",
    "        result = func(*args, **kwargs)  # run\n",
    "\n",
    "        end_time = time.time()  # end time\n",
    "        print('End time: ', time.ctime())\n",
    "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timer\n",
    "def test_fun(x):\n",
    "    time.sleep(x)\n",
    "\n",
    "test_fun(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaiming Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py.\n",
    "\n",
    "Modify tensor to np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03728282, -0.42833153, -0.62533594,  0.3412265 , -0.36481554,\n",
       "         0.83977459],\n",
       "       [ 0.06735772,  0.2505623 ,  0.90392449,  0.37878901, -0.23172341,\n",
       "        -0.14801214],\n",
       "       [-0.72738503,  0.87438122,  0.78539559, -0.61401962, -0.10661531,\n",
       "         0.28266122],\n",
       "       [-0.73258472, -0.32651081,  0.28859091, -0.41219357, -0.03792021,\n",
       "         0.10121226],\n",
       "       [ 0.69639868,  0.50526404,  0.40791515,  0.12630546,  0.39338233,\n",
       "         0.27513815]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    SELU              :math:`\\frac{3}{4}`\n",
    "    ================= ====================================================\n",
    "    \"\"\"\n",
    "    \n",
    "    if nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    elif nonlinearity == 'selu':\n",
    "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(array):\n",
    "    dimensions = len(array.shape)\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    num_input_fmaps = array.shape[1]\n",
    "    num_output_fmaps = array.shape[0]\n",
    "    receptive_field_size = 1\n",
    "    if dimensions > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in array.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(array, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
    "    fan = _calculate_correct_fan(array, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    return np.random.normal(0, std, array.shape)\n",
    "\n",
    "kaiming_normal_(np.array([0] * 30).reshape(5, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(object):\n",
    "    def __init__(self, data, requires_grad, skip_decay=False):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.skip_decay = skip_decay\n",
    "        self.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    preds = output.argmax(axis=-1, keepdims=True)\n",
    "    return np.mean(preds == target) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train_X, test_X, mode=None):\n",
    "    if mode == 'min-max':\n",
    "        min_each_feature = np.min(train_X, axis=0)\n",
    "        max_each_feature = np.max(train_X, axis=0)\n",
    "        scale = max_each_feature - min_each_feature\n",
    "        scale[scale == 0] = 1   # To avoid divided by 0\n",
    "        scaled_train = (train_X - min_each_feature) / scale\n",
    "        scaled_test = (test_X - min_each_feature) / scale\n",
    "        return scaled_train, scaled_test\n",
    "\n",
    "    if mode == 'norm':\n",
    "        std_each_feature = np.std(train_X, axis=0)\n",
    "        mean_each_feature = np.mean(train_X, axis=0)\n",
    "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
    "        norm_train = (train_X - mean_each_feature) / std_each_feature\n",
    "        norm_test = (test_X - mean_each_feature) / std_each_feature\n",
    "        return norm_train, norm_test\n",
    "\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        self.name = name \n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def backward(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.19805761, 0.        ],\n",
       "       [0.39725082, 0.        , 0.4378592 , 1.24392688],\n",
       "       [1.81033706, 0.        , 0.        , 1.05324767]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class relu(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_output[self.input <= 0] = 0\n",
    "        return grad_output\n",
    "    \n",
    "\n",
    "test_relu = relu('test_relu')\n",
    "_ = test_relu.forward(test_array)\n",
    "test_relu.backward(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class leaky_relu(Layer):\n",
    "    def __init__(self, name, alpha, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.where(input > 0, input, self.alpha * input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        tmp = np.where(self.input > 0, 1, self.alpha)\n",
    "        return tmp * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.y = 1. / (1. + np.exp(-input))   # save sigmoid for more convenient grad computation\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return self.y * (1 - self.y) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tanh(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.y = np.tanh(input)\n",
    "        return np.tanh(input)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return (1 - self.y ** 2) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23702116, 0.23702116, 0.28893652, 0.23702116],\n",
       "       [0.19819671, 0.13322097, 0.20641081, 0.46217152],\n",
       "       [0.55672233, 0.09107921, 0.09107921, 0.26111926]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class softmax(Layer):\n",
    "    def __init__(self, name, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "            input.shape = [batch size, num_class]\n",
    "        \"\"\"\n",
    "        x_max = input.max(axis=-1, keepdims=True)       # to avoid overflow\n",
    "        x_exp = np.exp(input - x_max)\n",
    "        return x_exp / x_exp.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # packaged in CrossEntropyLoss\n",
    "        return grad_output\n",
    "\n",
    "softmax('test_softmax').forward(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(Layer):\n",
    "    def __init__(self, name, in_num, out_num):\n",
    "        super().__init__(name, requires_grad=True)\n",
    "        self.in_num = in_num\n",
    "        self.out_num = out_num\n",
    "\n",
    "        W = kaiming_normal_(np.array([0] * in_num * out_num).reshape(in_num, out_num), a=math.sqrt(5))     # Kaiming Init\n",
    "        self.W = Parameter(W, self.requires_grad)\n",
    "        self.b = Parameter(np.zeros(out_num), self.requires_grad)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return input @ self.W.data + self.b.data      # [batch size, in_num] @ [in_num, out_num] + [out_num] => [batch size, out_num]\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "            grad_output: [batch size, out_num]\n",
    "        \"\"\"\n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.W.grad = self.input.T @ grad_output / batch_size\n",
    "        self.b.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return grad_output @ self.W.data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        self.softmax = softmax('softmax')\n",
    "\n",
    "    def __call__(self, input, ground_truth):\n",
    "        self.bacth_size = input.shape[0]\n",
    "        self.class_num = input.shape[1]\n",
    "\n",
    "        preds = self.softmax.forward(input)\n",
    "        ground_truth = self.one_hot_encoding(ground_truth)\n",
    "\n",
    "        self.grad = preds - ground_truth    #TODO: 推导要写在report上不？\n",
    "\n",
    "        loss = -1 * (ground_truth * np.log(preds + 1e-8)).sum() / self.bacth_size\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def one_hot_encoding(self, x):\n",
    "        one_hot_encoded = np.zeros((self.bacth_size, self.class_num))\n",
    "        one_hot_encoded[np.arange(x.size), x.flatten()] = 1\n",
    "        return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchnorm(Layer):\n",
    "    def __init__(self, name, shape, requires_grad=True):\n",
    "        super().__init__(name)\n",
    "        self.gamma = Parameter(np.random.uniform(0.9, 1.1, shape), requires_grad, skip_decay=True)\n",
    "        self.beta = Parameter(np.random.uniform(-0.1, 0.1, shape), requires_grad, skip_decay=True)\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        self.running_mean = Parameter(np.zeros(shape), False)\n",
    "        self.running_var = Parameter(np.zeros(shape), False)\n",
    "\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            batch_mean = input.mean(axis=0)\n",
    "            batch_var = input.var(axis=0)\n",
    "            batch_std = np.sqrt(batch_var + 1e-8)    # To avoid divided by 0\n",
    "\n",
    "            momentum = 0.9\n",
    "            self.running_mean.data = momentum * self.running_mean.data + (1 - momentum) * batch_mean\n",
    "            self.running_var.data = momentum * self.running_var.data + (1 - momentum) * batch_var\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            batch_mean = self.running_mean.data\n",
    "            batch_std = np.sqrt(self.running_var.data)\n",
    "\n",
    "        self.norm = (input - batch_mean) / batch_std\n",
    "        self.gamma_norm = self.gamma.data / batch_std\n",
    "\n",
    "        return self.gamma.data * self.norm + self.beta.data\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):        \n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.gamma.grad = (grad_output * self.norm).sum(axis=0) / batch_size\n",
    "        self.beta.grad = grad_output.sum(axis=0) / batch_size\n",
    "        return self.gamma_norm * (grad_output - self.norm * self.gamma.grad - self.beta.grad)       # TODO: 推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropout(Layer):\n",
    "    def __init__(self, name, drop_rate, requires_grad=False):\n",
    "        super().__init__(name, requires_grad)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.train:\n",
    "            self.mask = np.random.uniform(0, 1, input.shape) > self.drop_rate\n",
    "            return input * self.mask * self.fix_value\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.train:\n",
    "            return grad_output * self.mask\n",
    "        else:\n",
    "            return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.num_layers = 0\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if layer.requires_grad:\n",
    "            if hasattr(layer, 'W'):\n",
    "                self.params.append(layer.W)\n",
    "            if hasattr(layer, 'b'):\n",
    "                self.params.append(layer.b)\n",
    "            if hasattr(layer, 'gamma'):\n",
    "                self.params.append(layer.gamma)\n",
    "            if hasattr(layer, 'beta'):\n",
    "                self.params.append(layer.beta)\n",
    "        self.num_layers += 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, x):\n",
    "        for layer in self.layers[::-1]:\n",
    "            x = layer.backward(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = True\n",
    "    \n",
    "    def test(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, momentum, lr, weight_decay):\n",
    "        self.parameters = parameters\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "    def step(self):\n",
    "        for i, (v, p) in enumerate(zip(self.v, self.parameters)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            v = self.momentum * v + self.lr * p.grad\n",
    "            self.v[i] = v\n",
    "            p.data -= self.v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(object):\n",
    "    def __init__(self, parameters, lr, weight_decay=0, beta=(0.9, 0.999), eps=1e-8):\n",
    "        self.beta1 = beta[0]\n",
    "        self.beta2 = beta[1]\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.parameters = parameters\n",
    "        self.m = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
    "\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.iterations += 1\n",
    "        for i, (p, m, v) in enumerate(zip(self.parameters, self.m, self.v)):\n",
    "            if not p.skip_decay:\n",
    "                p.data -= self.weight_decay * p.data\n",
    "            m = self.beta1 * m + (1 - self.beta1) * p.grad\n",
    "            v = self.beta2 * v + (1 - self.beta2) * np.power(p.grad, 2)\n",
    "\n",
    "            self.m[i] = m\n",
    "            self.v[i] = v\n",
    "            \n",
    "            # bias correction\n",
    "            m = m / (1 - np.power(self.beta1, self.iterations))\n",
    "            v = v / (1 - np.power(self.beta2, self.iterations))\n",
    "\n",
    "            p.data -= self.lr * m / (np.sqrt(v + self.eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLR(object):\n",
    "    def __init__(self, optimizer, T_max):\n",
    "        self.optimizer = optimizer\n",
    "        self.T_max = T_max\n",
    "        self.n = -1\n",
    "        self.base_lr = optimizer.lr\n",
    "        self.step()\n",
    "\n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "        lr = self.get_lr()\n",
    "        self.optimizer.lr = lr\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.base_lr\n",
    "        cos = np.cos(np.pi * self.n / self.T_max)\n",
    "        return self.base_lr * (1 + cos) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, config, model=None, train_loader=None, val_loader=None):\n",
    "        self.config = config\n",
    "        self.epochs = self.config['epoch']\n",
    "        self.lr = self.config['lr']\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.print_freq = self.config['print_freq']\n",
    "        self.scheduler = self.config['scheduler']\n",
    "        self.train_precs = []\n",
    "        self.test_precs = []\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        if self.config['optimizer'] == 'sgd':\n",
    "            self.optimizer = SGD(self.model.params, self.config['momentum'], self.lr, self.config['weight_decay'])\n",
    "        elif self.config['optimizer'] == 'adam':\n",
    "            self.optimizer = Adam(self.model.params, self.lr, self.config['weight_decay'])\n",
    "        if self.scheduler:\n",
    "            self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
    "\n",
    "    @timer\n",
    "    def train(self):\n",
    "        best_acc1 = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
    "            self.train_per_epoch(epoch)\n",
    "            if self.scheduler:\n",
    "                self.train_scheduler.step()\n",
    "\n",
    "            # evaluate on validation set\n",
    "            acc1 = self.validate(epoch)\n",
    "\n",
    "            # remember best prec@1\n",
    "            best_acc1 = max(acc1, best_acc1)\n",
    "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
    "            print(output_best)\n",
    "\n",
    "    \n",
    "    def train_per_epoch(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        for i, (input, target) in enumerate(self.train_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            self.model.backward(self.criterion.grad)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.train_loader) - 1):\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        epoch + 1, i, len(self.train_loader) - 1, batch_time=batch_time,\n",
    "                        loss=losses, top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='train', top1=top1, losses=losses))\n",
    "        print(output)\n",
    "        self.train_losses.append(losses.avg)\n",
    "        self.train_precs.append(top1.avg)\n",
    "                \n",
    "    def validate(self, epoch):\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "\n",
    "        self.model.test()\n",
    "\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output, target)\n",
    "            losses.update(loss, input.shape[0])\n",
    "            top1.update(prec1, input.shape[0])\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if (i % self.print_freq == 0) or (i == len(self.val_loader) - 1):\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                        i, len(self.val_loader) - 1, batch_time=batch_time, loss=losses,\n",
    "                        top1=top1))\n",
    "        \n",
    "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='val', top1=top1, losses=losses))\n",
    "        print(output)\n",
    "        self.test_losses.append(losses.avg)\n",
    "        self.test_precs.append(top1.avg)\n",
    "\n",
    "        return top1.avg\n",
    "    \n",
    "    def plot_cm(self, save_path):\n",
    "        self.model.test()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for i, (input, target) in enumerate(self.val_loader):\n",
    "            # compute output\n",
    "            output = self.model.forward(input)\n",
    "            output = np.argmax(output, axis=1)\n",
    "            y_pred += list(output)\n",
    "            y_true += list(target.flatten())\n",
    "            \n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.ylabel(\"Ground Truth\")\n",
    "        plt.xlabel(\"Prediction\")\n",
    "        plt.savefig(save_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.index = np.arange(X.shape[0])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            if self.seed is not None:\n",
    "                np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.index)\n",
    "        self.n = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.n >= len(self.index):\n",
    "            raise StopIteration\n",
    "        \n",
    "        index = self.index[self.n:self.n + self.batch_size]\n",
    "        batch_X = self.X[index]\n",
    "        batch_y = self.y[index]\n",
    "        self.n += self.batch_size\n",
    "\n",
    "        return batch_X, batch_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            num of batch\n",
    "        \"\"\"\n",
    "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configs of Baseline vs Best Model\n",
    "\n",
    "| Modules                | Baseline          | Best Model |\n",
    "| ---------------------- | ----------------- | ---------- |\n",
    "| Batch size             | 128               |            |\n",
    "| Learning rate          | 0.1               |            |\n",
    "| Scheduler              | CosineAnnealingLR |            |\n",
    "| Epoch                  | 100               |            |\n",
    "| Pre-processing         | Yes               |            |\n",
    "| Number of Hidden layer | 2                 | 2          |\n",
    "| Hidden units           | [64, 32]          |            |\n",
    "| Activations            | [Relu, Relu]      |            |\n",
    "| Weight initialisation  | Kaiming           | Kaiming    |\n",
    "| Weight decay           | 5e-4              |            |\n",
    "| Optimizer              | SGD with Momentum |            |\n",
    "| Momentum               | 0.9               |            |\n",
    "| Batch Normalisation    | Yes               |            |\n",
    "| Dropout rate           | 0.1               |            |\n",
    "| Accuracy               | 53.03%            |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:37:09 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/390]\tTime 0.007 (0.007)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.003 (0.004)\tLoss 2.4991 (3.3599)\tPrec@1 32.031 (22.636)\n",
      "Epoch: [1][156/390]\tTime 0.007 (0.004)\tLoss 2.4369 (2.8159)\tPrec@1 24.219 (26.866)\n",
      "Epoch: [1][234/390]\tTime 0.003 (0.004)\tLoss 1.9684 (2.5443)\tPrec@1 34.375 (29.451)\n",
      "Epoch: [1][312/390]\tTime 0.002 (0.004)\tLoss 1.8863 (2.3738)\tPrec@1 34.375 (31.178)\n",
      "Epoch: [1][390/390]\tTime 0.001 (0.005)\tLoss 1.6280 (2.2570)\tPrec@1 41.250 (32.432)\n",
      "EPOCH: 1 train Results: Prec@1 32.432 Loss: 2.2570\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.5203 (1.5203)\tPrec@1 47.656 (47.656)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4963 (1.6156)\tPrec@1 31.250 (42.860)\n",
      "EPOCH: 1 val Results: Prec@1 42.860 Loss: 1.6156\n",
      "Best Prec@1: 42.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/390]\tTime 0.011 (0.011)\tLoss 1.6112 (1.6112)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [2][78/390]\tTime 0.002 (0.003)\tLoss 1.5126 (1.6195)\tPrec@1 45.312 (43.008)\n",
      "Epoch: [2][156/390]\tTime 0.007 (0.003)\tLoss 1.6208 (1.6061)\tPrec@1 44.531 (43.252)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.003)\tLoss 1.7222 (1.5965)\tPrec@1 42.969 (43.544)\n",
      "Epoch: [2][312/390]\tTime 0.006 (0.003)\tLoss 1.7094 (1.5855)\tPrec@1 40.625 (43.960)\n",
      "Epoch: [2][390/390]\tTime 0.003 (0.003)\tLoss 1.4745 (1.5758)\tPrec@1 52.500 (44.314)\n",
      "EPOCH: 2 train Results: Prec@1 44.314 Loss: 1.5758\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3885 (1.3885)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4268 (1.5008)\tPrec@1 31.250 (46.490)\n",
      "EPOCH: 2 val Results: Prec@1 46.490 Loss: 1.5008\n",
      "Best Prec@1: 46.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/390]\tTime 0.004 (0.004)\tLoss 1.4851 (1.4851)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [3][78/390]\tTime 0.004 (0.004)\tLoss 1.4391 (1.4743)\tPrec@1 41.406 (47.646)\n",
      "Epoch: [3][156/390]\tTime 0.004 (0.004)\tLoss 1.3910 (1.4640)\tPrec@1 50.781 (48.134)\n",
      "Epoch: [3][234/390]\tTime 0.008 (0.004)\tLoss 1.4514 (1.4645)\tPrec@1 42.969 (48.311)\n",
      "Epoch: [3][312/390]\tTime 0.002 (0.004)\tLoss 1.5078 (1.4632)\tPrec@1 50.000 (48.230)\n",
      "Epoch: [3][390/390]\tTime 0.005 (0.004)\tLoss 1.4212 (1.4601)\tPrec@1 51.250 (48.280)\n",
      "EPOCH: 3 train Results: Prec@1 48.280 Loss: 1.4601\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3230 (1.3230)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4953 (1.4318)\tPrec@1 43.750 (49.310)\n",
      "EPOCH: 3 val Results: Prec@1 49.310 Loss: 1.4318\n",
      "Best Prec@1: 49.310\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/390]\tTime 0.002 (0.002)\tLoss 1.4993 (1.4993)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [4][78/390]\tTime 0.004 (0.004)\tLoss 1.4838 (1.3895)\tPrec@1 55.469 (51.226)\n",
      "Epoch: [4][156/390]\tTime 0.006 (0.004)\tLoss 1.4162 (1.3892)\tPrec@1 46.094 (50.931)\n",
      "Epoch: [4][234/390]\tTime 0.008 (0.005)\tLoss 1.5272 (1.3868)\tPrec@1 47.656 (50.775)\n",
      "Epoch: [4][312/390]\tTime 0.002 (0.007)\tLoss 1.3532 (1.3878)\tPrec@1 52.344 (50.774)\n",
      "Epoch: [4][390/390]\tTime 0.003 (0.008)\tLoss 1.3277 (1.3876)\tPrec@1 51.250 (50.776)\n",
      "EPOCH: 4 train Results: Prec@1 50.776 Loss: 1.3876\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2728 (1.2728)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4631 (1.3819)\tPrec@1 31.250 (50.290)\n",
      "EPOCH: 4 val Results: Prec@1 50.290 Loss: 1.3819\n",
      "Best Prec@1: 50.290\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/390]\tTime 0.003 (0.003)\tLoss 1.3705 (1.3705)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.004)\tLoss 1.3997 (1.3053)\tPrec@1 50.000 (53.807)\n",
      "Epoch: [5][156/390]\tTime 0.007 (0.004)\tLoss 1.3703 (1.3289)\tPrec@1 54.688 (52.831)\n",
      "Epoch: [5][234/390]\tTime 0.012 (0.007)\tLoss 1.3619 (1.3279)\tPrec@1 58.594 (52.836)\n",
      "Epoch: [5][312/390]\tTime 0.017 (0.007)\tLoss 1.2987 (1.3298)\tPrec@1 54.688 (52.733)\n",
      "Epoch: [5][390/390]\tTime 0.002 (0.007)\tLoss 1.3211 (1.3338)\tPrec@1 58.750 (52.574)\n",
      "EPOCH: 5 train Results: Prec@1 52.574 Loss: 1.3338\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2136 (1.2136)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3947 (1.3527)\tPrec@1 37.500 (51.490)\n",
      "EPOCH: 5 val Results: Prec@1 51.490 Loss: 1.3527\n",
      "Best Prec@1: 51.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/390]\tTime 0.002 (0.002)\tLoss 1.2183 (1.2183)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [6][78/390]\tTime 0.009 (0.008)\tLoss 1.3751 (1.2636)\tPrec@1 43.750 (55.667)\n",
      "Epoch: [6][156/390]\tTime 0.008 (0.006)\tLoss 1.1498 (1.2735)\tPrec@1 60.156 (55.096)\n",
      "Epoch: [6][234/390]\tTime 0.002 (0.006)\tLoss 1.3359 (1.2845)\tPrec@1 53.125 (54.731)\n",
      "Epoch: [6][312/390]\tTime 0.025 (0.006)\tLoss 1.3691 (1.2956)\tPrec@1 49.219 (54.288)\n",
      "Epoch: [6][390/390]\tTime 0.001 (0.006)\tLoss 1.5191 (1.3019)\tPrec@1 50.000 (54.026)\n",
      "EPOCH: 6 train Results: Prec@1 54.026 Loss: 1.3019\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2339 (1.2339)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3587 (1.3346)\tPrec@1 43.750 (51.320)\n",
      "EPOCH: 6 val Results: Prec@1 51.320 Loss: 1.3346\n",
      "Best Prec@1: 51.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/390]\tTime 0.003 (0.003)\tLoss 1.3259 (1.3259)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [7][78/390]\tTime 0.002 (0.006)\tLoss 1.2340 (1.2446)\tPrec@1 57.812 (55.973)\n",
      "Epoch: [7][156/390]\tTime 0.002 (0.005)\tLoss 1.4204 (1.2565)\tPrec@1 46.094 (55.519)\n",
      "Epoch: [7][234/390]\tTime 0.002 (0.004)\tLoss 1.3231 (1.2662)\tPrec@1 50.781 (54.990)\n",
      "Epoch: [7][312/390]\tTime 0.013 (0.005)\tLoss 1.2994 (1.2704)\tPrec@1 53.906 (54.825)\n",
      "Epoch: [7][390/390]\tTime 0.009 (0.006)\tLoss 1.3562 (1.2776)\tPrec@1 51.250 (54.622)\n",
      "EPOCH: 7 train Results: Prec@1 54.622 Loss: 1.2776\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2084 (1.2084)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4724 (1.3130)\tPrec@1 18.750 (52.880)\n",
      "EPOCH: 7 val Results: Prec@1 52.880 Loss: 1.3130\n",
      "Best Prec@1: 52.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/390]\tTime 0.014 (0.014)\tLoss 1.2051 (1.2051)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [8][78/390]\tTime 0.002 (0.005)\tLoss 1.2271 (1.2241)\tPrec@1 55.469 (56.458)\n",
      "Epoch: [8][156/390]\tTime 0.013 (0.006)\tLoss 1.3590 (1.2454)\tPrec@1 48.438 (55.419)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.005)\tLoss 1.2339 (1.2477)\tPrec@1 55.469 (55.416)\n",
      "Epoch: [8][312/390]\tTime 0.010 (0.005)\tLoss 1.1045 (1.2505)\tPrec@1 64.062 (55.511)\n",
      "Epoch: [8][390/390]\tTime 0.002 (0.005)\tLoss 1.3797 (1.2581)\tPrec@1 50.000 (55.340)\n",
      "EPOCH: 8 train Results: Prec@1 55.340 Loss: 1.2581\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1711 (1.1711)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4565 (1.3065)\tPrec@1 31.250 (53.270)\n",
      "EPOCH: 8 val Results: Prec@1 53.270 Loss: 1.3065\n",
      "Best Prec@1: 53.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/390]\tTime 0.003 (0.003)\tLoss 1.1923 (1.1923)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [9][78/390]\tTime 0.002 (0.005)\tLoss 1.1151 (1.2010)\tPrec@1 64.844 (57.466)\n",
      "Epoch: [9][156/390]\tTime 0.007 (0.007)\tLoss 1.3641 (1.2135)\tPrec@1 50.781 (57.011)\n",
      "Epoch: [9][234/390]\tTime 0.008 (0.007)\tLoss 1.3672 (1.2309)\tPrec@1 47.656 (56.220)\n",
      "Epoch: [9][312/390]\tTime 0.002 (0.007)\tLoss 1.3375 (1.2389)\tPrec@1 48.438 (55.960)\n",
      "Epoch: [9][390/390]\tTime 0.012 (0.007)\tLoss 1.4929 (1.2480)\tPrec@1 47.500 (55.558)\n",
      "EPOCH: 9 train Results: Prec@1 55.558 Loss: 1.2480\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1522 (1.1522)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3399 (1.3017)\tPrec@1 31.250 (53.560)\n",
      "EPOCH: 9 val Results: Prec@1 53.560 Loss: 1.3017\n",
      "Best Prec@1: 53.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/390]\tTime 0.004 (0.004)\tLoss 1.1641 (1.1641)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [10][78/390]\tTime 0.002 (0.004)\tLoss 1.0087 (1.1868)\tPrec@1 65.625 (57.812)\n",
      "Epoch: [10][156/390]\tTime 0.002 (0.005)\tLoss 1.2396 (1.2072)\tPrec@1 56.250 (57.195)\n",
      "Epoch: [10][234/390]\tTime 0.002 (0.005)\tLoss 1.1993 (1.2202)\tPrec@1 60.156 (56.533)\n",
      "Epoch: [10][312/390]\tTime 0.002 (0.005)\tLoss 1.3262 (1.2287)\tPrec@1 49.219 (56.257)\n",
      "Epoch: [10][390/390]\tTime 0.007 (0.005)\tLoss 1.0870 (1.2354)\tPrec@1 62.500 (55.962)\n",
      "EPOCH: 10 train Results: Prec@1 55.962 Loss: 1.2354\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2086 (1.2086)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.002)\tLoss 1.7199 (1.2792)\tPrec@1 25.000 (53.790)\n",
      "EPOCH: 10 val Results: Prec@1 53.790 Loss: 1.2792\n",
      "Best Prec@1: 53.790\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/390]\tTime 0.006 (0.006)\tLoss 1.1264 (1.1264)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [11][78/390]\tTime 0.002 (0.004)\tLoss 1.1389 (1.1872)\tPrec@1 57.812 (57.575)\n",
      "Epoch: [11][156/390]\tTime 0.012 (0.004)\tLoss 1.0176 (1.1970)\tPrec@1 65.625 (57.300)\n",
      "Epoch: [11][234/390]\tTime 0.002 (0.004)\tLoss 1.2753 (1.2061)\tPrec@1 53.906 (56.805)\n",
      "Epoch: [11][312/390]\tTime 0.005 (0.004)\tLoss 1.1048 (1.2123)\tPrec@1 63.281 (56.684)\n",
      "Epoch: [11][390/390]\tTime 0.004 (0.004)\tLoss 1.2133 (1.2213)\tPrec@1 58.750 (56.304)\n",
      "EPOCH: 11 train Results: Prec@1 56.304 Loss: 1.2213\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1051 (1.1051)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4246 (1.2827)\tPrec@1 37.500 (54.020)\n",
      "EPOCH: 11 val Results: Prec@1 54.020 Loss: 1.2827\n",
      "Best Prec@1: 54.020\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/390]\tTime 0.005 (0.005)\tLoss 1.0834 (1.0834)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [12][78/390]\tTime 0.003 (0.003)\tLoss 1.1179 (1.1682)\tPrec@1 60.156 (58.534)\n",
      "Epoch: [12][156/390]\tTime 0.003 (0.003)\tLoss 1.2895 (1.1816)\tPrec@1 54.688 (58.081)\n",
      "Epoch: [12][234/390]\tTime 0.002 (0.006)\tLoss 1.0579 (1.1943)\tPrec@1 70.312 (57.437)\n",
      "Epoch: [12][312/390]\tTime 0.002 (0.005)\tLoss 1.2523 (1.2032)\tPrec@1 58.594 (57.336)\n",
      "Epoch: [12][390/390]\tTime 0.001 (0.005)\tLoss 1.0246 (1.2108)\tPrec@1 61.250 (57.030)\n",
      "EPOCH: 12 train Results: Prec@1 57.030 Loss: 1.2108\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1413 (1.1413)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3414 (1.2785)\tPrec@1 50.000 (54.350)\n",
      "EPOCH: 12 val Results: Prec@1 54.350 Loss: 1.2785\n",
      "Best Prec@1: 54.350\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/390]\tTime 0.005 (0.005)\tLoss 1.1582 (1.1582)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.004)\tLoss 1.1310 (1.1562)\tPrec@1 58.594 (58.633)\n",
      "Epoch: [13][156/390]\tTime 0.003 (0.004)\tLoss 1.1211 (1.1674)\tPrec@1 60.938 (58.569)\n",
      "Epoch: [13][234/390]\tTime 0.002 (0.004)\tLoss 1.2753 (1.1844)\tPrec@1 56.250 (57.985)\n",
      "Epoch: [13][312/390]\tTime 0.011 (0.004)\tLoss 1.1894 (1.1968)\tPrec@1 55.469 (57.431)\n",
      "Epoch: [13][390/390]\tTime 0.003 (0.004)\tLoss 1.1324 (1.2011)\tPrec@1 57.500 (57.298)\n",
      "EPOCH: 13 train Results: Prec@1 57.298 Loss: 1.2011\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1078 (1.1078)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.008 (0.003)\tLoss 1.4721 (1.2647)\tPrec@1 25.000 (54.510)\n",
      "EPOCH: 13 val Results: Prec@1 54.510 Loss: 1.2647\n",
      "Best Prec@1: 54.510\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/390]\tTime 0.039 (0.039)\tLoss 1.0893 (1.0893)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [14][78/390]\tTime 0.004 (0.005)\tLoss 1.1517 (1.1479)\tPrec@1 55.469 (59.246)\n",
      "Epoch: [14][156/390]\tTime 0.012 (0.007)\tLoss 1.2876 (1.1618)\tPrec@1 53.125 (58.489)\n",
      "Epoch: [14][234/390]\tTime 0.007 (0.008)\tLoss 1.2420 (1.1779)\tPrec@1 57.031 (58.042)\n",
      "Epoch: [14][312/390]\tTime 0.005 (0.007)\tLoss 1.0632 (1.1844)\tPrec@1 57.812 (57.852)\n",
      "Epoch: [14][390/390]\tTime 0.009 (0.006)\tLoss 1.3009 (1.1905)\tPrec@1 51.250 (57.620)\n",
      "EPOCH: 14 train Results: Prec@1 57.620 Loss: 1.1905\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1148 (1.1148)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4374 (1.2633)\tPrec@1 37.500 (54.690)\n",
      "EPOCH: 14 val Results: Prec@1 54.690 Loss: 1.2633\n",
      "Best Prec@1: 54.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/390]\tTime 0.004 (0.004)\tLoss 1.0361 (1.0361)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [15][78/390]\tTime 0.002 (0.003)\tLoss 1.2745 (1.1277)\tPrec@1 53.125 (59.939)\n",
      "Epoch: [15][156/390]\tTime 0.002 (0.004)\tLoss 0.9859 (1.1443)\tPrec@1 67.188 (59.494)\n",
      "Epoch: [15][234/390]\tTime 0.003 (0.004)\tLoss 1.1105 (1.1587)\tPrec@1 63.281 (58.780)\n",
      "Epoch: [15][312/390]\tTime 0.002 (0.005)\tLoss 1.2683 (1.1706)\tPrec@1 57.031 (58.451)\n",
      "Epoch: [15][390/390]\tTime 0.002 (0.004)\tLoss 1.1553 (1.1773)\tPrec@1 47.500 (58.204)\n",
      "EPOCH: 15 train Results: Prec@1 58.204 Loss: 1.1773\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1225 (1.1225)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4208 (1.2652)\tPrec@1 43.750 (54.590)\n",
      "EPOCH: 15 val Results: Prec@1 54.590 Loss: 1.2652\n",
      "Best Prec@1: 54.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/390]\tTime 0.006 (0.006)\tLoss 1.0229 (1.0229)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.011)\tLoss 1.0704 (1.1061)\tPrec@1 65.625 (60.750)\n",
      "Epoch: [16][156/390]\tTime 0.002 (0.008)\tLoss 1.1696 (1.1312)\tPrec@1 55.469 (59.753)\n",
      "Epoch: [16][234/390]\tTime 0.002 (0.007)\tLoss 1.2645 (1.1511)\tPrec@1 53.906 (58.976)\n",
      "Epoch: [16][312/390]\tTime 0.003 (0.006)\tLoss 1.2219 (1.1660)\tPrec@1 52.344 (58.377)\n",
      "Epoch: [16][390/390]\tTime 0.002 (0.006)\tLoss 1.4107 (1.1748)\tPrec@1 50.000 (58.036)\n",
      "EPOCH: 16 train Results: Prec@1 58.036 Loss: 1.1748\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0294 (1.0294)\tPrec@1 70.312 (70.312)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2924 (1.2579)\tPrec@1 37.500 (54.960)\n",
      "EPOCH: 16 val Results: Prec@1 54.960 Loss: 1.2579\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/390]\tTime 0.002 (0.002)\tLoss 1.1394 (1.1394)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [17][78/390]\tTime 0.003 (0.003)\tLoss 1.2286 (1.1129)\tPrec@1 57.031 (60.789)\n",
      "Epoch: [17][156/390]\tTime 0.004 (0.003)\tLoss 1.2811 (1.1306)\tPrec@1 57.812 (60.062)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.003)\tLoss 1.1470 (1.1487)\tPrec@1 58.594 (59.292)\n",
      "Epoch: [17][312/390]\tTime 0.008 (0.003)\tLoss 1.2060 (1.1553)\tPrec@1 56.250 (58.891)\n",
      "Epoch: [17][390/390]\tTime 0.011 (0.004)\tLoss 1.1106 (1.1647)\tPrec@1 63.750 (58.556)\n",
      "EPOCH: 17 train Results: Prec@1 58.556 Loss: 1.1647\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1790 (1.1790)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4220 (1.2552)\tPrec@1 25.000 (54.870)\n",
      "EPOCH: 17 val Results: Prec@1 54.870 Loss: 1.2552\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/390]\tTime 0.003 (0.003)\tLoss 0.9817 (0.9817)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [18][78/390]\tTime 0.004 (0.004)\tLoss 1.1556 (1.1110)\tPrec@1 58.594 (60.542)\n",
      "Epoch: [18][156/390]\tTime 0.002 (0.004)\tLoss 1.0814 (1.1243)\tPrec@1 63.281 (60.171)\n",
      "Epoch: [18][234/390]\tTime 0.009 (0.004)\tLoss 1.0179 (1.1460)\tPrec@1 64.844 (59.235)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.004)\tLoss 1.1984 (1.1534)\tPrec@1 59.375 (58.886)\n",
      "Epoch: [18][390/390]\tTime 0.001 (0.004)\tLoss 1.2003 (1.1603)\tPrec@1 55.000 (58.688)\n",
      "EPOCH: 18 train Results: Prec@1 58.688 Loss: 1.1603\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1063 (1.1063)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1349 (1.2546)\tPrec@1 56.250 (54.920)\n",
      "EPOCH: 18 val Results: Prec@1 54.920 Loss: 1.2546\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/390]\tTime 0.005 (0.005)\tLoss 1.1313 (1.1313)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [19][78/390]\tTime 0.003 (0.004)\tLoss 1.2312 (1.0887)\tPrec@1 56.250 (61.333)\n",
      "Epoch: [19][156/390]\tTime 0.003 (0.004)\tLoss 1.0781 (1.1179)\tPrec@1 63.281 (60.395)\n",
      "Epoch: [19][234/390]\tTime 0.002 (0.004)\tLoss 1.1585 (1.1304)\tPrec@1 54.688 (59.927)\n",
      "Epoch: [19][312/390]\tTime 0.003 (0.004)\tLoss 1.0056 (1.1429)\tPrec@1 64.844 (59.542)\n",
      "Epoch: [19][390/390]\tTime 0.006 (0.004)\tLoss 1.4464 (1.1508)\tPrec@1 47.500 (59.166)\n",
      "EPOCH: 19 train Results: Prec@1 59.166 Loss: 1.1508\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1915 (1.1915)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.003 (0.002)\tLoss 1.6055 (1.2502)\tPrec@1 37.500 (55.210)\n",
      "EPOCH: 19 val Results: Prec@1 55.210 Loss: 1.2502\n",
      "Best Prec@1: 55.210\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/390]\tTime 0.009 (0.009)\tLoss 1.0732 (1.0732)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [20][78/390]\tTime 0.002 (0.003)\tLoss 1.0319 (1.0932)\tPrec@1 60.938 (61.076)\n",
      "Epoch: [20][156/390]\tTime 0.002 (0.003)\tLoss 1.0047 (1.1082)\tPrec@1 65.625 (60.644)\n",
      "Epoch: [20][234/390]\tTime 0.003 (0.003)\tLoss 1.0367 (1.1296)\tPrec@1 64.062 (59.797)\n",
      "Epoch: [20][312/390]\tTime 0.002 (0.003)\tLoss 1.1510 (1.1378)\tPrec@1 60.938 (59.567)\n",
      "Epoch: [20][390/390]\tTime 0.001 (0.003)\tLoss 1.3127 (1.1448)\tPrec@1 50.000 (59.306)\n",
      "EPOCH: 20 train Results: Prec@1 59.306 Loss: 1.1448\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1366 (1.1366)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2634 (1.2421)\tPrec@1 43.750 (55.820)\n",
      "EPOCH: 20 val Results: Prec@1 55.820 Loss: 1.2421\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/390]\tTime 0.002 (0.002)\tLoss 1.2056 (1.2056)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [21][78/390]\tTime 0.002 (0.004)\tLoss 1.2114 (1.0702)\tPrec@1 52.344 (62.223)\n",
      "Epoch: [21][156/390]\tTime 0.002 (0.008)\tLoss 1.1794 (1.1005)\tPrec@1 52.344 (60.928)\n",
      "Epoch: [21][234/390]\tTime 0.002 (0.007)\tLoss 1.1575 (1.1193)\tPrec@1 57.812 (60.166)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.006)\tLoss 1.1194 (1.1337)\tPrec@1 64.062 (59.732)\n",
      "Epoch: [21][390/390]\tTime 0.005 (0.006)\tLoss 1.1345 (1.1430)\tPrec@1 58.750 (59.432)\n",
      "EPOCH: 21 train Results: Prec@1 59.432 Loss: 1.1430\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1490 (1.1490)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.005)\tLoss 1.1926 (1.2412)\tPrec@1 43.750 (55.430)\n",
      "EPOCH: 21 val Results: Prec@1 55.430 Loss: 1.2412\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/390]\tTime 0.008 (0.008)\tLoss 1.1429 (1.1429)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [22][78/390]\tTime 0.004 (0.003)\tLoss 1.0447 (1.0988)\tPrec@1 65.625 (61.234)\n",
      "Epoch: [22][156/390]\tTime 0.003 (0.003)\tLoss 1.1255 (1.1106)\tPrec@1 60.938 (60.584)\n",
      "Epoch: [22][234/390]\tTime 0.002 (0.004)\tLoss 1.1368 (1.1206)\tPrec@1 62.500 (60.170)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.004)\tLoss 1.1809 (1.1331)\tPrec@1 57.812 (59.667)\n",
      "Epoch: [22][390/390]\tTime 0.002 (0.004)\tLoss 1.1565 (1.1427)\tPrec@1 65.000 (59.338)\n",
      "EPOCH: 22 train Results: Prec@1 59.338 Loss: 1.1427\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1340 (1.1340)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0669 (1.2483)\tPrec@1 62.500 (55.290)\n",
      "EPOCH: 22 val Results: Prec@1 55.290 Loss: 1.2483\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/390]\tTime 0.002 (0.002)\tLoss 1.0761 (1.0761)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [23][78/390]\tTime 0.002 (0.003)\tLoss 1.0389 (1.0739)\tPrec@1 64.062 (61.897)\n",
      "Epoch: [23][156/390]\tTime 0.002 (0.003)\tLoss 1.2716 (1.0976)\tPrec@1 52.344 (61.286)\n",
      "Epoch: [23][234/390]\tTime 0.004 (0.003)\tLoss 1.1561 (1.1066)\tPrec@1 59.375 (60.954)\n",
      "Epoch: [23][312/390]\tTime 0.004 (0.003)\tLoss 1.2206 (1.1222)\tPrec@1 52.344 (60.304)\n",
      "Epoch: [23][390/390]\tTime 0.001 (0.003)\tLoss 1.3163 (1.1350)\tPrec@1 52.500 (59.750)\n",
      "EPOCH: 23 train Results: Prec@1 59.750 Loss: 1.1350\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1983 (1.1983)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2615 (1.2485)\tPrec@1 37.500 (55.490)\n",
      "EPOCH: 23 val Results: Prec@1 55.490 Loss: 1.2485\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/390]\tTime 0.002 (0.002)\tLoss 1.1168 (1.1168)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.2432 (1.0667)\tPrec@1 60.938 (62.381)\n",
      "Epoch: [24][156/390]\tTime 0.002 (0.003)\tLoss 1.2218 (1.0929)\tPrec@1 54.688 (60.997)\n",
      "Epoch: [24][234/390]\tTime 0.006 (0.003)\tLoss 1.2551 (1.1149)\tPrec@1 54.688 (60.273)\n",
      "Epoch: [24][312/390]\tTime 0.004 (0.003)\tLoss 1.2940 (1.1250)\tPrec@1 53.906 (59.989)\n",
      "Epoch: [24][390/390]\tTime 0.005 (0.003)\tLoss 1.3143 (1.1309)\tPrec@1 57.500 (59.654)\n",
      "EPOCH: 24 train Results: Prec@1 59.654 Loss: 1.1309\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1614 (1.1614)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1625 (1.2555)\tPrec@1 43.750 (55.440)\n",
      "EPOCH: 24 val Results: Prec@1 55.440 Loss: 1.2555\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/390]\tTime 0.003 (0.003)\tLoss 1.0410 (1.0410)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [25][78/390]\tTime 0.004 (0.003)\tLoss 1.0798 (1.0687)\tPrec@1 61.719 (62.104)\n",
      "Epoch: [25][156/390]\tTime 0.003 (0.003)\tLoss 1.2158 (1.0910)\tPrec@1 57.812 (61.346)\n",
      "Epoch: [25][234/390]\tTime 0.002 (0.004)\tLoss 1.1880 (1.1078)\tPrec@1 57.812 (60.701)\n",
      "Epoch: [25][312/390]\tTime 0.003 (0.004)\tLoss 1.2508 (1.1194)\tPrec@1 57.031 (60.296)\n",
      "Epoch: [25][390/390]\tTime 0.005 (0.005)\tLoss 1.2591 (1.1245)\tPrec@1 51.250 (60.066)\n",
      "EPOCH: 25 train Results: Prec@1 60.066 Loss: 1.1245\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1582 (1.1582)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1056 (1.2498)\tPrec@1 56.250 (55.190)\n",
      "EPOCH: 25 val Results: Prec@1 55.190 Loss: 1.2498\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/390]\tTime 0.004 (0.004)\tLoss 1.0919 (1.0919)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [26][78/390]\tTime 0.007 (0.004)\tLoss 1.1185 (1.0567)\tPrec@1 63.281 (62.905)\n",
      "Epoch: [26][156/390]\tTime 0.002 (0.004)\tLoss 1.1518 (1.0837)\tPrec@1 59.375 (61.858)\n",
      "Epoch: [26][234/390]\tTime 0.002 (0.005)\tLoss 1.0749 (1.1009)\tPrec@1 60.938 (61.084)\n",
      "Epoch: [26][312/390]\tTime 0.041 (0.006)\tLoss 1.0487 (1.1133)\tPrec@1 63.281 (60.518)\n",
      "Epoch: [26][390/390]\tTime 0.002 (0.006)\tLoss 1.2085 (1.1202)\tPrec@1 56.250 (60.256)\n",
      "EPOCH: 26 train Results: Prec@1 60.256 Loss: 1.1202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1329 (1.1329)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3095 (1.2434)\tPrec@1 43.750 (55.870)\n",
      "EPOCH: 26 val Results: Prec@1 55.870 Loss: 1.2434\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/390]\tTime 0.003 (0.003)\tLoss 1.0716 (1.0716)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [27][78/390]\tTime 0.002 (0.009)\tLoss 0.9578 (1.0553)\tPrec@1 62.500 (62.747)\n",
      "Epoch: [27][156/390]\tTime 0.004 (0.006)\tLoss 0.9521 (1.0790)\tPrec@1 64.844 (61.699)\n",
      "Epoch: [27][234/390]\tTime 0.002 (0.005)\tLoss 0.9027 (1.0924)\tPrec@1 71.875 (61.184)\n",
      "Epoch: [27][312/390]\tTime 0.004 (0.005)\tLoss 1.1286 (1.1067)\tPrec@1 62.500 (60.698)\n",
      "Epoch: [27][390/390]\tTime 0.001 (0.004)\tLoss 1.4782 (1.1192)\tPrec@1 52.500 (60.334)\n",
      "EPOCH: 27 train Results: Prec@1 60.334 Loss: 1.1192\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1028 (1.1028)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1252 (1.2488)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 27 val Results: Prec@1 55.150 Loss: 1.2488\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/390]\tTime 0.002 (0.002)\tLoss 1.0103 (1.0103)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [28][78/390]\tTime 0.006 (0.003)\tLoss 1.0020 (1.0605)\tPrec@1 65.625 (62.727)\n",
      "Epoch: [28][156/390]\tTime 0.007 (0.004)\tLoss 1.1067 (1.0780)\tPrec@1 64.062 (61.958)\n",
      "Epoch: [28][234/390]\tTime 0.003 (0.004)\tLoss 1.1031 (1.0940)\tPrec@1 57.031 (61.316)\n",
      "Epoch: [28][312/390]\tTime 0.004 (0.004)\tLoss 1.2498 (1.1068)\tPrec@1 54.688 (60.788)\n",
      "Epoch: [28][390/390]\tTime 0.004 (0.004)\tLoss 1.3011 (1.1152)\tPrec@1 43.750 (60.450)\n",
      "EPOCH: 28 train Results: Prec@1 60.450 Loss: 1.1152\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1445 (1.1445)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1534 (1.2398)\tPrec@1 50.000 (55.800)\n",
      "EPOCH: 28 val Results: Prec@1 55.800 Loss: 1.2398\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/390]\tTime 0.004 (0.004)\tLoss 1.0195 (1.0195)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [29][78/390]\tTime 0.002 (0.003)\tLoss 1.2215 (1.0559)\tPrec@1 57.031 (62.747)\n",
      "Epoch: [29][156/390]\tTime 0.002 (0.006)\tLoss 1.0964 (1.0778)\tPrec@1 61.719 (61.709)\n",
      "Epoch: [29][234/390]\tTime 0.004 (0.006)\tLoss 0.9127 (1.0925)\tPrec@1 72.656 (61.094)\n",
      "Epoch: [29][312/390]\tTime 0.003 (0.005)\tLoss 1.0782 (1.1070)\tPrec@1 57.812 (60.655)\n",
      "Epoch: [29][390/390]\tTime 0.005 (0.005)\tLoss 1.3130 (1.1164)\tPrec@1 50.000 (60.320)\n",
      "EPOCH: 29 train Results: Prec@1 60.320 Loss: 1.1164\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0754 (1.0754)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9349 (1.2405)\tPrec@1 62.500 (55.870)\n",
      "EPOCH: 29 val Results: Prec@1 55.870 Loss: 1.2405\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/390]\tTime 0.003 (0.003)\tLoss 1.0915 (1.0915)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.004)\tLoss 1.0368 (1.0532)\tPrec@1 61.719 (62.352)\n",
      "Epoch: [30][156/390]\tTime 0.004 (0.004)\tLoss 1.0874 (1.0745)\tPrec@1 64.844 (61.828)\n",
      "Epoch: [30][234/390]\tTime 0.003 (0.004)\tLoss 1.0740 (1.0891)\tPrec@1 62.500 (61.293)\n",
      "Epoch: [30][312/390]\tTime 0.004 (0.004)\tLoss 1.1225 (1.1003)\tPrec@1 56.250 (60.763)\n",
      "Epoch: [30][390/390]\tTime 0.002 (0.005)\tLoss 1.0556 (1.1112)\tPrec@1 62.500 (60.394)\n",
      "EPOCH: 30 train Results: Prec@1 60.394 Loss: 1.1112\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0682 (1.0682)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0768 (1.2401)\tPrec@1 43.750 (55.880)\n",
      "EPOCH: 30 val Results: Prec@1 55.880 Loss: 1.2401\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/390]\tTime 0.004 (0.004)\tLoss 1.0081 (1.0081)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [31][78/390]\tTime 0.002 (0.005)\tLoss 0.9807 (1.0456)\tPrec@1 64.062 (63.093)\n",
      "Epoch: [31][156/390]\tTime 0.004 (0.005)\tLoss 1.1179 (1.0709)\tPrec@1 60.156 (61.878)\n",
      "Epoch: [31][234/390]\tTime 0.002 (0.005)\tLoss 1.1012 (1.0883)\tPrec@1 63.281 (61.230)\n",
      "Epoch: [31][312/390]\tTime 0.003 (0.005)\tLoss 1.1029 (1.0990)\tPrec@1 62.500 (60.798)\n",
      "Epoch: [31][390/390]\tTime 0.013 (0.006)\tLoss 1.1829 (1.1097)\tPrec@1 53.750 (60.354)\n",
      "EPOCH: 31 train Results: Prec@1 60.354 Loss: 1.1097\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1585 (1.1585)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2368 (1.2523)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 31 val Results: Prec@1 55.370 Loss: 1.2523\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/390]\tTime 0.002 (0.002)\tLoss 1.0223 (1.0223)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [32][78/390]\tTime 0.002 (0.004)\tLoss 1.0849 (1.0482)\tPrec@1 60.156 (63.222)\n",
      "Epoch: [32][156/390]\tTime 0.004 (0.004)\tLoss 0.9339 (1.0689)\tPrec@1 64.062 (62.097)\n",
      "Epoch: [32][234/390]\tTime 0.002 (0.004)\tLoss 1.1358 (1.0869)\tPrec@1 60.156 (61.443)\n",
      "Epoch: [32][312/390]\tTime 0.003 (0.004)\tLoss 1.0108 (1.0986)\tPrec@1 63.281 (60.895)\n",
      "Epoch: [32][390/390]\tTime 0.001 (0.004)\tLoss 1.0163 (1.1098)\tPrec@1 63.750 (60.520)\n",
      "EPOCH: 32 train Results: Prec@1 60.520 Loss: 1.1098\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1212 (1.1212)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2386 (1.2425)\tPrec@1 43.750 (55.810)\n",
      "EPOCH: 32 val Results: Prec@1 55.810 Loss: 1.2425\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/390]\tTime 0.003 (0.003)\tLoss 1.1745 (1.1745)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [33][78/390]\tTime 0.003 (0.003)\tLoss 1.1843 (1.0674)\tPrec@1 57.031 (61.828)\n",
      "Epoch: [33][156/390]\tTime 0.002 (0.003)\tLoss 1.2053 (1.0794)\tPrec@1 61.719 (61.291)\n",
      "Epoch: [33][234/390]\tTime 0.003 (0.003)\tLoss 1.2638 (1.0852)\tPrec@1 55.469 (60.981)\n",
      "Epoch: [33][312/390]\tTime 0.002 (0.003)\tLoss 1.1696 (1.0985)\tPrec@1 57.812 (60.506)\n",
      "Epoch: [33][390/390]\tTime 0.002 (0.003)\tLoss 1.0993 (1.1076)\tPrec@1 67.500 (60.340)\n",
      "EPOCH: 33 train Results: Prec@1 60.340 Loss: 1.1076\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0798 (1.0798)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3097 (1.2408)\tPrec@1 37.500 (55.510)\n",
      "EPOCH: 33 val Results: Prec@1 55.510 Loss: 1.2408\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/390]\tTime 0.002 (0.002)\tLoss 0.9312 (0.9312)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [34][78/390]\tTime 0.002 (0.004)\tLoss 1.1217 (1.0341)\tPrec@1 57.812 (63.627)\n",
      "Epoch: [34][156/390]\tTime 0.002 (0.003)\tLoss 1.0754 (1.0626)\tPrec@1 61.719 (62.371)\n",
      "Epoch: [34][234/390]\tTime 0.002 (0.003)\tLoss 1.0885 (1.0775)\tPrec@1 58.594 (61.825)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.0903 (1.0921)\tPrec@1 66.406 (61.212)\n",
      "Epoch: [34][390/390]\tTime 0.001 (0.003)\tLoss 1.0063 (1.1043)\tPrec@1 61.250 (60.778)\n",
      "EPOCH: 34 train Results: Prec@1 60.778 Loss: 1.1043\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1051 (1.1051)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2968 (1.2378)\tPrec@1 37.500 (56.120)\n",
      "EPOCH: 34 val Results: Prec@1 56.120 Loss: 1.2378\n",
      "Best Prec@1: 56.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/390]\tTime 0.003 (0.003)\tLoss 1.0633 (1.0633)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [35][78/390]\tTime 0.002 (0.003)\tLoss 1.2407 (1.0375)\tPrec@1 51.562 (62.905)\n",
      "Epoch: [35][156/390]\tTime 0.003 (0.003)\tLoss 1.2310 (1.0547)\tPrec@1 57.812 (62.261)\n",
      "Epoch: [35][234/390]\tTime 0.009 (0.003)\tLoss 1.1838 (1.0792)\tPrec@1 57.812 (61.443)\n",
      "Epoch: [35][312/390]\tTime 0.005 (0.003)\tLoss 1.1745 (1.0906)\tPrec@1 57.812 (61.007)\n",
      "Epoch: [35][390/390]\tTime 0.003 (0.003)\tLoss 1.0454 (1.0995)\tPrec@1 65.000 (60.804)\n",
      "EPOCH: 35 train Results: Prec@1 60.804 Loss: 1.0995\n",
      "Test: [0/78]\tTime 0.035 (0.035)\tLoss 1.1081 (1.1081)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2095 (1.2361)\tPrec@1 56.250 (55.930)\n",
      "EPOCH: 35 val Results: Prec@1 55.930 Loss: 1.2361\n",
      "Best Prec@1: 56.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/390]\tTime 0.005 (0.005)\tLoss 1.1356 (1.1356)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [36][78/390]\tTime 0.002 (0.005)\tLoss 1.0478 (1.0352)\tPrec@1 64.844 (63.143)\n",
      "Epoch: [36][156/390]\tTime 0.003 (0.004)\tLoss 1.1877 (1.0575)\tPrec@1 57.031 (62.515)\n",
      "Epoch: [36][234/390]\tTime 0.002 (0.004)\tLoss 1.0784 (1.0703)\tPrec@1 60.938 (61.772)\n",
      "Epoch: [36][312/390]\tTime 0.007 (0.004)\tLoss 1.1817 (1.0824)\tPrec@1 59.375 (61.367)\n",
      "Epoch: [36][390/390]\tTime 0.001 (0.003)\tLoss 1.2017 (1.0933)\tPrec@1 56.250 (60.980)\n",
      "EPOCH: 36 train Results: Prec@1 60.980 Loss: 1.0933\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0659 (1.0659)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1774 (1.2335)\tPrec@1 37.500 (56.570)\n",
      "EPOCH: 36 val Results: Prec@1 56.570 Loss: 1.2335\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/390]\tTime 0.002 (0.002)\tLoss 0.9658 (0.9658)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 0.9547 (1.0238)\tPrec@1 66.406 (63.528)\n",
      "Epoch: [37][156/390]\tTime 0.004 (0.003)\tLoss 0.9859 (1.0574)\tPrec@1 57.812 (62.435)\n",
      "Epoch: [37][234/390]\tTime 0.014 (0.004)\tLoss 1.3719 (1.0739)\tPrec@1 48.438 (61.765)\n",
      "Epoch: [37][312/390]\tTime 0.013 (0.006)\tLoss 0.9941 (1.0918)\tPrec@1 64.062 (61.122)\n",
      "Epoch: [37][390/390]\tTime 0.001 (0.005)\tLoss 1.0434 (1.0977)\tPrec@1 61.250 (60.952)\n",
      "EPOCH: 37 train Results: Prec@1 60.952 Loss: 1.0977\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1059 (1.1059)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1030 (1.2337)\tPrec@1 56.250 (55.810)\n",
      "EPOCH: 37 val Results: Prec@1 55.810 Loss: 1.2337\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/390]\tTime 0.003 (0.003)\tLoss 1.0013 (1.0013)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.003)\tLoss 1.2116 (1.0476)\tPrec@1 58.594 (62.648)\n",
      "Epoch: [38][156/390]\tTime 0.006 (0.005)\tLoss 0.9461 (1.0573)\tPrec@1 66.406 (62.619)\n",
      "Epoch: [38][234/390]\tTime 0.003 (0.005)\tLoss 1.0352 (1.0743)\tPrec@1 65.625 (61.932)\n",
      "Epoch: [38][312/390]\tTime 0.002 (0.005)\tLoss 1.2506 (1.0854)\tPrec@1 55.469 (61.539)\n",
      "Epoch: [38][390/390]\tTime 0.006 (0.005)\tLoss 1.1811 (1.0945)\tPrec@1 61.250 (61.218)\n",
      "EPOCH: 38 train Results: Prec@1 61.218 Loss: 1.0945\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0928 (1.0928)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0500 (1.2259)\tPrec@1 56.250 (56.260)\n",
      "EPOCH: 38 val Results: Prec@1 56.260 Loss: 1.2259\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/390]\tTime 0.008 (0.008)\tLoss 1.0577 (1.0577)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.004)\tLoss 1.2317 (1.0186)\tPrec@1 53.125 (63.845)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.004)\tLoss 1.0833 (1.0463)\tPrec@1 59.375 (62.883)\n",
      "Epoch: [39][234/390]\tTime 0.004 (0.004)\tLoss 1.0763 (1.0677)\tPrec@1 61.719 (62.161)\n",
      "Epoch: [39][312/390]\tTime 0.005 (0.003)\tLoss 1.4371 (1.0786)\tPrec@1 50.781 (61.791)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2381 (1.0928)\tPrec@1 52.500 (61.236)\n",
      "EPOCH: 39 train Results: Prec@1 61.236 Loss: 1.0928\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0667 (1.0667)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3399 (1.2383)\tPrec@1 50.000 (56.050)\n",
      "EPOCH: 39 val Results: Prec@1 56.050 Loss: 1.2383\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/390]\tTime 0.007 (0.007)\tLoss 1.0323 (1.0323)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [40][78/390]\tTime 0.011 (0.013)\tLoss 1.0073 (1.0323)\tPrec@1 64.062 (63.281)\n",
      "Epoch: [40][156/390]\tTime 0.015 (0.011)\tLoss 1.0334 (1.0403)\tPrec@1 60.938 (63.052)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.009)\tLoss 1.3641 (1.0554)\tPrec@1 47.656 (62.686)\n",
      "Epoch: [40][312/390]\tTime 0.003 (0.007)\tLoss 1.3373 (1.0719)\tPrec@1 53.906 (62.043)\n",
      "Epoch: [40][390/390]\tTime 0.002 (0.007)\tLoss 1.2058 (1.0879)\tPrec@1 55.000 (61.406)\n",
      "EPOCH: 40 train Results: Prec@1 61.406 Loss: 1.0879\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1085 (1.1085)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4187 (1.2358)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 40 val Results: Prec@1 55.610 Loss: 1.2358\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/390]\tTime 0.004 (0.004)\tLoss 1.1918 (1.1918)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [41][78/390]\tTime 0.002 (0.004)\tLoss 0.9655 (1.0191)\tPrec@1 62.500 (63.578)\n",
      "Epoch: [41][156/390]\tTime 0.005 (0.004)\tLoss 1.2946 (1.0488)\tPrec@1 53.906 (62.440)\n",
      "Epoch: [41][234/390]\tTime 0.003 (0.004)\tLoss 1.1530 (1.0651)\tPrec@1 59.375 (62.001)\n",
      "Epoch: [41][312/390]\tTime 0.003 (0.004)\tLoss 1.1300 (1.0800)\tPrec@1 62.500 (61.472)\n",
      "Epoch: [41][390/390]\tTime 0.002 (0.003)\tLoss 1.0270 (1.0883)\tPrec@1 67.500 (61.230)\n",
      "EPOCH: 41 train Results: Prec@1 61.230 Loss: 1.0883\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0386 (1.0386)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2643 (1.2309)\tPrec@1 50.000 (56.180)\n",
      "EPOCH: 41 val Results: Prec@1 56.180 Loss: 1.2309\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/390]\tTime 0.002 (0.002)\tLoss 1.0428 (1.0428)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [42][78/390]\tTime 0.002 (0.003)\tLoss 0.9533 (1.0209)\tPrec@1 60.938 (64.132)\n",
      "Epoch: [42][156/390]\tTime 0.003 (0.003)\tLoss 1.0046 (1.0367)\tPrec@1 62.500 (63.112)\n",
      "Epoch: [42][234/390]\tTime 0.005 (0.003)\tLoss 1.0839 (1.0601)\tPrec@1 60.156 (62.284)\n",
      "Epoch: [42][312/390]\tTime 0.003 (0.003)\tLoss 1.2025 (1.0756)\tPrec@1 53.125 (61.646)\n",
      "Epoch: [42][390/390]\tTime 0.003 (0.003)\tLoss 1.1462 (1.0884)\tPrec@1 62.500 (61.116)\n",
      "EPOCH: 42 train Results: Prec@1 61.116 Loss: 1.0884\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0363 (1.0363)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1616 (1.2306)\tPrec@1 50.000 (56.430)\n",
      "EPOCH: 42 val Results: Prec@1 56.430 Loss: 1.2306\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/390]\tTime 0.008 (0.008)\tLoss 1.0596 (1.0596)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [43][78/390]\tTime 0.002 (0.004)\tLoss 0.9660 (1.0152)\tPrec@1 66.406 (64.201)\n",
      "Epoch: [43][156/390]\tTime 0.002 (0.004)\tLoss 1.2549 (1.0487)\tPrec@1 55.469 (62.709)\n",
      "Epoch: [43][234/390]\tTime 0.002 (0.004)\tLoss 1.0750 (1.0593)\tPrec@1 60.156 (62.294)\n",
      "Epoch: [43][312/390]\tTime 0.005 (0.003)\tLoss 1.0265 (1.0779)\tPrec@1 69.531 (61.721)\n",
      "Epoch: [43][390/390]\tTime 0.007 (0.004)\tLoss 0.8992 (1.0855)\tPrec@1 67.500 (61.370)\n",
      "EPOCH: 43 train Results: Prec@1 61.370 Loss: 1.0855\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1254 (1.1254)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1500 (1.2354)\tPrec@1 56.250 (56.190)\n",
      "EPOCH: 43 val Results: Prec@1 56.190 Loss: 1.2354\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/390]\tTime 0.003 (0.003)\tLoss 1.0153 (1.0153)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [44][78/390]\tTime 0.005 (0.005)\tLoss 1.0246 (1.0219)\tPrec@1 64.844 (64.181)\n",
      "Epoch: [44][156/390]\tTime 0.002 (0.005)\tLoss 1.0080 (1.0385)\tPrec@1 67.188 (63.122)\n",
      "Epoch: [44][234/390]\tTime 0.003 (0.004)\tLoss 1.0068 (1.0586)\tPrec@1 63.281 (62.507)\n",
      "Epoch: [44][312/390]\tTime 0.003 (0.005)\tLoss 1.0897 (1.0716)\tPrec@1 63.281 (61.906)\n",
      "Epoch: [44][390/390]\tTime 0.002 (0.004)\tLoss 1.1486 (1.0830)\tPrec@1 58.750 (61.478)\n",
      "EPOCH: 44 train Results: Prec@1 61.478 Loss: 1.0830\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0574 (1.0574)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3908 (1.2333)\tPrec@1 43.750 (56.000)\n",
      "EPOCH: 44 val Results: Prec@1 56.000 Loss: 1.2333\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/390]\tTime 0.005 (0.005)\tLoss 1.1085 (1.1085)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [45][78/390]\tTime 0.012 (0.003)\tLoss 1.1525 (1.0191)\tPrec@1 57.812 (63.973)\n",
      "Epoch: [45][156/390]\tTime 0.002 (0.004)\tLoss 1.1550 (1.0364)\tPrec@1 57.031 (63.172)\n",
      "Epoch: [45][234/390]\tTime 0.007 (0.003)\tLoss 1.1217 (1.0544)\tPrec@1 53.906 (62.497)\n",
      "Epoch: [45][312/390]\tTime 0.002 (0.004)\tLoss 0.9422 (1.0699)\tPrec@1 64.844 (61.901)\n",
      "Epoch: [45][390/390]\tTime 0.003 (0.004)\tLoss 1.3568 (1.0803)\tPrec@1 46.250 (61.514)\n",
      "EPOCH: 45 train Results: Prec@1 61.514 Loss: 1.0803\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1004 (1.1004)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2943 (1.2446)\tPrec@1 50.000 (55.620)\n",
      "EPOCH: 45 val Results: Prec@1 55.620 Loss: 1.2446\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/390]\tTime 0.003 (0.003)\tLoss 1.0538 (1.0538)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [46][78/390]\tTime 0.002 (0.003)\tLoss 1.0264 (1.0125)\tPrec@1 64.062 (64.547)\n",
      "Epoch: [46][156/390]\tTime 0.002 (0.003)\tLoss 1.1429 (1.0294)\tPrec@1 57.031 (63.640)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.1672 (1.0524)\tPrec@1 57.031 (62.696)\n",
      "Epoch: [46][312/390]\tTime 0.008 (0.003)\tLoss 1.1250 (1.0636)\tPrec@1 60.156 (62.293)\n",
      "Epoch: [46][390/390]\tTime 0.001 (0.004)\tLoss 1.0907 (1.0779)\tPrec@1 60.000 (61.614)\n",
      "EPOCH: 46 train Results: Prec@1 61.614 Loss: 1.0779\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0525 (1.0525)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2440 (1.2547)\tPrec@1 37.500 (55.910)\n",
      "EPOCH: 46 val Results: Prec@1 55.910 Loss: 1.2547\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/390]\tTime 0.005 (0.005)\tLoss 1.3587 (1.3587)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [47][78/390]\tTime 0.006 (0.009)\tLoss 1.1388 (1.0075)\tPrec@1 56.250 (63.914)\n",
      "Epoch: [47][156/390]\tTime 0.004 (0.006)\tLoss 1.0526 (1.0319)\tPrec@1 70.312 (63.152)\n",
      "Epoch: [47][234/390]\tTime 0.006 (0.006)\tLoss 1.0682 (1.0534)\tPrec@1 57.031 (62.384)\n",
      "Epoch: [47][312/390]\tTime 0.002 (0.006)\tLoss 1.1603 (1.0641)\tPrec@1 55.469 (61.951)\n",
      "Epoch: [47][390/390]\tTime 0.004 (0.006)\tLoss 1.2064 (1.0796)\tPrec@1 61.250 (61.462)\n",
      "EPOCH: 47 train Results: Prec@1 61.462 Loss: 1.0796\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 0.9457 (0.9457)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1423 (1.2275)\tPrec@1 50.000 (56.560)\n",
      "EPOCH: 47 val Results: Prec@1 56.560 Loss: 1.2275\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/390]\tTime 0.004 (0.004)\tLoss 1.0413 (1.0413)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [48][78/390]\tTime 0.003 (0.004)\tLoss 1.0637 (1.0243)\tPrec@1 61.719 (63.548)\n",
      "Epoch: [48][156/390]\tTime 0.002 (0.003)\tLoss 1.1119 (1.0402)\tPrec@1 64.062 (62.938)\n",
      "Epoch: [48][234/390]\tTime 0.004 (0.003)\tLoss 0.9855 (1.0504)\tPrec@1 66.406 (62.387)\n",
      "Epoch: [48][312/390]\tTime 0.002 (0.003)\tLoss 1.0270 (1.0639)\tPrec@1 60.156 (61.938)\n",
      "Epoch: [48][390/390]\tTime 0.005 (0.005)\tLoss 1.3480 (1.0775)\tPrec@1 55.000 (61.432)\n",
      "EPOCH: 48 train Results: Prec@1 61.432 Loss: 1.0775\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1088 (1.1088)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2407 (1.2477)\tPrec@1 37.500 (55.310)\n",
      "EPOCH: 48 val Results: Prec@1 55.310 Loss: 1.2477\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/390]\tTime 0.006 (0.006)\tLoss 0.9801 (0.9801)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [49][78/390]\tTime 0.012 (0.005)\tLoss 1.1149 (1.0033)\tPrec@1 65.625 (65.081)\n",
      "Epoch: [49][156/390]\tTime 0.005 (0.006)\tLoss 1.1125 (1.0324)\tPrec@1 58.594 (63.699)\n",
      "Epoch: [49][234/390]\tTime 0.004 (0.005)\tLoss 1.0695 (1.0503)\tPrec@1 59.375 (62.743)\n",
      "Epoch: [49][312/390]\tTime 0.005 (0.005)\tLoss 1.1795 (1.0629)\tPrec@1 58.594 (62.250)\n",
      "Epoch: [49][390/390]\tTime 0.001 (0.005)\tLoss 1.0962 (1.0766)\tPrec@1 61.250 (61.776)\n",
      "EPOCH: 49 train Results: Prec@1 61.776 Loss: 1.0766\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1238 (1.1238)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.005)\tLoss 1.0022 (1.2289)\tPrec@1 50.000 (56.190)\n",
      "EPOCH: 49 val Results: Prec@1 56.190 Loss: 1.2289\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/390]\tTime 0.006 (0.006)\tLoss 0.9661 (0.9661)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [50][78/390]\tTime 0.004 (0.003)\tLoss 1.0167 (1.0110)\tPrec@1 62.500 (64.290)\n",
      "Epoch: [50][156/390]\tTime 0.003 (0.004)\tLoss 1.0452 (1.0311)\tPrec@1 57.031 (63.097)\n",
      "Epoch: [50][234/390]\tTime 0.002 (0.004)\tLoss 1.1962 (1.0550)\tPrec@1 54.688 (62.360)\n",
      "Epoch: [50][312/390]\tTime 0.003 (0.004)\tLoss 1.0505 (1.0665)\tPrec@1 59.375 (61.928)\n",
      "Epoch: [50][390/390]\tTime 0.002 (0.004)\tLoss 0.9876 (1.0771)\tPrec@1 65.000 (61.580)\n",
      "EPOCH: 50 train Results: Prec@1 61.580 Loss: 1.0771\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0859 (1.0859)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3232 (1.2388)\tPrec@1 31.250 (55.980)\n",
      "EPOCH: 50 val Results: Prec@1 55.980 Loss: 1.2388\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/390]\tTime 0.004 (0.004)\tLoss 1.1422 (1.1422)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [51][78/390]\tTime 0.002 (0.005)\tLoss 1.0377 (1.0164)\tPrec@1 66.406 (63.865)\n",
      "Epoch: [51][156/390]\tTime 0.002 (0.004)\tLoss 0.9774 (1.0377)\tPrec@1 67.969 (63.112)\n",
      "Epoch: [51][234/390]\tTime 0.002 (0.005)\tLoss 1.2104 (1.0526)\tPrec@1 55.469 (62.477)\n",
      "Epoch: [51][312/390]\tTime 0.002 (0.005)\tLoss 0.9558 (1.0638)\tPrec@1 67.188 (62.123)\n",
      "Epoch: [51][390/390]\tTime 0.022 (0.005)\tLoss 1.0942 (1.0755)\tPrec@1 61.250 (61.696)\n",
      "EPOCH: 51 train Results: Prec@1 61.696 Loss: 1.0755\n",
      "Test: [0/78]\tTime 0.014 (0.014)\tLoss 1.1376 (1.1376)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.1632 (1.2343)\tPrec@1 56.250 (57.080)\n",
      "EPOCH: 51 val Results: Prec@1 57.080 Loss: 1.2343\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/390]\tTime 0.003 (0.003)\tLoss 0.9675 (0.9675)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [52][78/390]\tTime 0.003 (0.004)\tLoss 1.0334 (1.0123)\tPrec@1 60.938 (64.260)\n",
      "Epoch: [52][156/390]\tTime 0.002 (0.004)\tLoss 1.0596 (1.0256)\tPrec@1 59.375 (63.659)\n",
      "Epoch: [52][234/390]\tTime 0.002 (0.005)\tLoss 0.9318 (1.0468)\tPrec@1 70.312 (62.689)\n",
      "Epoch: [52][312/390]\tTime 0.002 (0.005)\tLoss 1.2748 (1.0629)\tPrec@1 57.812 (62.098)\n",
      "Epoch: [52][390/390]\tTime 0.002 (0.005)\tLoss 1.3121 (1.0773)\tPrec@1 52.500 (61.684)\n",
      "EPOCH: 52 train Results: Prec@1 61.684 Loss: 1.0773\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.0846 (1.0846)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1441 (1.2409)\tPrec@1 50.000 (55.960)\n",
      "EPOCH: 52 val Results: Prec@1 55.960 Loss: 1.2409\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/390]\tTime 0.003 (0.003)\tLoss 1.0499 (1.0499)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [53][78/390]\tTime 0.010 (0.008)\tLoss 0.9750 (1.0093)\tPrec@1 63.281 (64.419)\n",
      "Epoch: [53][156/390]\tTime 0.002 (0.007)\tLoss 1.0038 (1.0230)\tPrec@1 63.281 (63.868)\n",
      "Epoch: [53][234/390]\tTime 0.002 (0.008)\tLoss 1.0473 (1.0411)\tPrec@1 57.812 (63.098)\n",
      "Epoch: [53][312/390]\tTime 0.005 (0.007)\tLoss 1.4865 (1.0589)\tPrec@1 43.750 (62.380)\n",
      "Epoch: [53][390/390]\tTime 0.006 (0.007)\tLoss 1.0523 (1.0695)\tPrec@1 70.000 (62.024)\n",
      "EPOCH: 53 train Results: Prec@1 62.024 Loss: 1.0695\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0960 (1.0960)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4281 (1.2446)\tPrec@1 37.500 (55.780)\n",
      "EPOCH: 53 val Results: Prec@1 55.780 Loss: 1.2446\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/390]\tTime 0.005 (0.005)\tLoss 0.9632 (0.9632)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [54][78/390]\tTime 0.002 (0.005)\tLoss 1.0023 (0.9966)\tPrec@1 61.719 (65.111)\n",
      "Epoch: [54][156/390]\tTime 0.005 (0.004)\tLoss 0.9930 (1.0211)\tPrec@1 64.844 (63.948)\n",
      "Epoch: [54][234/390]\tTime 0.005 (0.004)\tLoss 1.0908 (1.0434)\tPrec@1 60.938 (63.092)\n",
      "Epoch: [54][312/390]\tTime 0.004 (0.004)\tLoss 1.2466 (1.0603)\tPrec@1 58.594 (62.368)\n",
      "Epoch: [54][390/390]\tTime 0.001 (0.004)\tLoss 1.2099 (1.0752)\tPrec@1 53.750 (61.856)\n",
      "EPOCH: 54 train Results: Prec@1 61.856 Loss: 1.0752\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0638 (1.0638)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 0.9847 (1.2517)\tPrec@1 50.000 (55.380)\n",
      "EPOCH: 54 val Results: Prec@1 55.380 Loss: 1.2517\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/390]\tTime 0.014 (0.014)\tLoss 1.0683 (1.0683)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.004)\tLoss 1.0507 (0.9965)\tPrec@1 68.750 (64.933)\n",
      "Epoch: [55][156/390]\tTime 0.009 (0.004)\tLoss 1.1328 (1.0263)\tPrec@1 57.031 (63.436)\n",
      "Epoch: [55][234/390]\tTime 0.004 (0.004)\tLoss 1.2275 (1.0462)\tPrec@1 60.156 (62.653)\n",
      "Epoch: [55][312/390]\tTime 0.004 (0.004)\tLoss 0.9427 (1.0572)\tPrec@1 66.406 (62.200)\n",
      "Epoch: [55][390/390]\tTime 0.021 (0.005)\tLoss 1.1144 (1.0700)\tPrec@1 61.250 (61.828)\n",
      "EPOCH: 55 train Results: Prec@1 61.828 Loss: 1.0700\n",
      "Test: [0/78]\tTime 0.025 (0.025)\tLoss 1.0683 (1.0683)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.003)\tLoss 1.2787 (1.2460)\tPrec@1 43.750 (55.740)\n",
      "EPOCH: 55 val Results: Prec@1 55.740 Loss: 1.2460\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/390]\tTime 0.007 (0.007)\tLoss 0.9113 (0.9113)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [56][78/390]\tTime 0.002 (0.004)\tLoss 1.0051 (1.0181)\tPrec@1 63.281 (64.033)\n",
      "Epoch: [56][156/390]\tTime 0.004 (0.004)\tLoss 0.9790 (1.0301)\tPrec@1 66.406 (63.236)\n",
      "Epoch: [56][234/390]\tTime 0.002 (0.004)\tLoss 1.0947 (1.0430)\tPrec@1 59.375 (62.819)\n",
      "Epoch: [56][312/390]\tTime 0.002 (0.004)\tLoss 1.0068 (1.0574)\tPrec@1 64.062 (62.323)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.005)\tLoss 1.2188 (1.0703)\tPrec@1 58.750 (61.924)\n",
      "EPOCH: 56 train Results: Prec@1 61.924 Loss: 1.0703\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0349 (1.0349)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0674 (1.2510)\tPrec@1 56.250 (55.270)\n",
      "EPOCH: 56 val Results: Prec@1 55.270 Loss: 1.2510\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/390]\tTime 0.011 (0.011)\tLoss 1.0261 (1.0261)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [57][78/390]\tTime 0.002 (0.004)\tLoss 1.0285 (0.9987)\tPrec@1 64.844 (64.537)\n",
      "Epoch: [57][156/390]\tTime 0.002 (0.005)\tLoss 0.9721 (1.0147)\tPrec@1 67.188 (63.858)\n",
      "Epoch: [57][234/390]\tTime 0.002 (0.005)\tLoss 1.0475 (1.0347)\tPrec@1 60.156 (63.032)\n",
      "Epoch: [57][312/390]\tTime 0.002 (0.005)\tLoss 1.1036 (1.0514)\tPrec@1 60.156 (62.285)\n",
      "Epoch: [57][390/390]\tTime 0.005 (0.005)\tLoss 1.1143 (1.0674)\tPrec@1 58.750 (61.780)\n",
      "EPOCH: 57 train Results: Prec@1 61.780 Loss: 1.0674\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1661 (1.1661)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2095 (1.2447)\tPrec@1 37.500 (56.100)\n",
      "EPOCH: 57 val Results: Prec@1 56.100 Loss: 1.2447\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/390]\tTime 0.002 (0.002)\tLoss 0.9584 (0.9584)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [58][78/390]\tTime 0.003 (0.003)\tLoss 1.0581 (0.9995)\tPrec@1 67.188 (64.330)\n",
      "Epoch: [58][156/390]\tTime 0.003 (0.004)\tLoss 1.0664 (1.0252)\tPrec@1 57.812 (63.530)\n",
      "Epoch: [58][234/390]\tTime 0.009 (0.004)\tLoss 0.9978 (1.0422)\tPrec@1 60.938 (62.896)\n",
      "Epoch: [58][312/390]\tTime 0.003 (0.004)\tLoss 1.0153 (1.0561)\tPrec@1 64.062 (62.572)\n",
      "Epoch: [58][390/390]\tTime 0.002 (0.005)\tLoss 1.2186 (1.0687)\tPrec@1 57.500 (61.948)\n",
      "EPOCH: 58 train Results: Prec@1 61.948 Loss: 1.0687\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0766 (1.0766)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2192 (1.2240)\tPrec@1 56.250 (56.560)\n",
      "EPOCH: 58 val Results: Prec@1 56.560 Loss: 1.2240\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/390]\tTime 0.003 (0.003)\tLoss 1.0632 (1.0632)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [59][78/390]\tTime 0.002 (0.004)\tLoss 1.0865 (1.0017)\tPrec@1 60.938 (64.458)\n",
      "Epoch: [59][156/390]\tTime 0.003 (0.006)\tLoss 1.1633 (1.0186)\tPrec@1 61.719 (63.679)\n",
      "Epoch: [59][234/390]\tTime 0.002 (0.005)\tLoss 1.0424 (1.0427)\tPrec@1 62.500 (62.866)\n",
      "Epoch: [59][312/390]\tTime 0.004 (0.006)\tLoss 1.0632 (1.0563)\tPrec@1 63.281 (62.335)\n",
      "Epoch: [59][390/390]\tTime 0.001 (0.006)\tLoss 1.0799 (1.0675)\tPrec@1 66.250 (61.976)\n",
      "EPOCH: 59 train Results: Prec@1 61.976 Loss: 1.0675\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1553 (1.1553)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.004)\tLoss 0.9773 (1.2481)\tPrec@1 56.250 (55.620)\n",
      "EPOCH: 59 val Results: Prec@1 55.620 Loss: 1.2481\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/390]\tTime 0.009 (0.009)\tLoss 0.8647 (0.8647)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [60][78/390]\tTime 0.002 (0.003)\tLoss 1.0055 (1.0043)\tPrec@1 60.938 (64.775)\n",
      "Epoch: [60][156/390]\tTime 0.005 (0.003)\tLoss 0.9821 (1.0252)\tPrec@1 68.750 (64.053)\n",
      "Epoch: [60][234/390]\tTime 0.004 (0.003)\tLoss 1.0309 (1.0450)\tPrec@1 58.594 (62.979)\n",
      "Epoch: [60][312/390]\tTime 0.002 (0.003)\tLoss 1.0896 (1.0616)\tPrec@1 60.938 (62.213)\n",
      "Epoch: [60][390/390]\tTime 0.001 (0.003)\tLoss 1.1470 (1.0712)\tPrec@1 61.250 (61.962)\n",
      "EPOCH: 60 train Results: Prec@1 61.962 Loss: 1.0712\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1229 (1.1229)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9329 (1.2547)\tPrec@1 56.250 (55.160)\n",
      "EPOCH: 60 val Results: Prec@1 55.160 Loss: 1.2547\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/390]\tTime 0.003 (0.003)\tLoss 0.9310 (0.9310)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [61][78/390]\tTime 0.004 (0.003)\tLoss 1.0692 (0.9968)\tPrec@1 60.156 (64.597)\n",
      "Epoch: [61][156/390]\tTime 0.002 (0.003)\tLoss 1.1953 (1.0219)\tPrec@1 58.594 (63.749)\n",
      "Epoch: [61][234/390]\tTime 0.005 (0.004)\tLoss 1.1054 (1.0386)\tPrec@1 59.375 (62.919)\n",
      "Epoch: [61][312/390]\tTime 0.002 (0.005)\tLoss 1.1067 (1.0536)\tPrec@1 65.625 (62.410)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.005)\tLoss 1.0956 (1.0650)\tPrec@1 65.000 (62.032)\n",
      "EPOCH: 61 train Results: Prec@1 62.032 Loss: 1.0650\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0990 (1.0990)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2317 (1.2480)\tPrec@1 43.750 (55.400)\n",
      "EPOCH: 61 val Results: Prec@1 55.400 Loss: 1.2480\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/390]\tTime 0.010 (0.010)\tLoss 0.9615 (0.9615)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [62][78/390]\tTime 0.016 (0.005)\tLoss 1.1075 (1.0019)\tPrec@1 57.812 (64.320)\n",
      "Epoch: [62][156/390]\tTime 0.002 (0.007)\tLoss 0.9611 (1.0216)\tPrec@1 70.312 (63.470)\n",
      "Epoch: [62][234/390]\tTime 0.038 (0.007)\tLoss 1.1981 (1.0439)\tPrec@1 58.594 (62.726)\n",
      "Epoch: [62][312/390]\tTime 0.002 (0.007)\tLoss 1.1427 (1.0557)\tPrec@1 62.500 (62.278)\n",
      "Epoch: [62][390/390]\tTime 0.001 (0.007)\tLoss 0.8561 (1.0664)\tPrec@1 71.250 (61.920)\n",
      "EPOCH: 62 train Results: Prec@1 61.920 Loss: 1.0664\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1448 (1.1448)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.1346 (1.2385)\tPrec@1 37.500 (56.200)\n",
      "EPOCH: 62 val Results: Prec@1 56.200 Loss: 1.2385\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/390]\tTime 0.002 (0.002)\tLoss 1.0115 (1.0115)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [63][78/390]\tTime 0.003 (0.004)\tLoss 1.0446 (0.9993)\tPrec@1 61.719 (64.310)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.005)\tLoss 0.9818 (1.0238)\tPrec@1 65.625 (63.421)\n",
      "Epoch: [63][234/390]\tTime 0.002 (0.005)\tLoss 1.1920 (1.0435)\tPrec@1 59.375 (62.945)\n",
      "Epoch: [63][312/390]\tTime 0.004 (0.005)\tLoss 1.2560 (1.0597)\tPrec@1 53.906 (62.418)\n",
      "Epoch: [63][390/390]\tTime 0.001 (0.006)\tLoss 1.1675 (1.0670)\tPrec@1 60.000 (62.110)\n",
      "EPOCH: 63 train Results: Prec@1 62.110 Loss: 1.0670\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1781 (1.1781)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9917 (1.2371)\tPrec@1 43.750 (56.150)\n",
      "EPOCH: 63 val Results: Prec@1 56.150 Loss: 1.2371\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/390]\tTime 0.012 (0.012)\tLoss 0.8951 (0.8951)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [64][78/390]\tTime 0.002 (0.008)\tLoss 1.0898 (0.9904)\tPrec@1 61.719 (64.656)\n",
      "Epoch: [64][156/390]\tTime 0.002 (0.007)\tLoss 1.1889 (1.0202)\tPrec@1 55.469 (63.854)\n",
      "Epoch: [64][234/390]\tTime 0.002 (0.006)\tLoss 1.1720 (1.0371)\tPrec@1 57.812 (63.088)\n",
      "Epoch: [64][312/390]\tTime 0.002 (0.006)\tLoss 1.1169 (1.0526)\tPrec@1 59.375 (62.468)\n",
      "Epoch: [64][390/390]\tTime 0.002 (0.005)\tLoss 1.1646 (1.0668)\tPrec@1 61.250 (62.112)\n",
      "EPOCH: 64 train Results: Prec@1 62.112 Loss: 1.0668\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1184 (1.1184)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 0.8996 (1.2500)\tPrec@1 68.750 (55.540)\n",
      "EPOCH: 64 val Results: Prec@1 55.540 Loss: 1.2500\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/390]\tTime 0.009 (0.009)\tLoss 0.9510 (0.9510)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [65][78/390]\tTime 0.005 (0.004)\tLoss 1.1174 (0.9966)\tPrec@1 60.938 (64.488)\n",
      "Epoch: [65][156/390]\tTime 0.002 (0.004)\tLoss 0.9726 (1.0168)\tPrec@1 68.750 (63.943)\n",
      "Epoch: [65][234/390]\tTime 0.003 (0.004)\tLoss 1.0382 (1.0373)\tPrec@1 62.500 (63.112)\n",
      "Epoch: [65][312/390]\tTime 0.005 (0.003)\tLoss 1.1794 (1.0537)\tPrec@1 58.594 (62.567)\n",
      "Epoch: [65][390/390]\tTime 0.004 (0.004)\tLoss 1.0932 (1.0663)\tPrec@1 56.250 (62.114)\n",
      "EPOCH: 65 train Results: Prec@1 62.114 Loss: 1.0663\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1215 (1.1215)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1999 (1.2359)\tPrec@1 56.250 (55.890)\n",
      "EPOCH: 65 val Results: Prec@1 55.890 Loss: 1.2359\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/390]\tTime 0.005 (0.005)\tLoss 0.9631 (0.9631)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [66][78/390]\tTime 0.003 (0.005)\tLoss 1.1053 (0.9818)\tPrec@1 54.688 (64.992)\n",
      "Epoch: [66][156/390]\tTime 0.008 (0.004)\tLoss 1.1588 (1.0144)\tPrec@1 57.812 (63.505)\n",
      "Epoch: [66][234/390]\tTime 0.005 (0.004)\tLoss 1.1047 (1.0327)\tPrec@1 63.281 (63.075)\n",
      "Epoch: [66][312/390]\tTime 0.004 (0.004)\tLoss 1.1727 (1.0542)\tPrec@1 59.375 (62.373)\n",
      "Epoch: [66][390/390]\tTime 0.001 (0.004)\tLoss 1.1304 (1.0648)\tPrec@1 58.750 (62.138)\n",
      "EPOCH: 66 train Results: Prec@1 62.138 Loss: 1.0648\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0908 (1.0908)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1754 (1.2458)\tPrec@1 56.250 (55.910)\n",
      "EPOCH: 66 val Results: Prec@1 55.910 Loss: 1.2458\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/390]\tTime 0.005 (0.005)\tLoss 0.9933 (0.9933)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [67][78/390]\tTime 0.002 (0.003)\tLoss 0.9975 (0.9930)\tPrec@1 65.625 (64.765)\n",
      "Epoch: [67][156/390]\tTime 0.002 (0.003)\tLoss 1.2347 (1.0141)\tPrec@1 54.688 (64.087)\n",
      "Epoch: [67][234/390]\tTime 0.005 (0.003)\tLoss 1.0290 (1.0364)\tPrec@1 60.156 (63.285)\n",
      "Epoch: [67][312/390]\tTime 0.002 (0.003)\tLoss 1.0252 (1.0485)\tPrec@1 66.406 (62.800)\n",
      "Epoch: [67][390/390]\tTime 0.003 (0.003)\tLoss 1.1149 (1.0631)\tPrec@1 57.500 (62.190)\n",
      "EPOCH: 67 train Results: Prec@1 62.190 Loss: 1.0631\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0907 (1.0907)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8853 (1.2416)\tPrec@1 56.250 (55.520)\n",
      "EPOCH: 67 val Results: Prec@1 55.520 Loss: 1.2416\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/390]\tTime 0.004 (0.004)\tLoss 0.9180 (0.9180)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [68][78/390]\tTime 0.004 (0.003)\tLoss 0.9186 (0.9878)\tPrec@1 67.969 (65.002)\n",
      "Epoch: [68][156/390]\tTime 0.010 (0.003)\tLoss 1.1380 (1.0260)\tPrec@1 63.281 (63.555)\n",
      "Epoch: [68][234/390]\tTime 0.002 (0.003)\tLoss 1.0983 (1.0414)\tPrec@1 61.719 (62.932)\n",
      "Epoch: [68][312/390]\tTime 0.012 (0.003)\tLoss 1.2538 (1.0535)\tPrec@1 56.250 (62.617)\n",
      "Epoch: [68][390/390]\tTime 0.001 (0.003)\tLoss 1.3653 (1.0636)\tPrec@1 55.000 (62.248)\n",
      "EPOCH: 68 train Results: Prec@1 62.248 Loss: 1.0636\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0771 (1.0771)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0894 (1.2453)\tPrec@1 62.500 (55.830)\n",
      "EPOCH: 68 val Results: Prec@1 55.830 Loss: 1.2453\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/390]\tTime 0.002 (0.002)\tLoss 1.0494 (1.0494)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.003)\tLoss 0.9851 (0.9678)\tPrec@1 66.406 (65.645)\n",
      "Epoch: [69][156/390]\tTime 0.006 (0.003)\tLoss 1.0701 (1.0097)\tPrec@1 65.625 (64.137)\n",
      "Epoch: [69][234/390]\tTime 0.002 (0.003)\tLoss 0.9368 (1.0330)\tPrec@1 61.719 (63.191)\n",
      "Epoch: [69][312/390]\tTime 0.002 (0.003)\tLoss 1.0984 (1.0491)\tPrec@1 62.500 (62.413)\n",
      "Epoch: [69][390/390]\tTime 0.004 (0.003)\tLoss 1.0791 (1.0627)\tPrec@1 62.500 (61.974)\n",
      "EPOCH: 69 train Results: Prec@1 61.974 Loss: 1.0627\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0781 (1.0781)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1498 (1.2327)\tPrec@1 56.250 (56.460)\n",
      "EPOCH: 69 val Results: Prec@1 56.460 Loss: 1.2327\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/390]\tTime 0.003 (0.003)\tLoss 0.8643 (0.8643)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [70][78/390]\tTime 0.002 (0.003)\tLoss 1.1463 (0.9966)\tPrec@1 56.250 (64.597)\n",
      "Epoch: [70][156/390]\tTime 0.002 (0.003)\tLoss 1.0156 (1.0151)\tPrec@1 61.719 (63.893)\n",
      "Epoch: [70][234/390]\tTime 0.003 (0.004)\tLoss 0.9795 (1.0342)\tPrec@1 62.500 (63.195)\n",
      "Epoch: [70][312/390]\tTime 0.009 (0.003)\tLoss 0.9757 (1.0557)\tPrec@1 64.062 (62.562)\n",
      "Epoch: [70][390/390]\tTime 0.003 (0.003)\tLoss 1.0775 (1.0636)\tPrec@1 60.000 (62.214)\n",
      "EPOCH: 70 train Results: Prec@1 62.214 Loss: 1.0636\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0963 (1.0963)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2400 (1.2473)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 70 val Results: Prec@1 55.370 Loss: 1.2473\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/390]\tTime 0.005 (0.005)\tLoss 0.8830 (0.8830)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [71][78/390]\tTime 0.004 (0.003)\tLoss 1.0149 (0.9896)\tPrec@1 60.156 (65.190)\n",
      "Epoch: [71][156/390]\tTime 0.003 (0.003)\tLoss 1.0093 (1.0203)\tPrec@1 60.938 (63.814)\n",
      "Epoch: [71][234/390]\tTime 0.008 (0.003)\tLoss 0.9486 (1.0403)\tPrec@1 67.969 (62.972)\n",
      "Epoch: [71][312/390]\tTime 0.002 (0.003)\tLoss 0.9758 (1.0526)\tPrec@1 61.719 (62.443)\n",
      "Epoch: [71][390/390]\tTime 0.001 (0.003)\tLoss 0.8306 (1.0665)\tPrec@1 73.750 (61.888)\n",
      "EPOCH: 71 train Results: Prec@1 61.888 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.0804 (1.0804)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0180 (1.2509)\tPrec@1 62.500 (55.770)\n",
      "EPOCH: 71 val Results: Prec@1 55.770 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/390]\tTime 0.006 (0.006)\tLoss 0.9683 (0.9683)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [72][78/390]\tTime 0.002 (0.004)\tLoss 1.0971 (0.9957)\tPrec@1 57.812 (65.111)\n",
      "Epoch: [72][156/390]\tTime 0.004 (0.004)\tLoss 0.9829 (1.0094)\tPrec@1 67.188 (64.356)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.004)\tLoss 1.0428 (1.0326)\tPrec@1 60.156 (63.188)\n",
      "Epoch: [72][312/390]\tTime 0.005 (0.004)\tLoss 1.2079 (1.0484)\tPrec@1 60.156 (62.607)\n",
      "Epoch: [72][390/390]\tTime 0.004 (0.004)\tLoss 0.9681 (1.0605)\tPrec@1 66.250 (62.256)\n",
      "EPOCH: 72 train Results: Prec@1 62.256 Loss: 1.0605\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0527 (1.0527)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1151 (1.2390)\tPrec@1 50.000 (55.620)\n",
      "EPOCH: 72 val Results: Prec@1 55.620 Loss: 1.2390\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/390]\tTime 0.003 (0.003)\tLoss 0.9666 (0.9666)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [73][78/390]\tTime 0.003 (0.003)\tLoss 0.9904 (1.0043)\tPrec@1 60.938 (64.409)\n",
      "Epoch: [73][156/390]\tTime 0.062 (0.003)\tLoss 1.1971 (1.0127)\tPrec@1 57.031 (63.699)\n",
      "Epoch: [73][234/390]\tTime 0.011 (0.004)\tLoss 0.9964 (1.0312)\tPrec@1 64.062 (63.112)\n",
      "Epoch: [73][312/390]\tTime 0.002 (0.004)\tLoss 0.9593 (1.0495)\tPrec@1 66.406 (62.455)\n",
      "Epoch: [73][390/390]\tTime 0.001 (0.004)\tLoss 1.0205 (1.0598)\tPrec@1 58.750 (62.196)\n",
      "EPOCH: 73 train Results: Prec@1 62.196 Loss: 1.0598\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0460 (1.0460)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9960 (1.2463)\tPrec@1 62.500 (56.190)\n",
      "EPOCH: 73 val Results: Prec@1 56.190 Loss: 1.2463\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/390]\tTime 0.003 (0.003)\tLoss 1.0569 (1.0569)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [74][78/390]\tTime 0.002 (0.003)\tLoss 0.9938 (0.9875)\tPrec@1 64.062 (65.328)\n",
      "Epoch: [74][156/390]\tTime 0.002 (0.004)\tLoss 1.1053 (1.0177)\tPrec@1 64.844 (64.072)\n",
      "Epoch: [74][234/390]\tTime 0.004 (0.004)\tLoss 0.9630 (1.0327)\tPrec@1 69.531 (63.617)\n",
      "Epoch: [74][312/390]\tTime 0.018 (0.004)\tLoss 1.0407 (1.0462)\tPrec@1 66.406 (62.974)\n",
      "Epoch: [74][390/390]\tTime 0.004 (0.004)\tLoss 1.1614 (1.0583)\tPrec@1 61.250 (62.544)\n",
      "EPOCH: 74 train Results: Prec@1 62.544 Loss: 1.0583\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1146 (1.1146)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2241 (1.2450)\tPrec@1 37.500 (55.550)\n",
      "EPOCH: 74 val Results: Prec@1 55.550 Loss: 1.2450\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/390]\tTime 0.003 (0.003)\tLoss 0.9704 (0.9704)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [75][78/390]\tTime 0.003 (0.003)\tLoss 1.1996 (0.9696)\tPrec@1 60.156 (65.813)\n",
      "Epoch: [75][156/390]\tTime 0.015 (0.003)\tLoss 0.8171 (1.0142)\tPrec@1 71.875 (63.993)\n",
      "Epoch: [75][234/390]\tTime 0.005 (0.003)\tLoss 1.2150 (1.0327)\tPrec@1 57.812 (63.348)\n",
      "Epoch: [75][312/390]\tTime 0.002 (0.003)\tLoss 1.2596 (1.0476)\tPrec@1 55.469 (62.785)\n",
      "Epoch: [75][390/390]\tTime 0.002 (0.003)\tLoss 1.0817 (1.0617)\tPrec@1 65.000 (62.334)\n",
      "EPOCH: 75 train Results: Prec@1 62.334 Loss: 1.0617\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0885 (1.0885)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2118 (1.2487)\tPrec@1 50.000 (55.540)\n",
      "EPOCH: 75 val Results: Prec@1 55.540 Loss: 1.2487\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/390]\tTime 0.002 (0.002)\tLoss 1.0850 (1.0850)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.003)\tLoss 1.0743 (0.9924)\tPrec@1 64.844 (64.705)\n",
      "Epoch: [76][156/390]\tTime 0.002 (0.003)\tLoss 0.9073 (1.0187)\tPrec@1 66.406 (63.515)\n",
      "Epoch: [76][234/390]\tTime 0.003 (0.003)\tLoss 1.0721 (1.0377)\tPrec@1 64.844 (62.856)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 0.9264 (1.0502)\tPrec@1 66.406 (62.522)\n",
      "Epoch: [76][390/390]\tTime 0.002 (0.003)\tLoss 0.8953 (1.0582)\tPrec@1 71.250 (62.310)\n",
      "EPOCH: 76 train Results: Prec@1 62.310 Loss: 1.0582\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1226 (1.1226)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3837 (1.2391)\tPrec@1 43.750 (56.110)\n",
      "EPOCH: 76 val Results: Prec@1 56.110 Loss: 1.2391\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/390]\tTime 0.007 (0.007)\tLoss 1.0682 (1.0682)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [77][78/390]\tTime 0.002 (0.004)\tLoss 1.1511 (0.9934)\tPrec@1 62.500 (65.042)\n",
      "Epoch: [77][156/390]\tTime 0.003 (0.003)\tLoss 1.0579 (1.0165)\tPrec@1 58.594 (63.824)\n",
      "Epoch: [77][234/390]\tTime 0.003 (0.003)\tLoss 0.9579 (1.0358)\tPrec@1 67.188 (63.075)\n",
      "Epoch: [77][312/390]\tTime 0.010 (0.003)\tLoss 1.1555 (1.0471)\tPrec@1 56.250 (62.562)\n",
      "Epoch: [77][390/390]\tTime 0.003 (0.003)\tLoss 1.2501 (1.0592)\tPrec@1 58.750 (62.104)\n",
      "EPOCH: 77 train Results: Prec@1 62.104 Loss: 1.0592\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.0743 (1.0743)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3370 (1.2435)\tPrec@1 37.500 (56.360)\n",
      "EPOCH: 77 val Results: Prec@1 56.360 Loss: 1.2435\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/390]\tTime 0.002 (0.002)\tLoss 0.9345 (0.9345)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [78][78/390]\tTime 0.002 (0.003)\tLoss 1.0865 (0.9747)\tPrec@1 63.281 (65.951)\n",
      "Epoch: [78][156/390]\tTime 0.007 (0.003)\tLoss 1.0512 (1.0067)\tPrec@1 57.812 (64.291)\n",
      "Epoch: [78][234/390]\tTime 0.002 (0.004)\tLoss 1.0499 (1.0307)\tPrec@1 58.594 (63.414)\n",
      "Epoch: [78][312/390]\tTime 0.002 (0.003)\tLoss 1.0263 (1.0494)\tPrec@1 63.281 (62.725)\n",
      "Epoch: [78][390/390]\tTime 0.005 (0.003)\tLoss 0.9560 (1.0601)\tPrec@1 65.000 (62.236)\n",
      "EPOCH: 78 train Results: Prec@1 62.236 Loss: 1.0601\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0642 (1.0642)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1562 (1.2264)\tPrec@1 50.000 (56.410)\n",
      "EPOCH: 78 val Results: Prec@1 56.410 Loss: 1.2264\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/390]\tTime 0.004 (0.004)\tLoss 0.9350 (0.9350)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [79][78/390]\tTime 0.004 (0.003)\tLoss 0.9212 (0.9652)\tPrec@1 66.406 (65.536)\n",
      "Epoch: [79][156/390]\tTime 0.003 (0.003)\tLoss 1.0073 (0.9898)\tPrec@1 63.281 (64.779)\n",
      "Epoch: [79][234/390]\tTime 0.007 (0.004)\tLoss 1.0801 (1.0174)\tPrec@1 57.812 (63.684)\n",
      "Epoch: [79][312/390]\tTime 0.002 (0.004)\tLoss 1.1711 (1.0402)\tPrec@1 61.719 (62.807)\n",
      "Epoch: [79][390/390]\tTime 0.011 (0.004)\tLoss 1.0613 (1.0582)\tPrec@1 62.500 (62.166)\n",
      "EPOCH: 79 train Results: Prec@1 62.166 Loss: 1.0582\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1012 (1.1012)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0789 (1.2368)\tPrec@1 50.000 (56.130)\n",
      "EPOCH: 79 val Results: Prec@1 56.130 Loss: 1.2368\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/390]\tTime 0.006 (0.006)\tLoss 1.0653 (1.0653)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [80][78/390]\tTime 0.004 (0.003)\tLoss 0.9917 (0.9768)\tPrec@1 67.969 (65.338)\n",
      "Epoch: [80][156/390]\tTime 0.010 (0.003)\tLoss 1.0682 (1.0098)\tPrec@1 64.844 (64.048)\n",
      "Epoch: [80][234/390]\tTime 0.006 (0.003)\tLoss 0.9361 (1.0345)\tPrec@1 71.875 (63.191)\n",
      "Epoch: [80][312/390]\tTime 0.003 (0.003)\tLoss 1.1971 (1.0509)\tPrec@1 55.469 (62.662)\n",
      "Epoch: [80][390/390]\tTime 0.004 (0.003)\tLoss 0.9871 (1.0588)\tPrec@1 63.750 (62.376)\n",
      "EPOCH: 80 train Results: Prec@1 62.376 Loss: 1.0588\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0967 (1.0967)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3533 (1.2382)\tPrec@1 37.500 (56.360)\n",
      "EPOCH: 80 val Results: Prec@1 56.360 Loss: 1.2382\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/390]\tTime 0.002 (0.002)\tLoss 1.0092 (1.0092)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [81][78/390]\tTime 0.002 (0.003)\tLoss 0.9845 (0.9870)\tPrec@1 65.625 (64.893)\n",
      "Epoch: [81][156/390]\tTime 0.002 (0.003)\tLoss 0.9456 (1.0194)\tPrec@1 65.625 (63.878)\n",
      "Epoch: [81][234/390]\tTime 0.002 (0.004)\tLoss 0.9140 (1.0297)\tPrec@1 67.969 (63.381)\n",
      "Epoch: [81][312/390]\tTime 0.005 (0.004)\tLoss 1.3317 (1.0470)\tPrec@1 53.906 (62.819)\n",
      "Epoch: [81][390/390]\tTime 0.001 (0.003)\tLoss 0.9416 (1.0591)\tPrec@1 65.000 (62.296)\n",
      "EPOCH: 81 train Results: Prec@1 62.296 Loss: 1.0591\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0528 (1.0528)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2105 (1.2388)\tPrec@1 50.000 (56.070)\n",
      "EPOCH: 81 val Results: Prec@1 56.070 Loss: 1.2388\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/390]\tTime 0.002 (0.002)\tLoss 0.9328 (0.9328)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [82][78/390]\tTime 0.002 (0.003)\tLoss 0.8914 (0.9804)\tPrec@1 68.750 (64.972)\n",
      "Epoch: [82][156/390]\tTime 0.007 (0.003)\tLoss 1.1088 (1.0058)\tPrec@1 58.594 (64.033)\n",
      "Epoch: [82][234/390]\tTime 0.020 (0.004)\tLoss 1.0809 (1.0249)\tPrec@1 54.688 (63.477)\n",
      "Epoch: [82][312/390]\tTime 0.002 (0.004)\tLoss 0.9501 (1.0394)\tPrec@1 67.188 (63.114)\n",
      "Epoch: [82][390/390]\tTime 0.002 (0.004)\tLoss 1.1346 (1.0595)\tPrec@1 65.000 (62.384)\n",
      "EPOCH: 82 train Results: Prec@1 62.384 Loss: 1.0595\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0937 (1.0937)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0692 (1.2347)\tPrec@1 50.000 (55.730)\n",
      "EPOCH: 82 val Results: Prec@1 55.730 Loss: 1.2347\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/390]\tTime 0.003 (0.003)\tLoss 0.8932 (0.8932)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [83][78/390]\tTime 0.004 (0.003)\tLoss 1.1726 (0.9789)\tPrec@1 60.156 (65.398)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.2147 (1.0056)\tPrec@1 54.688 (64.122)\n",
      "Epoch: [83][234/390]\tTime 0.003 (0.004)\tLoss 1.0634 (1.0267)\tPrec@1 59.375 (63.338)\n",
      "Epoch: [83][312/390]\tTime 0.004 (0.004)\tLoss 1.1273 (1.0444)\tPrec@1 59.375 (62.790)\n",
      "Epoch: [83][390/390]\tTime 0.001 (0.004)\tLoss 1.2299 (1.0543)\tPrec@1 61.250 (62.356)\n",
      "EPOCH: 83 train Results: Prec@1 62.356 Loss: 1.0543\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.0239 (1.0239)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1605 (1.2462)\tPrec@1 37.500 (55.440)\n",
      "EPOCH: 83 val Results: Prec@1 55.440 Loss: 1.2462\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/390]\tTime 0.004 (0.004)\tLoss 0.8401 (0.8401)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [84][78/390]\tTime 0.004 (0.005)\tLoss 0.9206 (0.9969)\tPrec@1 69.531 (64.715)\n",
      "Epoch: [84][156/390]\tTime 0.002 (0.004)\tLoss 1.1842 (1.0151)\tPrec@1 60.156 (63.649)\n",
      "Epoch: [84][234/390]\tTime 0.002 (0.004)\tLoss 0.9738 (1.0361)\tPrec@1 67.188 (63.098)\n",
      "Epoch: [84][312/390]\tTime 0.002 (0.004)\tLoss 1.0543 (1.0467)\tPrec@1 60.156 (62.627)\n",
      "Epoch: [84][390/390]\tTime 0.001 (0.004)\tLoss 1.3391 (1.0536)\tPrec@1 55.000 (62.484)\n",
      "EPOCH: 84 train Results: Prec@1 62.484 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0489 (1.0489)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9687 (1.2547)\tPrec@1 62.500 (55.430)\n",
      "EPOCH: 84 val Results: Prec@1 55.430 Loss: 1.2547\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/390]\tTime 0.002 (0.002)\tLoss 0.9323 (0.9323)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [85][78/390]\tTime 0.002 (0.004)\tLoss 0.8057 (0.9630)\tPrec@1 70.312 (65.635)\n",
      "Epoch: [85][156/390]\tTime 0.006 (0.004)\tLoss 0.7996 (1.0001)\tPrec@1 71.094 (64.157)\n",
      "Epoch: [85][234/390]\tTime 0.003 (0.004)\tLoss 1.2340 (1.0238)\tPrec@1 53.906 (63.255)\n",
      "Epoch: [85][312/390]\tTime 0.002 (0.004)\tLoss 1.0246 (1.0428)\tPrec@1 66.406 (62.580)\n",
      "Epoch: [85][390/390]\tTime 0.002 (0.004)\tLoss 1.0914 (1.0555)\tPrec@1 56.250 (62.134)\n",
      "EPOCH: 85 train Results: Prec@1 62.134 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0969 (1.0969)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2887 (1.2496)\tPrec@1 31.250 (55.460)\n",
      "EPOCH: 85 val Results: Prec@1 55.460 Loss: 1.2496\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/390]\tTime 0.003 (0.003)\tLoss 0.8980 (0.8980)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [86][78/390]\tTime 0.011 (0.004)\tLoss 0.9312 (0.9929)\tPrec@1 67.969 (64.864)\n",
      "Epoch: [86][156/390]\tTime 0.017 (0.004)\tLoss 0.9313 (1.0079)\tPrec@1 66.406 (64.222)\n",
      "Epoch: [86][234/390]\tTime 0.002 (0.004)\tLoss 1.0326 (1.0247)\tPrec@1 65.625 (63.597)\n",
      "Epoch: [86][312/390]\tTime 0.003 (0.004)\tLoss 1.2417 (1.0376)\tPrec@1 57.812 (63.109)\n",
      "Epoch: [86][390/390]\tTime 0.003 (0.004)\tLoss 1.2067 (1.0521)\tPrec@1 57.500 (62.514)\n",
      "EPOCH: 86 train Results: Prec@1 62.514 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1562 (1.1562)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0967 (1.2604)\tPrec@1 43.750 (55.510)\n",
      "EPOCH: 86 val Results: Prec@1 55.510 Loss: 1.2604\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/390]\tTime 0.003 (0.003)\tLoss 1.0135 (1.0135)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [87][78/390]\tTime 0.004 (0.004)\tLoss 0.8647 (0.9788)\tPrec@1 69.531 (65.150)\n",
      "Epoch: [87][156/390]\tTime 0.002 (0.003)\tLoss 1.0418 (1.0100)\tPrec@1 59.375 (63.923)\n",
      "Epoch: [87][234/390]\tTime 0.004 (0.003)\tLoss 0.9866 (1.0246)\tPrec@1 68.750 (63.491)\n",
      "Epoch: [87][312/390]\tTime 0.007 (0.003)\tLoss 1.2780 (1.0428)\tPrec@1 55.469 (62.802)\n",
      "Epoch: [87][390/390]\tTime 0.002 (0.003)\tLoss 1.2523 (1.0552)\tPrec@1 55.000 (62.298)\n",
      "EPOCH: 87 train Results: Prec@1 62.298 Loss: 1.0552\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.0651 (1.0651)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9280 (1.2507)\tPrec@1 81.250 (55.830)\n",
      "EPOCH: 87 val Results: Prec@1 55.830 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/390]\tTime 0.005 (0.005)\tLoss 0.9995 (0.9995)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [88][78/390]\tTime 0.003 (0.003)\tLoss 0.9000 (0.9654)\tPrec@1 61.719 (65.961)\n",
      "Epoch: [88][156/390]\tTime 0.003 (0.003)\tLoss 1.1129 (1.0053)\tPrec@1 58.594 (64.237)\n",
      "Epoch: [88][234/390]\tTime 0.013 (0.003)\tLoss 1.0010 (1.0249)\tPrec@1 64.062 (63.414)\n",
      "Epoch: [88][312/390]\tTime 0.002 (0.003)\tLoss 1.0291 (1.0408)\tPrec@1 60.938 (62.767)\n",
      "Epoch: [88][390/390]\tTime 0.002 (0.003)\tLoss 1.2130 (1.0526)\tPrec@1 53.750 (62.344)\n",
      "EPOCH: 88 train Results: Prec@1 62.344 Loss: 1.0526\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1739 (1.1739)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9819 (1.2612)\tPrec@1 56.250 (55.480)\n",
      "EPOCH: 88 val Results: Prec@1 55.480 Loss: 1.2612\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/390]\tTime 0.004 (0.004)\tLoss 0.8559 (0.8559)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [89][78/390]\tTime 0.003 (0.004)\tLoss 0.9956 (0.9863)\tPrec@1 67.969 (64.814)\n",
      "Epoch: [89][156/390]\tTime 0.008 (0.003)\tLoss 1.1244 (1.0013)\tPrec@1 60.938 (64.003)\n",
      "Epoch: [89][234/390]\tTime 0.003 (0.004)\tLoss 1.0521 (1.0211)\tPrec@1 64.844 (63.338)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.1018 (1.0424)\tPrec@1 64.062 (62.742)\n",
      "Epoch: [89][390/390]\tTime 0.002 (0.003)\tLoss 1.2623 (1.0571)\tPrec@1 47.500 (62.278)\n",
      "EPOCH: 89 train Results: Prec@1 62.278 Loss: 1.0571\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0993 (1.0993)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5994 (1.2553)\tPrec@1 43.750 (55.680)\n",
      "EPOCH: 89 val Results: Prec@1 55.680 Loss: 1.2553\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/390]\tTime 0.005 (0.005)\tLoss 1.0333 (1.0333)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [90][78/390]\tTime 0.002 (0.003)\tLoss 1.0047 (0.9770)\tPrec@1 59.375 (65.269)\n",
      "Epoch: [90][156/390]\tTime 0.003 (0.003)\tLoss 1.1533 (1.0010)\tPrec@1 55.469 (64.545)\n",
      "Epoch: [90][234/390]\tTime 0.004 (0.004)\tLoss 1.1356 (1.0262)\tPrec@1 55.469 (63.467)\n",
      "Epoch: [90][312/390]\tTime 0.003 (0.003)\tLoss 1.1258 (1.0369)\tPrec@1 62.500 (63.136)\n",
      "Epoch: [90][390/390]\tTime 0.002 (0.003)\tLoss 0.9424 (1.0507)\tPrec@1 61.250 (62.734)\n",
      "EPOCH: 90 train Results: Prec@1 62.734 Loss: 1.0507\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0546 (1.0546)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4359 (1.2632)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 90 val Results: Prec@1 55.300 Loss: 1.2632\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/390]\tTime 0.003 (0.003)\tLoss 1.1339 (1.1339)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [91][78/390]\tTime 0.004 (0.003)\tLoss 1.0576 (0.9768)\tPrec@1 63.281 (65.833)\n",
      "Epoch: [91][156/390]\tTime 0.002 (0.003)\tLoss 1.1931 (1.0058)\tPrec@1 53.125 (64.306)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.003)\tLoss 1.1554 (1.0209)\tPrec@1 55.469 (63.893)\n",
      "Epoch: [91][312/390]\tTime 0.002 (0.003)\tLoss 1.1233 (1.0426)\tPrec@1 63.281 (63.186)\n",
      "Epoch: [91][390/390]\tTime 0.001 (0.003)\tLoss 1.1399 (1.0569)\tPrec@1 61.250 (62.574)\n",
      "EPOCH: 91 train Results: Prec@1 62.574 Loss: 1.0569\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0760 (1.0760)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1623 (1.2602)\tPrec@1 43.750 (55.310)\n",
      "EPOCH: 91 val Results: Prec@1 55.310 Loss: 1.2602\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/390]\tTime 0.003 (0.003)\tLoss 0.9916 (0.9916)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.003)\tLoss 0.9857 (0.9783)\tPrec@1 64.062 (65.051)\n",
      "Epoch: [92][156/390]\tTime 0.002 (0.003)\tLoss 1.1312 (1.0153)\tPrec@1 59.375 (63.878)\n",
      "Epoch: [92][234/390]\tTime 0.002 (0.003)\tLoss 1.0429 (1.0284)\tPrec@1 62.500 (63.338)\n",
      "Epoch: [92][312/390]\tTime 0.005 (0.003)\tLoss 1.1341 (1.0430)\tPrec@1 54.688 (62.834)\n",
      "Epoch: [92][390/390]\tTime 0.008 (0.003)\tLoss 1.0723 (1.0556)\tPrec@1 55.000 (62.358)\n",
      "EPOCH: 92 train Results: Prec@1 62.358 Loss: 1.0556\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1095 (1.1095)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2882 (1.2490)\tPrec@1 56.250 (55.770)\n",
      "EPOCH: 92 val Results: Prec@1 55.770 Loss: 1.2490\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 1.1153 (1.1153)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.003)\tLoss 0.9583 (0.9637)\tPrec@1 61.719 (66.011)\n",
      "Epoch: [93][156/390]\tTime 0.003 (0.003)\tLoss 1.0579 (0.9988)\tPrec@1 62.500 (64.485)\n",
      "Epoch: [93][234/390]\tTime 0.012 (0.003)\tLoss 1.0428 (1.0191)\tPrec@1 60.938 (63.800)\n",
      "Epoch: [93][312/390]\tTime 0.002 (0.003)\tLoss 1.0828 (1.0350)\tPrec@1 61.719 (63.151)\n",
      "Epoch: [93][390/390]\tTime 0.002 (0.003)\tLoss 1.0638 (1.0515)\tPrec@1 67.500 (62.572)\n",
      "EPOCH: 93 train Results: Prec@1 62.572 Loss: 1.0515\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1133 (1.1133)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0813 (1.2490)\tPrec@1 50.000 (56.050)\n",
      "EPOCH: 93 val Results: Prec@1 56.050 Loss: 1.2490\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/390]\tTime 0.007 (0.007)\tLoss 0.8406 (0.8406)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [94][78/390]\tTime 0.007 (0.004)\tLoss 0.9173 (0.9631)\tPrec@1 61.719 (66.119)\n",
      "Epoch: [94][156/390]\tTime 0.011 (0.003)\tLoss 1.0470 (0.9945)\tPrec@1 64.062 (64.804)\n",
      "Epoch: [94][234/390]\tTime 0.002 (0.003)\tLoss 0.9596 (1.0226)\tPrec@1 67.188 (63.680)\n",
      "Epoch: [94][312/390]\tTime 0.002 (0.003)\tLoss 1.1657 (1.0376)\tPrec@1 59.375 (63.174)\n",
      "Epoch: [94][390/390]\tTime 0.003 (0.003)\tLoss 1.3645 (1.0508)\tPrec@1 61.250 (62.662)\n",
      "EPOCH: 94 train Results: Prec@1 62.662 Loss: 1.0508\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1694 (1.1694)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2530 (1.2521)\tPrec@1 50.000 (55.590)\n",
      "EPOCH: 94 val Results: Prec@1 55.590 Loss: 1.2521\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/390]\tTime 0.003 (0.003)\tLoss 0.9955 (0.9955)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [95][78/390]\tTime 0.003 (0.003)\tLoss 1.0948 (0.9753)\tPrec@1 58.594 (65.615)\n",
      "Epoch: [95][156/390]\tTime 0.005 (0.003)\tLoss 0.9292 (1.0011)\tPrec@1 65.625 (64.570)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.003)\tLoss 1.0815 (1.0234)\tPrec@1 60.938 (63.780)\n",
      "Epoch: [95][312/390]\tTime 0.004 (0.003)\tLoss 1.0083 (1.0413)\tPrec@1 69.531 (63.034)\n",
      "Epoch: [95][390/390]\tTime 0.002 (0.003)\tLoss 1.2892 (1.0548)\tPrec@1 55.000 (62.504)\n",
      "EPOCH: 95 train Results: Prec@1 62.504 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0758 (1.0758)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2688 (1.2507)\tPrec@1 56.250 (56.000)\n",
      "EPOCH: 95 val Results: Prec@1 56.000 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/390]\tTime 0.003 (0.003)\tLoss 1.0427 (1.0427)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.003)\tLoss 0.9270 (0.9719)\tPrec@1 61.719 (65.645)\n",
      "Epoch: [96][156/390]\tTime 0.002 (0.004)\tLoss 0.9314 (1.0010)\tPrec@1 70.312 (64.391)\n",
      "Epoch: [96][234/390]\tTime 0.002 (0.003)\tLoss 0.9764 (1.0256)\tPrec@1 60.938 (63.497)\n",
      "Epoch: [96][312/390]\tTime 0.003 (0.004)\tLoss 1.0744 (1.0378)\tPrec@1 62.500 (63.047)\n",
      "Epoch: [96][390/390]\tTime 0.003 (0.003)\tLoss 1.1342 (1.0506)\tPrec@1 56.250 (62.570)\n",
      "EPOCH: 96 train Results: Prec@1 62.570 Loss: 1.0506\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1188 (1.1188)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1228 (1.2556)\tPrec@1 56.250 (55.160)\n",
      "EPOCH: 96 val Results: Prec@1 55.160 Loss: 1.2556\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/390]\tTime 0.003 (0.003)\tLoss 1.0088 (1.0088)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [97][78/390]\tTime 0.008 (0.003)\tLoss 0.9967 (0.9887)\tPrec@1 66.406 (64.547)\n",
      "Epoch: [97][156/390]\tTime 0.012 (0.003)\tLoss 1.1829 (1.0084)\tPrec@1 57.812 (63.893)\n",
      "Epoch: [97][234/390]\tTime 0.006 (0.003)\tLoss 1.2903 (1.0248)\tPrec@1 52.344 (63.251)\n",
      "Epoch: [97][312/390]\tTime 0.002 (0.004)\tLoss 1.0482 (1.0389)\tPrec@1 64.844 (62.837)\n",
      "Epoch: [97][390/390]\tTime 0.003 (0.004)\tLoss 1.0343 (1.0521)\tPrec@1 60.000 (62.310)\n",
      "EPOCH: 97 train Results: Prec@1 62.310 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1167 (1.1167)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0366 (1.2541)\tPrec@1 50.000 (55.810)\n",
      "EPOCH: 97 val Results: Prec@1 55.810 Loss: 1.2541\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/390]\tTime 0.003 (0.003)\tLoss 1.0014 (1.0014)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [98][78/390]\tTime 0.004 (0.003)\tLoss 0.8379 (0.9678)\tPrec@1 71.094 (65.506)\n",
      "Epoch: [98][156/390]\tTime 0.006 (0.003)\tLoss 1.2156 (0.9963)\tPrec@1 57.031 (64.704)\n",
      "Epoch: [98][234/390]\tTime 0.011 (0.003)\tLoss 0.9183 (1.0151)\tPrec@1 67.188 (63.873)\n",
      "Epoch: [98][312/390]\tTime 0.005 (0.003)\tLoss 1.2128 (1.0313)\tPrec@1 57.812 (63.256)\n",
      "Epoch: [98][390/390]\tTime 0.006 (0.003)\tLoss 1.0868 (1.0448)\tPrec@1 58.750 (62.808)\n",
      "EPOCH: 98 train Results: Prec@1 62.808 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1962 (1.1962)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4396 (1.2620)\tPrec@1 25.000 (55.340)\n",
      "EPOCH: 98 val Results: Prec@1 55.340 Loss: 1.2620\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/390]\tTime 0.002 (0.002)\tLoss 0.9243 (0.9243)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.003)\tLoss 0.8636 (0.9757)\tPrec@1 70.312 (65.674)\n",
      "Epoch: [99][156/390]\tTime 0.003 (0.003)\tLoss 1.0347 (1.0092)\tPrec@1 60.156 (64.197)\n",
      "Epoch: [99][234/390]\tTime 0.002 (0.003)\tLoss 1.2268 (1.0236)\tPrec@1 52.344 (63.710)\n",
      "Epoch: [99][312/390]\tTime 0.002 (0.003)\tLoss 1.1035 (1.0377)\tPrec@1 56.250 (63.069)\n",
      "Epoch: [99][390/390]\tTime 0.003 (0.003)\tLoss 1.0416 (1.0497)\tPrec@1 60.000 (62.538)\n",
      "EPOCH: 99 train Results: Prec@1 62.538 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1437 (1.1437)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4257 (1.2477)\tPrec@1 50.000 (55.610)\n",
      "EPOCH: 99 val Results: Prec@1 55.610 Loss: 1.2477\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/390]\tTime 0.002 (0.002)\tLoss 0.8875 (0.8875)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [100][78/390]\tTime 0.004 (0.003)\tLoss 1.0347 (0.9757)\tPrec@1 58.594 (64.666)\n",
      "Epoch: [100][156/390]\tTime 0.002 (0.003)\tLoss 1.0858 (1.0032)\tPrec@1 57.812 (64.003)\n",
      "Epoch: [100][234/390]\tTime 0.002 (0.003)\tLoss 1.2427 (1.0248)\tPrec@1 53.906 (63.291)\n",
      "Epoch: [100][312/390]\tTime 0.002 (0.003)\tLoss 1.0586 (1.0436)\tPrec@1 64.062 (62.527)\n",
      "Epoch: [100][390/390]\tTime 0.002 (0.003)\tLoss 1.1984 (1.0530)\tPrec@1 56.250 (62.242)\n",
      "EPOCH: 100 train Results: Prec@1 62.242 Loss: 1.0530\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1344 (1.1344)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2422 (1.2494)\tPrec@1 37.500 (56.000)\n",
      "EPOCH: 100 val Results: Prec@1 56.000 Loss: 1.2494\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/390]\tTime 0.004 (0.004)\tLoss 0.9254 (0.9254)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [101][78/390]\tTime 0.004 (0.003)\tLoss 1.0826 (0.9830)\tPrec@1 53.906 (65.140)\n",
      "Epoch: [101][156/390]\tTime 0.003 (0.003)\tLoss 1.1910 (1.0097)\tPrec@1 56.250 (63.988)\n",
      "Epoch: [101][234/390]\tTime 0.008 (0.004)\tLoss 1.0678 (1.0299)\tPrec@1 67.188 (63.275)\n",
      "Epoch: [101][312/390]\tTime 0.004 (0.004)\tLoss 1.0244 (1.0359)\tPrec@1 64.844 (62.992)\n",
      "Epoch: [101][390/390]\tTime 0.003 (0.004)\tLoss 1.1678 (1.0487)\tPrec@1 60.000 (62.598)\n",
      "EPOCH: 101 train Results: Prec@1 62.598 Loss: 1.0487\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1532 (1.1532)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3408 (1.2666)\tPrec@1 18.750 (54.950)\n",
      "EPOCH: 101 val Results: Prec@1 54.950 Loss: 1.2666\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/390]\tTime 0.004 (0.004)\tLoss 1.0317 (1.0317)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [102][78/390]\tTime 0.002 (0.003)\tLoss 0.9895 (0.9583)\tPrec@1 59.375 (66.080)\n",
      "Epoch: [102][156/390]\tTime 0.005 (0.003)\tLoss 1.0654 (0.9975)\tPrec@1 63.281 (64.645)\n",
      "Epoch: [102][234/390]\tTime 0.008 (0.003)\tLoss 1.0530 (1.0209)\tPrec@1 64.062 (63.797)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.003)\tLoss 1.0630 (1.0392)\tPrec@1 54.688 (62.992)\n",
      "Epoch: [102][390/390]\tTime 0.003 (0.003)\tLoss 1.0967 (1.0507)\tPrec@1 58.750 (62.600)\n",
      "EPOCH: 102 train Results: Prec@1 62.600 Loss: 1.0507\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1662 (1.1662)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3014 (1.2577)\tPrec@1 37.500 (55.270)\n",
      "EPOCH: 102 val Results: Prec@1 55.270 Loss: 1.2577\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/390]\tTime 0.008 (0.008)\tLoss 0.9682 (0.9682)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [103][78/390]\tTime 0.004 (0.003)\tLoss 0.8984 (0.9784)\tPrec@1 64.062 (65.239)\n",
      "Epoch: [103][156/390]\tTime 0.004 (0.003)\tLoss 1.1070 (1.0056)\tPrec@1 61.719 (64.043)\n",
      "Epoch: [103][234/390]\tTime 0.002 (0.003)\tLoss 1.1402 (1.0241)\tPrec@1 62.500 (63.457)\n",
      "Epoch: [103][312/390]\tTime 0.003 (0.003)\tLoss 1.2198 (1.0367)\tPrec@1 57.031 (63.144)\n",
      "Epoch: [103][390/390]\tTime 0.007 (0.003)\tLoss 1.1931 (1.0488)\tPrec@1 60.000 (62.696)\n",
      "EPOCH: 103 train Results: Prec@1 62.696 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0719 (1.0719)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2487 (1.2660)\tPrec@1 56.250 (55.050)\n",
      "EPOCH: 103 val Results: Prec@1 55.050 Loss: 1.2660\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/390]\tTime 0.005 (0.005)\tLoss 1.0331 (1.0331)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 1.0011 (0.9796)\tPrec@1 66.406 (65.121)\n",
      "Epoch: [104][156/390]\tTime 0.003 (0.003)\tLoss 0.9663 (1.0074)\tPrec@1 63.281 (64.077)\n",
      "Epoch: [104][234/390]\tTime 0.005 (0.003)\tLoss 0.9742 (1.0189)\tPrec@1 64.062 (63.680)\n",
      "Epoch: [104][312/390]\tTime 0.002 (0.003)\tLoss 0.9747 (1.0376)\tPrec@1 64.062 (62.927)\n",
      "Epoch: [104][390/390]\tTime 0.002 (0.003)\tLoss 1.0613 (1.0492)\tPrec@1 65.000 (62.474)\n",
      "EPOCH: 104 train Results: Prec@1 62.474 Loss: 1.0492\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1067 (1.1067)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1503 (1.2508)\tPrec@1 37.500 (55.550)\n",
      "EPOCH: 104 val Results: Prec@1 55.550 Loss: 1.2508\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/390]\tTime 0.003 (0.003)\tLoss 0.8887 (0.8887)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [105][78/390]\tTime 0.002 (0.004)\tLoss 0.9612 (0.9843)\tPrec@1 63.281 (65.368)\n",
      "Epoch: [105][156/390]\tTime 0.004 (0.004)\tLoss 1.1261 (1.0031)\tPrec@1 59.375 (64.540)\n",
      "Epoch: [105][234/390]\tTime 0.002 (0.004)\tLoss 1.2823 (1.0195)\tPrec@1 58.594 (63.757)\n",
      "Epoch: [105][312/390]\tTime 0.003 (0.004)\tLoss 1.1225 (1.0329)\tPrec@1 61.719 (63.289)\n",
      "Epoch: [105][390/390]\tTime 0.003 (0.004)\tLoss 1.1396 (1.0504)\tPrec@1 58.750 (62.800)\n",
      "EPOCH: 105 train Results: Prec@1 62.800 Loss: 1.0504\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.0322 (1.0322)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1672 (1.2508)\tPrec@1 43.750 (55.840)\n",
      "EPOCH: 105 val Results: Prec@1 55.840 Loss: 1.2508\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/390]\tTime 0.002 (0.002)\tLoss 0.8676 (0.8676)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [106][78/390]\tTime 0.002 (0.003)\tLoss 0.8870 (0.9637)\tPrec@1 64.844 (65.388)\n",
      "Epoch: [106][156/390]\tTime 0.002 (0.003)\tLoss 1.0311 (0.9966)\tPrec@1 60.156 (64.416)\n",
      "Epoch: [106][234/390]\tTime 0.007 (0.003)\tLoss 1.0268 (1.0190)\tPrec@1 63.281 (63.541)\n",
      "Epoch: [106][312/390]\tTime 0.003 (0.003)\tLoss 1.0719 (1.0369)\tPrec@1 61.719 (62.947)\n",
      "Epoch: [106][390/390]\tTime 0.001 (0.003)\tLoss 1.2130 (1.0498)\tPrec@1 63.750 (62.414)\n",
      "EPOCH: 106 train Results: Prec@1 62.414 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 0.9733 (0.9733)\tPrec@1 69.531 (69.531)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3981 (1.2439)\tPrec@1 37.500 (55.830)\n",
      "EPOCH: 106 val Results: Prec@1 55.830 Loss: 1.2439\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/390]\tTime 0.002 (0.002)\tLoss 0.8487 (0.8487)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [107][78/390]\tTime 0.002 (0.004)\tLoss 1.0857 (0.9655)\tPrec@1 66.406 (65.773)\n",
      "Epoch: [107][156/390]\tTime 0.003 (0.004)\tLoss 1.2772 (0.9917)\tPrec@1 49.219 (64.515)\n",
      "Epoch: [107][234/390]\tTime 0.003 (0.004)\tLoss 1.2490 (1.0126)\tPrec@1 56.250 (63.830)\n",
      "Epoch: [107][312/390]\tTime 0.005 (0.004)\tLoss 1.1009 (1.0383)\tPrec@1 60.156 (62.892)\n",
      "Epoch: [107][390/390]\tTime 0.002 (0.004)\tLoss 1.0090 (1.0536)\tPrec@1 66.250 (62.330)\n",
      "EPOCH: 107 train Results: Prec@1 62.330 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1421 (1.1421)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4340 (1.2430)\tPrec@1 31.250 (56.090)\n",
      "EPOCH: 107 val Results: Prec@1 56.090 Loss: 1.2430\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/390]\tTime 0.004 (0.004)\tLoss 0.9561 (0.9561)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [108][78/390]\tTime 0.002 (0.003)\tLoss 1.0664 (0.9586)\tPrec@1 65.625 (65.427)\n",
      "Epoch: [108][156/390]\tTime 0.002 (0.003)\tLoss 1.0058 (0.9943)\tPrec@1 66.406 (64.431)\n",
      "Epoch: [108][234/390]\tTime 0.008 (0.003)\tLoss 1.1536 (1.0170)\tPrec@1 57.031 (63.687)\n",
      "Epoch: [108][312/390]\tTime 0.002 (0.003)\tLoss 1.1132 (1.0324)\tPrec@1 63.281 (63.107)\n",
      "Epoch: [108][390/390]\tTime 0.002 (0.004)\tLoss 1.0228 (1.0460)\tPrec@1 62.500 (62.678)\n",
      "EPOCH: 108 train Results: Prec@1 62.678 Loss: 1.0460\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0986 (1.0986)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3707 (1.2564)\tPrec@1 31.250 (55.430)\n",
      "EPOCH: 108 val Results: Prec@1 55.430 Loss: 1.2564\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/390]\tTime 0.003 (0.003)\tLoss 1.0514 (1.0514)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [109][78/390]\tTime 0.002 (0.003)\tLoss 1.0287 (0.9789)\tPrec@1 60.938 (65.496)\n",
      "Epoch: [109][156/390]\tTime 0.008 (0.003)\tLoss 1.1056 (1.0128)\tPrec@1 63.281 (64.147)\n",
      "Epoch: [109][234/390]\tTime 0.002 (0.003)\tLoss 0.8665 (1.0242)\tPrec@1 63.281 (63.541)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.003)\tLoss 1.0765 (1.0368)\tPrec@1 59.375 (63.062)\n",
      "Epoch: [109][390/390]\tTime 0.001 (0.003)\tLoss 1.0126 (1.0458)\tPrec@1 66.250 (62.826)\n",
      "EPOCH: 109 train Results: Prec@1 62.826 Loss: 1.0458\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0866 (1.0866)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4049 (1.2472)\tPrec@1 43.750 (55.830)\n",
      "EPOCH: 109 val Results: Prec@1 55.830 Loss: 1.2472\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/390]\tTime 0.003 (0.003)\tLoss 0.9575 (0.9575)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [110][78/390]\tTime 0.022 (0.023)\tLoss 0.9617 (0.9704)\tPrec@1 64.844 (65.754)\n",
      "Epoch: [110][156/390]\tTime 0.003 (0.022)\tLoss 0.9745 (1.0015)\tPrec@1 66.406 (64.366)\n",
      "Epoch: [110][234/390]\tTime 0.010 (0.023)\tLoss 1.1295 (1.0162)\tPrec@1 54.688 (63.737)\n",
      "Epoch: [110][312/390]\tTime 0.041 (0.023)\tLoss 1.0311 (1.0375)\tPrec@1 60.938 (62.964)\n",
      "Epoch: [110][390/390]\tTime 0.022 (0.023)\tLoss 1.2203 (1.0472)\tPrec@1 60.000 (62.600)\n",
      "EPOCH: 110 train Results: Prec@1 62.600 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.016 (0.016)\tLoss 1.0518 (1.0518)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.005)\tLoss 1.4546 (1.2513)\tPrec@1 31.250 (55.780)\n",
      "EPOCH: 110 val Results: Prec@1 55.780 Loss: 1.2513\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/390]\tTime 0.003 (0.003)\tLoss 0.9275 (0.9275)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [111][78/390]\tTime 0.002 (0.004)\tLoss 1.0038 (0.9714)\tPrec@1 65.625 (65.388)\n",
      "Epoch: [111][156/390]\tTime 0.002 (0.004)\tLoss 0.9791 (1.0022)\tPrec@1 63.281 (64.082)\n",
      "Epoch: [111][234/390]\tTime 0.003 (0.004)\tLoss 1.2667 (1.0251)\tPrec@1 57.031 (63.298)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.004)\tLoss 1.0145 (1.0410)\tPrec@1 63.281 (62.610)\n",
      "Epoch: [111][390/390]\tTime 0.002 (0.004)\tLoss 1.0910 (1.0516)\tPrec@1 63.750 (62.280)\n",
      "EPOCH: 111 train Results: Prec@1 62.280 Loss: 1.0516\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0741 (1.0741)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1872 (1.2686)\tPrec@1 56.250 (55.430)\n",
      "EPOCH: 111 val Results: Prec@1 55.430 Loss: 1.2686\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/390]\tTime 0.002 (0.002)\tLoss 0.8688 (0.8688)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.0722 (0.9618)\tPrec@1 64.844 (66.386)\n",
      "Epoch: [112][156/390]\tTime 0.002 (0.003)\tLoss 1.0915 (0.9913)\tPrec@1 57.031 (65.048)\n",
      "Epoch: [112][234/390]\tTime 0.003 (0.003)\tLoss 1.1248 (1.0128)\tPrec@1 57.812 (64.219)\n",
      "Epoch: [112][312/390]\tTime 0.002 (0.003)\tLoss 0.9686 (1.0398)\tPrec@1 67.188 (63.256)\n",
      "Epoch: [112][390/390]\tTime 0.003 (0.003)\tLoss 1.2718 (1.0478)\tPrec@1 52.500 (62.950)\n",
      "EPOCH: 112 train Results: Prec@1 62.950 Loss: 1.0478\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0866 (1.0866)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2353 (1.2671)\tPrec@1 56.250 (55.170)\n",
      "EPOCH: 112 val Results: Prec@1 55.170 Loss: 1.2671\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 0.9353 (0.9353)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [113][78/390]\tTime 0.002 (0.003)\tLoss 0.9818 (0.9667)\tPrec@1 67.969 (66.021)\n",
      "Epoch: [113][156/390]\tTime 0.002 (0.003)\tLoss 1.0095 (0.9895)\tPrec@1 64.062 (64.864)\n",
      "Epoch: [113][234/390]\tTime 0.004 (0.004)\tLoss 0.8883 (1.0094)\tPrec@1 67.188 (64.026)\n",
      "Epoch: [113][312/390]\tTime 0.003 (0.004)\tLoss 1.0436 (1.0262)\tPrec@1 60.938 (63.441)\n",
      "Epoch: [113][390/390]\tTime 0.003 (0.004)\tLoss 1.1023 (1.0431)\tPrec@1 60.000 (62.872)\n",
      "EPOCH: 113 train Results: Prec@1 62.872 Loss: 1.0431\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1274 (1.1274)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 0.9470 (1.2451)\tPrec@1 56.250 (55.880)\n",
      "EPOCH: 113 val Results: Prec@1 55.880 Loss: 1.2451\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/390]\tTime 0.003 (0.003)\tLoss 0.9677 (0.9677)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [114][78/390]\tTime 0.006 (0.003)\tLoss 0.7941 (0.9793)\tPrec@1 75.000 (65.506)\n",
      "Epoch: [114][156/390]\tTime 0.002 (0.003)\tLoss 1.0040 (1.0067)\tPrec@1 65.625 (64.286)\n",
      "Epoch: [114][234/390]\tTime 0.002 (0.004)\tLoss 1.0510 (1.0231)\tPrec@1 60.156 (63.604)\n",
      "Epoch: [114][312/390]\tTime 0.002 (0.004)\tLoss 1.0073 (1.0352)\tPrec@1 65.625 (63.221)\n",
      "Epoch: [114][390/390]\tTime 0.009 (0.004)\tLoss 0.9860 (1.0466)\tPrec@1 66.250 (62.690)\n",
      "EPOCH: 114 train Results: Prec@1 62.690 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0382 (1.0382)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3069 (1.2575)\tPrec@1 50.000 (55.460)\n",
      "EPOCH: 114 val Results: Prec@1 55.460 Loss: 1.2575\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/390]\tTime 0.002 (0.002)\tLoss 0.7814 (0.7814)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [115][78/390]\tTime 0.003 (0.003)\tLoss 0.8657 (0.9689)\tPrec@1 71.094 (65.645)\n",
      "Epoch: [115][156/390]\tTime 0.003 (0.003)\tLoss 1.1498 (0.9985)\tPrec@1 57.031 (64.331)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.003)\tLoss 1.1475 (1.0192)\tPrec@1 56.250 (63.501)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.003)\tLoss 1.2164 (1.0376)\tPrec@1 54.688 (62.927)\n",
      "Epoch: [115][390/390]\tTime 0.003 (0.003)\tLoss 1.3975 (1.0477)\tPrec@1 55.000 (62.592)\n",
      "EPOCH: 115 train Results: Prec@1 62.592 Loss: 1.0477\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0696 (1.0696)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3099 (1.2569)\tPrec@1 50.000 (55.730)\n",
      "EPOCH: 115 val Results: Prec@1 55.730 Loss: 1.2569\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/390]\tTime 0.004 (0.004)\tLoss 0.9875 (0.9875)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [116][78/390]\tTime 0.002 (0.003)\tLoss 0.8823 (0.9635)\tPrec@1 69.531 (65.734)\n",
      "Epoch: [116][156/390]\tTime 0.002 (0.003)\tLoss 1.1125 (0.9933)\tPrec@1 61.719 (64.421)\n",
      "Epoch: [116][234/390]\tTime 0.002 (0.003)\tLoss 1.0027 (1.0165)\tPrec@1 65.625 (63.737)\n",
      "Epoch: [116][312/390]\tTime 0.003 (0.003)\tLoss 1.1709 (1.0354)\tPrec@1 57.031 (63.176)\n",
      "Epoch: [116][390/390]\tTime 0.001 (0.003)\tLoss 1.1687 (1.0459)\tPrec@1 61.250 (62.776)\n",
      "EPOCH: 116 train Results: Prec@1 62.776 Loss: 1.0459\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0481 (1.0481)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0719 (1.2552)\tPrec@1 43.750 (55.720)\n",
      "EPOCH: 116 val Results: Prec@1 55.720 Loss: 1.2552\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/390]\tTime 0.002 (0.002)\tLoss 0.9939 (0.9939)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [117][78/390]\tTime 0.002 (0.003)\tLoss 0.9940 (0.9623)\tPrec@1 63.281 (66.119)\n",
      "Epoch: [117][156/390]\tTime 0.006 (0.003)\tLoss 1.1332 (0.9940)\tPrec@1 61.719 (64.670)\n",
      "Epoch: [117][234/390]\tTime 0.003 (0.003)\tLoss 1.1604 (1.0174)\tPrec@1 59.375 (63.836)\n",
      "Epoch: [117][312/390]\tTime 0.002 (0.003)\tLoss 1.0499 (1.0337)\tPrec@1 56.250 (63.156)\n",
      "Epoch: [117][390/390]\tTime 0.002 (0.003)\tLoss 1.0532 (1.0426)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 117 train Results: Prec@1 62.850 Loss: 1.0426\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 0.9877 (0.9877)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4792 (1.2551)\tPrec@1 25.000 (55.770)\n",
      "EPOCH: 117 val Results: Prec@1 55.770 Loss: 1.2551\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/390]\tTime 0.007 (0.007)\tLoss 1.0176 (1.0176)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [118][78/390]\tTime 0.005 (0.003)\tLoss 0.9719 (0.9888)\tPrec@1 69.531 (65.506)\n",
      "Epoch: [118][156/390]\tTime 0.007 (0.003)\tLoss 1.0682 (0.9991)\tPrec@1 63.281 (64.625)\n",
      "Epoch: [118][234/390]\tTime 0.008 (0.005)\tLoss 1.1744 (1.0164)\tPrec@1 60.156 (63.973)\n",
      "Epoch: [118][312/390]\tTime 0.002 (0.004)\tLoss 1.1091 (1.0370)\tPrec@1 64.844 (63.057)\n",
      "Epoch: [118][390/390]\tTime 0.001 (0.004)\tLoss 1.1544 (1.0515)\tPrec@1 60.000 (62.562)\n",
      "EPOCH: 118 train Results: Prec@1 62.562 Loss: 1.0515\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.0916 (1.0916)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1223 (1.2681)\tPrec@1 37.500 (55.330)\n",
      "EPOCH: 118 val Results: Prec@1 55.330 Loss: 1.2681\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/390]\tTime 0.004 (0.004)\tLoss 0.9615 (0.9615)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [119][78/390]\tTime 0.002 (0.003)\tLoss 1.0151 (0.9596)\tPrec@1 64.844 (66.317)\n",
      "Epoch: [119][156/390]\tTime 0.002 (0.003)\tLoss 1.0276 (0.9860)\tPrec@1 63.281 (65.147)\n",
      "Epoch: [119][234/390]\tTime 0.003 (0.003)\tLoss 0.9933 (1.0087)\tPrec@1 66.406 (64.119)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.003)\tLoss 1.0602 (1.0250)\tPrec@1 63.281 (63.498)\n",
      "Epoch: [119][390/390]\tTime 0.001 (0.003)\tLoss 1.2058 (1.0399)\tPrec@1 58.750 (62.928)\n",
      "EPOCH: 119 train Results: Prec@1 62.928 Loss: 1.0399\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0256 (1.0256)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2464 (1.2539)\tPrec@1 43.750 (55.800)\n",
      "EPOCH: 119 val Results: Prec@1 55.800 Loss: 1.2539\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/390]\tTime 0.002 (0.002)\tLoss 0.8039 (0.8039)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [120][78/390]\tTime 0.004 (0.004)\tLoss 1.0426 (0.9665)\tPrec@1 61.719 (65.872)\n",
      "Epoch: [120][156/390]\tTime 0.005 (0.004)\tLoss 0.9273 (0.9957)\tPrec@1 65.625 (64.759)\n",
      "Epoch: [120][234/390]\tTime 0.002 (0.004)\tLoss 1.0779 (1.0159)\tPrec@1 64.062 (63.973)\n",
      "Epoch: [120][312/390]\tTime 0.002 (0.004)\tLoss 1.2875 (1.0279)\tPrec@1 53.125 (63.354)\n",
      "Epoch: [120][390/390]\tTime 0.003 (0.004)\tLoss 1.0466 (1.0410)\tPrec@1 62.500 (62.834)\n",
      "EPOCH: 120 train Results: Prec@1 62.834 Loss: 1.0410\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0010 (1.0010)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0320 (1.2478)\tPrec@1 56.250 (56.240)\n",
      "EPOCH: 120 val Results: Prec@1 56.240 Loss: 1.2478\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/390]\tTime 0.002 (0.002)\tLoss 0.8149 (0.8149)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [121][78/390]\tTime 0.002 (0.003)\tLoss 1.0520 (0.9612)\tPrec@1 60.938 (65.783)\n",
      "Epoch: [121][156/390]\tTime 0.002 (0.003)\tLoss 1.0134 (0.9966)\tPrec@1 63.281 (64.396)\n",
      "Epoch: [121][234/390]\tTime 0.003 (0.003)\tLoss 1.0476 (1.0121)\tPrec@1 58.594 (63.976)\n",
      "Epoch: [121][312/390]\tTime 0.002 (0.003)\tLoss 0.8982 (1.0299)\tPrec@1 72.656 (63.291)\n",
      "Epoch: [121][390/390]\tTime 0.006 (0.003)\tLoss 0.8254 (1.0419)\tPrec@1 71.250 (62.956)\n",
      "EPOCH: 121 train Results: Prec@1 62.956 Loss: 1.0419\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0666 (1.0666)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2321 (1.2533)\tPrec@1 37.500 (55.430)\n",
      "EPOCH: 121 val Results: Prec@1 55.430 Loss: 1.2533\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/390]\tTime 0.003 (0.003)\tLoss 0.9738 (0.9738)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [122][78/390]\tTime 0.002 (0.003)\tLoss 0.9679 (0.9587)\tPrec@1 65.625 (66.317)\n",
      "Epoch: [122][156/390]\tTime 0.002 (0.003)\tLoss 1.0490 (0.9898)\tPrec@1 66.406 (65.112)\n",
      "Epoch: [122][234/390]\tTime 0.002 (0.003)\tLoss 1.1844 (1.0146)\tPrec@1 60.156 (64.209)\n",
      "Epoch: [122][312/390]\tTime 0.007 (0.003)\tLoss 1.2389 (1.0327)\tPrec@1 55.469 (63.319)\n",
      "Epoch: [122][390/390]\tTime 0.004 (0.003)\tLoss 1.2570 (1.0456)\tPrec@1 60.000 (62.876)\n",
      "EPOCH: 122 train Results: Prec@1 62.876 Loss: 1.0456\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1073 (1.1073)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2241 (1.2696)\tPrec@1 37.500 (55.730)\n",
      "EPOCH: 122 val Results: Prec@1 55.730 Loss: 1.2696\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/390]\tTime 0.002 (0.002)\tLoss 0.8362 (0.8362)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [123][78/390]\tTime 0.003 (0.003)\tLoss 1.0265 (0.9625)\tPrec@1 57.031 (65.496)\n",
      "Epoch: [123][156/390]\tTime 0.002 (0.003)\tLoss 1.1098 (0.9918)\tPrec@1 60.156 (64.202)\n",
      "Epoch: [123][234/390]\tTime 0.004 (0.003)\tLoss 1.3122 (1.0198)\tPrec@1 53.906 (63.418)\n",
      "Epoch: [123][312/390]\tTime 0.008 (0.003)\tLoss 1.1775 (1.0357)\tPrec@1 58.594 (62.992)\n",
      "Epoch: [123][390/390]\tTime 0.003 (0.003)\tLoss 1.0744 (1.0457)\tPrec@1 56.250 (62.558)\n",
      "EPOCH: 123 train Results: Prec@1 62.558 Loss: 1.0457\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1335 (1.1335)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2625 (1.2468)\tPrec@1 31.250 (56.130)\n",
      "EPOCH: 123 val Results: Prec@1 56.130 Loss: 1.2468\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/390]\tTime 0.011 (0.011)\tLoss 0.9352 (0.9352)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [124][78/390]\tTime 0.003 (0.005)\tLoss 1.0013 (0.9687)\tPrec@1 61.719 (65.813)\n",
      "Epoch: [124][156/390]\tTime 0.002 (0.004)\tLoss 0.9886 (1.0036)\tPrec@1 63.281 (64.316)\n",
      "Epoch: [124][234/390]\tTime 0.003 (0.004)\tLoss 1.0894 (1.0214)\tPrec@1 57.031 (63.723)\n",
      "Epoch: [124][312/390]\tTime 0.003 (0.004)\tLoss 1.1819 (1.0314)\tPrec@1 61.719 (63.284)\n",
      "Epoch: [124][390/390]\tTime 0.003 (0.004)\tLoss 1.1828 (1.0461)\tPrec@1 57.500 (62.700)\n",
      "EPOCH: 124 train Results: Prec@1 62.700 Loss: 1.0461\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1242 (1.1242)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5048 (1.2539)\tPrec@1 43.750 (55.380)\n",
      "EPOCH: 124 val Results: Prec@1 55.380 Loss: 1.2539\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/390]\tTime 0.003 (0.003)\tLoss 0.9336 (0.9336)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [125][78/390]\tTime 0.003 (0.004)\tLoss 0.8606 (0.9742)\tPrec@1 71.875 (65.566)\n",
      "Epoch: [125][156/390]\tTime 0.006 (0.004)\tLoss 0.9661 (0.9989)\tPrec@1 65.625 (64.704)\n",
      "Epoch: [125][234/390]\tTime 0.018 (0.004)\tLoss 1.0395 (1.0173)\tPrec@1 60.938 (63.949)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.004)\tLoss 1.1245 (1.0322)\tPrec@1 63.281 (63.356)\n",
      "Epoch: [125][390/390]\tTime 0.003 (0.004)\tLoss 0.8532 (1.0448)\tPrec@1 75.000 (62.880)\n",
      "EPOCH: 125 train Results: Prec@1 62.880 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0923 (1.0923)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1707 (1.2563)\tPrec@1 43.750 (55.810)\n",
      "EPOCH: 125 val Results: Prec@1 55.810 Loss: 1.2563\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/390]\tTime 0.003 (0.003)\tLoss 0.8251 (0.8251)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [126][78/390]\tTime 0.002 (0.003)\tLoss 0.8869 (0.9596)\tPrec@1 69.531 (65.843)\n",
      "Epoch: [126][156/390]\tTime 0.003 (0.003)\tLoss 1.1164 (0.9909)\tPrec@1 64.062 (64.819)\n",
      "Epoch: [126][234/390]\tTime 0.002 (0.003)\tLoss 1.1743 (1.0155)\tPrec@1 60.156 (63.900)\n",
      "Epoch: [126][312/390]\tTime 0.002 (0.003)\tLoss 1.0575 (1.0336)\tPrec@1 62.500 (63.201)\n",
      "Epoch: [126][390/390]\tTime 0.024 (0.003)\tLoss 1.0516 (1.0475)\tPrec@1 63.750 (62.786)\n",
      "EPOCH: 126 train Results: Prec@1 62.786 Loss: 1.0475\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1671 (1.1671)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3018 (1.2702)\tPrec@1 31.250 (55.350)\n",
      "EPOCH: 126 val Results: Prec@1 55.350 Loss: 1.2702\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/390]\tTime 0.003 (0.003)\tLoss 1.1470 (1.1470)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [127][78/390]\tTime 0.003 (0.003)\tLoss 0.8391 (0.9719)\tPrec@1 71.875 (65.773)\n",
      "Epoch: [127][156/390]\tTime 0.003 (0.003)\tLoss 0.9970 (0.9967)\tPrec@1 64.844 (64.794)\n",
      "Epoch: [127][234/390]\tTime 0.008 (0.003)\tLoss 1.3657 (1.0135)\tPrec@1 52.344 (64.345)\n",
      "Epoch: [127][312/390]\tTime 0.003 (0.004)\tLoss 1.2052 (1.0350)\tPrec@1 56.250 (63.354)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.004)\tLoss 0.9823 (1.0467)\tPrec@1 67.500 (62.916)\n",
      "EPOCH: 127 train Results: Prec@1 62.916 Loss: 1.0467\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1420 (1.1420)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3432 (1.2447)\tPrec@1 37.500 (56.460)\n",
      "EPOCH: 127 val Results: Prec@1 56.460 Loss: 1.2447\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/390]\tTime 0.003 (0.003)\tLoss 0.9997 (0.9997)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [128][78/390]\tTime 0.002 (0.004)\tLoss 1.1222 (0.9664)\tPrec@1 63.281 (65.437)\n",
      "Epoch: [128][156/390]\tTime 0.016 (0.004)\tLoss 0.9906 (0.9957)\tPrec@1 63.281 (64.476)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.004)\tLoss 1.0852 (1.0107)\tPrec@1 67.188 (64.033)\n",
      "Epoch: [128][312/390]\tTime 0.002 (0.004)\tLoss 1.0654 (1.0281)\tPrec@1 62.500 (63.406)\n",
      "Epoch: [128][390/390]\tTime 0.002 (0.004)\tLoss 1.2253 (1.0422)\tPrec@1 61.250 (62.930)\n",
      "EPOCH: 128 train Results: Prec@1 62.930 Loss: 1.0422\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0613 (1.0613)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1367 (1.2634)\tPrec@1 50.000 (55.410)\n",
      "EPOCH: 128 val Results: Prec@1 55.410 Loss: 1.2634\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/390]\tTime 0.002 (0.002)\tLoss 1.0806 (1.0806)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 0.8898 (0.9509)\tPrec@1 71.094 (66.367)\n",
      "Epoch: [129][156/390]\tTime 0.006 (0.003)\tLoss 0.9814 (0.9820)\tPrec@1 62.500 (65.043)\n",
      "Epoch: [129][234/390]\tTime 0.002 (0.003)\tLoss 1.2676 (1.0111)\tPrec@1 52.344 (64.036)\n",
      "Epoch: [129][312/390]\tTime 0.002 (0.003)\tLoss 0.9256 (1.0284)\tPrec@1 62.500 (63.256)\n",
      "Epoch: [129][390/390]\tTime 0.009 (0.003)\tLoss 0.9818 (1.0432)\tPrec@1 61.250 (62.686)\n",
      "EPOCH: 129 train Results: Prec@1 62.686 Loss: 1.0432\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1689 (1.1689)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2535 (1.2553)\tPrec@1 50.000 (55.600)\n",
      "EPOCH: 129 val Results: Prec@1 55.600 Loss: 1.2553\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/390]\tTime 0.003 (0.003)\tLoss 0.8594 (0.8594)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [130][78/390]\tTime 0.002 (0.004)\tLoss 0.9041 (0.9534)\tPrec@1 64.844 (66.297)\n",
      "Epoch: [130][156/390]\tTime 0.003 (0.003)\tLoss 1.1269 (0.9805)\tPrec@1 61.719 (65.351)\n",
      "Epoch: [130][234/390]\tTime 0.002 (0.003)\tLoss 1.2835 (1.0102)\tPrec@1 59.375 (64.279)\n",
      "Epoch: [130][312/390]\tTime 0.002 (0.003)\tLoss 1.0876 (1.0296)\tPrec@1 59.375 (63.491)\n",
      "Epoch: [130][390/390]\tTime 0.003 (0.003)\tLoss 1.1877 (1.0431)\tPrec@1 58.750 (63.012)\n",
      "EPOCH: 130 train Results: Prec@1 63.012 Loss: 1.0431\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.1415 (1.1415)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2079 (1.2564)\tPrec@1 50.000 (55.260)\n",
      "EPOCH: 130 val Results: Prec@1 55.260 Loss: 1.2564\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/390]\tTime 0.005 (0.005)\tLoss 0.8249 (0.8249)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [131][78/390]\tTime 0.002 (0.003)\tLoss 0.9226 (0.9712)\tPrec@1 69.531 (65.625)\n",
      "Epoch: [131][156/390]\tTime 0.002 (0.003)\tLoss 1.1400 (0.9979)\tPrec@1 55.469 (64.346)\n",
      "Epoch: [131][234/390]\tTime 0.002 (0.003)\tLoss 1.1151 (1.0174)\tPrec@1 59.375 (63.590)\n",
      "Epoch: [131][312/390]\tTime 0.002 (0.003)\tLoss 1.2877 (1.0350)\tPrec@1 53.906 (62.919)\n",
      "Epoch: [131][390/390]\tTime 0.004 (0.003)\tLoss 1.0121 (1.0483)\tPrec@1 58.750 (62.502)\n",
      "EPOCH: 131 train Results: Prec@1 62.502 Loss: 1.0483\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0985 (1.0985)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3842 (1.2578)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 131 val Results: Prec@1 55.630 Loss: 1.2578\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/390]\tTime 0.005 (0.005)\tLoss 0.9006 (0.9006)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [132][78/390]\tTime 0.002 (0.003)\tLoss 0.9782 (0.9633)\tPrec@1 70.312 (65.892)\n",
      "Epoch: [132][156/390]\tTime 0.002 (0.003)\tLoss 1.1256 (1.0025)\tPrec@1 64.844 (64.401)\n",
      "Epoch: [132][234/390]\tTime 0.002 (0.003)\tLoss 0.9438 (1.0249)\tPrec@1 68.750 (63.657)\n",
      "Epoch: [132][312/390]\tTime 0.002 (0.003)\tLoss 1.3169 (1.0375)\tPrec@1 53.906 (63.144)\n",
      "Epoch: [132][390/390]\tTime 0.002 (0.003)\tLoss 1.1137 (1.0466)\tPrec@1 66.250 (62.684)\n",
      "EPOCH: 132 train Results: Prec@1 62.684 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0834 (1.0834)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0564 (1.2543)\tPrec@1 37.500 (55.450)\n",
      "EPOCH: 132 val Results: Prec@1 55.450 Loss: 1.2543\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/390]\tTime 0.006 (0.006)\tLoss 0.8773 (0.8773)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [133][78/390]\tTime 0.002 (0.004)\tLoss 0.8910 (0.9653)\tPrec@1 62.500 (66.070)\n",
      "Epoch: [133][156/390]\tTime 0.002 (0.004)\tLoss 0.9027 (0.9879)\tPrec@1 67.969 (65.013)\n",
      "Epoch: [133][234/390]\tTime 0.002 (0.003)\tLoss 1.0643 (1.0107)\tPrec@1 60.938 (64.129)\n",
      "Epoch: [133][312/390]\tTime 0.003 (0.003)\tLoss 1.2222 (1.0328)\tPrec@1 61.719 (63.344)\n",
      "Epoch: [133][390/390]\tTime 0.001 (0.003)\tLoss 1.0638 (1.0448)\tPrec@1 60.000 (62.896)\n",
      "EPOCH: 133 train Results: Prec@1 62.896 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1949 (1.1949)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3216 (1.2540)\tPrec@1 43.750 (55.380)\n",
      "EPOCH: 133 val Results: Prec@1 55.380 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/390]\tTime 0.004 (0.004)\tLoss 0.9230 (0.9230)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [134][78/390]\tTime 0.002 (0.003)\tLoss 0.9859 (0.9703)\tPrec@1 67.969 (65.724)\n",
      "Epoch: [134][156/390]\tTime 0.002 (0.003)\tLoss 1.0846 (1.0032)\tPrec@1 64.844 (64.466)\n",
      "Epoch: [134][234/390]\tTime 0.004 (0.003)\tLoss 1.1020 (1.0200)\tPrec@1 62.500 (63.747)\n",
      "Epoch: [134][312/390]\tTime 0.002 (0.003)\tLoss 1.1761 (1.0334)\tPrec@1 55.469 (63.181)\n",
      "Epoch: [134][390/390]\tTime 0.001 (0.003)\tLoss 0.9589 (1.0449)\tPrec@1 62.500 (62.738)\n",
      "EPOCH: 134 train Results: Prec@1 62.738 Loss: 1.0449\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1199 (1.1199)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1088 (1.2456)\tPrec@1 43.750 (56.060)\n",
      "EPOCH: 134 val Results: Prec@1 56.060 Loss: 1.2456\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/390]\tTime 0.002 (0.002)\tLoss 0.9780 (0.9780)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [135][78/390]\tTime 0.002 (0.004)\tLoss 0.9668 (0.9591)\tPrec@1 60.938 (66.169)\n",
      "Epoch: [135][156/390]\tTime 0.002 (0.004)\tLoss 0.9644 (0.9900)\tPrec@1 68.750 (64.918)\n",
      "Epoch: [135][234/390]\tTime 0.002 (0.004)\tLoss 1.1231 (1.0134)\tPrec@1 60.938 (63.986)\n",
      "Epoch: [135][312/390]\tTime 0.002 (0.004)\tLoss 0.9859 (1.0338)\tPrec@1 65.625 (63.244)\n",
      "Epoch: [135][390/390]\tTime 0.004 (0.004)\tLoss 1.2863 (1.0495)\tPrec@1 56.250 (62.644)\n",
      "EPOCH: 135 train Results: Prec@1 62.644 Loss: 1.0495\n",
      "Test: [0/78]\tTime 0.020 (0.020)\tLoss 1.1453 (1.1453)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2387 (1.2515)\tPrec@1 56.250 (55.600)\n",
      "EPOCH: 135 val Results: Prec@1 55.600 Loss: 1.2515\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/390]\tTime 0.003 (0.003)\tLoss 0.8147 (0.8147)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [136][78/390]\tTime 0.002 (0.003)\tLoss 1.1504 (0.9609)\tPrec@1 58.594 (66.090)\n",
      "Epoch: [136][156/390]\tTime 0.004 (0.004)\tLoss 0.9398 (0.9911)\tPrec@1 65.625 (64.998)\n",
      "Epoch: [136][234/390]\tTime 0.002 (0.004)\tLoss 0.9710 (1.0157)\tPrec@1 68.750 (64.059)\n",
      "Epoch: [136][312/390]\tTime 0.006 (0.003)\tLoss 1.0954 (1.0301)\tPrec@1 57.031 (63.451)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.1552 (1.0433)\tPrec@1 52.500 (62.794)\n",
      "EPOCH: 136 train Results: Prec@1 62.794 Loss: 1.0433\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1034 (1.1034)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3191 (1.2559)\tPrec@1 50.000 (55.370)\n",
      "EPOCH: 136 val Results: Prec@1 55.370 Loss: 1.2559\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/390]\tTime 0.004 (0.004)\tLoss 0.9621 (0.9621)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [137][78/390]\tTime 0.002 (0.003)\tLoss 1.0570 (0.9898)\tPrec@1 55.469 (65.309)\n",
      "Epoch: [137][156/390]\tTime 0.002 (0.003)\tLoss 0.9577 (1.0047)\tPrec@1 72.656 (64.426)\n",
      "Epoch: [137][234/390]\tTime 0.002 (0.003)\tLoss 0.9403 (1.0140)\tPrec@1 64.062 (63.890)\n",
      "Epoch: [137][312/390]\tTime 0.003 (0.003)\tLoss 1.1225 (1.0299)\tPrec@1 60.938 (63.326)\n",
      "Epoch: [137][390/390]\tTime 0.001 (0.003)\tLoss 1.2078 (1.0455)\tPrec@1 55.000 (62.838)\n",
      "EPOCH: 137 train Results: Prec@1 62.838 Loss: 1.0455\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0365 (1.0365)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1414 (1.2563)\tPrec@1 50.000 (55.760)\n",
      "EPOCH: 137 val Results: Prec@1 55.760 Loss: 1.2563\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/390]\tTime 0.004 (0.004)\tLoss 1.0435 (1.0435)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [138][78/390]\tTime 0.002 (0.003)\tLoss 0.9471 (0.9741)\tPrec@1 65.625 (65.417)\n",
      "Epoch: [138][156/390]\tTime 0.006 (0.003)\tLoss 0.9413 (1.0032)\tPrec@1 67.969 (64.276)\n",
      "Epoch: [138][234/390]\tTime 0.002 (0.003)\tLoss 1.1344 (1.0196)\tPrec@1 60.938 (63.896)\n",
      "Epoch: [138][312/390]\tTime 0.002 (0.003)\tLoss 1.1732 (1.0360)\tPrec@1 57.812 (63.176)\n",
      "Epoch: [138][390/390]\tTime 0.004 (0.003)\tLoss 1.0621 (1.0490)\tPrec@1 60.000 (62.674)\n",
      "EPOCH: 138 train Results: Prec@1 62.674 Loss: 1.0490\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0948 (1.0948)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2067 (1.2515)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 138 val Results: Prec@1 55.370 Loss: 1.2515\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/390]\tTime 0.003 (0.003)\tLoss 0.9109 (0.9109)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [139][78/390]\tTime 0.002 (0.003)\tLoss 0.9646 (0.9668)\tPrec@1 67.969 (65.773)\n",
      "Epoch: [139][156/390]\tTime 0.004 (0.004)\tLoss 0.9542 (0.9938)\tPrec@1 67.969 (65.117)\n",
      "Epoch: [139][234/390]\tTime 0.002 (0.004)\tLoss 1.1142 (1.0165)\tPrec@1 65.625 (64.249)\n",
      "Epoch: [139][312/390]\tTime 0.004 (0.004)\tLoss 1.0781 (1.0313)\tPrec@1 60.156 (63.633)\n",
      "Epoch: [139][390/390]\tTime 0.002 (0.004)\tLoss 1.1061 (1.0440)\tPrec@1 67.500 (63.142)\n",
      "EPOCH: 139 train Results: Prec@1 63.142 Loss: 1.0440\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1426 (1.1426)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1866 (1.2560)\tPrec@1 50.000 (55.410)\n",
      "EPOCH: 139 val Results: Prec@1 55.410 Loss: 1.2560\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/390]\tTime 0.003 (0.003)\tLoss 0.9686 (0.9686)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [140][78/390]\tTime 0.002 (0.003)\tLoss 1.0099 (0.9689)\tPrec@1 57.812 (65.892)\n",
      "Epoch: [140][156/390]\tTime 0.002 (0.003)\tLoss 1.0686 (0.9939)\tPrec@1 58.594 (64.849)\n",
      "Epoch: [140][234/390]\tTime 0.002 (0.003)\tLoss 1.0380 (1.0122)\tPrec@1 63.281 (64.166)\n",
      "Epoch: [140][312/390]\tTime 0.002 (0.003)\tLoss 1.1139 (1.0251)\tPrec@1 58.594 (63.628)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.003)\tLoss 1.2424 (1.0417)\tPrec@1 55.000 (62.974)\n",
      "EPOCH: 140 train Results: Prec@1 62.974 Loss: 1.0417\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1572 (1.1572)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4207 (1.2619)\tPrec@1 37.500 (55.010)\n",
      "EPOCH: 140 val Results: Prec@1 55.010 Loss: 1.2619\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/390]\tTime 0.011 (0.011)\tLoss 0.7216 (0.7216)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [141][78/390]\tTime 0.002 (0.003)\tLoss 1.0543 (0.9561)\tPrec@1 66.406 (66.070)\n",
      "Epoch: [141][156/390]\tTime 0.007 (0.003)\tLoss 1.1572 (1.0004)\tPrec@1 54.688 (64.381)\n",
      "Epoch: [141][234/390]\tTime 0.002 (0.003)\tLoss 1.0562 (1.0214)\tPrec@1 62.500 (63.561)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.003)\tLoss 1.1364 (1.0376)\tPrec@1 64.062 (62.897)\n",
      "Epoch: [141][390/390]\tTime 0.003 (0.003)\tLoss 1.1157 (1.0468)\tPrec@1 60.000 (62.634)\n",
      "EPOCH: 141 train Results: Prec@1 62.634 Loss: 1.0468\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1252 (1.1252)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3861 (1.2561)\tPrec@1 56.250 (55.230)\n",
      "EPOCH: 141 val Results: Prec@1 55.230 Loss: 1.2561\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/390]\tTime 0.005 (0.005)\tLoss 0.9118 (0.9118)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [142][78/390]\tTime 0.007 (0.004)\tLoss 0.9621 (0.9564)\tPrec@1 64.844 (66.466)\n",
      "Epoch: [142][156/390]\tTime 0.002 (0.004)\tLoss 0.9968 (0.9877)\tPrec@1 60.938 (64.689)\n",
      "Epoch: [142][234/390]\tTime 0.008 (0.004)\tLoss 1.1511 (1.0084)\tPrec@1 63.281 (63.966)\n",
      "Epoch: [142][312/390]\tTime 0.005 (0.004)\tLoss 1.0986 (1.0274)\tPrec@1 62.500 (63.431)\n",
      "Epoch: [142][390/390]\tTime 0.001 (0.004)\tLoss 1.2483 (1.0449)\tPrec@1 52.500 (62.702)\n",
      "EPOCH: 142 train Results: Prec@1 62.702 Loss: 1.0449\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1646 (1.1646)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2183 (1.2665)\tPrec@1 56.250 (55.030)\n",
      "EPOCH: 142 val Results: Prec@1 55.030 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/390]\tTime 0.005 (0.005)\tLoss 0.9452 (0.9452)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [143][78/390]\tTime 0.002 (0.003)\tLoss 0.9839 (0.9681)\tPrec@1 72.656 (65.763)\n",
      "Epoch: [143][156/390]\tTime 0.004 (0.004)\tLoss 0.9165 (0.9951)\tPrec@1 65.625 (64.675)\n",
      "Epoch: [143][234/390]\tTime 0.004 (0.004)\tLoss 1.0934 (1.0143)\tPrec@1 65.625 (63.893)\n",
      "Epoch: [143][312/390]\tTime 0.002 (0.004)\tLoss 1.1003 (1.0295)\tPrec@1 59.375 (63.339)\n",
      "Epoch: [143][390/390]\tTime 0.003 (0.004)\tLoss 0.9992 (1.0423)\tPrec@1 63.750 (62.804)\n",
      "EPOCH: 143 train Results: Prec@1 62.804 Loss: 1.0423\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1058 (1.1058)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0594 (1.2537)\tPrec@1 62.500 (55.590)\n",
      "EPOCH: 143 val Results: Prec@1 55.590 Loss: 1.2537\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/390]\tTime 0.002 (0.002)\tLoss 0.8698 (0.8698)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [144][78/390]\tTime 0.004 (0.004)\tLoss 1.0005 (0.9737)\tPrec@1 64.062 (65.546)\n",
      "Epoch: [144][156/390]\tTime 0.002 (0.004)\tLoss 1.2666 (0.9986)\tPrec@1 58.594 (64.500)\n",
      "Epoch: [144][234/390]\tTime 0.002 (0.004)\tLoss 1.1271 (1.0187)\tPrec@1 60.938 (63.906)\n",
      "Epoch: [144][312/390]\tTime 0.002 (0.003)\tLoss 1.0188 (1.0289)\tPrec@1 60.938 (63.496)\n",
      "Epoch: [144][390/390]\tTime 0.002 (0.003)\tLoss 0.9962 (1.0391)\tPrec@1 60.000 (63.096)\n",
      "EPOCH: 144 train Results: Prec@1 63.096 Loss: 1.0391\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0925 (1.0925)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3413 (1.2509)\tPrec@1 37.500 (56.040)\n",
      "EPOCH: 144 val Results: Prec@1 56.040 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/390]\tTime 0.003 (0.003)\tLoss 0.9920 (0.9920)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [145][78/390]\tTime 0.005 (0.004)\tLoss 0.9096 (0.9706)\tPrec@1 69.531 (65.358)\n",
      "Epoch: [145][156/390]\tTime 0.002 (0.004)\tLoss 1.0535 (0.9841)\tPrec@1 64.062 (65.068)\n",
      "Epoch: [145][234/390]\tTime 0.029 (0.004)\tLoss 0.9857 (1.0115)\tPrec@1 66.406 (64.029)\n",
      "Epoch: [145][312/390]\tTime 0.002 (0.004)\tLoss 1.2132 (1.0310)\tPrec@1 60.938 (63.249)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.004)\tLoss 1.1423 (1.0425)\tPrec@1 57.500 (62.912)\n",
      "EPOCH: 145 train Results: Prec@1 62.912 Loss: 1.0425\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0954 (1.0954)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3588 (1.2584)\tPrec@1 43.750 (55.740)\n",
      "EPOCH: 145 val Results: Prec@1 55.740 Loss: 1.2584\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/390]\tTime 0.005 (0.005)\tLoss 1.0094 (1.0094)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [146][78/390]\tTime 0.003 (0.004)\tLoss 0.9580 (0.9682)\tPrec@1 65.625 (65.180)\n",
      "Epoch: [146][156/390]\tTime 0.002 (0.004)\tLoss 1.0319 (0.9979)\tPrec@1 62.500 (64.167)\n",
      "Epoch: [146][234/390]\tTime 0.002 (0.003)\tLoss 0.8903 (1.0146)\tPrec@1 71.094 (63.630)\n",
      "Epoch: [146][312/390]\tTime 0.002 (0.003)\tLoss 0.9520 (1.0332)\tPrec@1 68.750 (63.109)\n",
      "Epoch: [146][390/390]\tTime 0.002 (0.003)\tLoss 0.9097 (1.0450)\tPrec@1 66.250 (62.646)\n",
      "EPOCH: 146 train Results: Prec@1 62.646 Loss: 1.0450\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.0285 (1.0285)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1653 (1.2540)\tPrec@1 50.000 (55.100)\n",
      "EPOCH: 146 val Results: Prec@1 55.100 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/390]\tTime 0.005 (0.005)\tLoss 0.8659 (0.8659)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [147][78/390]\tTime 0.003 (0.004)\tLoss 0.9897 (0.9682)\tPrec@1 64.062 (65.318)\n",
      "Epoch: [147][156/390]\tTime 0.002 (0.004)\tLoss 1.1112 (0.9906)\tPrec@1 62.500 (64.446)\n",
      "Epoch: [147][234/390]\tTime 0.004 (0.003)\tLoss 1.2271 (1.0086)\tPrec@1 57.031 (63.933)\n",
      "Epoch: [147][312/390]\tTime 0.002 (0.003)\tLoss 1.1038 (1.0258)\tPrec@1 62.500 (63.286)\n",
      "Epoch: [147][390/390]\tTime 0.002 (0.003)\tLoss 1.0875 (1.0383)\tPrec@1 60.000 (62.922)\n",
      "EPOCH: 147 train Results: Prec@1 62.922 Loss: 1.0383\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1317 (1.1317)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6665 (1.2587)\tPrec@1 25.000 (55.460)\n",
      "EPOCH: 147 val Results: Prec@1 55.460 Loss: 1.2587\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/390]\tTime 0.004 (0.004)\tLoss 1.0457 (1.0457)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [148][78/390]\tTime 0.002 (0.003)\tLoss 1.0713 (0.9785)\tPrec@1 61.719 (65.239)\n",
      "Epoch: [148][156/390]\tTime 0.003 (0.003)\tLoss 1.1139 (1.0046)\tPrec@1 62.500 (64.202)\n",
      "Epoch: [148][234/390]\tTime 0.004 (0.003)\tLoss 1.0792 (1.0243)\tPrec@1 59.375 (63.524)\n",
      "Epoch: [148][312/390]\tTime 0.002 (0.003)\tLoss 0.9690 (1.0360)\tPrec@1 67.188 (63.194)\n",
      "Epoch: [148][390/390]\tTime 0.001 (0.004)\tLoss 0.9833 (1.0474)\tPrec@1 62.500 (62.804)\n",
      "EPOCH: 148 train Results: Prec@1 62.804 Loss: 1.0474\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1394 (1.1394)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5144 (1.2580)\tPrec@1 37.500 (55.710)\n",
      "EPOCH: 148 val Results: Prec@1 55.710 Loss: 1.2580\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/390]\tTime 0.003 (0.003)\tLoss 0.8219 (0.8219)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [149][78/390]\tTime 0.004 (0.003)\tLoss 1.0457 (0.9596)\tPrec@1 61.719 (66.199)\n",
      "Epoch: [149][156/390]\tTime 0.005 (0.003)\tLoss 1.0027 (0.9873)\tPrec@1 63.281 (65.098)\n",
      "Epoch: [149][234/390]\tTime 0.005 (0.003)\tLoss 0.9663 (1.0108)\tPrec@1 64.062 (64.295)\n",
      "Epoch: [149][312/390]\tTime 0.002 (0.003)\tLoss 1.1420 (1.0235)\tPrec@1 57.031 (63.820)\n",
      "Epoch: [149][390/390]\tTime 0.002 (0.003)\tLoss 1.0933 (1.0401)\tPrec@1 60.000 (63.184)\n",
      "EPOCH: 149 train Results: Prec@1 63.184 Loss: 1.0401\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0873 (1.0873)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3228 (1.2584)\tPrec@1 50.000 (55.700)\n",
      "EPOCH: 149 val Results: Prec@1 55.700 Loss: 1.2584\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/390]\tTime 0.004 (0.004)\tLoss 0.9870 (0.9870)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [150][78/390]\tTime 0.002 (0.003)\tLoss 1.1757 (0.9565)\tPrec@1 60.938 (66.317)\n",
      "Epoch: [150][156/390]\tTime 0.002 (0.003)\tLoss 1.0894 (0.9821)\tPrec@1 63.281 (65.272)\n",
      "Epoch: [150][234/390]\tTime 0.007 (0.003)\tLoss 1.2217 (1.0191)\tPrec@1 63.281 (63.677)\n",
      "Epoch: [150][312/390]\tTime 0.002 (0.003)\tLoss 1.0654 (1.0349)\tPrec@1 59.375 (63.042)\n",
      "Epoch: [150][390/390]\tTime 0.001 (0.003)\tLoss 1.0127 (1.0466)\tPrec@1 66.250 (62.584)\n",
      "EPOCH: 150 train Results: Prec@1 62.584 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1446 (1.1446)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1663 (1.2527)\tPrec@1 50.000 (55.580)\n",
      "EPOCH: 150 val Results: Prec@1 55.580 Loss: 1.2527\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/390]\tTime 0.002 (0.002)\tLoss 0.9232 (0.9232)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [151][78/390]\tTime 0.002 (0.003)\tLoss 1.1241 (0.9616)\tPrec@1 60.938 (65.823)\n",
      "Epoch: [151][156/390]\tTime 0.003 (0.003)\tLoss 1.0124 (0.9863)\tPrec@1 63.281 (64.809)\n",
      "Epoch: [151][234/390]\tTime 0.004 (0.004)\tLoss 1.1055 (1.0094)\tPrec@1 60.938 (63.979)\n",
      "Epoch: [151][312/390]\tTime 0.002 (0.004)\tLoss 1.1386 (1.0230)\tPrec@1 59.375 (63.366)\n",
      "Epoch: [151][390/390]\tTime 0.002 (0.004)\tLoss 1.1851 (1.0416)\tPrec@1 58.750 (62.646)\n",
      "EPOCH: 151 train Results: Prec@1 62.646 Loss: 1.0416\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1213 (1.1213)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1523 (1.2588)\tPrec@1 56.250 (55.730)\n",
      "EPOCH: 151 val Results: Prec@1 55.730 Loss: 1.2588\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/390]\tTime 0.002 (0.002)\tLoss 0.7736 (0.7736)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [152][78/390]\tTime 0.002 (0.003)\tLoss 1.0548 (0.9701)\tPrec@1 60.156 (65.882)\n",
      "Epoch: [152][156/390]\tTime 0.002 (0.003)\tLoss 1.0168 (0.9966)\tPrec@1 60.156 (64.759)\n",
      "Epoch: [152][234/390]\tTime 0.002 (0.003)\tLoss 1.1780 (1.0149)\tPrec@1 57.812 (63.953)\n",
      "Epoch: [152][312/390]\tTime 0.002 (0.003)\tLoss 1.0821 (1.0279)\tPrec@1 59.375 (63.531)\n",
      "Epoch: [152][390/390]\tTime 0.016 (0.003)\tLoss 0.9581 (1.0403)\tPrec@1 62.500 (63.074)\n",
      "EPOCH: 152 train Results: Prec@1 63.074 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1255 (1.1255)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3912 (1.2665)\tPrec@1 31.250 (54.960)\n",
      "EPOCH: 152 val Results: Prec@1 54.960 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/390]\tTime 0.007 (0.007)\tLoss 0.7744 (0.7744)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [153][78/390]\tTime 0.002 (0.003)\tLoss 1.0226 (0.9600)\tPrec@1 61.719 (65.684)\n",
      "Epoch: [153][156/390]\tTime 0.002 (0.004)\tLoss 0.9712 (0.9910)\tPrec@1 67.969 (64.515)\n",
      "Epoch: [153][234/390]\tTime 0.003 (0.004)\tLoss 1.0107 (1.0126)\tPrec@1 64.844 (63.850)\n",
      "Epoch: [153][312/390]\tTime 0.002 (0.003)\tLoss 1.0050 (1.0290)\tPrec@1 62.500 (63.314)\n",
      "Epoch: [153][390/390]\tTime 0.001 (0.003)\tLoss 1.1472 (1.0391)\tPrec@1 61.250 (62.974)\n",
      "EPOCH: 153 train Results: Prec@1 62.974 Loss: 1.0391\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1421 (1.1421)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4642 (1.2509)\tPrec@1 31.250 (55.630)\n",
      "EPOCH: 153 val Results: Prec@1 55.630 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/390]\tTime 0.003 (0.003)\tLoss 0.8329 (0.8329)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [154][78/390]\tTime 0.005 (0.003)\tLoss 0.8981 (0.9786)\tPrec@1 67.188 (64.972)\n",
      "Epoch: [154][156/390]\tTime 0.006 (0.003)\tLoss 1.1282 (0.9967)\tPrec@1 57.812 (64.276)\n",
      "Epoch: [154][234/390]\tTime 0.005 (0.003)\tLoss 1.2091 (1.0173)\tPrec@1 57.031 (63.577)\n",
      "Epoch: [154][312/390]\tTime 0.002 (0.004)\tLoss 1.1627 (1.0389)\tPrec@1 51.562 (62.869)\n",
      "Epoch: [154][390/390]\tTime 0.001 (0.004)\tLoss 1.1018 (1.0486)\tPrec@1 60.000 (62.574)\n",
      "EPOCH: 154 train Results: Prec@1 62.574 Loss: 1.0486\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2229 (1.2229)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5655 (1.2557)\tPrec@1 37.500 (55.640)\n",
      "EPOCH: 154 val Results: Prec@1 55.640 Loss: 1.2557\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/390]\tTime 0.005 (0.005)\tLoss 0.9959 (0.9959)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [155][78/390]\tTime 0.003 (0.003)\tLoss 0.9013 (0.9648)\tPrec@1 66.406 (65.605)\n",
      "Epoch: [155][156/390]\tTime 0.002 (0.003)\tLoss 0.9608 (0.9879)\tPrec@1 66.406 (64.615)\n",
      "Epoch: [155][234/390]\tTime 0.019 (0.003)\tLoss 1.1423 (1.0052)\tPrec@1 57.812 (64.059)\n",
      "Epoch: [155][312/390]\tTime 0.002 (0.003)\tLoss 1.2328 (1.0259)\tPrec@1 55.469 (63.386)\n",
      "Epoch: [155][390/390]\tTime 0.002 (0.003)\tLoss 0.9871 (1.0434)\tPrec@1 66.250 (62.756)\n",
      "EPOCH: 155 train Results: Prec@1 62.756 Loss: 1.0434\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1525 (1.1525)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3506 (1.2752)\tPrec@1 37.500 (54.860)\n",
      "EPOCH: 155 val Results: Prec@1 54.860 Loss: 1.2752\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/390]\tTime 0.003 (0.003)\tLoss 0.9833 (0.9833)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [156][78/390]\tTime 0.002 (0.004)\tLoss 0.9597 (0.9653)\tPrec@1 65.625 (64.745)\n",
      "Epoch: [156][156/390]\tTime 0.002 (0.004)\tLoss 1.2532 (0.9929)\tPrec@1 57.812 (64.117)\n",
      "Epoch: [156][234/390]\tTime 0.003 (0.004)\tLoss 1.0005 (1.0143)\tPrec@1 64.844 (63.418)\n",
      "Epoch: [156][312/390]\tTime 0.004 (0.004)\tLoss 1.1343 (1.0348)\tPrec@1 60.156 (62.725)\n",
      "Epoch: [156][390/390]\tTime 0.002 (0.004)\tLoss 1.0664 (1.0467)\tPrec@1 58.750 (62.420)\n",
      "EPOCH: 156 train Results: Prec@1 62.420 Loss: 1.0467\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1723 (1.1723)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5726 (1.2541)\tPrec@1 50.000 (55.360)\n",
      "EPOCH: 156 val Results: Prec@1 55.360 Loss: 1.2541\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/390]\tTime 0.002 (0.002)\tLoss 1.1631 (1.1631)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [157][78/390]\tTime 0.003 (0.003)\tLoss 0.9838 (0.9671)\tPrec@1 59.375 (65.912)\n",
      "Epoch: [157][156/390]\tTime 0.004 (0.003)\tLoss 0.8294 (0.9881)\tPrec@1 67.969 (64.988)\n",
      "Epoch: [157][234/390]\tTime 0.003 (0.004)\tLoss 1.0183 (1.0123)\tPrec@1 62.500 (63.833)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.003)\tLoss 1.0516 (1.0275)\tPrec@1 64.844 (63.219)\n",
      "Epoch: [157][390/390]\tTime 0.001 (0.003)\tLoss 1.1160 (1.0368)\tPrec@1 60.000 (62.976)\n",
      "EPOCH: 157 train Results: Prec@1 62.976 Loss: 1.0368\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1468 (1.1468)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3970 (1.2591)\tPrec@1 43.750 (55.360)\n",
      "EPOCH: 157 val Results: Prec@1 55.360 Loss: 1.2591\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/390]\tTime 0.003 (0.003)\tLoss 0.9229 (0.9229)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [158][78/390]\tTime 0.002 (0.003)\tLoss 1.0895 (0.9698)\tPrec@1 61.719 (65.328)\n",
      "Epoch: [158][156/390]\tTime 0.002 (0.003)\tLoss 1.1152 (0.9942)\tPrec@1 66.406 (64.655)\n",
      "Epoch: [158][234/390]\tTime 0.003 (0.003)\tLoss 0.9208 (1.0167)\tPrec@1 68.750 (63.820)\n",
      "Epoch: [158][312/390]\tTime 0.002 (0.003)\tLoss 1.1618 (1.0328)\tPrec@1 57.812 (63.166)\n",
      "Epoch: [158][390/390]\tTime 0.003 (0.003)\tLoss 1.1040 (1.0457)\tPrec@1 62.500 (62.790)\n",
      "EPOCH: 158 train Results: Prec@1 62.790 Loss: 1.0457\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0743 (1.0743)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0856 (1.2540)\tPrec@1 68.750 (55.420)\n",
      "EPOCH: 158 val Results: Prec@1 55.420 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/390]\tTime 0.004 (0.004)\tLoss 0.8832 (0.8832)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [159][78/390]\tTime 0.003 (0.003)\tLoss 1.0089 (0.9702)\tPrec@1 61.719 (65.536)\n",
      "Epoch: [159][156/390]\tTime 0.015 (0.003)\tLoss 0.9773 (0.9899)\tPrec@1 67.188 (64.431)\n",
      "Epoch: [159][234/390]\tTime 0.002 (0.003)\tLoss 0.9889 (1.0100)\tPrec@1 69.531 (63.797)\n",
      "Epoch: [159][312/390]\tTime 0.003 (0.003)\tLoss 1.0462 (1.0263)\tPrec@1 61.719 (63.306)\n",
      "Epoch: [159][390/390]\tTime 0.002 (0.003)\tLoss 1.1535 (1.0385)\tPrec@1 62.500 (62.852)\n",
      "EPOCH: 159 train Results: Prec@1 62.852 Loss: 1.0385\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1455 (1.1455)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6612 (1.2542)\tPrec@1 37.500 (55.410)\n",
      "EPOCH: 159 val Results: Prec@1 55.410 Loss: 1.2542\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/390]\tTime 0.003 (0.003)\tLoss 0.8923 (0.8923)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [160][78/390]\tTime 0.002 (0.005)\tLoss 1.0480 (0.9943)\tPrec@1 62.500 (64.814)\n",
      "Epoch: [160][156/390]\tTime 0.004 (0.004)\tLoss 0.9889 (1.0003)\tPrec@1 67.188 (64.585)\n",
      "Epoch: [160][234/390]\tTime 0.002 (0.004)\tLoss 0.9305 (1.0150)\tPrec@1 63.281 (64.023)\n",
      "Epoch: [160][312/390]\tTime 0.002 (0.004)\tLoss 0.9514 (1.0298)\tPrec@1 60.156 (63.548)\n",
      "Epoch: [160][390/390]\tTime 0.007 (0.004)\tLoss 0.9708 (1.0347)\tPrec@1 66.250 (63.348)\n",
      "EPOCH: 160 train Results: Prec@1 63.348 Loss: 1.0347\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0861 (1.0861)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0772 (1.2633)\tPrec@1 56.250 (55.640)\n",
      "EPOCH: 160 val Results: Prec@1 55.640 Loss: 1.2633\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/390]\tTime 0.004 (0.004)\tLoss 0.9541 (0.9541)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [161][78/390]\tTime 0.002 (0.003)\tLoss 0.9255 (0.9601)\tPrec@1 67.969 (65.546)\n",
      "Epoch: [161][156/390]\tTime 0.006 (0.003)\tLoss 1.0636 (0.9898)\tPrec@1 61.719 (64.605)\n",
      "Epoch: [161][234/390]\tTime 0.004 (0.003)\tLoss 1.0757 (1.0124)\tPrec@1 57.812 (63.763)\n",
      "Epoch: [161][312/390]\tTime 0.004 (0.003)\tLoss 1.0445 (1.0275)\tPrec@1 63.281 (63.274)\n",
      "Epoch: [161][390/390]\tTime 0.003 (0.003)\tLoss 1.3551 (1.0414)\tPrec@1 51.250 (62.740)\n",
      "EPOCH: 161 train Results: Prec@1 62.740 Loss: 1.0414\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1011 (1.1011)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9777 (1.2394)\tPrec@1 56.250 (56.190)\n",
      "EPOCH: 161 val Results: Prec@1 56.190 Loss: 1.2394\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.8686 (0.8686)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [162][78/390]\tTime 0.002 (0.003)\tLoss 1.1031 (0.9683)\tPrec@1 64.062 (65.566)\n",
      "Epoch: [162][156/390]\tTime 0.004 (0.003)\tLoss 0.9650 (0.9976)\tPrec@1 64.844 (64.426)\n",
      "Epoch: [162][234/390]\tTime 0.002 (0.003)\tLoss 1.0176 (1.0142)\tPrec@1 61.719 (63.797)\n",
      "Epoch: [162][312/390]\tTime 0.002 (0.004)\tLoss 0.9812 (1.0233)\tPrec@1 68.750 (63.406)\n",
      "Epoch: [162][390/390]\tTime 0.003 (0.004)\tLoss 1.0465 (1.0380)\tPrec@1 61.250 (62.884)\n",
      "EPOCH: 162 train Results: Prec@1 62.884 Loss: 1.0380\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1441 (1.1441)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5393 (1.2560)\tPrec@1 37.500 (55.840)\n",
      "EPOCH: 162 val Results: Prec@1 55.840 Loss: 1.2560\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/390]\tTime 0.003 (0.003)\tLoss 0.9694 (0.9694)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [163][78/390]\tTime 0.002 (0.003)\tLoss 0.9485 (0.9602)\tPrec@1 60.938 (65.991)\n",
      "Epoch: [163][156/390]\tTime 0.002 (0.003)\tLoss 0.9904 (0.9834)\tPrec@1 59.375 (65.028)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.003)\tLoss 0.9693 (1.0145)\tPrec@1 64.062 (63.823)\n",
      "Epoch: [163][312/390]\tTime 0.006 (0.003)\tLoss 1.0528 (1.0293)\tPrec@1 65.625 (63.239)\n",
      "Epoch: [163][390/390]\tTime 0.002 (0.003)\tLoss 1.0161 (1.0421)\tPrec@1 62.500 (62.860)\n",
      "EPOCH: 163 train Results: Prec@1 62.860 Loss: 1.0421\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1744 (1.1744)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9451 (1.2603)\tPrec@1 56.250 (56.150)\n",
      "EPOCH: 163 val Results: Prec@1 56.150 Loss: 1.2603\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/390]\tTime 0.003 (0.003)\tLoss 1.0625 (1.0625)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [164][78/390]\tTime 0.002 (0.003)\tLoss 1.0075 (0.9629)\tPrec@1 65.625 (66.080)\n",
      "Epoch: [164][156/390]\tTime 0.003 (0.003)\tLoss 1.0998 (0.9887)\tPrec@1 58.594 (64.884)\n",
      "Epoch: [164][234/390]\tTime 0.002 (0.003)\tLoss 1.0351 (1.0071)\tPrec@1 61.719 (64.076)\n",
      "Epoch: [164][312/390]\tTime 0.003 (0.003)\tLoss 1.2434 (1.0277)\tPrec@1 54.688 (63.274)\n",
      "Epoch: [164][390/390]\tTime 0.002 (0.003)\tLoss 1.2739 (1.0386)\tPrec@1 58.750 (62.882)\n",
      "EPOCH: 164 train Results: Prec@1 62.882 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1496 (1.1496)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1512 (1.2575)\tPrec@1 50.000 (56.150)\n",
      "EPOCH: 164 val Results: Prec@1 56.150 Loss: 1.2575\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/390]\tTime 0.003 (0.003)\tLoss 1.1056 (1.1056)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [165][78/390]\tTime 0.003 (0.004)\tLoss 0.9257 (0.9413)\tPrec@1 69.531 (66.762)\n",
      "Epoch: [165][156/390]\tTime 0.002 (0.004)\tLoss 1.0825 (0.9834)\tPrec@1 58.594 (65.182)\n",
      "Epoch: [165][234/390]\tTime 0.013 (0.004)\tLoss 1.2299 (1.0073)\tPrec@1 56.250 (64.262)\n",
      "Epoch: [165][312/390]\tTime 0.002 (0.003)\tLoss 0.9964 (1.0268)\tPrec@1 66.406 (63.488)\n",
      "Epoch: [165][390/390]\tTime 0.002 (0.004)\tLoss 1.1832 (1.0409)\tPrec@1 60.000 (62.902)\n",
      "EPOCH: 165 train Results: Prec@1 62.902 Loss: 1.0409\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1954 (1.1954)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1757 (1.2632)\tPrec@1 50.000 (55.270)\n",
      "EPOCH: 165 val Results: Prec@1 55.270 Loss: 1.2632\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/390]\tTime 0.003 (0.003)\tLoss 1.0573 (1.0573)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 0.9817 (0.9603)\tPrec@1 67.188 (65.803)\n",
      "Epoch: [166][156/390]\tTime 0.004 (0.003)\tLoss 1.0635 (0.9875)\tPrec@1 64.844 (64.495)\n",
      "Epoch: [166][234/390]\tTime 0.004 (0.003)\tLoss 1.1614 (1.0132)\tPrec@1 58.594 (63.517)\n",
      "Epoch: [166][312/390]\tTime 0.002 (0.003)\tLoss 1.1649 (1.0281)\tPrec@1 62.500 (63.097)\n",
      "Epoch: [166][390/390]\tTime 0.003 (0.003)\tLoss 1.0196 (1.0415)\tPrec@1 65.000 (62.708)\n",
      "EPOCH: 166 train Results: Prec@1 62.708 Loss: 1.0415\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1410 (1.1410)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1646 (1.2630)\tPrec@1 56.250 (54.990)\n",
      "EPOCH: 166 val Results: Prec@1 54.990 Loss: 1.2630\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/390]\tTime 0.004 (0.004)\tLoss 1.0606 (1.0606)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [167][78/390]\tTime 0.002 (0.004)\tLoss 1.0835 (0.9738)\tPrec@1 61.719 (65.595)\n",
      "Epoch: [167][156/390]\tTime 0.004 (0.004)\tLoss 0.9967 (0.9954)\tPrec@1 72.656 (64.779)\n",
      "Epoch: [167][234/390]\tTime 0.009 (0.004)\tLoss 1.0381 (1.0199)\tPrec@1 63.281 (63.930)\n",
      "Epoch: [167][312/390]\tTime 0.002 (0.005)\tLoss 1.0867 (1.0309)\tPrec@1 57.031 (63.458)\n",
      "Epoch: [167][390/390]\tTime 0.002 (0.005)\tLoss 1.0525 (1.0437)\tPrec@1 66.250 (63.020)\n",
      "EPOCH: 167 train Results: Prec@1 63.020 Loss: 1.0437\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1413 (1.1413)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0818 (1.2648)\tPrec@1 37.500 (54.970)\n",
      "EPOCH: 167 val Results: Prec@1 54.970 Loss: 1.2648\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/390]\tTime 0.002 (0.002)\tLoss 0.9201 (0.9201)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [168][78/390]\tTime 0.002 (0.004)\tLoss 0.9706 (0.9535)\tPrec@1 66.406 (66.307)\n",
      "Epoch: [168][156/390]\tTime 0.003 (0.003)\tLoss 0.9558 (0.9846)\tPrec@1 64.844 (65.043)\n",
      "Epoch: [168][234/390]\tTime 0.002 (0.004)\tLoss 1.0849 (1.0090)\tPrec@1 59.375 (63.993)\n",
      "Epoch: [168][312/390]\tTime 0.004 (0.004)\tLoss 1.0765 (1.0320)\tPrec@1 59.375 (63.181)\n",
      "Epoch: [168][390/390]\tTime 0.001 (0.004)\tLoss 1.1060 (1.0446)\tPrec@1 57.500 (62.666)\n",
      "EPOCH: 168 train Results: Prec@1 62.666 Loss: 1.0446\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0970 (1.0970)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3915 (1.2582)\tPrec@1 37.500 (54.840)\n",
      "EPOCH: 168 val Results: Prec@1 54.840 Loss: 1.2582\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/390]\tTime 0.007 (0.007)\tLoss 0.9263 (0.9263)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [169][78/390]\tTime 0.007 (0.004)\tLoss 1.0027 (0.9684)\tPrec@1 62.500 (65.615)\n",
      "Epoch: [169][156/390]\tTime 0.002 (0.003)\tLoss 0.9978 (0.9858)\tPrec@1 57.812 (64.953)\n",
      "Epoch: [169][234/390]\tTime 0.002 (0.003)\tLoss 0.9767 (1.0105)\tPrec@1 66.406 (63.963)\n",
      "Epoch: [169][312/390]\tTime 0.003 (0.003)\tLoss 1.0819 (1.0301)\tPrec@1 57.031 (63.276)\n",
      "Epoch: [169][390/390]\tTime 0.004 (0.003)\tLoss 0.9615 (1.0426)\tPrec@1 65.000 (62.772)\n",
      "EPOCH: 169 train Results: Prec@1 62.772 Loss: 1.0426\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1625 (1.1625)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1572 (1.2659)\tPrec@1 56.250 (55.320)\n",
      "EPOCH: 169 val Results: Prec@1 55.320 Loss: 1.2659\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/390]\tTime 0.004 (0.004)\tLoss 1.0642 (1.0642)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [170][78/390]\tTime 0.004 (0.003)\tLoss 1.0309 (0.9646)\tPrec@1 60.156 (65.289)\n",
      "Epoch: [170][156/390]\tTime 0.006 (0.003)\tLoss 0.8946 (0.9901)\tPrec@1 64.062 (64.421)\n",
      "Epoch: [170][234/390]\tTime 0.002 (0.003)\tLoss 1.2221 (1.0105)\tPrec@1 61.719 (63.667)\n",
      "Epoch: [170][312/390]\tTime 0.002 (0.003)\tLoss 0.9918 (1.0303)\tPrec@1 63.281 (63.064)\n",
      "Epoch: [170][390/390]\tTime 0.002 (0.004)\tLoss 0.9428 (1.0386)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 170 train Results: Prec@1 62.850 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1521 (1.1521)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0672 (1.2534)\tPrec@1 50.000 (55.990)\n",
      "EPOCH: 170 val Results: Prec@1 55.990 Loss: 1.2534\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/390]\tTime 0.003 (0.003)\tLoss 0.9359 (0.9359)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [171][78/390]\tTime 0.003 (0.003)\tLoss 0.9667 (0.9650)\tPrec@1 61.719 (65.971)\n",
      "Epoch: [171][156/390]\tTime 0.004 (0.003)\tLoss 1.0672 (0.9942)\tPrec@1 63.281 (64.884)\n",
      "Epoch: [171][234/390]\tTime 0.002 (0.003)\tLoss 0.9533 (1.0139)\tPrec@1 64.062 (64.026)\n",
      "Epoch: [171][312/390]\tTime 0.008 (0.003)\tLoss 0.9727 (1.0269)\tPrec@1 68.750 (63.693)\n",
      "Epoch: [171][390/390]\tTime 0.001 (0.003)\tLoss 1.2712 (1.0398)\tPrec@1 57.500 (63.082)\n",
      "EPOCH: 171 train Results: Prec@1 63.082 Loss: 1.0398\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1526 (1.1526)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3498 (1.2671)\tPrec@1 31.250 (55.290)\n",
      "EPOCH: 171 val Results: Prec@1 55.290 Loss: 1.2671\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/390]\tTime 0.005 (0.005)\tLoss 0.8262 (0.8262)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [172][78/390]\tTime 0.003 (0.003)\tLoss 0.9509 (0.9558)\tPrec@1 66.406 (66.258)\n",
      "Epoch: [172][156/390]\tTime 0.002 (0.003)\tLoss 0.9302 (0.9860)\tPrec@1 64.844 (64.953)\n",
      "Epoch: [172][234/390]\tTime 0.002 (0.003)\tLoss 1.0324 (1.0095)\tPrec@1 61.719 (64.102)\n",
      "Epoch: [172][312/390]\tTime 0.003 (0.003)\tLoss 1.2029 (1.0288)\tPrec@1 60.156 (63.274)\n",
      "Epoch: [172][390/390]\tTime 0.002 (0.003)\tLoss 1.1008 (1.0412)\tPrec@1 62.500 (62.804)\n",
      "EPOCH: 172 train Results: Prec@1 62.804 Loss: 1.0412\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1372 (1.1372)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3897 (1.2439)\tPrec@1 37.500 (55.830)\n",
      "EPOCH: 172 val Results: Prec@1 55.830 Loss: 1.2439\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/390]\tTime 0.004 (0.004)\tLoss 1.0715 (1.0715)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [173][78/390]\tTime 0.005 (0.004)\tLoss 1.1467 (0.9815)\tPrec@1 60.156 (65.388)\n",
      "Epoch: [173][156/390]\tTime 0.004 (0.005)\tLoss 1.1837 (1.0014)\tPrec@1 58.594 (64.456)\n",
      "Epoch: [173][234/390]\tTime 0.006 (0.004)\tLoss 1.0020 (1.0120)\tPrec@1 64.844 (64.116)\n",
      "Epoch: [173][312/390]\tTime 0.003 (0.004)\tLoss 1.0244 (1.0303)\tPrec@1 60.938 (63.304)\n",
      "Epoch: [173][390/390]\tTime 0.002 (0.004)\tLoss 1.0039 (1.0441)\tPrec@1 58.750 (62.878)\n",
      "EPOCH: 173 train Results: Prec@1 62.878 Loss: 1.0441\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1869 (1.1869)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3126 (1.2601)\tPrec@1 31.250 (55.140)\n",
      "EPOCH: 173 val Results: Prec@1 55.140 Loss: 1.2601\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/390]\tTime 0.006 (0.006)\tLoss 0.8756 (0.8756)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [174][78/390]\tTime 0.003 (0.003)\tLoss 0.8887 (0.9568)\tPrec@1 71.094 (65.536)\n",
      "Epoch: [174][156/390]\tTime 0.002 (0.003)\tLoss 0.9676 (0.9892)\tPrec@1 60.938 (64.227)\n",
      "Epoch: [174][234/390]\tTime 0.003 (0.003)\tLoss 0.9939 (1.0075)\tPrec@1 68.750 (63.680)\n",
      "Epoch: [174][312/390]\tTime 0.007 (0.003)\tLoss 1.0733 (1.0225)\tPrec@1 64.844 (63.161)\n",
      "Epoch: [174][390/390]\tTime 0.002 (0.003)\tLoss 1.1157 (1.0400)\tPrec@1 58.750 (62.586)\n",
      "EPOCH: 174 train Results: Prec@1 62.586 Loss: 1.0400\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1234 (1.1234)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3508 (1.2568)\tPrec@1 43.750 (55.830)\n",
      "EPOCH: 174 val Results: Prec@1 55.830 Loss: 1.2568\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/390]\tTime 0.003 (0.003)\tLoss 0.9405 (0.9405)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [175][78/390]\tTime 0.002 (0.003)\tLoss 1.1308 (0.9747)\tPrec@1 61.719 (65.684)\n",
      "Epoch: [175][156/390]\tTime 0.004 (0.003)\tLoss 1.0733 (0.9950)\tPrec@1 60.156 (64.933)\n",
      "Epoch: [175][234/390]\tTime 0.017 (0.003)\tLoss 1.1571 (1.0136)\tPrec@1 57.812 (64.062)\n",
      "Epoch: [175][312/390]\tTime 0.007 (0.003)\tLoss 1.1721 (1.0262)\tPrec@1 57.031 (63.618)\n",
      "Epoch: [175][390/390]\tTime 0.001 (0.003)\tLoss 1.1606 (1.0405)\tPrec@1 57.500 (62.988)\n",
      "EPOCH: 175 train Results: Prec@1 62.988 Loss: 1.0405\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2057 (1.2057)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9283 (1.2571)\tPrec@1 62.500 (55.720)\n",
      "EPOCH: 175 val Results: Prec@1 55.720 Loss: 1.2571\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/390]\tTime 0.002 (0.002)\tLoss 1.0280 (1.0280)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [176][78/390]\tTime 0.002 (0.003)\tLoss 1.1981 (0.9637)\tPrec@1 64.062 (66.119)\n",
      "Epoch: [176][156/390]\tTime 0.002 (0.003)\tLoss 1.0241 (0.9952)\tPrec@1 61.719 (64.620)\n",
      "Epoch: [176][234/390]\tTime 0.006 (0.003)\tLoss 1.0635 (1.0163)\tPrec@1 66.406 (63.770)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.003)\tLoss 0.9901 (1.0298)\tPrec@1 65.625 (63.161)\n",
      "Epoch: [176][390/390]\tTime 0.001 (0.003)\tLoss 1.1216 (1.0397)\tPrec@1 57.500 (62.768)\n",
      "EPOCH: 176 train Results: Prec@1 62.768 Loss: 1.0397\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1012 (1.1012)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3045 (1.2630)\tPrec@1 31.250 (55.470)\n",
      "EPOCH: 176 val Results: Prec@1 55.470 Loss: 1.2630\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/390]\tTime 0.006 (0.006)\tLoss 0.9615 (0.9615)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [177][78/390]\tTime 0.004 (0.005)\tLoss 0.8913 (0.9604)\tPrec@1 72.656 (65.892)\n",
      "Epoch: [177][156/390]\tTime 0.002 (0.004)\tLoss 1.0596 (0.9974)\tPrec@1 57.812 (64.301)\n",
      "Epoch: [177][234/390]\tTime 0.006 (0.004)\tLoss 1.0258 (1.0140)\tPrec@1 61.719 (63.737)\n",
      "Epoch: [177][312/390]\tTime 0.002 (0.004)\tLoss 1.1249 (1.0330)\tPrec@1 58.594 (63.139)\n",
      "Epoch: [177][390/390]\tTime 0.012 (0.004)\tLoss 1.0529 (1.0403)\tPrec@1 66.250 (62.958)\n",
      "EPOCH: 177 train Results: Prec@1 62.958 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2380 (1.2380)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2157 (1.2621)\tPrec@1 56.250 (55.770)\n",
      "EPOCH: 177 val Results: Prec@1 55.770 Loss: 1.2621\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/390]\tTime 0.003 (0.003)\tLoss 0.9500 (0.9500)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [178][78/390]\tTime 0.002 (0.004)\tLoss 1.0915 (0.9695)\tPrec@1 59.375 (65.813)\n",
      "Epoch: [178][156/390]\tTime 0.004 (0.004)\tLoss 1.0604 (0.9924)\tPrec@1 59.375 (64.555)\n",
      "Epoch: [178][234/390]\tTime 0.002 (0.004)\tLoss 1.1284 (1.0165)\tPrec@1 62.500 (63.577)\n",
      "Epoch: [178][312/390]\tTime 0.003 (0.004)\tLoss 1.0617 (1.0315)\tPrec@1 58.594 (63.159)\n",
      "Epoch: [178][390/390]\tTime 0.007 (0.004)\tLoss 1.0887 (1.0417)\tPrec@1 60.000 (62.774)\n",
      "EPOCH: 178 train Results: Prec@1 62.774 Loss: 1.0417\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0856 (1.0856)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3111 (1.2658)\tPrec@1 50.000 (55.440)\n",
      "EPOCH: 178 val Results: Prec@1 55.440 Loss: 1.2658\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/390]\tTime 0.003 (0.003)\tLoss 1.0824 (1.0824)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [179][78/390]\tTime 0.002 (0.003)\tLoss 0.9673 (0.9579)\tPrec@1 63.281 (66.129)\n",
      "Epoch: [179][156/390]\tTime 0.002 (0.003)\tLoss 0.9908 (0.9860)\tPrec@1 64.844 (64.933)\n",
      "Epoch: [179][234/390]\tTime 0.003 (0.003)\tLoss 1.2443 (1.0125)\tPrec@1 57.031 (63.813)\n",
      "Epoch: [179][312/390]\tTime 0.003 (0.003)\tLoss 1.0227 (1.0253)\tPrec@1 60.938 (63.341)\n",
      "Epoch: [179][390/390]\tTime 0.004 (0.003)\tLoss 1.4208 (1.0395)\tPrec@1 50.000 (62.872)\n",
      "EPOCH: 179 train Results: Prec@1 62.872 Loss: 1.0395\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1275 (1.1275)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0097 (1.2590)\tPrec@1 62.500 (55.630)\n",
      "EPOCH: 179 val Results: Prec@1 55.630 Loss: 1.2590\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/390]\tTime 0.002 (0.002)\tLoss 1.0008 (1.0008)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [180][78/390]\tTime 0.002 (0.003)\tLoss 1.0472 (0.9675)\tPrec@1 58.594 (64.913)\n",
      "Epoch: [180][156/390]\tTime 0.003 (0.004)\tLoss 0.9197 (0.9893)\tPrec@1 67.188 (64.207)\n",
      "Epoch: [180][234/390]\tTime 0.010 (0.004)\tLoss 1.0607 (1.0130)\tPrec@1 61.719 (63.441)\n",
      "Epoch: [180][312/390]\tTime 0.007 (0.004)\tLoss 0.9945 (1.0305)\tPrec@1 64.844 (62.989)\n",
      "Epoch: [180][390/390]\tTime 0.001 (0.004)\tLoss 1.0556 (1.0413)\tPrec@1 62.500 (62.552)\n",
      "EPOCH: 180 train Results: Prec@1 62.552 Loss: 1.0413\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0998 (1.0998)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3197 (1.2742)\tPrec@1 50.000 (55.120)\n",
      "EPOCH: 180 val Results: Prec@1 55.120 Loss: 1.2742\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/390]\tTime 0.005 (0.005)\tLoss 0.9546 (0.9546)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [181][78/390]\tTime 0.003 (0.003)\tLoss 0.9079 (0.9498)\tPrec@1 64.844 (66.416)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 0.9227 (0.9772)\tPrec@1 64.844 (65.132)\n",
      "Epoch: [181][234/390]\tTime 0.005 (0.003)\tLoss 1.0000 (1.0033)\tPrec@1 62.500 (64.315)\n",
      "Epoch: [181][312/390]\tTime 0.007 (0.003)\tLoss 1.1410 (1.0234)\tPrec@1 61.719 (63.703)\n",
      "Epoch: [181][390/390]\tTime 0.001 (0.003)\tLoss 1.2092 (1.0355)\tPrec@1 57.500 (63.256)\n",
      "EPOCH: 181 train Results: Prec@1 63.256 Loss: 1.0355\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0896 (1.0896)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1791 (1.2583)\tPrec@1 56.250 (55.710)\n",
      "EPOCH: 181 val Results: Prec@1 55.710 Loss: 1.2583\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/390]\tTime 0.003 (0.003)\tLoss 1.0062 (1.0062)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [182][78/390]\tTime 0.002 (0.003)\tLoss 0.9986 (0.9589)\tPrec@1 62.500 (65.783)\n",
      "Epoch: [182][156/390]\tTime 0.008 (0.003)\tLoss 1.0123 (0.9858)\tPrec@1 69.531 (64.874)\n",
      "Epoch: [182][234/390]\tTime 0.002 (0.003)\tLoss 1.1027 (1.0121)\tPrec@1 53.906 (63.813)\n",
      "Epoch: [182][312/390]\tTime 0.006 (0.003)\tLoss 1.0014 (1.0275)\tPrec@1 67.969 (63.366)\n",
      "Epoch: [182][390/390]\tTime 0.002 (0.004)\tLoss 1.2717 (1.0386)\tPrec@1 50.000 (62.818)\n",
      "EPOCH: 182 train Results: Prec@1 62.818 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1373 (1.1373)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2751 (1.2650)\tPrec@1 43.750 (55.010)\n",
      "EPOCH: 182 val Results: Prec@1 55.010 Loss: 1.2650\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/390]\tTime 0.003 (0.003)\tLoss 0.8945 (0.8945)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [183][78/390]\tTime 0.004 (0.004)\tLoss 1.0455 (0.9624)\tPrec@1 60.156 (65.773)\n",
      "Epoch: [183][156/390]\tTime 0.004 (0.004)\tLoss 0.9910 (0.9884)\tPrec@1 67.969 (64.645)\n",
      "Epoch: [183][234/390]\tTime 0.002 (0.004)\tLoss 1.0674 (1.0139)\tPrec@1 60.938 (63.670)\n",
      "Epoch: [183][312/390]\tTime 0.002 (0.004)\tLoss 0.9209 (1.0322)\tPrec@1 67.188 (63.014)\n",
      "Epoch: [183][390/390]\tTime 0.001 (0.004)\tLoss 1.2227 (1.0433)\tPrec@1 57.500 (62.580)\n",
      "EPOCH: 183 train Results: Prec@1 62.580 Loss: 1.0433\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1607 (1.1607)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4244 (1.2612)\tPrec@1 43.750 (55.160)\n",
      "EPOCH: 183 val Results: Prec@1 55.160 Loss: 1.2612\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/390]\tTime 0.003 (0.003)\tLoss 0.7851 (0.7851)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [184][78/390]\tTime 0.003 (0.004)\tLoss 1.1139 (0.9569)\tPrec@1 64.062 (66.466)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.004)\tLoss 1.1260 (0.9866)\tPrec@1 59.375 (65.063)\n",
      "Epoch: [184][234/390]\tTime 0.004 (0.004)\tLoss 1.2253 (1.0129)\tPrec@1 57.031 (64.122)\n",
      "Epoch: [184][312/390]\tTime 0.002 (0.004)\tLoss 1.1117 (1.0303)\tPrec@1 63.281 (63.468)\n",
      "Epoch: [184][390/390]\tTime 0.003 (0.004)\tLoss 1.3250 (1.0377)\tPrec@1 55.000 (63.236)\n",
      "EPOCH: 184 train Results: Prec@1 63.236 Loss: 1.0377\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0549 (1.0549)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1437 (1.2665)\tPrec@1 50.000 (55.680)\n",
      "EPOCH: 184 val Results: Prec@1 55.680 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/390]\tTime 0.005 (0.005)\tLoss 0.9262 (0.9262)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [185][78/390]\tTime 0.004 (0.003)\tLoss 1.1399 (0.9653)\tPrec@1 53.906 (65.655)\n",
      "Epoch: [185][156/390]\tTime 0.003 (0.004)\tLoss 0.8836 (0.9868)\tPrec@1 71.094 (64.764)\n",
      "Epoch: [185][234/390]\tTime 0.003 (0.004)\tLoss 1.1568 (1.0044)\tPrec@1 60.938 (64.212)\n",
      "Epoch: [185][312/390]\tTime 0.002 (0.004)\tLoss 0.9169 (1.0175)\tPrec@1 66.406 (63.606)\n",
      "Epoch: [185][390/390]\tTime 0.002 (0.004)\tLoss 1.1631 (1.0364)\tPrec@1 56.250 (63.002)\n",
      "EPOCH: 185 train Results: Prec@1 63.002 Loss: 1.0364\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1206 (1.1206)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0623 (1.2678)\tPrec@1 43.750 (54.900)\n",
      "EPOCH: 185 val Results: Prec@1 54.900 Loss: 1.2678\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/390]\tTime 0.003 (0.003)\tLoss 0.9338 (0.9338)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [186][78/390]\tTime 0.004 (0.003)\tLoss 0.9244 (0.9696)\tPrec@1 67.969 (65.902)\n",
      "Epoch: [186][156/390]\tTime 0.009 (0.003)\tLoss 1.0440 (0.9948)\tPrec@1 60.938 (64.515)\n",
      "Epoch: [186][234/390]\tTime 0.002 (0.003)\tLoss 1.0264 (1.0189)\tPrec@1 64.062 (63.564)\n",
      "Epoch: [186][312/390]\tTime 0.002 (0.003)\tLoss 1.1488 (1.0378)\tPrec@1 60.156 (62.957)\n",
      "Epoch: [186][390/390]\tTime 0.001 (0.003)\tLoss 0.9323 (1.0458)\tPrec@1 66.250 (62.668)\n",
      "EPOCH: 186 train Results: Prec@1 62.668 Loss: 1.0458\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0556 (1.0556)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3919 (1.2554)\tPrec@1 31.250 (55.460)\n",
      "EPOCH: 186 val Results: Prec@1 55.460 Loss: 1.2554\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/390]\tTime 0.002 (0.002)\tLoss 1.0179 (1.0179)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [187][78/390]\tTime 0.005 (0.004)\tLoss 0.8661 (0.9578)\tPrec@1 71.094 (66.406)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.004)\tLoss 1.2731 (0.9769)\tPrec@1 57.031 (65.431)\n",
      "Epoch: [187][234/390]\tTime 0.004 (0.004)\tLoss 1.0739 (1.0033)\tPrec@1 61.719 (64.412)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.004)\tLoss 1.0026 (1.0246)\tPrec@1 63.281 (63.623)\n",
      "Epoch: [187][390/390]\tTime 0.004 (0.004)\tLoss 1.2358 (1.0395)\tPrec@1 55.000 (63.058)\n",
      "EPOCH: 187 train Results: Prec@1 63.058 Loss: 1.0395\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1415 (1.1415)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2319 (1.2586)\tPrec@1 31.250 (55.810)\n",
      "EPOCH: 187 val Results: Prec@1 55.810 Loss: 1.2586\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/390]\tTime 0.003 (0.003)\tLoss 0.8565 (0.8565)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [188][78/390]\tTime 0.002 (0.004)\tLoss 1.2340 (0.9463)\tPrec@1 57.812 (66.466)\n",
      "Epoch: [188][156/390]\tTime 0.003 (0.004)\tLoss 1.2041 (0.9773)\tPrec@1 58.594 (65.197)\n",
      "Epoch: [188][234/390]\tTime 0.003 (0.004)\tLoss 1.1170 (0.9991)\tPrec@1 57.031 (64.229)\n",
      "Epoch: [188][312/390]\tTime 0.005 (0.004)\tLoss 1.2157 (1.0180)\tPrec@1 60.156 (63.638)\n",
      "Epoch: [188][390/390]\tTime 0.005 (0.004)\tLoss 1.1162 (1.0337)\tPrec@1 60.000 (63.126)\n",
      "EPOCH: 188 train Results: Prec@1 63.126 Loss: 1.0337\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0716 (1.0716)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1874 (1.2582)\tPrec@1 43.750 (55.330)\n",
      "EPOCH: 188 val Results: Prec@1 55.330 Loss: 1.2582\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/390]\tTime 0.003 (0.003)\tLoss 1.0638 (1.0638)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [189][78/390]\tTime 0.004 (0.003)\tLoss 0.9302 (0.9464)\tPrec@1 65.625 (66.831)\n",
      "Epoch: [189][156/390]\tTime 0.013 (0.005)\tLoss 1.1635 (0.9861)\tPrec@1 53.906 (65.103)\n",
      "Epoch: [189][234/390]\tTime 0.013 (0.004)\tLoss 1.1479 (1.0076)\tPrec@1 58.594 (64.342)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.004)\tLoss 1.1609 (1.0242)\tPrec@1 55.469 (63.616)\n",
      "Epoch: [189][390/390]\tTime 0.001 (0.004)\tLoss 1.2353 (1.0403)\tPrec@1 60.000 (63.100)\n",
      "EPOCH: 189 train Results: Prec@1 63.100 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0850 (1.0850)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3733 (1.2507)\tPrec@1 37.500 (55.800)\n",
      "EPOCH: 189 val Results: Prec@1 55.800 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/390]\tTime 0.003 (0.003)\tLoss 0.7902 (0.7902)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [190][78/390]\tTime 0.005 (0.003)\tLoss 0.9821 (0.9530)\tPrec@1 64.844 (65.961)\n",
      "Epoch: [190][156/390]\tTime 0.002 (0.003)\tLoss 0.9590 (0.9805)\tPrec@1 62.500 (64.744)\n",
      "Epoch: [190][234/390]\tTime 0.019 (0.004)\tLoss 0.9350 (1.0050)\tPrec@1 67.969 (63.906)\n",
      "Epoch: [190][312/390]\tTime 0.003 (0.004)\tLoss 1.0562 (1.0234)\tPrec@1 58.594 (63.301)\n",
      "Epoch: [190][390/390]\tTime 0.003 (0.004)\tLoss 1.2047 (1.0384)\tPrec@1 51.250 (62.862)\n",
      "EPOCH: 190 train Results: Prec@1 62.862 Loss: 1.0384\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0754 (1.0754)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3310 (1.2581)\tPrec@1 37.500 (55.530)\n",
      "EPOCH: 190 val Results: Prec@1 55.530 Loss: 1.2581\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/390]\tTime 0.003 (0.003)\tLoss 1.0238 (1.0238)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.004)\tLoss 0.8718 (0.9624)\tPrec@1 70.312 (65.200)\n",
      "Epoch: [191][156/390]\tTime 0.012 (0.004)\tLoss 1.1122 (0.9889)\tPrec@1 57.812 (64.530)\n",
      "Epoch: [191][234/390]\tTime 0.002 (0.005)\tLoss 0.9893 (1.0103)\tPrec@1 64.062 (63.640)\n",
      "Epoch: [191][312/390]\tTime 0.002 (0.004)\tLoss 0.9334 (1.0239)\tPrec@1 66.406 (63.149)\n",
      "Epoch: [191][390/390]\tTime 0.002 (0.004)\tLoss 0.9803 (1.0404)\tPrec@1 60.000 (62.558)\n",
      "EPOCH: 191 train Results: Prec@1 62.558 Loss: 1.0404\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0181 (1.0181)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0932 (1.2414)\tPrec@1 37.500 (55.970)\n",
      "EPOCH: 191 val Results: Prec@1 55.970 Loss: 1.2414\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/390]\tTime 0.003 (0.003)\tLoss 0.8081 (0.8081)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [192][78/390]\tTime 0.003 (0.003)\tLoss 0.9439 (0.9649)\tPrec@1 60.938 (66.218)\n",
      "Epoch: [192][156/390]\tTime 0.002 (0.003)\tLoss 1.0101 (0.9939)\tPrec@1 67.188 (64.520)\n",
      "Epoch: [192][234/390]\tTime 0.006 (0.003)\tLoss 1.0538 (1.0166)\tPrec@1 65.625 (63.750)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.003)\tLoss 1.2486 (1.0294)\tPrec@1 60.156 (63.336)\n",
      "Epoch: [192][390/390]\tTime 0.002 (0.003)\tLoss 1.0758 (1.0418)\tPrec@1 61.250 (62.892)\n",
      "EPOCH: 192 train Results: Prec@1 62.892 Loss: 1.0418\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0804 (1.0804)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2320 (1.2535)\tPrec@1 37.500 (55.650)\n",
      "EPOCH: 192 val Results: Prec@1 55.650 Loss: 1.2535\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/390]\tTime 0.002 (0.002)\tLoss 0.8078 (0.8078)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [193][78/390]\tTime 0.002 (0.003)\tLoss 0.9409 (0.9689)\tPrec@1 69.531 (65.902)\n",
      "Epoch: [193][156/390]\tTime 0.002 (0.004)\tLoss 1.1053 (0.9943)\tPrec@1 57.031 (64.694)\n",
      "Epoch: [193][234/390]\tTime 0.010 (0.004)\tLoss 1.0052 (1.0099)\tPrec@1 61.719 (64.016)\n",
      "Epoch: [193][312/390]\tTime 0.002 (0.004)\tLoss 1.2498 (1.0304)\tPrec@1 60.938 (63.276)\n",
      "Epoch: [193][390/390]\tTime 0.009 (0.004)\tLoss 1.0358 (1.0410)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 193 train Results: Prec@1 62.850 Loss: 1.0410\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0595 (1.0595)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2559 (1.2607)\tPrec@1 25.000 (55.140)\n",
      "EPOCH: 193 val Results: Prec@1 55.140 Loss: 1.2607\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/390]\tTime 0.003 (0.003)\tLoss 0.9384 (0.9384)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [194][78/390]\tTime 0.002 (0.004)\tLoss 0.9088 (0.9649)\tPrec@1 64.062 (65.635)\n",
      "Epoch: [194][156/390]\tTime 0.002 (0.003)\tLoss 0.9268 (0.9914)\tPrec@1 67.969 (64.719)\n",
      "Epoch: [194][234/390]\tTime 0.005 (0.003)\tLoss 1.0127 (1.0074)\tPrec@1 63.281 (64.126)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.003)\tLoss 1.0544 (1.0227)\tPrec@1 60.938 (63.661)\n",
      "Epoch: [194][390/390]\tTime 0.003 (0.003)\tLoss 1.0680 (1.0365)\tPrec@1 57.500 (63.264)\n",
      "EPOCH: 194 train Results: Prec@1 63.264 Loss: 1.0365\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1395 (1.1395)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3373 (1.2526)\tPrec@1 43.750 (55.260)\n",
      "EPOCH: 194 val Results: Prec@1 55.260 Loss: 1.2526\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/390]\tTime 0.004 (0.004)\tLoss 0.8456 (0.8456)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [195][78/390]\tTime 0.003 (0.004)\tLoss 0.9408 (0.9636)\tPrec@1 67.969 (65.585)\n",
      "Epoch: [195][156/390]\tTime 0.002 (0.004)\tLoss 0.9584 (0.9905)\tPrec@1 67.969 (64.515)\n",
      "Epoch: [195][234/390]\tTime 0.003 (0.004)\tLoss 1.0783 (1.0095)\tPrec@1 58.594 (63.873)\n",
      "Epoch: [195][312/390]\tTime 0.002 (0.004)\tLoss 1.1658 (1.0264)\tPrec@1 57.031 (63.344)\n",
      "Epoch: [195][390/390]\tTime 0.001 (0.004)\tLoss 1.1824 (1.0386)\tPrec@1 57.500 (62.882)\n",
      "EPOCH: 195 train Results: Prec@1 62.882 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0615 (1.0615)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3907 (1.2681)\tPrec@1 37.500 (55.180)\n",
      "EPOCH: 195 val Results: Prec@1 55.180 Loss: 1.2681\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/390]\tTime 0.003 (0.003)\tLoss 0.8733 (0.8733)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 0.9902 (0.9458)\tPrec@1 62.500 (66.307)\n",
      "Epoch: [196][156/390]\tTime 0.002 (0.003)\tLoss 1.0167 (0.9765)\tPrec@1 63.281 (65.192)\n",
      "Epoch: [196][234/390]\tTime 0.008 (0.004)\tLoss 0.9534 (1.0095)\tPrec@1 66.406 (64.079)\n",
      "Epoch: [196][312/390]\tTime 0.003 (0.004)\tLoss 1.0817 (1.0230)\tPrec@1 59.375 (63.621)\n",
      "Epoch: [196][390/390]\tTime 0.001 (0.004)\tLoss 0.9751 (1.0372)\tPrec@1 61.250 (63.076)\n",
      "EPOCH: 196 train Results: Prec@1 63.076 Loss: 1.0372\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1523 (1.1523)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3476 (1.2492)\tPrec@1 43.750 (55.790)\n",
      "EPOCH: 196 val Results: Prec@1 55.790 Loss: 1.2492\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/390]\tTime 0.003 (0.003)\tLoss 0.8293 (0.8293)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [197][78/390]\tTime 0.003 (0.003)\tLoss 0.9901 (0.9463)\tPrec@1 63.281 (66.426)\n",
      "Epoch: [197][156/390]\tTime 0.002 (0.004)\tLoss 1.1288 (0.9772)\tPrec@1 63.281 (65.341)\n",
      "Epoch: [197][234/390]\tTime 0.006 (0.004)\tLoss 1.0022 (1.0065)\tPrec@1 61.719 (64.365)\n",
      "Epoch: [197][312/390]\tTime 0.003 (0.004)\tLoss 1.0794 (1.0269)\tPrec@1 58.594 (63.563)\n",
      "Epoch: [197][390/390]\tTime 0.002 (0.004)\tLoss 0.9502 (1.0383)\tPrec@1 66.250 (63.214)\n",
      "EPOCH: 197 train Results: Prec@1 63.214 Loss: 1.0383\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1654 (1.1654)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3319 (1.2544)\tPrec@1 43.750 (55.090)\n",
      "EPOCH: 197 val Results: Prec@1 55.090 Loss: 1.2544\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/390]\tTime 0.004 (0.004)\tLoss 0.9726 (0.9726)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [198][78/390]\tTime 0.002 (0.004)\tLoss 0.9118 (0.9646)\tPrec@1 62.500 (65.259)\n",
      "Epoch: [198][156/390]\tTime 0.003 (0.003)\tLoss 1.0433 (0.9876)\tPrec@1 68.750 (64.381)\n",
      "Epoch: [198][234/390]\tTime 0.002 (0.003)\tLoss 1.3244 (1.0101)\tPrec@1 60.156 (63.531)\n",
      "Epoch: [198][312/390]\tTime 0.002 (0.003)\tLoss 0.9022 (1.0238)\tPrec@1 75.000 (63.112)\n",
      "Epoch: [198][390/390]\tTime 0.003 (0.004)\tLoss 0.9602 (1.0346)\tPrec@1 68.750 (62.922)\n",
      "EPOCH: 198 train Results: Prec@1 62.922 Loss: 1.0346\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1381 (1.1381)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1587 (1.2621)\tPrec@1 43.750 (55.140)\n",
      "EPOCH: 198 val Results: Prec@1 55.140 Loss: 1.2621\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/390]\tTime 0.003 (0.003)\tLoss 1.0403 (1.0403)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [199][78/390]\tTime 0.006 (0.003)\tLoss 0.9257 (0.9463)\tPrec@1 65.625 (66.515)\n",
      "Epoch: [199][156/390]\tTime 0.002 (0.003)\tLoss 1.0147 (0.9715)\tPrec@1 67.188 (65.525)\n",
      "Epoch: [199][234/390]\tTime 0.003 (0.003)\tLoss 1.0157 (1.0004)\tPrec@1 64.062 (64.405)\n",
      "Epoch: [199][312/390]\tTime 0.002 (0.003)\tLoss 1.1317 (1.0233)\tPrec@1 60.156 (63.486)\n",
      "Epoch: [199][390/390]\tTime 0.004 (0.003)\tLoss 1.0976 (1.0372)\tPrec@1 63.750 (62.988)\n",
      "EPOCH: 199 train Results: Prec@1 62.988 Loss: 1.0372\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0649 (1.0649)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2117 (1.2525)\tPrec@1 37.500 (55.640)\n",
      "EPOCH: 199 val Results: Prec@1 55.640 Loss: 1.2525\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/390]\tTime 0.003 (0.003)\tLoss 0.8200 (0.8200)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [200][78/390]\tTime 0.002 (0.004)\tLoss 1.0655 (0.9640)\tPrec@1 64.062 (65.566)\n",
      "Epoch: [200][156/390]\tTime 0.003 (0.004)\tLoss 0.7913 (0.9894)\tPrec@1 72.656 (64.859)\n",
      "Epoch: [200][234/390]\tTime 0.002 (0.004)\tLoss 1.0190 (1.0121)\tPrec@1 60.156 (63.790)\n",
      "Epoch: [200][312/390]\tTime 0.003 (0.004)\tLoss 1.0874 (1.0254)\tPrec@1 61.719 (63.429)\n",
      "Epoch: [200][390/390]\tTime 0.005 (0.004)\tLoss 1.3638 (1.0393)\tPrec@1 56.250 (63.014)\n",
      "EPOCH: 200 train Results: Prec@1 63.014 Loss: 1.0393\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1261 (1.1261)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0260 (1.2463)\tPrec@1 56.250 (55.920)\n",
      "EPOCH: 200 val Results: Prec@1 55.920 Loss: 1.2463\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "End time:  Thu Apr  4 23:42:35 2024\n",
      "train executed in 325.3088 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'scheduler': None, \n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAHFCAYAAABb+zt/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADpGElEQVR4nOzddVgV2RvA8S/d3SCtqIiJ3d21uqtrrN3dio0BNriKjWLX2rp2u8YqugZiJwoiId3c3x/o1Sug4N4L/tbzeZ77PMyZMzPvvXPv8M45Z2aUJBKJBEEQBEEQBAVRLuwABEEQBEH4bxPJhiAIgiAICiWSDUEQBEEQFEokG4IgCIIgKJRINgRBEARBUCiRbAiCIAiCoFAi2RAEQRAEQaFEsiEIgiAIgkKJZEMQBEEQBIUSyYbwn3br1i169uyJo6Mjmpqa6OrqUqFCBebNm0dUVJRCt33jxg3q1KmDgYEBSkpK+Pr6yn0bSkpKTJ8+Xe7r/ZqAgACUlJRQUlLizJkz2eZLJBKKFi2KkpISdevW/aZtLFu2jICAgHwtc+bMmVxjEgSh8KgWdgCCoCirV69m0KBBFC9enLFjx+Lq6kpaWhrXrl1jxYoVXLp0iT179ihs+7169SIhIYFt27ZhZGSEg4OD3Ldx6dIlihQpIvf15pWenh7+/v7ZEoqzZ8/y+PFj9PT0vnndy5Ytw9TUlB49euR5mQoVKnDp0iVcXV2/ebuCIMifSDaE/6RLly4xcOBAGjVqxN69e9HQ0JDOa9SoEaNHj+bIkSMKjeHOnTv07duXZs2aKWwbVatWVdi686Jjx45s3rwZPz8/9PX1peX+/v5Uq1aN2NjYAokjLS0NJSUl9PX1C/0zEQQhO9GNIvwneXl5oaSkxKpVq2QSjQ/U1dVp3bq1dDozM5N58+ZRokQJNDQ0MDc3p1u3boSEhMgsV7duXdzc3Lh69Sq1atVCW1sbJycn5syZQ2ZmJvCxiyE9PZ3ly5dLuxsApk+fLv37Ux+WefbsmbTs1KlT1K1bFxMTE7S0tLCzs6N9+/YkJiZK6+TUjXLnzh3atGmDkZERmpqalCtXjvXr18vU+dDdsHXrViZNmoS1tTX6+vo0bNiQ+/fv5+1DBjp16gTA1q1bpWUxMTHs2rWLXr165biMp6cnVapUwdjYGH19fSpUqIC/vz+fPhPSwcGBoKAgzp49K/38PrQMfYh948aNjB49GhsbGzQ0NHj06FG2bpSIiAhsbW2pXr06aWlp0vXfvXsXHR0dfvvttzy/V0EQvp1INoT/nIyMDE6dOoW7uzu2trZ5WmbgwIGMHz+eRo0asX//fmbOnMmRI0eoXr06ERERMnXDwsLo0qULXbt2Zf/+/TRr1gwPDw82bdoEQIsWLbh06RIAP//8M5cuXZJO59WzZ89o0aIF6urqrF27liNHjjBnzhx0dHRITU3Ndbn79+9TvXp1goKC+P3339m9ezeurq706NGDefPmZas/ceJEnj9/zpo1a1i1ahUPHz6kVatWZGRk5ClOfX19fv75Z9auXSst27p1K8rKynTs2DHX99a/f3927NjB7t27adeuHUOHDmXmzJnSOnv27MHJyYny5ctLP7/Pu7w8PDx48eIFK1as4MCBA5ibm2fblqmpKdu2bePq1auMHz8egMTERH755Rfs7OxYsWJFnt6nIAj/kkQQ/mPCwsIkgOTXX3/NU/3g4GAJIBk0aJBM+ZUrVySAZOLEidKyOnXqSADJlStXZOq6urpKmjRpIlMGSAYPHixTNm3aNElOP7t169ZJAMnTp08lEolE8scff0gAyT///PPF2AHJtGnTpNO//vqrRENDQ/LixQuZes2aNZNoa2tL3r17J5FIJJLTp09LAEnz5s1l6u3YsUMCSC5duvTF7X6I9+rVq9J13blzRyKRSCSVKlWS9OjRQyKRSCSlSpWS1KlTJ9f1ZGRkSNLS0iQzZsyQmJiYSDIzM6Xzclv2w/Zq166d67zTp0/LlM+dO1cCSPbs2SPp3r27REtLS3Lr1q0vvkdBEORHtGwIP7zTp08DZBuIWLlyZUqWLMnJkydlyi0tLalcubJMWZkyZXj+/LncYipXrhzq6ur069eP9evX8+TJkzwtd+rUKRo0aJCtRadHjx4kJiZma2H5tCsJst4HkK/3UqdOHZydnVm7di23b9/m6tWruXahfIixYcOGGBgYoKKigpqaGlOnTiUyMpLw8PA8b7d9+/Z5rjt27FhatGhBp06dWL9+PUuWLKF06dJ5Xl4QhH9HJBvCf46pqSna2to8ffo0T/UjIyMBsLKyyjbP2tpaOv8DExOTbPU0NDRISkr6hmhz5uzszIkTJzA3N2fw4ME4Ozvj7OzM4sWLv7hcZGRkru/jw/xPff5ePoxvyc97UVJSomfPnmzatIkVK1bg4uJCrVq1cqz7999/07hxYyDraqG//vqLq1evMmnSpHxvN6f3+aUYe/ToQXJyMpaWlmKshiAUMJFsCP85KioqNGjQgMDAwGwDPHPy4R9uaGhotnmvX7/G1NRUbrFpamoCkJKSIlP++bgQgFq1anHgwAFiYmK4fPky1apVY8SIEWzbti3X9ZuYmOT6PgC5vpdP9ejRg4iICFasWEHPnj1zrbdt2zbU1NQ4ePAgHTp0oHr16lSsWPGbtpnTQNvchIaGMnjwYMqVK0dkZCRjxoz5pm0KgvBtRLIh/Cd5eHggkUjo27dvjgMq09LSOHDgAAD169cHkA7w/ODq1asEBwfToEEDucX14YqKW7duyZR/iCUnKioqVKlSBT8/PwCuX7+ea90GDRpw6tQpaXLxwYYNG9DW1lbYZaE2NjaMHTuWVq1a0b1791zrKSkpoaqqioqKirQsKSmJjRs3Zqsrr9aijIwMOnXqhJKSEocPH8bb25slS5awe/fuf71uQRDyRtxnQ/hPqlatGsuXL2fQoEG4u7szcOBASpUqRVpaGjdu3GDVqlW4ubnRqlUrihcvTr9+/ViyZAnKyso0a9aMZ8+eMWXKFGxtbRk5cqTc4mrevDnGxsb07t2bGTNmoKqqSkBAAC9fvpSpt2LFCk6dOkWLFi2ws7MjOTlZesVHw4YNc13/tGnTOHjwIPXq1WPq1KkYGxuzefNmDh06xLx58zAwMJDbe/ncnDlzvlqnRYsWLFq0iM6dO9OvXz8iIyNZsGBBjpcnly5dmm3btrF9+3acnJzQ1NT8pnEW06ZN4/z58xw7dgxLS0tGjx7N2bNn6d27N+XLl8fR0THf6xQEIX9EsiH8Z/Xt25fKlSvj4+PD3LlzCQsLQ01NDRcXFzp37syQIUOkdZcvX46zszP+/v74+flhYGBA06ZN8fb2znGMxrfS19fnyJEjjBgxgq5du2JoaEifPn1o1qwZffr0kdYrV64cx44dY9q0aYSFhaGrq4ubmxv79++XjnnISfHixbl48SITJ05k8ODBJCUlUbJkSdatW5evO3EqSv369Vm7di1z586lVatW2NjY0LdvX8zNzendu7dMXU9PT0JDQ+nbty9xcXHY29vL3IckL44fP463tzdTpkyRaaEKCAigfPnydOzYkQsXLqCuri6PtycIQi6UJJJP7qQjCIIgCIIgZ2LMhiAIgiAICiWSDUEQBEEQFEokG4IgCIIgKJRINgRBEARBUCiRbAiCIAiCoFAi2RAEQRAEQaFEsiEIgiAIgkL9J2/qZdVvV2GH8EV3fdsWdghfFJOUVtgh5MpI+/u++VJqRmZhh/BF2uoqX69USL73O/6kZ37f+zYoJLawQ8iVo5lOYYfwRdaGij+uaJUf8vVKeZB0Y6lc1lPQRMuGIAiCIAgK9Z9s2RAEQRCE74rSj31uL5INQRAEQVA0JaXCjqBQiWRDEARBEBTtB2/Z+LHfvSAIgiAICidaNgRBEARB0UQ3iiAIgiAICiW6UQRBEARBEBRHtGwIgiAIgqKJbhRBEARBEBRKdKMIgiAIgiAojmjZEARBEARFE90oP57RrUoyppWrTFl4TDJlxx5CVUWJ8W1K0aC0JfamOsQmpXE+OJzZu+/wJiZZWn9e1/LUKmmOhYEWiSnpXH0cyezdd3gUFif3eHft2MbuP7YR+voVAE5ORenVbyDVa9bOVnfOrGns3bWTEWMm8GuXbnKPJScZ6els9F/BqWOHiI6MxNjUlEbNW9O5Rz+UlbMazyQSCZv8V/Dn/l3Ex8ZSolRpBo/2wMGpqEJjC/BfxemTx3n+7AkaGpqULlueoSNGY+/gKFPv6ZPHLF28kOuBV5FkZuLkXBSveT5YWlkrNL7PbVy7mpV+vvzSqSvDx3gAMHvaRA4f3CdTz9WtDKvWb1V4PIHXrrIhwJ/gu0FEvH3LQt+l1GvQMMe6szynsvuPHYwe50GX37orPDaA6x/iC86Kb4HvUurV/xife5kSOS43fORYuvXsrdDYAvxXcebkiU++e+UY8tl3LzIyAj/fRVy5/BdxcXGUr1CR0eMnYmfvIPd47t+5wZFdm3j2+D4xUREMmTSXCtXqyNR5/fIpf6zz4/6dG2RKJNjYOTJw/GxMzC0BCA8NYbv/Eh7evUl6Wipu7tXo0n8UBkYmco/3bfgbVvn58PfFC6SkpFDEzp6xkzwpXrIUAAGrl3Hq+GHevnmDqpoqLiVc6T1gGK5uZeQei1z84N0oP2SyAXDvVQwdfM5LpzMzsx45qaWuQmk7Q3wOBnM3JAYDbXVmdCzD+sHVaep1Slr/1vN37L7ykpCoRIx01BndqiTbRtSkssdhMuX89EpzCwsGDx1JETt7AA4d2Mu4kUPYsG0XTs7FpPXOnj5B0O1bmJmZyzeAr9i+aR2H9u5kzOSZ2Ds58zD4Lgu9pqKjo8dPHbsAsGPTOnZv28joyTMoYmvPloDVeIwYgP/WfWjrKO6JkNcDr/JLx86ULOVGRkYGy5f6MnRgb7bvPoiWljYAIS9f0LdnF1q3bU+/gUPQ1dXj6ZPHqGtoKCyunAQH3Wb/np04F3PJNq9K9ZpMnDZLOq2mplYgMSUnJeHiUoLWbdsxduSwXOudPnmCO7dvYWZesN+9pKQkXIq/j29U9viOnjovM33xwjlmTJtM/UaNFR7bjcBr/NyxE66l3EjPyGDF0sUMG9iHbbsPoKWljUQiYdzIoaiqqjLfZyk6urps2RjA0AG9pXXkKSU5CVunYtRs1BI/L49s88NDQ/Ae159ajVrRpktftHR0CX35DDV1denyC6cMx9axKOO8sp48umfTKn6fMZZJC9dITyzkIS42hqH9ulG+QiXm+C7HyMiYV69eoqunL61TxM6e4WMmYmVThJSUFP7YupFxw/qzadchDI2M5RaLIB8/bLKRninhbWxKtvK4pHR+9b0gUzZp602OTKqPjbEWr6KSANh0/ql0fkhkInP3BnFqWiNsTXV4/jZBrrHWqlNPZnrgkBHs2bmNO7duSZON8PA3LJgzm8XLVjFq6EC5bv9rgu/cpFqtulSpkdXSYmllw+kTh3l4LwjIatXYu2Mzv3bvQ826WWedY6bM4teW9Tl9/E9atP1FYbH9vmy1zPRUTy+a1K9B8N0gKrhXAmD5Ul9q1KzNsJFjpfVsitgqLKacJCYm4Dl5POMme7Lef2W2+epq6piYmhVoTAA1atWmRq3sLWifCn/zhrleM/FbuYZhg/sXUGRZvhaf6Wef2ZnTp6hYqQpFCmD/Ll62SmZ6iudsmtavyb27dynvXpGXL55z59ZNtv6xD6eiWb/jcROn0rR+TY4d/pM27X6WazxlKlanTMXquc7fvWEFZSpWp0OvodIyc0sb6d8P794iIjyU6b9vQEs76wSh14jJDP21McG3rlGqXGW5xbp141rMzS0ZP/Vjgm1pbSNTp2GTFjLTg4aP5c/9u3n86AHularKLRa5+cG7UQq1XSckJIRJkyZRr149SpYsiaurK/Xq1WPSpEm8fPlSodt2MtflxrzmXPFqyvK+lbEzzf3sWl9bjcxMCTGJaTnO11JX4dcaDjx/m8DrqERFhQxARkYGx4/8SVJSEqXLlAUgMzMTz8kT6Nq9l0xLR0FxK1Oef679TciLZwA8fnifoJs3qFStFgBhr18RFRmBe+Vq0mXU1dUpXc6du7dvFmis8fFZ3VwGBgZA1mf31/mz2Nk7MHRgH5rUq0HPrh05c+pEgca1aM4sqtesTaUq1XKcfyPwKi0b1uLXn5ozd+ZUoqMiCzS+3GRmZjJ54ji69eyNc9GC/+7lR2RkBBfOn6XNT+0LZfsfvnv67797qampADItaCoqKqipqXHzxvUCjS0zM5Ob1y5iYW3HwinDGd6lGTNH9eL6pbPSOulpqSihhOonrWpqauooKSvzMEi+v+OL585QvKQr0z1G8VPTOvT97RcO7v0j1/ppaWkc3PsHOrp6FC1WXK6xyI2Ssnxe/6cKrWXjwoULNGvWDFtbWxo3bkzjxo2RSCSEh4ezd+9elixZwuHDh6lRo4bct33jaRTD1l3l8Zt4zPQ1GdG8BAfG16Xu9ONEJ6TK1NVQVWbST27s+fsl8cnpMvO613FiSvvS6Giq8jA0lo6+50nLkHMfynuPHj6gb/dOpKamoqWlzdyFv+PonDXeYeO6NaioqNChU1eFbPtrOvzWi4SEePp0aouysgqZmRn06D+Ueo2bARAVFQGAkbFsv66RsQnhYa8LLE6JRILvwrmULe+Oc1GX97FFkpiYyPq1axgweBhDh4/m0sULjB89jOWrA6hQUX5na7k5cfRPHtwLZvXG7TnOr1qjFvUaNsHSyprXr0NYs3wJwwb0wn/TTtTfN3EXloC1q1FVUaFTl98KNY68OLhvLzraOtRvqPgulM9JJBIWL5xH2fIVpEmZg4MjVlbWLPvdhwlTpqOlpcWWjeuJjIggIuJtgcYXFxNNSlIif/6xgXa/9eeXnoO5HXgZP68JjPPyo3jpCjiVcENDU5Od6/xo320gIGHnOj8kmZnERMs3+X39OoR9u3fwS6dudOnRl+Cg2yxZNAc1dXWaNG8trXfpwllmTB5LSnIyJqZmLFiyCgNDI7nGIjc/eMtGoSUbI0eOpE+fPvj4+OQ6f8SIEVy9evWL60lJSSElRbY7RJKRhpJK7n3ap+68kf5971Us1x5Hcnl2UzpUs2fliYfSeaoqSqzoVwVlZZiw5Ua29ez++wXngsOxMNBkQONirOpXhdZzz5CSnvnFmL+FvYMDG7btJj4ujtMnjzFj6kSWr1lPSkoK27duZP2WXSgV0pf57IkjnDx6iAnTvbF3KsrjB/dYsXg+JqZmNPrkwPD5j00ikRToD3C+90wePbjPqoDNH2N4P8Cmdt36dP6tBwAuJUpy6+YNdv+xXeHJxpuwUBYvmMMiv1Vo5DJGpMH7pA3AqWgxSpR04+eWDbl04Sx16jdSaHxfcjfoDls3bWTLjsL77uXHvr27aNaiZa6fsyLN957Fowf3WRmwSVqmqqaG98LFzJ4+mUa1q6GiokKlKtWoVqNWgceXmZl1zCpftTaN23YCwM7JhcfBtzh9eA/FS1dA38CIgRO82LhsHicP7EBJSZkqdRph71xcruM1ACSZmRQvWYq+g4YDUKx4SZ49fcz+Xdtlko1y7pVYs/EPYt5Fc3DfLjwnjmHZ2s3ZTmyEwldoycadO3fYtGlTrvP79+/PihUrvroeb29vPD09Zcp0KvyCnnvHPMeSlJpB8KsYHM11pWWqKkqs6lcFWxNtfll0PlurBmSN74hLiudpeDyBTyK559uaZuWt2Xs1JM/bzis1NXVs3w8QLVnKjbtBd9i+dSMOjs5ER0XRtnkDad2MjAx+XzSPbZs3sPdPxXcHrPbzoeNvvajbKOufoqNzMcLDQtm2wZ9GzVtjbGwKQHRkhMy4g3fRUQV2UJg/Zxbnzp5m5dqNWFhYSssNjQxRUVXF0dlZpr6Do1OBNGXfD75LdFQkfbp2kJZlZGRw8/o1du/YyqlLN1BRUZFZxtTMDEsra16+eK7w+L7kxvVAoqIiad64vrQsIyMDnwVz2bJpPYeOnvrC0gXrRuA1nj97ypz5OZ/cKNKCObM4f/Y0K9dukPnuAZR0LcWmHXuIj4sjLS0NI2NjenXtSAlXtwKNUU/fEBUVFaxtHWTKrWwdeHj3YxeJW4UqzF2zi7iYd6ioqKCtq8eIrs0xtZDvVVsmpmbYO8r+Ju0dnDh/WvZ4pqWljY2tHTa2driWLkvX9i34c/8euvToI9d45OL/uAtEHgot2bCysuLixYsUL55z/9qlS5ewsrL66no8PDwYNWqUTJnLyD/zFYu6qjLFrPS48jCruf9DouForsvPC89l61rJjZISqKuqfL2iXEhITU2jWYvW2fr5RwzqS9MWrWnZ5qcCiSQlORmlz35IyioqSCRZZ0uW1jYYm5hy/eplihYvCWT1sd7+J5De789cFEUikbBgzizOnDrB8jXrsbEpIjNfTU0dV1c3Xjx7KlP+4vmzArnstWLlqmzYvlemzMtzEvYOTnTp3jtbogEQ8+4d4W/CCmXA6KdatGpNlaqy373BA/rQomUbWrctmO9eXu3d8wclXUvhUjznS2EVIeu7N5uzp06wbE0A1p999z6lq6cHZH3vgu8G0W9Q7lf+KIKqmhoOxVwJe/VCpjzs1UtMzLMfh/UMDAEIvnmNuJhoylWRb2tMqTLlePn8mUxZyItnWFh++X+CBAlpaXk7Xhc4kWwUjjFjxjBgwAACAwNp1KgRFhYWKCkpERYWxvHjx1mzZg2+vr5fXY+Ghka2ZtEvdaEATP25NMdvhRISmYipvgYjmpdET1ONnZdeoKKsxOr+VSltZ0i3pRdRVlbCTD9r/e8SUknLkGBnqkObikU4e/cNkfEpWBpqMaRpcZJSMzh5J+ybP5PcLF/iQ7UatTC3tCIxIYHjR//k+rWr+PitwsDQEANDQ5n6KqqqmJiaZruXhKJUrVmHbetXY25hib2TM48f3GP3to00btEGACUlJdp26MK2Df5ZZyFF7Ni6wR8NTU3qNWqu0Njmec3g6OFDLPBdiraOjrQvXFdXD01NTQC69ujFpHGjKV+hIu6VqnDp4gUunDvD8jXrFRobgLaOjvRKhA80tbTRNzDAqWgxEhMTWLtyGXUbNMLE1IzQ169Y5bcYA0Mj6tTL+X4X8pSYmMDLFx//Ab16FcL9e8HoGxhgZWWN4Wf946rvv3sOjk4Kjy2n+F5/Fh9AfHw8J44dZeSY8QUS0wfzvWZy9PAh5vsuRUdHh8j33z2dT757J48dwdDIGEsrKx49fIDPPG9q12tA1eryH6uWnJRIeOjHVteIN6958eQBOrr6mJhb0rRdF1bMm4xLqXKUKOPOncDL3Pz7AuO8/aTLnD9+EGtbB/QMDHl87zZbVvnQqM2vWBWxl2usv3TqxpA+v7EpYDX1GjQh+O5tDu7dxSiPqQAkJSWyad1qatSqi7GpGbEx79i3aztvw99Qp0HBj8kRvq7Qko1BgwZhYmKCj48PK1euJCMjA8gaje3u7s6GDRvo0KHDV9bybayMtFjWpzLGuhpExqVw/WkULeecJiQqkSIm2jQtl3WQOjlV9mDebsFZLj2IICUtgyrFTOnbsCgG2uq8jU3mysMIWs89Q2Rc9stp/62oyEimT55AZMRbdHX1cC7mgo/fKqpUzf0ytoI0aOQE1q/2Y+kCL95FR2FiakbzNj/TpdfHyyA7dO1JakoKSxd4ERcXSwnX0nj7LFfoPTYAdu3cBsCAPrI3mZrq6SVt+alXvxETJk9jvf8qFs7zws7ekTkLFlOuvLtCY8sLFWUVnjx6wJFD+4mPi8XE1IwKFSvj6b1A4Z8dZI3L6Nfr42e3aP4cAFq1bovn7DkK3/7X3A26Q//e2eNr2botnrOy/j525BASJDRp1iLHdSjKh+/ewM++e1M8Z0u/exERb/FdOI+oyAhMzcxo1rINvfsNUEg8zx4GM2/iYOn0tjWLAajRoDm9R07FvXpdug0az6Gd69myygdLGzsGT/TGpVQ56TJhr56za/0yEuJjMTW3omWHHtIxHvJUwtWNmfN8Wb3Mlw3+K7CytmHwyHE0atoSyPpdvHz+lGl/7ifmXTT6BoYUL1mK31eux1HBNwr8Zsrf/7gmRVKSSCSKuXwiH9LS0oiIyOrCMDU1/dc3LLLqt0seYSnMXd+2hR3CF8Uk5XyJ7/fASLtwr774mtQM+Q8Olidt9YLq5su/wj8SfVl65ve9b4NCYgs7hFw5mik+Mf43rA0Vf1zRqj9bLutJOjVJLuspaN/FTb3U1NTyND5DEARBEIT/P99FsiEIgiAI/2n/B5eHK5JINgRBEARB0X7wq1F+7HcvCIIgCP9hr169omvXrpiYmKCtrU25cuUIDAyUzpdIJEyfPh1ra2u0tLSoW7cuQUFBMutISUlh6NChmJqaoqOjQ+vWrQkJyd/9pESyIQiCIAiKpqQkn1c+REdHU6NGDdTU1Dh8+DB3795l4cKFGH5yu4R58+axaNEili5dytWrV7G0tKRRo0bExcVJ64wYMYI9e/awbds2Lly4QHx8PC1btpReRZoXohtFEARBEBStELpR5s6di62tLevWrZOWOTg4SP+WSCT4+voyadIk2rVrB8D69euxsLBgy5Yt9O/fn5iYGPz9/dm4cSMNG2bdDmLTpk3Y2tpy4sQJmjRpkqdYRMuGIAiCIChaIbRs7N+/n4oVK/LLL79gbm5O+fLlWb16tXT+06dPCQsLo3HjjzdC09DQoE6dOly8eBGAwMBA0tLSZOpYW1vj5uYmrZMXItkQBEEQhP8TKSkpxMbGyrw+fxjpB0+ePGH58uUUK1aMo0ePMmDAAIYNG8aGDRsACAvLuuO1hYWFzHIWFhbSeWFhYairq2NkZJRrnbwQyYYgCIIgKJqSslxe3t7eGBgYyLy8vb1z3GRmZiYVKlTAy8uL8uXL079/f/r27cvy5ctlQ8vhidxfe5JzXup8SiQbgiAIgqBocupG8fDwICYmRubl4eGR4yatrKxwdXWVKStZsiQv3j9PyNIy6ynEn7dQhIeHS1s7LC0tSU1NJTo6Otc6eSGSDUEQBEH4P6GhoYG+vr7M6/OHkX5Qo0YN7t+/L1P24MED7O2zHpzn6OiIpaUlx48fl85PTU3l7NmzVK+e9ewtd3d31NTUZOqEhoZy584daZ28EFejCIIgCIKiFcLVKCNHjqR69ep4eXnRoUMH/v77b1atWsWqVauyQlJSYsSIEXh5eVGsWDGKFSuGl5cX2tradO7cGQADAwN69+7N6NGjMTExwdjYmDFjxlC6dGnp1Sl5IZINQRAEQVC0QrhdeaVKldizZw8eHh7MmDEDR0dHfH196dKli7TOuHHjSEpKYtCgQURHR1OlShWOHTuGnp6etI6Pjw+qqqp06NCBpKQkGjRoQEBAACoqeX+w43fx1Fd5exufXtghfJHdL0sKO4Qvito/srBDyFVKet5vIlMY1FW/755JJb7f5zN8709V/c7D+66f1hwc9v0+kRagiauZwreh1eJ3uawn6dAwuaynoImWDUEQBEFQtB/82Sgi2RAEQRAERfvBk40f+90LgiAIgqBwomVDEARBEBStEAaIfk9EsiEIgiAIivaDd6OIZEMQBEEQFO0Hb9n4sVMtQRAEQRAUTrRsCIIgCIKiiW4UQRAEQRAUSnSjCIIgCIIgKI5o2RAEQRAEBVP6wVs2RLIhCIIgCAomkg0hm41rV7PSz5dfOnVl+BgPAPxX+nHy6GHC34ShqqZG8ZKu9Bs0nFKly8h9+5O6VGVy12oyZWFRCTh2yXossLmhNrN61aRhBXsMdDS4cOcVo5af5vHrdwDYmetzf33vHNfdZfZBdl94KPeYP7Xcbwkrly+VKTMxMeXk2b8Uut2cBPiv4szJEzx/9gQNDU1Kly3HkBGjsXdwlNZJTEzAb7EPZ0+fJDbmHVbWNnTo1JX2HX4t8HgBEhLiWbbkd06dPEF0VCTFS5Rk3IRJlCpdulDi+dT3tG8Brl+7ysaAtQQHBxHx9i0LfJdQt/7Hx14nJiawxHcRZ0+dJOb9vv21c1d+7thJ4bEF+K/i9Mnjn3z3yjP0s++e5xQPDh3YK7OcW+kyrN24XeHxdf2pKW/CXmcrb9WuI8PGTgLg+bMnrPHz4daNQCSSTOwdnZkyawHmllZyjeVR0D+c3LuFl4/vExsdSZ8JXpSpUls6P/ZdFPs3LOfeP3+TlBCPc6my/NxnJObWttnWJZFIWDFzDME3rmRbj1B4RLLxmeCg2+zfsxPnYi4y5bZ29owcPwlrmyKkpKSwY/MGRg3uy7Z9hzEyMpZ7HEHPImgxcZd0OiPz48N5d0xtRVp6Jr/M2E9sQirD2lXgT6/2lO+/nsSUdEIi4nDovFJmfb2alWbUzxU5eu2Z3GPNiXPRYqxcs046rayc90cRy9ONwGv83LETrqXcSM/IYMXSxQwb2Idtuw+gpaUNgO/8uQReu4Ln7LlYWdtw5dJfzPeeiamZGXXqNSjwmGdMncKjRw+Z5T0XM3Nz/jywnwF9e7Jr3yHMLSwKPJ7PfS/7FiApKYlixYvTqu1PjBs1PNv8RfPmcO3q38zwnoe1tQ2XL/3F3NkzMDU3p66C9+31wKv80rEzJUu5kZGRwfKlvgwd2Jvtuw9Kv3sA1WrUYornbOm0mpqaQuP6YOnaLWR+8ijbZ48fMX54P+o0aAzA65CXjOzfnWatfqJ7n0Ho6Orx4tkT1NTV5R5LanISNg5FqVq/Bf7zJsnMk0gkrPH2QEVVlb4ec9DU1uH0/m34TR/BxN83oaGpJVP/zIEd32crwncYUkESycYnEhMT8Jw8nnGTPVnvL/vPunGzljLTQ0eN4+C+XTx++ICKlavKPZb0jEzeRCdmKy9qY0iVktZU6L+B4BeRAAz3O8WLrf3pULcEAUfvkJkpybZs6+pF+ePcAxKSC+Yx1CoqKpiaKv6xzV+zeNkqmekpnrNpWr8m9+7epbx7RQBu3/qH5q3a4l6pMgA//dyBPbt2EHw3qMCTjeTkZE6eOIbP7364V6wEwIDBQzl96iQ7t29l8LARBRpPTr6XfQtQo1ZtatTK/cz11s1/aNm6DRXf79t2P3dg987tBAfdUXiy8fuy1TLTUz29aFK/BsF3g6jgXklarqamXiifp+FnJ0nbNvhjbWNLmfJZv4t1K5dQuXot+g4ZJa1jZVNEIbG4ulfD1b1ajvPevn7JswdBeCzegJWdEwAd+o1mYo9WBJ4/QfVGraR1Xz19yOn92xkzfzWTe7VRSKzf6rtMgAqQuBrlE4vmzKJ6zdpUqpLzl/6DtLRU9u3eia6uHkWLFVdILEVtjHiyqS/B63qxYUJzHCwNANBQyzqLTE5Ll9bNzJSQmp5J9VLWOa6rfFFzyjmbs/7oHYXEmpMXL57TqF5Nmjepz/gxIwl5+bLAtv0l8fFxAOgbGEjLypavwPkzpwl/8waJRMK1q1d4+fwZVavXKPD4MjLSycjIQF1DQ6ZcQ1ODG9cDCzyenHyv+zYn5Sq4c+7Tffv3FV48f0a16jULPJYP3z2DT757ANev/U2TejVo37opsz2nEBUVWeCxpaWlcfLoIZq0bIuSkhKZmZlcuXiOIrb2TBgxgF+a12Fo7878dfZUgceWnp51gqSq9vE3oayigqqaGk+Cb0nLUlOSCVjkyc99R6JvZFLgcQpf9l0nGy9fvqRXr14Fsq0TR//kwb1g+g8ZmWudv86doVHNitSvVoEdWzbgs2w1hkZGco/l6v0w+iw4QqvJuxm0+AQWRtqcXtgRYz1N7r+M5vmbGGb2qImhrgZqqsqM+aUSVsY6WBrr5Li+7k3cCH4RyeXgULnHmpPSZcowy2suy1b6M3X6LCIiIuje9VfevYsukO3nRiKRsHjhPMqWr4Bz0WLS8tHjJ+Lo5EyrJvWoUaksIwb1Y+zEqZQr717gMero6FKmbDlWr1hGePgbMjIyOHRgP3du3SIi4m2Bx/O573Xf5mbshKx927xRXaq6l2HowL6MnzSVchUKdt9KJBJ8F86lbHl3nIt+7KKtXrMWM7zmsWz1OkaMHs/doDsM6tuD1NTUAo3v4tlTxMfH0bhFVmvAu+gokhIT2b7Rn0pVauDtu5IadRrg6TGSm9evFWhsFjb2GJtZcmDTChLjY0lPS+P4ro3ERkcSG/0xMdu99nccS7hRpkqtAo0vr5SUlOTy+n/1XXejREVFsX79etauXZtrnZSUFFJSUmTL0lTQ+OzM8EvehIWyeMEcFvmt+uJyFSpVZt3WXbx7944De/5g6oTRrFq/FSNj+WbRxz4ZVxFEJFeCXxO0thddG7ry+57rdJp1kOUjGhG6cxDpGZmcuvGCI1ef5rguTXUVOtYtzpytV+Qa45fUrFVH+ncxoGzZcrRs1ogD+/byW/eeBRbH5+Z7z+LRg/usDNgkU759yybu3L7JgsV+WFpZ88/1a8z3moGpqSmVq1Yv8Dhnec9j+tSJNKlfBxUVFUqUdKVZ85YEB98t8Fg+973u29xs27yJ27dusuj3ZVhZW3M98FrWmA0zM6oU4L6d7z2TRw/usypgs0x5oybNpX87F3WhpGspWjdryF/nz1Dv/diJgnD44B4qV62BqZk5gHQsR7Va9Wjf6TcAirqUIOj2Pxzcu4OyFSoWWGwqqqr0Gj+LrUvnMOG35igrq+BS1h3XCh+7r2//fYGHt68zbmHu/ysK2/9zoiAPhZps7N+//4vznzx58tV1eHt74+npKVM2xmMK4yZOzXMc94PvEh0VSZ+uHaRlGRkZ3Lx+jd07tnLq0g1UVFTQ0tKmiK09RWztcStdll/bNuPg3t381qtvnrf1LRJT0gl6FoGzjSEANx6FU3XIZvS11VFXUyEiJolzPr8S+PBNtmV/qumCtoYam08GKzTGL9HS1qZoMRdePH9WaDEsmDOL82dPs3LtBiwsLKXlycnJLF/iy9xFS6hZO+sfaTGX4jy4f4/NGwIKJdmwtbPDP2ATSYmJxCfEY2ZmzvjRI7FRUH/5v/E97NvcJCcn4/e7Lwt8f6dm7brA+317L5hNAesKLNmYP2cW586eZuXajTLfvZyYmpljZWXFixfPCyQ2gDehr7lx9TLTvH2kZQaGRqioqGLv6CxT187BiTs3bxRYbNLtOpdgvE8ASQnxpKenoWdgxMJxfbF1LgHAg9uBRIS9YnzXZjLL+c+bjHPJMgybtTSn1RYokWwUorZts/oHJRJJrnW+toM8PDwYNWqUTFlsWv5Gx1esXJUN2/fKlHl5TsLewYku3XujopLz+iQSCalpim/uVFdToYSdMX8FvZIpj03M2raztSEVilngufFitmV7NCnFoStPiIhJUnicuUlNTeXp08dUcC/4bgmJRMKCObM5e+oEy9YEYP3ZP+z09HTS09NRVpb9nikrK8uM1C8MWtraaGlrExsTw8WLFxgxakyhxpOTwty3X5O1b9NQ+uyZFMoqKmRKFL9vs757szhz6gTL16zPU7L47l00b96EFeiA0aOH9mJoZEyV6h+7H9TU1CheshQvXzyTqfvqxXMs5HzZa35o6egCEP76JS8e36d556wTvUbtulKtYSuZunNGdKNdz6G4VSr4sVdCdoWabFhZWeHn50fbtm1znP/PP//g/pWDmIaGRrauj5T49Fxq50xbRwenT/rwATS1tNE3MMCpaDGSkhLZ4L+KGnXqYWpqRsy7d+zZuY234W+o17BJvraVF959anHoyhNehsdhbqjN+E5V0NNWZ/OJrGb0djWL8TYmiZdv43BzMGHBgLocuPSYk9dfyKzHycqAmm5FaDt1j9xj/JJF8+dSu249rKysiIqKYvXK5STEx9OqzU8FGgfAfK+ZHD18iPm+S9HR0SHy/bgHHV09NDU10dXVpYJ7JZb4LEBDQzOrqf3aVQ4f3M/w0eMLPF6Ai3+dRyIBBwdHXr54js/C+Tg4ONK6bbtCiedT39O+hawryF6++Pi9f/UqhPv3gjEwMMDSypoKFSuxeNF8NDQ1sbKy5nrgVf48sI+RYxS/b+d5zeDo4UMs8F2Kto6OdMyN7vvvXmJiAqtX+FGvQSNMTc0Jff2KZUt8MDQ0om79RgqPD7K6S44e2kej5q1RUZX9d/BLlx7MnjKWMuUqULZCZa5e/otLf51loZ+/3ONISUrkbdjHk6nIN6GEPH2Itq4exmaW3PjrFLoGhhiZWvD6+RN2+y+mTOValCyXdZWRvpFJjoNCjcwsMLHIeeB8gfuxGzYKN9lwd3fn+vXruSYbX2v1KCjKyio8f/aUwwf3EfMuGn0DQ0qWcsNvzQacnIvKfXs2pnpsGN8cE30tImKS+PteKHVGbuNFeNZodktjHeb2q4O5oTZhUQlsPnkX7xzGZHRv7MbryHhOXC+4JlmAN2/C8Bg3iujodxgZG1GmTDk2bNmBtbVNgcYBsGvnNgAG9ukuUz7FczYt3/+DnDV3AX6/+zBt4jhiY2OwtLJmwJDhtPulY4HHCxAfF88S30W8eROGgYEhDRo1YvCwkQV2/4Uv+Z72LcDdoCAG9P64b33mzwWgZeu2TJ/ljde8hfgt9mGKx1hiY7L27cChIwrkhm0fvnsDPvvuTfX0omWbn1BWVuHRwwf8eWAfcXFxmJqZ4l6xCl7zFqGjk/Ngb3m7fvUy4WGhNG3ZNtu8mnUbMHzcFLZu8Mdv0VyK2DswzWsRbmUryD2OF4/vsWTKMOn0nnVLAKhcrxldh00iNjqSPeuWEhcThb6RCZXrNqXJLz3kHoci/ejdKEqSQvxvfv78eRISEmjatGmO8xMSErh27Rp16tTJcX5u3uazZaOg2f2ypLBD+KKo/blfkVPYUtIzCjuEL1JX/a4v8ELpOz69Si/kbquv+c7DIyapYO6h8y2Cw2ILO4QvauKq+G4rwy6bvl4pD95t7iqX9RS0Qm3ZqFXry5co6ejo5DvREARBEITvzY/esvFdX/oqCIIgCP8FP3qy8X23+QqCIAiC8H9PtGwIgiAIgoL96C0bItkQBEEQBEX7sXMN0Y0iCIIgCIJiiZYNQRAEQVAw0Y0iCIIgCIJCiWRDEARBEASF+tGTDTFmQxAEQRAEhRItG4IgCIKgaD92w4ZINgRBEARB0UQ3iiAIgiAIggL9J1s2MjIL/7H0XxK+Z3hhh/BF/XbcLOwQcjW9sUthh/BFwW++76dblrMxLOwQcpWW8X3/btVUvu9zMyNttcIOIVdVHI0LO4RC96O3bPwnkw1BEARB+J786MnG952qC4IgCILwf0+0bAiCIAiCgv3oLRsi2RAEQRAERfuxcw3RjSIIgiAIgmKJlg1BEARBUDDRjSIIgiAIgkKJZEMQBEEQBIX60ZMNMWZDEARBEASFEsmGIAiCICiakpxe+TB9+nSUlJRkXpaWltL5EomE6dOnY21tjZaWFnXr1iUoKEhmHSkpKQwdOhRTU1N0dHRo3bo1ISEh+X77ItkQBEEQBAX7/J/+t77yq1SpUoSGhkpft2/fls6bN28eixYtYunSpVy9ehVLS0saNWpEXFyctM6IESPYs2cP27Zt48KFC8THx9OyZUsyMjLyFYcYsyEIgiAI/1GqqqoyrRkfSCQSfH19mTRpEu3atQNg/fr1WFhYsGXLFvr3709MTAz+/v5s3LiRhg0bArBp0yZsbW05ceIETZo0yXsc8nk7/9/WrfIjYPVymTJjYxP2HD0LgPf0SRw5tE9mvqtbGZav21Iw8fmv4vTJ4zx7+gQNDU3KlCvP0BGjcXBwlNY5deIYu//YQXBwEDHv3rF5+26Klygp91gaFDOhfjETzHTVAQh5l8zeO2+49TorE+5X1ZZazrIPXXoUkYDn0UfSaQNNVX6tYIWbpR5aasqExqaw/044V1/GyD1egMTEBDau9uPiudPEREfh7FKc/sPH4VLSDcj60W1eu4Ij+3cTHxdLcVc3Bo3ywN6pqNxjObl7E7cvnyP81XPU1DWwL+5Gy98GYG5jJ60jkUg4tmMdl48fIDEhDvtirrTrMxJLO0eZdT27f4fDW1bz4mEwyiqq2DgWpe+k+ahpaMg15rfhb1jt58vfly6QmpJCETt7xkzyxKWEKwBRkZGs9vMh8O9LxMfFUaZ8BYaM8qCInb1c4/hcRno6G9eu4PSxQ0RHRmJsakqjZq3p1KMfyspZjbYLZk3hxOH9MsuVcC2N7+pNCo3tg6zPzieHz64UAA2qls5xuX5DRtGxa0+FxpaX44pEImHVCj/27NpBXGwspUqXYbzHFJyLFlNobAH+qzhz8gTPn2XFVrpsOYaMGI39J7FFRkbg57uIK5f/Ii4ujvIVKjJ6/ETs7B0UGtu3ktcA0ZSUFFJSUmTKNDQ00Mjld//w4UOsra3R0NCgSpUqeHl54eTkxNOnTwkLC6Nx48Yy66lTpw4XL16kf//+BAYGkpaWJlPH2toaNzc3Ll68KJKNb+HoVJSFfmuk0yqfPeGxcrWaTJg6SzqtplZwT1i8fu0qv3TsjGspNzIyMli2xJchA3qzc/dBtLS1AUhKSqJsufI0bNyEWZ5TFRZLVGIaO/4J5U1c1pe9ppMxI2s7MPnwA17FZJXdfB3L6ksvpcukf/YU3gHV7dBSU8Hn7FPiUjKo7mDIkJr2TD3ykOfRSXKPefEcT54/ecSYKbMwMTXj1NFDTBwxgBWbdmFqZsEfmwPYs30ToybNwMbWnm3rVzNp5EBWbd2LtraOXGN5HPQP1Zv+hF3REmRmZvDnltWsmjGasYs3oKGpBcDpvVs4e2AHvw7xwMzalhN/bGDljFGMX7IZTa2s/f3s/h1WzxpL/Z+68FPvEaioqvL6+WOUlOU74j0uNpbh/bpTzr0Sc3yWYWhkzOtXL9HV1QOy/hlNHT8cVVVVZsxbjI6ODju3bmTssH6s3boHrffxKsKOzev4c+9ORk+eib2jMw/v3WXR7Kno6OrRtkMXab2KVWswauIM6XRB/XbjYmMY3q/b+89u+Sefnb60zs5Dp2WW+fvSeRbMnkateg0VHl9ejivr161hy8YAps3wws7eAf/VKxg8oDe79h1GR0e+v41P3Qi8xs8dO+Fayo30jAxWLF3MsIF92Lb7AFpa2kgkEsaNHIqqqirzfZaio6vLlo0BDB3QW1rneyOvZMPb2xtPT0+ZsmnTpjF9+vRsdatUqcKGDRtwcXHhzZs3zJo1i+rVqxMUFERYWBgAFhYWMstYWFjw/PlzAMLCwlBXV8fIyChbnQ/L55VINt5TUVHBxNQ01/nq6upfnK9IS5avlpmeNsOLRvVqEBwcRAX3SgC0aNUGgNevXik0lhuvZB+h/sfNMBoUM6GoqY402UjPkBCTnJ7rOoqaahNw9RVPIrMSi313wmlSwgwHYy25JxspKcn8dfYkU719KF3OHYCuvQdy+fxpDu3ZSbe+g9m7czO/dutDjToNABg9aSadW9fnzLHDNG/7s1zj6Tdlgcz0r4M9mNarNSGP7+NcqhwSiYRzB3fSsP1vlKlaB4BOQycyrVdbbpw/TrXGWft537ql1GzengbtukrXZWZtK9dYAbZtXIuZhQXjpsyUllla20j/Dnn5nOA7t/DfshuH9y1Bw8dOon2zupw6dpgWbdrLPaYPgu/cpGqtulSpXjsrLisbzhw/zIN7sgPc1NTUMTYp+N9u1mdnybgpH09SPv3sgGxx/XXuNOXcK2NtI/99+bmvHVckEglbN2+gZ5/+1G+YdWbrOWsOjevX5MifB2n/S0eFxbZ42SqZ6Smes2lavyb37t6lvHtFXr54zp1bN9n6xz6c3reyjJs4lab1a3Ls8J+0aSff3+33xMPDg1GjRsmU5daq0axZM+nfpUuXplq1ajg7O7N+/XqqVq0KZE+CJBLJVxOjvNT5nBgg+l7Iyxe0a1aPjm2a4DlxDK9DXsrM/yfwKm0a16ZL+xbMmzWN6KjIQooU4uOzuiz09Q0KLQYAJSWoam+IhqoyD98mSMtLWOji196Vea1K0KtKEfQ1ZHPaB28TqGJviI66CkpkrUNNWYngN/FyjzEjI4PMjAzU1WV/jOoamty9dYOw16+IjoygQuVq0nlq6uqULleR4Dv/yD2ezyUnZr1nbb2ss92oN6HEvYvCpWwlaR1VNXWcS5Xl2f07AMTFRPPi4V10DYz4feJApvVqg9+UoTwJviX3+C6eP0PxkqXwnDia9s3q0L9bBw7t/UM6Py01FUDm81VRUUFNTY07N2/IPZ5PlSpTnn+u/U3Ii2cAPHl4n6BbN6hUrZZMvVs3rtGxRV16/9oK3zmevIsumN9u1mfniufEUe8/u19kPrvPRUVGcOWv8zRr9VOBxPe5z48rr16FEBkRQdVqNaR11NXVqeBeiVsK3re5xmaQFVvqh++dRvbv3c0b1ws0tryS1wBRDQ0N9PX1ZV65JRuf09HRoXTp0jx8+FA6juPzForw8HBpa4elpSWpqalER0fnWievCj3ZSEpK4sKFC9y9ezfbvOTkZDZs2KDwGEqWKsNETy/mL1nJ2InTiYqMYHDvrsS8ewdAleo1mTxzDj7L/Bk0fCz3795h5MDe0i98QZJIJCxaMJdy5d0pWsylwLcPUMRQk9Ud3Fj3axl6VC7C4nPPeB37vgslNI7lfz3H+8QTtl5/jZOxNh4NnVD9pHl/6YXnqCjBil/cWNupDD3fryM8Xv6fp7a2DiXdyrA1YBWREeFkZGRw6ugh7t+9TVRkBNFREQAYGsuOMzE0MlZ4QimRSNgXsBTHkmWwsnMCIPZd1jb1DGXj0TMwJjY6CoCoN68BOLZ9HVUbtqLv5PkUcXJhxfSRvH0tmyT/W6GvQ9i/ewc2tnbM8V1By59+YanPXI79mTUOws7BEQtLa9YsX0xcbCxpaWls3eBPVGQEUZERco3lcx269qJuw6b07dyWFrXdGdyzI207dKVeo49nc5Wq1mDcNC/mLllN3yGjeRAcxPihfQvkt/vxs7P/5LObI/3sPnfsz/1o62hTq67iu1A+l9NxJTIia/+ZfNb6YmJiIp1XULEtXjiPsuUrSMeKODg4YmVlzbLffYiNjSEtLZX1a1cTGRFBRMTbAostXwrh0tfPpaSkEBwcjJWVFY6OjlhaWnL8+HHp/NTUVM6ePUv16tUBcHd3R01NTaZOaGgod+7ckdbJq0LtRnnw4AGNGzfmxYsXKCkpUatWLbZu3YqVlRUAMTEx9OzZk27duuW6jpwGy6SkKOc50wOoWuOTM6GiUKpMWTq3bcaRQ/vo2KU79Rt/PHg5FS1GCddSdGjViMsXzlK7fqM8b0ce5nnP5NHD+6wJ2Fyg2/1UaGwKk/58gI66CpXsDOhXzY7Zxx/xOjaFK8/fSeuFxCTzJDIR37YlKWejz7X3A0B/LmuFjroK3iceE5+SjrutAUNqOTDr+CNC3iXLPd4xU2bj4z2d39o2RllFhaIuJajbqBmPHtyT1lH67FcsQZKtTN52r/Eh9PkThsxemm3e5y2UEj42W2ZmZgJQrXFrKtdvDkARJxce3grk71N/0qJrf7nFKMnMxKVkKfoMHA5AseIlef7kMft376Bx89aoqqoxfc4iFsyeRtvGNVFWUcG9UhUqV6sptxhyc/bkEU4dO8T46d7YOxbl8cN7rFw8HxNTMxo1bw1AnYZNpfUdnIpRrEQpurdvyt8Xz1FTwf/Uc//sttP4fXyfOnJwDw0at5A5Wy8oXzquZPsufkMT+r8x33sWjx7cZ2XAx0G9qmpqeC9czOzpk2lUuxoqKipUqlKNajVqfWFNP54xY8bQqlUr7OzsCA8PZ9asWcTGxtK9e3eUlJQYMWIEXl5eFCtWjGLFiuHl5YW2tjadO3cGwMDAgN69ezN69GhMTEwwNjZmzJgxlC5dWnp1Sl4VarIxfvx4SpcuzbVr13j37h2jRo2iRo0anDlzBjs7u6+vgJwHy4yeMJkxHt8+SFJLSxvHosUIefk8x/kmpmZYWFkT8vLFN2/jW8zznsW5M6dZtXYjFhbZL2UqKBmZEmkrxNOoJByNtWlSwox1f2e/0UtMcjoRCWlY6GVdvWKuq07j4qZMOHhPOsbjxbtkXMx0aOhiQsDf8h9zYmVjy7yl/iQnJZGYEI+xqRneU8dhaWWNkXHWWVt0VCTGpmYf446OztbaIU+71/gSdPUvBs9cgqGJubRc39AEgNjoKPSNPp5RxsdEo2eYNUhL3yirjkURB5l1mhexJzrijVzjNDY1w97BSabMzsGRc2dOSKddSriyauNO4uPjSE9Lw9DImMG9OuNSspRcY/ncGj+f960bWScDjs7FCA8LZftGf2my8TkTUzPMLa15HaL4327WZ+csU2bn4CTz2X1w659AXj5/xpRZC7LNU7TcjisfxqhFRERgavbxOxoVFYWxiUmBxLZgzizOnz3NyrUbsh3zSrqWYtOOPcTHxZGWloaRsTG9unakhKtbgcSWX4Vxu/KQkBA6depEREQEZmZmVK1alcuXL2Nvn3Wl2Lhx40hKSmLQoEFER0dTpUoVjh07hp6ennQdPj4+qKqq0qFDB5KSkmjQoAEBAQGoqKjkK5ZC7Ua5ePEiXl5emJqaUrRoUfbv30+zZs2oVasWT548ydM6PDw8iImJkXkNHTX+X8WVmprKi2dPMTExy3F+zLt3vH0ThnEBDRiVSCTM9ZrJ6ZPHWb56HTZFihTIdvNKSQnUcrkKQlddBWMdNd4lZQ0YVVfN+spJZC9QIVMCygpuSdDU0sLY1Iy42Fiu/32RqjXrYmltg5GJKdevXpLWS0tL4/Y/1yjpVk7uMUgkEnav9uH2lXMMnO6LiYW1zHxjCyv0DI15cOuatCw9LY3HQTdxKJ51EDU2t0Lf2JTw17L/MN+GhmBsJt8k1K1MOV6+HxPxQcjL51hYWmWrq6urh6GRMSEvnvPg3l1q1K4n11g+l5KcLL3E9QNlZRUkksxcl4mNecfb8DCMc/lty1POn92zHD+7w/t341LCFedixRUe1wdfO67Y2BTBxNSUK5cvSsvS0lK5HniVMmXLKzy2+d6zOHPyBH6r1mJtk/sxT1dPDyNjY148f0bw3SBq162v0Ni+VWHc1Gvbtm28fv2a1NRUXr16xa5du3B1dZWJafr06YSGhpKcnMzZs2dxc5NN1jQ1NVmyZAmRkZEkJiZy4MABbG3zP4C5UFs2kpKSUFWVDcHPzw9lZWXq1KnDli1fv49FTtcXJ8am5SuOZb7zqV6rLhaWVkRHR7HBfyUJCfE0bdmGxMREAlb5Ubt+I0xMzQgLfcVqv8UYGBpRu4D6Vud6zeDI4UMs9F2Kto6OtE9SV1cPTU1NAGJi3hEWGsrbt+EAPH/2FMg6OzE1ld+B9Zeyltx8HUdUYiqaaipUtTekpLku808/QUNVmXalLbj6MoZ3SWmY6qjToZwV8SnpBL7vQgmNSSYsNoWeVYqw9fpr4lMycC9igJuVLovOPJVbnJ8KvHIRiURCETsHXr96wVo/H2xsHWjUog1KSkq0/aULOzb6Y1PEHmtbO7ZvWIOGhhZ1P+k+k5fdq324fv4EvSZ4oaGlTez7wYpa2rqoaWigpKRE7Za/cHLXJsysimBqVYSTuzahrqFB+VpZXXZKSkrUa/MrR7evw9qhKDYORbl65gjhr57TfcyML20+39r/+hvD+nZjc8Bq6jZowr27tzm09w9GTpgmrXP25DEMDI0wt7Ti6eOH+C2aS43a9ahYJX99uvlVpUYdtq1fjZmFJfaOzjx+cI892zfSuEXWFTtJiYlsWrucGnUbYmxiypvQ1wSsXIKBgSHVayv+H1L7X7sxrO9vn312uxg5QbbVNSEhnnOnjjNg2BiFx/Sprx1XlJSU6NSlG+v8V2FnZ4+tnT3r/FehqalJ0+YtFRrbfK+ZHD18iPm+S9HR0SHyfWw6nxzzTh47gqGRMZZWVjx6+ACfed7UrteAqtVrfGnVheYHfw4bShLJ5+eYBady5coMHTqU3377Ldu8IUOGsHnzZmJjY/N9W9SwfCYbnhPHcPNGIDHvojE0MsbVrQy9BwzFwcmZlORkJo0dxsP794iPi8XE1Izy7pXpPWAI5jmcoeSFjnr+mp8qls355lzTZnjRqk3WyPUD+/bgOXVitjp9Bwym/8Ah+dreoF23c53Xp0oRXC31MNRSJSktgxfRyRy6G86dsHjUVJQYUdsRB2NNtNVUeJecTnBYPH/cCiMq8eM+sdBTp2M5K1zMdNBUU+ZNXCp/Br/lr6fRuW73g+mN8z8o9tzJowSsXELE2zfo6RtQo04Duvcbgs4n94rYvHYFh/fven9Tr9IMGuUhvZQzP4LfxH5x/uj2tXMs7zjYg8r1m0njObZjHZeO7ScpIR67YiVp13ekdBDpByd3b+KvI3tIio/DysGZlr8NxKlkmS9uv5yNYd7fzHuXLpzFf/liQl6+wMrKhp87/UaLTy4J3r19Mzs2B0i7oho3a0XXXv3zfT+LtIz8HYoSExLYsNqPi+dO8S46ChNTM+o0akaXnlnbTklJxnPCCB4/uEdCfBzGJmaUqVCJ7n0HY/YN3ZBqKvlvCM767Hw/+ey6yXx2AAf37mSZzzx2HDolvX/JtzDQyt+5Y16OKx9u6rX7j+3ExcbiVroM4zym5HtwekY+/81UKeeaY/kUz9m0fB/b9i0b2bR+HVGREZiamdGsZRt69xuAmpp6vrYFYKiVv2Pytyg65rBc1vNogfxPggpCoSYb3t7enD9/nj///DPH+YMGDWLFihXSAXF5ld9ko6DlN9koaF9KNgrbtyQbBelryUZh+5Zko6DkN9koaN+SbBSk/CYbBSm/yUZBK4hko9jYI3JZz8P5Tb9e6TtUqL8eDw+PXBMNgGXLluU70RAEQRCE742Sknxe/6++71RdEARBEIT/e99vu5sgCIIg/EcUxqWv3xORbAiCIAiCgv3guYboRhEEQRAEQbFEy4YgCIIgKJhyLjc+/FGIZEMQBEEQFEx0owiCIAiCICiQaNkQBEEQBAUTV6MIgiAIgqBQP3iuIZINQRAEQVC0H71lQ4zZEARBEARBoUTLhiAIgiAo2I/esvGfTDZ0Nb7vt/W9f+c8Gxcv7BBy5XPhaWGH8EUjajoUdghf9DwisbBDyJWBdv4eSV/QNFS/76c162l+v8e91PTv/YGait+33/txX9FEN4ogCIIgCAr1/abCgiAIgvAfIbpRBEEQBEFQqB881xDdKIIgCIIgKJZo2RAEQRAEBRPdKIIgCIIgKNQPnmuIbhRBEARBEBRLtGwIgiAIgoKJbhRBEARBEBTqB881RLIhCIIgCIr2o7dsiDEbgiAIgiAolGjZEARBEAQF+8EbNkSy8cH1a1fZEOBPcHAQEW/fssB3KfXqN5TOdy9TIsflho8cS7eevRUaW+CH2O5mxbbQdyn1GjTMse4sz6ns/mMHo8d50OW37gqN64OM9HQ2rV3B6eOHiI6MxNjElIbNW9Opez+UlbMaz5rVLJvjsr0HjeTnzj3kFktNR0NqOhph/P6hXmFxKRy5F8HdNwkoK0FLVzNKWehioqNOcloG998msC/oLbHJ6dJ16Gmo0NbNghLmOmioKhMen8qx+xH88zpObnF+KjExgY2r/bh47jQx0VE4uxSn//BxuJR0A0AikbB57QqO7N9NfFwsxV3dGDTKA3unonKP5f6dGxzZtYlnj+8TExXBkElzqVCtjnR+r5ZVc1zul55DaNa+KwBpaans8P+dK+eOk5qSgmvZinQdNA5jU/N/FVvQzevs276BJw+DiY6MYNyMBVSpWQ+A9PQ0tq5dzvUrF3gT+gptHV3KVKhC175DMTY1k64jLTWV9St8uXDqCKmpKZQuX5l+IyZgYmbxr2LLTWJiApvW+HHpfNa+dSpWnH7DPu7bpMREAlYu5vKF08TFxGBuaU3rnzvRvG0HhcTzqQD/VZw+eZznz56goaFJ6bLlGTpiNPYOjjL1nj55zNLFC7keeBVJZiZOzkXxmueDpZW1wmJbu9KPdauXy5QZm5iw7+hZIOs3sW7VMvbv+YO4uFhcS5Vm1PjJODrL/zchLz96N4pINt5LSkrCpXgJWrdtx9hRw7LNP3rqvMz0xQvnmDFtMvUbNVZ4bMlJSbi4vI9tZPbYPjh98gR3bt/CzPzfHdTza8fmdfy5byejJ83E3tGZB/fu4uM1FR0dPdp26ALA5n0nZZa5dvkCvnOmU6NOzknTt3qXlM7+oHDeJqQBUMXOgL5VbZl76gnvktKxNdTkyP0IXsWkoK2mTLsylvSvWoT5Z55J19GtojVaqiqsuvyS+JQMKtrq07OyDfNPPyUkJkWu8QIsnuPJ8yePGDNlFiamZpw6eoiJIwawYtMuTM0s+GNzAHu2b2LUpBnY2Nqzbf1qJo0cyKqte9HW1pFrLCnJSdg6FaNmo5b4eXlkm++z8ZDM9K1rlwj4fTbuNepJy7au8uHm3xfoP24munoGbPf/ncWeo5nmG4Cyyrc/XTMlOQkHZxfqN23N/OljP5uXzJOH9/j5tz44OLmQEB/HWr8FzJk8knkrNknrrfVbwLVL5xk5xRs9fQPWL/fBa+II5q3YhMq/iC03S+Z68vzpI0ZPmoWxqRmnjx1i8qgBLNuQtW9XL53P7RvXGD15NhaW1ty4eollPt4Ym5hRtVa9r2/gX7geeJVfOnamZCk3MjIyWL7Ul6EDe7N990G0tLQBCHn5gr49u9C6bXv6DRyCrq4eT588Rl1DQ6GxATg6FcVn2RrptLLKx17/LevXsn3LBiZOm4WtnQPr/VcycnBftuw6iLaOfH8TgnyIZOO9GrVqU6NW7Vznm35ydgRw5vQpKlaqQpEitooO7auxAYS/ecNcr5n4rVzDsMH9FR7Tp+4F3aRqzbpUrp4Vo4WVDWdPHObh/SBpHWMTU5llLl84Q5kKlbCyKSLXWO6ExctMH7z7lpqORjgYa3H5eQx+f72Umf/HzTDG1nPESEuV6KSs1g1HY222/xPK8+hkAI7ej6ReUWOKGGrKPdlISUnmr7MnmertQ+ly7gB07T2Qy+dPc2jPTrr1HczenZv5tVsfatRpAMDoSTPp3Lo+Z44dpnnbn+UaT5mK1SlTsXqu8w2MTGSm/7lyjhKl3TG3tAEgMSGe88cP0HfUNEqVqwxA39HTGdOzDXf/uYqbe84tI3lRoUoNKlSpkeM8HV09ps1fJlPWZ+g4xg/qxts3oZhZWJEQH8epw/sY5jGTsu5VABg+cRb9f23OretXKF8p9/f9LVJSkvnr3EmmePng9n7fduk1kMsXTnN4705+6zuEe0G3qN+0FWXKVwKgaeufObx/Fw/v31V4svH7stUy01M9vWhSvwbBd4Oo4J4Vz/KlvtSoWZthIz8mdzYFcMwDUFFVwcTUNFu5RCJhx9aNdOvZjzr1GwEwydOLNo3rcPzIIdq0V3yr0Lf4wRs2xADRbxEZGcGF82dp81P7wg4FgMzMTCZPHEe3nr1xLlqswLdfqnR5/gn8m5AXzwB48vA+QbduUKlqrRzrR0dF8vfF8zRp8ZNC41ICKtjoo66ixLOopBzraKkpkymRkJSWKS17HJlIhSL6aKspS9ehqqzMo4hEuceYkZFBZkYG6uqyZ4rqGprcvXWDsNeviI6MoELlatJ5aurqlC5XkeA7/8g9nvyIiY7k1tW/qNW4lbTs+aN7ZKSnU6pCFWmZkYkZNnZOPLp3u0DjS0iIR0lJCR1dPQCePAgmPT2dshU/JjzGpmbYOjhzP+iW3Lf/Yd+q5bBvg27fAMC1dHn+/usMEW/fIJFIuHX9Kq9fPqdCZfkmPnkRH5/VTWhgYABkHVf+On8WO3sHhg7sQ5N6NejZtSNnTp0okHhCXrygbdN6dGjdhGkeY3gdknWiEPoqhKjICCpV/fgZqaurU65CRe7c+qdAYvsWSkpKcnn9vyr0lo3g4GAuX75MtWrVKFGiBPfu3WPx4sWkpKTQtWtX6tev/8XlU1JSSEmRPdtMQx0NBTbzHdy3Fx1tHeo3VHwXSl4ErF2NqooKnbr8Vijb/6VrLxIS4unXpS3KyipkZmbQvd9Q6jZqlmP9E4f3o6WtLT1TlzcrfQ1G13FAVVmJlPRM1lwJISwuNVs9VWUlWpcyJ/BlLMnpH5ONdX+/omdlG+a2LE5GpoTUjExWXw4h4n3XjDxpa+tQ0q0MWwNWYevgiKGRCWdPHOH+3dtYF7EjOioCAENjY5nlDI2MCX8TKvd48uPiyT/R1NLBvXpdaVlMdCSqqmro6OrL1DUwMiYmOrLAYktNTWHz6iXUatAUbR1dAN5FR6KqpoaunmxshkbGvIuSf2za2jqUKFWGbetXYWuftW/PnTzCg/f7FqD/8PEsmedJj/ZNUFFRRUlZiWHjplGqTHm5x/MlEokE34VzKVveHeeiLgBERUWSmJjI+rVrGDB4GEOHj+bSxQuMHz2M5asDqFCxssLicXUrwyRPL2zt7YmOjGS9/0oG9u7Khu37iIzM+k0Ym8i2shmZmBAW+lphMQn/TqEmG0eOHKFNmzbo6uqSmJjInj176NatG2XLlkUikdCkSROOHj36xYTD29sbT09PmTKPSVOZOGW6wuLet3cXzVq0VGhCk1d3g+6wddNGtuzYVWhZ79mTRzh17BDjpnlj71iUJw/vsfL3+RibmtGoWets9Y8d2ku9xs0V1u8bHpfCnFNP0FJToZy1Hl3drfn9/HOZhENZCXpWskFJSYkdN8Nklm/paoa2mgpLLjwnISWDMtZ69Kpsg+/554TGyn/Mxpgps/Hxns5vbRujrKJCUZcS1G3UjEcP7knrKCG7byVIspUVtPMnDlK1buNsZ+45kUgKLt709DQWzfQgMzOTvsMnfLW+BMUN3hs9eTaL50yne7usfetcrAR1Gjbj8ft9e+CPLdy/e5sp3osxt7Tizj/XWb7IC2MTU8pV/PYup/ya7z2TRw/usypgs7RMkikBoHbd+nT+rQcALiVKcuvmDXb/sV2hyUbVGp+0ihaFUmXK8mvbZhw+uI9SpctklX+2zyQSyXd95v8dh1YgCrUbZcaMGYwdO5bIyEjWrVtH586d6du3L8ePH+fEiROMGzeOOXPmfHEdHh4exMTEyLxGj8s+sE1ebgRe4/mzp7Rt94vCtpEfN64HEhUVSfPG9alUrhSVypUi9PVrfBbMpUWTL7cKyYv/Mh86dOlF3YbNcHQuRoOmrfipQ1d2bPTPVvfOzeuEvHhG05btFBZPhgQiEtJ4+S6ZA3ff8jomhTrOH1sGlJWgV+UimOiosfSvFzKtGqY6atRxNmbz9dc8eJvIq9gUDt+L4OW7ZGo7GSkkXisbW+Yt9Wf38Uts2HUE39WbSU9Px9LKGiPjrD7r6M/OvGOio7O1dhSkB3f+ISzkObUat5EpNzAyIT09jYT4WJny2HfR6BspPt709DQWek4gPPQ10+Yvk7ZqABgamZCelkZ8nGxsMdFRGCgoNisbW+Ys8eePo5cI2HkEn1WbyUhPx8LKmpSUZDasXkKfIaOpUqMOjs4utGr/K7XqN2H3tg0KiScn8+fM4tzZ0yxbsx4LC0tpuaGRISqqqjg6O8vUd3B0Iiy0YFvVtLS0cXIuRsjL55i8H/8VFREhU+ddVBTGxiY5Lf5d+NG7UQo12QgKCqJHjx4AdOjQgbi4ONq3/zgOolOnTty69eW+VA0NDfT19WVeimxx2LvnD0q6lsKleM6Xwha0Fq1as33XPrbu3CN9mZmb061Hb/xWrPn6CuQgJTkZJWXZr5KyigqSzMxsdY8e3EOx4q44FSteILF9oKac9SP9kGiY6aqx9MILElMzZOu9H/Eu+Wz5TIlE4eflmlpaGJuaERcby/W/L1K1Zl0srW0wMjHl+tVL0nppaWnc/ucaJd3KKTii3J0/vh/7oiWwc5IdI2RftAQqqqoE3fhbWvYuKoJXL55QtERphcb0IdEIffWSaQuWo2dgKDPfyaUkqqqq3Ay8LC2LjnzLy2ePKV6qjEJj+7Bv4+NiuX41a99mpKeTnp6OktJnvx1l5Rx/O/ImkUiY7z2TMyePs2zVOmw+G6ytpqaOq6sbL549lSl/8fyZQi97zUlqairPnz3FxNQMK5siGJuYcvWK7G/in+vXcCtTrkDjEvKu0MdsfKCsrIympiaGhobSMj09PWJiYgpk+4mJCbx88UI6/fpVCPfvBaNvYIDV+x9WfHw8J44dZeSY8QUSU26xvfosNkND2TNuVVVVTExNcXB0KpD4qtSow7YNqzG3sMTe0ZlHD+6xe/tGGjeXPetNSIjn/Olj9B0yWmGxtHI14+6beKKT0tFQVca9iD7FzLRZ9tdLlJWgd5Ui2BposvLSS5SUsu6pAZCYmkGGBN7EpRAen8qv5azYe+cNCakZlLHSo7i5DisvvfzK1r9N4JWLSCQSitg58PrVC9b6+WBj60CjFm1QUlKi7S9d2LHRH5si9ljb2rF9wxo0NLSo2zjnMTH/RnJSIuGhIdLpiDevefHkATq6+piYZ531JiUmcPXCKTr2zn4ZtraOLrUatWK7/+/o6hmgo6fPDv8lFLF3xrVcpX8VW1JSImGvPu6D8NDXPH10H109fYxNzVgwfTxPHt5jopcvmZkZ0vEuunoGqKmpoaOrR/1mbVi/3Bc9fUN09fTZsMIXO8eilPlkQKs8Bf59ESQSbGwdCH31grXLs/Ztw+ZtUFVVw62cO2uX+6CuoYG5hTV3bl7j1NGD9FHgb+SDeV4zOHr4EAt8l6Kto0NExFsAdHX10NTUBKBrj15MGjea8hUq4l6pCpcuXuDCuTMsX7NeobH5+c6neq26WFhaER0dxQb/lSQkxNOsZdZvokOn39i0bjW2dnYUsbVn47rVaGhq0qhpC4XG9W/8P7dKyEOhJhsODg48evSIokWzbsRy6dIl7OzspPNfvnyJlZVVgcRyN+gO/Xt/vAnWovlZ3TctW7fFc1bW38eOHEKChCbNCvYLfTfoDv16ZY+tVeu2eM7+cjdTQRg4cgIbVvvht9CLd9FRGJua0bz1z3TuKXsJ7tkTR0ACdRvK/5/kB3oaqvzmbo2+pirJ6Zm8jklh2V8vuf82AWNtNcpYZV2ZMKGBbCK2+PxzHkUkkimBFRdf0LqUOf2q2qKhqkxEQiqbAl9z902CQmJOiI8jYOUSIt6+QU/fgBp1GtC93xBUVbNuTPZzlx6kpCTjt8jr/U29SjPLZ7nc77EB8OxhMPMmDpZOb1uzGIAaDZrTe+RUAK6cOw5IqFIn5wHSnfqOQEVFheVzJ5GWmkLJMhUZPnLBv7rHBsDj+3eZNurjdypg+SIA6jZpScfu/bl6MeuGT6P7dpJZznPRStzKVQSg5+DRqKiosnDGBFJTkildvjIes6cr5B4bAInxcaxf9X7f6hlQvU4DuvX9uG/HT5vL+lW/s2DmROJjYzG3tOK3vkNo1kbx3bS7dm4DYEAf2Zv/TfX0omWbrCvF6tVvxITJ01jvv4qF87yws3dkzoLFlCvvrtDYwt+8wXPSOGLeRWNoZEwptzKsWLdF2qLSuXsvUlKSWThnFvFxsZR0K8Oipau+63ts/OC5BkoSieTzFuMCs2LFCmxtbWnRIud/3pMmTeLNmzesWZO/7oD4lEJ7S3nyvX/p3ijgxlXy4nPh6dcrFaIRNR0KO4QvCnuXXNgh5Mrg/V1fv1caqopJSOTFXL/wB6znJiU94+uVCpG5nuK/e3V9L8plPWdGFPxl0fJQqC0bAwYM+OL82bNnF1AkgiAIgiAoynczZkMQBEEQ/qu+9xZtRRPJhiAIgiAo2I8+QFTcrlwQBEEQBIUSLRuCIAiCoGA/eMOGSDYEQRAEQdGUf/BsQ3SjCIIgCMIPwNvbGyUlJUaMGCEtk0gkTJ8+HWtra7S0tKhbty5BQUEyy6WkpDB06FBMTU3R0dGhdevWhISEkB/5btnIyMggICCAkydPEh4eTuZnt9U9depUflcpCIIgCP9phd2wcfXqVVatWkWZMrK35p83bx6LFi0iICAAFxcXZs2aRaNGjbh//z56elk3QRwxYgQHDhxg27ZtmJiYMHr0aFq2bElgYGCeb4iX75aN4cOHM3z4cDIyMnBzc6Ns2bIyL0EQBEEQZBXmg9ji4+Pp0qULq1evxsjo4+MtJBIJvr6+TJo0iXbt2uHm5sb69etJTExky5YtAMTExODv78/ChQtp2LAh5cuXZ9OmTdy+fZsTJ07kOYZ8t2xs27aNHTt20Lx58/wuKgiCIAg/JGU5tWykpKSQkiJ7l2cNDY0vPoB08ODBtGjRgoYNGzJr1ixp+dOnTwkLC6Nx44+PHtDQ0KBOnTpcvHiR/v37ExgYSFpamkwda2tr3NzcuHjxIk2aNMlT3Plu2VBXV5c+y0QQBEEQhILj7e2NgYGBzMvb2zvX+tu2beP69es51gkLCwPAwsJCptzCwkI6LywsDHV1dZkWkc/r5EW+k43Ro0ezePFiCvGRKoIgCILwf0Ve3SgeHh7ExMTIvDw8PHLc5suXLxk+fDibNm2SPsk3t9g+JZFIvtplk5c6n8pTN0q7du1kpk+dOsXhw4cpVaoUamqyD7DZvXt3njcuCIIgCD8CeQ0Q/VqXyacCAwMJDw/H3f3jU3ozMjI4d+4cS5cu5f79+0BW68WnT1gPDw+XtnZYWlqSmppKdHS0TOtGeHg41avn/aFweUo2DAwMZKZ/+umnPG+gMCSlft9PGNTR+L6fHpmYml7YIeRqcoPvuwuv4sQ/CzuEL7ozv1Vhh5Cr8Njv92nDAOkZmV+vVIjkNSZAEXQ1xC2dCkODBg24ffu2TFnPnj0pUaIE48ePx8nJCUtLS44fP0758uUBSE1N5ezZs8ydOxcAd3d31NTUOH78OB06dAAgNDSUO3fuMG/evDzHkqdvwLp16/K8QkEQBEEQZClR8Nmgnp4ebm5uMmU6OjqYmJhIy0eMGIGXlxfFihWjWLFieHl5oa2tTefOnYGsxobevXszevRoTExMMDY2ZsyYMZQuXZqGDRvmOZZ8p5v169dn9+7dGBoaypTHxsbStm1bcZ8NQRAEQfjM99ryNG7cOJKSkhg0aBDR0dFUqVKFY8eOSe+xAeDj44OqqiodOnQgKSmJBg0aEBAQkOd7bAAoSfI50lNZWZmwsDDMzc1lysPDw7GxsSEtLS0/q1OIt3HfbzcAfP/dKE/eJhR2CLky08tbX2VhEd0o3050o/w7RYy1CjuEXKl8r/9p39NWV3x8rVddlct69verJJf1FLQ8t2zcunVL+vfdu3dlLnnJyMjgyJEj2NjYyDc6QRAEQfgP+NEfMZ/nZKNcuXLSS2/q16+fbb6WlhZLliyRa3CCIAiC8F/wg+caeU82nj59ikQiwcnJib///hszMzPpPHV1dczNzfPVfyMIgiAIwo8hz8mGvb09QLYHrwmCIAiC8GU/+iPm8301yoYNG744v1u3bt8cjCAIgiD8F/3guUb+k43hw4fLTKelpZGYmIi6ujra2toi2RAEQRCEz/zoA0Tz/WyU6OhomVd8fDz379+nZs2abN26VRExCoIgCILwfyzfyUZOihUrxpw5c7K1egiCIAiCkNWNIo/X/yu53bBeRUWF169fy2t1giAIgvCfIQaI5tP+/ftlpiUSCaGhoSxdupQaNWrILbDCtHHdalb6+fJLp64MH5316N6aFUvlWHfQsNF07tZLofEEXrvKhgB/7t4NIuLtWxb5LqVeg4/3pJdIJKxcvpRdf+wgLjYWt9Jl8Jg0FeeixRQST9DN6+zbvoEnD4OJjoxg3IwFVKlZD4D09DS2rl3O9SsXeBP6Cm0dXcpUqELXvkMxNv14uXRaairrV/hy4dQRUlNTKF2+Mv1GTMDEzEKusa5d5UfA6uUyZcbGJuw9epb09DRWL1/C5b/OE/oqBB1dXSpWrkr/ISMxNTPPZY3/zsjmxRnVvIRMWXhsMu4Tj8rU6VLDAQMtNW48j2by9ls8CIuTWaaCoxHjWpakvIMRaRkS7r6KoduySySnyfdqsQD/VZw+eZznz56goaFJ6bLlGTpiNPYOjtI6nlM8OHRgr8xybqXLsHbjdrnGkpPExAQ2rfHj0vnTxERH4VSsOP2GjcOlZNZzH1rWLpfjcj0HjqB9px5yjSXoZiB7tm/g8YOs38WEmQup+v53AXDp3EmOHtjF4wf3iIt9x6LVW3EqWlw6Py42hq0BK/jn2mUiwt+gb2BIlRp16dxrIDq6ejlt8l9Z937fPnuatW/LlMvatw6f7FuJRMKqFX7s2ZV1bClVugzjPaYo7Njyqa8d906eOMaundsJvhvEu3fv2LZzD8VLlFR4XMK3yXey0bZtW5lpJSUlzMzMqF+/PgsXLpRXXIUmOOg2+/fsxLmYi0z5viNnZKYvX7zAnJlTqFO/kcJjSkpKwsWlBK3btmPMyGHZ5gesXcOmDQF4zvLG3t6B1atWMKBfL/YeOIyOjq7c40lJTsLB2YX6TVszf/rYz+Yl8+ThPX7+rQ8OTi4kxMex1m8BcyaPZN6KTdJ6a/0WcO3SeUZO8UZP34D1y33wmjiCeSs2yf1+LY5ORVnkt0Y6raKS1XuYnJzMw3t36d67P0WLFScuLpYli+biMXoIqzfskGsMn7r/OpZOSy5KpzM+eWLAwIZF6VvPmVGbbvA0PJ5hTV3YMrQ6dWacJCEl6zb8FRyN2DioGn7HHjJ1521SMzJxtdEnM18PHsib64FX+aVjZ0qWciMjI4PlS30ZOrA323cfREtLW1qvWo1aTPGcLZ1WU1OTfzA5WDLXk+dPHzF60iyMTc04fewQk0cNYNmGXZiaWbBxzwmZ+teuXOD3uZ7UqJP3B0jlVXJyMo7OLjRo2pq508bmMD+Jkm7lqFG3EX4LZmabHxX5lqiIt/QYMAJbeyfevgllhY8XUZFvGe85X+7xXr+WtW9d3+/bZUt8GTKgNzt3H0RLO2vfrl+3hi0bA5g2wws7ewf8V69g8IDe7Np3GB0dHbnH9KmvHfeSkpIoW64CDRs3Zeb0KQqNRR5+7HaNb0g2FH2fDYlEUmijdhMTE/CcMp5xkzxZ779SZp7JJ2flABfOnqJCxcrYFLFVeFw1a9WmZq3aOc6TSCRs2bSB3n0H0KBhYwBmzp5Dg7o1OHzoID93+FXu8VSoUoMKVXJuxdLR1WPa/GUyZX2GjmP8oG68fROKmYUVCfFxnDq8j2EeMynrXgWA4RNn0f/X5ty6foXylarLNV4VFRVMTE2zlevq6skkIQDDx3jQv0cn3oSFYmFpJdc4PkjPlPA2LufngPSu58ySow84cjMUgJEbb3DdqyltK9qw+a/nAExr58a6M09YdvyhdLlnCnqeze/LVstMT/X0okn9GgTfDaKC+8dnNKipqWP62W9E0VJSkvnr3EmmePngVs4dgC69BnL5wmkO793Jb32HYGQiu9+vXDhD6fKVsLQuIvd43KvUwD2X3wVAvcYtAXgTlnN3s71jUSbMWCCdtrKxpUvvwfh4TSYjIx0VFfk+pn3Jctl9O22GF43q1SA4OGvfSiQStm7eQM8+/an//tjiOWsOjevX5MifB2n/S0e5xvO5Lx33AFq2agPA61chCo1DXsTVKPmQlpaGk5MTd+/eVVQ8aGhoEBwcrLD1f8miubOoXqM2lapU+2K9qMgILl44R4s27Qoosty9CgkhIuIt1ap/PMipq6vj7l6JmzdvFGJkHyUkxKOkpCRtCn7yIJj09HTKVqwqrWNsaoatgzP3g27ltppvFvLyBT81q0eHNk2YPnEMr0Ne5h5rfFasugpotv7A0UyHa7Ob8Nf0hvj1dMfOJOss0s5EGwsDTc7deyutm5qeyZVHEbg7GQNgoqtOBUdjIuJT2DOqFte9mrBzeA0qvZ+vaPHxWd05BgYGMuXXr/1Nk3o1aN+6KbM9pxAVFanwWDIyMsjMyEBNXfbhfOoamgTdzv7dj46K5OqlCzRu0VbhsclLYkI82to6ck80cvJh3+rrZ+3bV69CiIyIoGo12WNLBfdK3PpOji3C/498fYPV1NRISUmRS4Y2atSoHMszMjKYM2cOJiYmACxatOiL60lJSSElRfYsMSVVBQ2N/D0d9MTRP3lwL5jVG77ez3z44D60dbSpU0/xXShfExGZ9Y/J+P3n9YGJiQmhoYU/YDc1NYXNq5dQq0FTtN936byLjkRVTQ1dPX2ZuoZGxryT8z8p11JlmOjpha2dPdGRkWxYu5JBvbuyfvs+DAwNZeqmpKSw0s+Hhk2ao6Mr/+4ngBvPohmx8TpPw+Mx1dNkWFMX9oyuRYPZpzDTz/rORnzW6vE2LoUixu8TEtOsputRzUswa08QQSEx/FzZlq1Dq9PQ67TCWjggqxXNd+FcypZ3x7nox27G6jVr0aBRE6ysrXn96hUr/H5nUN8ebNi6C3V1dYXFo62tQ4lSZdi2fhW29o4YGplw7uQRHty9jXURu2z1Tx7Zj5a2NtVrN1BYTPIUG/OOHRtX06RVe4VvSyKRsGjBXMqVd6fo+y7kyIgIAEw+ax0yMTEhVFwMkG/f+YNvFS7f6fLQoUOZO3cua9asQVX127NtX19fypYti+FnB3yJREJwcDA6Ojp5Smq8vb3x9PSUKRszYQrjJk7NcyxvwkJZvHAOi5auylOScmj/Hho3bZnvhEaRPv+sJDmUFbT09DQWzfQgMzOTvsMnfLW+ImKuWqPWx4miUKpMWTq1bcaRQ/vo2KW7TKyek8aSmSlh1HjF9f+euRv+yVQcgU+juDC9Ib9UseP60ygg6zfwKaVPyj4csDZfeMaOyy8ACAqJoUZxUzpWs2PufsW1Cs73nsmjB/dZFbBZprxRk+bSv52LulDStRStmzXkr/NnqNegscLiARg9eTaL50yne7vGKKuo4FysBHUaNuPxg3vZ6p74cx91GzVH/Tv63eYmMSGeWR7DsLV3omP3fgrf3jzvmTx6eJ81n+1byH65ZWF2df8/+9E/szxnCy9evKBIkSJcuXKFkydPcuzYMUqXLp1tkNDu3bvztL7Zs2ezevVqFi5cKPMUWTU1NQICAnB1dc3Tejw8PLK1ksSm5m+A4f17d4mOiqTPbx2kZRkZGdy8cY3dO7Zy6uIN6aDFmzcCefH8KZ7eC3JbXYEyNcnqJ4+MiMDskysooiIjs7V2FKT09DQWek4gPPQ1ngtXSFs1AAyNTEhPSyM+LlamdSMmOoripcooNC4tLW2cihYj5OVzmVineYwm9HUIvsvWKqxVIydJqRncex2Lo5kOR9+P0zDT1yQ89mPrhqmehnSMx4fyz69OeRQWj42RlsLinD9nFufOnmbl2o1YWFh+sa6pmTlWVla8ePH8i/XkwcrGljlL/ElOSiIxIR5jUzPmThuHhZW1TL07N68T8uIZ46bPVXhM/1ZSYgKe44egqaXNhJkLUVVV7GDbed6zOHfmNKs+27cfxjlFRETIXJ0VFRVVqMcW4f9TnsdsODo6EhERgaGhIe3bt6dJkyZYW1tjYGAg88orDw8Ptm/fzsCBAxkzZgxpaWnf9AY0NDTQ19eXeeW3xaFipaps2LaXdZt3SV8lXEvRuGlL1m3eJXN1xMF9uyheshTFXEp8YY0Fx6ZIEUxNzbh86ePVDWlpqQQGXqVs2fKFEtOHRCP01UumLViOnoGhzHwnl5KoqqpyM/CytCw68i0vnz1WeLKRmprK82dPMXmfpH1INEJevMDHb022rhVFU1dVppiFHm9iknkRmcibmGRqlfg40FJNRYkqRU0JfJLV6vEyMpGwd0k4m8smRI7mOryKSpJ7fBKJhPneMzlz8jjLVq3DxubrAyvfvYvmzZuwAh0wqqmlhbGpGfFxsVy/epGqNevKzD9+aA9Fi7vKXGr6PUpMiGf62EGoqqoxabYP6uqKa4WRSCTM9ZrJ6ZPHWb56HTZFZPetjU0RTExNuXJZ9thyPfAqZQrp2PL/TNzUK48+NOOuW7dObhuvVKkSgYGBDB48mIoVK7Jp06ZCaWrS1tHB6bPrxjU1tdE3NJApT4iP5/SJYwwZkf2yNkVKTEzg5YsX0ulXr0K4fy8YfQMDrKys6dy1G/5rVmJnb4+dnT3+q1eiqalJsxYtFRJPUlIiYa8+DrIMD33N00f30dXTx9jUjAXTx/Pk4T0mevmSmZlBdFRW36+ungFqamro6OpRv1kb1i/3RU/fEF09fTas8MXOsShlKlSRa6x+vvOpUasu5pZWvIuOYoP/ShIS4mnasg3p6elMGT+KB/fuMtfHj4yMTGk/tb6BgUIu35z8UylO3A7jVXQSJroaDGvqgq6mKn9cyfo8/U8/ZkhjF56FJ/D0bTxDmriQnJbB3muvpOtYceIRo1qU4O6rGO6GxPJzFVuKWugxwP+q3OOd5zWDo4cPscB3Kdo6OkREZI0R0tXVQ1NTk8TEBFav8KNeg0aYmpoT+voVy5b4YGhoRN0CuCw88O+LIJFgY+tA6KsXrF3ug42tAw2bt5HWSUyI58KZ4/QePFqhsSQlJRIq87t4xZNH99HT08fMwoq42BjehocR9f4zfP3iGQBGxiYYGZuSlJjA9LGDSElJZsLEWSQmJpCYmDUGR9/ASO6XhM/1msGRw4dYmMu+VVJSolOXbqzzX4WdnT22dvas81+FpqYmTZsr5tjyqa8d92Ji3hEWGkp4eFbX5LNnT4GsFpmCvjIqL0Q3SiHT1dVl/fr1bNu2jUaNGpGRkVHYIeXqxLE/kUgkNGza/OuV5ehu0B369vo4vmDh/DkAtGrdlhmz59CjVx9SUpLxnjWD2NgY3EqXYflKf4XcYwPg8f27TBvVXzodsDxrEG/dJi3p2L0/Vy+eBWB0304yy3kuWolbuYoA9Bw8GhUVVRbOmEBqSjKly1fGY/Z0uR9Q34a/wXPyOGLeRWNoZIyrWxlWrN2CpZU1oa9f8de50wD06vKzzHKLV6ylvHtlucYCYGWoydKeFTHSUScqPoXrz6Jps/A8r6KzWiWWn3iEproKszqWwUBbjX+eRdNl6UXpPTYA/M88QUNNhWntS2OorcbdV7F0XnqR5xGJco93185tAAzo012mfKqnFy3b/ISysgqPHj7gzwP7iIuLw9TMFPeKVfCat0jh92EASIyPY/2qJUS8fYOengHV6zSgW98hMl0P504eAQnUadBUobE8un+XKSM/jq9Yuyzrd1GvSSuGT/Dk74tnWTJ3unT+gplZNwzs2L0fnXoM4NGDYB4E3wFgYNePyRLAyq0HsbCU7Rr6t/7YkbVv+/eW3bfTZnjRqs1PAHTv2YeUlBTmeM2Q3jBw6fI1BbJvv3bcO3v6FNOmTJTOnzA2qzu9/8DBDBg0VOHx5dePPkBUSfL5aLRcKCsrM2vWLHS/0p89bFj2m6/kVUhICIGBgTRs2PBffZnfxqV/vVIh0tGQ7z9UeXuiwCsa/i0zve97cF/FiX8WdghfdGd+q8IOIVefjlP5HqVnKPYeQ/9WEWPFjdn5t1S+8/+02uqKj6/HVvlc1h/QSbFdzYqSr5aNFStWfPHMU0lJ6V8lG0WKFKFIEfnfbEcQBEEQCpPoRsmHa9euYW6umGdGCIIgCMJ/1Y+dauTjapQfPSsTBEEQBOHb5PtqFEEQBEEQ8kc8Yj6Ppk2b9tXBoYIgCIIgZPeD5xr5SzYEQRAEQRDyq9DvsyEIgiAI/3U/+rhHkWwIgiAIgoL94LlG3q9GEQRBEARB+BaiZUMQBEEQFExcjZIH5cuXz3N/0/Xr1/9VQIIgCILwX/OD5xp5Szbatm0r/Ts5OZlly5bh6upKtWrVALh8+TJBQUEMGjRIIUEKgiAIwv8zMUA0Dz697LVPnz4MGzaMmTNnZqvz8uXLzxcVBEEQBOEHl+envn5gYGDAtWvXKFasmEz5w4cPqVixIjExMXIN8Fskpoq7nf4bkfGphR1Crs49fVvYIXxR5SImhR3CF00/dr+wQ8jVgtauhR3CF8Ukft9Pk/6en/qa+Z3fgVpfU/HXSgzdEyyX9Sz5qaRc1lPQ8v0Ja2lpceHChWzlFy5cQFNTUy5BCYIgCMJ/iZKSklxe/6/yfTXKiBEjGDhwIIGBgVStWhXIGrOxdu1apk6dKvcABUEQBEH4/5bvZGPChAk4OTmxePFitmzZAkDJkiUJCAigQ4cOcg9QEARBEP7fKf//NkrIxTfdZ6NDhw4isRAEQRCEPBLJxjdKTU0lPDyczMxMmXI7O7t/HZQgCIIgCP8d+U42Hj58SK9evbh48aJMuUQiQUlJiYyMDLkFJwiCIAj/Bf/PgzvlId/JRo8ePVBVVeXgwYNYWVn98B+gIAiCIHyN6EbJp3/++YfAwEBKlCihiHgEQRAEQfiPyXey4erqSkREhCJiEQRBEIT/pB+9EyDfN/WaO3cu48aN48yZM0RGRhIbGyvzEgRBEARBlrKSklxe/6/y3bLRsGFDABo0aCBTLgaICoIgCELOFH9D9O9bvt//6dOnOX36NKdOnZJ5fSgTBEEQBKHwLV++nDJlyqCvr4++vj7VqlXj8OHD0vkSiYTp06djbW2NlpYWdevWJSgoSGYdKSkpDB06FFNTU3R0dGjdujUhISH5jiXfLRt16tTJ90b+HwReu8qGAH/u3g0i4u1bFvkupV6DhtL5J08cY9fO7QTfDeLdu3ds27mH4iUK5oE4X4tNIpGwcvlSdv2xg7jYWNxKl8Fj0lScixb7wlrlp8tPTXkT9jpbeet2HRk0chzrVi7lysXzhL0OQUdXj/IVq9Bn0AhMzczlHsuFfVu4d/UCEa9foKqugW0xVxp06oepta20zpk/1hN06TSxUW9RUVHFytGFeh17UaSo7P58+SCI0zvW8urxPZRVVLC0L0rn8d6oqWvINebExAQ2rfHj0vnTxERH4VSsOP2GjcOlpBsA0VGRBKzw5cbVyyTEx1GqbAX6Dx+Pja29XONo4GJCQxdTzHTUAQiJSWbPrTBuvo6T1rHW1+DXCtaUtNBFSQlevUvm93PPiExMA6BXlSK4WelhpKVGcnomD98msPX6a0JjU+QaK8C6VctYv2a5TJmRsQl7jpwBICoygpVLfbh25RLxcXGUKe/O8DEeFLGT7+f2QdDNQPZs28CjB3eJjozAY+YiqtaqJ50vkUjYFrCSowd3kRAXh0tJN/qP8MDO0VlaJ/TVS9Yt9yH49g3S0tKoULk6/YaNx9BY/g/4u/7+uBIcnHVcWeC7lHr1Px5X3MvkfBHA8JFj6dazt9zj+dQ6/1WcPnmc50+foKGhSZly5RkyYjQODo451veaMY09u3YwcuwEOnftrtDYvlVh9IAUKVKEOXPmULRoUQDWr19PmzZtuHHjBqVKlWLevHksWrSIgIAAXFxcmDVrFo0aNeL+/fvo6ekBWY8oOXDgANu2bcPExITRo0fTsmVLAgMDUVFRyXMs+U42zp0798X5tWvXzu8qvwtJSUm4uJSgddt2jBk5LMf5ZctVoGHjpsycPuW7ii1g7Ro2bQjAc5Y39vYOrF61ggH9erH3wGF0dHQVHp/f2i0yN3d7+vgR44f3o3aDxiQnJ/PwfjBde/bHuZgLcXGxLPOdx9Rxw1i2bpvcY3kefIuKjVpj7VyCzIwMTu/wZ/OccQyctxZ1zaynYppYFaFZj6EYmVuRlpbKlT//YLP3eIb4bEBH3xDISjS2zPWgRptONO0xFBUVVd68eKyQS72XzPXk+dNHjJ40C2NTM04fO8TkUQNYtmEXJqbmzJo0ElUVVSZ7+aCto8ve7RuZPGoAyzfsRlNLfk/6jEpMY9v117yJy3rqby1nI0bVdWTioQe8iknGXFedqU2LcfZRJLtuhpGYloGNgSZpmR+f6Pk0KomLT6OJSEhDV0OFdmUsmdDQmRF77qKIB386OBVl4dLV0mkVlazGWolEwuSxw1FVVWX2gt/R1tFh55YNjB7Sl4Dte9HS0pZ7LMnJSTg4u9CgWWvmTB2Tbf7urQHs27mJ4RM8sS5iz46Nq5k6ZgDLNu5FW1uH5KQkpo8dhIOzCzN9VgGwxX8ZsyYOZ96yDSgry7chPikpCZfiWceVsaOyH1eOnjovM33xwjlmTJtM/UaN5RpHTq5fu8ovHTvjWsqNjIwMli/xZeiA3uzYfRAtbdl9d+bUCe7cuYWZAk5e5Kkwxlu0atVKZnr27NksX76cy5cv4+rqiq+vL5MmTaJdu3ZAVjJiYWHBli1b6N+/PzExMfj7+7Nx40bpEIpNmzZha2vLiRMnaNKkSZ5jyXeyUbdu3Wxlnx6A/1/HbNSsVZuatXJPlFq2agPA61f5bz76t74Um0QiYcumDfTuO4AGDbMOAjNnz6FB3RocPnSQnzv8qvD4DI2MZaa3bfDH2saWsuUroqSkxLzfV8nMHzLKgyG9O/MmLBQLSyu5xtJlwhyZ6db9x7FwQHtCnz7EvmQZAErXkB1v1LjrQG6cOcybF09wcqsAwLFNy6nc5Cdqtu4krWdiVUSusQKkpCTz17mTTPHywa2ce9Z76DWQyxdOc3jvTuo3bcX9oFv4rf8De8ess5OBoybStU19zp48TJOW7eQWy40Q2QHeO/8Jo6GLKUXNtHkVk0yH8lbcfBXL1uuh0jpv41Nlljn9MFL6d0QC7PwnlDmtSmCmo074Z3XlQUVFBRNT02zlIS+ec/fOLdZt3YOjc9bnNmLcZH5qUoeTRw/Tsm17ucfiXqUm7lVq5jhPIpFw4I8t/NK1N9VqZ33/RnjMpPtPDTh34jBNW/9M8J1/CA97jc/qrWi/P0kYNsGTLq3qcOv635SrWFWu8daoVZsaXzjmmZqayUyfOX2KipWqUKSIbS5LyM+S5atlpqfO8KJxvRoEBwdRwb2StDz8zRvme8/i9+WrGTl0gMLj+h6kpKSQkiLbUqihoYGGxpdbXDMyMti5cycJCQlUq1aNp0+fEhYWRuPGH5NHDQ0N6tSpw8WLF+nfvz+BgYGkpaXJ1LG2tsbNzY2LFy/mK9nId6ocHR0t8woPD+fIkSNUqlSJY8eO5Xd1wr/0KiSEiIi3VKteQ1qmrq6Ou3slbt68UeDxpKWlceLoIZq2bJtrK0BCfDxKSkrovm+mU6SUxAQAtHRz3lZGehqBpw6hoa2DpV1Wc3ZCTDSvHgWjY2DI2mlDWTigPQEzRvLi3m25x5eRkUFmRka2rhl1DU2Cbt8gLTXrH7T6J/NVVFRQVVXj7i3F7V8lJajqYIiGqjKP3iagBJSz0Sc0NoXxDZxY9kspPJsVw93WINd1aKgqU6eoMeFxKdJuFnl79fIF7ZvX59c2TfGcNJbXr14CkJb2/nPT+OxzU1Pj9s3rConlS96EviI6KoLylapJy9TU1SlVzp17QTeBDzEroaamLlNHWVmZ4Nv/FHDEsiIjI7hw/ixtfpJ/kpYX8fFZXXn6+h+/b5mZmUybNJ6uPXoVWJfxv6GkJJ+Xt7c3BgYGMi9vb+9ct3v79m10dXXR0NBgwIAB7NmzB1dXV8LCwgCwsLCQqW9hYSGdFxYWhrq6OkZGRrnWyat8t2wYGGQ/uDRq1AgNDQ1GjhxJYGBgflcpFR0dzfr163n48CFWVlZ0794dW9svZ9E5ZXkZSupfzfL+KyIi3wJgbCLbp2tiYkJoaPZxFIr219lTxMfH0bhFmxznp6ak4L/cl/qNmyu8i0cikXBs03Jsi7thbivb1/vg+iV2LZlFWmoKeobGdPWYh/b7A1l0eNaZ+9ld62nUeQAWDs7cOn+cjV5jGTB3jVxbOLS1dShRqgzb1q/C1t4RQyMTzp08woO7t7EuYkcRewfMLa1Yv+p3hoyZgoamFnu3byQ6KoKoSPnf78bWUJPpTYuhpqJMcnomPmee8iomBQNNVbTUVGjlZs7Of8LYdj2UMtZ6jKjjwOxjj7gXniBdR0MXEzpVsEZTTYVXMcl4n3hMRqb8+1Bc3UrjMX02tnb2REVFsnHtKgb3/o2AbXuxc3DEwsqa1X6+jPaYiqaWNju2rCcqMoKoQrhPUHRU1jYNPmsFNDQyIfxN1vetuGtpNLW0WL9yMb/1HYJEAutXLiYzM1O6fGE5uG8vOto61G+o+C6Uz0kkEnwWzKVceXeKFnORlq9ftwYVFRV+7fxbgcf0LeR1B1EPDw9GjRolU/al/3fFixfnn3/+4d27d+zatYvu3btz9uxZ6fzPTwo/XFn6JXmp8zm5dQKamZlx//79fC1jbW1NZGRWs+vTp09xdXVl7ty5PHz4kJUrV1K6dGnu3bv3xXXklOUtmJd7lvdfle0Lk0NZQTh8cA+Vq9bIcfBnenoas6aOIzMzk2FjJyk+loDfefPiCe2HTM42z8G1HP29V9Fr+u84l63Ert9nkhATDWT9kAAq1G9JubpNsXIoRpPfBmFiVYR/zh6Re5yjJ88GCXRv15ifGlZm/x9bqNOwGcrKWS0YE2cu5NXL5/zaojbtG1fl9j/XcK9SQ+59+ACvY1OYeOg+0w4/4OSDCAbUsMfGQEM6uO36y1iOBL/leXQSB4LCuRESSwMX2W6Mv55GM/HQfWYefUhYbArDajugpoB7NVepXos69RvhVNSFipWrMcfHD4Cjh/ahqqrGjDmLePniOa0a1qRJ7Ur8E3iNKtVroqxSeBch5nhgJ6vMwNCYcdPncfXSOTo2q0GnFrVITIjH2aWkQvZ1fuzbu4tmLVoWykncPO+ZPHp4n1lzF0jLgu8GsW3zRqbN9P7hHpmhoaEhvbrkw+tL+0VdXZ2iRYtSsWJFvL29KVu2LIsXL8bS0hIgWwtFeHi4tLXD0tKS1NRUoqOjc62TV/lu2bh165bMtEQiITQ0lDlz5lC2bNl8rSssLEw6xmPixImUKFGCQ4cOoa2tTUpKCj///DNTpkxh586dua4jpywvQ0k9l9r/PaYmWf2qkRERMgOkoiIjs7V2KNqb0NfcuHqZad4+2ealp6cxc9JYwl6/Yv7SNQpv1TgcsIQHgZfoPtUHfROzbPPVNbUwtrTB2NKGIsVcWTqyGzfOHKZmm87oGmadfZoVkb1qwdTGnpiIcLnHamVjy5wl/iQnJZGYEI+xqRlzp43DwsoagKLFXVmydgcJ8XGkp6dhYGjMqP5dKVbcVe6xZGRKpANEn0Yl4WSiTZMSZqy/+or0TAmvYpJl6r+OSaa4uY5MWVJaJklpqbyJS+VhxDNWdXSjop0Bl569k3u8n9LS0sapaDFCXr4AoHjJUvhv/oP4+DjS09IwNDJmYM/OFC8p/8/ta4yMsxKyd1GRGH/yfYx5F4Wh8cfWjvKVqrFyywFi30WjrKKKrp4e3X9qiHl9mwKP+YMbgdd4/uwpc+Zn/10r2nzvWZw7c5pVazdiYWH5Mabr14iOiqRV0/rSsoyMDBYvnMe2zRvYf/hkgcf6Nd/LDbkkEgkpKSk4OjpiaWnJ8ePHKV++PJD1NPezZ88yd+5cANzd3VFTU+P48eN06NABgNDQUO7cucO8efPytd18JxvlypVDSUlJevb3QdWqVVm7dm1+Vyd15coV1qxZg/b7kcYaGhpMnjyZn3/++YvL5TQwJjFVAcPev1M2RYpgamrG5UsXKfH+IJqWlkpg4FWGjxhdoLEcObQXQyNjqlavJVP+IdF4FfKcBUv9MTAwVFgMEomEIwFLuHftAt0mL+J/7d11XBT5H8fxF9IC0iktiJ3Y3WKfvzNOPfX07O48WzHOrrNRUbG7sD3PM7BFbBGDlu6Y3x94qysYeLssnt/n47GPhzszO/t2d3b4zPf7nRljiy8bgCohkZ6WNa7AyNwKA2NTIl/LDwZ+E/ySImUr5fRyhdDR1UVHV5f4uFiuX73IL32Hys3Xezvu5NWL5zx+cI8uPfsrLcv7NNULkJEp8TQiEetC8r81q0LaRCR8ejyGGmpo5sGReWpqKs8Dn1KmXAW56fpvP7eXQc95EOBPjz4DlZ7lQ5bWhTE2MeOm3yWcXbNOKU1LS8P/5jW69hmSbflCRll95LevXyEm+g2Vq6vukgP79u6ieImSFHXLu/thSZLEPM8ZnD19kj/WbaSwrXzXZbMWrahcpZrctMH9euHRohUt2yhu0LQiqaLWGD9+PB4eHtjZ2REXF4ePjw9nz57l2LFjqKmpMXToUGbNmoWrqyuurq7MmjWLggUL0qlTJyBr2ETPnj0ZMWIEpqammJiYMHLkSEqXLi07O+VL5brYePbsmdzzAgUKYG5ujo6OTm5XBbxrVkxJSclxoEp4ePhXrTe3EhMTeBEUJHv+6tVLHtwPoJChIdbWNsTERBMSHExYWNaRbWBg1udgamaWbdR2Xmfr1KUr69auwt7BAXt7B9atWYWOjg4ezVsoNdf7MjMzOX54P42atUJd491mlZGeztTxI3j8IIAZvy8jMzNTNtbAoJAhmpqaCs1xdMMS7lw8RYcR09HWLUh89BsAtAvqoamlTWpyEn/u24JbxeroG5mSFB+D34kDxL4Jp0TVrB26mpoa1Vp04NyujVg6OGPl4MKt875EvA7ix6GTFZoX4NqViyBJFLZzJPhVEOtXLqSwnSMNm2WNe7lwxpdCRsZYWFoT+OQRq5fOpWrNelSoXF2hOdqXs+bW61giE9LQ1SxAVUcjSljqM+f0EwAO3wtjUC0H7ofGcy80njI2hahga8gM38cAmOtrUc3RiNuv44hLTse4oCYtS1mSmpHJzdeKv5XBisW/U71WHSwtrYmKesPm9atJTEigydvxQmdPHsfQ2ARLKyuePn7E0gVzqFmnPpWqKvZz+0dSYiLBbweoAoSGvOLpowcYFCqEuaU1LX/sxC7vdVjb2mNT2J5dW9ahpaND7YYestecPLofO3snChkZ88D/NmuXzaNVu87Y2jsqPO+H+5XXH+xXAOLj4znpe5xhI8co/P0/Zc6saRw/epjfFy2joJ4eERFZfwf09Q3Q0dHByMgYIyP5QYsamhqYmpl99Foc36PQ0FB+/vlngoODMTQ0pEyZMhw7doxGjRoBMHr0aJKSkujfvz9RUVFUqVIFX19f2TU2ABYuXIiGhgbt27cnKSmJBg0a4OXllatrbACoSR82UeShAgUKUKpUKTQ0NHj06BGbNm3ihx9+kM0/f/48nTp1yvXVyr6mZcPv6mV69ch+MZiWrdowbeZsDuzbw+Tfxmeb36ffAPr2H5Tr91NkNtlFvXbuIDY2RnZRr/cHU+VG5Fecouh3+SJjh/bFa/sBuR1jSPArurT1yPE1vy9fR7kKuWspOP/s08XntE4Ncpzeqs8oytVpSnpqKnuWz+TV4wAS42LR1S+ETRE3arXpTOEi8kduFw5sw893P0kJcVjaO9Pwp97YFyv9yfevbJv7rqs/Tx9n4+qlRISHYmBgSPU6Dejaa6CsJePArq3s2baR6KhIjE3Nqd+kBR279f6qQm2K78fHVfWqZkdJKwOMdDVITMvgRVQyB/1DuRscL1umThETWpWyxKSgJsGxKey+Fcy1t6fMGulq0KuaPU4muuhpqROTnM79sHj23g79oot6/d4qd90bUyeM4vaNa8RER2FkbEKJUmXo0Wcgjs5ZZxXt3r4Fn80biHoTiamZOY2btaRrz75fXeDGJKZ/cv6dG35MHNYr2/T6TVoyZNy0dxf1Orib+LhYipYoRZ8h43BwdpEtu3HVYk4fO0h8XAwWVjY0bfUjrdp1+aJxCbYmubvmit/Vy/TpmX2/0qJVG6bOyDqFfM+u7fw+15Pjp/6U+wOUW5m5/DNTqWzOF0ycNG0WLVv/kOO8Vh4N6Ni561dd1KuQjvJb3maeeqyQ9Uxo4PL5hfKhryo2zp07x++//05AQABqamoUL16cUaNGUatWrc+/+D1Tp06Ve161alW583ZHjRrFy5cv2bZtW67W+z11oyjD1xQbeeVzxYaqfU2xkZc+VWyoWm6Ljbz2uWJD1XJbbOSl3BYbeS0vio1Zp54oZD3jGxT5/EL5UK67Uby9vfnll19o27YtgwcPRpIkLl68KGta+aev50tMnvzpJul58+blNp4gCIIg5DtKOCHrm5LrYmPmzJnMnTuXYcOGyaYNGTKEBQsWMH369FwVG4IgCIIg/Pfluu3o6dOn2a63DtCqVatsg0cFQRAEQchq2VDE41uV62LDzs6OU6eyn8N86tSpz17tUxAEQRC+R2pqagp5fKty3Y0yYsQIBg8ezM2bN6levTpqampcuHABLy8vFi9erIyMgiAIgiB8w3JdbPTr1w8rKyvmz5/Pjh07AChevDjbt2+ndeuc74chCIIgCN+zb7kLRBFyVWykp6czc+ZMevTowYULF5SVSRAEQRD+U77hHhCFyNWYDQ0NDebNmye7n4kgCIIgCMLn5HqAaMOGDTl79qwSogiCIAjCf1MBNTWFPL5VuR6z4eHhwbhx47h79y4VK1ZET0/+jo+tWrVSWDhBEARB+C8QYzZyqV+/fgAsWLAg2zw1NTXRxSIIgiAIgpxcFxuZmZnKyCEIgiAI/1nfcA+IQuS62BAEQRAEIXcK8H1XG19cbCQlJXHq1ClatGgBwLhx40hJeXfbaHV1daZPn46Ojo7iU+ZSgXzeOZaZmb/vgJifVbHL33dVjU5IU3WET1rStpSqI3yUfffNqo7wSa82dVV1hG/W935UD+Iz+OJiY9OmTRw6dEhWbCxbtoySJUuiq5t1W+P79+9jY2Mjd4M2QRAEQRCELz71dcuWLfTo0UNu2tatWzlz5gxnzpxh3rx5siuKCoIgCILwjrgR2xd6+PAhRYsWlT3X0dGhQIF3L69cuTL37t1TbDpBEARB+A8Q19n4QjExMWhovFs8PDxcbn5mZqbcGA5BEARBEATIRcuGra0td+/e/ej827dvY2trq5BQgiAIgvBfoqammMe36ouLjWbNmjFp0iSSk5OzzUtKSmLq1Kk0b95coeEEQRAE4b9AdKN8ofHjx7Njxw7c3NwYOHAgRYsWRU1Njfv377Ns2TLS09MZP368MrMKgiAIgvAN+uJiw9LSkosXL9KvXz/Gjh2LJGVdK0JNTY1GjRqxYsUKLC0tlRZUEARBEL5V33CjhELk6gqiTk5OHDt2jDdv3vD48WMAXFxcMDExUUo4QRAEQfgvyPUt1v9jvupy5SYmJlSuXFnRWQRBEARB+A8S90YRBEEQBCVT+877UUSxIQiCIAhK9n2XGqLY+Kh1a1Zx6oQvz549RVtHh3LlyjN0+EgcnZzzPMs1v6ts8lrHvXv+RISHs2DRMuo1aCibf+qkL7t3bifgnj/R0dH47NyLW7HieZav8w9NCQ15nW16q7YdGDxqAhvXruDsiWOEh4WgoamJq1sJevQdRPGSZfIkX2JiAt5rl3Px/Bliot7gXNSNPoNHU7T4u5uSBQU+ZcMfi7l78xpSZib2TkUYO20uFpbWCs0ScOc6h3dt5tmj+0S/iWDYpHm4V6+b47LrFs/i9NG9dOkzDI8fOsmmR7+JYOvaJdy9cZnkxESsbR1o1fEXqtRqoNCsAHt2+rB353aCg18B4OTsQo/e/ahWoxaQ9dmuXLKQ82dPExMTjbV1Ydr91Jm27ToqPMv4duWY0L683LTQ6ESce22XPXcrbMj0Lu7ULGFFATU1Al5E8fPCs7yMSJAtU7moOVN+qoi7ixlpGZncDnzDD7NOkJyaodC8u3f4sGenD69fZ312zkVc6Nm7H9Vr1gZAkiTW/rGcfXt2EhcbS8lSZRg1biLOLq4KzfEx19/uVwICsvYrvy9aRr367/YrFcsUy/F1Q4aNousvPfMk32av9e/lW0rd9/JFRkawdOF8Lv39F3FxcVSo4M6ocROwd3BUerav8S2ftqoIotj4CL+rV+jwU2dKli5NRnoGS5cspG+vnuw5cJiCBQvmaZakpCSKFi1GqzZtGTlscI7zy5arQMPGTZk+5bc8zQawfP1WMjMzZc+fPXnMmCG9qd2gMQC2dg4MHDEe68K2pKYks9tnM2OG9GXTzkMYGSt/cPGSOVN5/vQxIyfOwMTMnDO+h5kwrC8rN+/GzNyS4FcvGD3gFxo3b0OXHv0oqK/Pi8CnaGlpKzxLSnIS9k5FqdOoJYtmjPnocn4Xz/L4wV2MTc2zzVs5bzKJCfGMmLIAg0KG/HXmOEs9x2NpvQlHFzeF5rWwsKTf4GHY2tkDcOTgfsYMG4jXtt04F3Fh8fw5XL96hckzZmNtU5jLf//F/NkzMDO3oHbd+grNAnAvKIoW04/Lnme8t905WRpwYnozNp1+xMztN4hJTMXN1oiU94qIykXN2TehMfP33mbEukukpmdS2tFYKXditrC0pP/gYdjZOwBw+MA+Rg0dyGaf3Ti7uLLZax1bvTcyados7B0cWb/mDwb1+5Ud+46gp6en8DwfSkpKoqhb1n5l1PDs+5Xjp/+Ue37xwnmmTZ5I/UaNlZ7tn3yubm60bPMDo4cPkZsnSRIjhwxEQ0OD+YuXo6enz5bNXvTv3YOdew+hm8f7aOHzRLHxEStXr5N7Pm2GJ/VqVSPgnj8V3SvlaZaatWpTs1btj85v0bI1AK9fvcyrSHI+LBh8Nq3DprAdZcu7A9CgifzF3voOGcXRg3t5+vghFSpVVWq2lJRk/jp3it9mLaRUuYoAdO7Rj7//PMORfTvp2msgm1Yvw71qTXr0f3fHYmsb5VwNt1ylGpSrVOOTy7yJCMNrxTzGzljCvEnZ76L8KOAOvwwcSxG3kgD80Kknx/ZuI/DxfYUXGzXr1JN73nfgEPbu8sH/zi2ci7hw9/YtmrVsTQX3rAHjbf7Xnv27d3L/3l2lFBvpmZmERiflOG/yTxXwvfGSid5+smmBYfFyy8zpVpmVR+4xf98d2bQnIbEKzwlQ64PPrt+goezZ6cPdO7dxKuKCz5ZN/PJrH+o1aJSVf7onHvVrcfzoIdr+2EEpmd5Xo1Ztanxiv2JmJl/onj1zGvdKVbC1tVN2NODT+YKeB3Ln9i227zlAkbctQWMnTKJx3RocP3qYNv9rlycZc+P7btcQZ+N8sfi4OAAKGRqqOEn+lpaWxsnjh2naok2OA6LS0tI4vG8XevoGFHFV7B/GnGRkZJCZkZGtlUJbW4d7t2+QmZnJ1b//pLCdA78N70enlvUY1rsLf58/rfRsOcnMzGTlvMm0+LELto5FclzGrWRZLp0/QXxcDJmZmfx91pe0tFSKl6mo1GwZGRmcOH6E5KQkSpUpC0DZchX489wZwsNCkSSJa1cv8yIokCrVPl1Qfa0iVoV4vKoD/st/xGtoHRwt9IGsaxg0rWDHo9ex7J/QmMC1HTk7qwUtKtnLXmteSIfKRS0Ij0nm1IzmPFvTkWNTPahWzEIpWd+XkZGB77EjJL397F6/eklkRARVqlWXLaOlpUV5d3fu3Lyp9Dy5FRkZwYU/z9H6h/+pOgoAaalpAGhrv/tdq6uro6Gpyc0b11UV65PE5cpV6MaNGzx79kz23Nvbmxo1amBnZ0fNmjXx8fH57DpSUlKIjY2Veyj6hnCSJPH7XE/KV6iIq2vRz7/gO/bXudPEx8fRuHlruemXLpyjRf0qNKvjzm4fb+YsXoWhkbHS8xQsqEexUmXw2biayIgwMjIyOH38MA/u3eFNZATRUW9ISkpk55b1VKhSnekLVlKtdn1mThzBnRt+n38DBTu4YyMF1NVp0vrjYx4GjfckMyOdPu0a0r1lddYtmcWwSfOwVFJrzJNHD2lQw526Vcszb+Y0POcvwcnZBYBho8fh5FyE1k3rU7tKOYYP7MOIsb9RtrziCx+/R+H0WvYnrWf6MvCPv7A00uX0zOaY6GtjYaiLga4mI9qU5sTNl7Sa4cvBK8/ZNrI+NUtkXWzQ0dIAgPHty+F18gFtZvpy62kkhyc1pYhVIYXnBXj86CF1q1WkVuVyzJkxlTkLluBcxIXIiAgATEzM5JY3MTEjMjJCKVn+jUP796FXUI/6DfOmC+VzHJ2csLaxYdnihcTGxpCWlorXujVERkQQERH++RUIeU6lxUbPnj0JDAwEYO3atfTu3Rt3d3cmTJhApUqV6NWrF+vXr//kOjw9PTE0NJR7zJvjqdCcnjOm8ejhQ+bMW6DQ9f4XHT20l8pVa2BmLn+0WLZiJVZt3Mni1ZuoVLUGMyaOJOpNZJ5kGjlxJpIEXX9oTJsGlTm4eyt1GnpQoIA6kpTV51+1Zl1+6PAzRVyL0b5LDypVr82R/bvyJN8/nj0K4Ph+H/qOmPzJ0+R2blxJQnwc4zyXM33pJjzadmbJzLEEPXuslFz2jo5s3Lab1Ru38kO7DsyYNJ5nT7Pea+e2Lfjfuc3chcvY4L2DQcNGMX/2dK5e/lvhOXxvvmL/5ef4B0Vx5k4w//M8CUDnui6yI77DfkEsO3yP24FvmL/vDkevv+DXRlkDHf8ZoLf+xAM2n33MrcA3jNl4hUevY+haXzmDMh0cHdm8fQ/rNm2jbfsOTJs0nqdP3n1P2b5nScqXp0ju37cbj+Yt5FoSVElDU5O5C5YQ9DyQ+jWrUrNyBa5dvUL1mrUoUCB/Ntirqakp5PGtUumYjQcPHlCkSFZT8YoVK1i0aBG9e/eWza9UqRIzZ86kR48eH13HuHHjGD58uNw0SV1xPwjPmdM5e/Y06zd6Y2llpbD1/heFBr/mxtVLTPZcmG2erm5BCtvZU9jOnhKlytKtXQuOHtxLp26/Kj2XdWE75ixbR3JSEokJ8ZiYmTN78mgsrW0oZGiMuroG9h90Wdg5OHHv9g2lZ3vf/bs3iI2OYvDPLWXTMjMz2LJmMcf2+rB40wFCX7/E98AO5vzhI+tmcXAuyoO7NzhxcCc9B49TeC5NTS1s3w5yLF6iFAH+d9mx1ZuhI8fyx7JFeM5fQo1adQBwKerGo4cP2LppA5WqVFN4lvclpqTjHxRFEetCRMalkJaeScCLGLllHryMkXWThEQnAnD/ZbTcMvdfxWBnppwBmZqaWrIBosVLZn1227dupusvWdt9ZGQ4Zubvxka8iYrExMRUKVm+1o1rfjwPfMbsedl/16pUvERJtu7cS3xcHGlpaRibmNCtUwdKlCyp6mg5yp8lUN5RabGhq6tLeHg49vb2vHr1iipVqsjNr1Klilw3S060tbWzVdvJ6f8+myRJeM6czulTJ1jntTnPBkV9y44d3oeRsQlVq9f67LKSJJGWlpoHqd7R0dVFR1eXuLhYrl+5yC/9hqKpqYlr8RK8DAqUW/b1i+dYWCn2tNfPqdmgGaXKy1+Zd86EwdRs4EHtRlkFSEpK1l2X1T44enu/lUbZ/vnu0tPTSU9Pz3YkWaBAATIlxZ/d8SEtjQK4FTbir4BQ0tIzufYkgqKF5btDXGwK8SIia5Do87B4Xr9JwNVGftyVq3UhfG/kzeBqSZJIS03DprAtpmZmXPn7b9yKlQAgLS2VG35+DBg6/DNryVv79u6ieImSFHXL+VRYVdM3yOoeC3oeSMC9u/QbmP3MGkH1VFpseHh4sHLlStauXUudOnXYtWsXZcuWlc3fsWMHLi4uKsk2a/pUjh45xKKlK9ArqEdEeFY/oL6BATo6OnmaJTExgRdBQbLnr1695MH9AAoZGmJtbUNMTDQhwcGEhYUBEBiYVaCZmpllG1GuLJmZmRw/vJ9GzVqhrvFus0pKSmSr1xqq1aqLqak5sbHRHNi9nfDwUOrUz5v+32uXLyIhYWvnSPCrINatWEhhO0caNcsaV/K/n7ozZ/JoSpWtQJkKlbh2+SKXL55n9pK1Cs+SnJRIyOsXsufhIa8JfPIAfQNDzCysMChkJLe8uroGhsam2Ng5AmBj54iljR3rlnjSudcQ9A0M8fv7LHdvXGbkVMUfef6xdBFVa9TC0sqKxIQEThw/yo1rV1mwbBV6+vqUr1iJZYt+R1tbGytrG25cu8rRwwcYPHy0wrPM+rkSR64F8SIiAfNCOoz5X1kMdDXZcjarW2LRgTtsGlaXC/dCOe8fTKNytjSraEfTKUdl61i0/y4TOpTnzvM33A58Q+c6LhQtbEjn+WcUnnfFkoVUq1kLS0trEhMTOHHsCNf9rrJo+WrU1NTo2LkrXutWY+fggJ29A15rV6Ojq0MTjxYKz5KTD/crrz/YrwDEx8dz0vc4w0Z+/DTtvMr3z37P0NAQK2sbTvoew8jYBCtrax4/esj8ObOoU68BVasrZ3Dyv/Utd4Eogpok5cEhyEe8fv2aGjVqYG9vj7u7OytXrqRixYoUL16cBw8ecOnSJfbu3UuzZs1ytV5FtGyULZnzmRLTZnjS+oe2/2rduT2n3+/qZXr16JZtestWbZg2czYH9u1h8m/js83v028AffsPynW+yPjctzj4Xb7I2KF98dp+AFt7R9n01JQUZk0eS4D/HWJjoihkaETR4iXp3L03xUqU+vgKPyIlPfdH73+ePo7XqqVEhIdiYGBIjboN6NprIHr6BrJlfA/vY6f3OiLCwihs70DnHv2oVqveJ9aas+iEtE/Ov3frGjPH9M02vVbD5vQdOSXb9CFdW9H0h45yF/UKeRWEz/plPPC/RUpSIpY2djT7XxdqNfz878TJInfXH5g19Tf8rlwiMiIcPX0DXFyL0qV7TypXzTqLIjIinJVLF3Hl0kViY2Owsrahddsf6di5W653rvbdN39yvtfQOtQsboVpIW0iYpO58jCc6duvc//lu66TrvVcGfFDGQqbFuTR6xhmbL/JYb8gufWMaFOa3k2KY6yvxZ3nUUz0vsrf98M+m+/Vpq65+v/MmDIRv8uXiIgIR1/fAJeiRfm5+6+yM1D+uajX3t07si7qVboMo8b9JjuVM7c0cjlWwe/qZfr0zL5fadGqDVNnzAZgz67t/D7Xk+On/sTAwCDbsl9KIvd/ZvyuXqHvR/JNmeGJz5bNbPZaT2RkJGbmZjRv2Zpf+/RDU1Mr1+9loK38To6dN7Nf+PBrtCtno5D15DWVFhsA0dHRzJ49m4MHD/L06VMyMzOxtramRo0aDBs2DHd391yvUxHFhjIp4wJCivQ1xUZe+ZpiIy99rthQtdwWG3npc8WGquW22MhruS028tLXFBt5SRQbyqfyi3oZGRkxe/ZsZs+ereoogiAIgqAU33s3isqLDUEQBEH4r8u/7U55QxQbgiAIgqBk33vLxvdebAmCIAiCoGSiZUMQBEEQlOz7btcQxYYgCIIgKN133osiulEEQRAEQVAu0bIhCIIgCEpW4DvvSBEtG4IgCIKgZGpqinnkhqenJ5UqVcLAwAALCwvatGnDgwcP5JaRJIkpU6ZgY2ODrq4udevWxd/fX26ZlJQUBg0ahJmZGXp6erRq1YqXL3N3PyFRbAiCIAjCf9C5c+cYMGAAly5d4sSJE6Snp9O4cWMSEhJky8ydO5cFCxawbNkyrl69ipWVFY0aNSIuLk62zNChQ9m7dy8+Pj5cuHCB+Ph4WrRoQUZGxhdnUfnlypVBXK783xGXK/964nLlX09crvzfEZcr/3p5cbnyw3c/f/+dL9G8lMVXvzY8PBwLCwvOnTtH7dq1kSQJGxsbhg4dypgxWTfbS0lJwdLSkjlz5tCnTx9iYmIwNzdn8+bNdOjQAci6r5mdnR1HjhyhSZMmX/Te+XfrFARBEIT/CEV1o6SkpBAbGyv3SElJ+aIMMTFZNy00MTEB4NmzZ4SEhNC48bs7cGtra1OnTh0uXrwIwLVr10hLS5NbxsbGhlKlSsmW+RKi2BAEQRCEb4SnpyeGhoZyD09Pz8++TpIkhg8fTs2aNSlVKuuO2yEhIQBYWlrKLWtpaSmbFxISgpaWFsbGxh9d5kv8J89GScvnTe3qBfL3qGR9nfy7WWikfXkfoSqk5vNtLzEl/35++b2bwrr1AlVH+KSwgyNUHeGjUtLy9+8iL7pRFHU2yrhx4xg+fLjcNG1t7c++buDAgdy+fZsLFy5km/fhpdQlSfrs5dW/ZJn3iZYNQRAEQVAyRXWjaGtrU6hQIbnH54qNQYMGceDAAc6cOYOtra1supWVFUC2FoqwsDBZa4eVlRWpqalERUV9dJkvIYoNQRAEQVAyVZz6KkkSAwcOZM+ePZw+fRonJye5+U5OTlhZWXHixAnZtNTUVM6dO0f16tUBqFixIpqamnLLBAcHc/fuXdkyXyL/tpcLgiAIgvDVBgwYwNatW9m/fz8GBgayFgxDQ0N0dXVRU1Nj6NChzJo1C1dXV1xdXZk1axYFCxakU6dOsmV79uzJiBEjMDU1xcTEhJEjR1K6dGkaNmz4xVlEsSEIgiAISqamgiuIrly5EoC6devKTd+wYQPdu3cHYPTo0SQlJdG/f3+ioqKoUqUKvr6+GBgYyJZfuHAhGhoatG/fnqSkJBo0aICXlxfq6upfnOU/eZ2NuOT8PRgpvw8Qzc/XskjO5wNEo/L5dTYM8vHgXwPd/JsNxADRfyO/DxA101f+tnfqfoRC1tOgmJlC1pPXxJgNQRAEQRCUKn8fSgiCIAjCf4AqulHyE1FsCIIgCIKS5fZMkv8a0Y0iCIIgCIJSiZYNQRAEQVAy0Y0iCIIgCIJS5fOTEJVOdKMIgiAIgqBUomUD2LBuNWdOnSDw2VO0tXUoU648g4aOwNHx3aVdJUli9R/L2bt7B3GxsZQsXYYx436jiIur0vNd87vKJq913LvnT0R4OAsWLaNeg3dXbjt10pfdO7cTcM+f6OhofHbuxa1YcaXn+sfuHT7s2eVD8OtXADg7u9Cjdz+q16wNwJlTJ9i3ewf3A/yJiY5mk89uirrlXb7wsFBWLVvIlYsXSElJwdbegdETp+JWvCSQ9d16rVnJoX27iIuLpXjJ0gwdNQGnIi4Kz+J/6xr7tm/iycMAoiIjGDt9PlVq1pPN//v8KXwP7ubJw/vExUazYM02nFzc5NYxcWgv/G9dk5tWs15jRkyarfC8nX9oSmjI62zTW7XtwOBRE/jz7EkO7dvFo/v3iI2J5o+NO3ApWkzhOXKye4cPe3b68Pqf7a6ICz3f2+4kSWLtH8vZt2dn1m+2VBlGjZuIsxJ/szam+sz4tQ6NKzmhq6XBo1dR9FtwjBuPQmXLTPi5Oj2blcVIX5ur94MZuuwkAc8jZfOXDmlM/fIOWJvqEZ+UxqV7r5i47jwPX7xRaNbrflfZ7LWegICs/crvi5ZSt/67/UpiYgJLFy3g3OlTxMREY21TmI6duvBjh58UmuNLbFq/hlXLF9Hupy4MHTkOgLOnT7B/9w4eBNwjJiaaDVt35el+Jbe+924U0bJB1o+uXYdObNjsw/JV68hIT2dg354kJSbKltm4YS1bN3sxeuxENm7ZgampGQP69iQhIUHp+ZKSkihatBhjx//20flly1Vg0FDVXNTHwtKSAYOG4bVlJ15bdlKxchVGDxvI0yePAEhOSqJM2fL0HzT8M2tSvLjYGAb26oqGhgZzFq/Ea/s++g8Zib5BIdky2zatZ+e2TQwZNZ4/vLZhYmrGyEG9SVTCd5ucnIxjkaL0Gjwmx/kpyUkUK1WOn3sP+uR6GjX/gfW7fWWPvsMnKDwrwPL1W9lx6LTsMWfxagBqN2gMZH23pUqX49f+Q5Ty/p9iYWlJ/8HD2Lh1Jxu37sS9UhVGDR3I08dZ291mr3Vs9d7IyLET2bBlByZmZgzq96vSfrNG+tqcXtiJtPQM2kzYRfle6xm76gzR8SmyZUa0r8zgtu4MW3aSmoO8CY1K4PDs9ujrasqWufEohN7zj1Lu1/W0Gr8TNTU1Dnm2o4CC2+GTkpJwdXNj9LiJOc5fMHc2f/91gWmec9m57zCdfu7GvNkzOXvmlEJzfE6A/x0O7N2Ji2tRuenJSUmULluevoOG5Wmer6WKe6PkJ6JlA1i6co3c88nTZtGoXg0CAvypULESkiSxbcsmfvm1D/UbZu1kp86YTeP6NTl25BD/a9dBqflq1qpNzVq1Pzq/RcvWALx+9VKpOT6mVp16cs/7DRzK3p0+3L19G+cirni0aAUgOwLNS1s3rcfCwoqxk2bIplnbFJb9W5Ikdvl406V7L2rXyzqqGzd5Jj80rcvJ44dp1ba9QvNUrFKDilVqfHR+3cYtAAjLoTXhfdo6OhibKP9KgkbGJnLPfTatw6awHWXLuwPQyKMlACHBef/dZtvuBg1lz04f7t65jVMRF3ze/mbrNWgEwOTpnnjUr8Xxo4do+6Pif7Mj2lfhZXgcfeYfk00LCo2VW2bADxWZu+0S+//KKoh+nXeU59v706F+CdYdvgXA+iO35V4/1esCV1d1x8HSkGfB0QrLW6NWbWp8Yr9y+9ZNWrRqjXulygC0/bE9e3ZuJ8D/LnXrNVBYjk9JTExg6sQxjJk4lY3rVsnNa9o8a78SrIL9ytf4husEhRAtGzmIj48DoFAhQwBevXpJZEQEVau9+yOhpaVFhYqVuH3rhkoy5lcZGRmcOHaEpKQkSpcpq+o4XPzzLG7FSzB57HDaNKnDr13acWjfLtn84NcveRMZQaWq7+5eqKWlRbkKFfG/fUsFib/M+ZNH6dq6PoO7/4jXyoUkJSq/hS0tLY2Txw/TtEUb1PLZIVZGRga+b7e7UmXK8vrtb7ZKNfnvtby7O3du3lRKhubVinD9UQhbJrbi+Y7+/L2iK794lJHNd7QyxNpUn5PXAmXTUtMy+PP2C6qWsMlxnQV1NOnapBTPgqN5GR6b4zLKUq5CRc6fPUNYaCiSJOF35TJBzwOpVr1mnmWYP3sG1WrWplKVann2noJyqLRlY9CgQbRv355atWp99TpSUlJISUmRm5YqaaKtrf1V65MkiQW/z6Fc+YqyZrvIiKxr2puayh9JmpqaEvz600eg34vHjx7Sq9tPpKamoqtbkDnzlyhlzENuvX71kv17dtC+U1e6/NKLAP87LJk/G01NLZo0b8WbyKy+cmMTU7nXGZuYEhocrIrIn1W7oQeW1oUxMjEl6NkTvNcsJfDJQ6b8vlKp7/vXudPEx8fRuHlrpb5Pbjx+9JBfu7633S1YgnMRF27fzDoIMPmg9cfExIyQYOX8Zp2sjejVohxLdvsxd9sl3ItZM79/fVLSMth60h8rEz0AwqLkC8Ow6ETsLQrJTevdshwzf62Dvq4W94MiaT52J2l5fM+iUWPHM2PKJJo1qou6hgYF1NSYOGU65SpUzJP3P3n8CA/vB7B28/Y8eT9lK5DPCvS8ptJiY/ny5axYsYIiRYrQs2dPunXrhpWVVa7W4enpydSpU+WmjZ0wifETJ39Vprme03n86AFrvbZkm/fhtiJJUr47wlMVB0dHNvnsIT4ujjOnfJk2aTwr125UecEhZWbiVrwkvd6OKXB1K07g0yfs372dJm+bYYFs36MkkW87SBu3aCv7t4OTCzaF7RjZtwtPHgZQpKjyBsgdPbSXylVrYGZuobT3yC0HR0c2b8/a7k6/t939I9vvU4m/2QJqalx/GMLkDX8CcOtJGCUcTOndohxbT/q/i/DB69R4u729x+fUPU5dC8TKVJ+hP1bCe2JL6g/dSkoe3ojQZ4s3d27fYsGSFVjb2HD9mh9zZk7DzNycKu+1BCpDaEgwi36fzcLlq7/6wDG/yZ97k7yj8m4UX19fmjVrxu+//469vT2tW7fm0KFDZGZ+WRU/btw4YmJi5B4jRo39qixzPWdw/uwZ/lizEUvLd0WPqVnW0VFEhPxd+968eYOJqfwR8fdKU1MLO3sHipcsRf/Bw3Ep6sb2bZtVHQtTM3McnIrITXNwdCYsNARA9v29iZT/bqOjIjEx+Ta+W+eixdHQ0CD4ZZDS3iM0+DU3rl7Co9X/lPYeX+P97W7A4OG4FnVj+9bNst9sZGS43PJvlPi9hryJJyAoUm7a/aA32FkYvJ2f1aJhaawnt4y5UUHCouVbO2ITU3nyOpq/7ryk0/T9uNmZ0LqG8s98+0dycjLLlyxi+Kgx1K5bD9eibnT4qTONmnjg7bVB6e//IOAeUW8i6dmlPbUrl6F25TLcuHaVXT5bqF25DBkZ+fvuz0J2Ki82SpcuzaJFi3j9+jXe3t6kpKTQpk0b7OzsmDBhAo8fP/7k67W1tSlUqJDcI7eVsCRJzJk1nTOnTrByzQYK29rKzS9c2BZTMzMuX7oom5aWlsr1a1cpU7Z8rt7r+yGRmqr6262XKlOOF88D5aa9CArE0soaAGsbW0xMzfC7/LdsflpaGjevX6NkPhhz8iWCAp+Qnp6OsanyBoweO7wPI2MTqlb/+i7PvCBJEmmpadi8/c1e+fv97zWVG35+lC5XTinv/bf/K4rayg+odbU1lg0SDQyJITgyngYVHGXzNTUKUKuMHZfufbprRw01tDTVFZ75Y9LT00lPT0NNTf5PRAF1dTIl5XfnVKxclc3b9+G1dbfsUaxESRp7tMBr627U1fPus1AYNQU9vlH55mwUTU1N2rdvT/v27QkKCmL9+vV4eXkxe/ZspVexc2ZN49jRw8xftIyCenpERGQdDenrG6Cjo4Oamho/de7KhnWrsbd3wM7egQ3rVqOjo0PTZi2Umg2yRmS/CHp31Prq1Use3A+gkKEh1tY2xMREExIcTFhYGACBgc+ArBYZMzNzpedbuXQh1WrUwsLKmsSEBE4cP8J1v6ssXJ51mmRMTDShIcFEvM33PDAwK5+pGaZKzteuU1cG9PwZ7w1rqNuwCff973Bo325GjJ8EZDWz/9ixC95ea7G1c6CwvT1bNqxBR0eHhk2aKzxPUlIiIa9eyJ6HBr/i2eMH6BsUwtzSmrjYGCLCQnjzdht8FRQIgJGJKcYmZgS/esH5k0epWLUmhQyNeBH4lA0rF+DsWoxipcopPC9AZmYmxw/vp1GzVqhryO8yYmNiCAsNJvJt3hdv85qYmmGixOIHYMWShVSrWQtLS2sSExM4cSxru1u0fDVqamp07NwVr3WrsXPI+s16rV2Njq4OTTyU85tduucaZxZ1YlTHKuw+/4BKbtb0aFaGgYt8Zcss33uNUT9V4fHrKB6/imJ0xyokpaSz/fQ9IGsQ6Y91i3HqWiAR0YnYmBkwokNlklLTOX71mULzfmy/YmhoiJW1DRXcK7F4wTy0dXSwtrbh+rWrHDm4n2Ejcz5tW5H09PSyXQ9FV7cghQwNZdNjY6IJCQkmIjxr2wt6e1CRF/uVr/G9X2dDTZI+7C3MOwUKFCAkJAQLi5z7gCVJ4uTJkzRq1ChX641Lzl3l7V42537uydNm0bL1D7Isq/9Yzp5d24mLjaVU6TKMHvdbtnO/v4R6Ls+X97t6mV49umWb3rJVG6bNnM2BfXuY/Nv4bPP79BtA3/6fvl5DTlJyORBt5pSJXL1yiciIcPT1DSjiWpSff/lV1q976MBeZkzOfh2Inn3606vvwFy9V/JX9Flf/PMca1Ys4uWLIKxtCtO+U1datPlRNv+fi3od3LuTuLhYSpQszZDRE3Aukvtm66iET7fm3L3px2/DemebXq9JSwaPncrpYwdYOmdKtvkduvWmY/e+RISFsHDmRIICn5CclIiZuSUVq9aiQ7feGLw9e+pTDHRyf3zhd/kiY4f2xWv7AWztHeXmHT+8n3kzsl//5eeefen2a/9cvY+Bbu6yzZgyEb/Ll4h4u925FC3Kz91/lZ2B8s9Fvd6/EN+of3EhPuvWCz67jEcVZ6b1qI1LYWMCQ2JYstuPDUdvyy3zz0W9jA10ZBf1uheY1Y1nbaLHiuFNKe9qibG+DmHRCVy485JZ3hd59DLqk+8ddjB319nxu3qFvj2z71datGrDlBmeRESEs3zxQi79/RexMTFYWdvww4/t6fxzt1yPe0lJ+/etIQN7d8elqJvsol6HD+xl1tTs1wjp0bs/PfsMyNW6zfSVf9x9+UmMQtZTpcjnf+f5kUqLDScnJ/z8/DBV8LiH3BYbeS23xUZey22xkZe+ptjIS58rNlTta4qNvJLbYiOvfUmxoUq5LTbykiKKDWXKi2LjylPFFBuVnb/NYkOlv+5nzxTbLCgIgiAI+VH+PsRUPpUPEBUEQRAE4b8tf7dbCoIgCMJ/wXfetCGKDUEQBEFQsu/9bBRRbAiCIAiCkuXTCxLnGTFmQxAEQRAEpRItG4IgCIKgZN95w4YoNgRBEARB6b7zakN0owiCIAiCoFSiZUMQBEEQlEycjSIIgiAIglKJs1EEQRAEQRCUSLRsCIIgCIKSfecNG//NYiMoMknVET7JRF9T1RE+ST8f3xlUr0D+zQagr52/8+X21uB5KZ/fDJmIQyNVHeGTzKoMUnWEj3p0On/fMTdP5PPtW9lEN4ogCIIgCEqVvw/DBEEQBOE/QJyNIgiCIAiCUuXjHsw8IYoNQRAEQVCy77zWEGM2BEEQBEFQLtGyIQiCIAjK9p03bYhiQxAEQRCU7HsfICq6UQRBEARBUCrRsiEIgiAISibORhEEQRAEQam+81pDdKMIgiAIgqBc32XLhv+ta+zfvoknjwKIioxgzLT5VKlZTzb/0vlT+B7azZOH94mLjWb+6m04ubjJ5oeFvKZvpxY5rnvkpDlUr9tI4ZnDw0JZtWwhVy5eICUlBVt7B0ZPnIpb8ZIASJKE15qVHNq3i7i4WIqXLM3QURNwKuKi8Cwfuu53lc1e6wkI8CciPJzfFy2lbv2GsvmJiQksXbSAc6dPERMTjbVNYTp26sKPHX5SerYN61Zz5tQJnj97ira2DmXKlWfg0BE4OjrJllm9chm+x44QGhKCpqYmxUqUoP/AoZQqUzbP8gW+l2/QB/lOn/Rlz64dBAT4ExMdzZbte3ArVlzp2SDru93kte6973YZ9d77bidPHMuhA/vkXlOqdFk2btmeJ/muvc13715WvgWLllGvwbt8kiSxauUydu/aQVxsLKVKl2HchEkUcXHN03wBb/PN/yDfHyuW4nv0CCGhIWhqaFK8REkGDB5KaSVsezbmhswY0prGNUqiq63Jo6Aw+k3dwo2AFwCsntqFn1tVlXvNldvPqNNtvty0KmWcmDKgBZVKO5KWnsHtB69oPXAFySlpCs0bHhbKmuULufL3BVLf7vNGTphK0WJZ+7wGVUvn+LreA4fTocsvCs2iEN9508Z3WWykJCfjWKQo9Zu2Yu6UUdnmJycnUaxUOarVacTK+dOzzTc1t2TdLl+5aScO7WGfz0bKV6mh8LxxsTEM7NWV8hUrMWfxSoyMTXj98gX6BoVky2zbtJ6d2zYxdtIMbO0d2Lx+NSMH9WbzzoMU1NNTeKb3JSUl4ermRss2PzB6+JBs8xfMnY3f1StM85yLjU1hLv39F3NmTsPMwoK69RooNdt1v6u069CJEiVLkZGRwcqlixjUtyc79hxCt2BBAOwdHBk1biKFbe1ISU5mm/dGBvb7lb0Hj2NsYpKn+VYsXcTAvj3Z+V6+pKQkypYrT8PGTZgxdZJS83woKSmJom7FaNWmLaOGD85xmeo1ajF5+izZc03NvLvRYFJSEkWLZuUbOSx7Pq/1a/He5MXUGZ44ODiyZvUf9O3dg30Hj6Knp6/0fMnv5RuVQz4HB0fGjP8ta9tLSWbL5o0M6NOT/Yd9FbrtGRnoctprOOeuPqLNwBWEvYnD2c6M6Dj5m1Ye/8ufPpO9Zc9T0zLk5lcp48T+Zf35fYMvw+fsJDU9gzJFC5OZKSksK2Tt84b07kq5ipWYvfDtPu/VC/T13+3zdh4+I/eaK3//ye8zJ1OrXsMPV5cvfO9no3yXxUaFKjWo8ImioG7jrFaLsJDXOc5XV1fH2MRMbtrlC2eoUa8xuroFFRf0ra2b1mNhYcXYSTNk06xtCsv+LUkSu3y86dK9F7Xf/tDGTZ7JD03rcvL4YVq1ba/wTO+rUas2NWrV/uj827du0qJVa9wrVQag7Y/t2bNzOwH+d5VebCxduUbu+aRps2hcrwYBAf5UqFgJgKbN5Fupho4cy/69u3n06AGVq1TL03yTp82i0Qf5mrdsDcDrV6+UmiUnn/tuATS1tDAzM8+jRPJq1qpNzY/kkySJrd6b6NmrLw0aNgZg+szZNKhbg6OHD/Fj+45Kz/e5z8+jeUu558NHjWXfnl08fPiAKlUVt+2N+KURL0Oi6DPlXSERFPwm23KpqemERsZ9dD1zR7Rlhc9Zft9wQjbtSVC4wnL+w2fzeswtrRj927t9ntV7+zwAE1P5ffBf589QrmJlbArbKTzPt+z8+fPMmzePa9euERwczN69e2nTpo1sviRJTJ06ldWrVxMVFUWVKlVYvnw5JUuWlC2TkpLCyJEj2bZtG0lJSTRo0IAVK1Zga2v7xTnEmA0FePLwHs8eP6CBRxulrP/in2dxK16CyWOH06ZJHX7t0o5D+3bJ5ge/fsmbyAgqVa0um6alpUW5ChXxv31LKZlyo1yFipw/e4aw0FAkScLvymWCngdSrXrNPM8SH5+1Iy1UyDDH+WlpqezdvQN9AwOKFi2Wl9GAz+fLj675XaFhner80LIJ06f8xpvISFVHAuDVy5dERIRTrfq7AwstLS0qVqzErVs3VJgsZ2lpqezZtT1r23NT7LbXvE5prt8LYsvcHjw/5cnf28bwyw/Vsy1Xy92V56c8ub1vEst/+wlz43etP+bG+lQu40T4m3jOeA0n8OQsfNcOoXo5Z4VmhXf7vKnjh/M/jzr06dqOw+/t8z70JjKCy3/9iUfLHxSeRVHU1BTzyK2EhATKli3LsmXLcpw/d+5cFixYwLJly7h69SpWVlY0atSIuLh3RefQoUPZu3cvPj4+XLhwgfj4eFq0aEFGRkaO68zJd9myoWgnj+zH1sGJYqWU08f/+tVL9u/ZQftOXenySy8C/O+wZP5sNDW1aNK8lWznbmxiKvc6YxNTQoODlZIpN0aNHc+MKZNo1qgu6hoaFFBTY+KU6ZSrUDFPc0iSxMLf51CufEVcXIvKzfvz3BkmjBlJcnISZmbmLPtjHUbGxnmeb8FH8uVXNWrWpmHjplhb2/D61UtWLl9C31+74719N1paWirNFhGZdcRtYir/uzA1NSU4OOdWS1U4f+4M40aNyNr2zM1ZuXo9xgre9pwKm9GrXS2WeJ9m7jpf3Es5MH/0j6SkpbP10BUAfP+6x54TNwgKfoNjYVMm9W/B0dWDqd5pLqlp6TjZZrUkTOjTjHEL93L7wUs6t6jMkVWDqNhulkJbOIJfv+TAnh38+FNXOnXrxf17d1i2cDaaWlo0btYq2/K+Rw5QUK8gtermzy4UUN2QDQ8PDzw8PHKcJ0kSixYtYsKECbRt2xaAjRs3YmlpydatW+nTpw8xMTGsW7eOzZs307Bh1ufr7e2NnZ0dJ0+epEmTJl+UQ+XFxtKlS/Hz86N58+a0b9+ezZs34+npSWZmJm3btmXatGloaHw8ZkpKCikpKXLTUlPS0dLWVnb0t++fzJ+njtLu515Kew8pMxO34iXp1T9rPISrW3ECnz5h/+7tNGn+7oen9kHZK0nki5O7fbZ4c+f2LRYsWYG1jQ3Xr/lljdkwN6dK1exHV8oy13M6jx89YI3Xlmzz3CtVYcuOPURHR7Fv907GjxrGBu/t2f5Q5UW+tTnky68aN20m+7eLa1GKlyxFiyYNuHD+LPXfdl2oWrbfRQ7TVKlSpSps27WX6Kgo9u7eyZiRQ9m0ZYdCt70CBdS4fi+IycsOAnDrwUtKFLGmd7tasmJjl+912fL3ngRz/V4QD45Mw6NWSfafvkWBAlmf2brdF9h84JJsPXUru9GtdTUmLT2gsLxSZiZFi5fk137v9nnPnz7hwJ7tORYbxw7tpUHj5nm23/8qCtrkcvqbp62tjfZX/N+fPXtGSEgIjRu/+61qa2tTp04dLl68SJ8+fbh27RppaWlyy9jY2FCqVCkuXrz4xcWGSrtRpk+fzoQJE0hISGDIkCHMmTOHYcOG0blzZ7p168batWuZPj37AM33eXp6YmhoKPdYs+z3PPofwN/nTpKakiwb56EMpmbmODgVkZvm4OhMWGgI8O7I7U1khNwy0VGRmJjk3R/LnCQnJ7N8ySKGjxpD7br1cC3qRoefOtOoiQfeXhvyLMc8zxmcP3uGlWs2YmlplW2+bsGC2Nk7ULpMOX6bOhN1DXX279udZ/nmvs33x0fyfSvMzS2wtrEhKOi5qqNgZpo1jiQyQv538SYyMk+LyM/RLVgQe3sHypQtx+RpM1FX12Df3o93GXyNkIhYAp6GyE27/ywEO6uPt6CERMQSFPwGF/uszzE4PBYg23oefGY9X8PEzBwHR/l9nv17+7z33b55jRfPA2nW+n8KzZBf5fQ3z9PT86vWFRKS9XlaWlrKTbe0tJTNCwkJQUtLK1tr2/vLfAmVtmx4eXnh5eVF27ZtuXXrFhUrVmTjxo107twZgGLFijF69GimTp360XWMGzeO4cOHy017EpGu1NzvO3V0P+7V62BopLwm91JlyvHieaDctBdBgVhaWQNgbWOLiakZfpf/xtUt65TItLQ0bl6/Rp+BQ5WW60ukp6eTnp6Gmpp8XVtAXZ1MKVPp7y9JEvM8Z3D29En+WLeRwl84oEmSIC01VcnpsvLNfZtvVS7y5VfR0VGEhgSrbMDo+wrb2mJmZs6lvy9SrHgJIGtcxLVrVxkydISK032cJEmkKnjb+/vmU4o6WMhNc7W3yHGQ6D9MDPWwtTQmOCKryHj+OpLXYdEUdZRfj4uDBb5/3VNo3lJlyvEiKFBu2ssX7/Z57zt6YA9Fi5WgiKtbtnn5iaLORsnpb97XtGq8L3uruPTZ1r8vWeZ9Ki02goODcXd3B6Bs2bIUKFCAcuXKyeZXqFCB168/3beaU/ORVlzCJ1+TlJRIyKsXsudhwa949vgB+gaFMLe0Ji42hoiwEN5EZPVBvnoRCICRiancWSjBr4K4d/s6EzyXfPb/+m+069SVAT1/xnvDGuo2bMJ9/zsc2rebEeOzToNUU1Pjx45d8PZai62dA4Xt7dmyYQ06Ojo0bNJcqdkg6zoaL4KCZM9fvXrJg/sBGBoaYmVtQwX3SixeMA9tHR2srW24fu0qRw7uZ9jIMUrPNmfWNI4fPczvi5ZRUE+PiLffqb6+ATo6OiQlJrJ+7Spq162HmZk5MTHR7Nq+jbDQEBo0+rLmwX+b79jRw8z/SD6AmJhoQoKDCQ8PA+B54DMATM3MlP5H/cPv9vXb77bQ2yOqVSuW0aBRY8zMzHn9+hXLlyzEyMhY7loSeZnv1Xv5rK1t6NSlK+vWrsLewQF7ewfWrVmFjo4OHs2V1xL5pfmMDI1Yu+YP6tStj5m5OTHR0ex8u+01atxUoTmWep/mjNcIRvVozO4T16lU0pEe/6vBwOnbANDT1WJi3+bsO3WT4PAYHGxMmTaoJZHR8Rw4/W6Q+cKNJ5nYtzl3Hr7i1oOXdGlZBTdHSzqNWqfQvP/r2JXBvX5mi9ca6jZowv17dzi8bzfDxsqf+p2QEM/50yfoO3ikQt9fGRTVc/e1XSY5sbLKakUNCQnB2vpdIRcWFiZr7bCysiI1NZWoqCi51o2wsDCqV//ybnA1SZIUe4J0Ljg7O7NixQqaNm3Ko0ePKFasGD4+PrRr1w6AI0eOMGDAAJ49e5ar9fq/+nSxcfemH5OG9842vV6TlgwaM5XTxw6wbO6UbPPbd+1Nx+59Zc+91y7l3IkjrNp2mAIFvrxHykQ/99chuPjnOdasWMTLF0FY2xSmfaeutGjzo2z+Pxf1Orh3J3FxsZQoWZohoyfgXCT3Fy/S18ldDep39Qp9e3bLNr1FqzZMmeFJREQ4yxcv5NLffxEbE4OVtQ0//Niezj93y3XfeW631kplc7741aRps2jZ+gdSUlKYOHYk/nduEx0dhaGRESVKlqZHr76ULJXzRYM+Jbf7E/eP5Jv8Nh/Awf17mTppfLZlevUdQJ9+A3OXL5eft9/Vy/T5yHc7buIURgwdwIOAAOLi4jAzN8e9UmX6DRyCVQ5HoJ9T4Ct2xn5XL9OrR/Z8LVu1YdrM2e8u6rVzB7GxMbKLen3NANyv2VH6Xb1M74/kGz9pKuPHjOTunVtER2VteyVLlubXPv2+atszqzLok/M9apVi2qBWuNibE/gqkiXep9mw9yIAOtqa7FjQm7LFbDEy0CUkIpZzVx8ybcUhXoZGy61n5C+N6NO+NsaGBbnz8BUTFu3j4s2nn3zvR6cX5Pr/8/eFc6xb+XafZ12YH3/qSvP39nkAh/btZMXCuew4fBp9fYNcv8c/bI2VP5j5cVjS5xf6Ai4Wul/9WjU1NblTXyVJwsbGhmHDhjF69GgAUlNTsbCwYM6cObIBoubm5nh7e9O+fdZlFIKDg7G1teXIkSNfPGZDpcXGxIkTWb16Na1bt+bUqVN07NiRLVu2MG7cONTU1Jg5cyY//vgjCxbkbkP9XLGhal9TbOSl3BYbeUl1W+uXyT/DDnOWnwZGfuhrio28lM83vc8WG6r0NcVGXsqLYuOJgoqNIrksNuLj43n8+DEA5cuXZ8GCBdSrVw8TExPs7e2ZM2cOnp6ebNiwAVdXV2bNmsXZs2d58OABBgZZBVy/fv04dOgQXl5emJiYMHLkSCIjI7l27Rrq6upflEOlf1WmTp2Krq4uly5dok+fPowZM4YyZcowevRoEhMTadmy5WcHiAqCIAhCvqeiYtrPz4969d7djuOf8R7dunXDy8uL0aNHk5SURP/+/WUX9fL19ZUVGgALFy5EQ0OD9u3byy7q5eXl9cWFBqi4ZUNZRMvGvyNaNr5ePj84Fy0b/0I+3/REy8a/kCctG+EKatkw//puFFXKv39VBEEQBOE/QtwbRRAEQRAEpcrHjYp5QtwbRRAEQRAEpRItG4IgCIKgZN95w4YoNgRBEARB6b7zakMUG4IgCIKgZN/7AFExZkMQBEEQBKUSLRuCIAiCoGTf+9kootgQBEEQBCX7zmsN0Y0iCIIgCIJyiZYNQRAEQVAy0Y0iCIIgCIKSfd/Vxn/yRmxhcWmqjvBJLyMVc0MeZSlmY/D5hVQkKS1D1RE+SVfry++CqAr5+deemZ/DAQkp+XvbS03PVHWEj3LrukbVET4p6dBApb/Hy6hUhawnL24apwyiZUMQBEEQlEx0owiCIAiCoFTfea0hzkYRBEEQBEG5RMuGIAiCICiZ6EYRBEEQBEGpvvd7o4hiQxAEQRCU7fuuNcSYDUEQBEEQlEu0bAiCIAiCkn3nDRui2BAEQRAEZfveB4iKbhRBEARBEJRKtGwIgiAIgpKJs1EE1q9azoY1K+WmmZiasv/4OQAkSWLD6hUc2LuLuLhYSpQszfAxE3Eq4qKUPAF3rnNo52aePrpP9JsIhk+eR6XqdXNcdu3iWZw6spef+wyjWdtOctPv3LhCVGQEOrq6FC1ehp96DqKwvaPC817zu8omr3Xcu+dPRHg4CxYto16DhrL5p076snvndgLu+RMdHY3Pzr24FSuu8Bw52bPTh707txMc/AoAJ2cXevTuR7UatQB4ExnBiiULuPL3ReLi4yhXviLDx0zAzt4hT/LlJCEhnhVLl3D61Emi3kTiVqw4o8dOoGTp0nme5Z/vNuDtdzv/g+/2jxVL8T16hJDQEDQ1NCleoiQDBg+ldJmyeZLvut9VNnutJyAgK9/vi5ZSt/67fJGRESxdOJ9Lf/9FXFwcFSq4M2rcBOwdHJWebcPq5Xh9uF8xMWXv2/3K+36fNZWDe3cycNgY2nX6WenZALr80JTQkNfZprds24HBoybITVs0exqH9++i35BRtO2o+HwTOlVmYqfKctNCohJw+nkDAHo6mszoXo2WVZ0xMdDheVgsKw7cZs3Ru7LlLY0KMqtHdeqXt8NAV4uHL6OYt/Mae/96ovC8X+X7rjVEsfEPJ2cXFq5YK3teQP1dD9PWjevZvnUT4yfPwM7ekY3rVjFsQC+27j5EQT09hWdJSU7C3rkodRq3ZOH0MR9d7urFszy+fxdjU/Ns85xci1GjflPMzK2Ij4tll/dqPMcPZMnG/RRQV+zNwpKSkihatBit2rRl5LDBOc4vW64CDRs3ZfqU3xT63p9jYWFJv8HDsLWzB+DIwf2MGTYQr227cXIuwpjhg9HQ0GD2wqXo6enj472RwX17snX3AXR1C+Zp1n9Mm/Qbjx8/YobnHMwtLDhy8AB9e/3C7v2HsbC0zNMsye99t6Ny+G4dHBwZM/43CtvakZKSzJbNGxnQpyf7D/tibGKi9HxJSUm4urnRss0PjB4+RG6eJEmMHDIQDQ0N5i9ejp6ePls2e9G/dw927j2EbkHlf79Ozi7MX/5uv6Kunr3n+s+zpwi4exszcwul53nfsvVbycx8d/O2wCePGTOkN3UaNJZb7q9zpwm4dwdTM+Xm838eSfMJ+2XPM97LNrdXTeqULswv80/wPDSWhuXtWdy/DsFvEjh0+RkA60Y0xFBPm3bTDxMRk0yHukXZPLoJNYbt4NbTCKVmFz5PjNl4S11DHVMzM9nD2DhrRylJEju2babrL72pU78Rzi6uTJg6i5TkZE4cO6yULOUq1aBD935Urln/o8u8iQjDa/k8BoyZjrpG9pqxQbO2FC9dAXMrG5xci9G+Wz8iw0MJDw1WeN6atWozYPBQGjRsnOP8Fi1b06ffAKpWrabw9/6cmnXqUb1mbewdHLF3cKTvwCHoFiyI/51bvAh6jv+dW4waP4kSJUvj4OjEyHG/kZSUyIljR/I8K0BycjKnTvoydPhIKrpXwt7egb4DBmFT2Jad27fleZ4an/luPZq3pEq16tja2VHExZXho8YSHx/Pw4cP8ixf/0FDqZ9DvqDngdy5fYuxEydTslRpHJ2cGDthEkmJiRw/qpzf7ofU1eX3K0bG8gVYeFgoi+fNYuL0OWjk8DtWJiNjE0xMzWSPS3+dw6awHWXKu8uWiQgLZdn8WYyb4qn0fOkZmYRGJ8oeEbHJsnlVilnhffo+f955RVBYHOuP+3P7WQQVXC3klllx8DZ+D8MIDI1lznY/ohNSKVck+8GYKqgp6PGtEsXGWy+DgmjTtB7tWzVh8riRvH75AoDgVy95ExlBparVZctqaWlRroI7d2/fVEnWzMxMls+dTIsfu2DnWOSzyycnJ3HO9yAWVjaYmuftkXF+kpGRwYnjR0hOSqJUmbKkpWbd8llL690tm9XV1dHU1OT2zesqyphORkYGWtractO1dbS5cf2aSjJ9qbS0VPbs2o6+gQFF3YqpOg5pqWkAaL/3Waqrq6OhqcnNG3nz/b58EURbj3p0aN2EqePf7Vcg63c8c/I4OnbprrQu2S+VlpbGqeOHadKiDWpvT5vIzMxkzrTxtOvcHUdn5edzsTHi6cZfCFjblU2jG+NoWUg27+K9YFpUdsLGNKsluXbpwrjaGHHyepDcMj/WcsVYXxs1NWhX2xVtzQKcv/NK6dm/hJqaYh7fKpV2owQHB7Ny5UouXLhAcHAw6urqODk50aZNG7p37466gpv7P6ZEqTJMmDoLOwcHoiIj2bhuFf16dmHT9v1ERmY1v5mYmsq9xtjUlJDg7P2deeHAjo2oq6vTtE3HTy7ne3AnW9cuJSU5CRs7R8Z7LkdDUzOPUuYfTx49pHf3TqSmpqKrWxDP+UtwcnYhPS0NK2sb/li2iNETJqOrq8s2741ERkQQER6ukqx6evqUKVuONX+swMnZGVNTM44dOczd27exd1DdOJJPOX/uDONGjSA5OQkzc3NWrl6PsbGxqmPh6OSEtY0NyxYvZPykKejq6rJl09vvN0L532/xkmUYP3UWtvZZ+5XN61cxoGcXvLbvx9DIiK0b16Gurs7/OnZRepbPuXjuNPHxcTRu3lo2bfvm9RRQ1+CH9p2V/v5XH4Tw64KTPHoVjYWRLmM7VuLM7/+jYv9tvIlLZsSq86wYVJ8nG38hLT2DTAn6LTnNxXvvWmp/nnOczWOa8NqnF2npGSSmpNNh5lGehcQqPb/weSorNvz8/GjYsCFOTk7o6ury8OFDOnfuTGpqKiNHjmTdunUcP34cAwODT64nJSWFlJQU+WmpBeSOZj6n6tvBggC4QMkyZenYxoOjh/ZTsnSZrOkflJSSJMmOAPLS00cBHNvnw6zl3p99/5r1PShdoQrRbyI4tMubxTPHMWXhWrS0vvyz+S+wd3Rk47bdxMXHcfbUCWZMGs/ytV44Obswa94iPKf9RtO61VFXV8e9clXZ4FFVmeE5lymTxtOkfh3U1dUpVrwEHs1aEBBwT6W5PqZSpSps27WX6Kgo9u7eyZiRQ9m0ZUe2Aj2vaWhqMnfBEqZPnkj9mlVRV1encpVqVK+ZN99vTvuVTm08OHZ4P+UquLPbx5s13jtVsh/50NFDe6lctYZs3MjD+/fYu2MLK7y250k+32vvWij8n8Pl+yH4r/2ZLg2KsWTfTQa0LEtlN0v+N+0QQWFx1Cxlw+J+dQh5k8CZWy8BmPJzVYz1tfGYsI/I2CRaVnVmy9imNByzB//nkUr/P3zO9342isq6UYYOHcqwYcO4ceMGFy9eZOPGjTx8+BAfHx+ePn1KUlISEydO/Ox6PD09MTQ0lHssmT/nX2XT1S2IcxFXXr54jqmpGQBvIuQHGEW/eYOJSd7vTO/fuUFsdBSDurSks0dVOntUJSI0GO81ixnUtZXcsgX19LEubE/x0hUYNnEOr18EcvWvs3meWdU0NbWwtXegeIlS9Bs0DJeibuzY6g1AsRIl2eizB99zlzjge5aFy1cTExONtU1hleW1s7dnnZc3F69c5+jJM3j77CQ9PZ3ChW1VlulTdAsWxN7egTJlyzF52kzU1TXYt3eXqmMBULxESbbu3MvZv65w7NR5lv6xhpjoGJV8lrq6BXFyydqv3L5xnaioN7Rv2Yj6VctSv2pZQoJfs2LxPDq0ynl8jLKEBr/mxtVLeLT6n2za3ZvXiI56Q+cfmtCkZnma1CxPaMhrVi2dT5cfmio9U2JKOv6BkRSxMURHS52pXasyZu0FjlwJ5G5gJH8cusOuPx8xtG15AJysCtGvZRn6LD7N2VsvufMsklnbrnL9cRh9WuT9WVw5Ed0oKnL9+nU2bdoke96pUyd69OhBaGgolpaWzJ07l+7du7N48eJPrmfcuHEMHz5cblpM6r+roVJTU3ke+Iwy5StiXdgWE1Mzrl7+m6JvT9dMS0vj5nU/+g4a9q/e52vUatiM0hXkTxHzHD+YWg08qNO45SdfKyGRnpaqzHjfBEmSSPvgc9B/24L2Iug59+/506vfIFVEk6NbsCC6BQsSGxPDxYsXGDp8pKojfRFJkkhNzV/b2T/fb9DzQALu3aXfwOxn1ihbamoqQYHPKFOuIo2btaRi5apy80cN7kNjj5Z4tGyTp7mOH96HkbEJVaq/a4lp6NGS8pXk840b2o+GHi1o8l5Xi7JoaRSgmJ0Jf/kHo6leAC1NdTIlSW6ZjEyJAm//+hbUzuoezsz8+DKCaqms2LCwsCA4OBhnZ2cAQkNDSU9Pp1ChrEFBrq6uvHnz5rPr0dbWztZlkhyXlqssyxfNo3qtulhaWRMV9YZN61aRkBCPR4vWqKmp0f6nn/HesAY7e3ts7RzYvGEN2jo6NGraPFfv86WSkxIJef1uIFl4yGsCnzxA38AQMwsrDAoZyS2vrqGBobEpNnaOAIQGv+TvcycoU7EqhQyNeRMRxsEdm9DS0qFc5RoKz5uYmMCLoHfNoK9eveTB/QAKGRpibW1DTEw0IcHBhIWFARAYmHWqmqmZGWZmyh0p/sfSRVStUQtLKysSExI4cfwoN65dZcGyVQCcPnEcI2NjLK2sefL4EYvmeVK7bn2qVFP85/SlLv71J5IEjo5OvAh6zsL583B0dKJVm7Z5nuVT362RoRFr1/xBnbr1MTM3JyY6mp3btxEWGkKjxso/+v1UPkNDQ6ysbTjpewwjYxOsrK15/Ogh8+fMok69BlStrvzvd8VH9itNW7TG0MgIQyMjueU1NDQwMTXD3tFJ6dn+kZmZyfHD+2nUrJXcWW2FDI0oZJhDPhNT7BwUn8+zRw0OX3nGi/A4LAwLMqajOwYFtdhy6j5xSWmcv/OKWT1qkJSaQVBYLLVKFaZz/WKMWXsBgAcvo3j8OpplA+sybv1fRMYm06qaMw3K2dF22iGF5xVyT2XFRps2bejbty/z5s1DW1ub6dOnU6dOHXR1dQF48OABhQvnTVN2WGgoUyeMJiY6CiNjE0qWKsMfG7ZiZW0DQKduPUhJSWb+7BnEx8VSvFQZFixbrZRrbAA8fRjA9NF9Zc83r1oIQO1Gzek3cspnX6+ppc2Duzc5uteHhPhYDI1MKF66PFMXrsXQSPHXPrjnf5dePbrJns+fNxuAlq3aMG3mbM6dOc3k38bL5o8dldUS1affAPr2V24Lwps3kUz7bSyREeHo6Rvg4lqUBctWUfnt2UUREeEsWTCXN5ERmJqZ49GiFb/06vuZtSpXfFw8SxctIDQ0BENDIxo0asSAwcPQVMHg3nv+d+n93ne74L3vdvykqQQ+e8ahA4OJjorC0MiIkiVLs27jFoq4uOZRPn/69nyXb+G8rC7UFq3aMGWGJxHh4SycN4fIyEjMzM1o3rI1v/bplyfZwsNCmTbx3X6lRKkyrFz/br+SH1y/eomwkGCatmij0hyFzfTYNKoJpoV0iIhN4sr9UOqM2ElQeBwAXeccZ1q3aniNbISxvg5BYXFM2XxJdlGv9IxM2kw5yIxu1dn1Wwv0dTV5EhzDrwtPctzvuSr/azLfewOLmiR90DaVR+Lj4+nZsyd79uwhIyODatWq4e3tjZNTVtXs6+tLTEwM7dq1y/W6w3LZspHXXkYmqTrCJxWz+fSgXFVKSstQdYRP0tXKmzOovpZqfu1f5sNm8vwmISV/b3up6ZmfX0hF3LquUXWET0o6NFDp7xGTpJjvx1D327xihcpaNvT19dm+fTvJycmkp6ejr68vN79x47wdJCUIgiAIgnKo/HLlOjo6qo4gCIIgCEr1vXejqLzYEARBEIT/uu+81hCXKxcEQRAEQblEy4YgCIIgKNt33rQhig1BEARBUDJxuXJBEARBEAQlEi0bgiAIgqBk4mwUQRAEQRCU6juvNUSxIQiCIAhK951XG2LMhiAIgiAISiVaNgRBEARByb73s1FEsSEIgiAISva9DxAV3SiCIAiCICiXJHxScnKyNHnyZCk5OVnVUXKUn/Pl52ySJPL9W/k5X37OJkki37+Rn7MJH6cmSZKk6oInP4uNjcXQ0JCYmBgKFSqk6jjZ5Od8+TkbiHz/Vn7Ol5+zgcj3b+TnbMLHiW4UQRAEQRCUShQbgiAIgiAolSg2BEEQBEFQKlFsfIa2tjaTJ09GW1tb1VFylJ/z5edsIPL9W/k5X37OBiLfv5GfswkfJwaICoIgCIKgVKJlQxAEQRAEpRLFhiAIgiAISiWKDUEQBEEQlEoUG4IgCIIgKJUoNj5jxYoVODk5oaOjQ8WKFfnzzz9VHQmA8+fP07JlS2xsbFBTU2Pfvn2qjiTj6elJpUqVMDAwwMLCgjZt2vDgwQNVx5JZuXIlZcqUoVChQhQqVIhq1apx9OhRVcfKkaenJ2pqagwdOlTVUQCYMmUKampqcg8rKytVx5Lz6tUrunTpgqmpKQULFqRcuXJcu3ZN1bEAcHR0zPb5qampMWDAAFVHIz09nYkTJ+Lk5ISuri7Ozs5MmzaNzMxMVUeTiYuLY+jQoTg4OKCrq0v16tW5evWqqmMJX0AUG5+wfft2hg4dyoQJE7hx4wa1atXCw8ODoKAgVUcjISGBsmXLsmzZMlVHyebcuXMMGDCAS5cuceLECdLT02ncuDEJCQmqjgaAra0ts2fPxs/PDz8/P+rXr0/r1q3x9/dXdTQ5V69eZfXq1ZQpU0bVUeSULFmS4OBg2ePOnTuqjiQTFRVFjRo10NTU5OjRo9y7d4/58+djZGSk6mhA1nf6/md34sQJANq1a6fiZDBnzhz++OMPli1bRkBAAHPnzmXevHksXbpU1dFkfv31V06cOMHmzZu5c+cOjRs3pmHDhrx69UrV0YTPUe2tWfK3ypUrS3379pWbVqxYMWns2LEqSpQzQNq7d6+qY3xUWFiYBEjnzp1TdZSPMjY2ltauXavqGDJxcXGSq6urdOLECalOnTrSkCFDVB1JkiRJmjx5slS2bFlVx/ioMWPGSDVr1lR1jC82ZMgQqUiRIlJmZqaqo0jNmzeXevToITetbdu2UpcuXVSUSF5iYqKkrq4uHTp0SG562bJlpQkTJqgolfClRMvGR6SmpnLt2jUaN24sN71x48ZcvHhRRam+TTExMQCYmJioOEl2GRkZ+Pj4kJCQQLVq1VQdR2bAgAE0b96chg0bqjpKNo8ePcLGxgYnJyc6duzI06dPVR1J5sCBA7i7u9OuXTssLCwoX748a9asUXWsHKWmpuLt7U2PHj1QU1NTdRxq1qzJqVOnePjwIQC3bt3iwoULNGvWTMXJsqSnp5ORkYGOjo7cdF1dXS5cuKCiVMKX0lB1gPwqIiKCjIwMLC0t5aZbWloSEhKiolTfHkmSGD58ODVr1qRUqVKqjiNz584dqlWrRnJyMvr6+uzdu5cSJUqoOhYAPj4+XL9+PV/2RVepUoVNmzZRtGhRQkNDmTFjBtWrV8ff3x9TU1NVx+Pp06esXLmS4cOHM378eK5cucLgwYPR1tama9euqo4nZ9++fURHR9O9e3dVRwFgzJgxxMTEUKxYMdTV1cnIyGDmzJn89NNPqo4GgIGBAdWqVWP69OkUL14cS0tLtm3bxuXLl3F1dVV1POEzRLHxGR8ecUiSlC+OQr4VAwcO5Pbt2/nuyMPNzY2bN28SHR3N7t276datG+fOnVN5wfHixQuGDBmCr69vtiO4/MDDw0P279KlS1OtWjWKFCnCxo0bGT58uAqTZcnMzMTd3Z1Zs2YBUL58efz9/Vm5cmW+KzbWrVuHh4cHNjY2qo4CZI1R8/b2ZuvWrZQsWZKbN28ydOhQbGxs6Natm6rjAbB582Z69OhB4cKFUVdXp0KFCnTq1Inr16+rOprwGaLY+AgzMzPU1dWztWKEhYVla+0QcjZo0CAOHDjA+fPnsbW1VXUcOVpaWri4uADg7u7O1atXWbx4MatWrVJprmvXrhEWFkbFihVl0zIyMjh//jzLli0jJSUFdXV1FSaUp6enR+nSpXn06JGqowBgbW2drWAsXrw4u3fvVlGinD1//pyTJ0+yZ88eVUeRGTVqFGPHjqVjx45AVjH5/PlzPD09802xUaRIEc6dO0dCQgKxsbFYW1vToUMHnJycVB1N+AwxZuMjtLS0qFixomy0+D9OnDhB9erVVZTq2yBJEgMHDmTPnj2cPn36m9gRSJJESkqKqmPQoEED7ty5w82bN2UPd3d3OnfuzM2bN/NVoQGQkpJCQEAA1tbWqo4CQI0aNbKdZv3w4UMcHBxUlChnGzZswMLCgubNm6s6ikxiYiIFCsj/SVBXV89Xp77+Q09PD2tra6Kiojh+/DitW7dWdSThM0TLxicMHz6cn3/+GXd3d6pVq8bq1asJCgqib9++qo5GfHw8jx8/lj1/9uwZN2/exMTEBHt7exUmyxrcuHXrVvbv34+BgYGsdcjQ0BBdXV2VZgMYP348Hh4e2NnZERcXh4+PD2fPnuXYsWOqjoaBgUG2sS16enqYmprmizEvI0eOpGXLltjb2xMWFsaMGTOIjY3NN0e+w4YNo3r16syaNYv27dtz5coVVq9ezerVq1UdTSYzM5MNGzbQrVs3NDTyzy64ZcuWzJw5E3t7e0qWLMmNGzdYsGABPXr0UHU0mePHjyNJEm5ubjx+/JhRo0bh5ubGL7/8oupowueo9FyYb8Dy5cslBwcHSUtLS6pQoUK+OX3zzJkzEpDt0a1bN1VHyzEXIG3YsEHV0SRJkqQePXrIvlNzc3OpQYMGkq+vr6pjfVR+OvW1Q4cOkrW1taSpqSnZ2NhIbdu2lfz9/VUdS87BgwelUqVKSdra2lKxYsWk1atXqzqSnOPHj0uA9ODBA1VHkRMbGysNGTJEsre3l3R0dCRnZ2dpwoQJUkpKiqqjyWzfvl1ydnaWtLS0JCsrK2nAgAFSdHS0qmMJX0DcYl4QBEEQBKUSYzYEQRAEQVAqUWwIgiAIgqBUotgQBEEQBEGpRLEhCIIgCIJSiWJDEARBEASlEsWGIAiCIAhKJYoNQRAEQRCUShQbgvAfMmXKFMqVKyd73r17d9q0afOv1qmIdQiC8H0TxYYg5IHu3bujpqaGmpoampqaODs7M3LkSBISEpT6vosXL8bLy+uLlg0MDERNTY2bN29+9ToEQRBykn8uzC8I/3FNmzZlw4YNpKWl8eeff/Lrr7+SkJDAypUr5ZZLS0tDU1NTIe9paGiYL9YhCML3TbRsCEIe0dbWxsrKCjs7Ozp16kTnzp3Zt2+frOtj/fr1ODs7o62tjSRJxMTE0Lt3bywsLChUqBD169fn1q1bcuucPXs2lpaWGBgY0LNnT5KTk+Xmf9gFkpmZyZw5c3BxcUFbWxt7e3tmzpwJILs7b/ny5VFTU6Nu3bo5riMlJYXBgwdjYWGBjo4ONWvW5OrVq7L5Z8+eRU1NjVOnTuHu7k7BggWpXr16truxCoLw/RDFhiCoiK6uLmlpaQA8fvyYHTt2sHv3blk3RvPmzQkJCeHIkSNcu3aNChUq0KBBA968eQPAjh07mDx5MjNnzsTPzw9ra2tWrFjxyfccN24cc+bM4bfffuPevXts3boVS0tLAK5cuQLAyZMnCQ4OZs+ePTmuY/To0ezevZuNGzdy/fp1XFxcaNKkiSzXPyZMmMD8+fPx8/NDQ0MjX909VBCEPKbiG8EJwnehW7duUuvWrWXPL1++LJmamkrt27eXJk+eLGlqakphYWGy+adOnZIKFSokJScny62nSJEi0qpVqyRJkqRq1apJffv2lZtfpUoVqWzZsjm+b2xsrKStrS2tWbMmx4zPnj2TAOnGjRsfzR4fHy9pampKW7Zskc1PTU2VbGxspLlz50qS9O6OxCdPnpQtc/jwYQmQkpKSPv4hCYLwnyVaNgQhjxw6dAh9fX10dHSoVq0atWvXZunSpQA4ODhgbm4uW/batWvEx8djamqKvr6+7PHs2TOePHkCQEBAANWqVZN7jw+fvy8gIICUlBQaNGjw1f+HJ0+ekJaWRo0aNWTTNDU1qVy5MgEBAXLLlilTRvZva2trAMLCwr76vQVB+HaJAaKCkEfq1avHypUr0dTUxMbGRm4QqJ6entyymZmZWFtbc/bs2WzrMTIy+qr319XV/arXvU+SJADU1NSyTf9w2vv/v3/mZWZm/usMgiB8e0TLhiDkET09PVxcXHBwcPjs2SYVKlQgJCQEDQ0NXFxc5B5mZmYAFC9enEuXLsm97sPn73N1dUVXV5dTp07lOF9LSwuAjIyMj67DxcUFLS0tLly4IJuWlpaGn58fxYsX/+T/SRCE75do2RCEfKhhw4ZUq1aNNm3aMGfOHNzc3Hj9+jVHjhyhTZs2uLu7M2TIELp164a7uzs1a9Zky5Yt+Pv74+zsnOM6dXR0GDNmDKNHj0ZLS4saNWoQHh6Ov78/PXv2xMLCAl1dXY4dO4atrS06OjrZTnvV09OjX79+jBo1ChMTE+zt7Zk7dy6JiYn07NkzLz4aQRC+QaLYEIR8SE1NjSNHjjBhwgR69OhBeHg4VlZW1K5dW3b2SIcOHXjy5AljxowhOTmZ//3vf/Tr14/jx49/dL2//fYbGhoaTJo0idevX2NtbU3fvn0B0NDQYMmSJUybNo1JkyZRq1atHLtxZs+eTWZmJj///DNxcXG4u7tz/PhxjI2NlfJZCILw7VOT/umEFQRBEARBUAIxZkMQBEEQBKUSxYYgCIIgCEolig1BEARBEJRKFBuCIAiCICiVKDYEQRAEQVAqUWwIgiAIgqBUotgQBEEQBEGpRLEhCIIgCIJSiWJDEARBEASlEsWGIAiCIAhKJYoNQRAEQRCUShQbgiAIgiAo1f8Bfk3xVlwlJeUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot_cm('./figs/cm.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainer):\n",
    "    train_accu = trainer.train_precs\n",
    "    test_accu = trainer.test_precs\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_accu, 'r-', label='Training Accuracy')\n",
    "    plt.plot(x, test_accu, 'b-', label='Testing Accuracy')\n",
    "    plt.title('Training and Testing Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(trainer):\n",
    "    train_accu = trainer.train_losses\n",
    "    test_accu = trainer.test_losses\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, train_accu, 'r-', label='Training Loss')\n",
    "    plt.plot(x, test_accu, 'b-', label='Testing Loss')\n",
    "    plt.title('Training and Testing Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 22:55:29 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/390]\tTime 0.011 (0.011)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.002 (0.005)\tLoss 2.4991 (3.3599)\tPrec@1 32.031 (22.636)\n",
      "Epoch: [1][156/390]\tTime 0.009 (0.004)\tLoss 2.4369 (2.8159)\tPrec@1 24.219 (26.866)\n",
      "Epoch: [1][234/390]\tTime 0.002 (0.004)\tLoss 1.9684 (2.5443)\tPrec@1 34.375 (29.451)\n",
      "Epoch: [1][312/390]\tTime 0.005 (0.005)\tLoss 1.8863 (2.3738)\tPrec@1 34.375 (31.178)\n",
      "Epoch: [1][390/390]\tTime 0.001 (0.005)\tLoss 1.6280 (2.2570)\tPrec@1 41.250 (32.432)\n",
      "EPOCH: 1 train Results: Prec@1 32.432 Loss: 2.2570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.5203 (1.5203)\tPrec@1 47.656 (47.656)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4963 (1.6156)\tPrec@1 31.250 (42.860)\n",
      "EPOCH: 1 val Results: Prec@1 42.860 Loss: 1.6156\n",
      "Best Prec@1: 42.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/390]\tTime 0.005 (0.005)\tLoss 1.6112 (1.6112)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [2][78/390]\tTime 0.002 (0.003)\tLoss 1.5126 (1.6195)\tPrec@1 45.312 (43.008)\n",
      "Epoch: [2][156/390]\tTime 0.002 (0.004)\tLoss 1.6208 (1.6061)\tPrec@1 44.531 (43.252)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.004)\tLoss 1.7222 (1.5965)\tPrec@1 42.969 (43.544)\n",
      "Epoch: [2][312/390]\tTime 0.003 (0.004)\tLoss 1.7094 (1.5855)\tPrec@1 40.625 (43.960)\n",
      "Epoch: [2][390/390]\tTime 0.007 (0.004)\tLoss 1.4745 (1.5758)\tPrec@1 52.500 (44.314)\n",
      "EPOCH: 2 train Results: Prec@1 44.314 Loss: 1.5758\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3885 (1.3885)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4268 (1.5008)\tPrec@1 31.250 (46.490)\n",
      "EPOCH: 2 val Results: Prec@1 46.490 Loss: 1.5008\n",
      "Best Prec@1: 46.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/390]\tTime 0.005 (0.005)\tLoss 1.4851 (1.4851)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [3][78/390]\tTime 0.008 (0.009)\tLoss 1.4391 (1.4743)\tPrec@1 41.406 (47.646)\n",
      "Epoch: [3][156/390]\tTime 0.002 (0.008)\tLoss 1.3910 (1.4640)\tPrec@1 50.781 (48.134)\n",
      "Epoch: [3][234/390]\tTime 0.008 (0.006)\tLoss 1.4514 (1.4645)\tPrec@1 42.969 (48.311)\n",
      "Epoch: [3][312/390]\tTime 0.002 (0.006)\tLoss 1.5078 (1.4632)\tPrec@1 50.000 (48.230)\n",
      "Epoch: [3][390/390]\tTime 0.001 (0.006)\tLoss 1.4212 (1.4601)\tPrec@1 51.250 (48.280)\n",
      "EPOCH: 3 train Results: Prec@1 48.280 Loss: 1.4601\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.3230 (1.3230)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4953 (1.4318)\tPrec@1 43.750 (49.310)\n",
      "EPOCH: 3 val Results: Prec@1 49.310 Loss: 1.4318\n",
      "Best Prec@1: 49.310\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/390]\tTime 0.002 (0.002)\tLoss 1.4993 (1.4993)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [4][78/390]\tTime 0.006 (0.008)\tLoss 1.4838 (1.3895)\tPrec@1 55.469 (51.226)\n",
      "Epoch: [4][156/390]\tTime 0.002 (0.006)\tLoss 1.4162 (1.3892)\tPrec@1 46.094 (50.931)\n",
      "Epoch: [4][234/390]\tTime 0.002 (0.005)\tLoss 1.5272 (1.3868)\tPrec@1 47.656 (50.775)\n",
      "Epoch: [4][312/390]\tTime 0.002 (0.005)\tLoss 1.3532 (1.3878)\tPrec@1 52.344 (50.774)\n",
      "Epoch: [4][390/390]\tTime 0.003 (0.005)\tLoss 1.3277 (1.3876)\tPrec@1 51.250 (50.776)\n",
      "EPOCH: 4 train Results: Prec@1 50.776 Loss: 1.3876\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2728 (1.2728)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4631 (1.3819)\tPrec@1 31.250 (50.290)\n",
      "EPOCH: 4 val Results: Prec@1 50.290 Loss: 1.3819\n",
      "Best Prec@1: 50.290\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/390]\tTime 0.004 (0.004)\tLoss 1.3705 (1.3705)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.005)\tLoss 1.3997 (1.3053)\tPrec@1 50.000 (53.807)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.005)\tLoss 1.3703 (1.3289)\tPrec@1 54.688 (52.831)\n",
      "Epoch: [5][234/390]\tTime 0.007 (0.005)\tLoss 1.3619 (1.3279)\tPrec@1 58.594 (52.836)\n",
      "Epoch: [5][312/390]\tTime 0.002 (0.005)\tLoss 1.2987 (1.3298)\tPrec@1 54.688 (52.733)\n",
      "Epoch: [5][390/390]\tTime 0.005 (0.005)\tLoss 1.3211 (1.3338)\tPrec@1 58.750 (52.574)\n",
      "EPOCH: 5 train Results: Prec@1 52.574 Loss: 1.3338\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2136 (1.2136)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.006 (0.001)\tLoss 1.3947 (1.3527)\tPrec@1 37.500 (51.490)\n",
      "EPOCH: 5 val Results: Prec@1 51.490 Loss: 1.3527\n",
      "Best Prec@1: 51.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/390]\tTime 0.003 (0.003)\tLoss 1.2183 (1.2183)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [6][78/390]\tTime 0.002 (0.005)\tLoss 1.3751 (1.2636)\tPrec@1 43.750 (55.667)\n",
      "Epoch: [6][156/390]\tTime 0.003 (0.005)\tLoss 1.1498 (1.2735)\tPrec@1 60.156 (55.096)\n",
      "Epoch: [6][234/390]\tTime 0.002 (0.004)\tLoss 1.3359 (1.2845)\tPrec@1 53.125 (54.731)\n",
      "Epoch: [6][312/390]\tTime 0.004 (0.004)\tLoss 1.3691 (1.2956)\tPrec@1 49.219 (54.288)\n",
      "Epoch: [6][390/390]\tTime 0.007 (0.005)\tLoss 1.5191 (1.3019)\tPrec@1 50.000 (54.026)\n",
      "EPOCH: 6 train Results: Prec@1 54.026 Loss: 1.3019\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.2339 (1.2339)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.004)\tLoss 1.3587 (1.3346)\tPrec@1 43.750 (51.320)\n",
      "EPOCH: 6 val Results: Prec@1 51.320 Loss: 1.3346\n",
      "Best Prec@1: 51.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/390]\tTime 0.005 (0.005)\tLoss 1.3259 (1.3259)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [7][78/390]\tTime 0.002 (0.005)\tLoss 1.2340 (1.2446)\tPrec@1 57.812 (55.973)\n",
      "Epoch: [7][156/390]\tTime 0.004 (0.004)\tLoss 1.4204 (1.2565)\tPrec@1 46.094 (55.519)\n",
      "Epoch: [7][234/390]\tTime 0.009 (0.006)\tLoss 1.3231 (1.2662)\tPrec@1 50.781 (54.990)\n",
      "Epoch: [7][312/390]\tTime 0.002 (0.006)\tLoss 1.2994 (1.2704)\tPrec@1 53.906 (54.825)\n",
      "Epoch: [7][390/390]\tTime 0.001 (0.005)\tLoss 1.3562 (1.2776)\tPrec@1 51.250 (54.622)\n",
      "EPOCH: 7 train Results: Prec@1 54.622 Loss: 1.2776\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2084 (1.2084)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4724 (1.3130)\tPrec@1 18.750 (52.880)\n",
      "EPOCH: 7 val Results: Prec@1 52.880 Loss: 1.3130\n",
      "Best Prec@1: 52.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/390]\tTime 0.005 (0.005)\tLoss 1.2051 (1.2051)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [8][78/390]\tTime 0.002 (0.004)\tLoss 1.2271 (1.2241)\tPrec@1 55.469 (56.458)\n",
      "Epoch: [8][156/390]\tTime 0.002 (0.004)\tLoss 1.3590 (1.2454)\tPrec@1 48.438 (55.419)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.004)\tLoss 1.2339 (1.2477)\tPrec@1 55.469 (55.416)\n",
      "Epoch: [8][312/390]\tTime 0.010 (0.004)\tLoss 1.1045 (1.2505)\tPrec@1 64.062 (55.511)\n",
      "Epoch: [8][390/390]\tTime 0.003 (0.004)\tLoss 1.3797 (1.2581)\tPrec@1 50.000 (55.340)\n",
      "EPOCH: 8 train Results: Prec@1 55.340 Loss: 1.2581\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1711 (1.1711)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4565 (1.3065)\tPrec@1 31.250 (53.270)\n",
      "EPOCH: 8 val Results: Prec@1 53.270 Loss: 1.3065\n",
      "Best Prec@1: 53.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/390]\tTime 0.004 (0.004)\tLoss 1.1923 (1.1923)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [9][78/390]\tTime 0.002 (0.004)\tLoss 1.1151 (1.2010)\tPrec@1 64.844 (57.466)\n",
      "Epoch: [9][156/390]\tTime 0.007 (0.005)\tLoss 1.3641 (1.2135)\tPrec@1 50.781 (57.011)\n",
      "Epoch: [9][234/390]\tTime 0.008 (0.005)\tLoss 1.3672 (1.2309)\tPrec@1 47.656 (56.220)\n",
      "Epoch: [9][312/390]\tTime 0.006 (0.005)\tLoss 1.3375 (1.2389)\tPrec@1 48.438 (55.960)\n",
      "Epoch: [9][390/390]\tTime 0.002 (0.005)\tLoss 1.4929 (1.2480)\tPrec@1 47.500 (55.558)\n",
      "EPOCH: 9 train Results: Prec@1 55.558 Loss: 1.2480\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1522 (1.1522)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3399 (1.3017)\tPrec@1 31.250 (53.560)\n",
      "EPOCH: 9 val Results: Prec@1 53.560 Loss: 1.3017\n",
      "Best Prec@1: 53.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/390]\tTime 0.013 (0.013)\tLoss 1.1641 (1.1641)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [10][78/390]\tTime 0.004 (0.004)\tLoss 1.0087 (1.1868)\tPrec@1 65.625 (57.812)\n",
      "Epoch: [10][156/390]\tTime 0.007 (0.004)\tLoss 1.2396 (1.2072)\tPrec@1 56.250 (57.195)\n",
      "Epoch: [10][234/390]\tTime 0.003 (0.005)\tLoss 1.1993 (1.2202)\tPrec@1 60.156 (56.533)\n",
      "Epoch: [10][312/390]\tTime 0.003 (0.005)\tLoss 1.3262 (1.2287)\tPrec@1 49.219 (56.257)\n",
      "Epoch: [10][390/390]\tTime 0.002 (0.005)\tLoss 1.0870 (1.2354)\tPrec@1 62.500 (55.962)\n",
      "EPOCH: 10 train Results: Prec@1 55.962 Loss: 1.2354\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2086 (1.2086)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.007 (0.002)\tLoss 1.7199 (1.2792)\tPrec@1 25.000 (53.790)\n",
      "EPOCH: 10 val Results: Prec@1 53.790 Loss: 1.2792\n",
      "Best Prec@1: 53.790\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/390]\tTime 0.016 (0.016)\tLoss 1.1264 (1.1264)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [11][78/390]\tTime 0.002 (0.006)\tLoss 1.1389 (1.1872)\tPrec@1 57.812 (57.575)\n",
      "Epoch: [11][156/390]\tTime 0.004 (0.004)\tLoss 1.0176 (1.1970)\tPrec@1 65.625 (57.300)\n",
      "Epoch: [11][234/390]\tTime 0.003 (0.006)\tLoss 1.2753 (1.2061)\tPrec@1 53.906 (56.805)\n",
      "Epoch: [11][312/390]\tTime 0.011 (0.006)\tLoss 1.1048 (1.2123)\tPrec@1 63.281 (56.684)\n",
      "Epoch: [11][390/390]\tTime 0.012 (0.006)\tLoss 1.2133 (1.2213)\tPrec@1 58.750 (56.304)\n",
      "EPOCH: 11 train Results: Prec@1 56.304 Loss: 1.2213\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1051 (1.1051)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4246 (1.2827)\tPrec@1 37.500 (54.020)\n",
      "EPOCH: 11 val Results: Prec@1 54.020 Loss: 1.2827\n",
      "Best Prec@1: 54.020\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/390]\tTime 0.008 (0.008)\tLoss 1.0834 (1.0834)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [12][78/390]\tTime 0.002 (0.005)\tLoss 1.1179 (1.1682)\tPrec@1 60.156 (58.534)\n",
      "Epoch: [12][156/390]\tTime 0.002 (0.005)\tLoss 1.2895 (1.1816)\tPrec@1 54.688 (58.081)\n",
      "Epoch: [12][234/390]\tTime 0.006 (0.005)\tLoss 1.0579 (1.1943)\tPrec@1 70.312 (57.437)\n",
      "Epoch: [12][312/390]\tTime 0.015 (0.005)\tLoss 1.2523 (1.2032)\tPrec@1 58.594 (57.336)\n",
      "Epoch: [12][390/390]\tTime 0.005 (0.005)\tLoss 1.0246 (1.2108)\tPrec@1 61.250 (57.030)\n",
      "EPOCH: 12 train Results: Prec@1 57.030 Loss: 1.2108\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1413 (1.1413)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3414 (1.2785)\tPrec@1 50.000 (54.350)\n",
      "EPOCH: 12 val Results: Prec@1 54.350 Loss: 1.2785\n",
      "Best Prec@1: 54.350\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/390]\tTime 0.009 (0.009)\tLoss 1.1582 (1.1582)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [13][78/390]\tTime 0.016 (0.007)\tLoss 1.1310 (1.1562)\tPrec@1 58.594 (58.633)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.006)\tLoss 1.1211 (1.1674)\tPrec@1 60.938 (58.569)\n",
      "Epoch: [13][234/390]\tTime 0.003 (0.005)\tLoss 1.2753 (1.1844)\tPrec@1 56.250 (57.985)\n",
      "Epoch: [13][312/390]\tTime 0.003 (0.005)\tLoss 1.1894 (1.1968)\tPrec@1 55.469 (57.431)\n",
      "Epoch: [13][390/390]\tTime 0.001 (0.005)\tLoss 1.1324 (1.2011)\tPrec@1 57.500 (57.298)\n",
      "EPOCH: 13 train Results: Prec@1 57.298 Loss: 1.2011\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1078 (1.1078)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4721 (1.2647)\tPrec@1 25.000 (54.510)\n",
      "EPOCH: 13 val Results: Prec@1 54.510 Loss: 1.2647\n",
      "Best Prec@1: 54.510\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/390]\tTime 0.002 (0.002)\tLoss 1.0893 (1.0893)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [14][78/390]\tTime 0.003 (0.007)\tLoss 1.1517 (1.1479)\tPrec@1 55.469 (59.246)\n",
      "Epoch: [14][156/390]\tTime 0.002 (0.005)\tLoss 1.2876 (1.1618)\tPrec@1 53.125 (58.489)\n",
      "Epoch: [14][234/390]\tTime 0.002 (0.005)\tLoss 1.2420 (1.1779)\tPrec@1 57.031 (58.042)\n",
      "Epoch: [14][312/390]\tTime 0.003 (0.004)\tLoss 1.0632 (1.1844)\tPrec@1 57.812 (57.852)\n",
      "Epoch: [14][390/390]\tTime 0.002 (0.005)\tLoss 1.3009 (1.1905)\tPrec@1 51.250 (57.620)\n",
      "EPOCH: 14 train Results: Prec@1 57.620 Loss: 1.1905\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1148 (1.1148)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4374 (1.2633)\tPrec@1 37.500 (54.690)\n",
      "EPOCH: 14 val Results: Prec@1 54.690 Loss: 1.2633\n",
      "Best Prec@1: 54.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/390]\tTime 0.013 (0.013)\tLoss 1.0361 (1.0361)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [15][78/390]\tTime 0.007 (0.007)\tLoss 1.2745 (1.1277)\tPrec@1 53.125 (59.939)\n",
      "Epoch: [15][156/390]\tTime 0.002 (0.005)\tLoss 0.9859 (1.1443)\tPrec@1 67.188 (59.494)\n",
      "Epoch: [15][234/390]\tTime 0.004 (0.005)\tLoss 1.1105 (1.1587)\tPrec@1 63.281 (58.780)\n",
      "Epoch: [15][312/390]\tTime 0.004 (0.005)\tLoss 1.2683 (1.1706)\tPrec@1 57.031 (58.451)\n",
      "Epoch: [15][390/390]\tTime 0.001 (0.005)\tLoss 1.1553 (1.1773)\tPrec@1 47.500 (58.204)\n",
      "EPOCH: 15 train Results: Prec@1 58.204 Loss: 1.1773\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1225 (1.1225)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4208 (1.2652)\tPrec@1 43.750 (54.590)\n",
      "EPOCH: 15 val Results: Prec@1 54.590 Loss: 1.2652\n",
      "Best Prec@1: 54.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/390]\tTime 0.005 (0.005)\tLoss 1.0229 (1.0229)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.003)\tLoss 1.0704 (1.1061)\tPrec@1 65.625 (60.750)\n",
      "Epoch: [16][156/390]\tTime 0.003 (0.004)\tLoss 1.1696 (1.1312)\tPrec@1 55.469 (59.753)\n",
      "Epoch: [16][234/390]\tTime 0.006 (0.005)\tLoss 1.2645 (1.1511)\tPrec@1 53.906 (58.976)\n",
      "Epoch: [16][312/390]\tTime 0.002 (0.005)\tLoss 1.2219 (1.1660)\tPrec@1 52.344 (58.377)\n",
      "Epoch: [16][390/390]\tTime 0.001 (0.005)\tLoss 1.4107 (1.1748)\tPrec@1 50.000 (58.036)\n",
      "EPOCH: 16 train Results: Prec@1 58.036 Loss: 1.1748\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0294 (1.0294)\tPrec@1 70.312 (70.312)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2924 (1.2579)\tPrec@1 37.500 (54.960)\n",
      "EPOCH: 16 val Results: Prec@1 54.960 Loss: 1.2579\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/390]\tTime 0.005 (0.005)\tLoss 1.1394 (1.1394)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [17][78/390]\tTime 0.003 (0.003)\tLoss 1.2286 (1.1129)\tPrec@1 57.031 (60.789)\n",
      "Epoch: [17][156/390]\tTime 0.004 (0.004)\tLoss 1.2811 (1.1306)\tPrec@1 57.812 (60.062)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.004)\tLoss 1.1470 (1.1487)\tPrec@1 58.594 (59.292)\n",
      "Epoch: [17][312/390]\tTime 0.002 (0.004)\tLoss 1.2060 (1.1553)\tPrec@1 56.250 (58.891)\n",
      "Epoch: [17][390/390]\tTime 0.002 (0.004)\tLoss 1.1106 (1.1647)\tPrec@1 63.750 (58.556)\n",
      "EPOCH: 17 train Results: Prec@1 58.556 Loss: 1.1647\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1790 (1.1790)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4220 (1.2552)\tPrec@1 25.000 (54.870)\n",
      "EPOCH: 17 val Results: Prec@1 54.870 Loss: 1.2552\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/390]\tTime 0.007 (0.007)\tLoss 0.9817 (0.9817)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [18][78/390]\tTime 0.004 (0.009)\tLoss 1.1556 (1.1110)\tPrec@1 58.594 (60.542)\n",
      "Epoch: [18][156/390]\tTime 0.008 (0.008)\tLoss 1.0814 (1.1243)\tPrec@1 63.281 (60.171)\n",
      "Epoch: [18][234/390]\tTime 0.004 (0.006)\tLoss 1.0179 (1.1460)\tPrec@1 64.844 (59.235)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.005)\tLoss 1.1984 (1.1534)\tPrec@1 59.375 (58.886)\n",
      "Epoch: [18][390/390]\tTime 0.002 (0.005)\tLoss 1.2003 (1.1603)\tPrec@1 55.000 (58.688)\n",
      "EPOCH: 18 train Results: Prec@1 58.688 Loss: 1.1603\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1063 (1.1063)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1349 (1.2546)\tPrec@1 56.250 (54.920)\n",
      "EPOCH: 18 val Results: Prec@1 54.920 Loss: 1.2546\n",
      "Best Prec@1: 54.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/390]\tTime 0.002 (0.002)\tLoss 1.1313 (1.1313)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [19][78/390]\tTime 0.026 (0.003)\tLoss 1.2312 (1.0887)\tPrec@1 56.250 (61.333)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.005)\tLoss 1.0781 (1.1179)\tPrec@1 63.281 (60.395)\n",
      "Epoch: [19][234/390]\tTime 0.003 (0.004)\tLoss 1.1585 (1.1304)\tPrec@1 54.688 (59.927)\n",
      "Epoch: [19][312/390]\tTime 0.007 (0.004)\tLoss 1.0056 (1.1429)\tPrec@1 64.844 (59.542)\n",
      "Epoch: [19][390/390]\tTime 0.006 (0.004)\tLoss 1.4464 (1.1508)\tPrec@1 47.500 (59.166)\n",
      "EPOCH: 19 train Results: Prec@1 59.166 Loss: 1.1508\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1915 (1.1915)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.6055 (1.2502)\tPrec@1 37.500 (55.210)\n",
      "EPOCH: 19 val Results: Prec@1 55.210 Loss: 1.2502\n",
      "Best Prec@1: 55.210\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/390]\tTime 0.007 (0.007)\tLoss 1.0732 (1.0732)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [20][78/390]\tTime 0.002 (0.004)\tLoss 1.0319 (1.0932)\tPrec@1 60.938 (61.076)\n",
      "Epoch: [20][156/390]\tTime 0.004 (0.003)\tLoss 1.0047 (1.1082)\tPrec@1 65.625 (60.644)\n",
      "Epoch: [20][234/390]\tTime 0.004 (0.004)\tLoss 1.0367 (1.1296)\tPrec@1 64.062 (59.797)\n",
      "Epoch: [20][312/390]\tTime 0.004 (0.004)\tLoss 1.1510 (1.1378)\tPrec@1 60.938 (59.567)\n",
      "Epoch: [20][390/390]\tTime 0.002 (0.004)\tLoss 1.3127 (1.1448)\tPrec@1 50.000 (59.306)\n",
      "EPOCH: 20 train Results: Prec@1 59.306 Loss: 1.1448\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1366 (1.1366)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2634 (1.2421)\tPrec@1 43.750 (55.820)\n",
      "EPOCH: 20 val Results: Prec@1 55.820 Loss: 1.2421\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/390]\tTime 0.003 (0.003)\tLoss 1.2056 (1.2056)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [21][78/390]\tTime 0.002 (0.005)\tLoss 1.2114 (1.0702)\tPrec@1 52.344 (62.223)\n",
      "Epoch: [21][156/390]\tTime 0.003 (0.005)\tLoss 1.1794 (1.1005)\tPrec@1 52.344 (60.928)\n",
      "Epoch: [21][234/390]\tTime 0.007 (0.005)\tLoss 1.1575 (1.1193)\tPrec@1 57.812 (60.166)\n",
      "Epoch: [21][312/390]\tTime 0.006 (0.005)\tLoss 1.1194 (1.1337)\tPrec@1 64.062 (59.732)\n",
      "Epoch: [21][390/390]\tTime 0.003 (0.005)\tLoss 1.1345 (1.1430)\tPrec@1 58.750 (59.432)\n",
      "EPOCH: 21 train Results: Prec@1 59.432 Loss: 1.1430\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1490 (1.1490)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1926 (1.2412)\tPrec@1 43.750 (55.430)\n",
      "EPOCH: 21 val Results: Prec@1 55.430 Loss: 1.2412\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/390]\tTime 0.006 (0.006)\tLoss 1.1429 (1.1429)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [22][78/390]\tTime 0.005 (0.004)\tLoss 1.0447 (1.0988)\tPrec@1 65.625 (61.234)\n",
      "Epoch: [22][156/390]\tTime 0.002 (0.004)\tLoss 1.1255 (1.1106)\tPrec@1 60.938 (60.584)\n",
      "Epoch: [22][234/390]\tTime 0.002 (0.004)\tLoss 1.1368 (1.1206)\tPrec@1 62.500 (60.170)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.005)\tLoss 1.1809 (1.1331)\tPrec@1 57.812 (59.667)\n",
      "Epoch: [22][390/390]\tTime 0.002 (0.005)\tLoss 1.1565 (1.1427)\tPrec@1 65.000 (59.338)\n",
      "EPOCH: 22 train Results: Prec@1 59.338 Loss: 1.1427\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1340 (1.1340)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.005)\tLoss 1.0669 (1.2483)\tPrec@1 62.500 (55.290)\n",
      "EPOCH: 22 val Results: Prec@1 55.290 Loss: 1.2483\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/390]\tTime 0.005 (0.005)\tLoss 1.0761 (1.0761)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [23][78/390]\tTime 0.002 (0.006)\tLoss 1.0389 (1.0739)\tPrec@1 64.062 (61.897)\n",
      "Epoch: [23][156/390]\tTime 0.004 (0.006)\tLoss 1.2716 (1.0976)\tPrec@1 52.344 (61.286)\n",
      "Epoch: [23][234/390]\tTime 0.004 (0.006)\tLoss 1.1561 (1.1066)\tPrec@1 59.375 (60.954)\n",
      "Epoch: [23][312/390]\tTime 0.006 (0.006)\tLoss 1.2206 (1.1222)\tPrec@1 52.344 (60.304)\n",
      "Epoch: [23][390/390]\tTime 0.002 (0.006)\tLoss 1.3163 (1.1350)\tPrec@1 52.500 (59.750)\n",
      "EPOCH: 23 train Results: Prec@1 59.750 Loss: 1.1350\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1983 (1.1983)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2615 (1.2485)\tPrec@1 37.500 (55.490)\n",
      "EPOCH: 23 val Results: Prec@1 55.490 Loss: 1.2485\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/390]\tTime 0.004 (0.004)\tLoss 1.1168 (1.1168)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.006)\tLoss 1.2432 (1.0667)\tPrec@1 60.938 (62.381)\n",
      "Epoch: [24][156/390]\tTime 0.016 (0.006)\tLoss 1.2218 (1.0929)\tPrec@1 54.688 (60.997)\n",
      "Epoch: [24][234/390]\tTime 0.002 (0.005)\tLoss 1.2551 (1.1149)\tPrec@1 54.688 (60.273)\n",
      "Epoch: [24][312/390]\tTime 0.002 (0.005)\tLoss 1.2940 (1.1250)\tPrec@1 53.906 (59.989)\n",
      "Epoch: [24][390/390]\tTime 0.004 (0.005)\tLoss 1.3143 (1.1309)\tPrec@1 57.500 (59.654)\n",
      "EPOCH: 24 train Results: Prec@1 59.654 Loss: 1.1309\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1614 (1.1614)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1625 (1.2555)\tPrec@1 43.750 (55.440)\n",
      "EPOCH: 24 val Results: Prec@1 55.440 Loss: 1.2555\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/390]\tTime 0.002 (0.002)\tLoss 1.0410 (1.0410)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [25][78/390]\tTime 0.003 (0.004)\tLoss 1.0798 (1.0687)\tPrec@1 61.719 (62.104)\n",
      "Epoch: [25][156/390]\tTime 0.002 (0.005)\tLoss 1.2158 (1.0910)\tPrec@1 57.812 (61.346)\n",
      "Epoch: [25][234/390]\tTime 0.002 (0.005)\tLoss 1.1880 (1.1078)\tPrec@1 57.812 (60.701)\n",
      "Epoch: [25][312/390]\tTime 0.003 (0.005)\tLoss 1.2508 (1.1194)\tPrec@1 57.031 (60.296)\n",
      "Epoch: [25][390/390]\tTime 0.008 (0.005)\tLoss 1.2591 (1.1245)\tPrec@1 51.250 (60.066)\n",
      "EPOCH: 25 train Results: Prec@1 60.066 Loss: 1.1245\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1582 (1.1582)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1056 (1.2498)\tPrec@1 56.250 (55.190)\n",
      "EPOCH: 25 val Results: Prec@1 55.190 Loss: 1.2498\n",
      "Best Prec@1: 55.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/390]\tTime 0.006 (0.006)\tLoss 1.0919 (1.0919)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [26][78/390]\tTime 0.002 (0.005)\tLoss 1.1185 (1.0567)\tPrec@1 63.281 (62.905)\n",
      "Epoch: [26][156/390]\tTime 0.007 (0.004)\tLoss 1.1518 (1.0837)\tPrec@1 59.375 (61.858)\n",
      "Epoch: [26][234/390]\tTime 0.006 (0.005)\tLoss 1.0749 (1.1009)\tPrec@1 60.938 (61.084)\n",
      "Epoch: [26][312/390]\tTime 0.002 (0.004)\tLoss 1.0487 (1.1133)\tPrec@1 63.281 (60.518)\n",
      "Epoch: [26][390/390]\tTime 0.001 (0.004)\tLoss 1.2085 (1.1202)\tPrec@1 56.250 (60.256)\n",
      "EPOCH: 26 train Results: Prec@1 60.256 Loss: 1.1202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1329 (1.1329)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3095 (1.2434)\tPrec@1 43.750 (55.870)\n",
      "EPOCH: 26 val Results: Prec@1 55.870 Loss: 1.2434\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/390]\tTime 0.003 (0.003)\tLoss 1.0716 (1.0716)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [27][78/390]\tTime 0.002 (0.003)\tLoss 0.9578 (1.0553)\tPrec@1 62.500 (62.747)\n",
      "Epoch: [27][156/390]\tTime 0.002 (0.003)\tLoss 0.9521 (1.0790)\tPrec@1 64.844 (61.699)\n",
      "Epoch: [27][234/390]\tTime 0.005 (0.003)\tLoss 0.9027 (1.0924)\tPrec@1 71.875 (61.184)\n",
      "Epoch: [27][312/390]\tTime 0.002 (0.003)\tLoss 1.1286 (1.1067)\tPrec@1 62.500 (60.698)\n",
      "Epoch: [27][390/390]\tTime 0.003 (0.003)\tLoss 1.4782 (1.1192)\tPrec@1 52.500 (60.334)\n",
      "EPOCH: 27 train Results: Prec@1 60.334 Loss: 1.1192\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1028 (1.1028)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1252 (1.2488)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 27 val Results: Prec@1 55.150 Loss: 1.2488\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/390]\tTime 0.003 (0.003)\tLoss 1.0103 (1.0103)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [28][78/390]\tTime 0.002 (0.003)\tLoss 1.0020 (1.0605)\tPrec@1 65.625 (62.727)\n",
      "Epoch: [28][156/390]\tTime 0.002 (0.003)\tLoss 1.1067 (1.0780)\tPrec@1 64.062 (61.958)\n",
      "Epoch: [28][234/390]\tTime 0.002 (0.003)\tLoss 1.1031 (1.0940)\tPrec@1 57.031 (61.316)\n",
      "Epoch: [28][312/390]\tTime 0.002 (0.003)\tLoss 1.2498 (1.1068)\tPrec@1 54.688 (60.788)\n",
      "Epoch: [28][390/390]\tTime 0.003 (0.003)\tLoss 1.3011 (1.1152)\tPrec@1 43.750 (60.450)\n",
      "EPOCH: 28 train Results: Prec@1 60.450 Loss: 1.1152\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1445 (1.1445)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1534 (1.2398)\tPrec@1 50.000 (55.800)\n",
      "EPOCH: 28 val Results: Prec@1 55.800 Loss: 1.2398\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/390]\tTime 0.002 (0.002)\tLoss 1.0195 (1.0195)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [29][78/390]\tTime 0.002 (0.003)\tLoss 1.2215 (1.0559)\tPrec@1 57.031 (62.747)\n",
      "Epoch: [29][156/390]\tTime 0.004 (0.003)\tLoss 1.0964 (1.0778)\tPrec@1 61.719 (61.709)\n",
      "Epoch: [29][234/390]\tTime 0.003 (0.003)\tLoss 0.9127 (1.0925)\tPrec@1 72.656 (61.094)\n",
      "Epoch: [29][312/390]\tTime 0.003 (0.004)\tLoss 1.0782 (1.1070)\tPrec@1 57.812 (60.655)\n",
      "Epoch: [29][390/390]\tTime 0.001 (0.004)\tLoss 1.3130 (1.1164)\tPrec@1 50.000 (60.320)\n",
      "EPOCH: 29 train Results: Prec@1 60.320 Loss: 1.1164\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.0754 (1.0754)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9349 (1.2405)\tPrec@1 62.500 (55.870)\n",
      "EPOCH: 29 val Results: Prec@1 55.870 Loss: 1.2405\n",
      "Best Prec@1: 55.870\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/390]\tTime 0.005 (0.005)\tLoss 1.0915 (1.0915)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [30][78/390]\tTime 0.005 (0.003)\tLoss 1.0368 (1.0532)\tPrec@1 61.719 (62.352)\n",
      "Epoch: [30][156/390]\tTime 0.003 (0.003)\tLoss 1.0874 (1.0745)\tPrec@1 64.844 (61.828)\n",
      "Epoch: [30][234/390]\tTime 0.004 (0.003)\tLoss 1.0740 (1.0891)\tPrec@1 62.500 (61.293)\n",
      "Epoch: [30][312/390]\tTime 0.002 (0.003)\tLoss 1.1225 (1.1003)\tPrec@1 56.250 (60.763)\n",
      "Epoch: [30][390/390]\tTime 0.002 (0.003)\tLoss 1.0556 (1.1112)\tPrec@1 62.500 (60.394)\n",
      "EPOCH: 30 train Results: Prec@1 60.394 Loss: 1.1112\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0682 (1.0682)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0768 (1.2401)\tPrec@1 43.750 (55.880)\n",
      "EPOCH: 30 val Results: Prec@1 55.880 Loss: 1.2401\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/390]\tTime 0.002 (0.002)\tLoss 1.0081 (1.0081)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [31][78/390]\tTime 0.008 (0.003)\tLoss 0.9807 (1.0456)\tPrec@1 64.062 (63.093)\n",
      "Epoch: [31][156/390]\tTime 0.002 (0.003)\tLoss 1.1179 (1.0709)\tPrec@1 60.156 (61.878)\n",
      "Epoch: [31][234/390]\tTime 0.002 (0.003)\tLoss 1.1012 (1.0883)\tPrec@1 63.281 (61.230)\n",
      "Epoch: [31][312/390]\tTime 0.005 (0.003)\tLoss 1.1029 (1.0990)\tPrec@1 62.500 (60.798)\n",
      "Epoch: [31][390/390]\tTime 0.003 (0.003)\tLoss 1.1829 (1.1097)\tPrec@1 53.750 (60.354)\n",
      "EPOCH: 31 train Results: Prec@1 60.354 Loss: 1.1097\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1585 (1.1585)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2368 (1.2523)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 31 val Results: Prec@1 55.370 Loss: 1.2523\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/390]\tTime 0.002 (0.002)\tLoss 1.0223 (1.0223)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [32][78/390]\tTime 0.002 (0.003)\tLoss 1.0849 (1.0482)\tPrec@1 60.156 (63.222)\n",
      "Epoch: [32][156/390]\tTime 0.003 (0.003)\tLoss 0.9339 (1.0689)\tPrec@1 64.062 (62.097)\n",
      "Epoch: [32][234/390]\tTime 0.003 (0.003)\tLoss 1.1358 (1.0869)\tPrec@1 60.156 (61.443)\n",
      "Epoch: [32][312/390]\tTime 0.003 (0.003)\tLoss 1.0108 (1.0986)\tPrec@1 63.281 (60.895)\n",
      "Epoch: [32][390/390]\tTime 0.002 (0.003)\tLoss 1.0163 (1.1098)\tPrec@1 63.750 (60.520)\n",
      "EPOCH: 32 train Results: Prec@1 60.520 Loss: 1.1098\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1212 (1.1212)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2386 (1.2425)\tPrec@1 43.750 (55.810)\n",
      "EPOCH: 32 val Results: Prec@1 55.810 Loss: 1.2425\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/390]\tTime 0.003 (0.003)\tLoss 1.1745 (1.1745)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [33][78/390]\tTime 0.002 (0.003)\tLoss 1.1843 (1.0674)\tPrec@1 57.031 (61.828)\n",
      "Epoch: [33][156/390]\tTime 0.003 (0.003)\tLoss 1.2053 (1.0794)\tPrec@1 61.719 (61.291)\n",
      "Epoch: [33][234/390]\tTime 0.003 (0.003)\tLoss 1.2638 (1.0852)\tPrec@1 55.469 (60.981)\n",
      "Epoch: [33][312/390]\tTime 0.002 (0.004)\tLoss 1.1696 (1.0985)\tPrec@1 57.812 (60.506)\n",
      "Epoch: [33][390/390]\tTime 0.001 (0.004)\tLoss 1.0993 (1.1076)\tPrec@1 67.500 (60.340)\n",
      "EPOCH: 33 train Results: Prec@1 60.340 Loss: 1.1076\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0798 (1.0798)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3097 (1.2408)\tPrec@1 37.500 (55.510)\n",
      "EPOCH: 33 val Results: Prec@1 55.510 Loss: 1.2408\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/390]\tTime 0.003 (0.003)\tLoss 0.9312 (0.9312)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [34][78/390]\tTime 0.002 (0.004)\tLoss 1.1217 (1.0341)\tPrec@1 57.812 (63.627)\n",
      "Epoch: [34][156/390]\tTime 0.004 (0.003)\tLoss 1.0754 (1.0626)\tPrec@1 61.719 (62.371)\n",
      "Epoch: [34][234/390]\tTime 0.003 (0.003)\tLoss 1.0885 (1.0775)\tPrec@1 58.594 (61.825)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.0903 (1.0921)\tPrec@1 66.406 (61.212)\n",
      "Epoch: [34][390/390]\tTime 0.001 (0.003)\tLoss 1.0063 (1.1043)\tPrec@1 61.250 (60.778)\n",
      "EPOCH: 34 train Results: Prec@1 60.778 Loss: 1.1043\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1051 (1.1051)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2968 (1.2378)\tPrec@1 37.500 (56.120)\n",
      "EPOCH: 34 val Results: Prec@1 56.120 Loss: 1.2378\n",
      "Best Prec@1: 56.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/390]\tTime 0.008 (0.008)\tLoss 1.0633 (1.0633)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [35][78/390]\tTime 0.002 (0.003)\tLoss 1.2407 (1.0375)\tPrec@1 51.562 (62.905)\n",
      "Epoch: [35][156/390]\tTime 0.006 (0.003)\tLoss 1.2310 (1.0547)\tPrec@1 57.812 (62.261)\n",
      "Epoch: [35][234/390]\tTime 0.002 (0.003)\tLoss 1.1838 (1.0792)\tPrec@1 57.812 (61.443)\n",
      "Epoch: [35][312/390]\tTime 0.005 (0.003)\tLoss 1.1745 (1.0906)\tPrec@1 57.812 (61.007)\n",
      "Epoch: [35][390/390]\tTime 0.002 (0.003)\tLoss 1.0454 (1.0995)\tPrec@1 65.000 (60.804)\n",
      "EPOCH: 35 train Results: Prec@1 60.804 Loss: 1.0995\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1081 (1.1081)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2095 (1.2361)\tPrec@1 56.250 (55.930)\n",
      "EPOCH: 35 val Results: Prec@1 55.930 Loss: 1.2361\n",
      "Best Prec@1: 56.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/390]\tTime 0.002 (0.002)\tLoss 1.1356 (1.1356)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [36][78/390]\tTime 0.002 (0.003)\tLoss 1.0478 (1.0352)\tPrec@1 64.844 (63.143)\n",
      "Epoch: [36][156/390]\tTime 0.005 (0.003)\tLoss 1.1877 (1.0575)\tPrec@1 57.031 (62.515)\n",
      "Epoch: [36][234/390]\tTime 0.003 (0.003)\tLoss 1.0784 (1.0703)\tPrec@1 60.938 (61.772)\n",
      "Epoch: [36][312/390]\tTime 0.003 (0.003)\tLoss 1.1817 (1.0824)\tPrec@1 59.375 (61.367)\n",
      "Epoch: [36][390/390]\tTime 0.004 (0.003)\tLoss 1.2017 (1.0933)\tPrec@1 56.250 (60.980)\n",
      "EPOCH: 36 train Results: Prec@1 60.980 Loss: 1.0933\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0659 (1.0659)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1774 (1.2335)\tPrec@1 37.500 (56.570)\n",
      "EPOCH: 36 val Results: Prec@1 56.570 Loss: 1.2335\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/390]\tTime 0.002 (0.002)\tLoss 0.9658 (0.9658)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 0.9547 (1.0238)\tPrec@1 66.406 (63.528)\n",
      "Epoch: [37][156/390]\tTime 0.009 (0.003)\tLoss 0.9859 (1.0574)\tPrec@1 57.812 (62.435)\n",
      "Epoch: [37][234/390]\tTime 0.004 (0.003)\tLoss 1.3719 (1.0739)\tPrec@1 48.438 (61.765)\n",
      "Epoch: [37][312/390]\tTime 0.004 (0.003)\tLoss 0.9941 (1.0918)\tPrec@1 64.062 (61.122)\n",
      "Epoch: [37][390/390]\tTime 0.004 (0.003)\tLoss 1.0434 (1.0977)\tPrec@1 61.250 (60.952)\n",
      "EPOCH: 37 train Results: Prec@1 60.952 Loss: 1.0977\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1059 (1.1059)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1030 (1.2337)\tPrec@1 56.250 (55.810)\n",
      "EPOCH: 37 val Results: Prec@1 55.810 Loss: 1.2337\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/390]\tTime 0.002 (0.002)\tLoss 1.0013 (1.0013)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.004)\tLoss 1.2116 (1.0476)\tPrec@1 58.594 (62.648)\n",
      "Epoch: [38][156/390]\tTime 0.004 (0.003)\tLoss 0.9461 (1.0573)\tPrec@1 66.406 (62.619)\n",
      "Epoch: [38][234/390]\tTime 0.003 (0.003)\tLoss 1.0352 (1.0743)\tPrec@1 65.625 (61.932)\n",
      "Epoch: [38][312/390]\tTime 0.003 (0.003)\tLoss 1.2506 (1.0854)\tPrec@1 55.469 (61.539)\n",
      "Epoch: [38][390/390]\tTime 0.001 (0.003)\tLoss 1.1811 (1.0945)\tPrec@1 61.250 (61.218)\n",
      "EPOCH: 38 train Results: Prec@1 61.218 Loss: 1.0945\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.0928 (1.0928)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0500 (1.2259)\tPrec@1 56.250 (56.260)\n",
      "EPOCH: 38 val Results: Prec@1 56.260 Loss: 1.2259\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/390]\tTime 0.003 (0.003)\tLoss 1.0577 (1.0577)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.003)\tLoss 1.2317 (1.0186)\tPrec@1 53.125 (63.845)\n",
      "Epoch: [39][156/390]\tTime 0.002 (0.003)\tLoss 1.0833 (1.0463)\tPrec@1 59.375 (62.883)\n",
      "Epoch: [39][234/390]\tTime 0.002 (0.003)\tLoss 1.0763 (1.0677)\tPrec@1 61.719 (62.161)\n",
      "Epoch: [39][312/390]\tTime 0.002 (0.003)\tLoss 1.4371 (1.0786)\tPrec@1 50.781 (61.791)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2381 (1.0928)\tPrec@1 52.500 (61.236)\n",
      "EPOCH: 39 train Results: Prec@1 61.236 Loss: 1.0928\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0667 (1.0667)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3399 (1.2383)\tPrec@1 50.000 (56.050)\n",
      "EPOCH: 39 val Results: Prec@1 56.050 Loss: 1.2383\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/390]\tTime 0.003 (0.003)\tLoss 1.0323 (1.0323)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [40][78/390]\tTime 0.003 (0.003)\tLoss 1.0073 (1.0323)\tPrec@1 64.062 (63.281)\n",
      "Epoch: [40][156/390]\tTime 0.002 (0.002)\tLoss 1.0334 (1.0403)\tPrec@1 60.938 (63.052)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.003)\tLoss 1.3641 (1.0554)\tPrec@1 47.656 (62.686)\n",
      "Epoch: [40][312/390]\tTime 0.003 (0.003)\tLoss 1.3373 (1.0719)\tPrec@1 53.906 (62.043)\n",
      "Epoch: [40][390/390]\tTime 0.001 (0.003)\tLoss 1.2058 (1.0879)\tPrec@1 55.000 (61.406)\n",
      "EPOCH: 40 train Results: Prec@1 61.406 Loss: 1.0879\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1085 (1.1085)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4187 (1.2358)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 40 val Results: Prec@1 55.610 Loss: 1.2358\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/390]\tTime 0.002 (0.002)\tLoss 1.1918 (1.1918)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [41][78/390]\tTime 0.002 (0.003)\tLoss 0.9655 (1.0191)\tPrec@1 62.500 (63.578)\n",
      "Epoch: [41][156/390]\tTime 0.002 (0.004)\tLoss 1.2946 (1.0488)\tPrec@1 53.906 (62.440)\n",
      "Epoch: [41][234/390]\tTime 0.004 (0.004)\tLoss 1.1530 (1.0651)\tPrec@1 59.375 (62.001)\n",
      "Epoch: [41][312/390]\tTime 0.002 (0.004)\tLoss 1.1300 (1.0800)\tPrec@1 62.500 (61.472)\n",
      "Epoch: [41][390/390]\tTime 0.001 (0.004)\tLoss 1.0270 (1.0883)\tPrec@1 67.500 (61.230)\n",
      "EPOCH: 41 train Results: Prec@1 61.230 Loss: 1.0883\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0386 (1.0386)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2643 (1.2309)\tPrec@1 50.000 (56.180)\n",
      "EPOCH: 41 val Results: Prec@1 56.180 Loss: 1.2309\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/390]\tTime 0.002 (0.002)\tLoss 1.0428 (1.0428)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [42][78/390]\tTime 0.003 (0.004)\tLoss 0.9533 (1.0209)\tPrec@1 60.938 (64.132)\n",
      "Epoch: [42][156/390]\tTime 0.002 (0.003)\tLoss 1.0046 (1.0367)\tPrec@1 62.500 (63.112)\n",
      "Epoch: [42][234/390]\tTime 0.003 (0.004)\tLoss 1.0839 (1.0601)\tPrec@1 60.156 (62.284)\n",
      "Epoch: [42][312/390]\tTime 0.004 (0.004)\tLoss 1.2025 (1.0756)\tPrec@1 53.125 (61.646)\n",
      "Epoch: [42][390/390]\tTime 0.010 (0.003)\tLoss 1.1462 (1.0884)\tPrec@1 62.500 (61.116)\n",
      "EPOCH: 42 train Results: Prec@1 61.116 Loss: 1.0884\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0363 (1.0363)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1616 (1.2306)\tPrec@1 50.000 (56.430)\n",
      "EPOCH: 42 val Results: Prec@1 56.430 Loss: 1.2306\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/390]\tTime 0.002 (0.002)\tLoss 1.0596 (1.0596)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [43][78/390]\tTime 0.006 (0.004)\tLoss 0.9660 (1.0152)\tPrec@1 66.406 (64.201)\n",
      "Epoch: [43][156/390]\tTime 0.011 (0.003)\tLoss 1.2549 (1.0487)\tPrec@1 55.469 (62.709)\n",
      "Epoch: [43][234/390]\tTime 0.002 (0.003)\tLoss 1.0750 (1.0593)\tPrec@1 60.156 (62.294)\n",
      "Epoch: [43][312/390]\tTime 0.002 (0.003)\tLoss 1.0265 (1.0779)\tPrec@1 69.531 (61.721)\n",
      "Epoch: [43][390/390]\tTime 0.003 (0.003)\tLoss 0.8992 (1.0855)\tPrec@1 67.500 (61.370)\n",
      "EPOCH: 43 train Results: Prec@1 61.370 Loss: 1.0855\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1254 (1.1254)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1500 (1.2354)\tPrec@1 56.250 (56.190)\n",
      "EPOCH: 43 val Results: Prec@1 56.190 Loss: 1.2354\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/390]\tTime 0.002 (0.002)\tLoss 1.0153 (1.0153)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.003)\tLoss 1.0246 (1.0219)\tPrec@1 64.844 (64.181)\n",
      "Epoch: [44][156/390]\tTime 0.002 (0.003)\tLoss 1.0080 (1.0385)\tPrec@1 67.188 (63.122)\n",
      "Epoch: [44][234/390]\tTime 0.004 (0.003)\tLoss 1.0068 (1.0586)\tPrec@1 63.281 (62.507)\n",
      "Epoch: [44][312/390]\tTime 0.002 (0.003)\tLoss 1.0897 (1.0716)\tPrec@1 63.281 (61.906)\n",
      "Epoch: [44][390/390]\tTime 0.001 (0.003)\tLoss 1.1486 (1.0830)\tPrec@1 58.750 (61.478)\n",
      "EPOCH: 44 train Results: Prec@1 61.478 Loss: 1.0830\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0574 (1.0574)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3908 (1.2333)\tPrec@1 43.750 (56.000)\n",
      "EPOCH: 44 val Results: Prec@1 56.000 Loss: 1.2333\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/390]\tTime 0.005 (0.005)\tLoss 1.1085 (1.1085)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [45][78/390]\tTime 0.003 (0.003)\tLoss 1.1525 (1.0191)\tPrec@1 57.812 (63.973)\n",
      "Epoch: [45][156/390]\tTime 0.006 (0.003)\tLoss 1.1550 (1.0364)\tPrec@1 57.031 (63.172)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.003)\tLoss 1.1217 (1.0544)\tPrec@1 53.906 (62.497)\n",
      "Epoch: [45][312/390]\tTime 0.005 (0.003)\tLoss 0.9422 (1.0699)\tPrec@1 64.844 (61.901)\n",
      "Epoch: [45][390/390]\tTime 0.001 (0.003)\tLoss 1.3568 (1.0803)\tPrec@1 46.250 (61.514)\n",
      "EPOCH: 45 train Results: Prec@1 61.514 Loss: 1.0803\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1004 (1.1004)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2943 (1.2446)\tPrec@1 50.000 (55.620)\n",
      "EPOCH: 45 val Results: Prec@1 55.620 Loss: 1.2446\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/390]\tTime 0.003 (0.003)\tLoss 1.0538 (1.0538)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [46][78/390]\tTime 0.003 (0.003)\tLoss 1.0264 (1.0125)\tPrec@1 64.062 (64.547)\n",
      "Epoch: [46][156/390]\tTime 0.010 (0.003)\tLoss 1.1429 (1.0294)\tPrec@1 57.031 (63.640)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.1672 (1.0524)\tPrec@1 57.031 (62.696)\n",
      "Epoch: [46][312/390]\tTime 0.003 (0.003)\tLoss 1.1250 (1.0636)\tPrec@1 60.156 (62.293)\n",
      "Epoch: [46][390/390]\tTime 0.001 (0.004)\tLoss 1.0907 (1.0779)\tPrec@1 60.000 (61.614)\n",
      "EPOCH: 46 train Results: Prec@1 61.614 Loss: 1.0779\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.0525 (1.0525)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2440 (1.2547)\tPrec@1 37.500 (55.910)\n",
      "EPOCH: 46 val Results: Prec@1 55.910 Loss: 1.2547\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/390]\tTime 0.003 (0.003)\tLoss 1.3587 (1.3587)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [47][78/390]\tTime 0.008 (0.003)\tLoss 1.1388 (1.0075)\tPrec@1 56.250 (63.914)\n",
      "Epoch: [47][156/390]\tTime 0.023 (0.004)\tLoss 1.0526 (1.0319)\tPrec@1 70.312 (63.152)\n",
      "Epoch: [47][234/390]\tTime 0.002 (0.004)\tLoss 1.0682 (1.0534)\tPrec@1 57.031 (62.384)\n",
      "Epoch: [47][312/390]\tTime 0.008 (0.004)\tLoss 1.1603 (1.0641)\tPrec@1 55.469 (61.951)\n",
      "Epoch: [47][390/390]\tTime 0.003 (0.004)\tLoss 1.2064 (1.0796)\tPrec@1 61.250 (61.462)\n",
      "EPOCH: 47 train Results: Prec@1 61.462 Loss: 1.0796\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 0.9457 (0.9457)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1423 (1.2275)\tPrec@1 50.000 (56.560)\n",
      "EPOCH: 47 val Results: Prec@1 56.560 Loss: 1.2275\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/390]\tTime 0.010 (0.010)\tLoss 1.0413 (1.0413)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [48][78/390]\tTime 0.003 (0.003)\tLoss 1.0637 (1.0243)\tPrec@1 61.719 (63.548)\n",
      "Epoch: [48][156/390]\tTime 0.005 (0.003)\tLoss 1.1119 (1.0402)\tPrec@1 64.062 (62.938)\n",
      "Epoch: [48][234/390]\tTime 0.004 (0.003)\tLoss 0.9855 (1.0504)\tPrec@1 66.406 (62.387)\n",
      "Epoch: [48][312/390]\tTime 0.002 (0.003)\tLoss 1.0270 (1.0639)\tPrec@1 60.156 (61.938)\n",
      "Epoch: [48][390/390]\tTime 0.003 (0.003)\tLoss 1.3480 (1.0775)\tPrec@1 55.000 (61.432)\n",
      "EPOCH: 48 train Results: Prec@1 61.432 Loss: 1.0775\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1088 (1.1088)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2407 (1.2477)\tPrec@1 37.500 (55.310)\n",
      "EPOCH: 48 val Results: Prec@1 55.310 Loss: 1.2477\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/390]\tTime 0.002 (0.002)\tLoss 0.9801 (0.9801)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [49][78/390]\tTime 0.002 (0.003)\tLoss 1.1149 (1.0033)\tPrec@1 65.625 (65.081)\n",
      "Epoch: [49][156/390]\tTime 0.010 (0.003)\tLoss 1.1125 (1.0324)\tPrec@1 58.594 (63.699)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.003)\tLoss 1.0695 (1.0503)\tPrec@1 59.375 (62.743)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.1795 (1.0629)\tPrec@1 58.594 (62.250)\n",
      "Epoch: [49][390/390]\tTime 0.002 (0.003)\tLoss 1.0962 (1.0766)\tPrec@1 61.250 (61.776)\n",
      "EPOCH: 49 train Results: Prec@1 61.776 Loss: 1.0766\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1238 (1.1238)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0022 (1.2289)\tPrec@1 50.000 (56.190)\n",
      "EPOCH: 49 val Results: Prec@1 56.190 Loss: 1.2289\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/390]\tTime 0.002 (0.002)\tLoss 0.9661 (0.9661)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [50][78/390]\tTime 0.003 (0.003)\tLoss 1.0167 (1.0110)\tPrec@1 62.500 (64.290)\n",
      "Epoch: [50][156/390]\tTime 0.002 (0.003)\tLoss 1.0452 (1.0311)\tPrec@1 57.031 (63.097)\n",
      "Epoch: [50][234/390]\tTime 0.003 (0.003)\tLoss 1.1962 (1.0550)\tPrec@1 54.688 (62.360)\n",
      "Epoch: [50][312/390]\tTime 0.002 (0.003)\tLoss 1.0505 (1.0665)\tPrec@1 59.375 (61.928)\n",
      "Epoch: [50][390/390]\tTime 0.003 (0.003)\tLoss 0.9876 (1.0771)\tPrec@1 65.000 (61.580)\n",
      "EPOCH: 50 train Results: Prec@1 61.580 Loss: 1.0771\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0859 (1.0859)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3232 (1.2388)\tPrec@1 31.250 (55.980)\n",
      "EPOCH: 50 val Results: Prec@1 55.980 Loss: 1.2388\n",
      "Best Prec@1: 56.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/390]\tTime 0.003 (0.003)\tLoss 1.1422 (1.1422)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [51][78/390]\tTime 0.002 (0.005)\tLoss 1.0377 (1.0164)\tPrec@1 66.406 (63.865)\n",
      "Epoch: [51][156/390]\tTime 0.003 (0.004)\tLoss 0.9774 (1.0377)\tPrec@1 67.969 (63.112)\n",
      "Epoch: [51][234/390]\tTime 0.005 (0.004)\tLoss 1.2104 (1.0526)\tPrec@1 55.469 (62.477)\n",
      "Epoch: [51][312/390]\tTime 0.002 (0.004)\tLoss 0.9558 (1.0638)\tPrec@1 67.188 (62.123)\n",
      "Epoch: [51][390/390]\tTime 0.001 (0.004)\tLoss 1.0942 (1.0755)\tPrec@1 61.250 (61.696)\n",
      "EPOCH: 51 train Results: Prec@1 61.696 Loss: 1.0755\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1376 (1.1376)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1632 (1.2343)\tPrec@1 56.250 (57.080)\n",
      "EPOCH: 51 val Results: Prec@1 57.080 Loss: 1.2343\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/390]\tTime 0.003 (0.003)\tLoss 0.9675 (0.9675)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [52][78/390]\tTime 0.004 (0.003)\tLoss 1.0334 (1.0123)\tPrec@1 60.938 (64.260)\n",
      "Epoch: [52][156/390]\tTime 0.003 (0.003)\tLoss 1.0596 (1.0256)\tPrec@1 59.375 (63.659)\n",
      "Epoch: [52][234/390]\tTime 0.003 (0.003)\tLoss 0.9318 (1.0468)\tPrec@1 70.312 (62.689)\n",
      "Epoch: [52][312/390]\tTime 0.002 (0.003)\tLoss 1.2748 (1.0629)\tPrec@1 57.812 (62.098)\n",
      "Epoch: [52][390/390]\tTime 0.002 (0.003)\tLoss 1.3121 (1.0773)\tPrec@1 52.500 (61.684)\n",
      "EPOCH: 52 train Results: Prec@1 61.684 Loss: 1.0773\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0846 (1.0846)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1441 (1.2409)\tPrec@1 50.000 (55.960)\n",
      "EPOCH: 52 val Results: Prec@1 55.960 Loss: 1.2409\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/390]\tTime 0.006 (0.006)\tLoss 1.0499 (1.0499)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [53][78/390]\tTime 0.005 (0.003)\tLoss 0.9750 (1.0093)\tPrec@1 63.281 (64.419)\n",
      "Epoch: [53][156/390]\tTime 0.003 (0.003)\tLoss 1.0038 (1.0230)\tPrec@1 63.281 (63.868)\n",
      "Epoch: [53][234/390]\tTime 0.002 (0.003)\tLoss 1.0473 (1.0411)\tPrec@1 57.812 (63.098)\n",
      "Epoch: [53][312/390]\tTime 0.002 (0.003)\tLoss 1.4865 (1.0589)\tPrec@1 43.750 (62.380)\n",
      "Epoch: [53][390/390]\tTime 0.003 (0.003)\tLoss 1.0523 (1.0695)\tPrec@1 70.000 (62.024)\n",
      "EPOCH: 53 train Results: Prec@1 62.024 Loss: 1.0695\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0960 (1.0960)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4281 (1.2446)\tPrec@1 37.500 (55.780)\n",
      "EPOCH: 53 val Results: Prec@1 55.780 Loss: 1.2446\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/390]\tTime 0.006 (0.006)\tLoss 0.9632 (0.9632)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [54][78/390]\tTime 0.005 (0.003)\tLoss 1.0023 (0.9966)\tPrec@1 61.719 (65.111)\n",
      "Epoch: [54][156/390]\tTime 0.002 (0.003)\tLoss 0.9930 (1.0211)\tPrec@1 64.844 (63.948)\n",
      "Epoch: [54][234/390]\tTime 0.003 (0.003)\tLoss 1.0908 (1.0434)\tPrec@1 60.938 (63.092)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2466 (1.0603)\tPrec@1 58.594 (62.368)\n",
      "Epoch: [54][390/390]\tTime 0.001 (0.003)\tLoss 1.2099 (1.0752)\tPrec@1 53.750 (61.856)\n",
      "EPOCH: 54 train Results: Prec@1 61.856 Loss: 1.0752\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0638 (1.0638)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9847 (1.2517)\tPrec@1 50.000 (55.380)\n",
      "EPOCH: 54 val Results: Prec@1 55.380 Loss: 1.2517\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/390]\tTime 0.007 (0.007)\tLoss 1.0683 (1.0683)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.0507 (0.9965)\tPrec@1 68.750 (64.933)\n",
      "Epoch: [55][156/390]\tTime 0.004 (0.003)\tLoss 1.1328 (1.0263)\tPrec@1 57.031 (63.436)\n",
      "Epoch: [55][234/390]\tTime 0.002 (0.003)\tLoss 1.2275 (1.0462)\tPrec@1 60.156 (62.653)\n",
      "Epoch: [55][312/390]\tTime 0.002 (0.003)\tLoss 0.9427 (1.0572)\tPrec@1 66.406 (62.200)\n",
      "Epoch: [55][390/390]\tTime 0.002 (0.003)\tLoss 1.1144 (1.0700)\tPrec@1 61.250 (61.828)\n",
      "EPOCH: 55 train Results: Prec@1 61.828 Loss: 1.0700\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0683 (1.0683)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2787 (1.2460)\tPrec@1 43.750 (55.740)\n",
      "EPOCH: 55 val Results: Prec@1 55.740 Loss: 1.2460\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/390]\tTime 0.003 (0.003)\tLoss 0.9113 (0.9113)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [56][78/390]\tTime 0.002 (0.003)\tLoss 1.0051 (1.0181)\tPrec@1 63.281 (64.033)\n",
      "Epoch: [56][156/390]\tTime 0.003 (0.004)\tLoss 0.9790 (1.0301)\tPrec@1 66.406 (63.236)\n",
      "Epoch: [56][234/390]\tTime 0.003 (0.004)\tLoss 1.0947 (1.0430)\tPrec@1 59.375 (62.819)\n",
      "Epoch: [56][312/390]\tTime 0.002 (0.004)\tLoss 1.0068 (1.0574)\tPrec@1 64.062 (62.323)\n",
      "Epoch: [56][390/390]\tTime 0.002 (0.003)\tLoss 1.2188 (1.0703)\tPrec@1 58.750 (61.924)\n",
      "EPOCH: 56 train Results: Prec@1 61.924 Loss: 1.0703\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0349 (1.0349)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0674 (1.2510)\tPrec@1 56.250 (55.270)\n",
      "EPOCH: 56 val Results: Prec@1 55.270 Loss: 1.2510\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/390]\tTime 0.002 (0.002)\tLoss 1.0261 (1.0261)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [57][78/390]\tTime 0.002 (0.003)\tLoss 1.0285 (0.9987)\tPrec@1 64.844 (64.537)\n",
      "Epoch: [57][156/390]\tTime 0.002 (0.003)\tLoss 0.9721 (1.0147)\tPrec@1 67.188 (63.858)\n",
      "Epoch: [57][234/390]\tTime 0.002 (0.003)\tLoss 1.0475 (1.0347)\tPrec@1 60.156 (63.032)\n",
      "Epoch: [57][312/390]\tTime 0.013 (0.003)\tLoss 1.1036 (1.0514)\tPrec@1 60.156 (62.285)\n",
      "Epoch: [57][390/390]\tTime 0.003 (0.003)\tLoss 1.1143 (1.0674)\tPrec@1 58.750 (61.780)\n",
      "EPOCH: 57 train Results: Prec@1 61.780 Loss: 1.0674\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1661 (1.1661)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2095 (1.2447)\tPrec@1 37.500 (56.100)\n",
      "EPOCH: 57 val Results: Prec@1 56.100 Loss: 1.2447\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/390]\tTime 0.004 (0.004)\tLoss 0.9584 (0.9584)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [58][78/390]\tTime 0.002 (0.003)\tLoss 1.0581 (0.9995)\tPrec@1 67.188 (64.330)\n",
      "Epoch: [58][156/390]\tTime 0.007 (0.003)\tLoss 1.0664 (1.0252)\tPrec@1 57.812 (63.530)\n",
      "Epoch: [58][234/390]\tTime 0.002 (0.003)\tLoss 0.9978 (1.0422)\tPrec@1 60.938 (62.896)\n",
      "Epoch: [58][312/390]\tTime 0.002 (0.003)\tLoss 1.0153 (1.0561)\tPrec@1 64.062 (62.572)\n",
      "Epoch: [58][390/390]\tTime 0.002 (0.003)\tLoss 1.2186 (1.0687)\tPrec@1 57.500 (61.948)\n",
      "EPOCH: 58 train Results: Prec@1 61.948 Loss: 1.0687\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0766 (1.0766)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2192 (1.2240)\tPrec@1 56.250 (56.560)\n",
      "EPOCH: 58 val Results: Prec@1 56.560 Loss: 1.2240\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/390]\tTime 0.005 (0.005)\tLoss 1.0632 (1.0632)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [59][78/390]\tTime 0.005 (0.003)\tLoss 1.0865 (1.0017)\tPrec@1 60.938 (64.458)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.1633 (1.0186)\tPrec@1 61.719 (63.679)\n",
      "Epoch: [59][234/390]\tTime 0.004 (0.003)\tLoss 1.0424 (1.0427)\tPrec@1 62.500 (62.866)\n",
      "Epoch: [59][312/390]\tTime 0.003 (0.003)\tLoss 1.0632 (1.0563)\tPrec@1 63.281 (62.335)\n",
      "Epoch: [59][390/390]\tTime 0.005 (0.003)\tLoss 1.0799 (1.0675)\tPrec@1 66.250 (61.976)\n",
      "EPOCH: 59 train Results: Prec@1 61.976 Loss: 1.0675\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1553 (1.1553)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9773 (1.2481)\tPrec@1 56.250 (55.620)\n",
      "EPOCH: 59 val Results: Prec@1 55.620 Loss: 1.2481\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/390]\tTime 0.002 (0.002)\tLoss 0.8647 (0.8647)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [60][78/390]\tTime 0.002 (0.003)\tLoss 1.0055 (1.0043)\tPrec@1 60.938 (64.775)\n",
      "Epoch: [60][156/390]\tTime 0.002 (0.003)\tLoss 0.9821 (1.0252)\tPrec@1 68.750 (64.053)\n",
      "Epoch: [60][234/390]\tTime 0.002 (0.003)\tLoss 1.0309 (1.0450)\tPrec@1 58.594 (62.979)\n",
      "Epoch: [60][312/390]\tTime 0.002 (0.003)\tLoss 1.0896 (1.0616)\tPrec@1 60.938 (62.213)\n",
      "Epoch: [60][390/390]\tTime 0.002 (0.003)\tLoss 1.1470 (1.0712)\tPrec@1 61.250 (61.962)\n",
      "EPOCH: 60 train Results: Prec@1 61.962 Loss: 1.0712\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1229 (1.1229)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9329 (1.2547)\tPrec@1 56.250 (55.160)\n",
      "EPOCH: 60 val Results: Prec@1 55.160 Loss: 1.2547\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/390]\tTime 0.004 (0.004)\tLoss 0.9310 (0.9310)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [61][78/390]\tTime 0.004 (0.003)\tLoss 1.0692 (0.9968)\tPrec@1 60.156 (64.597)\n",
      "Epoch: [61][156/390]\tTime 0.004 (0.003)\tLoss 1.1953 (1.0219)\tPrec@1 58.594 (63.749)\n",
      "Epoch: [61][234/390]\tTime 0.002 (0.003)\tLoss 1.1054 (1.0386)\tPrec@1 59.375 (62.919)\n",
      "Epoch: [61][312/390]\tTime 0.002 (0.003)\tLoss 1.1067 (1.0536)\tPrec@1 65.625 (62.410)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.003)\tLoss 1.0956 (1.0650)\tPrec@1 65.000 (62.032)\n",
      "EPOCH: 61 train Results: Prec@1 62.032 Loss: 1.0650\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0990 (1.0990)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2317 (1.2480)\tPrec@1 43.750 (55.400)\n",
      "EPOCH: 61 val Results: Prec@1 55.400 Loss: 1.2480\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/390]\tTime 0.003 (0.003)\tLoss 0.9615 (0.9615)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.1075 (1.0019)\tPrec@1 57.812 (64.320)\n",
      "Epoch: [62][156/390]\tTime 0.002 (0.004)\tLoss 0.9611 (1.0216)\tPrec@1 70.312 (63.470)\n",
      "Epoch: [62][234/390]\tTime 0.004 (0.003)\tLoss 1.1981 (1.0439)\tPrec@1 58.594 (62.726)\n",
      "Epoch: [62][312/390]\tTime 0.002 (0.003)\tLoss 1.1427 (1.0557)\tPrec@1 62.500 (62.278)\n",
      "Epoch: [62][390/390]\tTime 0.002 (0.003)\tLoss 0.8561 (1.0664)\tPrec@1 71.250 (61.920)\n",
      "EPOCH: 62 train Results: Prec@1 61.920 Loss: 1.0664\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1448 (1.1448)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1346 (1.2385)\tPrec@1 37.500 (56.200)\n",
      "EPOCH: 62 val Results: Prec@1 56.200 Loss: 1.2385\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/390]\tTime 0.002 (0.002)\tLoss 1.0115 (1.0115)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [63][78/390]\tTime 0.002 (0.004)\tLoss 1.0446 (0.9993)\tPrec@1 61.719 (64.310)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.003)\tLoss 0.9818 (1.0238)\tPrec@1 65.625 (63.421)\n",
      "Epoch: [63][234/390]\tTime 0.002 (0.003)\tLoss 1.1920 (1.0435)\tPrec@1 59.375 (62.945)\n",
      "Epoch: [63][312/390]\tTime 0.002 (0.003)\tLoss 1.2560 (1.0597)\tPrec@1 53.906 (62.418)\n",
      "Epoch: [63][390/390]\tTime 0.003 (0.003)\tLoss 1.1675 (1.0670)\tPrec@1 60.000 (62.110)\n",
      "EPOCH: 63 train Results: Prec@1 62.110 Loss: 1.0670\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1781 (1.1781)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9917 (1.2371)\tPrec@1 43.750 (56.150)\n",
      "EPOCH: 63 val Results: Prec@1 56.150 Loss: 1.2371\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/390]\tTime 0.004 (0.004)\tLoss 0.8951 (0.8951)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [64][78/390]\tTime 0.003 (0.003)\tLoss 1.0898 (0.9904)\tPrec@1 61.719 (64.656)\n",
      "Epoch: [64][156/390]\tTime 0.002 (0.003)\tLoss 1.1889 (1.0202)\tPrec@1 55.469 (63.854)\n",
      "Epoch: [64][234/390]\tTime 0.012 (0.003)\tLoss 1.1720 (1.0371)\tPrec@1 57.812 (63.088)\n",
      "Epoch: [64][312/390]\tTime 0.002 (0.003)\tLoss 1.1169 (1.0526)\tPrec@1 59.375 (62.468)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.003)\tLoss 1.1646 (1.0668)\tPrec@1 61.250 (62.112)\n",
      "EPOCH: 64 train Results: Prec@1 62.112 Loss: 1.0668\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1184 (1.1184)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8996 (1.2500)\tPrec@1 68.750 (55.540)\n",
      "EPOCH: 64 val Results: Prec@1 55.540 Loss: 1.2500\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/390]\tTime 0.002 (0.002)\tLoss 0.9510 (0.9510)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [65][78/390]\tTime 0.004 (0.003)\tLoss 1.1174 (0.9966)\tPrec@1 60.938 (64.488)\n",
      "Epoch: [65][156/390]\tTime 0.002 (0.003)\tLoss 0.9726 (1.0168)\tPrec@1 68.750 (63.943)\n",
      "Epoch: [65][234/390]\tTime 0.002 (0.003)\tLoss 1.0382 (1.0373)\tPrec@1 62.500 (63.112)\n",
      "Epoch: [65][312/390]\tTime 0.004 (0.003)\tLoss 1.1794 (1.0537)\tPrec@1 58.594 (62.567)\n",
      "Epoch: [65][390/390]\tTime 0.001 (0.003)\tLoss 1.0932 (1.0663)\tPrec@1 56.250 (62.114)\n",
      "EPOCH: 65 train Results: Prec@1 62.114 Loss: 1.0663\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1215 (1.1215)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1999 (1.2359)\tPrec@1 56.250 (55.890)\n",
      "EPOCH: 65 val Results: Prec@1 55.890 Loss: 1.2359\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/390]\tTime 0.004 (0.004)\tLoss 0.9631 (0.9631)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [66][78/390]\tTime 0.002 (0.003)\tLoss 1.1053 (0.9818)\tPrec@1 54.688 (64.992)\n",
      "Epoch: [66][156/390]\tTime 0.002 (0.003)\tLoss 1.1588 (1.0144)\tPrec@1 57.812 (63.505)\n",
      "Epoch: [66][234/390]\tTime 0.003 (0.003)\tLoss 1.1047 (1.0327)\tPrec@1 63.281 (63.075)\n",
      "Epoch: [66][312/390]\tTime 0.002 (0.003)\tLoss 1.1727 (1.0542)\tPrec@1 59.375 (62.373)\n",
      "Epoch: [66][390/390]\tTime 0.003 (0.003)\tLoss 1.1304 (1.0648)\tPrec@1 58.750 (62.138)\n",
      "EPOCH: 66 train Results: Prec@1 62.138 Loss: 1.0648\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0908 (1.0908)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1754 (1.2458)\tPrec@1 56.250 (55.910)\n",
      "EPOCH: 66 val Results: Prec@1 55.910 Loss: 1.2458\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/390]\tTime 0.003 (0.003)\tLoss 0.9933 (0.9933)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [67][78/390]\tTime 0.002 (0.003)\tLoss 0.9975 (0.9930)\tPrec@1 65.625 (64.765)\n",
      "Epoch: [67][156/390]\tTime 0.007 (0.003)\tLoss 1.2347 (1.0141)\tPrec@1 54.688 (64.087)\n",
      "Epoch: [67][234/390]\tTime 0.002 (0.003)\tLoss 1.0290 (1.0364)\tPrec@1 60.156 (63.285)\n",
      "Epoch: [67][312/390]\tTime 0.007 (0.003)\tLoss 1.0252 (1.0485)\tPrec@1 66.406 (62.800)\n",
      "Epoch: [67][390/390]\tTime 0.004 (0.003)\tLoss 1.1149 (1.0631)\tPrec@1 57.500 (62.190)\n",
      "EPOCH: 67 train Results: Prec@1 62.190 Loss: 1.0631\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0907 (1.0907)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8853 (1.2416)\tPrec@1 56.250 (55.520)\n",
      "EPOCH: 67 val Results: Prec@1 55.520 Loss: 1.2416\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/390]\tTime 0.003 (0.003)\tLoss 0.9180 (0.9180)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [68][78/390]\tTime 0.002 (0.003)\tLoss 0.9186 (0.9878)\tPrec@1 67.969 (65.002)\n",
      "Epoch: [68][156/390]\tTime 0.002 (0.003)\tLoss 1.1380 (1.0260)\tPrec@1 63.281 (63.555)\n",
      "Epoch: [68][234/390]\tTime 0.002 (0.003)\tLoss 1.0983 (1.0414)\tPrec@1 61.719 (62.932)\n",
      "Epoch: [68][312/390]\tTime 0.002 (0.003)\tLoss 1.2538 (1.0535)\tPrec@1 56.250 (62.617)\n",
      "Epoch: [68][390/390]\tTime 0.003 (0.003)\tLoss 1.3653 (1.0636)\tPrec@1 55.000 (62.248)\n",
      "EPOCH: 68 train Results: Prec@1 62.248 Loss: 1.0636\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0771 (1.0771)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0894 (1.2453)\tPrec@1 62.500 (55.830)\n",
      "EPOCH: 68 val Results: Prec@1 55.830 Loss: 1.2453\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/390]\tTime 0.006 (0.006)\tLoss 1.0494 (1.0494)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.003)\tLoss 0.9851 (0.9678)\tPrec@1 66.406 (65.645)\n",
      "Epoch: [69][156/390]\tTime 0.003 (0.004)\tLoss 1.0701 (1.0097)\tPrec@1 65.625 (64.137)\n",
      "Epoch: [69][234/390]\tTime 0.004 (0.003)\tLoss 0.9368 (1.0330)\tPrec@1 61.719 (63.191)\n",
      "Epoch: [69][312/390]\tTime 0.006 (0.003)\tLoss 1.0984 (1.0491)\tPrec@1 62.500 (62.413)\n",
      "Epoch: [69][390/390]\tTime 0.007 (0.003)\tLoss 1.0791 (1.0627)\tPrec@1 62.500 (61.974)\n",
      "EPOCH: 69 train Results: Prec@1 61.974 Loss: 1.0627\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0781 (1.0781)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1498 (1.2327)\tPrec@1 56.250 (56.460)\n",
      "EPOCH: 69 val Results: Prec@1 56.460 Loss: 1.2327\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/390]\tTime 0.002 (0.002)\tLoss 0.8643 (0.8643)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [70][78/390]\tTime 0.002 (0.003)\tLoss 1.1463 (0.9966)\tPrec@1 56.250 (64.597)\n",
      "Epoch: [70][156/390]\tTime 0.002 (0.003)\tLoss 1.0156 (1.0151)\tPrec@1 61.719 (63.893)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.004)\tLoss 0.9795 (1.0342)\tPrec@1 62.500 (63.195)\n",
      "Epoch: [70][312/390]\tTime 0.002 (0.004)\tLoss 0.9757 (1.0557)\tPrec@1 64.062 (62.562)\n",
      "Epoch: [70][390/390]\tTime 0.001 (0.004)\tLoss 1.0775 (1.0636)\tPrec@1 60.000 (62.214)\n",
      "EPOCH: 70 train Results: Prec@1 62.214 Loss: 1.0636\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0963 (1.0963)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2400 (1.2473)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 70 val Results: Prec@1 55.370 Loss: 1.2473\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/390]\tTime 0.007 (0.007)\tLoss 0.8830 (0.8830)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [71][78/390]\tTime 0.003 (0.003)\tLoss 1.0149 (0.9896)\tPrec@1 60.156 (65.190)\n",
      "Epoch: [71][156/390]\tTime 0.003 (0.003)\tLoss 1.0093 (1.0203)\tPrec@1 60.938 (63.814)\n",
      "Epoch: [71][234/390]\tTime 0.026 (0.004)\tLoss 0.9486 (1.0403)\tPrec@1 67.969 (62.972)\n",
      "Epoch: [71][312/390]\tTime 0.002 (0.003)\tLoss 0.9758 (1.0526)\tPrec@1 61.719 (62.443)\n",
      "Epoch: [71][390/390]\tTime 0.001 (0.003)\tLoss 0.8306 (1.0665)\tPrec@1 73.750 (61.888)\n",
      "EPOCH: 71 train Results: Prec@1 61.888 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0804 (1.0804)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0180 (1.2509)\tPrec@1 62.500 (55.770)\n",
      "EPOCH: 71 val Results: Prec@1 55.770 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/390]\tTime 0.004 (0.004)\tLoss 0.9683 (0.9683)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [72][78/390]\tTime 0.003 (0.003)\tLoss 1.0971 (0.9957)\tPrec@1 57.812 (65.111)\n",
      "Epoch: [72][156/390]\tTime 0.003 (0.003)\tLoss 0.9829 (1.0094)\tPrec@1 67.188 (64.356)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.003)\tLoss 1.0428 (1.0326)\tPrec@1 60.156 (63.188)\n",
      "Epoch: [72][312/390]\tTime 0.002 (0.003)\tLoss 1.2079 (1.0484)\tPrec@1 60.156 (62.607)\n",
      "Epoch: [72][390/390]\tTime 0.002 (0.003)\tLoss 0.9681 (1.0605)\tPrec@1 66.250 (62.256)\n",
      "EPOCH: 72 train Results: Prec@1 62.256 Loss: 1.0605\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0527 (1.0527)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1151 (1.2390)\tPrec@1 50.000 (55.620)\n",
      "EPOCH: 72 val Results: Prec@1 55.620 Loss: 1.2390\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/390]\tTime 0.004 (0.004)\tLoss 0.9666 (0.9666)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [73][78/390]\tTime 0.003 (0.003)\tLoss 0.9904 (1.0043)\tPrec@1 60.938 (64.409)\n",
      "Epoch: [73][156/390]\tTime 0.005 (0.003)\tLoss 1.1971 (1.0127)\tPrec@1 57.031 (63.699)\n",
      "Epoch: [73][234/390]\tTime 0.004 (0.003)\tLoss 0.9964 (1.0312)\tPrec@1 64.062 (63.112)\n",
      "Epoch: [73][312/390]\tTime 0.004 (0.004)\tLoss 0.9593 (1.0495)\tPrec@1 66.406 (62.455)\n",
      "Epoch: [73][390/390]\tTime 0.001 (0.004)\tLoss 1.0205 (1.0598)\tPrec@1 58.750 (62.196)\n",
      "EPOCH: 73 train Results: Prec@1 62.196 Loss: 1.0598\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0460 (1.0460)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9960 (1.2463)\tPrec@1 62.500 (56.190)\n",
      "EPOCH: 73 val Results: Prec@1 56.190 Loss: 1.2463\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/390]\tTime 0.003 (0.003)\tLoss 1.0569 (1.0569)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [74][78/390]\tTime 0.002 (0.003)\tLoss 0.9938 (0.9875)\tPrec@1 64.062 (65.328)\n",
      "Epoch: [74][156/390]\tTime 0.007 (0.004)\tLoss 1.1053 (1.0177)\tPrec@1 64.844 (64.072)\n",
      "Epoch: [74][234/390]\tTime 0.002 (0.004)\tLoss 0.9630 (1.0327)\tPrec@1 69.531 (63.617)\n",
      "Epoch: [74][312/390]\tTime 0.003 (0.004)\tLoss 1.0407 (1.0462)\tPrec@1 66.406 (62.974)\n",
      "Epoch: [74][390/390]\tTime 0.001 (0.003)\tLoss 1.1614 (1.0583)\tPrec@1 61.250 (62.544)\n",
      "EPOCH: 74 train Results: Prec@1 62.544 Loss: 1.0583\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1146 (1.1146)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2241 (1.2450)\tPrec@1 37.500 (55.550)\n",
      "EPOCH: 74 val Results: Prec@1 55.550 Loss: 1.2450\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/390]\tTime 0.004 (0.004)\tLoss 0.9704 (0.9704)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [75][78/390]\tTime 0.002 (0.004)\tLoss 1.1996 (0.9696)\tPrec@1 60.156 (65.813)\n",
      "Epoch: [75][156/390]\tTime 0.002 (0.003)\tLoss 0.8171 (1.0142)\tPrec@1 71.875 (63.993)\n",
      "Epoch: [75][234/390]\tTime 0.003 (0.003)\tLoss 1.2150 (1.0327)\tPrec@1 57.812 (63.348)\n",
      "Epoch: [75][312/390]\tTime 0.009 (0.003)\tLoss 1.2596 (1.0476)\tPrec@1 55.469 (62.785)\n",
      "Epoch: [75][390/390]\tTime 0.001 (0.003)\tLoss 1.0817 (1.0617)\tPrec@1 65.000 (62.334)\n",
      "EPOCH: 75 train Results: Prec@1 62.334 Loss: 1.0617\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0885 (1.0885)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2118 (1.2487)\tPrec@1 50.000 (55.540)\n",
      "EPOCH: 75 val Results: Prec@1 55.540 Loss: 1.2487\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/390]\tTime 0.005 (0.005)\tLoss 1.0850 (1.0850)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [76][78/390]\tTime 0.003 (0.003)\tLoss 1.0743 (0.9924)\tPrec@1 64.844 (64.705)\n",
      "Epoch: [76][156/390]\tTime 0.002 (0.003)\tLoss 0.9073 (1.0187)\tPrec@1 66.406 (63.515)\n",
      "Epoch: [76][234/390]\tTime 0.004 (0.003)\tLoss 1.0721 (1.0377)\tPrec@1 64.844 (62.856)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 0.9264 (1.0502)\tPrec@1 66.406 (62.522)\n",
      "Epoch: [76][390/390]\tTime 0.001 (0.003)\tLoss 0.8953 (1.0582)\tPrec@1 71.250 (62.310)\n",
      "EPOCH: 76 train Results: Prec@1 62.310 Loss: 1.0582\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1226 (1.1226)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3837 (1.2391)\tPrec@1 43.750 (56.110)\n",
      "EPOCH: 76 val Results: Prec@1 56.110 Loss: 1.2391\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/390]\tTime 0.003 (0.003)\tLoss 1.0682 (1.0682)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [77][78/390]\tTime 0.008 (0.003)\tLoss 1.1511 (0.9934)\tPrec@1 62.500 (65.042)\n",
      "Epoch: [77][156/390]\tTime 0.002 (0.003)\tLoss 1.0579 (1.0165)\tPrec@1 58.594 (63.824)\n",
      "Epoch: [77][234/390]\tTime 0.002 (0.003)\tLoss 0.9579 (1.0358)\tPrec@1 67.188 (63.075)\n",
      "Epoch: [77][312/390]\tTime 0.004 (0.003)\tLoss 1.1555 (1.0471)\tPrec@1 56.250 (62.562)\n",
      "Epoch: [77][390/390]\tTime 0.005 (0.003)\tLoss 1.2501 (1.0592)\tPrec@1 58.750 (62.104)\n",
      "EPOCH: 77 train Results: Prec@1 62.104 Loss: 1.0592\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0743 (1.0743)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3370 (1.2435)\tPrec@1 37.500 (56.360)\n",
      "EPOCH: 77 val Results: Prec@1 56.360 Loss: 1.2435\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/390]\tTime 0.002 (0.002)\tLoss 0.9345 (0.9345)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [78][78/390]\tTime 0.002 (0.003)\tLoss 1.0865 (0.9747)\tPrec@1 63.281 (65.951)\n",
      "Epoch: [78][156/390]\tTime 0.002 (0.003)\tLoss 1.0512 (1.0067)\tPrec@1 57.812 (64.291)\n",
      "Epoch: [78][234/390]\tTime 0.003 (0.003)\tLoss 1.0499 (1.0307)\tPrec@1 58.594 (63.414)\n",
      "Epoch: [78][312/390]\tTime 0.003 (0.003)\tLoss 1.0263 (1.0494)\tPrec@1 63.281 (62.725)\n",
      "Epoch: [78][390/390]\tTime 0.003 (0.003)\tLoss 0.9560 (1.0601)\tPrec@1 65.000 (62.236)\n",
      "EPOCH: 78 train Results: Prec@1 62.236 Loss: 1.0601\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0642 (1.0642)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1562 (1.2264)\tPrec@1 50.000 (56.410)\n",
      "EPOCH: 78 val Results: Prec@1 56.410 Loss: 1.2264\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/390]\tTime 0.002 (0.002)\tLoss 0.9350 (0.9350)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [79][78/390]\tTime 0.003 (0.004)\tLoss 0.9212 (0.9652)\tPrec@1 66.406 (65.536)\n",
      "Epoch: [79][156/390]\tTime 0.003 (0.004)\tLoss 1.0073 (0.9898)\tPrec@1 63.281 (64.779)\n",
      "Epoch: [79][234/390]\tTime 0.002 (0.004)\tLoss 1.0801 (1.0174)\tPrec@1 57.812 (63.684)\n",
      "Epoch: [79][312/390]\tTime 0.002 (0.003)\tLoss 1.1711 (1.0402)\tPrec@1 61.719 (62.807)\n",
      "Epoch: [79][390/390]\tTime 0.003 (0.003)\tLoss 1.0613 (1.0582)\tPrec@1 62.500 (62.166)\n",
      "EPOCH: 79 train Results: Prec@1 62.166 Loss: 1.0582\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1012 (1.1012)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0789 (1.2368)\tPrec@1 50.000 (56.130)\n",
      "EPOCH: 79 val Results: Prec@1 56.130 Loss: 1.2368\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/390]\tTime 0.004 (0.004)\tLoss 1.0653 (1.0653)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [80][78/390]\tTime 0.003 (0.003)\tLoss 0.9917 (0.9768)\tPrec@1 67.969 (65.338)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.0682 (1.0098)\tPrec@1 64.844 (64.048)\n",
      "Epoch: [80][234/390]\tTime 0.050 (0.003)\tLoss 0.9361 (1.0345)\tPrec@1 71.875 (63.191)\n",
      "Epoch: [80][312/390]\tTime 0.003 (0.003)\tLoss 1.1971 (1.0509)\tPrec@1 55.469 (62.662)\n",
      "Epoch: [80][390/390]\tTime 0.004 (0.003)\tLoss 0.9871 (1.0588)\tPrec@1 63.750 (62.376)\n",
      "EPOCH: 80 train Results: Prec@1 62.376 Loss: 1.0588\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0967 (1.0967)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3533 (1.2382)\tPrec@1 37.500 (56.360)\n",
      "EPOCH: 80 val Results: Prec@1 56.360 Loss: 1.2382\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/390]\tTime 0.004 (0.004)\tLoss 1.0092 (1.0092)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [81][78/390]\tTime 0.005 (0.004)\tLoss 0.9845 (0.9870)\tPrec@1 65.625 (64.893)\n",
      "Epoch: [81][156/390]\tTime 0.004 (0.003)\tLoss 0.9456 (1.0194)\tPrec@1 65.625 (63.878)\n",
      "Epoch: [81][234/390]\tTime 0.009 (0.003)\tLoss 0.9140 (1.0297)\tPrec@1 67.969 (63.381)\n",
      "Epoch: [81][312/390]\tTime 0.008 (0.003)\tLoss 1.3317 (1.0470)\tPrec@1 53.906 (62.819)\n",
      "Epoch: [81][390/390]\tTime 0.001 (0.003)\tLoss 0.9416 (1.0591)\tPrec@1 65.000 (62.296)\n",
      "EPOCH: 81 train Results: Prec@1 62.296 Loss: 1.0591\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0528 (1.0528)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2105 (1.2388)\tPrec@1 50.000 (56.070)\n",
      "EPOCH: 81 val Results: Prec@1 56.070 Loss: 1.2388\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/390]\tTime 0.002 (0.002)\tLoss 0.9328 (0.9328)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [82][78/390]\tTime 0.002 (0.003)\tLoss 0.8914 (0.9804)\tPrec@1 68.750 (64.972)\n",
      "Epoch: [82][156/390]\tTime 0.002 (0.003)\tLoss 1.1088 (1.0058)\tPrec@1 58.594 (64.033)\n",
      "Epoch: [82][234/390]\tTime 0.007 (0.003)\tLoss 1.0809 (1.0249)\tPrec@1 54.688 (63.477)\n",
      "Epoch: [82][312/390]\tTime 0.002 (0.003)\tLoss 0.9501 (1.0394)\tPrec@1 67.188 (63.114)\n",
      "Epoch: [82][390/390]\tTime 0.002 (0.003)\tLoss 1.1346 (1.0595)\tPrec@1 65.000 (62.384)\n",
      "EPOCH: 82 train Results: Prec@1 62.384 Loss: 1.0595\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0937 (1.0937)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0692 (1.2347)\tPrec@1 50.000 (55.730)\n",
      "EPOCH: 82 val Results: Prec@1 55.730 Loss: 1.2347\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/390]\tTime 0.004 (0.004)\tLoss 0.8932 (0.8932)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [83][78/390]\tTime 0.002 (0.003)\tLoss 1.1726 (0.9789)\tPrec@1 60.156 (65.398)\n",
      "Epoch: [83][156/390]\tTime 0.003 (0.003)\tLoss 1.2147 (1.0056)\tPrec@1 54.688 (64.122)\n",
      "Epoch: [83][234/390]\tTime 0.002 (0.003)\tLoss 1.0634 (1.0267)\tPrec@1 59.375 (63.338)\n",
      "Epoch: [83][312/390]\tTime 0.014 (0.003)\tLoss 1.1273 (1.0444)\tPrec@1 59.375 (62.790)\n",
      "Epoch: [83][390/390]\tTime 0.002 (0.003)\tLoss 1.2299 (1.0543)\tPrec@1 61.250 (62.356)\n",
      "EPOCH: 83 train Results: Prec@1 62.356 Loss: 1.0543\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0239 (1.0239)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1605 (1.2462)\tPrec@1 37.500 (55.440)\n",
      "EPOCH: 83 val Results: Prec@1 55.440 Loss: 1.2462\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/390]\tTime 0.005 (0.005)\tLoss 0.8401 (0.8401)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [84][78/390]\tTime 0.002 (0.003)\tLoss 0.9206 (0.9969)\tPrec@1 69.531 (64.715)\n",
      "Epoch: [84][156/390]\tTime 0.002 (0.003)\tLoss 1.1842 (1.0151)\tPrec@1 60.156 (63.649)\n",
      "Epoch: [84][234/390]\tTime 0.002 (0.003)\tLoss 0.9738 (1.0361)\tPrec@1 67.188 (63.098)\n",
      "Epoch: [84][312/390]\tTime 0.002 (0.003)\tLoss 1.0543 (1.0467)\tPrec@1 60.156 (62.627)\n",
      "Epoch: [84][390/390]\tTime 0.001 (0.003)\tLoss 1.3391 (1.0536)\tPrec@1 55.000 (62.484)\n",
      "EPOCH: 84 train Results: Prec@1 62.484 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0489 (1.0489)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9687 (1.2547)\tPrec@1 62.500 (55.430)\n",
      "EPOCH: 84 val Results: Prec@1 55.430 Loss: 1.2547\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/390]\tTime 0.006 (0.006)\tLoss 0.9323 (0.9323)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [85][78/390]\tTime 0.008 (0.003)\tLoss 0.8057 (0.9630)\tPrec@1 70.312 (65.635)\n",
      "Epoch: [85][156/390]\tTime 0.004 (0.003)\tLoss 0.7996 (1.0001)\tPrec@1 71.094 (64.157)\n",
      "Epoch: [85][234/390]\tTime 0.003 (0.003)\tLoss 1.2340 (1.0238)\tPrec@1 53.906 (63.255)\n",
      "Epoch: [85][312/390]\tTime 0.010 (0.003)\tLoss 1.0246 (1.0428)\tPrec@1 66.406 (62.580)\n",
      "Epoch: [85][390/390]\tTime 0.001 (0.003)\tLoss 1.0914 (1.0555)\tPrec@1 56.250 (62.134)\n",
      "EPOCH: 85 train Results: Prec@1 62.134 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.0969 (1.0969)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2887 (1.2496)\tPrec@1 31.250 (55.460)\n",
      "EPOCH: 85 val Results: Prec@1 55.460 Loss: 1.2496\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/390]\tTime 0.009 (0.009)\tLoss 0.8980 (0.8980)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [86][78/390]\tTime 0.003 (0.003)\tLoss 0.9312 (0.9929)\tPrec@1 67.969 (64.864)\n",
      "Epoch: [86][156/390]\tTime 0.002 (0.003)\tLoss 0.9313 (1.0079)\tPrec@1 66.406 (64.222)\n",
      "Epoch: [86][234/390]\tTime 0.003 (0.003)\tLoss 1.0326 (1.0247)\tPrec@1 65.625 (63.597)\n",
      "Epoch: [86][312/390]\tTime 0.002 (0.003)\tLoss 1.2417 (1.0376)\tPrec@1 57.812 (63.109)\n",
      "Epoch: [86][390/390]\tTime 0.002 (0.003)\tLoss 1.2067 (1.0521)\tPrec@1 57.500 (62.514)\n",
      "EPOCH: 86 train Results: Prec@1 62.514 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1562 (1.1562)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0967 (1.2604)\tPrec@1 43.750 (55.510)\n",
      "EPOCH: 86 val Results: Prec@1 55.510 Loss: 1.2604\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/390]\tTime 0.002 (0.002)\tLoss 1.0135 (1.0135)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [87][78/390]\tTime 0.002 (0.005)\tLoss 0.8647 (0.9788)\tPrec@1 69.531 (65.150)\n",
      "Epoch: [87][156/390]\tTime 0.003 (0.004)\tLoss 1.0418 (1.0100)\tPrec@1 59.375 (63.923)\n",
      "Epoch: [87][234/390]\tTime 0.002 (0.004)\tLoss 0.9866 (1.0246)\tPrec@1 68.750 (63.491)\n",
      "Epoch: [87][312/390]\tTime 0.009 (0.004)\tLoss 1.2780 (1.0428)\tPrec@1 55.469 (62.802)\n",
      "Epoch: [87][390/390]\tTime 0.001 (0.003)\tLoss 1.2523 (1.0552)\tPrec@1 55.000 (62.298)\n",
      "EPOCH: 87 train Results: Prec@1 62.298 Loss: 1.0552\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0651 (1.0651)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9280 (1.2507)\tPrec@1 81.250 (55.830)\n",
      "EPOCH: 87 val Results: Prec@1 55.830 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/390]\tTime 0.003 (0.003)\tLoss 0.9995 (0.9995)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [88][78/390]\tTime 0.002 (0.003)\tLoss 0.9000 (0.9654)\tPrec@1 61.719 (65.961)\n",
      "Epoch: [88][156/390]\tTime 0.003 (0.003)\tLoss 1.1129 (1.0053)\tPrec@1 58.594 (64.237)\n",
      "Epoch: [88][234/390]\tTime 0.004 (0.003)\tLoss 1.0010 (1.0249)\tPrec@1 64.062 (63.414)\n",
      "Epoch: [88][312/390]\tTime 0.002 (0.003)\tLoss 1.0291 (1.0408)\tPrec@1 60.938 (62.767)\n",
      "Epoch: [88][390/390]\tTime 0.001 (0.003)\tLoss 1.2130 (1.0526)\tPrec@1 53.750 (62.344)\n",
      "EPOCH: 88 train Results: Prec@1 62.344 Loss: 1.0526\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1739 (1.1739)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9819 (1.2612)\tPrec@1 56.250 (55.480)\n",
      "EPOCH: 88 val Results: Prec@1 55.480 Loss: 1.2612\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/390]\tTime 0.005 (0.005)\tLoss 0.8559 (0.8559)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [89][78/390]\tTime 0.003 (0.003)\tLoss 0.9956 (0.9863)\tPrec@1 67.969 (64.814)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1244 (1.0013)\tPrec@1 60.938 (64.003)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 1.0521 (1.0211)\tPrec@1 64.844 (63.338)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.1018 (1.0424)\tPrec@1 64.062 (62.742)\n",
      "Epoch: [89][390/390]\tTime 0.002 (0.003)\tLoss 1.2623 (1.0571)\tPrec@1 47.500 (62.278)\n",
      "EPOCH: 89 train Results: Prec@1 62.278 Loss: 1.0571\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0993 (1.0993)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5994 (1.2553)\tPrec@1 43.750 (55.680)\n",
      "EPOCH: 89 val Results: Prec@1 55.680 Loss: 1.2553\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/390]\tTime 0.010 (0.010)\tLoss 1.0333 (1.0333)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [90][78/390]\tTime 0.003 (0.003)\tLoss 1.0047 (0.9770)\tPrec@1 59.375 (65.269)\n",
      "Epoch: [90][156/390]\tTime 0.008 (0.003)\tLoss 1.1533 (1.0010)\tPrec@1 55.469 (64.545)\n",
      "Epoch: [90][234/390]\tTime 0.004 (0.004)\tLoss 1.1356 (1.0262)\tPrec@1 55.469 (63.467)\n",
      "Epoch: [90][312/390]\tTime 0.002 (0.004)\tLoss 1.1258 (1.0369)\tPrec@1 62.500 (63.136)\n",
      "Epoch: [90][390/390]\tTime 0.003 (0.004)\tLoss 0.9424 (1.0507)\tPrec@1 61.250 (62.734)\n",
      "EPOCH: 90 train Results: Prec@1 62.734 Loss: 1.0507\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0546 (1.0546)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4359 (1.2632)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 90 val Results: Prec@1 55.300 Loss: 1.2632\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/390]\tTime 0.006 (0.006)\tLoss 1.1339 (1.1339)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [91][78/390]\tTime 0.004 (0.003)\tLoss 1.0576 (0.9768)\tPrec@1 63.281 (65.833)\n",
      "Epoch: [91][156/390]\tTime 0.004 (0.003)\tLoss 1.1931 (1.0058)\tPrec@1 53.125 (64.306)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.003)\tLoss 1.1554 (1.0209)\tPrec@1 55.469 (63.893)\n",
      "Epoch: [91][312/390]\tTime 0.004 (0.003)\tLoss 1.1233 (1.0426)\tPrec@1 63.281 (63.186)\n",
      "Epoch: [91][390/390]\tTime 0.003 (0.003)\tLoss 1.1399 (1.0569)\tPrec@1 61.250 (62.574)\n",
      "EPOCH: 91 train Results: Prec@1 62.574 Loss: 1.0569\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0760 (1.0760)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1623 (1.2602)\tPrec@1 43.750 (55.310)\n",
      "EPOCH: 91 val Results: Prec@1 55.310 Loss: 1.2602\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/390]\tTime 0.005 (0.005)\tLoss 0.9916 (0.9916)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.004)\tLoss 0.9857 (0.9783)\tPrec@1 64.062 (65.051)\n",
      "Epoch: [92][156/390]\tTime 0.002 (0.003)\tLoss 1.1312 (1.0153)\tPrec@1 59.375 (63.878)\n",
      "Epoch: [92][234/390]\tTime 0.007 (0.003)\tLoss 1.0429 (1.0284)\tPrec@1 62.500 (63.338)\n",
      "Epoch: [92][312/390]\tTime 0.002 (0.003)\tLoss 1.1341 (1.0430)\tPrec@1 54.688 (62.834)\n",
      "Epoch: [92][390/390]\tTime 0.003 (0.003)\tLoss 1.0723 (1.0556)\tPrec@1 55.000 (62.358)\n",
      "EPOCH: 92 train Results: Prec@1 62.358 Loss: 1.0556\n",
      "Test: [0/78]\tTime 0.013 (0.013)\tLoss 1.1095 (1.1095)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2882 (1.2490)\tPrec@1 56.250 (55.770)\n",
      "EPOCH: 92 val Results: Prec@1 55.770 Loss: 1.2490\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 1.1153 (1.1153)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.003)\tLoss 0.9583 (0.9637)\tPrec@1 61.719 (66.011)\n",
      "Epoch: [93][156/390]\tTime 0.002 (0.003)\tLoss 1.0579 (0.9988)\tPrec@1 62.500 (64.485)\n",
      "Epoch: [93][234/390]\tTime 0.006 (0.003)\tLoss 1.0428 (1.0191)\tPrec@1 60.938 (63.800)\n",
      "Epoch: [93][312/390]\tTime 0.002 (0.004)\tLoss 1.0828 (1.0350)\tPrec@1 61.719 (63.151)\n",
      "Epoch: [93][390/390]\tTime 0.001 (0.003)\tLoss 1.0638 (1.0515)\tPrec@1 67.500 (62.572)\n",
      "EPOCH: 93 train Results: Prec@1 62.572 Loss: 1.0515\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1133 (1.1133)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0813 (1.2490)\tPrec@1 50.000 (56.050)\n",
      "EPOCH: 93 val Results: Prec@1 56.050 Loss: 1.2490\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/390]\tTime 0.007 (0.007)\tLoss 0.8406 (0.8406)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [94][78/390]\tTime 0.004 (0.004)\tLoss 0.9173 (0.9631)\tPrec@1 61.719 (66.119)\n",
      "Epoch: [94][156/390]\tTime 0.004 (0.003)\tLoss 1.0470 (0.9945)\tPrec@1 64.062 (64.804)\n",
      "Epoch: [94][234/390]\tTime 0.003 (0.003)\tLoss 0.9596 (1.0226)\tPrec@1 67.188 (63.680)\n",
      "Epoch: [94][312/390]\tTime 0.004 (0.003)\tLoss 1.1657 (1.0376)\tPrec@1 59.375 (63.174)\n",
      "Epoch: [94][390/390]\tTime 0.005 (0.003)\tLoss 1.3645 (1.0508)\tPrec@1 61.250 (62.662)\n",
      "EPOCH: 94 train Results: Prec@1 62.662 Loss: 1.0508\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1694 (1.1694)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2530 (1.2521)\tPrec@1 50.000 (55.590)\n",
      "EPOCH: 94 val Results: Prec@1 55.590 Loss: 1.2521\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/390]\tTime 0.002 (0.002)\tLoss 0.9955 (0.9955)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [95][78/390]\tTime 0.003 (0.003)\tLoss 1.0948 (0.9753)\tPrec@1 58.594 (65.615)\n",
      "Epoch: [95][156/390]\tTime 0.004 (0.003)\tLoss 0.9292 (1.0011)\tPrec@1 65.625 (64.570)\n",
      "Epoch: [95][234/390]\tTime 0.004 (0.003)\tLoss 1.0815 (1.0234)\tPrec@1 60.938 (63.780)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.003)\tLoss 1.0083 (1.0413)\tPrec@1 69.531 (63.034)\n",
      "Epoch: [95][390/390]\tTime 0.001 (0.003)\tLoss 1.2892 (1.0548)\tPrec@1 55.000 (62.504)\n",
      "EPOCH: 95 train Results: Prec@1 62.504 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0758 (1.0758)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2688 (1.2507)\tPrec@1 56.250 (56.000)\n",
      "EPOCH: 95 val Results: Prec@1 56.000 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/390]\tTime 0.004 (0.004)\tLoss 1.0427 (1.0427)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.003)\tLoss 0.9270 (0.9719)\tPrec@1 61.719 (65.645)\n",
      "Epoch: [96][156/390]\tTime 0.002 (0.003)\tLoss 0.9314 (1.0010)\tPrec@1 70.312 (64.391)\n",
      "Epoch: [96][234/390]\tTime 0.002 (0.003)\tLoss 0.9764 (1.0256)\tPrec@1 60.938 (63.497)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 1.0744 (1.0378)\tPrec@1 62.500 (63.047)\n",
      "Epoch: [96][390/390]\tTime 0.002 (0.003)\tLoss 1.1342 (1.0506)\tPrec@1 56.250 (62.570)\n",
      "EPOCH: 96 train Results: Prec@1 62.570 Loss: 1.0506\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1188 (1.1188)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1228 (1.2556)\tPrec@1 56.250 (55.160)\n",
      "EPOCH: 96 val Results: Prec@1 55.160 Loss: 1.2556\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/390]\tTime 0.003 (0.003)\tLoss 1.0088 (1.0088)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [97][78/390]\tTime 0.002 (0.003)\tLoss 0.9967 (0.9887)\tPrec@1 66.406 (64.547)\n",
      "Epoch: [97][156/390]\tTime 0.004 (0.003)\tLoss 1.1829 (1.0084)\tPrec@1 57.812 (63.893)\n",
      "Epoch: [97][234/390]\tTime 0.005 (0.003)\tLoss 1.2903 (1.0248)\tPrec@1 52.344 (63.251)\n",
      "Epoch: [97][312/390]\tTime 0.004 (0.003)\tLoss 1.0482 (1.0389)\tPrec@1 64.844 (62.837)\n",
      "Epoch: [97][390/390]\tTime 0.001 (0.003)\tLoss 1.0343 (1.0521)\tPrec@1 60.000 (62.310)\n",
      "EPOCH: 97 train Results: Prec@1 62.310 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1167 (1.1167)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0366 (1.2541)\tPrec@1 50.000 (55.810)\n",
      "EPOCH: 97 val Results: Prec@1 55.810 Loss: 1.2541\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/390]\tTime 0.002 (0.002)\tLoss 1.0014 (1.0014)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [98][78/390]\tTime 0.002 (0.004)\tLoss 0.8379 (0.9678)\tPrec@1 71.094 (65.506)\n",
      "Epoch: [98][156/390]\tTime 0.003 (0.004)\tLoss 1.2156 (0.9963)\tPrec@1 57.031 (64.704)\n",
      "Epoch: [98][234/390]\tTime 0.004 (0.003)\tLoss 0.9183 (1.0151)\tPrec@1 67.188 (63.873)\n",
      "Epoch: [98][312/390]\tTime 0.003 (0.004)\tLoss 1.2128 (1.0313)\tPrec@1 57.812 (63.256)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.004)\tLoss 1.0868 (1.0448)\tPrec@1 58.750 (62.808)\n",
      "EPOCH: 98 train Results: Prec@1 62.808 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1962 (1.1962)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4396 (1.2620)\tPrec@1 25.000 (55.340)\n",
      "EPOCH: 98 val Results: Prec@1 55.340 Loss: 1.2620\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/390]\tTime 0.003 (0.003)\tLoss 0.9243 (0.9243)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.003)\tLoss 0.8636 (0.9757)\tPrec@1 70.312 (65.674)\n",
      "Epoch: [99][156/390]\tTime 0.003 (0.003)\tLoss 1.0347 (1.0092)\tPrec@1 60.156 (64.197)\n",
      "Epoch: [99][234/390]\tTime 0.003 (0.003)\tLoss 1.2268 (1.0236)\tPrec@1 52.344 (63.710)\n",
      "Epoch: [99][312/390]\tTime 0.004 (0.003)\tLoss 1.1035 (1.0377)\tPrec@1 56.250 (63.069)\n",
      "Epoch: [99][390/390]\tTime 0.002 (0.003)\tLoss 1.0416 (1.0497)\tPrec@1 60.000 (62.538)\n",
      "EPOCH: 99 train Results: Prec@1 62.538 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1437 (1.1437)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4257 (1.2477)\tPrec@1 50.000 (55.610)\n",
      "EPOCH: 99 val Results: Prec@1 55.610 Loss: 1.2477\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/390]\tTime 0.002 (0.002)\tLoss 0.8875 (0.8875)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [100][78/390]\tTime 0.002 (0.003)\tLoss 1.0347 (0.9757)\tPrec@1 58.594 (64.666)\n",
      "Epoch: [100][156/390]\tTime 0.003 (0.003)\tLoss 1.0858 (1.0032)\tPrec@1 57.812 (64.003)\n",
      "Epoch: [100][234/390]\tTime 0.005 (0.003)\tLoss 1.2427 (1.0248)\tPrec@1 53.906 (63.291)\n",
      "Epoch: [100][312/390]\tTime 0.002 (0.003)\tLoss 1.0586 (1.0436)\tPrec@1 64.062 (62.527)\n",
      "Epoch: [100][390/390]\tTime 0.003 (0.003)\tLoss 1.1984 (1.0530)\tPrec@1 56.250 (62.242)\n",
      "EPOCH: 100 train Results: Prec@1 62.242 Loss: 1.0530\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1344 (1.1344)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2422 (1.2494)\tPrec@1 37.500 (56.000)\n",
      "EPOCH: 100 val Results: Prec@1 56.000 Loss: 1.2494\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/390]\tTime 0.002 (0.002)\tLoss 0.9254 (0.9254)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.004)\tLoss 1.0826 (0.9830)\tPrec@1 53.906 (65.140)\n",
      "Epoch: [101][156/390]\tTime 0.010 (0.003)\tLoss 1.1910 (1.0097)\tPrec@1 56.250 (63.988)\n",
      "Epoch: [101][234/390]\tTime 0.003 (0.003)\tLoss 1.0678 (1.0299)\tPrec@1 67.188 (63.275)\n",
      "Epoch: [101][312/390]\tTime 0.004 (0.003)\tLoss 1.0244 (1.0359)\tPrec@1 64.844 (62.992)\n",
      "Epoch: [101][390/390]\tTime 0.001 (0.004)\tLoss 1.1678 (1.0487)\tPrec@1 60.000 (62.598)\n",
      "EPOCH: 101 train Results: Prec@1 62.598 Loss: 1.0487\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1532 (1.1532)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3408 (1.2666)\tPrec@1 18.750 (54.950)\n",
      "EPOCH: 101 val Results: Prec@1 54.950 Loss: 1.2666\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/390]\tTime 0.004 (0.004)\tLoss 1.0317 (1.0317)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [102][78/390]\tTime 0.003 (0.004)\tLoss 0.9895 (0.9583)\tPrec@1 59.375 (66.080)\n",
      "Epoch: [102][156/390]\tTime 0.003 (0.003)\tLoss 1.0654 (0.9975)\tPrec@1 63.281 (64.645)\n",
      "Epoch: [102][234/390]\tTime 0.004 (0.003)\tLoss 1.0530 (1.0209)\tPrec@1 64.062 (63.797)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.003)\tLoss 1.0630 (1.0392)\tPrec@1 54.688 (62.992)\n",
      "Epoch: [102][390/390]\tTime 0.003 (0.003)\tLoss 1.0967 (1.0507)\tPrec@1 58.750 (62.600)\n",
      "EPOCH: 102 train Results: Prec@1 62.600 Loss: 1.0507\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1662 (1.1662)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3014 (1.2577)\tPrec@1 37.500 (55.270)\n",
      "EPOCH: 102 val Results: Prec@1 55.270 Loss: 1.2577\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/390]\tTime 0.009 (0.009)\tLoss 0.9682 (0.9682)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [103][78/390]\tTime 0.002 (0.003)\tLoss 0.8984 (0.9784)\tPrec@1 64.062 (65.239)\n",
      "Epoch: [103][156/390]\tTime 0.008 (0.003)\tLoss 1.1070 (1.0056)\tPrec@1 61.719 (64.043)\n",
      "Epoch: [103][234/390]\tTime 0.003 (0.003)\tLoss 1.1402 (1.0241)\tPrec@1 62.500 (63.457)\n",
      "Epoch: [103][312/390]\tTime 0.003 (0.003)\tLoss 1.2198 (1.0367)\tPrec@1 57.031 (63.144)\n",
      "Epoch: [103][390/390]\tTime 0.001 (0.003)\tLoss 1.1931 (1.0488)\tPrec@1 60.000 (62.696)\n",
      "EPOCH: 103 train Results: Prec@1 62.696 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0719 (1.0719)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2487 (1.2660)\tPrec@1 56.250 (55.050)\n",
      "EPOCH: 103 val Results: Prec@1 55.050 Loss: 1.2660\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/390]\tTime 0.003 (0.003)\tLoss 1.0331 (1.0331)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 1.0011 (0.9796)\tPrec@1 66.406 (65.121)\n",
      "Epoch: [104][156/390]\tTime 0.005 (0.003)\tLoss 0.9663 (1.0074)\tPrec@1 63.281 (64.077)\n",
      "Epoch: [104][234/390]\tTime 0.004 (0.003)\tLoss 0.9742 (1.0189)\tPrec@1 64.062 (63.680)\n",
      "Epoch: [104][312/390]\tTime 0.002 (0.003)\tLoss 0.9747 (1.0376)\tPrec@1 64.062 (62.927)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.003)\tLoss 1.0613 (1.0492)\tPrec@1 65.000 (62.474)\n",
      "EPOCH: 104 train Results: Prec@1 62.474 Loss: 1.0492\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1067 (1.1067)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1503 (1.2508)\tPrec@1 37.500 (55.550)\n",
      "EPOCH: 104 val Results: Prec@1 55.550 Loss: 1.2508\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/390]\tTime 0.007 (0.007)\tLoss 0.8887 (0.8887)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [105][78/390]\tTime 0.002 (0.003)\tLoss 0.9612 (0.9843)\tPrec@1 63.281 (65.368)\n",
      "Epoch: [105][156/390]\tTime 0.003 (0.003)\tLoss 1.1261 (1.0031)\tPrec@1 59.375 (64.540)\n",
      "Epoch: [105][234/390]\tTime 0.010 (0.003)\tLoss 1.2823 (1.0195)\tPrec@1 58.594 (63.757)\n",
      "Epoch: [105][312/390]\tTime 0.003 (0.003)\tLoss 1.1225 (1.0329)\tPrec@1 61.719 (63.289)\n",
      "Epoch: [105][390/390]\tTime 0.001 (0.003)\tLoss 1.1396 (1.0504)\tPrec@1 58.750 (62.800)\n",
      "EPOCH: 105 train Results: Prec@1 62.800 Loss: 1.0504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0322 (1.0322)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1672 (1.2508)\tPrec@1 43.750 (55.840)\n",
      "EPOCH: 105 val Results: Prec@1 55.840 Loss: 1.2508\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/390]\tTime 0.002 (0.002)\tLoss 0.8676 (0.8676)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [106][78/390]\tTime 0.004 (0.003)\tLoss 0.8870 (0.9637)\tPrec@1 64.844 (65.388)\n",
      "Epoch: [106][156/390]\tTime 0.003 (0.003)\tLoss 1.0311 (0.9966)\tPrec@1 60.156 (64.416)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.003)\tLoss 1.0268 (1.0190)\tPrec@1 63.281 (63.541)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.003)\tLoss 1.0719 (1.0369)\tPrec@1 61.719 (62.947)\n",
      "Epoch: [106][390/390]\tTime 0.003 (0.003)\tLoss 1.2130 (1.0498)\tPrec@1 63.750 (62.414)\n",
      "EPOCH: 106 train Results: Prec@1 62.414 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 0.9733 (0.9733)\tPrec@1 69.531 (69.531)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3981 (1.2439)\tPrec@1 37.500 (55.830)\n",
      "EPOCH: 106 val Results: Prec@1 55.830 Loss: 1.2439\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/390]\tTime 0.002 (0.002)\tLoss 0.8487 (0.8487)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [107][78/390]\tTime 0.002 (0.003)\tLoss 1.0857 (0.9655)\tPrec@1 66.406 (65.773)\n",
      "Epoch: [107][156/390]\tTime 0.008 (0.004)\tLoss 1.2772 (0.9917)\tPrec@1 49.219 (64.515)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.004)\tLoss 1.2490 (1.0126)\tPrec@1 56.250 (63.830)\n",
      "Epoch: [107][312/390]\tTime 0.003 (0.003)\tLoss 1.1009 (1.0383)\tPrec@1 60.156 (62.892)\n",
      "Epoch: [107][390/390]\tTime 0.003 (0.003)\tLoss 1.0090 (1.0536)\tPrec@1 66.250 (62.330)\n",
      "EPOCH: 107 train Results: Prec@1 62.330 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1421 (1.1421)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4340 (1.2430)\tPrec@1 31.250 (56.090)\n",
      "EPOCH: 107 val Results: Prec@1 56.090 Loss: 1.2430\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/390]\tTime 0.003 (0.003)\tLoss 0.9561 (0.9561)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [108][78/390]\tTime 0.003 (0.003)\tLoss 1.0664 (0.9586)\tPrec@1 65.625 (65.427)\n",
      "Epoch: [108][156/390]\tTime 0.010 (0.003)\tLoss 1.0058 (0.9943)\tPrec@1 66.406 (64.431)\n",
      "Epoch: [108][234/390]\tTime 0.002 (0.003)\tLoss 1.1536 (1.0170)\tPrec@1 57.031 (63.687)\n",
      "Epoch: [108][312/390]\tTime 0.002 (0.003)\tLoss 1.1132 (1.0324)\tPrec@1 63.281 (63.107)\n",
      "Epoch: [108][390/390]\tTime 0.001 (0.003)\tLoss 1.0228 (1.0460)\tPrec@1 62.500 (62.678)\n",
      "EPOCH: 108 train Results: Prec@1 62.678 Loss: 1.0460\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0986 (1.0986)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3707 (1.2564)\tPrec@1 31.250 (55.430)\n",
      "EPOCH: 108 val Results: Prec@1 55.430 Loss: 1.2564\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/390]\tTime 0.004 (0.004)\tLoss 1.0514 (1.0514)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [109][78/390]\tTime 0.003 (0.003)\tLoss 1.0287 (0.9789)\tPrec@1 60.938 (65.496)\n",
      "Epoch: [109][156/390]\tTime 0.002 (0.003)\tLoss 1.1056 (1.0128)\tPrec@1 63.281 (64.147)\n",
      "Epoch: [109][234/390]\tTime 0.012 (0.003)\tLoss 0.8665 (1.0242)\tPrec@1 63.281 (63.541)\n",
      "Epoch: [109][312/390]\tTime 0.004 (0.003)\tLoss 1.0765 (1.0368)\tPrec@1 59.375 (63.062)\n",
      "Epoch: [109][390/390]\tTime 0.002 (0.003)\tLoss 1.0126 (1.0458)\tPrec@1 66.250 (62.826)\n",
      "EPOCH: 109 train Results: Prec@1 62.826 Loss: 1.0458\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0866 (1.0866)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4049 (1.2472)\tPrec@1 43.750 (55.830)\n",
      "EPOCH: 109 val Results: Prec@1 55.830 Loss: 1.2472\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/390]\tTime 0.003 (0.003)\tLoss 0.9575 (0.9575)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 0.9617 (0.9704)\tPrec@1 64.844 (65.754)\n",
      "Epoch: [110][156/390]\tTime 0.003 (0.003)\tLoss 0.9745 (1.0015)\tPrec@1 66.406 (64.366)\n",
      "Epoch: [110][234/390]\tTime 0.002 (0.004)\tLoss 1.1295 (1.0162)\tPrec@1 54.688 (63.737)\n",
      "Epoch: [110][312/390]\tTime 0.002 (0.004)\tLoss 1.0311 (1.0375)\tPrec@1 60.938 (62.964)\n",
      "Epoch: [110][390/390]\tTime 0.007 (0.003)\tLoss 1.2203 (1.0472)\tPrec@1 60.000 (62.600)\n",
      "EPOCH: 110 train Results: Prec@1 62.600 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0518 (1.0518)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4546 (1.2513)\tPrec@1 31.250 (55.780)\n",
      "EPOCH: 110 val Results: Prec@1 55.780 Loss: 1.2513\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/390]\tTime 0.004 (0.004)\tLoss 0.9275 (0.9275)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [111][78/390]\tTime 0.011 (0.003)\tLoss 1.0038 (0.9714)\tPrec@1 65.625 (65.388)\n",
      "Epoch: [111][156/390]\tTime 0.002 (0.004)\tLoss 0.9791 (1.0022)\tPrec@1 63.281 (64.082)\n",
      "Epoch: [111][234/390]\tTime 0.002 (0.004)\tLoss 1.2667 (1.0251)\tPrec@1 57.031 (63.298)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.004)\tLoss 1.0145 (1.0410)\tPrec@1 63.281 (62.610)\n",
      "Epoch: [111][390/390]\tTime 0.001 (0.004)\tLoss 1.0910 (1.0516)\tPrec@1 63.750 (62.280)\n",
      "EPOCH: 111 train Results: Prec@1 62.280 Loss: 1.0516\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0741 (1.0741)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1872 (1.2686)\tPrec@1 56.250 (55.430)\n",
      "EPOCH: 111 val Results: Prec@1 55.430 Loss: 1.2686\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/390]\tTime 0.004 (0.004)\tLoss 0.8688 (0.8688)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.0722 (0.9618)\tPrec@1 64.844 (66.386)\n",
      "Epoch: [112][156/390]\tTime 0.002 (0.003)\tLoss 1.0915 (0.9913)\tPrec@1 57.031 (65.048)\n",
      "Epoch: [112][234/390]\tTime 0.004 (0.003)\tLoss 1.1248 (1.0128)\tPrec@1 57.812 (64.219)\n",
      "Epoch: [112][312/390]\tTime 0.007 (0.003)\tLoss 0.9686 (1.0398)\tPrec@1 67.188 (63.256)\n",
      "Epoch: [112][390/390]\tTime 0.002 (0.003)\tLoss 1.2718 (1.0478)\tPrec@1 52.500 (62.950)\n",
      "EPOCH: 112 train Results: Prec@1 62.950 Loss: 1.0478\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0866 (1.0866)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2353 (1.2671)\tPrec@1 56.250 (55.170)\n",
      "EPOCH: 112 val Results: Prec@1 55.170 Loss: 1.2671\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/390]\tTime 0.007 (0.007)\tLoss 0.9353 (0.9353)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [113][78/390]\tTime 0.002 (0.003)\tLoss 0.9818 (0.9667)\tPrec@1 67.969 (66.021)\n",
      "Epoch: [113][156/390]\tTime 0.002 (0.004)\tLoss 1.0095 (0.9895)\tPrec@1 64.062 (64.864)\n",
      "Epoch: [113][234/390]\tTime 0.002 (0.004)\tLoss 0.8883 (1.0094)\tPrec@1 67.188 (64.026)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.004)\tLoss 1.0436 (1.0262)\tPrec@1 60.938 (63.441)\n",
      "Epoch: [113][390/390]\tTime 0.002 (0.004)\tLoss 1.1023 (1.0431)\tPrec@1 60.000 (62.872)\n",
      "EPOCH: 113 train Results: Prec@1 62.872 Loss: 1.0431\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1274 (1.1274)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9470 (1.2451)\tPrec@1 56.250 (55.880)\n",
      "EPOCH: 113 val Results: Prec@1 55.880 Loss: 1.2451\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/390]\tTime 0.003 (0.003)\tLoss 0.9677 (0.9677)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.003)\tLoss 0.7941 (0.9793)\tPrec@1 75.000 (65.506)\n",
      "Epoch: [114][156/390]\tTime 0.002 (0.003)\tLoss 1.0040 (1.0067)\tPrec@1 65.625 (64.286)\n",
      "Epoch: [114][234/390]\tTime 0.009 (0.003)\tLoss 1.0510 (1.0231)\tPrec@1 60.156 (63.604)\n",
      "Epoch: [114][312/390]\tTime 0.002 (0.003)\tLoss 1.0073 (1.0352)\tPrec@1 65.625 (63.221)\n",
      "Epoch: [114][390/390]\tTime 0.001 (0.003)\tLoss 0.9860 (1.0466)\tPrec@1 66.250 (62.690)\n",
      "EPOCH: 114 train Results: Prec@1 62.690 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0382 (1.0382)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3069 (1.2575)\tPrec@1 50.000 (55.460)\n",
      "EPOCH: 114 val Results: Prec@1 55.460 Loss: 1.2575\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/390]\tTime 0.005 (0.005)\tLoss 0.7814 (0.7814)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [115][78/390]\tTime 0.004 (0.003)\tLoss 0.8657 (0.9689)\tPrec@1 71.094 (65.645)\n",
      "Epoch: [115][156/390]\tTime 0.002 (0.003)\tLoss 1.1498 (0.9985)\tPrec@1 57.031 (64.331)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.003)\tLoss 1.1475 (1.0192)\tPrec@1 56.250 (63.501)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.004)\tLoss 1.2164 (1.0376)\tPrec@1 54.688 (62.927)\n",
      "Epoch: [115][390/390]\tTime 0.002 (0.003)\tLoss 1.3975 (1.0477)\tPrec@1 55.000 (62.592)\n",
      "EPOCH: 115 train Results: Prec@1 62.592 Loss: 1.0477\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0696 (1.0696)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3099 (1.2569)\tPrec@1 50.000 (55.730)\n",
      "EPOCH: 115 val Results: Prec@1 55.730 Loss: 1.2569\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/390]\tTime 0.006 (0.006)\tLoss 0.9875 (0.9875)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [116][78/390]\tTime 0.002 (0.003)\tLoss 0.8823 (0.9635)\tPrec@1 69.531 (65.734)\n",
      "Epoch: [116][156/390]\tTime 0.005 (0.003)\tLoss 1.1125 (0.9933)\tPrec@1 61.719 (64.421)\n",
      "Epoch: [116][234/390]\tTime 0.004 (0.003)\tLoss 1.0027 (1.0165)\tPrec@1 65.625 (63.737)\n",
      "Epoch: [116][312/390]\tTime 0.015 (0.003)\tLoss 1.1709 (1.0354)\tPrec@1 57.031 (63.176)\n",
      "Epoch: [116][390/390]\tTime 0.001 (0.003)\tLoss 1.1687 (1.0459)\tPrec@1 61.250 (62.776)\n",
      "EPOCH: 116 train Results: Prec@1 62.776 Loss: 1.0459\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0481 (1.0481)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0719 (1.2552)\tPrec@1 43.750 (55.720)\n",
      "EPOCH: 116 val Results: Prec@1 55.720 Loss: 1.2552\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/390]\tTime 0.003 (0.003)\tLoss 0.9939 (0.9939)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [117][78/390]\tTime 0.002 (0.003)\tLoss 0.9940 (0.9623)\tPrec@1 63.281 (66.119)\n",
      "Epoch: [117][156/390]\tTime 0.002 (0.004)\tLoss 1.1332 (0.9940)\tPrec@1 61.719 (64.670)\n",
      "Epoch: [117][234/390]\tTime 0.007 (0.003)\tLoss 1.1604 (1.0174)\tPrec@1 59.375 (63.836)\n",
      "Epoch: [117][312/390]\tTime 0.003 (0.003)\tLoss 1.0499 (1.0337)\tPrec@1 56.250 (63.156)\n",
      "Epoch: [117][390/390]\tTime 0.001 (0.003)\tLoss 1.0532 (1.0426)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 117 train Results: Prec@1 62.850 Loss: 1.0426\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 0.9877 (0.9877)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4792 (1.2551)\tPrec@1 25.000 (55.770)\n",
      "EPOCH: 117 val Results: Prec@1 55.770 Loss: 1.2551\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/390]\tTime 0.009 (0.009)\tLoss 1.0176 (1.0176)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [118][78/390]\tTime 0.003 (0.004)\tLoss 0.9719 (0.9888)\tPrec@1 69.531 (65.506)\n",
      "Epoch: [118][156/390]\tTime 0.003 (0.003)\tLoss 1.0682 (0.9991)\tPrec@1 63.281 (64.625)\n",
      "Epoch: [118][234/390]\tTime 0.009 (0.003)\tLoss 1.1744 (1.0164)\tPrec@1 60.156 (63.973)\n",
      "Epoch: [118][312/390]\tTime 0.002 (0.003)\tLoss 1.1091 (1.0370)\tPrec@1 64.844 (63.057)\n",
      "Epoch: [118][390/390]\tTime 0.001 (0.003)\tLoss 1.1544 (1.0515)\tPrec@1 60.000 (62.562)\n",
      "EPOCH: 118 train Results: Prec@1 62.562 Loss: 1.0515\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0916 (1.0916)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1223 (1.2681)\tPrec@1 37.500 (55.330)\n",
      "EPOCH: 118 val Results: Prec@1 55.330 Loss: 1.2681\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/390]\tTime 0.002 (0.002)\tLoss 0.9615 (0.9615)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [119][78/390]\tTime 0.002 (0.003)\tLoss 1.0151 (0.9596)\tPrec@1 64.844 (66.317)\n",
      "Epoch: [119][156/390]\tTime 0.002 (0.003)\tLoss 1.0276 (0.9860)\tPrec@1 63.281 (65.147)\n",
      "Epoch: [119][234/390]\tTime 0.003 (0.003)\tLoss 0.9933 (1.0087)\tPrec@1 66.406 (64.119)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.003)\tLoss 1.0602 (1.0250)\tPrec@1 63.281 (63.498)\n",
      "Epoch: [119][390/390]\tTime 0.001 (0.003)\tLoss 1.2058 (1.0399)\tPrec@1 58.750 (62.928)\n",
      "EPOCH: 119 train Results: Prec@1 62.928 Loss: 1.0399\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.0256 (1.0256)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2464 (1.2539)\tPrec@1 43.750 (55.800)\n",
      "EPOCH: 119 val Results: Prec@1 55.800 Loss: 1.2539\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/390]\tTime 0.003 (0.003)\tLoss 0.8039 (0.8039)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.003)\tLoss 1.0426 (0.9665)\tPrec@1 61.719 (65.872)\n",
      "Epoch: [120][156/390]\tTime 0.004 (0.003)\tLoss 0.9273 (0.9957)\tPrec@1 65.625 (64.759)\n",
      "Epoch: [120][234/390]\tTime 0.045 (0.003)\tLoss 1.0779 (1.0159)\tPrec@1 64.062 (63.973)\n",
      "Epoch: [120][312/390]\tTime 0.002 (0.004)\tLoss 1.2875 (1.0279)\tPrec@1 53.125 (63.354)\n",
      "Epoch: [120][390/390]\tTime 0.002 (0.004)\tLoss 1.0466 (1.0410)\tPrec@1 62.500 (62.834)\n",
      "EPOCH: 120 train Results: Prec@1 62.834 Loss: 1.0410\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0010 (1.0010)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0320 (1.2478)\tPrec@1 56.250 (56.240)\n",
      "EPOCH: 120 val Results: Prec@1 56.240 Loss: 1.2478\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/390]\tTime 0.004 (0.004)\tLoss 0.8149 (0.8149)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [121][78/390]\tTime 0.002 (0.003)\tLoss 1.0520 (0.9612)\tPrec@1 60.938 (65.783)\n",
      "Epoch: [121][156/390]\tTime 0.002 (0.003)\tLoss 1.0134 (0.9966)\tPrec@1 63.281 (64.396)\n",
      "Epoch: [121][234/390]\tTime 0.002 (0.003)\tLoss 1.0476 (1.0121)\tPrec@1 58.594 (63.976)\n",
      "Epoch: [121][312/390]\tTime 0.004 (0.003)\tLoss 0.8982 (1.0299)\tPrec@1 72.656 (63.291)\n",
      "Epoch: [121][390/390]\tTime 0.002 (0.003)\tLoss 0.8254 (1.0419)\tPrec@1 71.250 (62.956)\n",
      "EPOCH: 121 train Results: Prec@1 62.956 Loss: 1.0419\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0666 (1.0666)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2321 (1.2533)\tPrec@1 37.500 (55.430)\n",
      "EPOCH: 121 val Results: Prec@1 55.430 Loss: 1.2533\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/390]\tTime 0.005 (0.005)\tLoss 0.9738 (0.9738)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [122][78/390]\tTime 0.003 (0.003)\tLoss 0.9679 (0.9587)\tPrec@1 65.625 (66.317)\n",
      "Epoch: [122][156/390]\tTime 0.004 (0.003)\tLoss 1.0490 (0.9898)\tPrec@1 66.406 (65.112)\n",
      "Epoch: [122][234/390]\tTime 0.002 (0.003)\tLoss 1.1844 (1.0146)\tPrec@1 60.156 (64.209)\n",
      "Epoch: [122][312/390]\tTime 0.002 (0.003)\tLoss 1.2389 (1.0327)\tPrec@1 55.469 (63.319)\n",
      "Epoch: [122][390/390]\tTime 0.020 (0.003)\tLoss 1.2570 (1.0456)\tPrec@1 60.000 (62.876)\n",
      "EPOCH: 122 train Results: Prec@1 62.876 Loss: 1.0456\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1073 (1.1073)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2241 (1.2696)\tPrec@1 37.500 (55.730)\n",
      "EPOCH: 122 val Results: Prec@1 55.730 Loss: 1.2696\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/390]\tTime 0.006 (0.006)\tLoss 0.8362 (0.8362)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [123][78/390]\tTime 0.002 (0.006)\tLoss 1.0265 (0.9625)\tPrec@1 57.031 (65.496)\n",
      "Epoch: [123][156/390]\tTime 0.009 (0.005)\tLoss 1.1098 (0.9918)\tPrec@1 60.156 (64.202)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.004)\tLoss 1.3122 (1.0198)\tPrec@1 53.906 (63.418)\n",
      "Epoch: [123][312/390]\tTime 0.002 (0.005)\tLoss 1.1775 (1.0357)\tPrec@1 58.594 (62.992)\n",
      "Epoch: [123][390/390]\tTime 0.003 (0.004)\tLoss 1.0744 (1.0457)\tPrec@1 56.250 (62.558)\n",
      "EPOCH: 123 train Results: Prec@1 62.558 Loss: 1.0457\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1335 (1.1335)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2625 (1.2468)\tPrec@1 31.250 (56.130)\n",
      "EPOCH: 123 val Results: Prec@1 56.130 Loss: 1.2468\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/390]\tTime 0.005 (0.005)\tLoss 0.9352 (0.9352)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [124][78/390]\tTime 0.004 (0.004)\tLoss 1.0013 (0.9687)\tPrec@1 61.719 (65.813)\n",
      "Epoch: [124][156/390]\tTime 0.013 (0.003)\tLoss 0.9886 (1.0036)\tPrec@1 63.281 (64.316)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.0894 (1.0214)\tPrec@1 57.031 (63.723)\n",
      "Epoch: [124][312/390]\tTime 0.003 (0.003)\tLoss 1.1819 (1.0314)\tPrec@1 61.719 (63.284)\n",
      "Epoch: [124][390/390]\tTime 0.003 (0.003)\tLoss 1.1828 (1.0461)\tPrec@1 57.500 (62.700)\n",
      "EPOCH: 124 train Results: Prec@1 62.700 Loss: 1.0461\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1242 (1.1242)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5048 (1.2539)\tPrec@1 43.750 (55.380)\n",
      "EPOCH: 124 val Results: Prec@1 55.380 Loss: 1.2539\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/390]\tTime 0.002 (0.002)\tLoss 0.9336 (0.9336)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [125][78/390]\tTime 0.003 (0.003)\tLoss 0.8606 (0.9742)\tPrec@1 71.875 (65.566)\n",
      "Epoch: [125][156/390]\tTime 0.002 (0.003)\tLoss 0.9661 (0.9989)\tPrec@1 65.625 (64.704)\n",
      "Epoch: [125][234/390]\tTime 0.003 (0.003)\tLoss 1.0395 (1.0173)\tPrec@1 60.938 (63.949)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.1245 (1.0322)\tPrec@1 63.281 (63.356)\n",
      "Epoch: [125][390/390]\tTime 0.009 (0.003)\tLoss 0.8532 (1.0448)\tPrec@1 75.000 (62.880)\n",
      "EPOCH: 125 train Results: Prec@1 62.880 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0923 (1.0923)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.1707 (1.2563)\tPrec@1 43.750 (55.810)\n",
      "EPOCH: 125 val Results: Prec@1 55.810 Loss: 1.2563\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/390]\tTime 0.004 (0.004)\tLoss 0.8251 (0.8251)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [126][78/390]\tTime 0.002 (0.003)\tLoss 0.8869 (0.9596)\tPrec@1 69.531 (65.843)\n",
      "Epoch: [126][156/390]\tTime 0.002 (0.003)\tLoss 1.1164 (0.9909)\tPrec@1 64.062 (64.819)\n",
      "Epoch: [126][234/390]\tTime 0.004 (0.003)\tLoss 1.1743 (1.0155)\tPrec@1 60.156 (63.900)\n",
      "Epoch: [126][312/390]\tTime 0.002 (0.003)\tLoss 1.0575 (1.0336)\tPrec@1 62.500 (63.201)\n",
      "Epoch: [126][390/390]\tTime 0.001 (0.003)\tLoss 1.0516 (1.0475)\tPrec@1 63.750 (62.786)\n",
      "EPOCH: 126 train Results: Prec@1 62.786 Loss: 1.0475\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1671 (1.1671)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3018 (1.2702)\tPrec@1 31.250 (55.350)\n",
      "EPOCH: 126 val Results: Prec@1 55.350 Loss: 1.2702\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/390]\tTime 0.002 (0.002)\tLoss 1.1470 (1.1470)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.003)\tLoss 0.8391 (0.9719)\tPrec@1 71.875 (65.773)\n",
      "Epoch: [127][156/390]\tTime 0.003 (0.003)\tLoss 0.9970 (0.9967)\tPrec@1 64.844 (64.794)\n",
      "Epoch: [127][234/390]\tTime 0.003 (0.003)\tLoss 1.3657 (1.0135)\tPrec@1 52.344 (64.345)\n",
      "Epoch: [127][312/390]\tTime 0.002 (0.003)\tLoss 1.2052 (1.0350)\tPrec@1 56.250 (63.354)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.003)\tLoss 0.9823 (1.0467)\tPrec@1 67.500 (62.916)\n",
      "EPOCH: 127 train Results: Prec@1 62.916 Loss: 1.0467\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1420 (1.1420)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3432 (1.2447)\tPrec@1 37.500 (56.460)\n",
      "EPOCH: 127 val Results: Prec@1 56.460 Loss: 1.2447\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/390]\tTime 0.004 (0.004)\tLoss 0.9997 (0.9997)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [128][78/390]\tTime 0.005 (0.003)\tLoss 1.1222 (0.9664)\tPrec@1 63.281 (65.437)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.003)\tLoss 0.9906 (0.9957)\tPrec@1 63.281 (64.476)\n",
      "Epoch: [128][234/390]\tTime 0.003 (0.003)\tLoss 1.0852 (1.0107)\tPrec@1 67.188 (64.033)\n",
      "Epoch: [128][312/390]\tTime 0.002 (0.003)\tLoss 1.0654 (1.0281)\tPrec@1 62.500 (63.406)\n",
      "Epoch: [128][390/390]\tTime 0.001 (0.003)\tLoss 1.2253 (1.0422)\tPrec@1 61.250 (62.930)\n",
      "EPOCH: 128 train Results: Prec@1 62.930 Loss: 1.0422\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0613 (1.0613)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1367 (1.2634)\tPrec@1 50.000 (55.410)\n",
      "EPOCH: 128 val Results: Prec@1 55.410 Loss: 1.2634\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/390]\tTime 0.003 (0.003)\tLoss 1.0806 (1.0806)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 0.8898 (0.9509)\tPrec@1 71.094 (66.367)\n",
      "Epoch: [129][156/390]\tTime 0.005 (0.003)\tLoss 0.9814 (0.9820)\tPrec@1 62.500 (65.043)\n",
      "Epoch: [129][234/390]\tTime 0.003 (0.003)\tLoss 1.2676 (1.0111)\tPrec@1 52.344 (64.036)\n",
      "Epoch: [129][312/390]\tTime 0.007 (0.003)\tLoss 0.9256 (1.0284)\tPrec@1 62.500 (63.256)\n",
      "Epoch: [129][390/390]\tTime 0.006 (0.003)\tLoss 0.9818 (1.0432)\tPrec@1 61.250 (62.686)\n",
      "EPOCH: 129 train Results: Prec@1 62.686 Loss: 1.0432\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1689 (1.1689)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2535 (1.2553)\tPrec@1 50.000 (55.600)\n",
      "EPOCH: 129 val Results: Prec@1 55.600 Loss: 1.2553\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/390]\tTime 0.005 (0.005)\tLoss 0.8594 (0.8594)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [130][78/390]\tTime 0.002 (0.003)\tLoss 0.9041 (0.9534)\tPrec@1 64.844 (66.297)\n",
      "Epoch: [130][156/390]\tTime 0.007 (0.003)\tLoss 1.1269 (0.9805)\tPrec@1 61.719 (65.351)\n",
      "Epoch: [130][234/390]\tTime 0.004 (0.003)\tLoss 1.2835 (1.0102)\tPrec@1 59.375 (64.279)\n",
      "Epoch: [130][312/390]\tTime 0.005 (0.003)\tLoss 1.0876 (1.0296)\tPrec@1 59.375 (63.491)\n",
      "Epoch: [130][390/390]\tTime 0.002 (0.003)\tLoss 1.1877 (1.0431)\tPrec@1 58.750 (63.012)\n",
      "EPOCH: 130 train Results: Prec@1 63.012 Loss: 1.0431\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1415 (1.1415)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2079 (1.2564)\tPrec@1 50.000 (55.260)\n",
      "EPOCH: 130 val Results: Prec@1 55.260 Loss: 1.2564\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/390]\tTime 0.003 (0.003)\tLoss 0.8249 (0.8249)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [131][78/390]\tTime 0.002 (0.003)\tLoss 0.9226 (0.9712)\tPrec@1 69.531 (65.625)\n",
      "Epoch: [131][156/390]\tTime 0.003 (0.003)\tLoss 1.1400 (0.9979)\tPrec@1 55.469 (64.346)\n",
      "Epoch: [131][234/390]\tTime 0.003 (0.004)\tLoss 1.1151 (1.0174)\tPrec@1 59.375 (63.590)\n",
      "Epoch: [131][312/390]\tTime 0.004 (0.004)\tLoss 1.2877 (1.0350)\tPrec@1 53.906 (62.919)\n",
      "Epoch: [131][390/390]\tTime 0.006 (0.004)\tLoss 1.0121 (1.0483)\tPrec@1 58.750 (62.502)\n",
      "EPOCH: 131 train Results: Prec@1 62.502 Loss: 1.0483\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0985 (1.0985)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3842 (1.2578)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 131 val Results: Prec@1 55.630 Loss: 1.2578\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/390]\tTime 0.006 (0.006)\tLoss 0.9006 (0.9006)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [132][78/390]\tTime 0.003 (0.004)\tLoss 0.9782 (0.9633)\tPrec@1 70.312 (65.892)\n",
      "Epoch: [132][156/390]\tTime 0.002 (0.004)\tLoss 1.1256 (1.0025)\tPrec@1 64.844 (64.401)\n",
      "Epoch: [132][234/390]\tTime 0.003 (0.003)\tLoss 0.9438 (1.0249)\tPrec@1 68.750 (63.657)\n",
      "Epoch: [132][312/390]\tTime 0.003 (0.003)\tLoss 1.3169 (1.0375)\tPrec@1 53.906 (63.144)\n",
      "Epoch: [132][390/390]\tTime 0.001 (0.003)\tLoss 1.1137 (1.0466)\tPrec@1 66.250 (62.684)\n",
      "EPOCH: 132 train Results: Prec@1 62.684 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0834 (1.0834)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0564 (1.2543)\tPrec@1 37.500 (55.450)\n",
      "EPOCH: 132 val Results: Prec@1 55.450 Loss: 1.2543\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/390]\tTime 0.005 (0.005)\tLoss 0.8773 (0.8773)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [133][78/390]\tTime 0.003 (0.003)\tLoss 0.8910 (0.9653)\tPrec@1 62.500 (66.070)\n",
      "Epoch: [133][156/390]\tTime 0.002 (0.003)\tLoss 0.9027 (0.9879)\tPrec@1 67.969 (65.013)\n",
      "Epoch: [133][234/390]\tTime 0.005 (0.003)\tLoss 1.0643 (1.0107)\tPrec@1 60.938 (64.129)\n",
      "Epoch: [133][312/390]\tTime 0.003 (0.003)\tLoss 1.2222 (1.0328)\tPrec@1 61.719 (63.344)\n",
      "Epoch: [133][390/390]\tTime 0.008 (0.003)\tLoss 1.0638 (1.0448)\tPrec@1 60.000 (62.896)\n",
      "EPOCH: 133 train Results: Prec@1 62.896 Loss: 1.0448\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1949 (1.1949)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3216 (1.2540)\tPrec@1 43.750 (55.380)\n",
      "EPOCH: 133 val Results: Prec@1 55.380 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/390]\tTime 0.002 (0.002)\tLoss 0.9230 (0.9230)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [134][78/390]\tTime 0.003 (0.003)\tLoss 0.9859 (0.9703)\tPrec@1 67.969 (65.724)\n",
      "Epoch: [134][156/390]\tTime 0.003 (0.003)\tLoss 1.0846 (1.0032)\tPrec@1 64.844 (64.466)\n",
      "Epoch: [134][234/390]\tTime 0.011 (0.003)\tLoss 1.1020 (1.0200)\tPrec@1 62.500 (63.747)\n",
      "Epoch: [134][312/390]\tTime 0.004 (0.003)\tLoss 1.1761 (1.0334)\tPrec@1 55.469 (63.181)\n",
      "Epoch: [134][390/390]\tTime 0.003 (0.003)\tLoss 0.9589 (1.0449)\tPrec@1 62.500 (62.738)\n",
      "EPOCH: 134 train Results: Prec@1 62.738 Loss: 1.0449\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1199 (1.1199)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1088 (1.2456)\tPrec@1 43.750 (56.060)\n",
      "EPOCH: 134 val Results: Prec@1 56.060 Loss: 1.2456\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/390]\tTime 0.007 (0.007)\tLoss 0.9780 (0.9780)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [135][78/390]\tTime 0.003 (0.003)\tLoss 0.9668 (0.9591)\tPrec@1 60.938 (66.169)\n",
      "Epoch: [135][156/390]\tTime 0.003 (0.004)\tLoss 0.9644 (0.9900)\tPrec@1 68.750 (64.918)\n",
      "Epoch: [135][234/390]\tTime 0.003 (0.003)\tLoss 1.1231 (1.0134)\tPrec@1 60.938 (63.986)\n",
      "Epoch: [135][312/390]\tTime 0.002 (0.003)\tLoss 0.9859 (1.0338)\tPrec@1 65.625 (63.244)\n",
      "Epoch: [135][390/390]\tTime 0.011 (0.003)\tLoss 1.2863 (1.0495)\tPrec@1 56.250 (62.644)\n",
      "EPOCH: 135 train Results: Prec@1 62.644 Loss: 1.0495\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1453 (1.1453)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2387 (1.2515)\tPrec@1 56.250 (55.600)\n",
      "EPOCH: 135 val Results: Prec@1 55.600 Loss: 1.2515\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/390]\tTime 0.004 (0.004)\tLoss 0.8147 (0.8147)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [136][78/390]\tTime 0.002 (0.003)\tLoss 1.1504 (0.9609)\tPrec@1 58.594 (66.090)\n",
      "Epoch: [136][156/390]\tTime 0.003 (0.003)\tLoss 0.9398 (0.9911)\tPrec@1 65.625 (64.998)\n",
      "Epoch: [136][234/390]\tTime 0.005 (0.003)\tLoss 0.9710 (1.0157)\tPrec@1 68.750 (64.059)\n",
      "Epoch: [136][312/390]\tTime 0.005 (0.003)\tLoss 1.0954 (1.0301)\tPrec@1 57.031 (63.451)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.1552 (1.0433)\tPrec@1 52.500 (62.794)\n",
      "EPOCH: 136 train Results: Prec@1 62.794 Loss: 1.0433\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1034 (1.1034)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3191 (1.2559)\tPrec@1 50.000 (55.370)\n",
      "EPOCH: 136 val Results: Prec@1 55.370 Loss: 1.2559\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/390]\tTime 0.003 (0.003)\tLoss 0.9621 (0.9621)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [137][78/390]\tTime 0.002 (0.003)\tLoss 1.0570 (0.9898)\tPrec@1 55.469 (65.309)\n",
      "Epoch: [137][156/390]\tTime 0.002 (0.003)\tLoss 0.9577 (1.0047)\tPrec@1 72.656 (64.426)\n",
      "Epoch: [137][234/390]\tTime 0.003 (0.003)\tLoss 0.9403 (1.0140)\tPrec@1 64.062 (63.890)\n",
      "Epoch: [137][312/390]\tTime 0.002 (0.003)\tLoss 1.1225 (1.0299)\tPrec@1 60.938 (63.326)\n",
      "Epoch: [137][390/390]\tTime 0.002 (0.003)\tLoss 1.2078 (1.0455)\tPrec@1 55.000 (62.838)\n",
      "EPOCH: 137 train Results: Prec@1 62.838 Loss: 1.0455\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0365 (1.0365)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1414 (1.2563)\tPrec@1 50.000 (55.760)\n",
      "EPOCH: 137 val Results: Prec@1 55.760 Loss: 1.2563\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/390]\tTime 0.005 (0.005)\tLoss 1.0435 (1.0435)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [138][78/390]\tTime 0.003 (0.003)\tLoss 0.9471 (0.9741)\tPrec@1 65.625 (65.417)\n",
      "Epoch: [138][156/390]\tTime 0.015 (0.003)\tLoss 0.9413 (1.0032)\tPrec@1 67.969 (64.276)\n",
      "Epoch: [138][234/390]\tTime 0.004 (0.003)\tLoss 1.1344 (1.0196)\tPrec@1 60.938 (63.896)\n",
      "Epoch: [138][312/390]\tTime 0.012 (0.003)\tLoss 1.1732 (1.0360)\tPrec@1 57.812 (63.176)\n",
      "Epoch: [138][390/390]\tTime 0.001 (0.003)\tLoss 1.0621 (1.0490)\tPrec@1 60.000 (62.674)\n",
      "EPOCH: 138 train Results: Prec@1 62.674 Loss: 1.0490\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0948 (1.0948)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2067 (1.2515)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 138 val Results: Prec@1 55.370 Loss: 1.2515\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/390]\tTime 0.002 (0.002)\tLoss 0.9109 (0.9109)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [139][78/390]\tTime 0.002 (0.004)\tLoss 0.9646 (0.9668)\tPrec@1 67.969 (65.773)\n",
      "Epoch: [139][156/390]\tTime 0.002 (0.004)\tLoss 0.9542 (0.9938)\tPrec@1 67.969 (65.117)\n",
      "Epoch: [139][234/390]\tTime 0.002 (0.003)\tLoss 1.1142 (1.0165)\tPrec@1 65.625 (64.249)\n",
      "Epoch: [139][312/390]\tTime 0.003 (0.003)\tLoss 1.0781 (1.0313)\tPrec@1 60.156 (63.633)\n",
      "Epoch: [139][390/390]\tTime 0.004 (0.003)\tLoss 1.1061 (1.0440)\tPrec@1 67.500 (63.142)\n",
      "EPOCH: 139 train Results: Prec@1 63.142 Loss: 1.0440\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1426 (1.1426)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1866 (1.2560)\tPrec@1 50.000 (55.410)\n",
      "EPOCH: 139 val Results: Prec@1 55.410 Loss: 1.2560\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/390]\tTime 0.002 (0.002)\tLoss 0.9686 (0.9686)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [140][78/390]\tTime 0.003 (0.003)\tLoss 1.0099 (0.9689)\tPrec@1 57.812 (65.892)\n",
      "Epoch: [140][156/390]\tTime 0.007 (0.003)\tLoss 1.0686 (0.9939)\tPrec@1 58.594 (64.849)\n",
      "Epoch: [140][234/390]\tTime 0.002 (0.003)\tLoss 1.0380 (1.0122)\tPrec@1 63.281 (64.166)\n",
      "Epoch: [140][312/390]\tTime 0.003 (0.003)\tLoss 1.1139 (1.0251)\tPrec@1 58.594 (63.628)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.003)\tLoss 1.2424 (1.0417)\tPrec@1 55.000 (62.974)\n",
      "EPOCH: 140 train Results: Prec@1 62.974 Loss: 1.0417\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1572 (1.1572)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4207 (1.2619)\tPrec@1 37.500 (55.010)\n",
      "EPOCH: 140 val Results: Prec@1 55.010 Loss: 1.2619\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/390]\tTime 0.002 (0.002)\tLoss 0.7216 (0.7216)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [141][78/390]\tTime 0.002 (0.003)\tLoss 1.0543 (0.9561)\tPrec@1 66.406 (66.070)\n",
      "Epoch: [141][156/390]\tTime 0.004 (0.003)\tLoss 1.1572 (1.0004)\tPrec@1 54.688 (64.381)\n",
      "Epoch: [141][234/390]\tTime 0.003 (0.003)\tLoss 1.0562 (1.0214)\tPrec@1 62.500 (63.561)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.003)\tLoss 1.1364 (1.0376)\tPrec@1 64.062 (62.897)\n",
      "Epoch: [141][390/390]\tTime 0.003 (0.003)\tLoss 1.1157 (1.0468)\tPrec@1 60.000 (62.634)\n",
      "EPOCH: 141 train Results: Prec@1 62.634 Loss: 1.0468\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1252 (1.1252)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3861 (1.2561)\tPrec@1 56.250 (55.230)\n",
      "EPOCH: 141 val Results: Prec@1 55.230 Loss: 1.2561\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/390]\tTime 0.004 (0.004)\tLoss 0.9118 (0.9118)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [142][78/390]\tTime 0.002 (0.003)\tLoss 0.9621 (0.9564)\tPrec@1 64.844 (66.466)\n",
      "Epoch: [142][156/390]\tTime 0.003 (0.003)\tLoss 0.9968 (0.9877)\tPrec@1 60.938 (64.689)\n",
      "Epoch: [142][234/390]\tTime 0.006 (0.003)\tLoss 1.1511 (1.0084)\tPrec@1 63.281 (63.966)\n",
      "Epoch: [142][312/390]\tTime 0.008 (0.003)\tLoss 1.0986 (1.0274)\tPrec@1 62.500 (63.431)\n",
      "Epoch: [142][390/390]\tTime 0.001 (0.003)\tLoss 1.2483 (1.0449)\tPrec@1 52.500 (62.702)\n",
      "EPOCH: 142 train Results: Prec@1 62.702 Loss: 1.0449\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1646 (1.1646)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.2183 (1.2665)\tPrec@1 56.250 (55.030)\n",
      "EPOCH: 142 val Results: Prec@1 55.030 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/390]\tTime 0.003 (0.003)\tLoss 0.9452 (0.9452)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [143][78/390]\tTime 0.002 (0.003)\tLoss 0.9839 (0.9681)\tPrec@1 72.656 (65.763)\n",
      "Epoch: [143][156/390]\tTime 0.002 (0.003)\tLoss 0.9165 (0.9951)\tPrec@1 65.625 (64.675)\n",
      "Epoch: [143][234/390]\tTime 0.004 (0.003)\tLoss 1.0934 (1.0143)\tPrec@1 65.625 (63.893)\n",
      "Epoch: [143][312/390]\tTime 0.005 (0.003)\tLoss 1.1003 (1.0295)\tPrec@1 59.375 (63.339)\n",
      "Epoch: [143][390/390]\tTime 0.008 (0.003)\tLoss 0.9992 (1.0423)\tPrec@1 63.750 (62.804)\n",
      "EPOCH: 143 train Results: Prec@1 62.804 Loss: 1.0423\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1058 (1.1058)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0594 (1.2537)\tPrec@1 62.500 (55.590)\n",
      "EPOCH: 143 val Results: Prec@1 55.590 Loss: 1.2537\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/390]\tTime 0.002 (0.002)\tLoss 0.8698 (0.8698)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [144][78/390]\tTime 0.008 (0.003)\tLoss 1.0005 (0.9737)\tPrec@1 64.062 (65.546)\n",
      "Epoch: [144][156/390]\tTime 0.003 (0.003)\tLoss 1.2666 (0.9986)\tPrec@1 58.594 (64.500)\n",
      "Epoch: [144][234/390]\tTime 0.005 (0.003)\tLoss 1.1271 (1.0187)\tPrec@1 60.938 (63.906)\n",
      "Epoch: [144][312/390]\tTime 0.002 (0.003)\tLoss 1.0188 (1.0289)\tPrec@1 60.938 (63.496)\n",
      "Epoch: [144][390/390]\tTime 0.004 (0.003)\tLoss 0.9962 (1.0391)\tPrec@1 60.000 (63.096)\n",
      "EPOCH: 144 train Results: Prec@1 63.096 Loss: 1.0391\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0925 (1.0925)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3413 (1.2509)\tPrec@1 37.500 (56.040)\n",
      "EPOCH: 144 val Results: Prec@1 56.040 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/390]\tTime 0.004 (0.004)\tLoss 0.9920 (0.9920)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [145][78/390]\tTime 0.003 (0.004)\tLoss 0.9096 (0.9706)\tPrec@1 69.531 (65.358)\n",
      "Epoch: [145][156/390]\tTime 0.002 (0.004)\tLoss 1.0535 (0.9841)\tPrec@1 64.062 (65.068)\n",
      "Epoch: [145][234/390]\tTime 0.010 (0.004)\tLoss 0.9857 (1.0115)\tPrec@1 66.406 (64.029)\n",
      "Epoch: [145][312/390]\tTime 0.002 (0.004)\tLoss 1.2132 (1.0310)\tPrec@1 60.938 (63.249)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.004)\tLoss 1.1423 (1.0425)\tPrec@1 57.500 (62.912)\n",
      "EPOCH: 145 train Results: Prec@1 62.912 Loss: 1.0425\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0954 (1.0954)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3588 (1.2584)\tPrec@1 43.750 (55.740)\n",
      "EPOCH: 145 val Results: Prec@1 55.740 Loss: 1.2584\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/390]\tTime 0.004 (0.004)\tLoss 1.0094 (1.0094)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [146][78/390]\tTime 0.003 (0.005)\tLoss 0.9580 (0.9682)\tPrec@1 65.625 (65.180)\n",
      "Epoch: [146][156/390]\tTime 0.005 (0.004)\tLoss 1.0319 (0.9979)\tPrec@1 62.500 (64.167)\n",
      "Epoch: [146][234/390]\tTime 0.002 (0.004)\tLoss 0.8903 (1.0146)\tPrec@1 71.094 (63.630)\n",
      "Epoch: [146][312/390]\tTime 0.003 (0.004)\tLoss 0.9520 (1.0332)\tPrec@1 68.750 (63.109)\n",
      "Epoch: [146][390/390]\tTime 0.002 (0.004)\tLoss 0.9097 (1.0450)\tPrec@1 66.250 (62.646)\n",
      "EPOCH: 146 train Results: Prec@1 62.646 Loss: 1.0450\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0285 (1.0285)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1653 (1.2540)\tPrec@1 50.000 (55.100)\n",
      "EPOCH: 146 val Results: Prec@1 55.100 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/390]\tTime 0.003 (0.003)\tLoss 0.8659 (0.8659)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [147][78/390]\tTime 0.003 (0.003)\tLoss 0.9897 (0.9682)\tPrec@1 64.062 (65.318)\n",
      "Epoch: [147][156/390]\tTime 0.002 (0.003)\tLoss 1.1112 (0.9906)\tPrec@1 62.500 (64.446)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.003)\tLoss 1.2271 (1.0086)\tPrec@1 57.031 (63.933)\n",
      "Epoch: [147][312/390]\tTime 0.006 (0.003)\tLoss 1.1038 (1.0258)\tPrec@1 62.500 (63.286)\n",
      "Epoch: [147][390/390]\tTime 0.002 (0.003)\tLoss 1.0875 (1.0383)\tPrec@1 60.000 (62.922)\n",
      "EPOCH: 147 train Results: Prec@1 62.922 Loss: 1.0383\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1317 (1.1317)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6665 (1.2587)\tPrec@1 25.000 (55.460)\n",
      "EPOCH: 147 val Results: Prec@1 55.460 Loss: 1.2587\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/390]\tTime 0.003 (0.003)\tLoss 1.0457 (1.0457)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [148][78/390]\tTime 0.003 (0.004)\tLoss 1.0713 (0.9785)\tPrec@1 61.719 (65.239)\n",
      "Epoch: [148][156/390]\tTime 0.004 (0.004)\tLoss 1.1139 (1.0046)\tPrec@1 62.500 (64.202)\n",
      "Epoch: [148][234/390]\tTime 0.002 (0.004)\tLoss 1.0792 (1.0243)\tPrec@1 59.375 (63.524)\n",
      "Epoch: [148][312/390]\tTime 0.003 (0.004)\tLoss 0.9690 (1.0360)\tPrec@1 67.188 (63.194)\n",
      "Epoch: [148][390/390]\tTime 0.001 (0.004)\tLoss 0.9833 (1.0474)\tPrec@1 62.500 (62.804)\n",
      "EPOCH: 148 train Results: Prec@1 62.804 Loss: 1.0474\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1394 (1.1394)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5144 (1.2580)\tPrec@1 37.500 (55.710)\n",
      "EPOCH: 148 val Results: Prec@1 55.710 Loss: 1.2580\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/390]\tTime 0.010 (0.010)\tLoss 0.8219 (0.8219)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [149][78/390]\tTime 0.003 (0.003)\tLoss 1.0457 (0.9596)\tPrec@1 61.719 (66.199)\n",
      "Epoch: [149][156/390]\tTime 0.003 (0.003)\tLoss 1.0027 (0.9873)\tPrec@1 63.281 (65.098)\n",
      "Epoch: [149][234/390]\tTime 0.002 (0.003)\tLoss 0.9663 (1.0108)\tPrec@1 64.062 (64.295)\n",
      "Epoch: [149][312/390]\tTime 0.004 (0.003)\tLoss 1.1420 (1.0235)\tPrec@1 57.031 (63.820)\n",
      "Epoch: [149][390/390]\tTime 0.002 (0.003)\tLoss 1.0933 (1.0401)\tPrec@1 60.000 (63.184)\n",
      "EPOCH: 149 train Results: Prec@1 63.184 Loss: 1.0401\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0873 (1.0873)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3228 (1.2584)\tPrec@1 50.000 (55.700)\n",
      "EPOCH: 149 val Results: Prec@1 55.700 Loss: 1.2584\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/390]\tTime 0.002 (0.002)\tLoss 0.9870 (0.9870)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [150][78/390]\tTime 0.003 (0.003)\tLoss 1.1757 (0.9565)\tPrec@1 60.938 (66.317)\n",
      "Epoch: [150][156/390]\tTime 0.002 (0.003)\tLoss 1.0894 (0.9821)\tPrec@1 63.281 (65.272)\n",
      "Epoch: [150][234/390]\tTime 0.004 (0.003)\tLoss 1.2217 (1.0191)\tPrec@1 63.281 (63.677)\n",
      "Epoch: [150][312/390]\tTime 0.004 (0.003)\tLoss 1.0654 (1.0349)\tPrec@1 59.375 (63.042)\n",
      "Epoch: [150][390/390]\tTime 0.001 (0.003)\tLoss 1.0127 (1.0466)\tPrec@1 66.250 (62.584)\n",
      "EPOCH: 150 train Results: Prec@1 62.584 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1446 (1.1446)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1663 (1.2527)\tPrec@1 50.000 (55.580)\n",
      "EPOCH: 150 val Results: Prec@1 55.580 Loss: 1.2527\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/390]\tTime 0.004 (0.004)\tLoss 0.9232 (0.9232)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [151][78/390]\tTime 0.002 (0.003)\tLoss 1.1241 (0.9616)\tPrec@1 60.938 (65.823)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.003)\tLoss 1.0124 (0.9863)\tPrec@1 63.281 (64.809)\n",
      "Epoch: [151][234/390]\tTime 0.002 (0.003)\tLoss 1.1055 (1.0094)\tPrec@1 60.938 (63.979)\n",
      "Epoch: [151][312/390]\tTime 0.004 (0.003)\tLoss 1.1386 (1.0230)\tPrec@1 59.375 (63.366)\n",
      "Epoch: [151][390/390]\tTime 0.003 (0.003)\tLoss 1.1851 (1.0416)\tPrec@1 58.750 (62.646)\n",
      "EPOCH: 151 train Results: Prec@1 62.646 Loss: 1.0416\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1213 (1.1213)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1523 (1.2588)\tPrec@1 56.250 (55.730)\n",
      "EPOCH: 151 val Results: Prec@1 55.730 Loss: 1.2588\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/390]\tTime 0.002 (0.002)\tLoss 0.7736 (0.7736)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [152][78/390]\tTime 0.002 (0.003)\tLoss 1.0548 (0.9701)\tPrec@1 60.156 (65.882)\n",
      "Epoch: [152][156/390]\tTime 0.005 (0.003)\tLoss 1.0168 (0.9966)\tPrec@1 60.156 (64.759)\n",
      "Epoch: [152][234/390]\tTime 0.003 (0.003)\tLoss 1.1780 (1.0149)\tPrec@1 57.812 (63.953)\n",
      "Epoch: [152][312/390]\tTime 0.007 (0.003)\tLoss 1.0821 (1.0279)\tPrec@1 59.375 (63.531)\n",
      "Epoch: [152][390/390]\tTime 0.003 (0.003)\tLoss 0.9581 (1.0403)\tPrec@1 62.500 (63.074)\n",
      "EPOCH: 152 train Results: Prec@1 63.074 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1255 (1.1255)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3912 (1.2665)\tPrec@1 31.250 (54.960)\n",
      "EPOCH: 152 val Results: Prec@1 54.960 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/390]\tTime 0.007 (0.007)\tLoss 0.7744 (0.7744)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [153][78/390]\tTime 0.002 (0.003)\tLoss 1.0226 (0.9600)\tPrec@1 61.719 (65.684)\n",
      "Epoch: [153][156/390]\tTime 0.002 (0.003)\tLoss 0.9712 (0.9910)\tPrec@1 67.969 (64.515)\n",
      "Epoch: [153][234/390]\tTime 0.003 (0.004)\tLoss 1.0107 (1.0126)\tPrec@1 64.844 (63.850)\n",
      "Epoch: [153][312/390]\tTime 0.004 (0.004)\tLoss 1.0050 (1.0290)\tPrec@1 62.500 (63.314)\n",
      "Epoch: [153][390/390]\tTime 0.003 (0.004)\tLoss 1.1472 (1.0391)\tPrec@1 61.250 (62.974)\n",
      "EPOCH: 153 train Results: Prec@1 62.974 Loss: 1.0391\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1421 (1.1421)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4642 (1.2509)\tPrec@1 31.250 (55.630)\n",
      "EPOCH: 153 val Results: Prec@1 55.630 Loss: 1.2509\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/390]\tTime 0.007 (0.007)\tLoss 0.8329 (0.8329)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [154][78/390]\tTime 0.004 (0.003)\tLoss 0.8981 (0.9786)\tPrec@1 67.188 (64.972)\n",
      "Epoch: [154][156/390]\tTime 0.002 (0.003)\tLoss 1.1282 (0.9967)\tPrec@1 57.812 (64.276)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.003)\tLoss 1.2091 (1.0173)\tPrec@1 57.031 (63.577)\n",
      "Epoch: [154][312/390]\tTime 0.002 (0.003)\tLoss 1.1627 (1.0389)\tPrec@1 51.562 (62.869)\n",
      "Epoch: [154][390/390]\tTime 0.006 (0.003)\tLoss 1.1018 (1.0486)\tPrec@1 60.000 (62.574)\n",
      "EPOCH: 154 train Results: Prec@1 62.574 Loss: 1.0486\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2229 (1.2229)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5655 (1.2557)\tPrec@1 37.500 (55.640)\n",
      "EPOCH: 154 val Results: Prec@1 55.640 Loss: 1.2557\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/390]\tTime 0.002 (0.002)\tLoss 0.9959 (0.9959)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [155][78/390]\tTime 0.002 (0.003)\tLoss 0.9013 (0.9648)\tPrec@1 66.406 (65.605)\n",
      "Epoch: [155][156/390]\tTime 0.003 (0.003)\tLoss 0.9608 (0.9879)\tPrec@1 66.406 (64.615)\n",
      "Epoch: [155][234/390]\tTime 0.002 (0.003)\tLoss 1.1423 (1.0052)\tPrec@1 57.812 (64.059)\n",
      "Epoch: [155][312/390]\tTime 0.002 (0.003)\tLoss 1.2328 (1.0259)\tPrec@1 55.469 (63.386)\n",
      "Epoch: [155][390/390]\tTime 0.001 (0.003)\tLoss 0.9871 (1.0434)\tPrec@1 66.250 (62.756)\n",
      "EPOCH: 155 train Results: Prec@1 62.756 Loss: 1.0434\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1525 (1.1525)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3506 (1.2752)\tPrec@1 37.500 (54.860)\n",
      "EPOCH: 155 val Results: Prec@1 54.860 Loss: 1.2752\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/390]\tTime 0.003 (0.003)\tLoss 0.9833 (0.9833)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [156][78/390]\tTime 0.002 (0.003)\tLoss 0.9597 (0.9653)\tPrec@1 65.625 (64.745)\n",
      "Epoch: [156][156/390]\tTime 0.002 (0.003)\tLoss 1.2532 (0.9929)\tPrec@1 57.812 (64.117)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.003)\tLoss 1.0005 (1.0143)\tPrec@1 64.844 (63.418)\n",
      "Epoch: [156][312/390]\tTime 0.002 (0.003)\tLoss 1.1343 (1.0348)\tPrec@1 60.156 (62.725)\n",
      "Epoch: [156][390/390]\tTime 0.010 (0.003)\tLoss 1.0664 (1.0467)\tPrec@1 58.750 (62.420)\n",
      "EPOCH: 156 train Results: Prec@1 62.420 Loss: 1.0467\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1723 (1.1723)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5726 (1.2541)\tPrec@1 50.000 (55.360)\n",
      "EPOCH: 156 val Results: Prec@1 55.360 Loss: 1.2541\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/390]\tTime 0.003 (0.003)\tLoss 1.1631 (1.1631)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [157][78/390]\tTime 0.008 (0.003)\tLoss 0.9838 (0.9671)\tPrec@1 59.375 (65.912)\n",
      "Epoch: [157][156/390]\tTime 0.002 (0.003)\tLoss 0.8294 (0.9881)\tPrec@1 67.969 (64.988)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.0183 (1.0123)\tPrec@1 62.500 (63.833)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.004)\tLoss 1.0516 (1.0275)\tPrec@1 64.844 (63.219)\n",
      "Epoch: [157][390/390]\tTime 0.008 (0.003)\tLoss 1.1160 (1.0368)\tPrec@1 60.000 (62.976)\n",
      "EPOCH: 157 train Results: Prec@1 62.976 Loss: 1.0368\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1468 (1.1468)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3970 (1.2591)\tPrec@1 43.750 (55.360)\n",
      "EPOCH: 157 val Results: Prec@1 55.360 Loss: 1.2591\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/390]\tTime 0.005 (0.005)\tLoss 0.9229 (0.9229)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [158][78/390]\tTime 0.004 (0.003)\tLoss 1.0895 (0.9698)\tPrec@1 61.719 (65.328)\n",
      "Epoch: [158][156/390]\tTime 0.003 (0.004)\tLoss 1.1152 (0.9942)\tPrec@1 66.406 (64.655)\n",
      "Epoch: [158][234/390]\tTime 0.002 (0.004)\tLoss 0.9208 (1.0167)\tPrec@1 68.750 (63.820)\n",
      "Epoch: [158][312/390]\tTime 0.002 (0.003)\tLoss 1.1618 (1.0328)\tPrec@1 57.812 (63.166)\n",
      "Epoch: [158][390/390]\tTime 0.001 (0.003)\tLoss 1.1040 (1.0457)\tPrec@1 62.500 (62.790)\n",
      "EPOCH: 158 train Results: Prec@1 62.790 Loss: 1.0457\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0743 (1.0743)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0856 (1.2540)\tPrec@1 68.750 (55.420)\n",
      "EPOCH: 158 val Results: Prec@1 55.420 Loss: 1.2540\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/390]\tTime 0.004 (0.004)\tLoss 0.8832 (0.8832)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [159][78/390]\tTime 0.002 (0.003)\tLoss 1.0089 (0.9702)\tPrec@1 61.719 (65.536)\n",
      "Epoch: [159][156/390]\tTime 0.003 (0.003)\tLoss 0.9773 (0.9899)\tPrec@1 67.188 (64.431)\n",
      "Epoch: [159][234/390]\tTime 0.002 (0.003)\tLoss 0.9889 (1.0100)\tPrec@1 69.531 (63.797)\n",
      "Epoch: [159][312/390]\tTime 0.002 (0.003)\tLoss 1.0462 (1.0263)\tPrec@1 61.719 (63.306)\n",
      "Epoch: [159][390/390]\tTime 0.056 (0.003)\tLoss 1.1535 (1.0385)\tPrec@1 62.500 (62.852)\n",
      "EPOCH: 159 train Results: Prec@1 62.852 Loss: 1.0385\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1455 (1.1455)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.6612 (1.2542)\tPrec@1 37.500 (55.410)\n",
      "EPOCH: 159 val Results: Prec@1 55.410 Loss: 1.2542\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/390]\tTime 0.002 (0.002)\tLoss 0.8923 (0.8923)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [160][78/390]\tTime 0.002 (0.003)\tLoss 1.0480 (0.9943)\tPrec@1 62.500 (64.814)\n",
      "Epoch: [160][156/390]\tTime 0.006 (0.003)\tLoss 0.9889 (1.0003)\tPrec@1 67.188 (64.585)\n",
      "Epoch: [160][234/390]\tTime 0.005 (0.003)\tLoss 0.9305 (1.0150)\tPrec@1 63.281 (64.023)\n",
      "Epoch: [160][312/390]\tTime 0.003 (0.003)\tLoss 0.9514 (1.0298)\tPrec@1 60.156 (63.548)\n",
      "Epoch: [160][390/390]\tTime 0.002 (0.003)\tLoss 0.9708 (1.0347)\tPrec@1 66.250 (63.348)\n",
      "EPOCH: 160 train Results: Prec@1 63.348 Loss: 1.0347\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0861 (1.0861)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.0772 (1.2633)\tPrec@1 56.250 (55.640)\n",
      "EPOCH: 160 val Results: Prec@1 55.640 Loss: 1.2633\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/390]\tTime 0.003 (0.003)\tLoss 0.9541 (0.9541)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [161][78/390]\tTime 0.002 (0.003)\tLoss 0.9255 (0.9601)\tPrec@1 67.969 (65.546)\n",
      "Epoch: [161][156/390]\tTime 0.002 (0.003)\tLoss 1.0636 (0.9898)\tPrec@1 61.719 (64.605)\n",
      "Epoch: [161][234/390]\tTime 0.003 (0.003)\tLoss 1.0757 (1.0124)\tPrec@1 57.812 (63.763)\n",
      "Epoch: [161][312/390]\tTime 0.002 (0.003)\tLoss 1.0445 (1.0275)\tPrec@1 63.281 (63.274)\n",
      "Epoch: [161][390/390]\tTime 0.001 (0.003)\tLoss 1.3551 (1.0414)\tPrec@1 51.250 (62.740)\n",
      "EPOCH: 161 train Results: Prec@1 62.740 Loss: 1.0414\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1011 (1.1011)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9777 (1.2394)\tPrec@1 56.250 (56.190)\n",
      "EPOCH: 161 val Results: Prec@1 56.190 Loss: 1.2394\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.8686 (0.8686)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [162][78/390]\tTime 0.002 (0.003)\tLoss 1.1031 (0.9683)\tPrec@1 64.062 (65.566)\n",
      "Epoch: [162][156/390]\tTime 0.004 (0.003)\tLoss 0.9650 (0.9976)\tPrec@1 64.844 (64.426)\n",
      "Epoch: [162][234/390]\tTime 0.010 (0.003)\tLoss 1.0176 (1.0142)\tPrec@1 61.719 (63.797)\n",
      "Epoch: [162][312/390]\tTime 0.005 (0.003)\tLoss 0.9812 (1.0233)\tPrec@1 68.750 (63.406)\n",
      "Epoch: [162][390/390]\tTime 0.001 (0.003)\tLoss 1.0465 (1.0380)\tPrec@1 61.250 (62.884)\n",
      "EPOCH: 162 train Results: Prec@1 62.884 Loss: 1.0380\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1441 (1.1441)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.5393 (1.2560)\tPrec@1 37.500 (55.840)\n",
      "EPOCH: 162 val Results: Prec@1 55.840 Loss: 1.2560\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/390]\tTime 0.005 (0.005)\tLoss 0.9694 (0.9694)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [163][78/390]\tTime 0.002 (0.003)\tLoss 0.9485 (0.9602)\tPrec@1 60.938 (65.991)\n",
      "Epoch: [163][156/390]\tTime 0.005 (0.003)\tLoss 0.9904 (0.9834)\tPrec@1 59.375 (65.028)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.003)\tLoss 0.9693 (1.0145)\tPrec@1 64.062 (63.823)\n",
      "Epoch: [163][312/390]\tTime 0.008 (0.003)\tLoss 1.0528 (1.0293)\tPrec@1 65.625 (63.239)\n",
      "Epoch: [163][390/390]\tTime 0.002 (0.003)\tLoss 1.0161 (1.0421)\tPrec@1 62.500 (62.860)\n",
      "EPOCH: 163 train Results: Prec@1 62.860 Loss: 1.0421\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1744 (1.1744)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9451 (1.2603)\tPrec@1 56.250 (56.150)\n",
      "EPOCH: 163 val Results: Prec@1 56.150 Loss: 1.2603\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/390]\tTime 0.004 (0.004)\tLoss 1.0625 (1.0625)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [164][78/390]\tTime 0.003 (0.003)\tLoss 1.0075 (0.9629)\tPrec@1 65.625 (66.080)\n",
      "Epoch: [164][156/390]\tTime 0.002 (0.003)\tLoss 1.0998 (0.9887)\tPrec@1 58.594 (64.884)\n",
      "Epoch: [164][234/390]\tTime 0.002 (0.003)\tLoss 1.0351 (1.0071)\tPrec@1 61.719 (64.076)\n",
      "Epoch: [164][312/390]\tTime 0.004 (0.003)\tLoss 1.2434 (1.0277)\tPrec@1 54.688 (63.274)\n",
      "Epoch: [164][390/390]\tTime 0.029 (0.003)\tLoss 1.2739 (1.0386)\tPrec@1 58.750 (62.882)\n",
      "EPOCH: 164 train Results: Prec@1 62.882 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.1496 (1.1496)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1512 (1.2575)\tPrec@1 50.000 (56.150)\n",
      "EPOCH: 164 val Results: Prec@1 56.150 Loss: 1.2575\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/390]\tTime 0.002 (0.002)\tLoss 1.1056 (1.1056)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [165][78/390]\tTime 0.002 (0.004)\tLoss 0.9257 (0.9413)\tPrec@1 69.531 (66.762)\n",
      "Epoch: [165][156/390]\tTime 0.002 (0.004)\tLoss 1.0825 (0.9834)\tPrec@1 58.594 (65.182)\n",
      "Epoch: [165][234/390]\tTime 0.004 (0.004)\tLoss 1.2299 (1.0073)\tPrec@1 56.250 (64.262)\n",
      "Epoch: [165][312/390]\tTime 0.002 (0.004)\tLoss 0.9964 (1.0268)\tPrec@1 66.406 (63.488)\n",
      "Epoch: [165][390/390]\tTime 0.002 (0.004)\tLoss 1.1832 (1.0409)\tPrec@1 60.000 (62.902)\n",
      "EPOCH: 165 train Results: Prec@1 62.902 Loss: 1.0409\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1954 (1.1954)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1757 (1.2632)\tPrec@1 50.000 (55.270)\n",
      "EPOCH: 165 val Results: Prec@1 55.270 Loss: 1.2632\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/390]\tTime 0.003 (0.003)\tLoss 1.0573 (1.0573)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 0.9817 (0.9603)\tPrec@1 67.188 (65.803)\n",
      "Epoch: [166][156/390]\tTime 0.008 (0.003)\tLoss 1.0635 (0.9875)\tPrec@1 64.844 (64.495)\n",
      "Epoch: [166][234/390]\tTime 0.002 (0.003)\tLoss 1.1614 (1.0132)\tPrec@1 58.594 (63.517)\n",
      "Epoch: [166][312/390]\tTime 0.002 (0.003)\tLoss 1.1649 (1.0281)\tPrec@1 62.500 (63.097)\n",
      "Epoch: [166][390/390]\tTime 0.001 (0.003)\tLoss 1.0196 (1.0415)\tPrec@1 65.000 (62.708)\n",
      "EPOCH: 166 train Results: Prec@1 62.708 Loss: 1.0415\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1410 (1.1410)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1646 (1.2630)\tPrec@1 56.250 (54.990)\n",
      "EPOCH: 166 val Results: Prec@1 54.990 Loss: 1.2630\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/390]\tTime 0.003 (0.003)\tLoss 1.0606 (1.0606)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [167][78/390]\tTime 0.007 (0.003)\tLoss 1.0835 (0.9738)\tPrec@1 61.719 (65.595)\n",
      "Epoch: [167][156/390]\tTime 0.002 (0.005)\tLoss 0.9967 (0.9954)\tPrec@1 72.656 (64.779)\n",
      "Epoch: [167][234/390]\tTime 0.004 (0.004)\tLoss 1.0381 (1.0199)\tPrec@1 63.281 (63.930)\n",
      "Epoch: [167][312/390]\tTime 0.008 (0.004)\tLoss 1.0867 (1.0309)\tPrec@1 57.031 (63.458)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.004)\tLoss 1.0525 (1.0437)\tPrec@1 66.250 (63.020)\n",
      "EPOCH: 167 train Results: Prec@1 63.020 Loss: 1.0437\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1413 (1.1413)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0818 (1.2648)\tPrec@1 37.500 (54.970)\n",
      "EPOCH: 167 val Results: Prec@1 54.970 Loss: 1.2648\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/390]\tTime 0.002 (0.002)\tLoss 0.9201 (0.9201)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [168][78/390]\tTime 0.005 (0.003)\tLoss 0.9706 (0.9535)\tPrec@1 66.406 (66.307)\n",
      "Epoch: [168][156/390]\tTime 0.002 (0.003)\tLoss 0.9558 (0.9846)\tPrec@1 64.844 (65.043)\n",
      "Epoch: [168][234/390]\tTime 0.003 (0.003)\tLoss 1.0849 (1.0090)\tPrec@1 59.375 (63.993)\n",
      "Epoch: [168][312/390]\tTime 0.009 (0.004)\tLoss 1.0765 (1.0320)\tPrec@1 59.375 (63.181)\n",
      "Epoch: [168][390/390]\tTime 0.002 (0.004)\tLoss 1.1060 (1.0446)\tPrec@1 57.500 (62.666)\n",
      "EPOCH: 168 train Results: Prec@1 62.666 Loss: 1.0446\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0970 (1.0970)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3915 (1.2582)\tPrec@1 37.500 (54.840)\n",
      "EPOCH: 168 val Results: Prec@1 54.840 Loss: 1.2582\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/390]\tTime 0.005 (0.005)\tLoss 0.9263 (0.9263)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [169][78/390]\tTime 0.002 (0.003)\tLoss 1.0027 (0.9684)\tPrec@1 62.500 (65.615)\n",
      "Epoch: [169][156/390]\tTime 0.005 (0.003)\tLoss 0.9978 (0.9858)\tPrec@1 57.812 (64.953)\n",
      "Epoch: [169][234/390]\tTime 0.003 (0.003)\tLoss 0.9767 (1.0105)\tPrec@1 66.406 (63.963)\n",
      "Epoch: [169][312/390]\tTime 0.026 (0.003)\tLoss 1.0819 (1.0301)\tPrec@1 57.031 (63.276)\n",
      "Epoch: [169][390/390]\tTime 0.003 (0.004)\tLoss 0.9615 (1.0426)\tPrec@1 65.000 (62.772)\n",
      "EPOCH: 169 train Results: Prec@1 62.772 Loss: 1.0426\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1625 (1.1625)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1572 (1.2659)\tPrec@1 56.250 (55.320)\n",
      "EPOCH: 169 val Results: Prec@1 55.320 Loss: 1.2659\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/390]\tTime 0.005 (0.005)\tLoss 1.0642 (1.0642)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [170][78/390]\tTime 0.003 (0.004)\tLoss 1.0309 (0.9646)\tPrec@1 60.156 (65.289)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.003)\tLoss 0.8946 (0.9901)\tPrec@1 64.062 (64.421)\n",
      "Epoch: [170][234/390]\tTime 0.002 (0.004)\tLoss 1.2221 (1.0105)\tPrec@1 61.719 (63.667)\n",
      "Epoch: [170][312/390]\tTime 0.003 (0.004)\tLoss 0.9918 (1.0303)\tPrec@1 63.281 (63.064)\n",
      "Epoch: [170][390/390]\tTime 0.001 (0.004)\tLoss 0.9428 (1.0386)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 170 train Results: Prec@1 62.850 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1521 (1.1521)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0672 (1.2534)\tPrec@1 50.000 (55.990)\n",
      "EPOCH: 170 val Results: Prec@1 55.990 Loss: 1.2534\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/390]\tTime 0.002 (0.002)\tLoss 0.9359 (0.9359)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [171][78/390]\tTime 0.003 (0.003)\tLoss 0.9667 (0.9650)\tPrec@1 61.719 (65.971)\n",
      "Epoch: [171][156/390]\tTime 0.009 (0.003)\tLoss 1.0672 (0.9942)\tPrec@1 63.281 (64.884)\n",
      "Epoch: [171][234/390]\tTime 0.011 (0.003)\tLoss 0.9533 (1.0139)\tPrec@1 64.062 (64.026)\n",
      "Epoch: [171][312/390]\tTime 0.003 (0.003)\tLoss 0.9727 (1.0269)\tPrec@1 68.750 (63.693)\n",
      "Epoch: [171][390/390]\tTime 0.004 (0.003)\tLoss 1.2712 (1.0398)\tPrec@1 57.500 (63.082)\n",
      "EPOCH: 171 train Results: Prec@1 63.082 Loss: 1.0398\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1526 (1.1526)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3498 (1.2671)\tPrec@1 31.250 (55.290)\n",
      "EPOCH: 171 val Results: Prec@1 55.290 Loss: 1.2671\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/390]\tTime 0.003 (0.003)\tLoss 0.8262 (0.8262)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [172][78/390]\tTime 0.002 (0.003)\tLoss 0.9509 (0.9558)\tPrec@1 66.406 (66.258)\n",
      "Epoch: [172][156/390]\tTime 0.003 (0.003)\tLoss 0.9302 (0.9860)\tPrec@1 64.844 (64.953)\n",
      "Epoch: [172][234/390]\tTime 0.002 (0.003)\tLoss 1.0324 (1.0095)\tPrec@1 61.719 (64.102)\n",
      "Epoch: [172][312/390]\tTime 0.003 (0.003)\tLoss 1.2029 (1.0288)\tPrec@1 60.156 (63.274)\n",
      "Epoch: [172][390/390]\tTime 0.001 (0.003)\tLoss 1.1008 (1.0412)\tPrec@1 62.500 (62.804)\n",
      "EPOCH: 172 train Results: Prec@1 62.804 Loss: 1.0412\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1372 (1.1372)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3897 (1.2439)\tPrec@1 37.500 (55.830)\n",
      "EPOCH: 172 val Results: Prec@1 55.830 Loss: 1.2439\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/390]\tTime 0.005 (0.005)\tLoss 1.0715 (1.0715)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [173][78/390]\tTime 0.003 (0.003)\tLoss 1.1467 (0.9815)\tPrec@1 60.156 (65.388)\n",
      "Epoch: [173][156/390]\tTime 0.087 (0.003)\tLoss 1.1837 (1.0014)\tPrec@1 58.594 (64.456)\n",
      "Epoch: [173][234/390]\tTime 0.003 (0.003)\tLoss 1.0020 (1.0120)\tPrec@1 64.844 (64.116)\n",
      "Epoch: [173][312/390]\tTime 0.002 (0.003)\tLoss 1.0244 (1.0303)\tPrec@1 60.938 (63.304)\n",
      "Epoch: [173][390/390]\tTime 0.003 (0.003)\tLoss 1.0039 (1.0441)\tPrec@1 58.750 (62.878)\n",
      "EPOCH: 173 train Results: Prec@1 62.878 Loss: 1.0441\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1869 (1.1869)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3126 (1.2601)\tPrec@1 31.250 (55.140)\n",
      "EPOCH: 173 val Results: Prec@1 55.140 Loss: 1.2601\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/390]\tTime 0.003 (0.003)\tLoss 0.8756 (0.8756)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [174][78/390]\tTime 0.002 (0.004)\tLoss 0.8887 (0.9568)\tPrec@1 71.094 (65.536)\n",
      "Epoch: [174][156/390]\tTime 0.002 (0.004)\tLoss 0.9676 (0.9892)\tPrec@1 60.938 (64.227)\n",
      "Epoch: [174][234/390]\tTime 0.002 (0.004)\tLoss 0.9939 (1.0075)\tPrec@1 68.750 (63.680)\n",
      "Epoch: [174][312/390]\tTime 0.002 (0.004)\tLoss 1.0733 (1.0225)\tPrec@1 64.844 (63.161)\n",
      "Epoch: [174][390/390]\tTime 0.001 (0.003)\tLoss 1.1157 (1.0400)\tPrec@1 58.750 (62.586)\n",
      "EPOCH: 174 train Results: Prec@1 62.586 Loss: 1.0400\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1234 (1.1234)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3508 (1.2568)\tPrec@1 43.750 (55.830)\n",
      "EPOCH: 174 val Results: Prec@1 55.830 Loss: 1.2568\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/390]\tTime 0.004 (0.004)\tLoss 0.9405 (0.9405)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [175][78/390]\tTime 0.002 (0.003)\tLoss 1.1308 (0.9747)\tPrec@1 61.719 (65.684)\n",
      "Epoch: [175][156/390]\tTime 0.003 (0.003)\tLoss 1.0733 (0.9950)\tPrec@1 60.156 (64.933)\n",
      "Epoch: [175][234/390]\tTime 0.009 (0.003)\tLoss 1.1571 (1.0136)\tPrec@1 57.812 (64.062)\n",
      "Epoch: [175][312/390]\tTime 0.005 (0.004)\tLoss 1.1721 (1.0262)\tPrec@1 57.031 (63.618)\n",
      "Epoch: [175][390/390]\tTime 0.002 (0.003)\tLoss 1.1606 (1.0405)\tPrec@1 57.500 (62.988)\n",
      "EPOCH: 175 train Results: Prec@1 62.988 Loss: 1.0405\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2057 (1.2057)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9283 (1.2571)\tPrec@1 62.500 (55.720)\n",
      "EPOCH: 175 val Results: Prec@1 55.720 Loss: 1.2571\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/390]\tTime 0.002 (0.002)\tLoss 1.0280 (1.0280)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [176][78/390]\tTime 0.002 (0.004)\tLoss 1.1981 (0.9637)\tPrec@1 64.062 (66.119)\n",
      "Epoch: [176][156/390]\tTime 0.002 (0.004)\tLoss 1.0241 (0.9952)\tPrec@1 61.719 (64.620)\n",
      "Epoch: [176][234/390]\tTime 0.004 (0.004)\tLoss 1.0635 (1.0163)\tPrec@1 66.406 (63.770)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.004)\tLoss 0.9901 (1.0298)\tPrec@1 65.625 (63.161)\n",
      "Epoch: [176][390/390]\tTime 0.001 (0.004)\tLoss 1.1216 (1.0397)\tPrec@1 57.500 (62.768)\n",
      "EPOCH: 176 train Results: Prec@1 62.768 Loss: 1.0397\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1012 (1.1012)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3045 (1.2630)\tPrec@1 31.250 (55.470)\n",
      "EPOCH: 176 val Results: Prec@1 55.470 Loss: 1.2630\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/390]\tTime 0.002 (0.002)\tLoss 0.9615 (0.9615)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [177][78/390]\tTime 0.003 (0.003)\tLoss 0.8913 (0.9604)\tPrec@1 72.656 (65.892)\n",
      "Epoch: [177][156/390]\tTime 0.002 (0.003)\tLoss 1.0596 (0.9974)\tPrec@1 57.812 (64.301)\n",
      "Epoch: [177][234/390]\tTime 0.009 (0.003)\tLoss 1.0258 (1.0140)\tPrec@1 61.719 (63.737)\n",
      "Epoch: [177][312/390]\tTime 0.002 (0.003)\tLoss 1.1249 (1.0330)\tPrec@1 58.594 (63.139)\n",
      "Epoch: [177][390/390]\tTime 0.001 (0.003)\tLoss 1.0529 (1.0403)\tPrec@1 66.250 (62.958)\n",
      "EPOCH: 177 train Results: Prec@1 62.958 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2380 (1.2380)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2157 (1.2621)\tPrec@1 56.250 (55.770)\n",
      "EPOCH: 177 val Results: Prec@1 55.770 Loss: 1.2621\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/390]\tTime 0.004 (0.004)\tLoss 0.9500 (0.9500)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [178][78/390]\tTime 0.006 (0.003)\tLoss 1.0915 (0.9695)\tPrec@1 59.375 (65.813)\n",
      "Epoch: [178][156/390]\tTime 0.003 (0.003)\tLoss 1.0604 (0.9924)\tPrec@1 59.375 (64.555)\n",
      "Epoch: [178][234/390]\tTime 0.005 (0.003)\tLoss 1.1284 (1.0165)\tPrec@1 62.500 (63.577)\n",
      "Epoch: [178][312/390]\tTime 0.002 (0.003)\tLoss 1.0617 (1.0315)\tPrec@1 58.594 (63.159)\n",
      "Epoch: [178][390/390]\tTime 0.003 (0.003)\tLoss 1.0887 (1.0417)\tPrec@1 60.000 (62.774)\n",
      "EPOCH: 178 train Results: Prec@1 62.774 Loss: 1.0417\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0856 (1.0856)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3111 (1.2658)\tPrec@1 50.000 (55.440)\n",
      "EPOCH: 178 val Results: Prec@1 55.440 Loss: 1.2658\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/390]\tTime 0.006 (0.006)\tLoss 1.0824 (1.0824)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [179][78/390]\tTime 0.004 (0.003)\tLoss 0.9673 (0.9579)\tPrec@1 63.281 (66.129)\n",
      "Epoch: [179][156/390]\tTime 0.003 (0.003)\tLoss 0.9908 (0.9860)\tPrec@1 64.844 (64.933)\n",
      "Epoch: [179][234/390]\tTime 0.002 (0.003)\tLoss 1.2443 (1.0125)\tPrec@1 57.031 (63.813)\n",
      "Epoch: [179][312/390]\tTime 0.003 (0.003)\tLoss 1.0227 (1.0253)\tPrec@1 60.938 (63.341)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.4208 (1.0395)\tPrec@1 50.000 (62.872)\n",
      "EPOCH: 179 train Results: Prec@1 62.872 Loss: 1.0395\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1275 (1.1275)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0097 (1.2590)\tPrec@1 62.500 (55.630)\n",
      "EPOCH: 179 val Results: Prec@1 55.630 Loss: 1.2590\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/390]\tTime 0.009 (0.009)\tLoss 1.0008 (1.0008)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [180][78/390]\tTime 0.002 (0.003)\tLoss 1.0472 (0.9675)\tPrec@1 58.594 (64.913)\n",
      "Epoch: [180][156/390]\tTime 0.067 (0.004)\tLoss 0.9197 (0.9893)\tPrec@1 67.188 (64.207)\n",
      "Epoch: [180][234/390]\tTime 0.002 (0.004)\tLoss 1.0607 (1.0130)\tPrec@1 61.719 (63.441)\n",
      "Epoch: [180][312/390]\tTime 0.002 (0.004)\tLoss 0.9945 (1.0305)\tPrec@1 64.844 (62.989)\n",
      "Epoch: [180][390/390]\tTime 0.003 (0.004)\tLoss 1.0556 (1.0413)\tPrec@1 62.500 (62.552)\n",
      "EPOCH: 180 train Results: Prec@1 62.552 Loss: 1.0413\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0998 (1.0998)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3197 (1.2742)\tPrec@1 50.000 (55.120)\n",
      "EPOCH: 180 val Results: Prec@1 55.120 Loss: 1.2742\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/390]\tTime 0.007 (0.007)\tLoss 0.9546 (0.9546)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [181][78/390]\tTime 0.002 (0.004)\tLoss 0.9079 (0.9498)\tPrec@1 64.844 (66.416)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.004)\tLoss 0.9227 (0.9772)\tPrec@1 64.844 (65.132)\n",
      "Epoch: [181][234/390]\tTime 0.003 (0.004)\tLoss 1.0000 (1.0033)\tPrec@1 62.500 (64.315)\n",
      "Epoch: [181][312/390]\tTime 0.002 (0.004)\tLoss 1.1410 (1.0234)\tPrec@1 61.719 (63.703)\n",
      "Epoch: [181][390/390]\tTime 0.005 (0.004)\tLoss 1.2092 (1.0355)\tPrec@1 57.500 (63.256)\n",
      "EPOCH: 181 train Results: Prec@1 63.256 Loss: 1.0355\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0896 (1.0896)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1791 (1.2583)\tPrec@1 56.250 (55.710)\n",
      "EPOCH: 181 val Results: Prec@1 55.710 Loss: 1.2583\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/390]\tTime 0.006 (0.006)\tLoss 1.0062 (1.0062)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [182][78/390]\tTime 0.006 (0.003)\tLoss 0.9986 (0.9589)\tPrec@1 62.500 (65.783)\n",
      "Epoch: [182][156/390]\tTime 0.002 (0.003)\tLoss 1.0123 (0.9858)\tPrec@1 69.531 (64.874)\n",
      "Epoch: [182][234/390]\tTime 0.002 (0.003)\tLoss 1.1027 (1.0121)\tPrec@1 53.906 (63.813)\n",
      "Epoch: [182][312/390]\tTime 0.002 (0.003)\tLoss 1.0014 (1.0275)\tPrec@1 67.969 (63.366)\n",
      "Epoch: [182][390/390]\tTime 0.001 (0.003)\tLoss 1.2717 (1.0386)\tPrec@1 50.000 (62.818)\n",
      "EPOCH: 182 train Results: Prec@1 62.818 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1373 (1.1373)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2751 (1.2650)\tPrec@1 43.750 (55.010)\n",
      "EPOCH: 182 val Results: Prec@1 55.010 Loss: 1.2650\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/390]\tTime 0.004 (0.004)\tLoss 0.8945 (0.8945)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [183][78/390]\tTime 0.002 (0.003)\tLoss 1.0455 (0.9624)\tPrec@1 60.156 (65.773)\n",
      "Epoch: [183][156/390]\tTime 0.002 (0.003)\tLoss 0.9910 (0.9884)\tPrec@1 67.969 (64.645)\n",
      "Epoch: [183][234/390]\tTime 0.003 (0.003)\tLoss 1.0674 (1.0139)\tPrec@1 60.938 (63.670)\n",
      "Epoch: [183][312/390]\tTime 0.006 (0.004)\tLoss 0.9209 (1.0322)\tPrec@1 67.188 (63.014)\n",
      "Epoch: [183][390/390]\tTime 0.003 (0.004)\tLoss 1.2227 (1.0433)\tPrec@1 57.500 (62.580)\n",
      "EPOCH: 183 train Results: Prec@1 62.580 Loss: 1.0433\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1607 (1.1607)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4244 (1.2612)\tPrec@1 43.750 (55.160)\n",
      "EPOCH: 183 val Results: Prec@1 55.160 Loss: 1.2612\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/390]\tTime 0.006 (0.006)\tLoss 0.7851 (0.7851)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [184][78/390]\tTime 0.002 (0.003)\tLoss 1.1139 (0.9569)\tPrec@1 64.062 (66.466)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.003)\tLoss 1.1260 (0.9866)\tPrec@1 59.375 (65.063)\n",
      "Epoch: [184][234/390]\tTime 0.002 (0.003)\tLoss 1.2253 (1.0129)\tPrec@1 57.031 (64.122)\n",
      "Epoch: [184][312/390]\tTime 0.003 (0.003)\tLoss 1.1117 (1.0303)\tPrec@1 63.281 (63.468)\n",
      "Epoch: [184][390/390]\tTime 0.002 (0.003)\tLoss 1.3250 (1.0377)\tPrec@1 55.000 (63.236)\n",
      "EPOCH: 184 train Results: Prec@1 63.236 Loss: 1.0377\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0549 (1.0549)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1437 (1.2665)\tPrec@1 50.000 (55.680)\n",
      "EPOCH: 184 val Results: Prec@1 55.680 Loss: 1.2665\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/390]\tTime 0.003 (0.003)\tLoss 0.9262 (0.9262)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [185][78/390]\tTime 0.003 (0.003)\tLoss 1.1399 (0.9653)\tPrec@1 53.906 (65.655)\n",
      "Epoch: [185][156/390]\tTime 0.004 (0.003)\tLoss 0.8836 (0.9868)\tPrec@1 71.094 (64.764)\n",
      "Epoch: [185][234/390]\tTime 0.002 (0.003)\tLoss 1.1568 (1.0044)\tPrec@1 60.938 (64.212)\n",
      "Epoch: [185][312/390]\tTime 0.004 (0.003)\tLoss 0.9169 (1.0175)\tPrec@1 66.406 (63.606)\n",
      "Epoch: [185][390/390]\tTime 0.009 (0.003)\tLoss 1.1631 (1.0364)\tPrec@1 56.250 (63.002)\n",
      "EPOCH: 185 train Results: Prec@1 63.002 Loss: 1.0364\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1206 (1.1206)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0623 (1.2678)\tPrec@1 43.750 (54.900)\n",
      "EPOCH: 185 val Results: Prec@1 54.900 Loss: 1.2678\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/390]\tTime 0.002 (0.002)\tLoss 0.9338 (0.9338)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [186][78/390]\tTime 0.002 (0.004)\tLoss 0.9244 (0.9696)\tPrec@1 67.969 (65.902)\n",
      "Epoch: [186][156/390]\tTime 0.003 (0.003)\tLoss 1.0440 (0.9948)\tPrec@1 60.938 (64.515)\n",
      "Epoch: [186][234/390]\tTime 0.010 (0.003)\tLoss 1.0264 (1.0189)\tPrec@1 64.062 (63.564)\n",
      "Epoch: [186][312/390]\tTime 0.002 (0.003)\tLoss 1.1488 (1.0378)\tPrec@1 60.156 (62.957)\n",
      "Epoch: [186][390/390]\tTime 0.001 (0.003)\tLoss 0.9323 (1.0458)\tPrec@1 66.250 (62.668)\n",
      "EPOCH: 186 train Results: Prec@1 62.668 Loss: 1.0458\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0556 (1.0556)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3919 (1.2554)\tPrec@1 31.250 (55.460)\n",
      "EPOCH: 186 val Results: Prec@1 55.460 Loss: 1.2554\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/390]\tTime 0.003 (0.003)\tLoss 1.0179 (1.0179)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [187][78/390]\tTime 0.003 (0.003)\tLoss 0.8661 (0.9578)\tPrec@1 71.094 (66.406)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.2731 (0.9769)\tPrec@1 57.031 (65.431)\n",
      "Epoch: [187][234/390]\tTime 0.004 (0.003)\tLoss 1.0739 (1.0033)\tPrec@1 61.719 (64.412)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.0026 (1.0246)\tPrec@1 63.281 (63.623)\n",
      "Epoch: [187][390/390]\tTime 0.001 (0.003)\tLoss 1.2358 (1.0395)\tPrec@1 55.000 (63.058)\n",
      "EPOCH: 187 train Results: Prec@1 63.058 Loss: 1.0395\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1415 (1.1415)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2319 (1.2586)\tPrec@1 31.250 (55.810)\n",
      "EPOCH: 187 val Results: Prec@1 55.810 Loss: 1.2586\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/390]\tTime 0.012 (0.012)\tLoss 0.8565 (0.8565)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [188][78/390]\tTime 0.002 (0.003)\tLoss 1.2340 (0.9463)\tPrec@1 57.812 (66.466)\n",
      "Epoch: [188][156/390]\tTime 0.002 (0.003)\tLoss 1.2041 (0.9773)\tPrec@1 58.594 (65.197)\n",
      "Epoch: [188][234/390]\tTime 0.004 (0.003)\tLoss 1.1170 (0.9991)\tPrec@1 57.031 (64.229)\n",
      "Epoch: [188][312/390]\tTime 0.003 (0.003)\tLoss 1.2157 (1.0180)\tPrec@1 60.156 (63.638)\n",
      "Epoch: [188][390/390]\tTime 0.053 (0.003)\tLoss 1.1162 (1.0337)\tPrec@1 60.000 (63.126)\n",
      "EPOCH: 188 train Results: Prec@1 63.126 Loss: 1.0337\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0716 (1.0716)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1874 (1.2582)\tPrec@1 43.750 (55.330)\n",
      "EPOCH: 188 val Results: Prec@1 55.330 Loss: 1.2582\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/390]\tTime 0.003 (0.003)\tLoss 1.0638 (1.0638)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.003)\tLoss 0.9302 (0.9464)\tPrec@1 65.625 (66.831)\n",
      "Epoch: [189][156/390]\tTime 0.005 (0.003)\tLoss 1.1635 (0.9861)\tPrec@1 53.906 (65.103)\n",
      "Epoch: [189][234/390]\tTime 0.006 (0.004)\tLoss 1.1479 (1.0076)\tPrec@1 58.594 (64.342)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.004)\tLoss 1.1609 (1.0242)\tPrec@1 55.469 (63.616)\n",
      "Epoch: [189][390/390]\tTime 0.002 (0.004)\tLoss 1.2353 (1.0403)\tPrec@1 60.000 (63.100)\n",
      "EPOCH: 189 train Results: Prec@1 63.100 Loss: 1.0403\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0850 (1.0850)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3733 (1.2507)\tPrec@1 37.500 (55.800)\n",
      "EPOCH: 189 val Results: Prec@1 55.800 Loss: 1.2507\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/390]\tTime 0.002 (0.002)\tLoss 0.7902 (0.7902)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.003)\tLoss 0.9821 (0.9530)\tPrec@1 64.844 (65.961)\n",
      "Epoch: [190][156/390]\tTime 0.002 (0.003)\tLoss 0.9590 (0.9805)\tPrec@1 62.500 (64.744)\n",
      "Epoch: [190][234/390]\tTime 0.003 (0.003)\tLoss 0.9350 (1.0050)\tPrec@1 67.969 (63.906)\n",
      "Epoch: [190][312/390]\tTime 0.002 (0.003)\tLoss 1.0562 (1.0234)\tPrec@1 58.594 (63.301)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.003)\tLoss 1.2047 (1.0384)\tPrec@1 51.250 (62.862)\n",
      "EPOCH: 190 train Results: Prec@1 62.862 Loss: 1.0384\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.0754 (1.0754)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3310 (1.2581)\tPrec@1 37.500 (55.530)\n",
      "EPOCH: 190 val Results: Prec@1 55.530 Loss: 1.2581\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/390]\tTime 0.003 (0.003)\tLoss 1.0238 (1.0238)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.004)\tLoss 0.8718 (0.9624)\tPrec@1 70.312 (65.200)\n",
      "Epoch: [191][156/390]\tTime 0.004 (0.004)\tLoss 1.1122 (0.9889)\tPrec@1 57.812 (64.530)\n",
      "Epoch: [191][234/390]\tTime 0.003 (0.003)\tLoss 0.9893 (1.0103)\tPrec@1 64.062 (63.640)\n",
      "Epoch: [191][312/390]\tTime 0.005 (0.004)\tLoss 0.9334 (1.0239)\tPrec@1 66.406 (63.149)\n",
      "Epoch: [191][390/390]\tTime 0.002 (0.004)\tLoss 0.9803 (1.0404)\tPrec@1 60.000 (62.558)\n",
      "EPOCH: 191 train Results: Prec@1 62.558 Loss: 1.0404\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0181 (1.0181)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0932 (1.2414)\tPrec@1 37.500 (55.970)\n",
      "EPOCH: 191 val Results: Prec@1 55.970 Loss: 1.2414\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/390]\tTime 0.003 (0.003)\tLoss 0.8081 (0.8081)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [192][78/390]\tTime 0.002 (0.003)\tLoss 0.9439 (0.9649)\tPrec@1 60.938 (66.218)\n",
      "Epoch: [192][156/390]\tTime 0.002 (0.003)\tLoss 1.0101 (0.9939)\tPrec@1 67.188 (64.520)\n",
      "Epoch: [192][234/390]\tTime 0.003 (0.003)\tLoss 1.0538 (1.0166)\tPrec@1 65.625 (63.750)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.003)\tLoss 1.2486 (1.0294)\tPrec@1 60.156 (63.336)\n",
      "Epoch: [192][390/390]\tTime 0.001 (0.003)\tLoss 1.0758 (1.0418)\tPrec@1 61.250 (62.892)\n",
      "EPOCH: 192 train Results: Prec@1 62.892 Loss: 1.0418\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0804 (1.0804)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.2320 (1.2535)\tPrec@1 37.500 (55.650)\n",
      "EPOCH: 192 val Results: Prec@1 55.650 Loss: 1.2535\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/390]\tTime 0.002 (0.002)\tLoss 0.8078 (0.8078)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [193][78/390]\tTime 0.003 (0.003)\tLoss 0.9409 (0.9689)\tPrec@1 69.531 (65.902)\n",
      "Epoch: [193][156/390]\tTime 0.003 (0.003)\tLoss 1.1053 (0.9943)\tPrec@1 57.031 (64.694)\n",
      "Epoch: [193][234/390]\tTime 0.002 (0.003)\tLoss 1.0052 (1.0099)\tPrec@1 61.719 (64.016)\n",
      "Epoch: [193][312/390]\tTime 0.003 (0.004)\tLoss 1.2498 (1.0304)\tPrec@1 60.938 (63.276)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.004)\tLoss 1.0358 (1.0410)\tPrec@1 61.250 (62.850)\n",
      "EPOCH: 193 train Results: Prec@1 62.850 Loss: 1.0410\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0595 (1.0595)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2559 (1.2607)\tPrec@1 25.000 (55.140)\n",
      "EPOCH: 193 val Results: Prec@1 55.140 Loss: 1.2607\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/390]\tTime 0.003 (0.003)\tLoss 0.9384 (0.9384)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [194][78/390]\tTime 0.008 (0.004)\tLoss 0.9088 (0.9649)\tPrec@1 64.062 (65.635)\n",
      "Epoch: [194][156/390]\tTime 0.003 (0.004)\tLoss 0.9268 (0.9914)\tPrec@1 67.969 (64.719)\n",
      "Epoch: [194][234/390]\tTime 0.002 (0.003)\tLoss 1.0127 (1.0074)\tPrec@1 63.281 (64.126)\n",
      "Epoch: [194][312/390]\tTime 0.003 (0.003)\tLoss 1.0544 (1.0227)\tPrec@1 60.938 (63.661)\n",
      "Epoch: [194][390/390]\tTime 0.002 (0.003)\tLoss 1.0680 (1.0365)\tPrec@1 57.500 (63.264)\n",
      "EPOCH: 194 train Results: Prec@1 63.264 Loss: 1.0365\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1395 (1.1395)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3373 (1.2526)\tPrec@1 43.750 (55.260)\n",
      "EPOCH: 194 val Results: Prec@1 55.260 Loss: 1.2526\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/390]\tTime 0.004 (0.004)\tLoss 0.8456 (0.8456)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [195][78/390]\tTime 0.004 (0.003)\tLoss 0.9408 (0.9636)\tPrec@1 67.969 (65.585)\n",
      "Epoch: [195][156/390]\tTime 0.003 (0.003)\tLoss 0.9584 (0.9905)\tPrec@1 67.969 (64.515)\n",
      "Epoch: [195][234/390]\tTime 0.004 (0.003)\tLoss 1.0783 (1.0095)\tPrec@1 58.594 (63.873)\n",
      "Epoch: [195][312/390]\tTime 0.007 (0.003)\tLoss 1.1658 (1.0264)\tPrec@1 57.031 (63.344)\n",
      "Epoch: [195][390/390]\tTime 0.003 (0.003)\tLoss 1.1824 (1.0386)\tPrec@1 57.500 (62.882)\n",
      "EPOCH: 195 train Results: Prec@1 62.882 Loss: 1.0386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0615 (1.0615)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3907 (1.2681)\tPrec@1 37.500 (55.180)\n",
      "EPOCH: 195 val Results: Prec@1 55.180 Loss: 1.2681\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/390]\tTime 0.003 (0.003)\tLoss 0.8733 (0.8733)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 0.9902 (0.9458)\tPrec@1 62.500 (66.307)\n",
      "Epoch: [196][156/390]\tTime 0.003 (0.003)\tLoss 1.0167 (0.9765)\tPrec@1 63.281 (65.192)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.003)\tLoss 0.9534 (1.0095)\tPrec@1 66.406 (64.079)\n",
      "Epoch: [196][312/390]\tTime 0.002 (0.003)\tLoss 1.0817 (1.0230)\tPrec@1 59.375 (63.621)\n",
      "Epoch: [196][390/390]\tTime 0.005 (0.003)\tLoss 0.9751 (1.0372)\tPrec@1 61.250 (63.076)\n",
      "EPOCH: 196 train Results: Prec@1 63.076 Loss: 1.0372\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1523 (1.1523)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3476 (1.2492)\tPrec@1 43.750 (55.790)\n",
      "EPOCH: 196 val Results: Prec@1 55.790 Loss: 1.2492\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/390]\tTime 0.003 (0.003)\tLoss 0.8293 (0.8293)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [197][78/390]\tTime 0.002 (0.003)\tLoss 0.9901 (0.9463)\tPrec@1 63.281 (66.426)\n",
      "Epoch: [197][156/390]\tTime 0.003 (0.003)\tLoss 1.1288 (0.9772)\tPrec@1 63.281 (65.341)\n",
      "Epoch: [197][234/390]\tTime 0.002 (0.003)\tLoss 1.0022 (1.0065)\tPrec@1 61.719 (64.365)\n",
      "Epoch: [197][312/390]\tTime 0.002 (0.003)\tLoss 1.0794 (1.0269)\tPrec@1 58.594 (63.563)\n",
      "Epoch: [197][390/390]\tTime 0.002 (0.003)\tLoss 0.9502 (1.0383)\tPrec@1 66.250 (63.214)\n",
      "EPOCH: 197 train Results: Prec@1 63.214 Loss: 1.0383\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1654 (1.1654)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3319 (1.2544)\tPrec@1 43.750 (55.090)\n",
      "EPOCH: 197 val Results: Prec@1 55.090 Loss: 1.2544\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/390]\tTime 0.002 (0.002)\tLoss 0.9726 (0.9726)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [198][78/390]\tTime 0.003 (0.003)\tLoss 0.9118 (0.9646)\tPrec@1 62.500 (65.259)\n",
      "Epoch: [198][156/390]\tTime 0.003 (0.003)\tLoss 1.0433 (0.9876)\tPrec@1 68.750 (64.381)\n",
      "Epoch: [198][234/390]\tTime 0.002 (0.003)\tLoss 1.3244 (1.0101)\tPrec@1 60.156 (63.531)\n",
      "Epoch: [198][312/390]\tTime 0.006 (0.003)\tLoss 0.9022 (1.0238)\tPrec@1 75.000 (63.112)\n",
      "Epoch: [198][390/390]\tTime 0.008 (0.003)\tLoss 0.9602 (1.0346)\tPrec@1 68.750 (62.922)\n",
      "EPOCH: 198 train Results: Prec@1 62.922 Loss: 1.0346\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1381 (1.1381)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1587 (1.2621)\tPrec@1 43.750 (55.140)\n",
      "EPOCH: 198 val Results: Prec@1 55.140 Loss: 1.2621\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/390]\tTime 0.004 (0.004)\tLoss 1.0403 (1.0403)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [199][78/390]\tTime 0.002 (0.003)\tLoss 0.9257 (0.9463)\tPrec@1 65.625 (66.515)\n",
      "Epoch: [199][156/390]\tTime 0.003 (0.003)\tLoss 1.0147 (0.9715)\tPrec@1 67.188 (65.525)\n",
      "Epoch: [199][234/390]\tTime 0.010 (0.003)\tLoss 1.0157 (1.0004)\tPrec@1 64.062 (64.405)\n",
      "Epoch: [199][312/390]\tTime 0.005 (0.003)\tLoss 1.1317 (1.0233)\tPrec@1 60.156 (63.486)\n",
      "Epoch: [199][390/390]\tTime 0.001 (0.003)\tLoss 1.0976 (1.0372)\tPrec@1 63.750 (62.988)\n",
      "EPOCH: 199 train Results: Prec@1 62.988 Loss: 1.0372\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0649 (1.0649)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2117 (1.2525)\tPrec@1 37.500 (55.640)\n",
      "EPOCH: 199 val Results: Prec@1 55.640 Loss: 1.2525\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/390]\tTime 0.003 (0.003)\tLoss 0.8200 (0.8200)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [200][78/390]\tTime 0.002 (0.004)\tLoss 1.0655 (0.9640)\tPrec@1 64.062 (65.566)\n",
      "Epoch: [200][156/390]\tTime 0.002 (0.003)\tLoss 0.7913 (0.9894)\tPrec@1 72.656 (64.859)\n",
      "Epoch: [200][234/390]\tTime 0.002 (0.003)\tLoss 1.0190 (1.0121)\tPrec@1 60.156 (63.790)\n",
      "Epoch: [200][312/390]\tTime 0.003 (0.004)\tLoss 1.0874 (1.0254)\tPrec@1 61.719 (63.429)\n",
      "Epoch: [200][390/390]\tTime 0.017 (0.004)\tLoss 1.3638 (1.0393)\tPrec@1 56.250 (63.014)\n",
      "EPOCH: 200 train Results: Prec@1 63.014 Loss: 1.0393\n",
      "Test: [0/78]\tTime 0.013 (0.013)\tLoss 1.1261 (1.1261)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0260 (1.2463)\tPrec@1 56.250 (55.920)\n",
      "EPOCH: 200 val Results: Prec@1 55.920 Loss: 1.2463\n",
      "Best Prec@1: 57.080\n",
      "\n",
      "End time:  Thu Apr  4 23:00:15 2024\n",
      "train executed in 286.7670 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'scheduler': 'cos', # cos, None\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer2 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:00:15 2024\n",
      "current lr 1.00000e-03\n",
      "Epoch: [1][0/390]\tTime 0.006 (0.006)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.003 (0.004)\tLoss 4.2110 (4.8313)\tPrec@1 11.719 (12.233)\n",
      "Epoch: [1][156/390]\tTime 0.002 (0.003)\tLoss 3.7790 (4.2625)\tPrec@1 14.062 (14.734)\n",
      "Epoch: [1][234/390]\tTime 0.006 (0.003)\tLoss 2.8561 (3.9043)\tPrec@1 25.781 (16.702)\n",
      "Epoch: [1][312/390]\tTime 0.003 (0.003)\tLoss 2.9212 (3.6516)\tPrec@1 18.750 (18.136)\n",
      "Epoch: [1][390/390]\tTime 0.003 (0.003)\tLoss 2.4238 (3.4550)\tPrec@1 27.500 (19.672)\n",
      "EPOCH: 1 train Results: Prec@1 19.672 Loss: 3.4550\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 2.3102 (2.3102)\tPrec@1 29.688 (29.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 2.1503 (2.2151)\tPrec@1 25.000 (29.720)\n",
      "EPOCH: 1 val Results: Prec@1 29.720 Loss: 2.2151\n",
      "Best Prec@1: 29.720\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [2][0/390]\tTime 0.002 (0.002)\tLoss 2.5185 (2.5185)\tPrec@1 24.219 (24.219)\n",
      "Epoch: [2][78/390]\tTime 0.002 (0.003)\tLoss 2.1063 (2.4423)\tPrec@1 32.812 (27.027)\n",
      "Epoch: [2][156/390]\tTime 0.003 (0.003)\tLoss 2.2830 (2.3822)\tPrec@1 25.781 (28.598)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.003)\tLoss 2.5502 (2.3290)\tPrec@1 24.219 (29.129)\n",
      "Epoch: [2][312/390]\tTime 0.008 (0.003)\tLoss 2.3671 (2.2837)\tPrec@1 28.906 (29.620)\n",
      "Epoch: [2][390/390]\tTime 0.001 (0.003)\tLoss 1.9201 (2.2434)\tPrec@1 35.000 (30.060)\n",
      "EPOCH: 2 train Results: Prec@1 30.060 Loss: 2.2434\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.8905 (1.8905)\tPrec@1 32.812 (32.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7333 (1.8615)\tPrec@1 18.750 (35.990)\n",
      "EPOCH: 2 val Results: Prec@1 35.990 Loss: 1.8615\n",
      "Best Prec@1: 35.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [3][0/390]\tTime 0.004 (0.004)\tLoss 1.9196 (1.9196)\tPrec@1 28.906 (28.906)\n",
      "Epoch: [3][78/390]\tTime 0.002 (0.003)\tLoss 1.8936 (1.9947)\tPrec@1 31.250 (33.900)\n",
      "Epoch: [3][156/390]\tTime 0.008 (0.003)\tLoss 1.8343 (1.9638)\tPrec@1 34.375 (34.345)\n",
      "Epoch: [3][234/390]\tTime 0.003 (0.003)\tLoss 1.9050 (1.9446)\tPrec@1 31.250 (34.591)\n",
      "Epoch: [3][312/390]\tTime 0.002 (0.003)\tLoss 1.9053 (1.9284)\tPrec@1 35.156 (34.867)\n",
      "Epoch: [3][390/390]\tTime 0.009 (0.003)\tLoss 1.7400 (1.9093)\tPrec@1 32.500 (35.180)\n",
      "EPOCH: 3 train Results: Prec@1 35.180 Loss: 1.9093\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.7195 (1.7195)\tPrec@1 38.281 (38.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5686 (1.7150)\tPrec@1 31.250 (39.410)\n",
      "EPOCH: 3 val Results: Prec@1 39.410 Loss: 1.7150\n",
      "Best Prec@1: 39.410\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [4][0/390]\tTime 0.002 (0.002)\tLoss 1.8887 (1.8887)\tPrec@1 28.906 (28.906)\n",
      "Epoch: [4][78/390]\tTime 0.002 (0.003)\tLoss 1.8872 (1.7806)\tPrec@1 40.625 (37.846)\n",
      "Epoch: [4][156/390]\tTime 0.005 (0.003)\tLoss 1.7901 (1.7652)\tPrec@1 40.625 (37.973)\n",
      "Epoch: [4][234/390]\tTime 0.003 (0.003)\tLoss 1.8730 (1.7546)\tPrec@1 26.562 (38.162)\n",
      "Epoch: [4][312/390]\tTime 0.009 (0.003)\tLoss 1.6024 (1.7463)\tPrec@1 39.844 (38.396)\n",
      "Epoch: [4][390/390]\tTime 0.002 (0.003)\tLoss 1.5602 (1.7361)\tPrec@1 40.000 (38.588)\n",
      "EPOCH: 4 train Results: Prec@1 38.588 Loss: 1.7361\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.6037 (1.6037)\tPrec@1 44.531 (44.531)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5198 (1.6368)\tPrec@1 31.250 (41.960)\n",
      "EPOCH: 4 val Results: Prec@1 41.960 Loss: 1.6368\n",
      "Best Prec@1: 41.960\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [5][0/390]\tTime 0.002 (0.002)\tLoss 1.8193 (1.8193)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.003)\tLoss 1.7869 (1.6379)\tPrec@1 38.281 (41.614)\n",
      "Epoch: [5][156/390]\tTime 0.003 (0.003)\tLoss 1.7283 (1.6530)\tPrec@1 39.844 (41.182)\n",
      "Epoch: [5][234/390]\tTime 0.015 (0.003)\tLoss 1.6349 (1.6443)\tPrec@1 42.188 (41.656)\n",
      "Epoch: [5][312/390]\tTime 0.004 (0.003)\tLoss 1.6002 (1.6414)\tPrec@1 42.969 (41.818)\n",
      "Epoch: [5][390/390]\tTime 0.001 (0.003)\tLoss 1.5735 (1.6365)\tPrec@1 41.250 (42.068)\n",
      "EPOCH: 5 train Results: Prec@1 42.068 Loss: 1.6365\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.5390 (1.5390)\tPrec@1 46.094 (46.094)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4554 (1.5870)\tPrec@1 31.250 (43.780)\n",
      "EPOCH: 5 val Results: Prec@1 43.780 Loss: 1.5870\n",
      "Best Prec@1: 43.780\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [6][0/390]\tTime 0.002 (0.002)\tLoss 1.6425 (1.6425)\tPrec@1 44.531 (44.531)\n",
      "Epoch: [6][78/390]\tTime 0.002 (0.003)\tLoss 1.6826 (1.5690)\tPrec@1 39.062 (45.253)\n",
      "Epoch: [6][156/390]\tTime 0.002 (0.003)\tLoss 1.4346 (1.5711)\tPrec@1 44.531 (44.909)\n",
      "Epoch: [6][234/390]\tTime 0.009 (0.003)\tLoss 1.6878 (1.5721)\tPrec@1 42.969 (44.774)\n",
      "Epoch: [6][312/390]\tTime 0.002 (0.003)\tLoss 1.6759 (1.5733)\tPrec@1 39.844 (44.761)\n",
      "Epoch: [6][390/390]\tTime 0.003 (0.003)\tLoss 1.7868 (1.5715)\tPrec@1 31.250 (44.900)\n",
      "EPOCH: 6 train Results: Prec@1 44.900 Loss: 1.5715\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4983 (1.4983)\tPrec@1 50.000 (50.000)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4741 (1.5517)\tPrec@1 31.250 (45.220)\n",
      "EPOCH: 6 val Results: Prec@1 45.220 Loss: 1.5517\n",
      "Best Prec@1: 45.220\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [7][0/390]\tTime 0.005 (0.005)\tLoss 1.5970 (1.5970)\tPrec@1 44.531 (44.531)\n",
      "Epoch: [7][78/390]\tTime 0.002 (0.003)\tLoss 1.5572 (1.5335)\tPrec@1 44.531 (46.746)\n",
      "Epoch: [7][156/390]\tTime 0.002 (0.003)\tLoss 1.5695 (1.5323)\tPrec@1 45.312 (47.094)\n",
      "Epoch: [7][234/390]\tTime 0.005 (0.003)\tLoss 1.5319 (1.5291)\tPrec@1 42.969 (46.649)\n",
      "Epoch: [7][312/390]\tTime 0.002 (0.003)\tLoss 1.4742 (1.5259)\tPrec@1 53.125 (46.728)\n",
      "Epoch: [7][390/390]\tTime 0.001 (0.003)\tLoss 1.5525 (1.5271)\tPrec@1 53.750 (46.738)\n",
      "EPOCH: 7 train Results: Prec@1 46.738 Loss: 1.5271\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4678 (1.4678)\tPrec@1 47.656 (47.656)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4615 (1.5206)\tPrec@1 25.000 (46.670)\n",
      "EPOCH: 7 val Results: Prec@1 46.670 Loss: 1.5206\n",
      "Best Prec@1: 46.670\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [8][0/390]\tTime 0.002 (0.002)\tLoss 1.5043 (1.5043)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [8][78/390]\tTime 0.002 (0.003)\tLoss 1.5064 (1.4984)\tPrec@1 49.219 (48.517)\n",
      "Epoch: [8][156/390]\tTime 0.003 (0.003)\tLoss 1.5475 (1.5006)\tPrec@1 42.188 (48.308)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.003)\tLoss 1.4685 (1.4955)\tPrec@1 48.438 (48.394)\n",
      "Epoch: [8][312/390]\tTime 0.002 (0.003)\tLoss 1.3896 (1.4920)\tPrec@1 53.906 (48.585)\n",
      "Epoch: [8][390/390]\tTime 0.002 (0.003)\tLoss 1.6164 (1.4916)\tPrec@1 40.000 (48.540)\n",
      "EPOCH: 8 train Results: Prec@1 48.540 Loss: 1.4916\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.4392 (1.4392)\tPrec@1 50.000 (50.000)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4359 (1.4909)\tPrec@1 25.000 (47.700)\n",
      "EPOCH: 8 val Results: Prec@1 47.700 Loss: 1.4909\n",
      "Best Prec@1: 47.700\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [9][0/390]\tTime 0.002 (0.002)\tLoss 1.4724 (1.4724)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [9][78/390]\tTime 0.002 (0.003)\tLoss 1.3937 (1.4494)\tPrec@1 54.688 (50.188)\n",
      "Epoch: [9][156/390]\tTime 0.003 (0.003)\tLoss 1.5986 (1.4491)\tPrec@1 40.625 (50.269)\n",
      "Epoch: [9][234/390]\tTime 0.004 (0.003)\tLoss 1.5204 (1.4569)\tPrec@1 48.438 (49.797)\n",
      "Epoch: [9][312/390]\tTime 0.005 (0.003)\tLoss 1.4896 (1.4569)\tPrec@1 51.562 (49.790)\n",
      "Epoch: [9][390/390]\tTime 0.006 (0.003)\tLoss 1.5971 (1.4577)\tPrec@1 43.750 (49.650)\n",
      "EPOCH: 9 train Results: Prec@1 49.650 Loss: 1.4577\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4086 (1.4086)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4565 (1.4636)\tPrec@1 25.000 (48.540)\n",
      "EPOCH: 9 val Results: Prec@1 48.540 Loss: 1.4636\n",
      "Best Prec@1: 48.540\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [10][0/390]\tTime 0.004 (0.004)\tLoss 1.4356 (1.4356)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [10][78/390]\tTime 0.002 (0.003)\tLoss 1.3544 (1.4152)\tPrec@1 53.906 (51.444)\n",
      "Epoch: [10][156/390]\tTime 0.002 (0.003)\tLoss 1.4932 (1.4295)\tPrec@1 52.344 (50.766)\n",
      "Epoch: [10][234/390]\tTime 0.002 (0.003)\tLoss 1.4627 (1.4312)\tPrec@1 44.531 (50.509)\n",
      "Epoch: [10][312/390]\tTime 0.002 (0.003)\tLoss 1.4602 (1.4293)\tPrec@1 52.344 (50.604)\n",
      "Epoch: [10][390/390]\tTime 0.002 (0.003)\tLoss 1.2831 (1.4287)\tPrec@1 60.000 (50.622)\n",
      "EPOCH: 10 train Results: Prec@1 50.622 Loss: 1.4287\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.3934 (1.3934)\tPrec@1 50.000 (50.000)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5305 (1.4259)\tPrec@1 25.000 (49.650)\n",
      "EPOCH: 10 val Results: Prec@1 49.650 Loss: 1.4259\n",
      "Best Prec@1: 49.650\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [11][0/390]\tTime 0.002 (0.002)\tLoss 1.3277 (1.3277)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [11][78/390]\tTime 0.009 (0.003)\tLoss 1.3634 (1.3851)\tPrec@1 51.562 (52.245)\n",
      "Epoch: [11][156/390]\tTime 0.008 (0.003)\tLoss 1.3062 (1.3904)\tPrec@1 53.906 (51.971)\n",
      "Epoch: [11][234/390]\tTime 0.004 (0.003)\tLoss 1.4672 (1.3945)\tPrec@1 50.781 (51.745)\n",
      "Epoch: [11][312/390]\tTime 0.005 (0.003)\tLoss 1.2731 (1.3934)\tPrec@1 60.156 (51.934)\n",
      "Epoch: [11][390/390]\tTime 0.003 (0.003)\tLoss 1.4546 (1.3950)\tPrec@1 48.750 (51.734)\n",
      "EPOCH: 11 train Results: Prec@1 51.734 Loss: 1.3950\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3406 (1.3406)\tPrec@1 50.000 (50.000)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.5129 (1.4033)\tPrec@1 31.250 (50.710)\n",
      "EPOCH: 11 val Results: Prec@1 50.710 Loss: 1.4033\n",
      "Best Prec@1: 50.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [12][0/390]\tTime 0.008 (0.008)\tLoss 1.3373 (1.3373)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [12][78/390]\tTime 0.004 (0.004)\tLoss 1.2698 (1.3540)\tPrec@1 55.469 (53.610)\n",
      "Epoch: [12][156/390]\tTime 0.004 (0.004)\tLoss 1.4288 (1.3602)\tPrec@1 53.125 (53.210)\n",
      "Epoch: [12][234/390]\tTime 0.022 (0.003)\tLoss 1.2068 (1.3617)\tPrec@1 60.938 (52.872)\n",
      "Epoch: [12][312/390]\tTime 0.002 (0.003)\tLoss 1.3963 (1.3641)\tPrec@1 52.344 (52.636)\n",
      "Epoch: [12][390/390]\tTime 0.003 (0.003)\tLoss 1.2294 (1.3671)\tPrec@1 60.000 (52.564)\n",
      "EPOCH: 12 train Results: Prec@1 52.564 Loss: 1.3671\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.3086 (1.3086)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3157 (1.3765)\tPrec@1 43.750 (51.630)\n",
      "EPOCH: 12 val Results: Prec@1 51.630 Loss: 1.3765\n",
      "Best Prec@1: 51.630\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [13][0/390]\tTime 0.006 (0.006)\tLoss 1.3183 (1.3183)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.2663 (1.3308)\tPrec@1 52.344 (53.600)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.003)\tLoss 1.2375 (1.3293)\tPrec@1 57.031 (54.120)\n",
      "Epoch: [13][234/390]\tTime 0.003 (0.003)\tLoss 1.4448 (1.3365)\tPrec@1 45.312 (53.803)\n",
      "Epoch: [13][312/390]\tTime 0.003 (0.003)\tLoss 1.3267 (1.3425)\tPrec@1 51.562 (53.365)\n",
      "Epoch: [13][390/390]\tTime 0.001 (0.003)\tLoss 1.3160 (1.3450)\tPrec@1 48.750 (53.292)\n",
      "EPOCH: 13 train Results: Prec@1 53.292 Loss: 1.3450\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2861 (1.2861)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4394 (1.3613)\tPrec@1 37.500 (51.870)\n",
      "EPOCH: 13 val Results: Prec@1 51.870 Loss: 1.3613\n",
      "Best Prec@1: 51.870\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [14][0/390]\tTime 0.002 (0.002)\tLoss 1.2310 (1.2310)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [14][78/390]\tTime 0.002 (0.003)\tLoss 1.3282 (1.3080)\tPrec@1 53.906 (54.589)\n",
      "Epoch: [14][156/390]\tTime 0.002 (0.003)\tLoss 1.3704 (1.3132)\tPrec@1 52.344 (54.284)\n",
      "Epoch: [14][234/390]\tTime 0.003 (0.003)\tLoss 1.3695 (1.3230)\tPrec@1 53.125 (53.730)\n",
      "Epoch: [14][312/390]\tTime 0.003 (0.003)\tLoss 1.2319 (1.3249)\tPrec@1 53.125 (53.727)\n",
      "Epoch: [14][390/390]\tTime 0.001 (0.003)\tLoss 1.4837 (1.3281)\tPrec@1 48.750 (53.580)\n",
      "EPOCH: 14 train Results: Prec@1 53.580 Loss: 1.3281\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2465 (1.2465)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2850 (1.3485)\tPrec@1 37.500 (52.260)\n",
      "EPOCH: 14 val Results: Prec@1 52.260 Loss: 1.3485\n",
      "Best Prec@1: 52.260\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [15][0/390]\tTime 0.002 (0.002)\tLoss 1.2006 (1.2006)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [15][78/390]\tTime 0.002 (0.003)\tLoss 1.4104 (1.2808)\tPrec@1 49.219 (55.439)\n",
      "Epoch: [15][156/390]\tTime 0.002 (0.003)\tLoss 1.1833 (1.2915)\tPrec@1 59.375 (55.329)\n",
      "Epoch: [15][234/390]\tTime 0.004 (0.003)\tLoss 1.2644 (1.2999)\tPrec@1 59.375 (54.741)\n",
      "Epoch: [15][312/390]\tTime 0.002 (0.003)\tLoss 1.4677 (1.3059)\tPrec@1 47.656 (54.415)\n",
      "Epoch: [15][390/390]\tTime 0.001 (0.003)\tLoss 1.2634 (1.3085)\tPrec@1 53.750 (54.394)\n",
      "EPOCH: 15 train Results: Prec@1 54.394 Loss: 1.3085\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2043 (1.2043)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1997 (1.3327)\tPrec@1 43.750 (52.740)\n",
      "EPOCH: 15 val Results: Prec@1 52.740 Loss: 1.3327\n",
      "Best Prec@1: 52.740\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [16][0/390]\tTime 0.003 (0.003)\tLoss 1.1712 (1.1712)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.003)\tLoss 1.2143 (1.2475)\tPrec@1 58.594 (56.913)\n",
      "Epoch: [16][156/390]\tTime 0.002 (0.003)\tLoss 1.2929 (1.2697)\tPrec@1 53.906 (55.802)\n",
      "Epoch: [16][234/390]\tTime 0.007 (0.003)\tLoss 1.4332 (1.2828)\tPrec@1 48.438 (55.213)\n",
      "Epoch: [16][312/390]\tTime 0.007 (0.003)\tLoss 1.2888 (1.2899)\tPrec@1 53.906 (54.922)\n",
      "Epoch: [16][390/390]\tTime 0.003 (0.003)\tLoss 1.4975 (1.2973)\tPrec@1 52.500 (54.676)\n",
      "EPOCH: 16 train Results: Prec@1 54.676 Loss: 1.2973\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1723 (1.1723)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3152 (1.3287)\tPrec@1 37.500 (53.320)\n",
      "EPOCH: 16 val Results: Prec@1 53.320 Loss: 1.3287\n",
      "Best Prec@1: 53.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [17][0/390]\tTime 0.003 (0.003)\tLoss 1.2942 (1.2942)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [17][78/390]\tTime 0.003 (0.003)\tLoss 1.3306 (1.2544)\tPrec@1 53.125 (56.794)\n",
      "Epoch: [17][156/390]\tTime 0.002 (0.003)\tLoss 1.4411 (1.2647)\tPrec@1 49.219 (56.170)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.003)\tLoss 1.3188 (1.2739)\tPrec@1 54.688 (55.725)\n",
      "Epoch: [17][312/390]\tTime 0.006 (0.003)\tLoss 1.3304 (1.2783)\tPrec@1 53.906 (55.479)\n",
      "Epoch: [17][390/390]\tTime 0.001 (0.003)\tLoss 1.2118 (1.2848)\tPrec@1 63.750 (55.288)\n",
      "EPOCH: 17 train Results: Prec@1 55.288 Loss: 1.2848\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2122 (1.2122)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1694 (1.3196)\tPrec@1 37.500 (53.410)\n",
      "EPOCH: 17 val Results: Prec@1 53.410 Loss: 1.3196\n",
      "Best Prec@1: 53.410\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [18][0/390]\tTime 0.004 (0.004)\tLoss 1.1946 (1.1946)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [18][78/390]\tTime 0.002 (0.003)\tLoss 1.2744 (1.2453)\tPrec@1 55.469 (56.744)\n",
      "Epoch: [18][156/390]\tTime 0.002 (0.003)\tLoss 1.2000 (1.2583)\tPrec@1 61.719 (56.111)\n",
      "Epoch: [18][234/390]\tTime 0.002 (0.003)\tLoss 1.1433 (1.2749)\tPrec@1 59.375 (55.512)\n",
      "Epoch: [18][312/390]\tTime 0.007 (0.003)\tLoss 1.2714 (1.2771)\tPrec@1 57.812 (55.431)\n",
      "Epoch: [18][390/390]\tTime 0.011 (0.003)\tLoss 1.1708 (1.2796)\tPrec@1 61.250 (55.372)\n",
      "EPOCH: 18 train Results: Prec@1 55.372 Loss: 1.2796\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1808 (1.1808)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3140 (1.3071)\tPrec@1 50.000 (53.490)\n",
      "EPOCH: 18 val Results: Prec@1 53.490 Loss: 1.3071\n",
      "Best Prec@1: 53.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [19][0/390]\tTime 0.003 (0.003)\tLoss 1.2323 (1.2323)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [19][78/390]\tTime 0.004 (0.003)\tLoss 1.4044 (1.2215)\tPrec@1 50.000 (57.882)\n",
      "Epoch: [19][156/390]\tTime 0.003 (0.003)\tLoss 1.2105 (1.2456)\tPrec@1 57.812 (56.937)\n",
      "Epoch: [19][234/390]\tTime 0.002 (0.003)\tLoss 1.2609 (1.2515)\tPrec@1 56.250 (56.629)\n",
      "Epoch: [19][312/390]\tTime 0.003 (0.003)\tLoss 1.1265 (1.2633)\tPrec@1 60.938 (56.043)\n",
      "Epoch: [19][390/390]\tTime 0.001 (0.003)\tLoss 1.4561 (1.2682)\tPrec@1 48.750 (55.668)\n",
      "EPOCH: 19 train Results: Prec@1 55.668 Loss: 1.2682\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2142 (1.2142)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5328 (1.3055)\tPrec@1 25.000 (53.620)\n",
      "EPOCH: 19 val Results: Prec@1 53.620 Loss: 1.3055\n",
      "Best Prec@1: 53.620\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [20][0/390]\tTime 0.003 (0.003)\tLoss 1.2077 (1.2077)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [20][78/390]\tTime 0.003 (0.003)\tLoss 1.0808 (1.2389)\tPrec@1 58.594 (57.071)\n",
      "Epoch: [20][156/390]\tTime 0.009 (0.003)\tLoss 1.1797 (1.2440)\tPrec@1 54.688 (56.807)\n",
      "Epoch: [20][234/390]\tTime 0.004 (0.004)\tLoss 1.2548 (1.2563)\tPrec@1 56.250 (56.031)\n",
      "Epoch: [20][312/390]\tTime 0.010 (0.003)\tLoss 1.3231 (1.2590)\tPrec@1 52.344 (55.936)\n",
      "Epoch: [20][390/390]\tTime 0.003 (0.003)\tLoss 1.3673 (1.2616)\tPrec@1 53.750 (55.770)\n",
      "EPOCH: 20 train Results: Prec@1 55.770 Loss: 1.2616\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1719 (1.1719)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2444 (1.3001)\tPrec@1 50.000 (53.870)\n",
      "EPOCH: 20 val Results: Prec@1 53.870 Loss: 1.3001\n",
      "Best Prec@1: 53.870\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [21][0/390]\tTime 0.006 (0.006)\tLoss 1.3077 (1.3077)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [21][78/390]\tTime 0.006 (0.003)\tLoss 1.3507 (1.2134)\tPrec@1 52.344 (58.396)\n",
      "Epoch: [21][156/390]\tTime 0.002 (0.003)\tLoss 1.3618 (1.2306)\tPrec@1 50.781 (57.404)\n",
      "Epoch: [21][234/390]\tTime 0.002 (0.003)\tLoss 1.2522 (1.2420)\tPrec@1 58.594 (56.951)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.1937 (1.2531)\tPrec@1 60.156 (56.467)\n",
      "Epoch: [21][390/390]\tTime 0.002 (0.003)\tLoss 1.2527 (1.2596)\tPrec@1 60.000 (56.118)\n",
      "EPOCH: 21 train Results: Prec@1 56.118 Loss: 1.2596\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2377 (1.2377)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1476 (1.2950)\tPrec@1 56.250 (54.210)\n",
      "EPOCH: 21 val Results: Prec@1 54.210 Loss: 1.2950\n",
      "Best Prec@1: 54.210\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [22][0/390]\tTime 0.002 (0.002)\tLoss 1.2462 (1.2462)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [22][78/390]\tTime 0.002 (0.003)\tLoss 1.1894 (1.2203)\tPrec@1 60.156 (58.030)\n",
      "Epoch: [22][156/390]\tTime 0.008 (0.003)\tLoss 1.1672 (1.2292)\tPrec@1 55.469 (57.370)\n",
      "Epoch: [22][234/390]\tTime 0.004 (0.003)\tLoss 1.2370 (1.2383)\tPrec@1 57.031 (56.932)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.003)\tLoss 1.2323 (1.2462)\tPrec@1 61.719 (56.564)\n",
      "Epoch: [22][390/390]\tTime 0.002 (0.003)\tLoss 1.1614 (1.2540)\tPrec@1 62.500 (56.200)\n",
      "EPOCH: 22 train Results: Prec@1 56.200 Loss: 1.2540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1902 (1.1902)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1482 (1.2935)\tPrec@1 56.250 (54.250)\n",
      "EPOCH: 22 val Results: Prec@1 54.250 Loss: 1.2935\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [23][0/390]\tTime 0.003 (0.003)\tLoss 1.2041 (1.2041)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [23][78/390]\tTime 0.002 (0.003)\tLoss 1.3602 (1.2158)\tPrec@1 54.688 (58.050)\n",
      "Epoch: [23][156/390]\tTime 0.006 (0.003)\tLoss 1.4187 (1.2317)\tPrec@1 54.688 (57.414)\n",
      "Epoch: [23][234/390]\tTime 0.002 (0.003)\tLoss 1.1843 (1.2328)\tPrec@1 63.281 (57.330)\n",
      "Epoch: [23][312/390]\tTime 0.003 (0.003)\tLoss 1.3568 (1.2424)\tPrec@1 54.688 (56.941)\n",
      "Epoch: [23][390/390]\tTime 0.002 (0.003)\tLoss 1.3833 (1.2504)\tPrec@1 50.000 (56.522)\n",
      "EPOCH: 23 train Results: Prec@1 56.522 Loss: 1.2504\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1708 (1.1708)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3363 (1.3008)\tPrec@1 37.500 (53.650)\n",
      "EPOCH: 23 val Results: Prec@1 53.650 Loss: 1.3008\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [24][0/390]\tTime 0.002 (0.002)\tLoss 1.2027 (1.2027)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.2558 (1.1900)\tPrec@1 60.156 (59.009)\n",
      "Epoch: [24][156/390]\tTime 0.002 (0.003)\tLoss 1.2777 (1.2153)\tPrec@1 57.031 (57.837)\n",
      "Epoch: [24][234/390]\tTime 0.002 (0.003)\tLoss 1.3026 (1.2324)\tPrec@1 54.688 (57.241)\n",
      "Epoch: [24][312/390]\tTime 0.002 (0.003)\tLoss 1.3514 (1.2400)\tPrec@1 52.344 (56.919)\n",
      "Epoch: [24][390/390]\tTime 0.002 (0.003)\tLoss 1.3063 (1.2435)\tPrec@1 55.000 (56.728)\n",
      "EPOCH: 24 train Results: Prec@1 56.728 Loss: 1.2435\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1712 (1.1712)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2113 (1.2939)\tPrec@1 50.000 (53.840)\n",
      "EPOCH: 24 val Results: Prec@1 53.840 Loss: 1.2939\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [25][0/390]\tTime 0.003 (0.003)\tLoss 1.2126 (1.2126)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [25][78/390]\tTime 0.003 (0.003)\tLoss 1.1414 (1.2019)\tPrec@1 58.594 (58.248)\n",
      "Epoch: [25][156/390]\tTime 0.005 (0.003)\tLoss 1.2451 (1.2187)\tPrec@1 57.031 (57.633)\n",
      "Epoch: [25][234/390]\tTime 0.002 (0.003)\tLoss 1.3126 (1.2280)\tPrec@1 50.000 (57.111)\n",
      "Epoch: [25][312/390]\tTime 0.003 (0.003)\tLoss 1.3784 (1.2355)\tPrec@1 50.781 (56.817)\n",
      "Epoch: [25][390/390]\tTime 0.004 (0.003)\tLoss 1.3782 (1.2395)\tPrec@1 48.750 (56.754)\n",
      "EPOCH: 25 train Results: Prec@1 56.754 Loss: 1.2395\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1392 (1.1392)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4426 (1.2920)\tPrec@1 31.250 (53.750)\n",
      "EPOCH: 25 val Results: Prec@1 53.750 Loss: 1.2920\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [26][0/390]\tTime 0.005 (0.005)\tLoss 1.2427 (1.2427)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [26][78/390]\tTime 0.005 (0.003)\tLoss 1.2066 (1.1891)\tPrec@1 57.031 (59.108)\n",
      "Epoch: [26][156/390]\tTime 0.007 (0.003)\tLoss 1.2069 (1.2083)\tPrec@1 55.469 (58.639)\n",
      "Epoch: [26][234/390]\tTime 0.002 (0.003)\tLoss 1.1917 (1.2200)\tPrec@1 58.594 (57.902)\n",
      "Epoch: [26][312/390]\tTime 0.009 (0.003)\tLoss 1.2242 (1.2308)\tPrec@1 54.688 (57.483)\n",
      "Epoch: [26][390/390]\tTime 0.001 (0.003)\tLoss 1.3103 (1.2358)\tPrec@1 55.000 (57.156)\n",
      "EPOCH: 26 train Results: Prec@1 57.156 Loss: 1.2358\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2012 (1.2012)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3542 (1.2925)\tPrec@1 43.750 (54.330)\n",
      "EPOCH: 26 val Results: Prec@1 54.330 Loss: 1.2925\n",
      "Best Prec@1: 54.330\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [27][0/390]\tTime 0.002 (0.002)\tLoss 1.3190 (1.3190)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [27][78/390]\tTime 0.002 (0.003)\tLoss 1.1632 (1.1946)\tPrec@1 60.156 (58.574)\n",
      "Epoch: [27][156/390]\tTime 0.002 (0.003)\tLoss 1.0880 (1.2080)\tPrec@1 57.031 (57.678)\n",
      "Epoch: [27][234/390]\tTime 0.002 (0.003)\tLoss 1.0996 (1.2173)\tPrec@1 64.062 (57.320)\n",
      "Epoch: [27][312/390]\tTime 0.002 (0.003)\tLoss 1.1819 (1.2264)\tPrec@1 58.594 (57.079)\n",
      "Epoch: [27][390/390]\tTime 0.005 (0.003)\tLoss 1.5788 (1.2350)\tPrec@1 48.750 (56.818)\n",
      "EPOCH: 27 train Results: Prec@1 56.818 Loss: 1.2350\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1502 (1.1502)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.3372 (1.2886)\tPrec@1 50.000 (53.970)\n",
      "EPOCH: 27 val Results: Prec@1 53.970 Loss: 1.2886\n",
      "Best Prec@1: 54.330\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [28][0/390]\tTime 0.005 (0.005)\tLoss 1.1592 (1.1592)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [28][78/390]\tTime 0.002 (0.003)\tLoss 1.1381 (1.1914)\tPrec@1 67.188 (59.335)\n",
      "Epoch: [28][156/390]\tTime 0.003 (0.003)\tLoss 1.2066 (1.2008)\tPrec@1 63.281 (58.753)\n",
      "Epoch: [28][234/390]\tTime 0.004 (0.003)\tLoss 1.2028 (1.2128)\tPrec@1 53.906 (58.268)\n",
      "Epoch: [28][312/390]\tTime 0.003 (0.003)\tLoss 1.2703 (1.2211)\tPrec@1 54.688 (57.862)\n",
      "Epoch: [28][390/390]\tTime 0.003 (0.003)\tLoss 1.4067 (1.2284)\tPrec@1 42.500 (57.446)\n",
      "EPOCH: 28 train Results: Prec@1 57.446 Loss: 1.2284\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1834 (1.1834)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1461 (1.2817)\tPrec@1 50.000 (54.630)\n",
      "EPOCH: 28 val Results: Prec@1 54.630 Loss: 1.2817\n",
      "Best Prec@1: 54.630\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [29][0/390]\tTime 0.003 (0.003)\tLoss 1.1016 (1.1016)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [29][78/390]\tTime 0.002 (0.003)\tLoss 1.3245 (1.2001)\tPrec@1 54.688 (58.752)\n",
      "Epoch: [29][156/390]\tTime 0.013 (0.003)\tLoss 1.1831 (1.2082)\tPrec@1 56.250 (58.395)\n",
      "Epoch: [29][234/390]\tTime 0.002 (0.003)\tLoss 1.1496 (1.2175)\tPrec@1 67.188 (57.949)\n",
      "Epoch: [29][312/390]\tTime 0.005 (0.003)\tLoss 1.2090 (1.2256)\tPrec@1 53.906 (57.500)\n",
      "Epoch: [29][390/390]\tTime 0.001 (0.003)\tLoss 1.3753 (1.2306)\tPrec@1 46.250 (57.292)\n",
      "EPOCH: 29 train Results: Prec@1 57.292 Loss: 1.2306\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1090 (1.1090)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1134 (1.2782)\tPrec@1 50.000 (54.990)\n",
      "EPOCH: 29 val Results: Prec@1 54.990 Loss: 1.2782\n",
      "Best Prec@1: 54.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [30][0/390]\tTime 0.004 (0.004)\tLoss 1.2258 (1.2258)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.003)\tLoss 1.2279 (1.1886)\tPrec@1 54.688 (58.653)\n",
      "Epoch: [30][156/390]\tTime 0.003 (0.003)\tLoss 1.1968 (1.2022)\tPrec@1 56.250 (58.425)\n",
      "Epoch: [30][234/390]\tTime 0.002 (0.003)\tLoss 1.2190 (1.2083)\tPrec@1 56.250 (57.989)\n",
      "Epoch: [30][312/390]\tTime 0.003 (0.003)\tLoss 1.1369 (1.2163)\tPrec@1 63.281 (57.578)\n",
      "Epoch: [30][390/390]\tTime 0.001 (0.003)\tLoss 1.1593 (1.2241)\tPrec@1 62.500 (57.290)\n",
      "EPOCH: 30 train Results: Prec@1 57.290 Loss: 1.2241\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1556 (1.1556)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2441 (1.2843)\tPrec@1 50.000 (54.690)\n",
      "EPOCH: 30 val Results: Prec@1 54.690 Loss: 1.2843\n",
      "Best Prec@1: 54.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [31][0/390]\tTime 0.004 (0.004)\tLoss 1.1747 (1.1747)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [31][78/390]\tTime 0.002 (0.003)\tLoss 1.2008 (1.1950)\tPrec@1 56.250 (58.633)\n",
      "Epoch: [31][156/390]\tTime 0.006 (0.003)\tLoss 1.1883 (1.2041)\tPrec@1 58.594 (58.400)\n",
      "Epoch: [31][234/390]\tTime 0.004 (0.003)\tLoss 1.2127 (1.2114)\tPrec@1 57.812 (58.168)\n",
      "Epoch: [31][312/390]\tTime 0.007 (0.003)\tLoss 1.1834 (1.2185)\tPrec@1 60.938 (57.835)\n",
      "Epoch: [31][390/390]\tTime 0.002 (0.003)\tLoss 1.2975 (1.2250)\tPrec@1 57.500 (57.458)\n",
      "EPOCH: 31 train Results: Prec@1 57.458 Loss: 1.2250\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2190 (1.2190)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2836 (1.2900)\tPrec@1 37.500 (54.180)\n",
      "EPOCH: 31 val Results: Prec@1 54.180 Loss: 1.2900\n",
      "Best Prec@1: 54.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [32][0/390]\tTime 0.011 (0.011)\tLoss 1.2279 (1.2279)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [32][78/390]\tTime 0.003 (0.003)\tLoss 1.1756 (1.1876)\tPrec@1 59.375 (59.118)\n",
      "Epoch: [32][156/390]\tTime 0.003 (0.003)\tLoss 0.9978 (1.1966)\tPrec@1 64.844 (58.444)\n",
      "Epoch: [32][234/390]\tTime 0.005 (0.003)\tLoss 1.2499 (1.2087)\tPrec@1 60.938 (57.862)\n",
      "Epoch: [32][312/390]\tTime 0.006 (0.003)\tLoss 1.1654 (1.2131)\tPrec@1 62.500 (57.753)\n",
      "Epoch: [32][390/390]\tTime 0.001 (0.003)\tLoss 1.1802 (1.2218)\tPrec@1 58.750 (57.480)\n",
      "EPOCH: 32 train Results: Prec@1 57.480 Loss: 1.2218\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2015 (1.2015)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2753 (1.2835)\tPrec@1 56.250 (54.890)\n",
      "EPOCH: 32 val Results: Prec@1 54.890 Loss: 1.2835\n",
      "Best Prec@1: 54.990\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [33][0/390]\tTime 0.004 (0.004)\tLoss 1.2607 (1.2607)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [33][78/390]\tTime 0.003 (0.003)\tLoss 1.3247 (1.1949)\tPrec@1 52.344 (58.534)\n",
      "Epoch: [33][156/390]\tTime 0.007 (0.003)\tLoss 1.2547 (1.2060)\tPrec@1 50.781 (57.892)\n",
      "Epoch: [33][234/390]\tTime 0.004 (0.003)\tLoss 1.2908 (1.2085)\tPrec@1 59.375 (57.902)\n",
      "Epoch: [33][312/390]\tTime 0.003 (0.003)\tLoss 1.2120 (1.2163)\tPrec@1 58.594 (57.535)\n",
      "Epoch: [33][390/390]\tTime 0.004 (0.003)\tLoss 1.2494 (1.2205)\tPrec@1 53.750 (57.434)\n",
      "EPOCH: 33 train Results: Prec@1 57.434 Loss: 1.2205\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1930 (1.1930)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3353 (1.2762)\tPrec@1 43.750 (55.060)\n",
      "EPOCH: 33 val Results: Prec@1 55.060 Loss: 1.2762\n",
      "Best Prec@1: 55.060\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [34][0/390]\tTime 0.002 (0.002)\tLoss 1.1239 (1.1239)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [34][78/390]\tTime 0.004 (0.003)\tLoss 1.2651 (1.1673)\tPrec@1 51.562 (60.028)\n",
      "Epoch: [34][156/390]\tTime 0.003 (0.003)\tLoss 1.2616 (1.1890)\tPrec@1 55.469 (58.862)\n",
      "Epoch: [34][234/390]\tTime 0.002 (0.003)\tLoss 1.1928 (1.1977)\tPrec@1 57.812 (58.511)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.1850 (1.2087)\tPrec@1 64.062 (58.105)\n",
      "Epoch: [34][390/390]\tTime 0.002 (0.003)\tLoss 1.2184 (1.2165)\tPrec@1 51.250 (57.720)\n",
      "EPOCH: 34 train Results: Prec@1 57.720 Loss: 1.2165\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1508 (1.1508)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2498 (1.2767)\tPrec@1 43.750 (55.250)\n",
      "EPOCH: 34 val Results: Prec@1 55.250 Loss: 1.2767\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [35][0/390]\tTime 0.005 (0.005)\tLoss 1.1959 (1.1959)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [35][78/390]\tTime 0.003 (0.003)\tLoss 1.2540 (1.1760)\tPrec@1 55.469 (59.484)\n",
      "Epoch: [35][156/390]\tTime 0.003 (0.003)\tLoss 1.4318 (1.1875)\tPrec@1 49.219 (58.658)\n",
      "Epoch: [35][234/390]\tTime 0.002 (0.003)\tLoss 1.1797 (1.2041)\tPrec@1 62.500 (57.979)\n",
      "Epoch: [35][312/390]\tTime 0.005 (0.003)\tLoss 1.2246 (1.2109)\tPrec@1 58.594 (57.817)\n",
      "Epoch: [35][390/390]\tTime 0.001 (0.003)\tLoss 1.1884 (1.2163)\tPrec@1 61.250 (57.718)\n",
      "EPOCH: 35 train Results: Prec@1 57.718 Loss: 1.2163\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1770 (1.1770)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2683 (1.2858)\tPrec@1 43.750 (54.540)\n",
      "EPOCH: 35 val Results: Prec@1 54.540 Loss: 1.2858\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [36][0/390]\tTime 0.003 (0.003)\tLoss 1.3414 (1.3414)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [36][78/390]\tTime 0.002 (0.003)\tLoss 1.2640 (1.1816)\tPrec@1 52.344 (58.920)\n",
      "Epoch: [36][156/390]\tTime 0.004 (0.003)\tLoss 1.2713 (1.1921)\tPrec@1 54.688 (58.703)\n",
      "Epoch: [36][234/390]\tTime 0.004 (0.003)\tLoss 1.2284 (1.1997)\tPrec@1 55.469 (58.301)\n",
      "Epoch: [36][312/390]\tTime 0.002 (0.003)\tLoss 1.2378 (1.2066)\tPrec@1 63.281 (58.075)\n",
      "Epoch: [36][390/390]\tTime 0.003 (0.003)\tLoss 1.2879 (1.2137)\tPrec@1 53.750 (57.724)\n",
      "EPOCH: 36 train Results: Prec@1 57.724 Loss: 1.2137\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1748 (1.1748)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2513 (1.2727)\tPrec@1 50.000 (54.740)\n",
      "EPOCH: 36 val Results: Prec@1 54.740 Loss: 1.2727\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [37][0/390]\tTime 0.003 (0.003)\tLoss 1.0349 (1.0349)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [37][78/390]\tTime 0.003 (0.003)\tLoss 1.0753 (1.1594)\tPrec@1 62.500 (60.483)\n",
      "Epoch: [37][156/390]\tTime 0.007 (0.003)\tLoss 1.1401 (1.1836)\tPrec@1 58.594 (59.395)\n",
      "Epoch: [37][234/390]\tTime 0.003 (0.003)\tLoss 1.3818 (1.1995)\tPrec@1 50.000 (58.557)\n",
      "Epoch: [37][312/390]\tTime 0.004 (0.003)\tLoss 1.1246 (1.2104)\tPrec@1 62.500 (57.967)\n",
      "Epoch: [37][390/390]\tTime 0.001 (0.003)\tLoss 1.2474 (1.2131)\tPrec@1 53.750 (57.914)\n",
      "EPOCH: 37 train Results: Prec@1 57.914 Loss: 1.2131\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.2057 (1.2057)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2860 (1.2761)\tPrec@1 56.250 (55.180)\n",
      "EPOCH: 37 val Results: Prec@1 55.180 Loss: 1.2761\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [38][0/390]\tTime 0.003 (0.003)\tLoss 1.1408 (1.1408)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.003)\tLoss 1.2491 (1.1890)\tPrec@1 54.688 (58.584)\n",
      "Epoch: [38][156/390]\tTime 0.006 (0.003)\tLoss 1.1607 (1.1969)\tPrec@1 63.281 (58.514)\n",
      "Epoch: [38][234/390]\tTime 0.002 (0.003)\tLoss 1.2032 (1.2037)\tPrec@1 55.469 (58.275)\n",
      "Epoch: [38][312/390]\tTime 0.002 (0.003)\tLoss 1.3407 (1.2073)\tPrec@1 52.344 (58.025)\n",
      "Epoch: [38][390/390]\tTime 0.001 (0.003)\tLoss 1.1961 (1.2142)\tPrec@1 58.750 (57.778)\n",
      "EPOCH: 38 train Results: Prec@1 57.778 Loss: 1.2142\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1779 (1.1779)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2356 (1.2788)\tPrec@1 50.000 (54.840)\n",
      "EPOCH: 38 val Results: Prec@1 54.840 Loss: 1.2788\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [39][0/390]\tTime 0.003 (0.003)\tLoss 1.2553 (1.2553)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.003)\tLoss 1.2630 (1.1591)\tPrec@1 55.469 (59.780)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.003)\tLoss 1.1733 (1.1814)\tPrec@1 56.250 (59.062)\n",
      "Epoch: [39][234/390]\tTime 0.008 (0.003)\tLoss 1.2593 (1.1948)\tPrec@1 52.344 (58.514)\n",
      "Epoch: [39][312/390]\tTime 0.003 (0.003)\tLoss 1.5311 (1.2013)\tPrec@1 44.531 (58.247)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2517 (1.2100)\tPrec@1 53.750 (57.914)\n",
      "EPOCH: 39 train Results: Prec@1 57.914 Loss: 1.2100\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1283 (1.1283)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4240 (1.2768)\tPrec@1 56.250 (54.540)\n",
      "EPOCH: 39 val Results: Prec@1 54.540 Loss: 1.2768\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [40][0/390]\tTime 0.003 (0.003)\tLoss 1.1427 (1.1427)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.004)\tLoss 1.1581 (1.1666)\tPrec@1 55.469 (59.998)\n",
      "Epoch: [40][156/390]\tTime 0.005 (0.004)\tLoss 1.2694 (1.1737)\tPrec@1 51.562 (59.519)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.004)\tLoss 1.4154 (1.1870)\tPrec@1 48.438 (58.910)\n",
      "Epoch: [40][312/390]\tTime 0.007 (0.003)\tLoss 1.3963 (1.1997)\tPrec@1 50.000 (58.409)\n",
      "Epoch: [40][390/390]\tTime 0.001 (0.004)\tLoss 1.3813 (1.2089)\tPrec@1 51.250 (58.030)\n",
      "EPOCH: 40 train Results: Prec@1 58.030 Loss: 1.2089\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1980 (1.1980)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3826 (1.2681)\tPrec@1 56.250 (55.130)\n",
      "EPOCH: 40 val Results: Prec@1 55.130 Loss: 1.2681\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [41][0/390]\tTime 0.007 (0.007)\tLoss 1.2963 (1.2963)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [41][78/390]\tTime 0.007 (0.006)\tLoss 1.1824 (1.1593)\tPrec@1 57.031 (59.869)\n",
      "Epoch: [41][156/390]\tTime 0.004 (0.005)\tLoss 1.2751 (1.1812)\tPrec@1 54.688 (59.151)\n",
      "Epoch: [41][234/390]\tTime 0.005 (0.005)\tLoss 1.3213 (1.1901)\tPrec@1 53.125 (58.733)\n",
      "Epoch: [41][312/390]\tTime 0.002 (0.005)\tLoss 1.1866 (1.2003)\tPrec@1 61.719 (58.432)\n",
      "Epoch: [41][390/390]\tTime 0.001 (0.004)\tLoss 1.1477 (1.2063)\tPrec@1 62.500 (58.210)\n",
      "EPOCH: 41 train Results: Prec@1 58.210 Loss: 1.2063\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0969 (1.0969)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3143 (1.2671)\tPrec@1 56.250 (55.190)\n",
      "EPOCH: 41 val Results: Prec@1 55.190 Loss: 1.2671\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [42][0/390]\tTime 0.002 (0.002)\tLoss 1.2393 (1.2393)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [42][78/390]\tTime 0.002 (0.003)\tLoss 1.1612 (1.1614)\tPrec@1 59.375 (60.196)\n",
      "Epoch: [42][156/390]\tTime 0.004 (0.003)\tLoss 1.1469 (1.1709)\tPrec@1 62.500 (59.629)\n",
      "Epoch: [42][234/390]\tTime 0.004 (0.003)\tLoss 1.2153 (1.1880)\tPrec@1 57.031 (58.807)\n",
      "Epoch: [42][312/390]\tTime 0.008 (0.003)\tLoss 1.2929 (1.1980)\tPrec@1 58.594 (58.434)\n",
      "Epoch: [42][390/390]\tTime 0.005 (0.003)\tLoss 1.2123 (1.2064)\tPrec@1 58.750 (58.060)\n",
      "EPOCH: 42 train Results: Prec@1 58.060 Loss: 1.2064\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1625 (1.1625)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2785 (1.2649)\tPrec@1 50.000 (54.990)\n",
      "EPOCH: 42 val Results: Prec@1 54.990 Loss: 1.2649\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [43][0/390]\tTime 0.010 (0.010)\tLoss 1.2356 (1.2356)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [43][78/390]\tTime 0.002 (0.003)\tLoss 1.1653 (1.1741)\tPrec@1 60.938 (59.276)\n",
      "Epoch: [43][156/390]\tTime 0.002 (0.003)\tLoss 1.2720 (1.1882)\tPrec@1 50.781 (58.544)\n",
      "Epoch: [43][234/390]\tTime 0.003 (0.003)\tLoss 1.1324 (1.1930)\tPrec@1 57.031 (58.467)\n",
      "Epoch: [43][312/390]\tTime 0.002 (0.003)\tLoss 1.1543 (1.2034)\tPrec@1 64.062 (57.990)\n",
      "Epoch: [43][390/390]\tTime 0.001 (0.003)\tLoss 1.0691 (1.2071)\tPrec@1 65.000 (57.818)\n",
      "EPOCH: 43 train Results: Prec@1 57.818 Loss: 1.2071\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1792 (1.1792)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2862 (1.2743)\tPrec@1 31.250 (55.170)\n",
      "EPOCH: 43 val Results: Prec@1 55.170 Loss: 1.2743\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [44][0/390]\tTime 0.002 (0.002)\tLoss 1.2228 (1.2228)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.003)\tLoss 0.9948 (1.1626)\tPrec@1 66.406 (60.047)\n",
      "Epoch: [44][156/390]\tTime 0.002 (0.003)\tLoss 1.0945 (1.1712)\tPrec@1 60.156 (59.445)\n",
      "Epoch: [44][234/390]\tTime 0.002 (0.003)\tLoss 1.1339 (1.1889)\tPrec@1 60.938 (58.674)\n",
      "Epoch: [44][312/390]\tTime 0.002 (0.003)\tLoss 1.1909 (1.1993)\tPrec@1 58.594 (58.142)\n",
      "Epoch: [44][390/390]\tTime 0.004 (0.003)\tLoss 1.1295 (1.2056)\tPrec@1 57.500 (57.882)\n",
      "EPOCH: 44 train Results: Prec@1 57.882 Loss: 1.2056\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1135 (1.1135)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3938 (1.2667)\tPrec@1 43.750 (55.250)\n",
      "EPOCH: 44 val Results: Prec@1 55.250 Loss: 1.2667\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [45][0/390]\tTime 0.003 (0.003)\tLoss 1.2090 (1.2090)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [45][78/390]\tTime 0.003 (0.003)\tLoss 1.2449 (1.1672)\tPrec@1 57.031 (59.385)\n",
      "Epoch: [45][156/390]\tTime 0.010 (0.003)\tLoss 1.1892 (1.1726)\tPrec@1 57.031 (59.280)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.003)\tLoss 1.2657 (1.1853)\tPrec@1 60.156 (58.787)\n",
      "Epoch: [45][312/390]\tTime 0.007 (0.003)\tLoss 1.1643 (1.1944)\tPrec@1 57.031 (58.486)\n",
      "Epoch: [45][390/390]\tTime 0.003 (0.003)\tLoss 1.3688 (1.2002)\tPrec@1 51.250 (58.242)\n",
      "EPOCH: 45 train Results: Prec@1 58.242 Loss: 1.2002\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1631 (1.1631)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2766 (1.2662)\tPrec@1 56.250 (55.320)\n",
      "EPOCH: 45 val Results: Prec@1 55.320 Loss: 1.2662\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [46][0/390]\tTime 0.006 (0.006)\tLoss 1.2206 (1.2206)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [46][78/390]\tTime 0.002 (0.003)\tLoss 1.1939 (1.1674)\tPrec@1 59.375 (59.227)\n",
      "Epoch: [46][156/390]\tTime 0.002 (0.003)\tLoss 1.2856 (1.1734)\tPrec@1 54.688 (59.042)\n",
      "Epoch: [46][234/390]\tTime 0.003 (0.003)\tLoss 1.2804 (1.1867)\tPrec@1 50.000 (58.547)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.003)\tLoss 1.2214 (1.1926)\tPrec@1 57.031 (58.352)\n",
      "Epoch: [46][390/390]\tTime 0.001 (0.003)\tLoss 1.1739 (1.2020)\tPrec@1 53.750 (57.920)\n",
      "EPOCH: 46 train Results: Prec@1 57.920 Loss: 1.2020\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1869 (1.1869)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5157 (1.2714)\tPrec@1 25.000 (55.050)\n",
      "EPOCH: 46 val Results: Prec@1 55.050 Loss: 1.2714\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [47][0/390]\tTime 0.003 (0.003)\tLoss 1.4628 (1.4628)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [47][78/390]\tTime 0.003 (0.003)\tLoss 1.3353 (1.1560)\tPrec@1 53.906 (60.087)\n",
      "Epoch: [47][156/390]\tTime 0.002 (0.003)\tLoss 1.2117 (1.1744)\tPrec@1 58.594 (59.360)\n",
      "Epoch: [47][234/390]\tTime 0.003 (0.003)\tLoss 1.1832 (1.1856)\tPrec@1 57.031 (58.750)\n",
      "Epoch: [47][312/390]\tTime 0.002 (0.003)\tLoss 1.1713 (1.1886)\tPrec@1 55.469 (58.499)\n",
      "Epoch: [47][390/390]\tTime 0.001 (0.003)\tLoss 1.1609 (1.1963)\tPrec@1 56.250 (58.214)\n",
      "EPOCH: 47 train Results: Prec@1 58.214 Loss: 1.1963\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0444 (1.0444)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3231 (1.2650)\tPrec@1 56.250 (54.950)\n",
      "EPOCH: 47 val Results: Prec@1 54.950 Loss: 1.2650\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [48][0/390]\tTime 0.004 (0.004)\tLoss 1.1714 (1.1714)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [48][78/390]\tTime 0.003 (0.003)\tLoss 1.1976 (1.1694)\tPrec@1 61.719 (59.612)\n",
      "Epoch: [48][156/390]\tTime 0.003 (0.003)\tLoss 1.2880 (1.1797)\tPrec@1 55.469 (59.106)\n",
      "Epoch: [48][234/390]\tTime 0.005 (0.003)\tLoss 1.1573 (1.1828)\tPrec@1 57.031 (58.886)\n",
      "Epoch: [48][312/390]\tTime 0.008 (0.003)\tLoss 1.1583 (1.1913)\tPrec@1 61.719 (58.451)\n",
      "Epoch: [48][390/390]\tTime 0.002 (0.003)\tLoss 1.3884 (1.1991)\tPrec@1 47.500 (58.234)\n",
      "EPOCH: 48 train Results: Prec@1 58.234 Loss: 1.1991\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1825 (1.1825)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5233 (1.2774)\tPrec@1 43.750 (54.740)\n",
      "EPOCH: 48 val Results: Prec@1 54.740 Loss: 1.2774\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [49][0/390]\tTime 0.005 (0.005)\tLoss 1.1302 (1.1302)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [49][78/390]\tTime 0.003 (0.003)\tLoss 1.2057 (1.1452)\tPrec@1 60.938 (60.750)\n",
      "Epoch: [49][156/390]\tTime 0.011 (0.003)\tLoss 1.2687 (1.1680)\tPrec@1 56.250 (59.748)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.003)\tLoss 1.2245 (1.1820)\tPrec@1 57.812 (59.092)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.1930 (1.1894)\tPrec@1 50.781 (58.671)\n",
      "Epoch: [49][390/390]\tTime 0.003 (0.003)\tLoss 1.3515 (1.1988)\tPrec@1 46.250 (58.290)\n",
      "EPOCH: 49 train Results: Prec@1 58.290 Loss: 1.1988\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1824 (1.1824)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2156 (1.2699)\tPrec@1 50.000 (54.980)\n",
      "EPOCH: 49 val Results: Prec@1 54.980 Loss: 1.2699\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [50][0/390]\tTime 0.002 (0.002)\tLoss 1.1672 (1.1672)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [50][78/390]\tTime 0.004 (0.004)\tLoss 1.1759 (1.1542)\tPrec@1 60.156 (60.206)\n",
      "Epoch: [50][156/390]\tTime 0.002 (0.003)\tLoss 1.1745 (1.1673)\tPrec@1 60.938 (59.619)\n",
      "Epoch: [50][234/390]\tTime 0.002 (0.003)\tLoss 1.2326 (1.1816)\tPrec@1 54.688 (59.003)\n",
      "Epoch: [50][312/390]\tTime 0.002 (0.003)\tLoss 1.1628 (1.1900)\tPrec@1 56.250 (58.614)\n",
      "Epoch: [50][390/390]\tTime 0.001 (0.003)\tLoss 1.1165 (1.1989)\tPrec@1 62.500 (58.258)\n",
      "EPOCH: 50 train Results: Prec@1 58.258 Loss: 1.1989\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1821 (1.1821)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4661 (1.2717)\tPrec@1 31.250 (55.030)\n",
      "EPOCH: 50 val Results: Prec@1 55.030 Loss: 1.2717\n",
      "Best Prec@1: 55.320\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [51][0/390]\tTime 0.002 (0.002)\tLoss 1.2405 (1.2405)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [51][78/390]\tTime 0.002 (0.003)\tLoss 1.1045 (1.1579)\tPrec@1 61.719 (59.919)\n",
      "Epoch: [51][156/390]\tTime 0.002 (0.003)\tLoss 1.1468 (1.1728)\tPrec@1 59.375 (59.062)\n",
      "Epoch: [51][234/390]\tTime 0.003 (0.003)\tLoss 1.2900 (1.1801)\tPrec@1 54.688 (58.893)\n",
      "Epoch: [51][312/390]\tTime 0.002 (0.003)\tLoss 1.1459 (1.1859)\tPrec@1 66.406 (58.734)\n",
      "Epoch: [51][390/390]\tTime 0.002 (0.003)\tLoss 1.0796 (1.1949)\tPrec@1 61.250 (58.394)\n",
      "EPOCH: 51 train Results: Prec@1 58.394 Loss: 1.1949\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2027 (1.2027)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4032 (1.2638)\tPrec@1 31.250 (55.560)\n",
      "EPOCH: 51 val Results: Prec@1 55.560 Loss: 1.2638\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [52][0/390]\tTime 0.005 (0.005)\tLoss 1.0995 (1.0995)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [52][78/390]\tTime 0.004 (0.003)\tLoss 1.2090 (1.1531)\tPrec@1 58.594 (60.562)\n",
      "Epoch: [52][156/390]\tTime 0.002 (0.003)\tLoss 1.1909 (1.1614)\tPrec@1 59.375 (59.878)\n",
      "Epoch: [52][234/390]\tTime 0.002 (0.003)\tLoss 1.1200 (1.1765)\tPrec@1 61.719 (59.119)\n",
      "Epoch: [52][312/390]\tTime 0.002 (0.003)\tLoss 1.2758 (1.1874)\tPrec@1 54.688 (58.556)\n",
      "Epoch: [52][390/390]\tTime 0.002 (0.003)\tLoss 1.4842 (1.1975)\tPrec@1 48.750 (58.230)\n",
      "EPOCH: 52 train Results: Prec@1 58.230 Loss: 1.1975\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1669 (1.1669)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2689 (1.2806)\tPrec@1 43.750 (54.340)\n",
      "EPOCH: 52 val Results: Prec@1 54.340 Loss: 1.2806\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [53][0/390]\tTime 0.004 (0.004)\tLoss 1.2712 (1.2712)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [53][78/390]\tTime 0.003 (0.003)\tLoss 1.1456 (1.1496)\tPrec@1 64.844 (60.364)\n",
      "Epoch: [53][156/390]\tTime 0.004 (0.003)\tLoss 1.1834 (1.1614)\tPrec@1 61.719 (59.853)\n",
      "Epoch: [53][234/390]\tTime 0.009 (0.003)\tLoss 1.1711 (1.1741)\tPrec@1 56.250 (59.352)\n",
      "Epoch: [53][312/390]\tTime 0.003 (0.003)\tLoss 1.4444 (1.1828)\tPrec@1 53.125 (59.058)\n",
      "Epoch: [53][390/390]\tTime 0.001 (0.003)\tLoss 1.2901 (1.1903)\tPrec@1 56.250 (58.652)\n",
      "EPOCH: 53 train Results: Prec@1 58.652 Loss: 1.1903\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1787 (1.1787)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4626 (1.2733)\tPrec@1 56.250 (54.770)\n",
      "EPOCH: 53 val Results: Prec@1 54.770 Loss: 1.2733\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [54][0/390]\tTime 0.002 (0.002)\tLoss 1.1175 (1.1175)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [54][78/390]\tTime 0.002 (0.003)\tLoss 1.1335 (1.1539)\tPrec@1 60.156 (60.433)\n",
      "Epoch: [54][156/390]\tTime 0.002 (0.003)\tLoss 1.1059 (1.1724)\tPrec@1 65.625 (59.206)\n",
      "Epoch: [54][234/390]\tTime 0.002 (0.003)\tLoss 1.2065 (1.1790)\tPrec@1 60.938 (58.943)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2856 (1.1873)\tPrec@1 57.031 (58.544)\n",
      "Epoch: [54][390/390]\tTime 0.002 (0.003)\tLoss 1.2911 (1.1971)\tPrec@1 48.750 (58.158)\n",
      "EPOCH: 54 train Results: Prec@1 58.158 Loss: 1.1971\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1608 (1.1608)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4760 (1.2682)\tPrec@1 50.000 (55.140)\n",
      "EPOCH: 54 val Results: Prec@1 55.140 Loss: 1.2682\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.2164 (1.2164)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.2549 (1.1491)\tPrec@1 61.719 (61.392)\n",
      "Epoch: [55][156/390]\tTime 0.003 (0.003)\tLoss 1.2884 (1.1704)\tPrec@1 53.906 (59.718)\n",
      "Epoch: [55][234/390]\tTime 0.003 (0.003)\tLoss 1.2090 (1.1820)\tPrec@1 57.031 (59.139)\n",
      "Epoch: [55][312/390]\tTime 0.004 (0.003)\tLoss 1.1111 (1.1846)\tPrec@1 64.062 (58.881)\n",
      "Epoch: [55][390/390]\tTime 0.003 (0.003)\tLoss 1.1312 (1.1923)\tPrec@1 62.500 (58.532)\n",
      "EPOCH: 55 train Results: Prec@1 58.532 Loss: 1.1923\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1572 (1.1572)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4509 (1.2675)\tPrec@1 31.250 (54.720)\n",
      "EPOCH: 55 val Results: Prec@1 54.720 Loss: 1.2675\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [56][0/390]\tTime 0.004 (0.004)\tLoss 1.0891 (1.0891)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [56][78/390]\tTime 0.003 (0.003)\tLoss 1.1325 (1.1641)\tPrec@1 60.938 (59.672)\n",
      "Epoch: [56][156/390]\tTime 0.002 (0.003)\tLoss 1.0989 (1.1671)\tPrec@1 65.625 (59.435)\n",
      "Epoch: [56][234/390]\tTime 0.004 (0.003)\tLoss 1.2376 (1.1740)\tPrec@1 60.156 (59.259)\n",
      "Epoch: [56][312/390]\tTime 0.002 (0.003)\tLoss 1.0720 (1.1827)\tPrec@1 64.062 (58.866)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.003)\tLoss 1.2713 (1.1901)\tPrec@1 61.250 (58.620)\n",
      "EPOCH: 56 train Results: Prec@1 58.620 Loss: 1.1901\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1037 (1.1037)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2749 (1.2699)\tPrec@1 50.000 (54.610)\n",
      "EPOCH: 56 val Results: Prec@1 54.610 Loss: 1.2699\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [57][0/390]\tTime 0.005 (0.005)\tLoss 1.2075 (1.2075)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [57][78/390]\tTime 0.003 (0.003)\tLoss 1.2237 (1.1457)\tPrec@1 54.688 (60.216)\n",
      "Epoch: [57][156/390]\tTime 0.003 (0.003)\tLoss 1.1806 (1.1592)\tPrec@1 60.156 (59.509)\n",
      "Epoch: [57][234/390]\tTime 0.002 (0.005)\tLoss 1.2492 (1.1683)\tPrec@1 56.250 (59.086)\n",
      "Epoch: [57][312/390]\tTime 0.003 (0.005)\tLoss 1.2272 (1.1773)\tPrec@1 56.250 (58.739)\n",
      "Epoch: [57][390/390]\tTime 0.001 (0.005)\tLoss 1.1773 (1.1893)\tPrec@1 56.250 (58.394)\n",
      "EPOCH: 57 train Results: Prec@1 58.394 Loss: 1.1893\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1848 (1.1848)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4239 (1.2675)\tPrec@1 31.250 (55.320)\n",
      "EPOCH: 57 val Results: Prec@1 55.320 Loss: 1.2675\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [58][0/390]\tTime 0.004 (0.004)\tLoss 1.1296 (1.1296)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [58][78/390]\tTime 0.002 (0.004)\tLoss 1.1760 (1.1493)\tPrec@1 57.031 (59.889)\n",
      "Epoch: [58][156/390]\tTime 0.002 (0.004)\tLoss 1.2286 (1.1624)\tPrec@1 55.469 (59.554)\n",
      "Epoch: [58][234/390]\tTime 0.002 (0.003)\tLoss 1.1697 (1.1711)\tPrec@1 60.938 (59.189)\n",
      "Epoch: [58][312/390]\tTime 0.002 (0.003)\tLoss 1.1045 (1.1784)\tPrec@1 60.156 (58.931)\n",
      "Epoch: [58][390/390]\tTime 0.001 (0.003)\tLoss 1.2563 (1.1865)\tPrec@1 57.500 (58.668)\n",
      "EPOCH: 58 train Results: Prec@1 58.668 Loss: 1.1865\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1691 (1.1691)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4091 (1.2639)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 58 val Results: Prec@1 55.370 Loss: 1.2639\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [59][0/390]\tTime 0.003 (0.003)\tLoss 1.2049 (1.2049)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [59][78/390]\tTime 0.003 (0.003)\tLoss 1.1305 (1.1426)\tPrec@1 53.906 (59.929)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.2231 (1.1556)\tPrec@1 53.125 (59.325)\n",
      "Epoch: [59][234/390]\tTime 0.004 (0.003)\tLoss 1.1794 (1.1732)\tPrec@1 60.938 (58.797)\n",
      "Epoch: [59][312/390]\tTime 0.002 (0.003)\tLoss 1.1900 (1.1803)\tPrec@1 59.375 (58.684)\n",
      "Epoch: [59][390/390]\tTime 0.001 (0.003)\tLoss 1.2000 (1.1894)\tPrec@1 61.250 (58.294)\n",
      "EPOCH: 59 train Results: Prec@1 58.294 Loss: 1.1894\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1958 (1.1958)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2071 (1.2713)\tPrec@1 56.250 (54.830)\n",
      "EPOCH: 59 val Results: Prec@1 54.830 Loss: 1.2713\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [60][0/390]\tTime 0.003 (0.003)\tLoss 1.1028 (1.1028)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [60][78/390]\tTime 0.002 (0.004)\tLoss 1.0910 (1.1544)\tPrec@1 60.156 (60.206)\n",
      "Epoch: [60][156/390]\tTime 0.003 (0.004)\tLoss 1.0982 (1.1657)\tPrec@1 61.719 (59.778)\n",
      "Epoch: [60][234/390]\tTime 0.006 (0.003)\tLoss 1.2534 (1.1749)\tPrec@1 55.469 (59.205)\n",
      "Epoch: [60][312/390]\tTime 0.002 (0.003)\tLoss 1.1214 (1.1847)\tPrec@1 63.281 (58.721)\n",
      "Epoch: [60][390/390]\tTime 0.001 (0.003)\tLoss 1.2907 (1.1898)\tPrec@1 51.250 (58.506)\n",
      "EPOCH: 60 train Results: Prec@1 58.506 Loss: 1.1898\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1242 (1.1242)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2383 (1.2716)\tPrec@1 56.250 (55.180)\n",
      "EPOCH: 60 val Results: Prec@1 55.180 Loss: 1.2716\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [61][0/390]\tTime 0.006 (0.006)\tLoss 1.0209 (1.0209)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [61][78/390]\tTime 0.002 (0.003)\tLoss 1.2506 (1.1478)\tPrec@1 53.125 (60.453)\n",
      "Epoch: [61][156/390]\tTime 0.002 (0.003)\tLoss 1.2996 (1.1605)\tPrec@1 54.688 (59.743)\n",
      "Epoch: [61][234/390]\tTime 0.004 (0.003)\tLoss 1.0832 (1.1717)\tPrec@1 63.281 (59.438)\n",
      "Epoch: [61][312/390]\tTime 0.002 (0.003)\tLoss 1.2501 (1.1816)\tPrec@1 54.688 (59.148)\n",
      "Epoch: [61][390/390]\tTime 0.002 (0.003)\tLoss 1.2907 (1.1879)\tPrec@1 52.500 (58.780)\n",
      "EPOCH: 61 train Results: Prec@1 58.780 Loss: 1.1879\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1423 (1.1423)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3111 (1.2627)\tPrec@1 37.500 (55.050)\n",
      "EPOCH: 61 val Results: Prec@1 55.050 Loss: 1.2627\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [62][0/390]\tTime 0.003 (0.003)\tLoss 1.1286 (1.1286)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.2353 (1.1395)\tPrec@1 57.031 (61.145)\n",
      "Epoch: [62][156/390]\tTime 0.002 (0.003)\tLoss 1.0598 (1.1549)\tPrec@1 67.188 (60.415)\n",
      "Epoch: [62][234/390]\tTime 0.002 (0.003)\tLoss 1.2535 (1.1669)\tPrec@1 56.250 (59.731)\n",
      "Epoch: [62][312/390]\tTime 0.005 (0.003)\tLoss 1.3084 (1.1744)\tPrec@1 53.906 (59.263)\n",
      "Epoch: [62][390/390]\tTime 0.001 (0.003)\tLoss 1.0272 (1.1842)\tPrec@1 66.250 (58.880)\n",
      "EPOCH: 62 train Results: Prec@1 58.880 Loss: 1.1842\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1682 (1.1682)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2613 (1.2656)\tPrec@1 37.500 (55.710)\n",
      "EPOCH: 62 val Results: Prec@1 55.710 Loss: 1.2656\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [63][0/390]\tTime 0.004 (0.004)\tLoss 1.1008 (1.1008)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [63][78/390]\tTime 0.002 (0.003)\tLoss 1.2121 (1.1370)\tPrec@1 59.375 (61.244)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.003)\tLoss 1.2240 (1.1577)\tPrec@1 59.375 (59.753)\n",
      "Epoch: [63][234/390]\tTime 0.002 (0.003)\tLoss 1.2426 (1.1700)\tPrec@1 57.031 (59.269)\n",
      "Epoch: [63][312/390]\tTime 0.007 (0.003)\tLoss 1.3428 (1.1791)\tPrec@1 54.688 (58.928)\n",
      "Epoch: [63][390/390]\tTime 0.002 (0.003)\tLoss 1.2438 (1.1833)\tPrec@1 62.500 (58.766)\n",
      "EPOCH: 63 train Results: Prec@1 58.766 Loss: 1.1833\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1483 (1.1483)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4740 (1.2784)\tPrec@1 37.500 (54.460)\n",
      "EPOCH: 63 val Results: Prec@1 54.460 Loss: 1.2784\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [64][0/390]\tTime 0.002 (0.002)\tLoss 1.1415 (1.1415)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [64][78/390]\tTime 0.005 (0.003)\tLoss 1.0494 (1.1319)\tPrec@1 61.719 (60.829)\n",
      "Epoch: [64][156/390]\tTime 0.003 (0.003)\tLoss 1.2573 (1.1572)\tPrec@1 51.562 (60.047)\n",
      "Epoch: [64][234/390]\tTime 0.002 (0.003)\tLoss 1.3226 (1.1712)\tPrec@1 55.469 (59.265)\n",
      "Epoch: [64][312/390]\tTime 0.002 (0.003)\tLoss 1.3380 (1.1822)\tPrec@1 52.344 (58.806)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.003)\tLoss 1.3344 (1.1896)\tPrec@1 50.000 (58.558)\n",
      "EPOCH: 64 train Results: Prec@1 58.558 Loss: 1.1896\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1879 (1.1879)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2101 (1.2691)\tPrec@1 56.250 (54.440)\n",
      "EPOCH: 64 val Results: Prec@1 54.440 Loss: 1.2691\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [65][0/390]\tTime 0.003 (0.003)\tLoss 1.1719 (1.1719)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.004)\tLoss 1.1748 (1.1383)\tPrec@1 57.031 (61.234)\n",
      "Epoch: [65][156/390]\tTime 0.006 (0.003)\tLoss 1.1206 (1.1492)\tPrec@1 64.844 (60.724)\n",
      "Epoch: [65][234/390]\tTime 0.003 (0.003)\tLoss 1.2497 (1.1666)\tPrec@1 53.125 (59.651)\n",
      "Epoch: [65][312/390]\tTime 0.002 (0.003)\tLoss 1.1494 (1.1781)\tPrec@1 59.375 (59.046)\n",
      "Epoch: [65][390/390]\tTime 0.004 (0.003)\tLoss 1.1306 (1.1841)\tPrec@1 57.500 (58.780)\n",
      "EPOCH: 65 train Results: Prec@1 58.780 Loss: 1.1841\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1777 (1.1777)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2552 (1.2608)\tPrec@1 50.000 (55.080)\n",
      "EPOCH: 65 val Results: Prec@1 55.080 Loss: 1.2608\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [66][0/390]\tTime 0.010 (0.010)\tLoss 1.1234 (1.1234)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [66][78/390]\tTime 0.003 (0.004)\tLoss 1.1585 (1.1482)\tPrec@1 57.812 (60.087)\n",
      "Epoch: [66][156/390]\tTime 0.002 (0.004)\tLoss 1.1428 (1.1582)\tPrec@1 60.938 (59.723)\n",
      "Epoch: [66][234/390]\tTime 0.002 (0.004)\tLoss 1.2583 (1.1677)\tPrec@1 57.031 (59.242)\n",
      "Epoch: [66][312/390]\tTime 0.003 (0.004)\tLoss 1.3251 (1.1794)\tPrec@1 57.812 (58.753)\n",
      "Epoch: [66][390/390]\tTime 0.013 (0.004)\tLoss 1.1607 (1.1846)\tPrec@1 61.250 (58.580)\n",
      "EPOCH: 66 train Results: Prec@1 58.580 Loss: 1.1846\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2054 (1.2054)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4612 (1.2643)\tPrec@1 31.250 (55.220)\n",
      "EPOCH: 66 val Results: Prec@1 55.220 Loss: 1.2643\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [67][0/390]\tTime 0.003 (0.003)\tLoss 1.2434 (1.2434)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [67][78/390]\tTime 0.003 (0.003)\tLoss 1.2037 (1.1390)\tPrec@1 58.594 (60.572)\n",
      "Epoch: [67][156/390]\tTime 0.002 (0.003)\tLoss 1.3599 (1.1526)\tPrec@1 53.906 (60.022)\n",
      "Epoch: [67][234/390]\tTime 0.004 (0.003)\tLoss 1.1554 (1.1668)\tPrec@1 60.156 (59.405)\n",
      "Epoch: [67][312/390]\tTime 0.002 (0.003)\tLoss 1.1784 (1.1722)\tPrec@1 58.594 (59.085)\n",
      "Epoch: [67][390/390]\tTime 0.001 (0.003)\tLoss 1.2751 (1.1814)\tPrec@1 55.000 (58.824)\n",
      "EPOCH: 67 train Results: Prec@1 58.824 Loss: 1.1814\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1776 (1.1776)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1278 (1.2603)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 67 val Results: Prec@1 55.230 Loss: 1.2603\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [68][0/390]\tTime 0.006 (0.006)\tLoss 1.1163 (1.1163)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [68][78/390]\tTime 0.003 (0.003)\tLoss 1.0997 (1.1236)\tPrec@1 62.500 (60.928)\n",
      "Epoch: [68][156/390]\tTime 0.003 (0.003)\tLoss 1.3056 (1.1559)\tPrec@1 56.250 (59.614)\n",
      "Epoch: [68][234/390]\tTime 0.002 (0.003)\tLoss 1.2520 (1.1657)\tPrec@1 57.812 (59.086)\n",
      "Epoch: [68][312/390]\tTime 0.004 (0.003)\tLoss 1.4106 (1.1767)\tPrec@1 52.344 (58.783)\n",
      "Epoch: [68][390/390]\tTime 0.003 (0.003)\tLoss 1.3610 (1.1831)\tPrec@1 56.250 (58.654)\n",
      "EPOCH: 68 train Results: Prec@1 58.654 Loss: 1.1831\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1274 (1.1274)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1946 (1.2621)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 68 val Results: Prec@1 55.230 Loss: 1.2621\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [69][0/390]\tTime 0.002 (0.002)\tLoss 1.1230 (1.1230)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.004)\tLoss 1.1516 (1.1282)\tPrec@1 57.031 (60.848)\n",
      "Epoch: [69][156/390]\tTime 0.002 (0.004)\tLoss 1.1588 (1.1459)\tPrec@1 60.938 (60.241)\n",
      "Epoch: [69][234/390]\tTime 0.003 (0.003)\tLoss 1.0474 (1.1607)\tPrec@1 63.281 (59.511)\n",
      "Epoch: [69][312/390]\tTime 0.002 (0.003)\tLoss 1.2280 (1.1726)\tPrec@1 60.156 (58.841)\n",
      "Epoch: [69][390/390]\tTime 0.001 (0.004)\tLoss 1.2777 (1.1806)\tPrec@1 52.500 (58.624)\n",
      "EPOCH: 69 train Results: Prec@1 58.624 Loss: 1.1806\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1526 (1.1526)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3472 (1.2626)\tPrec@1 37.500 (55.060)\n",
      "EPOCH: 69 val Results: Prec@1 55.060 Loss: 1.2626\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [70][0/390]\tTime 0.002 (0.002)\tLoss 1.0074 (1.0074)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [70][78/390]\tTime 0.003 (0.003)\tLoss 1.2882 (1.1309)\tPrec@1 52.344 (61.234)\n",
      "Epoch: [70][156/390]\tTime 0.007 (0.003)\tLoss 1.1996 (1.1492)\tPrec@1 54.688 (60.485)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.003)\tLoss 1.0880 (1.1599)\tPrec@1 64.062 (59.781)\n",
      "Epoch: [70][312/390]\tTime 0.003 (0.003)\tLoss 1.1827 (1.1755)\tPrec@1 62.500 (59.103)\n",
      "Epoch: [70][390/390]\tTime 0.005 (0.003)\tLoss 1.2280 (1.1802)\tPrec@1 57.500 (58.918)\n",
      "EPOCH: 70 train Results: Prec@1 58.918 Loss: 1.1802\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2170 (1.2170)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2085 (1.2601)\tPrec@1 43.750 (55.290)\n",
      "EPOCH: 70 val Results: Prec@1 55.290 Loss: 1.2601\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [71][0/390]\tTime 0.002 (0.002)\tLoss 1.1093 (1.1093)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [71][78/390]\tTime 0.003 (0.003)\tLoss 1.2181 (1.1306)\tPrec@1 60.156 (60.710)\n",
      "Epoch: [71][156/390]\tTime 0.002 (0.003)\tLoss 1.1722 (1.1482)\tPrec@1 55.469 (59.922)\n",
      "Epoch: [71][234/390]\tTime 0.002 (0.003)\tLoss 0.9769 (1.1645)\tPrec@1 67.969 (59.525)\n",
      "Epoch: [71][312/390]\tTime 0.002 (0.003)\tLoss 1.1490 (1.1687)\tPrec@1 63.281 (59.345)\n",
      "Epoch: [71][390/390]\tTime 0.005 (0.003)\tLoss 1.0855 (1.1798)\tPrec@1 58.750 (58.842)\n",
      "EPOCH: 71 train Results: Prec@1 58.842 Loss: 1.1798\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2037 (1.2037)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1893 (1.2679)\tPrec@1 43.750 (55.070)\n",
      "EPOCH: 71 val Results: Prec@1 55.070 Loss: 1.2679\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [72][0/390]\tTime 0.003 (0.003)\tLoss 1.0902 (1.0902)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [72][78/390]\tTime 0.002 (0.003)\tLoss 1.3339 (1.1418)\tPrec@1 50.781 (60.601)\n",
      "Epoch: [72][156/390]\tTime 0.003 (0.003)\tLoss 1.2161 (1.1490)\tPrec@1 59.375 (60.161)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.003)\tLoss 1.2007 (1.1611)\tPrec@1 54.688 (59.408)\n",
      "Epoch: [72][312/390]\tTime 0.002 (0.003)\tLoss 1.2017 (1.1715)\tPrec@1 58.594 (58.961)\n",
      "Epoch: [72][390/390]\tTime 0.001 (0.003)\tLoss 1.0218 (1.1776)\tPrec@1 68.750 (58.766)\n",
      "EPOCH: 72 train Results: Prec@1 58.766 Loss: 1.1776\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1327 (1.1327)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2597 (1.2582)\tPrec@1 62.500 (54.810)\n",
      "EPOCH: 72 val Results: Prec@1 54.810 Loss: 1.2582\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [73][0/390]\tTime 0.003 (0.003)\tLoss 1.0913 (1.0913)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.003)\tLoss 1.0527 (1.1339)\tPrec@1 61.719 (60.839)\n",
      "Epoch: [73][156/390]\tTime 0.007 (0.003)\tLoss 1.2818 (1.1458)\tPrec@1 53.125 (60.037)\n",
      "Epoch: [73][234/390]\tTime 0.002 (0.003)\tLoss 1.1605 (1.1594)\tPrec@1 60.938 (59.624)\n",
      "Epoch: [73][312/390]\tTime 0.002 (0.003)\tLoss 1.0447 (1.1725)\tPrec@1 64.062 (59.038)\n",
      "Epoch: [73][390/390]\tTime 0.002 (0.003)\tLoss 1.1866 (1.1785)\tPrec@1 57.500 (58.848)\n",
      "EPOCH: 73 train Results: Prec@1 58.848 Loss: 1.1785\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1582 (1.1582)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2145 (1.2564)\tPrec@1 50.000 (55.910)\n",
      "EPOCH: 73 val Results: Prec@1 55.910 Loss: 1.2564\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [74][0/390]\tTime 0.005 (0.005)\tLoss 1.2078 (1.2078)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [74][78/390]\tTime 0.002 (0.003)\tLoss 1.1833 (1.1379)\tPrec@1 63.281 (60.710)\n",
      "Epoch: [74][156/390]\tTime 0.003 (0.003)\tLoss 1.1253 (1.1520)\tPrec@1 60.156 (60.161)\n",
      "Epoch: [74][234/390]\tTime 0.004 (0.003)\tLoss 1.1876 (1.1612)\tPrec@1 52.344 (59.664)\n",
      "Epoch: [74][312/390]\tTime 0.021 (0.003)\tLoss 1.1014 (1.1670)\tPrec@1 64.062 (59.360)\n",
      "Epoch: [74][390/390]\tTime 0.003 (0.003)\tLoss 1.2447 (1.1758)\tPrec@1 57.500 (58.974)\n",
      "EPOCH: 74 train Results: Prec@1 58.974 Loss: 1.1758\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1858 (1.1858)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5115 (1.2663)\tPrec@1 31.250 (55.250)\n",
      "EPOCH: 74 val Results: Prec@1 55.250 Loss: 1.2663\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [75][0/390]\tTime 0.003 (0.003)\tLoss 1.1922 (1.1922)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [75][78/390]\tTime 0.002 (0.003)\tLoss 1.2618 (1.1286)\tPrec@1 57.812 (60.453)\n",
      "Epoch: [75][156/390]\tTime 0.002 (0.003)\tLoss 0.9152 (1.1534)\tPrec@1 67.969 (59.529)\n",
      "Epoch: [75][234/390]\tTime 0.002 (0.003)\tLoss 1.3059 (1.1636)\tPrec@1 51.562 (59.232)\n",
      "Epoch: [75][312/390]\tTime 0.002 (0.003)\tLoss 1.2593 (1.1697)\tPrec@1 56.250 (59.158)\n",
      "Epoch: [75][390/390]\tTime 0.010 (0.003)\tLoss 1.1872 (1.1781)\tPrec@1 60.000 (58.912)\n",
      "EPOCH: 75 train Results: Prec@1 58.912 Loss: 1.1781\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1837 (1.1837)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2485 (1.2665)\tPrec@1 50.000 (55.180)\n",
      "EPOCH: 75 val Results: Prec@1 55.180 Loss: 1.2665\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [76][0/390]\tTime 0.002 (0.002)\tLoss 1.2273 (1.2273)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.003)\tLoss 1.3169 (1.1395)\tPrec@1 56.250 (60.611)\n",
      "Epoch: [76][156/390]\tTime 0.008 (0.003)\tLoss 0.9831 (1.1522)\tPrec@1 68.750 (59.967)\n",
      "Epoch: [76][234/390]\tTime 0.005 (0.003)\tLoss 1.1100 (1.1626)\tPrec@1 64.844 (59.485)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 1.0373 (1.1671)\tPrec@1 62.500 (59.293)\n",
      "Epoch: [76][390/390]\tTime 0.001 (0.003)\tLoss 1.0499 (1.1732)\tPrec@1 66.250 (59.052)\n",
      "EPOCH: 76 train Results: Prec@1 59.052 Loss: 1.1732\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2039 (1.2039)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5484 (1.2635)\tPrec@1 25.000 (55.500)\n",
      "EPOCH: 76 val Results: Prec@1 55.500 Loss: 1.2635\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [77][0/390]\tTime 0.004 (0.004)\tLoss 1.1137 (1.1137)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [77][78/390]\tTime 0.003 (0.003)\tLoss 1.1964 (1.1324)\tPrec@1 61.719 (60.572)\n",
      "Epoch: [77][156/390]\tTime 0.002 (0.003)\tLoss 1.2276 (1.1493)\tPrec@1 56.250 (59.848)\n",
      "Epoch: [77][234/390]\tTime 0.004 (0.003)\tLoss 1.0763 (1.1627)\tPrec@1 64.844 (59.345)\n",
      "Epoch: [77][312/390]\tTime 0.003 (0.003)\tLoss 1.1849 (1.1711)\tPrec@1 57.812 (59.026)\n",
      "Epoch: [77][390/390]\tTime 0.001 (0.003)\tLoss 1.3006 (1.1792)\tPrec@1 53.750 (58.694)\n",
      "EPOCH: 77 train Results: Prec@1 58.694 Loss: 1.1792\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1447 (1.1447)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4159 (1.2652)\tPrec@1 43.750 (55.170)\n",
      "EPOCH: 77 val Results: Prec@1 55.170 Loss: 1.2652\n",
      "Best Prec@1: 55.910\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [78][0/390]\tTime 0.002 (0.002)\tLoss 1.0945 (1.0945)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [78][78/390]\tTime 0.003 (0.003)\tLoss 1.1446 (1.1194)\tPrec@1 59.375 (61.353)\n",
      "Epoch: [78][156/390]\tTime 0.003 (0.003)\tLoss 1.1951 (1.1444)\tPrec@1 54.688 (60.296)\n",
      "Epoch: [78][234/390]\tTime 0.010 (0.003)\tLoss 1.1483 (1.1600)\tPrec@1 60.938 (59.631)\n",
      "Epoch: [78][312/390]\tTime 0.004 (0.003)\tLoss 1.1734 (1.1715)\tPrec@1 60.938 (59.150)\n",
      "Epoch: [78][390/390]\tTime 0.002 (0.003)\tLoss 1.1431 (1.1763)\tPrec@1 56.250 (58.874)\n",
      "EPOCH: 78 train Results: Prec@1 58.874 Loss: 1.1763\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1079 (1.1079)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0915 (1.2431)\tPrec@1 43.750 (56.180)\n",
      "EPOCH: 78 val Results: Prec@1 56.180 Loss: 1.2431\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [79][0/390]\tTime 0.004 (0.004)\tLoss 1.0747 (1.0747)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [79][78/390]\tTime 0.002 (0.003)\tLoss 1.1223 (1.1044)\tPrec@1 59.375 (61.748)\n",
      "Epoch: [79][156/390]\tTime 0.003 (0.003)\tLoss 1.1076 (1.1278)\tPrec@1 64.062 (60.957)\n",
      "Epoch: [79][234/390]\tTime 0.005 (0.003)\tLoss 1.1935 (1.1454)\tPrec@1 59.375 (60.146)\n",
      "Epoch: [79][312/390]\tTime 0.003 (0.003)\tLoss 1.2168 (1.1609)\tPrec@1 55.469 (59.562)\n",
      "Epoch: [79][390/390]\tTime 0.005 (0.003)\tLoss 1.0503 (1.1724)\tPrec@1 62.500 (58.982)\n",
      "EPOCH: 79 train Results: Prec@1 58.982 Loss: 1.1724\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1472 (1.1472)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4113 (1.2512)\tPrec@1 43.750 (55.550)\n",
      "EPOCH: 79 val Results: Prec@1 55.550 Loss: 1.2512\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [80][0/390]\tTime 0.005 (0.005)\tLoss 1.1025 (1.1025)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [80][78/390]\tTime 0.006 (0.003)\tLoss 1.0898 (1.1306)\tPrec@1 62.500 (60.730)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.1042 (1.1493)\tPrec@1 63.281 (59.763)\n",
      "Epoch: [80][234/390]\tTime 0.009 (0.003)\tLoss 1.0873 (1.1606)\tPrec@1 64.062 (59.618)\n",
      "Epoch: [80][312/390]\tTime 0.002 (0.003)\tLoss 1.3647 (1.1706)\tPrec@1 51.562 (59.063)\n",
      "Epoch: [80][390/390]\tTime 0.004 (0.003)\tLoss 1.1829 (1.1746)\tPrec@1 53.750 (58.972)\n",
      "EPOCH: 80 train Results: Prec@1 58.972 Loss: 1.1746\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1855 (1.1855)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4518 (1.2546)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 80 val Results: Prec@1 55.150 Loss: 1.2546\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [81][0/390]\tTime 0.003 (0.003)\tLoss 1.0108 (1.0108)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [81][78/390]\tTime 0.002 (0.003)\tLoss 1.0775 (1.1209)\tPrec@1 57.031 (61.679)\n",
      "Epoch: [81][156/390]\tTime 0.003 (0.004)\tLoss 1.0646 (1.1428)\tPrec@1 64.062 (60.743)\n",
      "Epoch: [81][234/390]\tTime 0.005 (0.004)\tLoss 1.1104 (1.1501)\tPrec@1 61.719 (60.359)\n",
      "Epoch: [81][312/390]\tTime 0.003 (0.004)\tLoss 1.4085 (1.1635)\tPrec@1 48.438 (59.804)\n",
      "Epoch: [81][390/390]\tTime 0.010 (0.004)\tLoss 1.1096 (1.1731)\tPrec@1 65.000 (59.386)\n",
      "EPOCH: 81 train Results: Prec@1 59.386 Loss: 1.1731\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1748 (1.1748)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3500 (1.2590)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 81 val Results: Prec@1 55.300 Loss: 1.2590\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [82][0/390]\tTime 0.003 (0.003)\tLoss 1.1545 (1.1545)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [82][78/390]\tTime 0.002 (0.003)\tLoss 1.1122 (1.1301)\tPrec@1 62.500 (60.819)\n",
      "Epoch: [82][156/390]\tTime 0.002 (0.003)\tLoss 1.2218 (1.1431)\tPrec@1 63.281 (60.231)\n",
      "Epoch: [82][234/390]\tTime 0.003 (0.003)\tLoss 1.1268 (1.1534)\tPrec@1 61.719 (59.608)\n",
      "Epoch: [82][312/390]\tTime 0.004 (0.003)\tLoss 1.0939 (1.1580)\tPrec@1 59.375 (59.445)\n",
      "Epoch: [82][390/390]\tTime 0.002 (0.003)\tLoss 1.0388 (1.1732)\tPrec@1 61.250 (58.888)\n",
      "EPOCH: 82 train Results: Prec@1 58.888 Loss: 1.1732\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1649 (1.1649)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3947 (1.2554)\tPrec@1 37.500 (55.290)\n",
      "EPOCH: 82 val Results: Prec@1 55.290 Loss: 1.2554\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [83][0/390]\tTime 0.003 (0.003)\tLoss 1.1347 (1.1347)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [83][78/390]\tTime 0.004 (0.003)\tLoss 1.3550 (1.1268)\tPrec@1 58.594 (61.303)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.2594 (1.1454)\tPrec@1 53.125 (60.365)\n",
      "Epoch: [83][234/390]\tTime 0.003 (0.003)\tLoss 1.1009 (1.1566)\tPrec@1 61.719 (59.697)\n",
      "Epoch: [83][312/390]\tTime 0.002 (0.003)\tLoss 1.2362 (1.1663)\tPrec@1 57.031 (59.427)\n",
      "Epoch: [83][390/390]\tTime 0.003 (0.003)\tLoss 1.2709 (1.1730)\tPrec@1 56.250 (59.094)\n",
      "EPOCH: 83 train Results: Prec@1 59.094 Loss: 1.1730\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1082 (1.1082)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5019 (1.2612)\tPrec@1 25.000 (55.140)\n",
      "EPOCH: 83 val Results: Prec@1 55.140 Loss: 1.2612\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [84][0/390]\tTime 0.003 (0.003)\tLoss 0.9378 (0.9378)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [84][78/390]\tTime 0.002 (0.003)\tLoss 1.0043 (1.1298)\tPrec@1 68.750 (61.214)\n",
      "Epoch: [84][156/390]\tTime 0.005 (0.003)\tLoss 1.1682 (1.1476)\tPrec@1 64.062 (60.106)\n",
      "Epoch: [84][234/390]\tTime 0.003 (0.003)\tLoss 1.1147 (1.1610)\tPrec@1 60.156 (59.428)\n",
      "Epoch: [84][312/390]\tTime 0.002 (0.003)\tLoss 1.0786 (1.1648)\tPrec@1 61.719 (59.295)\n",
      "Epoch: [84][390/390]\tTime 0.007 (0.003)\tLoss 1.3547 (1.1684)\tPrec@1 46.250 (59.180)\n",
      "EPOCH: 84 train Results: Prec@1 59.180 Loss: 1.1684\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0878 (1.0878)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3468 (1.2666)\tPrec@1 50.000 (54.930)\n",
      "EPOCH: 84 val Results: Prec@1 54.930 Loss: 1.2666\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [85][0/390]\tTime 0.002 (0.002)\tLoss 1.0676 (1.0676)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [85][78/390]\tTime 0.004 (0.003)\tLoss 1.0503 (1.1188)\tPrec@1 65.625 (61.224)\n",
      "Epoch: [85][156/390]\tTime 0.003 (0.003)\tLoss 1.0092 (1.1390)\tPrec@1 66.406 (60.500)\n",
      "Epoch: [85][234/390]\tTime 0.002 (0.003)\tLoss 1.3166 (1.1515)\tPrec@1 53.125 (59.914)\n",
      "Epoch: [85][312/390]\tTime 0.003 (0.003)\tLoss 1.1315 (1.1632)\tPrec@1 63.281 (59.455)\n",
      "Epoch: [85][390/390]\tTime 0.007 (0.003)\tLoss 1.3046 (1.1708)\tPrec@1 52.500 (59.094)\n",
      "EPOCH: 85 train Results: Prec@1 59.094 Loss: 1.1708\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1530 (1.1530)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6249 (1.2545)\tPrec@1 37.500 (55.760)\n",
      "EPOCH: 85 val Results: Prec@1 55.760 Loss: 1.2545\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [86][0/390]\tTime 0.004 (0.004)\tLoss 1.0929 (1.0929)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [86][78/390]\tTime 0.005 (0.003)\tLoss 1.1914 (1.1481)\tPrec@1 62.500 (60.591)\n",
      "Epoch: [86][156/390]\tTime 0.003 (0.003)\tLoss 1.1077 (1.1464)\tPrec@1 59.375 (60.325)\n",
      "Epoch: [86][234/390]\tTime 0.003 (0.003)\tLoss 1.1283 (1.1562)\tPrec@1 59.375 (59.924)\n",
      "Epoch: [86][312/390]\tTime 0.005 (0.003)\tLoss 1.3260 (1.1657)\tPrec@1 53.906 (59.358)\n",
      "Epoch: [86][390/390]\tTime 0.001 (0.003)\tLoss 1.2855 (1.1752)\tPrec@1 53.750 (58.828)\n",
      "EPOCH: 86 train Results: Prec@1 58.828 Loss: 1.1752\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1681 (1.1681)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3391 (1.2503)\tPrec@1 37.500 (55.170)\n",
      "EPOCH: 86 val Results: Prec@1 55.170 Loss: 1.2503\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [87][0/390]\tTime 0.003 (0.003)\tLoss 1.0850 (1.0850)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [87][78/390]\tTime 0.002 (0.004)\tLoss 0.9426 (1.1081)\tPrec@1 67.969 (61.778)\n",
      "Epoch: [87][156/390]\tTime 0.003 (0.004)\tLoss 1.1558 (1.1364)\tPrec@1 58.594 (60.589)\n",
      "Epoch: [87][234/390]\tTime 0.004 (0.003)\tLoss 1.1298 (1.1518)\tPrec@1 60.156 (59.887)\n",
      "Epoch: [87][312/390]\tTime 0.004 (0.003)\tLoss 1.2644 (1.1655)\tPrec@1 55.469 (59.373)\n",
      "Epoch: [87][390/390]\tTime 0.060 (0.003)\tLoss 1.2813 (1.1715)\tPrec@1 47.500 (58.982)\n",
      "EPOCH: 87 train Results: Prec@1 58.982 Loss: 1.1715\n",
      "Test: [0/78]\tTime 0.016 (0.016)\tLoss 1.1017 (1.1017)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2996 (1.2536)\tPrec@1 43.750 (56.000)\n",
      "EPOCH: 87 val Results: Prec@1 56.000 Loss: 1.2536\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [88][0/390]\tTime 0.003 (0.003)\tLoss 1.0534 (1.0534)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [88][78/390]\tTime 0.002 (0.003)\tLoss 0.9917 (1.1101)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [88][156/390]\tTime 0.003 (0.003)\tLoss 1.1868 (1.1357)\tPrec@1 56.250 (60.778)\n",
      "Epoch: [88][234/390]\tTime 0.003 (0.003)\tLoss 1.1635 (1.1524)\tPrec@1 60.938 (60.050)\n",
      "Epoch: [88][312/390]\tTime 0.008 (0.003)\tLoss 1.1390 (1.1639)\tPrec@1 57.031 (59.330)\n",
      "Epoch: [88][390/390]\tTime 0.003 (0.003)\tLoss 1.5098 (1.1715)\tPrec@1 48.750 (59.052)\n",
      "EPOCH: 88 train Results: Prec@1 59.052 Loss: 1.1715\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1810 (1.1810)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2939 (1.2517)\tPrec@1 50.000 (55.520)\n",
      "EPOCH: 88 val Results: Prec@1 55.520 Loss: 1.2517\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [89][0/390]\tTime 0.004 (0.004)\tLoss 1.1325 (1.1325)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.003)\tLoss 1.0956 (1.1174)\tPrec@1 66.406 (61.155)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1902 (1.1373)\tPrec@1 58.594 (60.301)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 1.1494 (1.1481)\tPrec@1 61.719 (59.904)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.1583 (1.1619)\tPrec@1 63.281 (59.363)\n",
      "Epoch: [89][390/390]\tTime 0.001 (0.003)\tLoss 1.2988 (1.1683)\tPrec@1 57.500 (59.106)\n",
      "EPOCH: 89 train Results: Prec@1 59.106 Loss: 1.1683\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1533 (1.1533)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3914 (1.2604)\tPrec@1 43.750 (54.930)\n",
      "EPOCH: 89 val Results: Prec@1 54.930 Loss: 1.2604\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [90][0/390]\tTime 0.002 (0.002)\tLoss 1.1708 (1.1708)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [90][78/390]\tTime 0.003 (0.003)\tLoss 1.0643 (1.1193)\tPrec@1 64.062 (61.363)\n",
      "Epoch: [90][156/390]\tTime 0.002 (0.003)\tLoss 1.2336 (1.1405)\tPrec@1 55.469 (60.460)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.2358 (1.1524)\tPrec@1 50.781 (59.904)\n",
      "Epoch: [90][312/390]\tTime 0.004 (0.003)\tLoss 1.1836 (1.1582)\tPrec@1 58.594 (59.610)\n",
      "Epoch: [90][390/390]\tTime 0.003 (0.003)\tLoss 1.1445 (1.1672)\tPrec@1 56.250 (59.238)\n",
      "EPOCH: 90 train Results: Prec@1 59.238 Loss: 1.1672\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1213 (1.1213)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7331 (1.2548)\tPrec@1 25.000 (55.230)\n",
      "EPOCH: 90 val Results: Prec@1 55.230 Loss: 1.2548\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [91][0/390]\tTime 0.005 (0.005)\tLoss 1.2999 (1.2999)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [91][78/390]\tTime 0.002 (0.003)\tLoss 1.1429 (1.1134)\tPrec@1 58.594 (61.363)\n",
      "Epoch: [91][156/390]\tTime 0.006 (0.003)\tLoss 1.2280 (1.1344)\tPrec@1 57.812 (60.644)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.003)\tLoss 1.1512 (1.1459)\tPrec@1 58.594 (60.219)\n",
      "Epoch: [91][312/390]\tTime 0.004 (0.003)\tLoss 1.1172 (1.1579)\tPrec@1 59.375 (59.727)\n",
      "Epoch: [91][390/390]\tTime 0.003 (0.003)\tLoss 1.2097 (1.1693)\tPrec@1 55.000 (59.212)\n",
      "EPOCH: 91 train Results: Prec@1 59.212 Loss: 1.1693\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1399 (1.1399)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3508 (1.2530)\tPrec@1 37.500 (55.440)\n",
      "EPOCH: 91 val Results: Prec@1 55.440 Loss: 1.2530\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [92][0/390]\tTime 0.002 (0.002)\tLoss 1.1879 (1.1879)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.003)\tLoss 1.1679 (1.1193)\tPrec@1 57.031 (61.392)\n",
      "Epoch: [92][156/390]\tTime 0.013 (0.003)\tLoss 1.1429 (1.1404)\tPrec@1 64.062 (60.385)\n",
      "Epoch: [92][234/390]\tTime 0.002 (0.003)\tLoss 1.1364 (1.1499)\tPrec@1 60.156 (60.037)\n",
      "Epoch: [92][312/390]\tTime 0.003 (0.003)\tLoss 1.2271 (1.1591)\tPrec@1 53.125 (59.437)\n",
      "Epoch: [92][390/390]\tTime 0.003 (0.003)\tLoss 1.2388 (1.1663)\tPrec@1 60.000 (59.088)\n",
      "EPOCH: 92 train Results: Prec@1 59.088 Loss: 1.1663\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1283 (1.1283)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4066 (1.2560)\tPrec@1 50.000 (55.400)\n",
      "EPOCH: 92 val Results: Prec@1 55.400 Loss: 1.2560\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 1.2099 (1.2099)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [93][78/390]\tTime 0.003 (0.003)\tLoss 1.0982 (1.1030)\tPrec@1 65.625 (61.689)\n",
      "Epoch: [93][156/390]\tTime 0.002 (0.003)\tLoss 1.2040 (1.1269)\tPrec@1 60.156 (60.733)\n",
      "Epoch: [93][234/390]\tTime 0.015 (0.003)\tLoss 1.2940 (1.1462)\tPrec@1 57.812 (60.116)\n",
      "Epoch: [93][312/390]\tTime 0.002 (0.003)\tLoss 1.2617 (1.1553)\tPrec@1 57.031 (59.734)\n",
      "Epoch: [93][390/390]\tTime 0.003 (0.003)\tLoss 1.2021 (1.1639)\tPrec@1 55.000 (59.338)\n",
      "EPOCH: 93 train Results: Prec@1 59.338 Loss: 1.1639\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1017 (1.1017)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2938 (1.2616)\tPrec@1 50.000 (55.190)\n",
      "EPOCH: 93 val Results: Prec@1 55.190 Loss: 1.2616\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [94][0/390]\tTime 0.003 (0.003)\tLoss 0.9905 (0.9905)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [94][78/390]\tTime 0.009 (0.005)\tLoss 1.1120 (1.1077)\tPrec@1 60.938 (61.531)\n",
      "Epoch: [94][156/390]\tTime 0.006 (0.004)\tLoss 1.1145 (1.1298)\tPrec@1 64.844 (60.579)\n",
      "Epoch: [94][234/390]\tTime 0.002 (0.004)\tLoss 0.9897 (1.1497)\tPrec@1 64.062 (59.694)\n",
      "Epoch: [94][312/390]\tTime 0.002 (0.004)\tLoss 1.2345 (1.1597)\tPrec@1 57.031 (59.250)\n",
      "Epoch: [94][390/390]\tTime 0.002 (0.004)\tLoss 1.4270 (1.1677)\tPrec@1 55.000 (59.056)\n",
      "EPOCH: 94 train Results: Prec@1 59.056 Loss: 1.1677\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1847 (1.1847)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2998 (1.2552)\tPrec@1 56.250 (55.410)\n",
      "EPOCH: 94 val Results: Prec@1 55.410 Loss: 1.2552\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [95][0/390]\tTime 0.005 (0.005)\tLoss 1.1003 (1.1003)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [95][78/390]\tTime 0.002 (0.003)\tLoss 1.2000 (1.1067)\tPrec@1 51.562 (61.640)\n",
      "Epoch: [95][156/390]\tTime 0.006 (0.004)\tLoss 1.1721 (1.1272)\tPrec@1 55.469 (60.654)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.004)\tLoss 1.1269 (1.1404)\tPrec@1 57.812 (60.229)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.003)\tLoss 1.1024 (1.1532)\tPrec@1 66.406 (59.789)\n",
      "Epoch: [95][390/390]\tTime 0.003 (0.003)\tLoss 1.2754 (1.1641)\tPrec@1 57.500 (59.398)\n",
      "EPOCH: 95 train Results: Prec@1 59.398 Loss: 1.1641\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1276 (1.1276)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3529 (1.2528)\tPrec@1 43.750 (55.190)\n",
      "EPOCH: 95 val Results: Prec@1 55.190 Loss: 1.2528\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [96][0/390]\tTime 0.005 (0.005)\tLoss 1.1409 (1.1409)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.004)\tLoss 1.1534 (1.1200)\tPrec@1 57.031 (61.274)\n",
      "Epoch: [96][156/390]\tTime 0.003 (0.003)\tLoss 1.0143 (1.1388)\tPrec@1 66.406 (60.679)\n",
      "Epoch: [96][234/390]\tTime 0.004 (0.003)\tLoss 1.1599 (1.1545)\tPrec@1 60.938 (59.894)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 1.0964 (1.1574)\tPrec@1 62.500 (59.680)\n",
      "Epoch: [96][390/390]\tTime 0.001 (0.003)\tLoss 1.2850 (1.1645)\tPrec@1 53.750 (59.402)\n",
      "EPOCH: 96 train Results: Prec@1 59.402 Loss: 1.1645\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1428 (1.1428)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.3230 (1.2560)\tPrec@1 43.750 (55.660)\n",
      "EPOCH: 96 val Results: Prec@1 55.660 Loss: 1.2560\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [97][0/390]\tTime 0.005 (0.005)\tLoss 1.1736 (1.1736)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [97][78/390]\tTime 0.002 (0.003)\tLoss 1.1376 (1.1117)\tPrec@1 61.719 (61.343)\n",
      "Epoch: [97][156/390]\tTime 0.002 (0.003)\tLoss 1.3368 (1.1341)\tPrec@1 54.688 (60.554)\n",
      "Epoch: [97][234/390]\tTime 0.005 (0.003)\tLoss 1.2938 (1.1466)\tPrec@1 52.344 (60.017)\n",
      "Epoch: [97][312/390]\tTime 0.004 (0.003)\tLoss 1.2113 (1.1560)\tPrec@1 57.031 (59.622)\n",
      "Epoch: [97][390/390]\tTime 0.007 (0.003)\tLoss 0.9993 (1.1654)\tPrec@1 63.750 (59.268)\n",
      "EPOCH: 97 train Results: Prec@1 59.268 Loss: 1.1654\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1630 (1.1630)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5429 (1.2505)\tPrec@1 43.750 (56.000)\n",
      "EPOCH: 97 val Results: Prec@1 56.000 Loss: 1.2505\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [98][0/390]\tTime 0.015 (0.015)\tLoss 1.1468 (1.1468)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [98][78/390]\tTime 0.003 (0.003)\tLoss 0.9864 (1.1128)\tPrec@1 68.750 (61.027)\n",
      "Epoch: [98][156/390]\tTime 0.004 (0.003)\tLoss 1.2331 (1.1306)\tPrec@1 57.031 (60.425)\n",
      "Epoch: [98][234/390]\tTime 0.004 (0.004)\tLoss 1.1144 (1.1417)\tPrec@1 63.281 (60.013)\n",
      "Epoch: [98][312/390]\tTime 0.005 (0.004)\tLoss 1.3737 (1.1530)\tPrec@1 46.094 (59.472)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.004)\tLoss 1.1650 (1.1617)\tPrec@1 61.250 (59.254)\n",
      "EPOCH: 98 train Results: Prec@1 59.254 Loss: 1.1617\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1629 (1.1629)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6500 (1.2540)\tPrec@1 31.250 (55.190)\n",
      "EPOCH: 98 val Results: Prec@1 55.190 Loss: 1.2540\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [99][0/390]\tTime 0.003 (0.003)\tLoss 1.0752 (1.0752)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [99][78/390]\tTime 0.008 (0.003)\tLoss 1.0062 (1.1140)\tPrec@1 63.281 (61.412)\n",
      "Epoch: [99][156/390]\tTime 0.002 (0.003)\tLoss 1.2001 (1.1332)\tPrec@1 61.719 (60.584)\n",
      "Epoch: [99][234/390]\tTime 0.005 (0.003)\tLoss 1.3677 (1.1421)\tPrec@1 49.219 (60.083)\n",
      "Epoch: [99][312/390]\tTime 0.003 (0.003)\tLoss 1.0754 (1.1521)\tPrec@1 60.938 (59.602)\n",
      "Epoch: [99][390/390]\tTime 0.002 (0.003)\tLoss 1.0439 (1.1610)\tPrec@1 70.000 (59.296)\n",
      "EPOCH: 99 train Results: Prec@1 59.296 Loss: 1.1610\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1264 (1.1264)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5350 (1.2551)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 99 val Results: Prec@1 55.150 Loss: 1.2551\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [100][0/390]\tTime 0.005 (0.005)\tLoss 1.0674 (1.0674)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [100][78/390]\tTime 0.012 (0.003)\tLoss 1.2280 (1.1174)\tPrec@1 57.031 (60.730)\n",
      "Epoch: [100][156/390]\tTime 0.004 (0.003)\tLoss 1.1040 (1.1348)\tPrec@1 62.500 (60.186)\n",
      "Epoch: [100][234/390]\tTime 0.002 (0.003)\tLoss 1.1644 (1.1483)\tPrec@1 57.031 (59.535)\n",
      "Epoch: [100][312/390]\tTime 0.002 (0.003)\tLoss 1.1952 (1.1583)\tPrec@1 60.156 (59.228)\n",
      "Epoch: [100][390/390]\tTime 0.001 (0.003)\tLoss 1.2185 (1.1616)\tPrec@1 60.000 (59.196)\n",
      "EPOCH: 100 train Results: Prec@1 59.196 Loss: 1.1616\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1227 (1.1227)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5100 (1.2494)\tPrec@1 18.750 (55.800)\n",
      "EPOCH: 100 val Results: Prec@1 55.800 Loss: 1.2494\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [101][0/390]\tTime 0.004 (0.004)\tLoss 1.1172 (1.1172)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.003)\tLoss 1.1333 (1.1230)\tPrec@1 60.156 (60.690)\n",
      "Epoch: [101][156/390]\tTime 0.008 (0.003)\tLoss 1.2229 (1.1394)\tPrec@1 57.031 (60.032)\n",
      "Epoch: [101][234/390]\tTime 0.002 (0.003)\tLoss 1.2540 (1.1490)\tPrec@1 54.688 (59.890)\n",
      "Epoch: [101][312/390]\tTime 0.004 (0.003)\tLoss 1.1674 (1.1531)\tPrec@1 58.594 (59.647)\n",
      "Epoch: [101][390/390]\tTime 0.001 (0.003)\tLoss 1.1639 (1.1634)\tPrec@1 57.500 (59.240)\n",
      "EPOCH: 101 train Results: Prec@1 59.240 Loss: 1.1634\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1628 (1.1628)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3595 (1.2539)\tPrec@1 43.750 (55.250)\n",
      "EPOCH: 101 val Results: Prec@1 55.250 Loss: 1.2539\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [102][0/390]\tTime 0.004 (0.004)\tLoss 1.1053 (1.1053)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [102][78/390]\tTime 0.002 (0.004)\tLoss 1.1032 (1.1073)\tPrec@1 57.031 (61.570)\n",
      "Epoch: [102][156/390]\tTime 0.002 (0.004)\tLoss 1.1837 (1.1321)\tPrec@1 61.719 (60.738)\n",
      "Epoch: [102][234/390]\tTime 0.002 (0.003)\tLoss 1.2328 (1.1429)\tPrec@1 59.375 (60.153)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.003)\tLoss 1.2002 (1.1532)\tPrec@1 58.594 (59.754)\n",
      "Epoch: [102][390/390]\tTime 0.002 (0.003)\tLoss 1.2033 (1.1604)\tPrec@1 58.750 (59.540)\n",
      "EPOCH: 102 train Results: Prec@1 59.540 Loss: 1.1604\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0990 (1.0990)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2874 (1.2475)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 102 val Results: Prec@1 55.630 Loss: 1.2475\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [103][0/390]\tTime 0.002 (0.002)\tLoss 1.0731 (1.0731)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [103][78/390]\tTime 0.003 (0.004)\tLoss 1.0491 (1.1195)\tPrec@1 67.188 (61.333)\n",
      "Epoch: [103][156/390]\tTime 0.002 (0.004)\tLoss 1.2140 (1.1355)\tPrec@1 57.812 (60.370)\n",
      "Epoch: [103][234/390]\tTime 0.002 (0.004)\tLoss 1.1978 (1.1471)\tPrec@1 62.500 (59.884)\n",
      "Epoch: [103][312/390]\tTime 0.003 (0.003)\tLoss 1.2918 (1.1537)\tPrec@1 53.125 (59.764)\n",
      "Epoch: [103][390/390]\tTime 0.001 (0.003)\tLoss 1.2263 (1.1586)\tPrec@1 62.500 (59.594)\n",
      "EPOCH: 103 train Results: Prec@1 59.594 Loss: 1.1586\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1406 (1.1406)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3823 (1.2644)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 103 val Results: Prec@1 55.150 Loss: 1.2644\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [104][0/390]\tTime 0.005 (0.005)\tLoss 1.2194 (1.2194)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 1.1501 (1.1091)\tPrec@1 64.062 (61.758)\n",
      "Epoch: [104][156/390]\tTime 0.003 (0.003)\tLoss 1.1605 (1.1256)\tPrec@1 57.031 (60.853)\n",
      "Epoch: [104][234/390]\tTime 0.008 (0.003)\tLoss 1.1555 (1.1382)\tPrec@1 60.938 (60.422)\n",
      "Epoch: [104][312/390]\tTime 0.004 (0.003)\tLoss 1.1257 (1.1527)\tPrec@1 59.375 (59.847)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.003)\tLoss 1.0909 (1.1605)\tPrec@1 66.250 (59.530)\n",
      "EPOCH: 104 train Results: Prec@1 59.530 Loss: 1.1605\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1544 (1.1544)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3705 (1.2432)\tPrec@1 37.500 (55.750)\n",
      "EPOCH: 104 val Results: Prec@1 55.750 Loss: 1.2432\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [105][0/390]\tTime 0.003 (0.003)\tLoss 1.0700 (1.0700)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [105][78/390]\tTime 0.003 (0.003)\tLoss 1.1152 (1.1186)\tPrec@1 61.719 (61.363)\n",
      "Epoch: [105][156/390]\tTime 0.009 (0.003)\tLoss 1.1588 (1.1302)\tPrec@1 57.812 (60.559)\n",
      "Epoch: [105][234/390]\tTime 0.002 (0.003)\tLoss 1.2860 (1.1385)\tPrec@1 56.250 (59.937)\n",
      "Epoch: [105][312/390]\tTime 0.002 (0.003)\tLoss 1.1615 (1.1469)\tPrec@1 60.938 (59.759)\n",
      "Epoch: [105][390/390]\tTime 0.001 (0.003)\tLoss 1.1143 (1.1609)\tPrec@1 66.250 (59.280)\n",
      "EPOCH: 105 train Results: Prec@1 59.280 Loss: 1.1609\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1195 (1.1195)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2998 (1.2574)\tPrec@1 37.500 (55.220)\n",
      "EPOCH: 105 val Results: Prec@1 55.220 Loss: 1.2574\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [106][0/390]\tTime 0.003 (0.003)\tLoss 1.0140 (1.0140)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [106][78/390]\tTime 0.002 (0.004)\tLoss 0.9882 (1.0987)\tPrec@1 71.094 (62.025)\n",
      "Epoch: [106][156/390]\tTime 0.002 (0.003)\tLoss 1.2045 (1.1268)\tPrec@1 53.906 (60.549)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.003)\tLoss 1.0730 (1.1436)\tPrec@1 64.844 (60.060)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.003)\tLoss 1.1332 (1.1543)\tPrec@1 59.375 (59.595)\n",
      "Epoch: [106][390/390]\tTime 0.001 (0.003)\tLoss 1.2017 (1.1610)\tPrec@1 57.500 (59.258)\n",
      "EPOCH: 106 train Results: Prec@1 59.258 Loss: 1.1610\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1358 (1.1358)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6795 (1.2565)\tPrec@1 31.250 (55.420)\n",
      "EPOCH: 106 val Results: Prec@1 55.420 Loss: 1.2565\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [107][0/390]\tTime 0.007 (0.007)\tLoss 0.9820 (0.9820)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [107][78/390]\tTime 0.002 (0.003)\tLoss 1.2222 (1.1132)\tPrec@1 55.469 (61.472)\n",
      "Epoch: [107][156/390]\tTime 0.003 (0.003)\tLoss 1.2829 (1.1308)\tPrec@1 54.688 (60.584)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.003)\tLoss 1.1710 (1.1390)\tPrec@1 56.250 (60.193)\n",
      "Epoch: [107][312/390]\tTime 0.003 (0.003)\tLoss 1.1899 (1.1536)\tPrec@1 59.375 (59.615)\n",
      "Epoch: [107][390/390]\tTime 0.001 (0.003)\tLoss 1.0969 (1.1646)\tPrec@1 63.750 (59.166)\n",
      "EPOCH: 107 train Results: Prec@1 59.166 Loss: 1.1646\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1516 (1.1516)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5389 (1.2502)\tPrec@1 37.500 (55.330)\n",
      "EPOCH: 107 val Results: Prec@1 55.330 Loss: 1.2502\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [108][0/390]\tTime 0.004 (0.004)\tLoss 1.1104 (1.1104)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [108][78/390]\tTime 0.005 (0.003)\tLoss 1.1501 (1.1004)\tPrec@1 68.750 (62.085)\n",
      "Epoch: [108][156/390]\tTime 0.002 (0.003)\tLoss 1.1652 (1.1210)\tPrec@1 60.938 (61.151)\n",
      "Epoch: [108][234/390]\tTime 0.003 (0.003)\tLoss 1.2601 (1.1375)\tPrec@1 51.562 (60.442)\n",
      "Epoch: [108][312/390]\tTime 0.003 (0.003)\tLoss 1.0845 (1.1450)\tPrec@1 60.938 (60.104)\n",
      "Epoch: [108][390/390]\tTime 0.001 (0.003)\tLoss 1.1974 (1.1552)\tPrec@1 56.250 (59.748)\n",
      "EPOCH: 108 train Results: Prec@1 59.748 Loss: 1.1552\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1951 (1.1951)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6043 (1.2613)\tPrec@1 25.000 (54.870)\n",
      "EPOCH: 108 val Results: Prec@1 54.870 Loss: 1.2613\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [109][0/390]\tTime 0.004 (0.004)\tLoss 1.1584 (1.1584)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [109][78/390]\tTime 0.002 (0.003)\tLoss 1.1717 (1.1112)\tPrec@1 60.938 (61.580)\n",
      "Epoch: [109][156/390]\tTime 0.007 (0.003)\tLoss 1.1743 (1.1351)\tPrec@1 59.375 (60.669)\n",
      "Epoch: [109][234/390]\tTime 0.003 (0.003)\tLoss 1.0364 (1.1456)\tPrec@1 64.844 (59.930)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.003)\tLoss 1.1518 (1.1507)\tPrec@1 59.375 (59.754)\n",
      "Epoch: [109][390/390]\tTime 0.004 (0.003)\tLoss 1.2771 (1.1574)\tPrec@1 56.250 (59.472)\n",
      "EPOCH: 109 train Results: Prec@1 59.472 Loss: 1.1574\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1203 (1.1203)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3658 (1.2446)\tPrec@1 43.750 (55.930)\n",
      "EPOCH: 109 val Results: Prec@1 55.930 Loss: 1.2446\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [110][0/390]\tTime 0.003 (0.003)\tLoss 1.1548 (1.1548)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 1.1107 (1.1081)\tPrec@1 59.375 (61.284)\n",
      "Epoch: [110][156/390]\tTime 0.005 (0.004)\tLoss 1.0174 (1.1318)\tPrec@1 63.281 (60.375)\n",
      "Epoch: [110][234/390]\tTime 0.003 (0.004)\tLoss 1.2354 (1.1406)\tPrec@1 56.250 (60.126)\n",
      "Epoch: [110][312/390]\tTime 0.011 (0.004)\tLoss 1.2042 (1.1561)\tPrec@1 60.156 (59.697)\n",
      "Epoch: [110][390/390]\tTime 0.004 (0.004)\tLoss 1.2862 (1.1594)\tPrec@1 53.750 (59.472)\n",
      "EPOCH: 110 train Results: Prec@1 59.472 Loss: 1.1594\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1011 (1.1011)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6277 (1.2420)\tPrec@1 25.000 (55.760)\n",
      "EPOCH: 110 val Results: Prec@1 55.760 Loss: 1.2420\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [111][0/390]\tTime 0.004 (0.004)\tLoss 1.0992 (1.0992)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [111][78/390]\tTime 0.002 (0.003)\tLoss 1.0581 (1.1094)\tPrec@1 60.156 (61.590)\n",
      "Epoch: [111][156/390]\tTime 0.010 (0.003)\tLoss 1.1537 (1.1335)\tPrec@1 60.156 (60.544)\n",
      "Epoch: [111][234/390]\tTime 0.002 (0.003)\tLoss 1.3482 (1.1497)\tPrec@1 51.562 (60.086)\n",
      "Epoch: [111][312/390]\tTime 0.004 (0.003)\tLoss 1.1937 (1.1588)\tPrec@1 60.938 (59.600)\n",
      "Epoch: [111][390/390]\tTime 0.002 (0.003)\tLoss 1.0897 (1.1624)\tPrec@1 63.750 (59.412)\n",
      "EPOCH: 111 train Results: Prec@1 59.412 Loss: 1.1624\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1651 (1.1651)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3333 (1.2560)\tPrec@1 43.750 (55.210)\n",
      "EPOCH: 111 val Results: Prec@1 55.210 Loss: 1.2560\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [112][0/390]\tTime 0.005 (0.005)\tLoss 1.0849 (1.0849)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.0992 (1.1039)\tPrec@1 58.594 (61.788)\n",
      "Epoch: [112][156/390]\tTime 0.002 (0.003)\tLoss 1.2463 (1.1275)\tPrec@1 55.469 (60.823)\n",
      "Epoch: [112][234/390]\tTime 0.002 (0.003)\tLoss 1.1329 (1.1375)\tPrec@1 60.938 (60.572)\n",
      "Epoch: [112][312/390]\tTime 0.009 (0.003)\tLoss 1.0559 (1.1545)\tPrec@1 62.500 (59.857)\n",
      "Epoch: [112][390/390]\tTime 0.001 (0.003)\tLoss 1.3894 (1.1596)\tPrec@1 55.000 (59.618)\n",
      "EPOCH: 112 train Results: Prec@1 59.618 Loss: 1.1596\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1015 (1.1015)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5741 (1.2467)\tPrec@1 31.250 (55.740)\n",
      "EPOCH: 112 val Results: Prec@1 55.740 Loss: 1.2467\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 1.0990 (1.0990)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [113][78/390]\tTime 0.002 (0.003)\tLoss 1.1835 (1.1128)\tPrec@1 55.469 (61.343)\n",
      "Epoch: [113][156/390]\tTime 0.002 (0.003)\tLoss 1.1533 (1.1283)\tPrec@1 57.031 (60.793)\n",
      "Epoch: [113][234/390]\tTime 0.005 (0.003)\tLoss 0.9790 (1.1393)\tPrec@1 67.188 (60.316)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.003)\tLoss 1.0858 (1.1493)\tPrec@1 62.500 (59.927)\n",
      "Epoch: [113][390/390]\tTime 0.001 (0.003)\tLoss 1.2183 (1.1565)\tPrec@1 65.000 (59.640)\n",
      "EPOCH: 113 train Results: Prec@1 59.640 Loss: 1.1565\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2342 (1.2342)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3700 (1.2413)\tPrec@1 43.750 (55.730)\n",
      "EPOCH: 113 val Results: Prec@1 55.730 Loss: 1.2413\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [114][0/390]\tTime 0.008 (0.008)\tLoss 1.1404 (1.1404)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.003)\tLoss 0.9229 (1.1093)\tPrec@1 68.750 (61.729)\n",
      "Epoch: [114][156/390]\tTime 0.008 (0.004)\tLoss 1.1938 (1.1284)\tPrec@1 59.375 (61.037)\n",
      "Epoch: [114][234/390]\tTime 0.003 (0.004)\tLoss 1.1194 (1.1444)\tPrec@1 56.250 (60.316)\n",
      "Epoch: [114][312/390]\tTime 0.004 (0.003)\tLoss 1.0218 (1.1487)\tPrec@1 66.406 (60.206)\n",
      "Epoch: [114][390/390]\tTime 0.001 (0.003)\tLoss 1.2439 (1.1571)\tPrec@1 52.500 (59.756)\n",
      "EPOCH: 114 train Results: Prec@1 59.756 Loss: 1.1571\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1228 (1.1228)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6034 (1.2510)\tPrec@1 43.750 (55.490)\n",
      "EPOCH: 114 val Results: Prec@1 55.490 Loss: 1.2510\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [115][0/390]\tTime 0.004 (0.004)\tLoss 0.9699 (0.9699)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [115][78/390]\tTime 0.003 (0.003)\tLoss 1.0267 (1.0982)\tPrec@1 68.750 (62.223)\n",
      "Epoch: [115][156/390]\tTime 0.004 (0.003)\tLoss 1.3133 (1.1186)\tPrec@1 52.344 (61.137)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.004)\tLoss 1.2978 (1.1399)\tPrec@1 56.250 (60.133)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.004)\tLoss 1.3088 (1.1503)\tPrec@1 50.781 (59.844)\n",
      "Epoch: [115][390/390]\tTime 0.003 (0.004)\tLoss 1.3241 (1.1558)\tPrec@1 55.000 (59.566)\n",
      "EPOCH: 115 train Results: Prec@1 59.566 Loss: 1.1558\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1641 (1.1641)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5520 (1.2544)\tPrec@1 31.250 (55.560)\n",
      "EPOCH: 115 val Results: Prec@1 55.560 Loss: 1.2544\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [116][0/390]\tTime 0.002 (0.002)\tLoss 1.1826 (1.1826)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [116][78/390]\tTime 0.004 (0.003)\tLoss 1.1001 (1.1068)\tPrec@1 57.812 (62.154)\n",
      "Epoch: [116][156/390]\tTime 0.007 (0.003)\tLoss 1.2625 (1.1265)\tPrec@1 51.562 (60.813)\n",
      "Epoch: [116][234/390]\tTime 0.003 (0.003)\tLoss 1.1684 (1.1404)\tPrec@1 58.594 (60.249)\n",
      "Epoch: [116][312/390]\tTime 0.007 (0.003)\tLoss 1.2519 (1.1486)\tPrec@1 52.344 (59.907)\n",
      "Epoch: [116][390/390]\tTime 0.003 (0.003)\tLoss 1.2550 (1.1544)\tPrec@1 58.750 (59.690)\n",
      "EPOCH: 116 train Results: Prec@1 59.690 Loss: 1.1544\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1150 (1.1150)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3419 (1.2531)\tPrec@1 43.750 (55.390)\n",
      "EPOCH: 116 val Results: Prec@1 55.390 Loss: 1.2531\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [117][0/390]\tTime 0.004 (0.004)\tLoss 1.2014 (1.2014)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [117][78/390]\tTime 0.003 (0.003)\tLoss 1.1718 (1.1102)\tPrec@1 60.156 (61.650)\n",
      "Epoch: [117][156/390]\tTime 0.002 (0.004)\tLoss 1.1847 (1.1257)\tPrec@1 57.031 (60.923)\n",
      "Epoch: [117][234/390]\tTime 0.002 (0.004)\tLoss 1.2137 (1.1422)\tPrec@1 57.812 (60.243)\n",
      "Epoch: [117][312/390]\tTime 0.003 (0.004)\tLoss 1.1014 (1.1528)\tPrec@1 56.250 (59.769)\n",
      "Epoch: [117][390/390]\tTime 0.009 (0.004)\tLoss 1.1629 (1.1570)\tPrec@1 57.500 (59.576)\n",
      "EPOCH: 117 train Results: Prec@1 59.576 Loss: 1.1570\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0664 (1.0664)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.5636 (1.2429)\tPrec@1 31.250 (56.340)\n",
      "EPOCH: 117 val Results: Prec@1 56.340 Loss: 1.2429\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [118][0/390]\tTime 0.008 (0.008)\tLoss 1.0936 (1.0936)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [118][78/390]\tTime 0.014 (0.004)\tLoss 1.0596 (1.1198)\tPrec@1 60.938 (61.620)\n",
      "Epoch: [118][156/390]\tTime 0.002 (0.004)\tLoss 1.1126 (1.1223)\tPrec@1 60.156 (61.341)\n",
      "Epoch: [118][234/390]\tTime 0.004 (0.004)\tLoss 1.1953 (1.1308)\tPrec@1 51.562 (60.662)\n",
      "Epoch: [118][312/390]\tTime 0.004 (0.004)\tLoss 1.1852 (1.1452)\tPrec@1 64.844 (60.059)\n",
      "Epoch: [118][390/390]\tTime 0.004 (0.004)\tLoss 1.2045 (1.1570)\tPrec@1 63.750 (59.630)\n",
      "EPOCH: 118 train Results: Prec@1 59.630 Loss: 1.1570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1262 (1.1262)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2910 (1.2529)\tPrec@1 43.750 (55.640)\n",
      "EPOCH: 118 val Results: Prec@1 55.640 Loss: 1.2529\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [119][0/390]\tTime 0.006 (0.006)\tLoss 1.0976 (1.0976)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [119][78/390]\tTime 0.004 (0.004)\tLoss 1.2430 (1.0954)\tPrec@1 57.031 (62.816)\n",
      "Epoch: [119][156/390]\tTime 0.010 (0.004)\tLoss 1.2148 (1.1195)\tPrec@1 59.375 (61.321)\n",
      "Epoch: [119][234/390]\tTime 0.002 (0.004)\tLoss 1.1747 (1.1342)\tPrec@1 60.156 (60.588)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.003)\tLoss 1.1920 (1.1433)\tPrec@1 61.719 (60.074)\n",
      "Epoch: [119][390/390]\tTime 0.003 (0.003)\tLoss 1.2488 (1.1539)\tPrec@1 61.250 (59.710)\n",
      "EPOCH: 119 train Results: Prec@1 59.710 Loss: 1.1539\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1813 (1.1813)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4103 (1.2500)\tPrec@1 31.250 (55.720)\n",
      "EPOCH: 119 val Results: Prec@1 55.720 Loss: 1.2500\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [120][0/390]\tTime 0.002 (0.002)\tLoss 1.0023 (1.0023)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.003)\tLoss 1.1983 (1.1136)\tPrec@1 54.688 (61.501)\n",
      "Epoch: [120][156/390]\tTime 0.003 (0.003)\tLoss 1.0077 (1.1306)\tPrec@1 65.625 (60.738)\n",
      "Epoch: [120][234/390]\tTime 0.002 (0.003)\tLoss 1.1844 (1.1418)\tPrec@1 64.062 (60.346)\n",
      "Epoch: [120][312/390]\tTime 0.004 (0.003)\tLoss 1.2965 (1.1472)\tPrec@1 55.469 (60.016)\n",
      "Epoch: [120][390/390]\tTime 0.001 (0.003)\tLoss 1.0621 (1.1553)\tPrec@1 62.500 (59.640)\n",
      "EPOCH: 120 train Results: Prec@1 59.640 Loss: 1.1553\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1441 (1.1441)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1743 (1.2356)\tPrec@1 43.750 (56.140)\n",
      "EPOCH: 120 val Results: Prec@1 56.140 Loss: 1.2356\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [121][0/390]\tTime 0.002 (0.002)\tLoss 0.9757 (0.9757)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [121][78/390]\tTime 0.003 (0.003)\tLoss 1.0946 (1.0958)\tPrec@1 61.719 (61.353)\n",
      "Epoch: [121][156/390]\tTime 0.002 (0.003)\tLoss 1.1366 (1.1235)\tPrec@1 60.938 (60.704)\n",
      "Epoch: [121][234/390]\tTime 0.002 (0.003)\tLoss 1.2376 (1.1332)\tPrec@1 49.219 (60.436)\n",
      "Epoch: [121][312/390]\tTime 0.002 (0.003)\tLoss 1.0473 (1.1437)\tPrec@1 67.188 (60.024)\n",
      "Epoch: [121][390/390]\tTime 0.001 (0.003)\tLoss 0.9772 (1.1505)\tPrec@1 65.000 (59.786)\n",
      "EPOCH: 121 train Results: Prec@1 59.786 Loss: 1.1505\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1230 (1.1230)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4132 (1.2510)\tPrec@1 37.500 (55.070)\n",
      "EPOCH: 121 val Results: Prec@1 55.070 Loss: 1.2510\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [122][0/390]\tTime 0.002 (0.002)\tLoss 1.0414 (1.0414)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [122][78/390]\tTime 0.006 (0.005)\tLoss 0.9809 (1.0957)\tPrec@1 65.625 (62.391)\n",
      "Epoch: [122][156/390]\tTime 0.002 (0.004)\tLoss 1.0959 (1.1157)\tPrec@1 64.062 (61.664)\n",
      "Epoch: [122][234/390]\tTime 0.004 (0.004)\tLoss 1.2953 (1.1367)\tPrec@1 63.281 (60.768)\n",
      "Epoch: [122][312/390]\tTime 0.002 (0.003)\tLoss 1.1797 (1.1502)\tPrec@1 57.031 (60.111)\n",
      "Epoch: [122][390/390]\tTime 0.001 (0.003)\tLoss 1.3518 (1.1576)\tPrec@1 50.000 (59.784)\n",
      "EPOCH: 122 train Results: Prec@1 59.784 Loss: 1.1576\n",
      "Test: [0/78]\tTime 0.032 (0.032)\tLoss 1.1409 (1.1409)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3115 (1.2566)\tPrec@1 37.500 (55.970)\n",
      "EPOCH: 122 val Results: Prec@1 55.970 Loss: 1.2566\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [123][0/390]\tTime 0.003 (0.003)\tLoss 0.9790 (0.9790)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [123][78/390]\tTime 0.002 (0.003)\tLoss 1.1306 (1.0911)\tPrec@1 59.375 (62.500)\n",
      "Epoch: [123][156/390]\tTime 0.002 (0.003)\tLoss 1.2108 (1.1170)\tPrec@1 60.938 (61.390)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.003)\tLoss 1.3015 (1.1308)\tPrec@1 51.562 (60.898)\n",
      "Epoch: [123][312/390]\tTime 0.002 (0.003)\tLoss 1.2367 (1.1435)\tPrec@1 53.906 (60.378)\n",
      "Epoch: [123][390/390]\tTime 0.003 (0.003)\tLoss 1.1876 (1.1522)\tPrec@1 57.500 (59.896)\n",
      "EPOCH: 123 train Results: Prec@1 59.896 Loss: 1.1522\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1902 (1.1902)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4111 (1.2616)\tPrec@1 50.000 (55.460)\n",
      "EPOCH: 123 val Results: Prec@1 55.460 Loss: 1.2616\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [124][0/390]\tTime 0.003 (0.003)\tLoss 1.0534 (1.0534)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [124][78/390]\tTime 0.003 (0.003)\tLoss 1.1639 (1.1016)\tPrec@1 62.500 (61.788)\n",
      "Epoch: [124][156/390]\tTime 0.004 (0.003)\tLoss 1.1165 (1.1256)\tPrec@1 64.844 (61.052)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.2184 (1.1388)\tPrec@1 50.000 (60.419)\n",
      "Epoch: [124][312/390]\tTime 0.002 (0.003)\tLoss 1.1689 (1.1442)\tPrec@1 60.938 (60.109)\n",
      "Epoch: [124][390/390]\tTime 0.003 (0.003)\tLoss 1.2048 (1.1511)\tPrec@1 55.000 (59.834)\n",
      "EPOCH: 124 train Results: Prec@1 59.834 Loss: 1.1511\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1270 (1.1270)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4688 (1.2501)\tPrec@1 31.250 (55.060)\n",
      "EPOCH: 124 val Results: Prec@1 55.060 Loss: 1.2501\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [125][0/390]\tTime 0.004 (0.004)\tLoss 1.1011 (1.1011)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [125][78/390]\tTime 0.002 (0.003)\tLoss 0.9961 (1.1039)\tPrec@1 64.062 (61.521)\n",
      "Epoch: [125][156/390]\tTime 0.006 (0.003)\tLoss 1.1094 (1.1256)\tPrec@1 61.719 (60.828)\n",
      "Epoch: [125][234/390]\tTime 0.002 (0.003)\tLoss 1.1908 (1.1357)\tPrec@1 58.594 (60.585)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.2041 (1.1432)\tPrec@1 61.719 (60.318)\n",
      "Epoch: [125][390/390]\tTime 0.001 (0.003)\tLoss 1.0924 (1.1504)\tPrec@1 62.500 (59.998)\n",
      "EPOCH: 125 train Results: Prec@1 59.998 Loss: 1.1504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0948 (1.0948)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2835 (1.2593)\tPrec@1 43.750 (55.340)\n",
      "EPOCH: 125 val Results: Prec@1 55.340 Loss: 1.2593\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [126][0/390]\tTime 0.003 (0.003)\tLoss 0.9906 (0.9906)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [126][78/390]\tTime 0.004 (0.003)\tLoss 0.9394 (1.0932)\tPrec@1 67.188 (61.442)\n",
      "Epoch: [126][156/390]\tTime 0.005 (0.003)\tLoss 1.2332 (1.1186)\tPrec@1 57.812 (60.495)\n",
      "Epoch: [126][234/390]\tTime 0.002 (0.004)\tLoss 1.2466 (1.1298)\tPrec@1 53.906 (60.236)\n",
      "Epoch: [126][312/390]\tTime 0.006 (0.004)\tLoss 1.1222 (1.1413)\tPrec@1 55.469 (60.001)\n",
      "Epoch: [126][390/390]\tTime 0.001 (0.004)\tLoss 1.0944 (1.1491)\tPrec@1 62.500 (59.728)\n",
      "EPOCH: 126 train Results: Prec@1 59.728 Loss: 1.1491\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1385 (1.1385)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4670 (1.2498)\tPrec@1 37.500 (55.380)\n",
      "EPOCH: 126 val Results: Prec@1 55.380 Loss: 1.2498\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [127][0/390]\tTime 0.002 (0.002)\tLoss 1.1583 (1.1583)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.003)\tLoss 0.9643 (1.0910)\tPrec@1 70.312 (62.282)\n",
      "Epoch: [127][156/390]\tTime 0.004 (0.003)\tLoss 1.1667 (1.1134)\tPrec@1 54.688 (61.127)\n",
      "Epoch: [127][234/390]\tTime 0.009 (0.003)\tLoss 1.3443 (1.1282)\tPrec@1 50.781 (60.731)\n",
      "Epoch: [127][312/390]\tTime 0.002 (0.003)\tLoss 1.2595 (1.1434)\tPrec@1 55.469 (60.054)\n",
      "Epoch: [127][390/390]\tTime 0.004 (0.003)\tLoss 1.2107 (1.1504)\tPrec@1 57.500 (59.894)\n",
      "EPOCH: 127 train Results: Prec@1 59.894 Loss: 1.1504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1298 (1.1298)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5233 (1.2466)\tPrec@1 37.500 (56.000)\n",
      "EPOCH: 127 val Results: Prec@1 56.000 Loss: 1.2466\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [128][0/390]\tTime 0.004 (0.004)\tLoss 1.0780 (1.0780)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [128][78/390]\tTime 0.002 (0.003)\tLoss 1.1414 (1.1023)\tPrec@1 58.594 (62.263)\n",
      "Epoch: [128][156/390]\tTime 0.003 (0.003)\tLoss 1.0971 (1.1211)\tPrec@1 64.062 (61.092)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.003)\tLoss 1.0676 (1.1296)\tPrec@1 64.844 (60.625)\n",
      "Epoch: [128][312/390]\tTime 0.002 (0.003)\tLoss 1.0693 (1.1386)\tPrec@1 65.625 (60.348)\n",
      "Epoch: [128][390/390]\tTime 0.002 (0.003)\tLoss 1.3737 (1.1480)\tPrec@1 60.000 (59.984)\n",
      "EPOCH: 128 train Results: Prec@1 59.984 Loss: 1.1480\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0863 (1.0863)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2775 (1.2545)\tPrec@1 50.000 (54.980)\n",
      "EPOCH: 128 val Results: Prec@1 54.980 Loss: 1.2545\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [129][0/390]\tTime 0.004 (0.004)\tLoss 1.1941 (1.1941)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [129][78/390]\tTime 0.003 (0.003)\tLoss 1.0717 (1.0882)\tPrec@1 60.938 (62.470)\n",
      "Epoch: [129][156/390]\tTime 0.002 (0.003)\tLoss 1.1548 (1.1068)\tPrec@1 57.812 (61.843)\n",
      "Epoch: [129][234/390]\tTime 0.002 (0.003)\tLoss 1.2612 (1.1238)\tPrec@1 57.031 (61.120)\n",
      "Epoch: [129][312/390]\tTime 0.002 (0.003)\tLoss 1.0072 (1.1347)\tPrec@1 65.625 (60.481)\n",
      "Epoch: [129][390/390]\tTime 0.001 (0.003)\tLoss 1.1175 (1.1453)\tPrec@1 58.750 (60.076)\n",
      "EPOCH: 129 train Results: Prec@1 60.076 Loss: 1.1453\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1432 (1.1432)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4372 (1.2571)\tPrec@1 18.750 (55.310)\n",
      "EPOCH: 129 val Results: Prec@1 55.310 Loss: 1.2571\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [130][0/390]\tTime 0.002 (0.002)\tLoss 1.0536 (1.0536)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [130][78/390]\tTime 0.004 (0.006)\tLoss 0.9448 (1.0940)\tPrec@1 69.531 (62.184)\n",
      "Epoch: [130][156/390]\tTime 0.003 (0.005)\tLoss 1.2018 (1.1098)\tPrec@1 53.906 (61.331)\n",
      "Epoch: [130][234/390]\tTime 0.002 (0.005)\tLoss 1.3019 (1.1278)\tPrec@1 53.906 (60.688)\n",
      "Epoch: [130][312/390]\tTime 0.004 (0.005)\tLoss 1.1326 (1.1406)\tPrec@1 58.594 (60.156)\n",
      "Epoch: [130][390/390]\tTime 0.001 (0.005)\tLoss 1.3336 (1.1492)\tPrec@1 48.750 (59.832)\n",
      "EPOCH: 130 train Results: Prec@1 59.832 Loss: 1.1492\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1339 (1.1339)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1975 (1.2512)\tPrec@1 50.000 (55.920)\n",
      "EPOCH: 130 val Results: Prec@1 55.920 Loss: 1.2512\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [131][0/390]\tTime 0.010 (0.010)\tLoss 1.0103 (1.0103)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [131][78/390]\tTime 0.005 (0.003)\tLoss 1.1179 (1.1001)\tPrec@1 59.375 (61.798)\n",
      "Epoch: [131][156/390]\tTime 0.004 (0.003)\tLoss 1.1757 (1.1155)\tPrec@1 59.375 (61.027)\n",
      "Epoch: [131][234/390]\tTime 0.003 (0.003)\tLoss 1.1989 (1.1290)\tPrec@1 60.938 (60.499)\n",
      "Epoch: [131][312/390]\tTime 0.002 (0.003)\tLoss 1.3115 (1.1400)\tPrec@1 52.344 (60.076)\n",
      "Epoch: [131][390/390]\tTime 0.003 (0.003)\tLoss 1.0138 (1.1488)\tPrec@1 61.250 (59.736)\n",
      "EPOCH: 131 train Results: Prec@1 59.736 Loss: 1.1488\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1452 (1.1452)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6229 (1.2415)\tPrec@1 37.500 (55.770)\n",
      "EPOCH: 131 val Results: Prec@1 55.770 Loss: 1.2415\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [132][0/390]\tTime 0.003 (0.003)\tLoss 1.0855 (1.0855)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [132][78/390]\tTime 0.002 (0.003)\tLoss 1.1440 (1.0976)\tPrec@1 57.812 (62.144)\n",
      "Epoch: [132][156/390]\tTime 0.002 (0.003)\tLoss 1.2767 (1.1282)\tPrec@1 59.375 (60.868)\n",
      "Epoch: [132][234/390]\tTime 0.002 (0.003)\tLoss 1.0487 (1.1396)\tPrec@1 65.625 (60.306)\n",
      "Epoch: [132][312/390]\tTime 0.004 (0.003)\tLoss 1.4296 (1.1454)\tPrec@1 47.656 (59.939)\n",
      "Epoch: [132][390/390]\tTime 0.002 (0.003)\tLoss 1.1833 (1.1497)\tPrec@1 61.250 (59.720)\n",
      "EPOCH: 132 train Results: Prec@1 59.720 Loss: 1.1497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1070 (1.1070)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4059 (1.2402)\tPrec@1 43.750 (55.620)\n",
      "EPOCH: 132 val Results: Prec@1 55.620 Loss: 1.2402\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [133][0/390]\tTime 0.002 (0.002)\tLoss 0.9603 (0.9603)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [133][78/390]\tTime 0.003 (0.003)\tLoss 1.1173 (1.0968)\tPrec@1 57.031 (61.956)\n",
      "Epoch: [133][156/390]\tTime 0.002 (0.003)\tLoss 1.0990 (1.1099)\tPrec@1 62.500 (61.634)\n",
      "Epoch: [133][234/390]\tTime 0.004 (0.003)\tLoss 1.2531 (1.1242)\tPrec@1 55.469 (60.871)\n",
      "Epoch: [133][312/390]\tTime 0.009 (0.003)\tLoss 1.1994 (1.1376)\tPrec@1 60.938 (60.443)\n",
      "Epoch: [133][390/390]\tTime 0.001 (0.003)\tLoss 1.2176 (1.1466)\tPrec@1 50.000 (60.004)\n",
      "EPOCH: 133 train Results: Prec@1 60.004 Loss: 1.1466\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1612 (1.1612)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2432 (1.2553)\tPrec@1 56.250 (55.390)\n",
      "EPOCH: 133 val Results: Prec@1 55.390 Loss: 1.2553\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [134][0/390]\tTime 0.005 (0.005)\tLoss 1.1175 (1.1175)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [134][78/390]\tTime 0.005 (0.003)\tLoss 1.1281 (1.1076)\tPrec@1 64.062 (61.650)\n",
      "Epoch: [134][156/390]\tTime 0.009 (0.004)\tLoss 1.1467 (1.1290)\tPrec@1 58.594 (60.918)\n",
      "Epoch: [134][234/390]\tTime 0.005 (0.004)\tLoss 1.1290 (1.1384)\tPrec@1 55.469 (60.236)\n",
      "Epoch: [134][312/390]\tTime 0.004 (0.004)\tLoss 1.1206 (1.1442)\tPrec@1 63.281 (59.969)\n",
      "Epoch: [134][390/390]\tTime 0.001 (0.003)\tLoss 1.0960 (1.1492)\tPrec@1 63.750 (59.782)\n",
      "EPOCH: 134 train Results: Prec@1 59.782 Loss: 1.1492\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0845 (1.0845)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2780 (1.2495)\tPrec@1 31.250 (55.290)\n",
      "EPOCH: 134 val Results: Prec@1 55.290 Loss: 1.2495\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [135][0/390]\tTime 0.004 (0.004)\tLoss 1.0649 (1.0649)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [135][78/390]\tTime 0.002 (0.003)\tLoss 1.0546 (1.0966)\tPrec@1 64.844 (61.828)\n",
      "Epoch: [135][156/390]\tTime 0.005 (0.003)\tLoss 1.1629 (1.1128)\tPrec@1 60.156 (61.355)\n",
      "Epoch: [135][234/390]\tTime 0.002 (0.003)\tLoss 1.3060 (1.1270)\tPrec@1 53.125 (60.575)\n",
      "Epoch: [135][312/390]\tTime 0.003 (0.003)\tLoss 1.0905 (1.1399)\tPrec@1 56.250 (60.051)\n",
      "Epoch: [135][390/390]\tTime 0.011 (0.003)\tLoss 1.1912 (1.1502)\tPrec@1 63.750 (59.614)\n",
      "EPOCH: 135 train Results: Prec@1 59.614 Loss: 1.1502\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1646 (1.1646)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2105 (1.2404)\tPrec@1 43.750 (55.820)\n",
      "EPOCH: 135 val Results: Prec@1 55.820 Loss: 1.2404\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [136][0/390]\tTime 0.002 (0.002)\tLoss 0.8961 (0.8961)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [136][78/390]\tTime 0.002 (0.003)\tLoss 1.2695 (1.0830)\tPrec@1 53.125 (62.500)\n",
      "Epoch: [136][156/390]\tTime 0.003 (0.003)\tLoss 0.9731 (1.1084)\tPrec@1 67.188 (61.550)\n",
      "Epoch: [136][234/390]\tTime 0.002 (0.003)\tLoss 1.0884 (1.1259)\tPrec@1 67.188 (60.765)\n",
      "Epoch: [136][312/390]\tTime 0.003 (0.003)\tLoss 1.1901 (1.1354)\tPrec@1 57.031 (60.251)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.2459 (1.1440)\tPrec@1 56.250 (59.860)\n",
      "EPOCH: 136 train Results: Prec@1 59.860 Loss: 1.1440\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1110 (1.1110)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5573 (1.2399)\tPrec@1 37.500 (55.600)\n",
      "EPOCH: 136 val Results: Prec@1 55.600 Loss: 1.2399\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [137][0/390]\tTime 0.006 (0.006)\tLoss 1.1861 (1.1861)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [137][78/390]\tTime 0.002 (0.003)\tLoss 1.2124 (1.1054)\tPrec@1 56.250 (61.709)\n",
      "Epoch: [137][156/390]\tTime 0.004 (0.003)\tLoss 1.0470 (1.1172)\tPrec@1 64.844 (60.967)\n",
      "Epoch: [137][234/390]\tTime 0.008 (0.003)\tLoss 1.1154 (1.1249)\tPrec@1 59.375 (60.555)\n",
      "Epoch: [137][312/390]\tTime 0.002 (0.003)\tLoss 1.1874 (1.1337)\tPrec@1 60.156 (60.171)\n",
      "Epoch: [137][390/390]\tTime 0.001 (0.003)\tLoss 1.3120 (1.1458)\tPrec@1 61.250 (59.772)\n",
      "EPOCH: 137 train Results: Prec@1 59.772 Loss: 1.1458\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.0862 (1.0862)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5532 (1.2446)\tPrec@1 31.250 (55.690)\n",
      "EPOCH: 137 val Results: Prec@1 55.690 Loss: 1.2446\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [138][0/390]\tTime 0.004 (0.004)\tLoss 1.1835 (1.1835)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [138][78/390]\tTime 0.003 (0.003)\tLoss 1.0567 (1.1023)\tPrec@1 65.625 (61.778)\n",
      "Epoch: [138][156/390]\tTime 0.002 (0.003)\tLoss 1.0805 (1.1175)\tPrec@1 59.375 (61.171)\n",
      "Epoch: [138][234/390]\tTime 0.002 (0.003)\tLoss 1.2015 (1.1307)\tPrec@1 57.031 (60.559)\n",
      "Epoch: [138][312/390]\tTime 0.004 (0.003)\tLoss 1.3832 (1.1403)\tPrec@1 49.219 (60.181)\n",
      "Epoch: [138][390/390]\tTime 0.002 (0.004)\tLoss 1.2037 (1.1482)\tPrec@1 57.500 (59.794)\n",
      "EPOCH: 138 train Results: Prec@1 59.794 Loss: 1.1482\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0919 (1.0919)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3391 (1.2422)\tPrec@1 56.250 (55.830)\n",
      "EPOCH: 138 val Results: Prec@1 55.830 Loss: 1.2422\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [139][0/390]\tTime 0.003 (0.003)\tLoss 1.0929 (1.0929)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [139][78/390]\tTime 0.003 (0.004)\tLoss 1.0045 (1.0944)\tPrec@1 62.500 (61.402)\n",
      "Epoch: [139][156/390]\tTime 0.003 (0.004)\tLoss 1.0840 (1.1145)\tPrec@1 64.062 (61.002)\n",
      "Epoch: [139][234/390]\tTime 0.002 (0.004)\tLoss 1.1825 (1.1282)\tPrec@1 59.375 (60.578)\n",
      "Epoch: [139][312/390]\tTime 0.004 (0.003)\tLoss 1.1088 (1.1372)\tPrec@1 60.938 (60.271)\n",
      "Epoch: [139][390/390]\tTime 0.003 (0.003)\tLoss 1.1433 (1.1452)\tPrec@1 63.750 (59.928)\n",
      "EPOCH: 139 train Results: Prec@1 59.928 Loss: 1.1452\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2021 (1.2021)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3037 (1.2549)\tPrec@1 37.500 (54.950)\n",
      "EPOCH: 139 val Results: Prec@1 54.950 Loss: 1.2549\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [140][0/390]\tTime 0.003 (0.003)\tLoss 1.1463 (1.1463)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [140][78/390]\tTime 0.003 (0.003)\tLoss 1.2214 (1.0974)\tPrec@1 53.125 (62.233)\n",
      "Epoch: [140][156/390]\tTime 0.004 (0.003)\tLoss 1.1476 (1.1127)\tPrec@1 57.812 (61.291)\n",
      "Epoch: [140][234/390]\tTime 0.002 (0.003)\tLoss 0.9973 (1.1212)\tPrec@1 64.844 (61.004)\n",
      "Epoch: [140][312/390]\tTime 0.002 (0.003)\tLoss 1.1393 (1.1322)\tPrec@1 55.469 (60.473)\n",
      "Epoch: [140][390/390]\tTime 0.001 (0.003)\tLoss 1.3922 (1.1424)\tPrec@1 51.250 (59.998)\n",
      "EPOCH: 140 train Results: Prec@1 59.998 Loss: 1.1424\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1229 (1.1229)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4440 (1.2502)\tPrec@1 25.000 (55.320)\n",
      "EPOCH: 140 val Results: Prec@1 55.320 Loss: 1.2502\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [141][0/390]\tTime 0.003 (0.003)\tLoss 0.8732 (0.8732)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [141][78/390]\tTime 0.002 (0.003)\tLoss 1.2688 (1.0897)\tPrec@1 57.812 (62.045)\n",
      "Epoch: [141][156/390]\tTime 0.002 (0.003)\tLoss 1.1697 (1.1156)\tPrec@1 53.125 (60.962)\n",
      "Epoch: [141][234/390]\tTime 0.003 (0.003)\tLoss 1.2636 (1.1289)\tPrec@1 57.031 (60.346)\n",
      "Epoch: [141][312/390]\tTime 0.004 (0.003)\tLoss 1.2006 (1.1392)\tPrec@1 59.375 (59.969)\n",
      "Epoch: [141][390/390]\tTime 0.002 (0.003)\tLoss 1.2238 (1.1455)\tPrec@1 57.500 (59.806)\n",
      "EPOCH: 141 train Results: Prec@1 59.806 Loss: 1.1455\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1921 (1.1921)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3005 (1.2555)\tPrec@1 50.000 (54.990)\n",
      "EPOCH: 141 val Results: Prec@1 54.990 Loss: 1.2555\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [142][0/390]\tTime 0.004 (0.004)\tLoss 1.0938 (1.0938)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [142][78/390]\tTime 0.004 (0.004)\tLoss 1.0885 (1.0904)\tPrec@1 65.625 (62.708)\n",
      "Epoch: [142][156/390]\tTime 0.005 (0.004)\tLoss 1.0308 (1.1112)\tPrec@1 64.844 (61.510)\n",
      "Epoch: [142][234/390]\tTime 0.002 (0.004)\tLoss 1.2122 (1.1218)\tPrec@1 63.281 (61.114)\n",
      "Epoch: [142][312/390]\tTime 0.002 (0.004)\tLoss 1.1478 (1.1366)\tPrec@1 58.594 (60.513)\n",
      "Epoch: [142][390/390]\tTime 0.001 (0.004)\tLoss 1.2417 (1.1451)\tPrec@1 56.250 (60.090)\n",
      "EPOCH: 142 train Results: Prec@1 60.090 Loss: 1.1451\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1926 (1.1926)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2157 (1.2600)\tPrec@1 68.750 (55.360)\n",
      "EPOCH: 142 val Results: Prec@1 55.360 Loss: 1.2600\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [143][0/390]\tTime 0.005 (0.005)\tLoss 1.0383 (1.0383)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [143][78/390]\tTime 0.004 (0.003)\tLoss 1.0758 (1.0956)\tPrec@1 65.625 (62.302)\n",
      "Epoch: [143][156/390]\tTime 0.006 (0.003)\tLoss 1.1478 (1.1144)\tPrec@1 60.156 (61.654)\n",
      "Epoch: [143][234/390]\tTime 0.010 (0.003)\tLoss 1.0953 (1.1253)\tPrec@1 62.500 (61.150)\n",
      "Epoch: [143][312/390]\tTime 0.010 (0.003)\tLoss 1.2860 (1.1382)\tPrec@1 51.562 (60.446)\n",
      "Epoch: [143][390/390]\tTime 0.001 (0.003)\tLoss 1.1085 (1.1435)\tPrec@1 58.750 (60.280)\n",
      "EPOCH: 143 train Results: Prec@1 60.280 Loss: 1.1435\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1289 (1.1289)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2713 (1.2481)\tPrec@1 50.000 (56.140)\n",
      "EPOCH: 143 val Results: Prec@1 56.140 Loss: 1.2481\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [144][0/390]\tTime 0.004 (0.004)\tLoss 1.0751 (1.0751)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [144][78/390]\tTime 0.002 (0.003)\tLoss 1.1076 (1.1091)\tPrec@1 57.812 (61.195)\n",
      "Epoch: [144][156/390]\tTime 0.002 (0.004)\tLoss 1.3805 (1.1199)\tPrec@1 51.562 (60.843)\n",
      "Epoch: [144][234/390]\tTime 0.003 (0.003)\tLoss 1.1135 (1.1331)\tPrec@1 61.719 (60.512)\n",
      "Epoch: [144][312/390]\tTime 0.003 (0.003)\tLoss 1.1669 (1.1370)\tPrec@1 56.250 (60.473)\n",
      "Epoch: [144][390/390]\tTime 0.004 (0.003)\tLoss 1.1530 (1.1435)\tPrec@1 56.250 (60.182)\n",
      "EPOCH: 144 train Results: Prec@1 60.182 Loss: 1.1435\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0868 (1.0868)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5258 (1.2546)\tPrec@1 43.750 (55.350)\n",
      "EPOCH: 144 val Results: Prec@1 55.350 Loss: 1.2546\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [145][0/390]\tTime 0.002 (0.002)\tLoss 1.0388 (1.0388)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [145][78/390]\tTime 0.005 (0.004)\tLoss 0.9706 (1.1041)\tPrec@1 69.531 (61.303)\n",
      "Epoch: [145][156/390]\tTime 0.004 (0.003)\tLoss 1.1566 (1.1109)\tPrec@1 61.719 (61.465)\n",
      "Epoch: [145][234/390]\tTime 0.003 (0.003)\tLoss 1.1338 (1.1254)\tPrec@1 57.031 (60.901)\n",
      "Epoch: [145][312/390]\tTime 0.004 (0.003)\tLoss 1.1633 (1.1391)\tPrec@1 57.031 (60.201)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.003)\tLoss 1.1293 (1.1454)\tPrec@1 60.000 (60.044)\n",
      "EPOCH: 145 train Results: Prec@1 60.044 Loss: 1.1454\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1843 (1.1843)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6127 (1.2593)\tPrec@1 31.250 (55.310)\n",
      "EPOCH: 145 val Results: Prec@1 55.310 Loss: 1.2593\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [146][0/390]\tTime 0.007 (0.007)\tLoss 1.1269 (1.1269)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [146][78/390]\tTime 0.003 (0.004)\tLoss 1.0407 (1.0918)\tPrec@1 66.406 (62.362)\n",
      "Epoch: [146][156/390]\tTime 0.002 (0.003)\tLoss 1.0664 (1.1170)\tPrec@1 57.812 (61.087)\n",
      "Epoch: [146][234/390]\tTime 0.011 (0.004)\tLoss 1.0803 (1.1249)\tPrec@1 64.062 (60.888)\n",
      "Epoch: [146][312/390]\tTime 0.004 (0.004)\tLoss 1.0549 (1.1363)\tPrec@1 60.156 (60.348)\n",
      "Epoch: [146][390/390]\tTime 0.002 (0.004)\tLoss 0.9395 (1.1434)\tPrec@1 72.500 (60.074)\n",
      "EPOCH: 146 train Results: Prec@1 60.074 Loss: 1.1434\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1482 (1.1482)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2911 (1.2461)\tPrec@1 50.000 (56.060)\n",
      "EPOCH: 146 val Results: Prec@1 56.060 Loss: 1.2461\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [147][0/390]\tTime 0.003 (0.003)\tLoss 1.1092 (1.1092)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [147][78/390]\tTime 0.004 (0.003)\tLoss 1.0952 (1.0947)\tPrec@1 64.062 (62.203)\n",
      "Epoch: [147][156/390]\tTime 0.002 (0.003)\tLoss 1.1590 (1.1143)\tPrec@1 63.281 (61.221)\n",
      "Epoch: [147][234/390]\tTime 0.003 (0.004)\tLoss 1.1427 (1.1219)\tPrec@1 61.719 (60.911)\n",
      "Epoch: [147][312/390]\tTime 0.002 (0.003)\tLoss 1.1816 (1.1325)\tPrec@1 55.469 (60.518)\n",
      "Epoch: [147][390/390]\tTime 0.003 (0.003)\tLoss 1.1530 (1.1407)\tPrec@1 62.500 (60.170)\n",
      "EPOCH: 147 train Results: Prec@1 60.170 Loss: 1.1407\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1240 (1.1240)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6233 (1.2500)\tPrec@1 25.000 (55.530)\n",
      "EPOCH: 147 val Results: Prec@1 55.530 Loss: 1.2500\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [148][0/390]\tTime 0.005 (0.005)\tLoss 1.0665 (1.0665)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [148][78/390]\tTime 0.002 (0.003)\tLoss 1.1753 (1.0987)\tPrec@1 56.250 (62.391)\n",
      "Epoch: [148][156/390]\tTime 0.002 (0.003)\tLoss 1.2185 (1.1146)\tPrec@1 58.594 (61.540)\n",
      "Epoch: [148][234/390]\tTime 0.002 (0.003)\tLoss 1.2029 (1.1306)\tPrec@1 53.906 (60.615)\n",
      "Epoch: [148][312/390]\tTime 0.006 (0.003)\tLoss 1.0836 (1.1381)\tPrec@1 62.500 (60.284)\n",
      "Epoch: [148][390/390]\tTime 0.001 (0.003)\tLoss 1.0946 (1.1472)\tPrec@1 65.000 (59.988)\n",
      "EPOCH: 148 train Results: Prec@1 59.988 Loss: 1.1472\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.2157 (1.2157)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4765 (1.2592)\tPrec@1 31.250 (55.320)\n",
      "EPOCH: 148 val Results: Prec@1 55.320 Loss: 1.2592\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [149][0/390]\tTime 0.008 (0.008)\tLoss 0.9577 (0.9577)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [149][78/390]\tTime 0.002 (0.004)\tLoss 1.1148 (1.0861)\tPrec@1 57.031 (62.263)\n",
      "Epoch: [149][156/390]\tTime 0.007 (0.003)\tLoss 1.1931 (1.1034)\tPrec@1 57.031 (61.604)\n",
      "Epoch: [149][234/390]\tTime 0.004 (0.003)\tLoss 1.0228 (1.1203)\tPrec@1 67.188 (60.834)\n",
      "Epoch: [149][312/390]\tTime 0.003 (0.003)\tLoss 1.1448 (1.1309)\tPrec@1 60.938 (60.478)\n",
      "Epoch: [149][390/390]\tTime 0.004 (0.003)\tLoss 1.2735 (1.1415)\tPrec@1 55.000 (60.086)\n",
      "EPOCH: 149 train Results: Prec@1 60.086 Loss: 1.1415\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1343 (1.1343)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.2997 (1.2513)\tPrec@1 43.750 (55.160)\n",
      "EPOCH: 149 val Results: Prec@1 55.160 Loss: 1.2513\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [150][0/390]\tTime 0.005 (0.005)\tLoss 1.1374 (1.1374)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [150][78/390]\tTime 0.003 (0.004)\tLoss 1.2439 (1.0788)\tPrec@1 58.594 (62.896)\n",
      "Epoch: [150][156/390]\tTime 0.003 (0.004)\tLoss 1.2378 (1.0981)\tPrec@1 63.281 (62.032)\n",
      "Epoch: [150][234/390]\tTime 0.002 (0.004)\tLoss 1.3329 (1.1220)\tPrec@1 54.688 (60.798)\n",
      "Epoch: [150][312/390]\tTime 0.006 (0.003)\tLoss 1.1746 (1.1337)\tPrec@1 55.469 (60.321)\n",
      "Epoch: [150][390/390]\tTime 0.002 (0.003)\tLoss 1.1804 (1.1410)\tPrec@1 61.250 (59.978)\n",
      "EPOCH: 150 train Results: Prec@1 59.978 Loss: 1.1410\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1274 (1.1274)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2538 (1.2407)\tPrec@1 43.750 (55.610)\n",
      "EPOCH: 150 val Results: Prec@1 55.610 Loss: 1.2407\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [151][0/390]\tTime 0.002 (0.002)\tLoss 1.0541 (1.0541)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [151][78/390]\tTime 0.005 (0.003)\tLoss 1.1609 (1.0877)\tPrec@1 60.938 (61.659)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.003)\tLoss 1.1813 (1.1066)\tPrec@1 56.250 (61.067)\n",
      "Epoch: [151][234/390]\tTime 0.003 (0.003)\tLoss 1.2002 (1.1183)\tPrec@1 56.250 (60.642)\n",
      "Epoch: [151][312/390]\tTime 0.002 (0.003)\tLoss 1.2084 (1.1303)\tPrec@1 60.938 (60.134)\n",
      "Epoch: [151][390/390]\tTime 0.002 (0.003)\tLoss 1.2046 (1.1413)\tPrec@1 61.250 (59.774)\n",
      "EPOCH: 151 train Results: Prec@1 59.774 Loss: 1.1413\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1509 (1.1509)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3653 (1.2545)\tPrec@1 50.000 (55.490)\n",
      "EPOCH: 151 val Results: Prec@1 55.490 Loss: 1.2545\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [152][0/390]\tTime 0.005 (0.005)\tLoss 1.0240 (1.0240)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [152][78/390]\tTime 0.004 (0.003)\tLoss 1.0484 (1.0944)\tPrec@1 58.594 (62.095)\n",
      "Epoch: [152][156/390]\tTime 0.002 (0.004)\tLoss 1.0757 (1.1115)\tPrec@1 64.062 (61.321)\n",
      "Epoch: [152][234/390]\tTime 0.005 (0.003)\tLoss 1.2127 (1.1220)\tPrec@1 61.719 (60.981)\n",
      "Epoch: [152][312/390]\tTime 0.003 (0.003)\tLoss 1.2047 (1.1303)\tPrec@1 54.688 (60.518)\n",
      "Epoch: [152][390/390]\tTime 0.002 (0.003)\tLoss 1.0438 (1.1396)\tPrec@1 63.750 (60.260)\n",
      "EPOCH: 152 train Results: Prec@1 60.260 Loss: 1.1396\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1545 (1.1545)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3952 (1.2419)\tPrec@1 31.250 (55.750)\n",
      "EPOCH: 152 val Results: Prec@1 55.750 Loss: 1.2419\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [153][0/390]\tTime 0.003 (0.003)\tLoss 0.9424 (0.9424)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [153][78/390]\tTime 0.002 (0.004)\tLoss 1.0394 (1.0830)\tPrec@1 63.281 (63.054)\n",
      "Epoch: [153][156/390]\tTime 0.003 (0.003)\tLoss 1.1286 (1.1042)\tPrec@1 63.281 (61.903)\n",
      "Epoch: [153][234/390]\tTime 0.006 (0.003)\tLoss 1.0719 (1.1150)\tPrec@1 62.500 (61.230)\n",
      "Epoch: [153][312/390]\tTime 0.002 (0.003)\tLoss 1.0535 (1.1296)\tPrec@1 62.500 (60.770)\n",
      "Epoch: [153][390/390]\tTime 0.011 (0.003)\tLoss 1.2311 (1.1394)\tPrec@1 53.750 (60.298)\n",
      "EPOCH: 153 train Results: Prec@1 60.298 Loss: 1.1394\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1437 (1.1437)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4631 (1.2474)\tPrec@1 43.750 (55.450)\n",
      "EPOCH: 153 val Results: Prec@1 55.450 Loss: 1.2474\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [154][0/390]\tTime 0.004 (0.004)\tLoss 1.0334 (1.0334)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.003)\tLoss 1.0546 (1.0879)\tPrec@1 63.281 (62.540)\n",
      "Epoch: [154][156/390]\tTime 0.003 (0.003)\tLoss 1.1342 (1.1067)\tPrec@1 59.375 (61.729)\n",
      "Epoch: [154][234/390]\tTime 0.007 (0.003)\tLoss 1.2588 (1.1207)\tPrec@1 57.031 (60.931)\n",
      "Epoch: [154][312/390]\tTime 0.004 (0.003)\tLoss 1.2090 (1.1340)\tPrec@1 55.469 (60.473)\n",
      "Epoch: [154][390/390]\tTime 0.003 (0.003)\tLoss 1.2551 (1.1437)\tPrec@1 56.250 (60.054)\n",
      "EPOCH: 154 train Results: Prec@1 60.054 Loss: 1.1437\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1228 (1.1228)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6224 (1.2399)\tPrec@1 31.250 (55.500)\n",
      "EPOCH: 154 val Results: Prec@1 55.500 Loss: 1.2399\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [155][0/390]\tTime 0.002 (0.002)\tLoss 1.1409 (1.1409)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [155][78/390]\tTime 0.002 (0.003)\tLoss 1.0628 (1.0846)\tPrec@1 64.844 (62.045)\n",
      "Epoch: [155][156/390]\tTime 0.002 (0.003)\tLoss 1.0373 (1.1006)\tPrec@1 64.844 (61.574)\n",
      "Epoch: [155][234/390]\tTime 0.002 (0.003)\tLoss 1.2211 (1.1156)\tPrec@1 54.688 (60.934)\n",
      "Epoch: [155][312/390]\tTime 0.002 (0.003)\tLoss 1.2089 (1.1288)\tPrec@1 59.375 (60.483)\n",
      "Epoch: [155][390/390]\tTime 0.002 (0.003)\tLoss 1.0474 (1.1384)\tPrec@1 65.000 (60.138)\n",
      "EPOCH: 155 train Results: Prec@1 60.138 Loss: 1.1384\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1487 (1.1487)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2987 (1.2522)\tPrec@1 37.500 (55.440)\n",
      "EPOCH: 155 val Results: Prec@1 55.440 Loss: 1.2522\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [156][0/390]\tTime 0.003 (0.003)\tLoss 0.9841 (0.9841)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [156][78/390]\tTime 0.002 (0.003)\tLoss 1.1360 (1.0838)\tPrec@1 57.031 (62.391)\n",
      "Epoch: [156][156/390]\tTime 0.004 (0.004)\tLoss 1.3444 (1.1061)\tPrec@1 52.344 (61.629)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.004)\tLoss 1.1282 (1.1237)\tPrec@1 63.281 (60.695)\n",
      "Epoch: [156][312/390]\tTime 0.005 (0.005)\tLoss 1.2985 (1.1367)\tPrec@1 52.344 (60.224)\n",
      "Epoch: [156][390/390]\tTime 0.001 (0.005)\tLoss 1.2159 (1.1430)\tPrec@1 57.500 (60.016)\n",
      "EPOCH: 156 train Results: Prec@1 60.016 Loss: 1.1430\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1434 (1.1434)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4826 (1.2422)\tPrec@1 50.000 (56.270)\n",
      "EPOCH: 156 val Results: Prec@1 56.270 Loss: 1.2422\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [157][0/390]\tTime 0.005 (0.005)\tLoss 1.1553 (1.1553)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [157][78/390]\tTime 0.002 (0.003)\tLoss 1.0624 (1.0825)\tPrec@1 64.062 (62.688)\n",
      "Epoch: [157][156/390]\tTime 0.003 (0.004)\tLoss 1.0124 (1.1001)\tPrec@1 64.844 (61.803)\n",
      "Epoch: [157][234/390]\tTime 0.010 (0.004)\tLoss 1.0701 (1.1244)\tPrec@1 61.719 (60.602)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.004)\tLoss 1.2183 (1.1360)\tPrec@1 54.688 (60.244)\n",
      "Epoch: [157][390/390]\tTime 0.002 (0.004)\tLoss 1.1243 (1.1420)\tPrec@1 61.250 (60.030)\n",
      "EPOCH: 157 train Results: Prec@1 60.030 Loss: 1.1420\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1010 (1.1010)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3660 (1.2506)\tPrec@1 37.500 (55.240)\n",
      "EPOCH: 157 val Results: Prec@1 55.240 Loss: 1.2506\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [158][0/390]\tTime 0.004 (0.004)\tLoss 1.0280 (1.0280)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [158][78/390]\tTime 0.002 (0.003)\tLoss 1.2641 (1.0886)\tPrec@1 53.125 (61.857)\n",
      "Epoch: [158][156/390]\tTime 0.003 (0.003)\tLoss 1.1656 (1.1035)\tPrec@1 60.156 (61.002)\n",
      "Epoch: [158][234/390]\tTime 0.002 (0.004)\tLoss 1.1224 (1.1196)\tPrec@1 59.375 (60.386)\n",
      "Epoch: [158][312/390]\tTime 0.002 (0.004)\tLoss 1.1896 (1.1289)\tPrec@1 57.812 (60.144)\n",
      "Epoch: [158][390/390]\tTime 0.001 (0.004)\tLoss 1.2571 (1.1377)\tPrec@1 61.250 (59.818)\n",
      "EPOCH: 158 train Results: Prec@1 59.818 Loss: 1.1377\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1534 (1.1534)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4571 (1.2500)\tPrec@1 37.500 (55.310)\n",
      "EPOCH: 158 val Results: Prec@1 55.310 Loss: 1.2500\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [159][0/390]\tTime 0.003 (0.003)\tLoss 0.9971 (0.9971)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [159][78/390]\tTime 0.002 (0.004)\tLoss 1.1044 (1.1016)\tPrec@1 59.375 (61.511)\n",
      "Epoch: [159][156/390]\tTime 0.007 (0.005)\tLoss 1.0462 (1.1158)\tPrec@1 64.062 (60.833)\n",
      "Epoch: [159][234/390]\tTime 0.003 (0.005)\tLoss 0.9532 (1.1214)\tPrec@1 65.625 (60.731)\n",
      "Epoch: [159][312/390]\tTime 0.004 (0.005)\tLoss 1.1287 (1.1324)\tPrec@1 59.375 (60.316)\n",
      "Epoch: [159][390/390]\tTime 0.001 (0.005)\tLoss 1.2236 (1.1400)\tPrec@1 56.250 (59.928)\n",
      "EPOCH: 159 train Results: Prec@1 59.928 Loss: 1.1400\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0983 (1.0983)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5076 (1.2419)\tPrec@1 37.500 (55.960)\n",
      "EPOCH: 159 val Results: Prec@1 55.960 Loss: 1.2419\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [160][0/390]\tTime 0.002 (0.002)\tLoss 0.9532 (0.9532)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [160][78/390]\tTime 0.003 (0.003)\tLoss 1.1620 (1.1123)\tPrec@1 60.938 (61.521)\n",
      "Epoch: [160][156/390]\tTime 0.004 (0.003)\tLoss 1.1219 (1.1127)\tPrec@1 63.281 (61.072)\n",
      "Epoch: [160][234/390]\tTime 0.002 (0.004)\tLoss 1.0529 (1.1236)\tPrec@1 63.281 (60.615)\n",
      "Epoch: [160][312/390]\tTime 0.003 (0.004)\tLoss 1.1153 (1.1324)\tPrec@1 59.375 (60.229)\n",
      "Epoch: [160][390/390]\tTime 0.001 (0.004)\tLoss 1.0909 (1.1343)\tPrec@1 67.500 (60.250)\n",
      "EPOCH: 160 train Results: Prec@1 60.250 Loss: 1.1343\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1245 (1.1245)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3002 (1.2507)\tPrec@1 37.500 (55.670)\n",
      "EPOCH: 160 val Results: Prec@1 55.670 Loss: 1.2507\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [161][0/390]\tTime 0.013 (0.013)\tLoss 1.1373 (1.1373)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [161][78/390]\tTime 0.003 (0.003)\tLoss 1.0381 (1.0758)\tPrec@1 67.188 (62.520)\n",
      "Epoch: [161][156/390]\tTime 0.010 (0.003)\tLoss 1.1425 (1.1033)\tPrec@1 60.156 (61.480)\n",
      "Epoch: [161][234/390]\tTime 0.010 (0.004)\tLoss 1.1560 (1.1205)\tPrec@1 60.938 (60.921)\n",
      "Epoch: [161][312/390]\tTime 0.003 (0.004)\tLoss 1.1128 (1.1279)\tPrec@1 64.844 (60.630)\n",
      "Epoch: [161][390/390]\tTime 0.002 (0.004)\tLoss 1.4816 (1.1386)\tPrec@1 50.000 (60.152)\n",
      "EPOCH: 161 train Results: Prec@1 60.152 Loss: 1.1386\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1168 (1.1168)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3402 (1.2391)\tPrec@1 50.000 (55.710)\n",
      "EPOCH: 161 val Results: Prec@1 55.710 Loss: 1.2391\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.9790 (0.9790)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [162][78/390]\tTime 0.003 (0.004)\tLoss 1.1750 (1.0847)\tPrec@1 60.938 (62.549)\n",
      "Epoch: [162][156/390]\tTime 0.002 (0.004)\tLoss 1.0534 (1.1048)\tPrec@1 68.750 (61.793)\n",
      "Epoch: [162][234/390]\tTime 0.004 (0.004)\tLoss 1.0764 (1.1145)\tPrec@1 64.844 (61.253)\n",
      "Epoch: [162][312/390]\tTime 0.002 (0.004)\tLoss 1.0691 (1.1258)\tPrec@1 62.500 (60.613)\n",
      "Epoch: [162][390/390]\tTime 0.003 (0.004)\tLoss 1.1647 (1.1336)\tPrec@1 61.250 (60.322)\n",
      "EPOCH: 162 train Results: Prec@1 60.322 Loss: 1.1336\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1593 (1.1593)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6435 (1.2494)\tPrec@1 25.000 (55.360)\n",
      "EPOCH: 162 val Results: Prec@1 55.360 Loss: 1.2494\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [163][0/390]\tTime 0.005 (0.005)\tLoss 1.2057 (1.2057)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [163][78/390]\tTime 0.002 (0.003)\tLoss 1.0366 (1.0997)\tPrec@1 61.719 (61.640)\n",
      "Epoch: [163][156/390]\tTime 0.004 (0.003)\tLoss 1.1070 (1.1024)\tPrec@1 53.906 (61.589)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.003)\tLoss 1.0859 (1.1214)\tPrec@1 63.281 (60.731)\n",
      "Epoch: [163][312/390]\tTime 0.012 (0.003)\tLoss 1.0853 (1.1315)\tPrec@1 66.406 (60.426)\n",
      "Epoch: [163][390/390]\tTime 0.006 (0.003)\tLoss 1.1357 (1.1389)\tPrec@1 62.500 (60.078)\n",
      "EPOCH: 163 train Results: Prec@1 60.078 Loss: 1.1389\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1585 (1.1585)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.2784 (1.2343)\tPrec@1 37.500 (56.050)\n",
      "EPOCH: 163 val Results: Prec@1 56.050 Loss: 1.2343\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [164][0/390]\tTime 0.004 (0.004)\tLoss 1.0890 (1.0890)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [164][78/390]\tTime 0.002 (0.003)\tLoss 1.1576 (1.0640)\tPrec@1 63.281 (63.212)\n",
      "Epoch: [164][156/390]\tTime 0.012 (0.006)\tLoss 1.1916 (1.0933)\tPrec@1 57.812 (62.261)\n",
      "Epoch: [164][234/390]\tTime 0.004 (0.005)\tLoss 1.1104 (1.1088)\tPrec@1 59.375 (61.569)\n",
      "Epoch: [164][312/390]\tTime 0.008 (0.005)\tLoss 1.2226 (1.1261)\tPrec@1 55.469 (60.810)\n",
      "Epoch: [164][390/390]\tTime 0.003 (0.005)\tLoss 1.1511 (1.1342)\tPrec@1 60.000 (60.374)\n",
      "EPOCH: 164 train Results: Prec@1 60.374 Loss: 1.1342\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1231 (1.1231)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4422 (1.2483)\tPrec@1 31.250 (55.610)\n",
      "EPOCH: 164 val Results: Prec@1 55.610 Loss: 1.2483\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [165][0/390]\tTime 0.003 (0.003)\tLoss 1.1592 (1.1592)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [165][78/390]\tTime 0.004 (0.004)\tLoss 0.9965 (1.0569)\tPrec@1 68.750 (63.588)\n",
      "Epoch: [165][156/390]\tTime 0.004 (0.004)\tLoss 1.0846 (1.0949)\tPrec@1 62.500 (62.137)\n",
      "Epoch: [165][234/390]\tTime 0.002 (0.004)\tLoss 1.3274 (1.1099)\tPrec@1 51.562 (61.566)\n",
      "Epoch: [165][312/390]\tTime 0.002 (0.004)\tLoss 1.1677 (1.1235)\tPrec@1 64.062 (60.972)\n",
      "Epoch: [165][390/390]\tTime 0.001 (0.004)\tLoss 1.3698 (1.1336)\tPrec@1 57.500 (60.454)\n",
      "EPOCH: 165 train Results: Prec@1 60.454 Loss: 1.1336\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1565 (1.1565)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4033 (1.2500)\tPrec@1 25.000 (55.380)\n",
      "EPOCH: 165 val Results: Prec@1 55.380 Loss: 1.2500\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [166][0/390]\tTime 0.004 (0.004)\tLoss 1.1219 (1.1219)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 1.0086 (1.0697)\tPrec@1 69.531 (63.004)\n",
      "Epoch: [166][156/390]\tTime 0.007 (0.003)\tLoss 1.2159 (1.0974)\tPrec@1 59.375 (61.709)\n",
      "Epoch: [166][234/390]\tTime 0.002 (0.003)\tLoss 1.1860 (1.1143)\tPrec@1 54.688 (61.077)\n",
      "Epoch: [166][312/390]\tTime 0.006 (0.003)\tLoss 1.2055 (1.1245)\tPrec@1 59.375 (60.640)\n",
      "Epoch: [166][390/390]\tTime 0.002 (0.004)\tLoss 1.1792 (1.1354)\tPrec@1 61.250 (60.286)\n",
      "EPOCH: 166 train Results: Prec@1 60.286 Loss: 1.1354\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1538 (1.1538)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3983 (1.2506)\tPrec@1 25.000 (55.350)\n",
      "EPOCH: 166 val Results: Prec@1 55.350 Loss: 1.2506\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [167][0/390]\tTime 0.008 (0.008)\tLoss 1.1284 (1.1284)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [167][78/390]\tTime 0.002 (0.005)\tLoss 1.2253 (1.0856)\tPrec@1 53.906 (62.460)\n",
      "Epoch: [167][156/390]\tTime 0.002 (0.004)\tLoss 1.0905 (1.1008)\tPrec@1 64.062 (61.629)\n",
      "Epoch: [167][234/390]\tTime 0.002 (0.004)\tLoss 1.1046 (1.1186)\tPrec@1 60.938 (60.974)\n",
      "Epoch: [167][312/390]\tTime 0.007 (0.003)\tLoss 1.2167 (1.1253)\tPrec@1 56.250 (60.635)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.003)\tLoss 1.2230 (1.1367)\tPrec@1 57.500 (60.332)\n",
      "EPOCH: 167 train Results: Prec@1 60.332 Loss: 1.1367\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0905 (1.0905)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2744 (1.2441)\tPrec@1 37.500 (55.170)\n",
      "EPOCH: 167 val Results: Prec@1 55.170 Loss: 1.2441\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [168][0/390]\tTime 0.004 (0.004)\tLoss 0.9574 (0.9574)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [168][78/390]\tTime 0.002 (0.003)\tLoss 0.9445 (1.0783)\tPrec@1 68.750 (62.638)\n",
      "Epoch: [168][156/390]\tTime 0.002 (0.003)\tLoss 1.0877 (1.1029)\tPrec@1 58.594 (61.525)\n",
      "Epoch: [168][234/390]\tTime 0.002 (0.003)\tLoss 1.1067 (1.1182)\tPrec@1 63.281 (60.934)\n",
      "Epoch: [168][312/390]\tTime 0.004 (0.003)\tLoss 1.1523 (1.1302)\tPrec@1 60.156 (60.518)\n",
      "Epoch: [168][390/390]\tTime 0.001 (0.003)\tLoss 1.1231 (1.1375)\tPrec@1 65.000 (60.316)\n",
      "EPOCH: 168 train Results: Prec@1 60.316 Loss: 1.1375\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1270 (1.1270)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6533 (1.2435)\tPrec@1 25.000 (55.530)\n",
      "EPOCH: 168 val Results: Prec@1 55.530 Loss: 1.2435\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [169][0/390]\tTime 0.003 (0.003)\tLoss 1.0360 (1.0360)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [169][78/390]\tTime 0.004 (0.003)\tLoss 1.0834 (1.0874)\tPrec@1 62.500 (61.877)\n",
      "Epoch: [169][156/390]\tTime 0.003 (0.003)\tLoss 0.9912 (1.0972)\tPrec@1 64.062 (61.739)\n",
      "Epoch: [169][234/390]\tTime 0.003 (0.003)\tLoss 1.0795 (1.1126)\tPrec@1 61.719 (61.230)\n",
      "Epoch: [169][312/390]\tTime 0.002 (0.003)\tLoss 1.1350 (1.1253)\tPrec@1 57.031 (60.728)\n",
      "Epoch: [169][390/390]\tTime 0.002 (0.003)\tLoss 1.1426 (1.1333)\tPrec@1 60.000 (60.522)\n",
      "EPOCH: 169 train Results: Prec@1 60.522 Loss: 1.1333\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1695 (1.1695)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4176 (1.2483)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 169 val Results: Prec@1 55.630 Loss: 1.2483\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [170][0/390]\tTime 0.006 (0.006)\tLoss 1.1244 (1.1244)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [170][78/390]\tTime 0.003 (0.003)\tLoss 1.1012 (1.0824)\tPrec@1 59.375 (61.986)\n",
      "Epoch: [170][156/390]\tTime 0.007 (0.003)\tLoss 0.9459 (1.0982)\tPrec@1 67.969 (61.530)\n",
      "Epoch: [170][234/390]\tTime 0.002 (0.003)\tLoss 1.1206 (1.1103)\tPrec@1 64.844 (61.074)\n",
      "Epoch: [170][312/390]\tTime 0.002 (0.003)\tLoss 1.0725 (1.1276)\tPrec@1 67.188 (60.453)\n",
      "Epoch: [170][390/390]\tTime 0.003 (0.003)\tLoss 1.0739 (1.1342)\tPrec@1 61.250 (60.282)\n",
      "EPOCH: 170 train Results: Prec@1 60.282 Loss: 1.1342\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1727 (1.1727)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3230 (1.2353)\tPrec@1 37.500 (56.260)\n",
      "EPOCH: 170 val Results: Prec@1 56.260 Loss: 1.2353\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [171][0/390]\tTime 0.006 (0.006)\tLoss 0.9696 (0.9696)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [171][78/390]\tTime 0.003 (0.003)\tLoss 1.0352 (1.0721)\tPrec@1 64.062 (63.182)\n",
      "Epoch: [171][156/390]\tTime 0.002 (0.003)\tLoss 1.2025 (1.1000)\tPrec@1 57.812 (61.943)\n",
      "Epoch: [171][234/390]\tTime 0.003 (0.003)\tLoss 1.0710 (1.1099)\tPrec@1 63.281 (61.523)\n",
      "Epoch: [171][312/390]\tTime 0.006 (0.003)\tLoss 1.0977 (1.1175)\tPrec@1 62.500 (61.274)\n",
      "Epoch: [171][390/390]\tTime 0.001 (0.003)\tLoss 1.3246 (1.1292)\tPrec@1 55.000 (60.682)\n",
      "EPOCH: 171 train Results: Prec@1 60.682 Loss: 1.1292\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1913 (1.1913)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4073 (1.2469)\tPrec@1 31.250 (55.880)\n",
      "EPOCH: 171 val Results: Prec@1 55.880 Loss: 1.2469\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [172][0/390]\tTime 0.012 (0.012)\tLoss 0.9326 (0.9326)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [172][78/390]\tTime 0.002 (0.003)\tLoss 1.0282 (1.0815)\tPrec@1 62.500 (62.332)\n",
      "Epoch: [172][156/390]\tTime 0.003 (0.003)\tLoss 1.0390 (1.0970)\tPrec@1 62.500 (61.848)\n",
      "Epoch: [172][234/390]\tTime 0.003 (0.003)\tLoss 1.1400 (1.1139)\tPrec@1 64.062 (61.190)\n",
      "Epoch: [172][312/390]\tTime 0.002 (0.003)\tLoss 1.2499 (1.1268)\tPrec@1 54.688 (60.421)\n",
      "Epoch: [172][390/390]\tTime 0.002 (0.003)\tLoss 1.1469 (1.1354)\tPrec@1 65.000 (60.226)\n",
      "EPOCH: 172 train Results: Prec@1 60.226 Loss: 1.1354\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2001 (1.2001)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6323 (1.2491)\tPrec@1 31.250 (55.530)\n",
      "EPOCH: 172 val Results: Prec@1 55.530 Loss: 1.2491\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [173][0/390]\tTime 0.004 (0.004)\tLoss 1.1283 (1.1283)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [173][78/390]\tTime 0.003 (0.003)\tLoss 1.2264 (1.1004)\tPrec@1 58.594 (61.778)\n",
      "Epoch: [173][156/390]\tTime 0.004 (0.003)\tLoss 1.1788 (1.1086)\tPrec@1 61.719 (61.171)\n",
      "Epoch: [173][234/390]\tTime 0.002 (0.003)\tLoss 1.0455 (1.1137)\tPrec@1 62.500 (61.213)\n",
      "Epoch: [173][312/390]\tTime 0.004 (0.003)\tLoss 1.1313 (1.1246)\tPrec@1 60.938 (60.725)\n",
      "Epoch: [173][390/390]\tTime 0.001 (0.003)\tLoss 1.0816 (1.1332)\tPrec@1 61.250 (60.466)\n",
      "EPOCH: 173 train Results: Prec@1 60.466 Loss: 1.1332\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1926 (1.1926)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5212 (1.2578)\tPrec@1 25.000 (55.030)\n",
      "EPOCH: 173 val Results: Prec@1 55.030 Loss: 1.2578\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [174][0/390]\tTime 0.002 (0.002)\tLoss 1.0521 (1.0521)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [174][78/390]\tTime 0.003 (0.003)\tLoss 0.9563 (1.0816)\tPrec@1 67.969 (62.134)\n",
      "Epoch: [174][156/390]\tTime 0.002 (0.003)\tLoss 1.0514 (1.1006)\tPrec@1 60.938 (61.445)\n",
      "Epoch: [174][234/390]\tTime 0.002 (0.003)\tLoss 1.0640 (1.1125)\tPrec@1 65.625 (61.124)\n",
      "Epoch: [174][312/390]\tTime 0.003 (0.003)\tLoss 1.1121 (1.1256)\tPrec@1 60.938 (60.660)\n",
      "Epoch: [174][390/390]\tTime 0.002 (0.003)\tLoss 1.1710 (1.1368)\tPrec@1 56.250 (60.130)\n",
      "EPOCH: 174 train Results: Prec@1 60.130 Loss: 1.1368\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1462 (1.1462)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4550 (1.2507)\tPrec@1 37.500 (55.850)\n",
      "EPOCH: 174 val Results: Prec@1 55.850 Loss: 1.2507\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [175][0/390]\tTime 0.002 (0.002)\tLoss 1.0375 (1.0375)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [175][78/390]\tTime 0.002 (0.002)\tLoss 1.1539 (1.0717)\tPrec@1 57.812 (63.400)\n",
      "Epoch: [175][156/390]\tTime 0.003 (0.002)\tLoss 1.0930 (1.0914)\tPrec@1 60.156 (62.316)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.003)\tLoss 1.1996 (1.1085)\tPrec@1 57.812 (61.695)\n",
      "Epoch: [175][312/390]\tTime 0.003 (0.003)\tLoss 1.3312 (1.1203)\tPrec@1 51.562 (61.045)\n",
      "Epoch: [175][390/390]\tTime 0.001 (0.003)\tLoss 1.2560 (1.1311)\tPrec@1 55.000 (60.554)\n",
      "EPOCH: 175 train Results: Prec@1 60.554 Loss: 1.1311\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1582 (1.1582)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4172 (1.2472)\tPrec@1 37.500 (55.560)\n",
      "EPOCH: 175 val Results: Prec@1 55.560 Loss: 1.2472\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [176][0/390]\tTime 0.007 (0.007)\tLoss 1.0630 (1.0630)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [176][78/390]\tTime 0.002 (0.003)\tLoss 1.0995 (1.0813)\tPrec@1 62.500 (62.876)\n",
      "Epoch: [176][156/390]\tTime 0.029 (0.003)\tLoss 1.1016 (1.0988)\tPrec@1 62.500 (61.694)\n",
      "Epoch: [176][234/390]\tTime 0.003 (0.006)\tLoss 1.1695 (1.1120)\tPrec@1 61.719 (61.270)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.006)\tLoss 0.9828 (1.1230)\tPrec@1 70.312 (60.808)\n",
      "Epoch: [176][390/390]\tTime 0.004 (0.005)\tLoss 1.0756 (1.1302)\tPrec@1 61.250 (60.450)\n",
      "EPOCH: 176 train Results: Prec@1 60.450 Loss: 1.1302\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1290 (1.1290)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3587 (1.2453)\tPrec@1 50.000 (55.840)\n",
      "EPOCH: 176 val Results: Prec@1 55.840 Loss: 1.2453\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [177][0/390]\tTime 0.004 (0.004)\tLoss 1.1288 (1.1288)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [177][78/390]\tTime 0.005 (0.003)\tLoss 0.9346 (1.0715)\tPrec@1 67.969 (63.212)\n",
      "Epoch: [177][156/390]\tTime 0.004 (0.003)\tLoss 1.1015 (1.1023)\tPrec@1 64.062 (61.973)\n",
      "Epoch: [177][234/390]\tTime 0.004 (0.003)\tLoss 1.0986 (1.1150)\tPrec@1 61.719 (61.410)\n",
      "Epoch: [177][312/390]\tTime 0.003 (0.003)\tLoss 1.1254 (1.1249)\tPrec@1 59.375 (60.940)\n",
      "Epoch: [177][390/390]\tTime 0.002 (0.003)\tLoss 1.0565 (1.1310)\tPrec@1 66.250 (60.604)\n",
      "EPOCH: 177 train Results: Prec@1 60.604 Loss: 1.1310\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1983 (1.1983)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6953 (1.2438)\tPrec@1 31.250 (56.050)\n",
      "EPOCH: 177 val Results: Prec@1 56.050 Loss: 1.2438\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [178][0/390]\tTime 0.002 (0.002)\tLoss 1.1724 (1.1724)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [178][78/390]\tTime 0.002 (0.003)\tLoss 1.1043 (1.0717)\tPrec@1 64.062 (62.767)\n",
      "Epoch: [178][156/390]\tTime 0.004 (0.003)\tLoss 1.1496 (1.0992)\tPrec@1 64.844 (61.868)\n",
      "Epoch: [178][234/390]\tTime 0.003 (0.003)\tLoss 1.2145 (1.1166)\tPrec@1 55.469 (60.891)\n",
      "Epoch: [178][312/390]\tTime 0.003 (0.003)\tLoss 1.1359 (1.1250)\tPrec@1 58.594 (60.566)\n",
      "Epoch: [178][390/390]\tTime 0.001 (0.003)\tLoss 1.1765 (1.1325)\tPrec@1 62.500 (60.258)\n",
      "EPOCH: 178 train Results: Prec@1 60.258 Loss: 1.1325\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1762 (1.1762)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3789 (1.2557)\tPrec@1 43.750 (55.240)\n",
      "EPOCH: 178 val Results: Prec@1 55.240 Loss: 1.2557\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [179][0/390]\tTime 0.005 (0.005)\tLoss 1.1918 (1.1918)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [179][78/390]\tTime 0.003 (0.003)\tLoss 1.2497 (1.0690)\tPrec@1 57.031 (62.836)\n",
      "Epoch: [179][156/390]\tTime 0.003 (0.003)\tLoss 1.2482 (1.0965)\tPrec@1 57.031 (61.754)\n",
      "Epoch: [179][234/390]\tTime 0.002 (0.003)\tLoss 1.2442 (1.1156)\tPrec@1 56.250 (61.024)\n",
      "Epoch: [179][312/390]\tTime 0.010 (0.003)\tLoss 0.9870 (1.1253)\tPrec@1 64.844 (60.511)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.3478 (1.1342)\tPrec@1 51.250 (60.244)\n",
      "EPOCH: 179 train Results: Prec@1 60.244 Loss: 1.1342\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1277 (1.1277)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3584 (1.2446)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 179 val Results: Prec@1 55.150 Loss: 1.2446\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [180][0/390]\tTime 0.005 (0.005)\tLoss 1.1772 (1.1772)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [180][78/390]\tTime 0.003 (0.002)\tLoss 1.1715 (1.0860)\tPrec@1 62.500 (62.164)\n",
      "Epoch: [180][156/390]\tTime 0.003 (0.003)\tLoss 1.0672 (1.1002)\tPrec@1 64.062 (61.589)\n",
      "Epoch: [180][234/390]\tTime 0.004 (0.003)\tLoss 1.0585 (1.1130)\tPrec@1 57.812 (61.127)\n",
      "Epoch: [180][312/390]\tTime 0.002 (0.003)\tLoss 1.0595 (1.1264)\tPrec@1 66.406 (60.621)\n",
      "Epoch: [180][390/390]\tTime 0.002 (0.003)\tLoss 1.2681 (1.1323)\tPrec@1 56.250 (60.380)\n",
      "EPOCH: 180 train Results: Prec@1 60.380 Loss: 1.1323\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1589 (1.1589)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4476 (1.2465)\tPrec@1 37.500 (55.370)\n",
      "EPOCH: 180 val Results: Prec@1 55.370 Loss: 1.2465\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [181][0/390]\tTime 0.003 (0.003)\tLoss 1.0664 (1.0664)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [181][78/390]\tTime 0.002 (0.003)\tLoss 1.0805 (1.0627)\tPrec@1 60.938 (63.420)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 0.9881 (1.0857)\tPrec@1 70.312 (62.201)\n",
      "Epoch: [181][234/390]\tTime 0.003 (0.003)\tLoss 1.0674 (1.1052)\tPrec@1 63.281 (61.406)\n",
      "Epoch: [181][312/390]\tTime 0.002 (0.003)\tLoss 1.2082 (1.1189)\tPrec@1 58.594 (60.813)\n",
      "Epoch: [181][390/390]\tTime 0.002 (0.003)\tLoss 1.3053 (1.1274)\tPrec@1 56.250 (60.494)\n",
      "EPOCH: 181 train Results: Prec@1 60.494 Loss: 1.1274\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0991 (1.0991)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5002 (1.2391)\tPrec@1 43.750 (56.210)\n",
      "EPOCH: 181 val Results: Prec@1 56.210 Loss: 1.2391\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [182][0/390]\tTime 0.006 (0.006)\tLoss 1.1188 (1.1188)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [182][78/390]\tTime 0.003 (0.003)\tLoss 1.2336 (1.0785)\tPrec@1 51.562 (62.154)\n",
      "Epoch: [182][156/390]\tTime 0.005 (0.003)\tLoss 0.9942 (1.1012)\tPrec@1 67.969 (61.365)\n",
      "Epoch: [182][234/390]\tTime 0.004 (0.003)\tLoss 1.2998 (1.1192)\tPrec@1 53.906 (60.708)\n",
      "Epoch: [182][312/390]\tTime 0.002 (0.003)\tLoss 1.1422 (1.1254)\tPrec@1 63.281 (60.521)\n",
      "Epoch: [182][390/390]\tTime 0.003 (0.003)\tLoss 1.2164 (1.1314)\tPrec@1 58.750 (60.190)\n",
      "EPOCH: 182 train Results: Prec@1 60.190 Loss: 1.1314\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1541 (1.1541)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5318 (1.2414)\tPrec@1 31.250 (55.630)\n",
      "EPOCH: 182 val Results: Prec@1 55.630 Loss: 1.2414\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [183][0/390]\tTime 0.004 (0.004)\tLoss 1.0237 (1.0237)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [183][78/390]\tTime 0.002 (0.003)\tLoss 1.0984 (1.0780)\tPrec@1 60.938 (62.233)\n",
      "Epoch: [183][156/390]\tTime 0.005 (0.003)\tLoss 1.1071 (1.0937)\tPrec@1 66.406 (61.679)\n",
      "Epoch: [183][234/390]\tTime 0.002 (0.003)\tLoss 1.1405 (1.1118)\tPrec@1 56.250 (61.167)\n",
      "Epoch: [183][312/390]\tTime 0.003 (0.003)\tLoss 1.0787 (1.1215)\tPrec@1 60.156 (60.478)\n",
      "Epoch: [183][390/390]\tTime 0.001 (0.003)\tLoss 1.1983 (1.1324)\tPrec@1 53.750 (60.158)\n",
      "EPOCH: 183 train Results: Prec@1 60.158 Loss: 1.1324\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1321 (1.1321)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6793 (1.2524)\tPrec@1 31.250 (54.630)\n",
      "EPOCH: 183 val Results: Prec@1 54.630 Loss: 1.2524\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [184][0/390]\tTime 0.004 (0.004)\tLoss 0.9947 (0.9947)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [184][78/390]\tTime 0.002 (0.003)\tLoss 1.1263 (1.0751)\tPrec@1 59.375 (62.530)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.003)\tLoss 1.2000 (1.0969)\tPrec@1 57.812 (61.331)\n",
      "Epoch: [184][234/390]\tTime 0.002 (0.003)\tLoss 1.2483 (1.1154)\tPrec@1 54.688 (60.974)\n",
      "Epoch: [184][312/390]\tTime 0.003 (0.003)\tLoss 1.1320 (1.1258)\tPrec@1 57.031 (60.581)\n",
      "Epoch: [184][390/390]\tTime 0.002 (0.003)\tLoss 1.3121 (1.1310)\tPrec@1 51.250 (60.246)\n",
      "EPOCH: 184 train Results: Prec@1 60.246 Loss: 1.1310\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0847 (1.0847)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3908 (1.2449)\tPrec@1 50.000 (55.520)\n",
      "EPOCH: 184 val Results: Prec@1 55.520 Loss: 1.2449\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [185][0/390]\tTime 0.006 (0.006)\tLoss 1.0660 (1.0660)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [185][78/390]\tTime 0.003 (0.003)\tLoss 1.2359 (1.0844)\tPrec@1 60.156 (62.184)\n",
      "Epoch: [185][156/390]\tTime 0.003 (0.003)\tLoss 1.0078 (1.0958)\tPrec@1 71.875 (61.818)\n",
      "Epoch: [185][234/390]\tTime 0.005 (0.003)\tLoss 1.2397 (1.1033)\tPrec@1 60.156 (61.759)\n",
      "Epoch: [185][312/390]\tTime 0.002 (0.003)\tLoss 1.0332 (1.1132)\tPrec@1 58.594 (61.200)\n",
      "Epoch: [185][390/390]\tTime 0.001 (0.003)\tLoss 1.2868 (1.1292)\tPrec@1 51.250 (60.530)\n",
      "EPOCH: 185 train Results: Prec@1 60.530 Loss: 1.1292\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1921 (1.1921)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2369 (1.2610)\tPrec@1 56.250 (55.190)\n",
      "EPOCH: 185 val Results: Prec@1 55.190 Loss: 1.2610\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [186][0/390]\tTime 0.005 (0.005)\tLoss 1.0427 (1.0427)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [186][78/390]\tTime 0.002 (0.003)\tLoss 1.0845 (1.0732)\tPrec@1 64.844 (63.390)\n",
      "Epoch: [186][156/390]\tTime 0.003 (0.003)\tLoss 1.1136 (1.0991)\tPrec@1 59.375 (61.803)\n",
      "Epoch: [186][234/390]\tTime 0.004 (0.003)\tLoss 1.1514 (1.1146)\tPrec@1 56.250 (61.090)\n",
      "Epoch: [186][312/390]\tTime 0.008 (0.003)\tLoss 1.0419 (1.1257)\tPrec@1 67.969 (60.538)\n",
      "Epoch: [186][390/390]\tTime 0.001 (0.003)\tLoss 0.9135 (1.1298)\tPrec@1 68.750 (60.348)\n",
      "EPOCH: 186 train Results: Prec@1 60.348 Loss: 1.1298\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0996 (1.0996)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4357 (1.2482)\tPrec@1 43.750 (55.730)\n",
      "EPOCH: 186 val Results: Prec@1 55.730 Loss: 1.2482\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [187][0/390]\tTime 0.008 (0.008)\tLoss 1.1523 (1.1523)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [187][78/390]\tTime 0.002 (0.002)\tLoss 1.0036 (1.0649)\tPrec@1 62.500 (63.123)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.2868 (1.0927)\tPrec@1 52.344 (62.231)\n",
      "Epoch: [187][234/390]\tTime 0.005 (0.003)\tLoss 1.1420 (1.1054)\tPrec@1 60.156 (61.639)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.2406 (1.1215)\tPrec@1 57.812 (60.920)\n",
      "Epoch: [187][390/390]\tTime 0.002 (0.003)\tLoss 1.2681 (1.1311)\tPrec@1 55.000 (60.528)\n",
      "EPOCH: 187 train Results: Prec@1 60.528 Loss: 1.1311\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1705 (1.1705)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4499 (1.2432)\tPrec@1 43.750 (55.420)\n",
      "EPOCH: 187 val Results: Prec@1 55.420 Loss: 1.2432\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [188][0/390]\tTime 0.003 (0.003)\tLoss 1.0278 (1.0278)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [188][78/390]\tTime 0.005 (0.003)\tLoss 1.3176 (1.0688)\tPrec@1 53.906 (63.153)\n",
      "Epoch: [188][156/390]\tTime 0.002 (0.003)\tLoss 1.1646 (1.0919)\tPrec@1 56.250 (62.102)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.003)\tLoss 1.2336 (1.1045)\tPrec@1 51.562 (61.543)\n",
      "Epoch: [188][312/390]\tTime 0.025 (0.003)\tLoss 1.2254 (1.1138)\tPrec@1 58.594 (61.180)\n",
      "Epoch: [188][390/390]\tTime 0.001 (0.003)\tLoss 1.2317 (1.1270)\tPrec@1 53.750 (60.624)\n",
      "EPOCH: 188 train Results: Prec@1 60.624 Loss: 1.1270\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1674 (1.1674)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3605 (1.2483)\tPrec@1 37.500 (55.280)\n",
      "EPOCH: 188 val Results: Prec@1 55.280 Loss: 1.2483\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [189][0/390]\tTime 0.003 (0.003)\tLoss 1.1810 (1.1810)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.003)\tLoss 0.8955 (1.0694)\tPrec@1 71.875 (63.232)\n",
      "Epoch: [189][156/390]\tTime 0.002 (0.003)\tLoss 1.2525 (1.0944)\tPrec@1 56.250 (62.007)\n",
      "Epoch: [189][234/390]\tTime 0.003 (0.003)\tLoss 1.1649 (1.1067)\tPrec@1 56.250 (61.513)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.003)\tLoss 1.1538 (1.1184)\tPrec@1 59.375 (60.987)\n",
      "Epoch: [189][390/390]\tTime 0.005 (0.003)\tLoss 1.3224 (1.1310)\tPrec@1 50.000 (60.508)\n",
      "EPOCH: 189 train Results: Prec@1 60.508 Loss: 1.1310\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1313 (1.1313)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3793 (1.2403)\tPrec@1 50.000 (55.830)\n",
      "EPOCH: 189 val Results: Prec@1 55.830 Loss: 1.2403\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [190][0/390]\tTime 0.002 (0.002)\tLoss 0.9621 (0.9621)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.003)\tLoss 1.0588 (1.0766)\tPrec@1 60.156 (62.757)\n",
      "Epoch: [190][156/390]\tTime 0.003 (0.003)\tLoss 1.1904 (1.0911)\tPrec@1 57.812 (62.052)\n",
      "Epoch: [190][234/390]\tTime 0.002 (0.003)\tLoss 1.1332 (1.1089)\tPrec@1 57.812 (61.170)\n",
      "Epoch: [190][312/390]\tTime 0.006 (0.003)\tLoss 1.0893 (1.1196)\tPrec@1 64.062 (60.910)\n",
      "Epoch: [190][390/390]\tTime 0.003 (0.003)\tLoss 1.2237 (1.1311)\tPrec@1 55.000 (60.450)\n",
      "EPOCH: 190 train Results: Prec@1 60.450 Loss: 1.1311\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1722 (1.1722)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5370 (1.2469)\tPrec@1 37.500 (55.690)\n",
      "EPOCH: 190 val Results: Prec@1 55.690 Loss: 1.2469\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [191][0/390]\tTime 0.004 (0.004)\tLoss 1.1476 (1.1476)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.003)\tLoss 1.0057 (1.0801)\tPrec@1 69.531 (62.648)\n",
      "Epoch: [191][156/390]\tTime 0.004 (0.003)\tLoss 1.1269 (1.1005)\tPrec@1 60.938 (61.654)\n",
      "Epoch: [191][234/390]\tTime 0.004 (0.003)\tLoss 1.0614 (1.1092)\tPrec@1 62.500 (61.253)\n",
      "Epoch: [191][312/390]\tTime 0.003 (0.003)\tLoss 1.0174 (1.1186)\tPrec@1 64.062 (60.977)\n",
      "Epoch: [191][390/390]\tTime 0.003 (0.003)\tLoss 1.2866 (1.1302)\tPrec@1 48.750 (60.512)\n",
      "EPOCH: 191 train Results: Prec@1 60.512 Loss: 1.1302\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1177 (1.1177)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5175 (1.2381)\tPrec@1 37.500 (56.090)\n",
      "EPOCH: 191 val Results: Prec@1 56.090 Loss: 1.2381\n",
      "Best Prec@1: 56.340\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [192][0/390]\tTime 0.005 (0.005)\tLoss 0.9207 (0.9207)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [192][78/390]\tTime 0.020 (0.003)\tLoss 1.1284 (1.0870)\tPrec@1 53.906 (62.441)\n",
      "Epoch: [192][156/390]\tTime 0.002 (0.004)\tLoss 1.1017 (1.0954)\tPrec@1 64.062 (61.734)\n",
      "Epoch: [192][234/390]\tTime 0.003 (0.004)\tLoss 1.0379 (1.1076)\tPrec@1 66.406 (61.200)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.004)\tLoss 1.2840 (1.1136)\tPrec@1 55.469 (60.875)\n",
      "Epoch: [192][390/390]\tTime 0.001 (0.004)\tLoss 1.1093 (1.1232)\tPrec@1 65.000 (60.608)\n",
      "EPOCH: 192 train Results: Prec@1 60.608 Loss: 1.1232\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0941 (1.0941)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4403 (1.2374)\tPrec@1 43.750 (56.490)\n",
      "EPOCH: 192 val Results: Prec@1 56.490 Loss: 1.2374\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [193][0/390]\tTime 0.004 (0.004)\tLoss 0.9597 (0.9597)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [193][78/390]\tTime 0.002 (0.003)\tLoss 0.9996 (1.0707)\tPrec@1 61.719 (62.856)\n",
      "Epoch: [193][156/390]\tTime 0.002 (0.004)\tLoss 1.1503 (1.0984)\tPrec@1 58.594 (61.679)\n",
      "Epoch: [193][234/390]\tTime 0.003 (0.003)\tLoss 1.1683 (1.1140)\tPrec@1 58.594 (61.160)\n",
      "Epoch: [193][312/390]\tTime 0.002 (0.003)\tLoss 1.2386 (1.1258)\tPrec@1 58.594 (60.601)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.003)\tLoss 1.1825 (1.1302)\tPrec@1 56.250 (60.388)\n",
      "EPOCH: 193 train Results: Prec@1 60.388 Loss: 1.1302\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1069 (1.1069)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6484 (1.2465)\tPrec@1 37.500 (55.480)\n",
      "EPOCH: 193 val Results: Prec@1 55.480 Loss: 1.2465\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [194][0/390]\tTime 0.003 (0.003)\tLoss 1.0798 (1.0798)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [194][78/390]\tTime 0.002 (0.003)\tLoss 1.0052 (1.0824)\tPrec@1 64.062 (62.164)\n",
      "Epoch: [194][156/390]\tTime 0.004 (0.003)\tLoss 1.0182 (1.0980)\tPrec@1 58.594 (61.754)\n",
      "Epoch: [194][234/390]\tTime 0.004 (0.003)\tLoss 1.1334 (1.1080)\tPrec@1 60.938 (61.077)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.003)\tLoss 1.1695 (1.1182)\tPrec@1 58.594 (60.738)\n",
      "Epoch: [194][390/390]\tTime 0.001 (0.003)\tLoss 1.2137 (1.1258)\tPrec@1 56.250 (60.462)\n",
      "EPOCH: 194 train Results: Prec@1 60.462 Loss: 1.1258\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1686 (1.1686)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3290 (1.2369)\tPrec@1 50.000 (55.620)\n",
      "EPOCH: 194 val Results: Prec@1 55.620 Loss: 1.2369\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [195][0/390]\tTime 0.005 (0.005)\tLoss 0.9902 (0.9902)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [195][78/390]\tTime 0.004 (0.003)\tLoss 1.0855 (1.0672)\tPrec@1 63.281 (62.757)\n",
      "Epoch: [195][156/390]\tTime 0.003 (0.003)\tLoss 1.0080 (1.0895)\tPrec@1 61.719 (61.624)\n",
      "Epoch: [195][234/390]\tTime 0.002 (0.003)\tLoss 1.1615 (1.1016)\tPrec@1 57.812 (61.310)\n",
      "Epoch: [195][312/390]\tTime 0.003 (0.003)\tLoss 1.3348 (1.1137)\tPrec@1 53.125 (60.775)\n",
      "Epoch: [195][390/390]\tTime 0.004 (0.003)\tLoss 1.1572 (1.1223)\tPrec@1 56.250 (60.536)\n",
      "EPOCH: 195 train Results: Prec@1 60.536 Loss: 1.1223\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1311 (1.1311)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4009 (1.2461)\tPrec@1 50.000 (56.060)\n",
      "EPOCH: 195 val Results: Prec@1 56.060 Loss: 1.2461\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [196][0/390]\tTime 0.003 (0.003)\tLoss 0.9402 (0.9402)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 1.1328 (1.0643)\tPrec@1 60.156 (62.807)\n",
      "Epoch: [196][156/390]\tTime 0.004 (0.003)\tLoss 1.2759 (1.0879)\tPrec@1 51.562 (61.943)\n",
      "Epoch: [196][234/390]\tTime 0.004 (0.003)\tLoss 1.1118 (1.1124)\tPrec@1 64.062 (61.124)\n",
      "Epoch: [196][312/390]\tTime 0.004 (0.003)\tLoss 1.1395 (1.1191)\tPrec@1 57.031 (60.860)\n",
      "Epoch: [196][390/390]\tTime 0.003 (0.003)\tLoss 1.1492 (1.1259)\tPrec@1 60.000 (60.546)\n",
      "EPOCH: 196 train Results: Prec@1 60.546 Loss: 1.1259\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0952 (1.0952)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4786 (1.2369)\tPrec@1 31.250 (55.600)\n",
      "EPOCH: 196 val Results: Prec@1 55.600 Loss: 1.2369\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [197][0/390]\tTime 0.002 (0.002)\tLoss 0.9081 (0.9081)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [197][78/390]\tTime 0.004 (0.003)\tLoss 1.1061 (1.0533)\tPrec@1 57.031 (63.627)\n",
      "Epoch: [197][156/390]\tTime 0.003 (0.003)\tLoss 1.1415 (1.0793)\tPrec@1 61.719 (62.555)\n",
      "Epoch: [197][234/390]\tTime 0.002 (0.003)\tLoss 1.0515 (1.1017)\tPrec@1 63.281 (61.669)\n",
      "Epoch: [197][312/390]\tTime 0.002 (0.003)\tLoss 1.0814 (1.1198)\tPrec@1 60.938 (60.905)\n",
      "Epoch: [197][390/390]\tTime 0.001 (0.003)\tLoss 0.9241 (1.1290)\tPrec@1 68.750 (60.486)\n",
      "EPOCH: 197 train Results: Prec@1 60.486 Loss: 1.1290\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1425 (1.1425)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2388 (1.2369)\tPrec@1 37.500 (55.640)\n",
      "EPOCH: 197 val Results: Prec@1 55.640 Loss: 1.2369\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [198][0/390]\tTime 0.007 (0.007)\tLoss 1.0496 (1.0496)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [198][78/390]\tTime 0.004 (0.003)\tLoss 1.1616 (1.0772)\tPrec@1 57.812 (62.490)\n",
      "Epoch: [198][156/390]\tTime 0.004 (0.003)\tLoss 1.0482 (1.0962)\tPrec@1 67.188 (61.669)\n",
      "Epoch: [198][234/390]\tTime 0.004 (0.003)\tLoss 1.3560 (1.1122)\tPrec@1 52.344 (61.074)\n",
      "Epoch: [198][312/390]\tTime 0.003 (0.003)\tLoss 0.9964 (1.1185)\tPrec@1 66.406 (60.833)\n",
      "Epoch: [198][390/390]\tTime 0.001 (0.003)\tLoss 1.0974 (1.1265)\tPrec@1 55.000 (60.502)\n",
      "EPOCH: 198 train Results: Prec@1 60.502 Loss: 1.1265\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1333 (1.1333)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4060 (1.2496)\tPrec@1 25.000 (55.240)\n",
      "EPOCH: 198 val Results: Prec@1 55.240 Loss: 1.2496\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [199][0/390]\tTime 0.005 (0.005)\tLoss 1.1477 (1.1477)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [199][78/390]\tTime 0.002 (0.003)\tLoss 1.0367 (1.0634)\tPrec@1 63.281 (63.182)\n",
      "Epoch: [199][156/390]\tTime 0.004 (0.003)\tLoss 1.1449 (1.0796)\tPrec@1 58.594 (62.570)\n",
      "Epoch: [199][234/390]\tTime 0.003 (0.003)\tLoss 1.1860 (1.0991)\tPrec@1 57.812 (61.582)\n",
      "Epoch: [199][312/390]\tTime 0.002 (0.003)\tLoss 1.1893 (1.1145)\tPrec@1 57.031 (61.077)\n",
      "Epoch: [199][390/390]\tTime 0.001 (0.003)\tLoss 1.1566 (1.1234)\tPrec@1 62.500 (60.780)\n",
      "EPOCH: 199 train Results: Prec@1 60.780 Loss: 1.1234\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1041 (1.1041)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2478 (1.2448)\tPrec@1 43.750 (55.380)\n",
      "EPOCH: 199 val Results: Prec@1 55.380 Loss: 1.2448\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "current lr 1.00000e-03\n",
      "Epoch: [200][0/390]\tTime 0.003 (0.003)\tLoss 1.0072 (1.0072)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [200][78/390]\tTime 0.002 (0.003)\tLoss 1.1470 (1.0868)\tPrec@1 61.719 (62.184)\n",
      "Epoch: [200][156/390]\tTime 0.007 (0.003)\tLoss 0.8810 (1.1025)\tPrec@1 74.219 (61.689)\n",
      "Epoch: [200][234/390]\tTime 0.003 (0.003)\tLoss 1.0709 (1.1123)\tPrec@1 61.719 (61.193)\n",
      "Epoch: [200][312/390]\tTime 0.002 (0.003)\tLoss 1.1677 (1.1164)\tPrec@1 60.938 (61.040)\n",
      "Epoch: [200][390/390]\tTime 0.002 (0.003)\tLoss 1.4375 (1.1248)\tPrec@1 48.750 (60.520)\n",
      "EPOCH: 200 train Results: Prec@1 60.520 Loss: 1.1248\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1603 (1.1603)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1702 (1.2372)\tPrec@1 50.000 (55.740)\n",
      "EPOCH: 200 val Results: Prec@1 55.740 Loss: 1.2372\n",
      "Best Prec@1: 56.490\n",
      "\n",
      "End time:  Thu Apr  4 23:04:39 2024\n",
      "train executed in 263.9041 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.001, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer3 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:04:39 2024\n",
      "current lr 5.00000e-03\n",
      "Epoch: [1][0/390]\tTime 0.005 (0.005)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.002 (0.004)\tLoss 2.8495 (3.7941)\tPrec@1 25.000 (18.839)\n",
      "Epoch: [1][156/390]\tTime 0.002 (0.004)\tLoss 2.8395 (3.1958)\tPrec@1 22.656 (23.238)\n",
      "Epoch: [1][234/390]\tTime 0.002 (0.004)\tLoss 2.1464 (2.8916)\tPrec@1 32.031 (25.967)\n",
      "Epoch: [1][312/390]\tTime 0.003 (0.004)\tLoss 2.1551 (2.6953)\tPrec@1 27.344 (27.611)\n",
      "Epoch: [1][390/390]\tTime 0.009 (0.004)\tLoss 1.8144 (2.5549)\tPrec@1 42.500 (28.946)\n",
      "EPOCH: 1 train Results: Prec@1 28.946 Loss: 2.5549\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.6903 (1.6903)\tPrec@1 39.062 (39.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6257 (1.7333)\tPrec@1 25.000 (39.680)\n",
      "EPOCH: 1 val Results: Prec@1 39.680 Loss: 1.7333\n",
      "Best Prec@1: 39.680\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [2][0/390]\tTime 0.002 (0.002)\tLoss 1.7873 (1.7873)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [2][78/390]\tTime 0.005 (0.004)\tLoss 1.6409 (1.7902)\tPrec@1 40.625 (38.687)\n",
      "Epoch: [2][156/390]\tTime 0.002 (0.003)\tLoss 1.7499 (1.7657)\tPrec@1 39.062 (39.237)\n",
      "Epoch: [2][234/390]\tTime 0.004 (0.003)\tLoss 1.9392 (1.7445)\tPrec@1 37.500 (39.681)\n",
      "Epoch: [2][312/390]\tTime 0.002 (0.003)\tLoss 1.8323 (1.7255)\tPrec@1 33.594 (40.151)\n",
      "Epoch: [2][390/390]\tTime 0.001 (0.003)\tLoss 1.5227 (1.7095)\tPrec@1 46.250 (40.494)\n",
      "EPOCH: 2 train Results: Prec@1 40.494 Loss: 1.7095\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4828 (1.4828)\tPrec@1 52.344 (52.344)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4950 (1.5752)\tPrec@1 25.000 (44.020)\n",
      "EPOCH: 2 val Results: Prec@1 44.020 Loss: 1.5752\n",
      "Best Prec@1: 44.020\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [3][0/390]\tTime 0.004 (0.004)\tLoss 1.5607 (1.5607)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [3][78/390]\tTime 0.002 (0.003)\tLoss 1.5141 (1.5747)\tPrec@1 43.750 (44.254)\n",
      "Epoch: [3][156/390]\tTime 0.002 (0.003)\tLoss 1.4676 (1.5616)\tPrec@1 49.219 (44.850)\n",
      "Epoch: [3][234/390]\tTime 0.002 (0.003)\tLoss 1.5768 (1.5578)\tPrec@1 37.500 (44.943)\n",
      "Epoch: [3][312/390]\tTime 0.003 (0.003)\tLoss 1.5774 (1.5544)\tPrec@1 43.750 (45.073)\n",
      "Epoch: [3][390/390]\tTime 0.001 (0.003)\tLoss 1.4644 (1.5493)\tPrec@1 51.250 (45.236)\n",
      "EPOCH: 3 train Results: Prec@1 45.236 Loss: 1.5493\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.4153 (1.4153)\tPrec@1 51.562 (51.562)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4978 (1.5016)\tPrec@1 25.000 (46.680)\n",
      "EPOCH: 3 val Results: Prec@1 46.680 Loss: 1.5016\n",
      "Best Prec@1: 46.680\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [4][0/390]\tTime 0.003 (0.003)\tLoss 1.5971 (1.5971)\tPrec@1 42.188 (42.188)\n",
      "Epoch: [4][78/390]\tTime 0.002 (0.004)\tLoss 1.5943 (1.4767)\tPrec@1 48.438 (47.894)\n",
      "Epoch: [4][156/390]\tTime 0.002 (0.003)\tLoss 1.5025 (1.4735)\tPrec@1 48.438 (47.935)\n",
      "Epoch: [4][234/390]\tTime 0.002 (0.003)\tLoss 1.5751 (1.4692)\tPrec@1 41.406 (48.019)\n",
      "Epoch: [4][312/390]\tTime 0.003 (0.003)\tLoss 1.3837 (1.4686)\tPrec@1 51.562 (48.121)\n",
      "Epoch: [4][390/390]\tTime 0.003 (0.003)\tLoss 1.3926 (1.4660)\tPrec@1 50.000 (48.208)\n",
      "EPOCH: 4 train Results: Prec@1 48.208 Loss: 1.4660\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3594 (1.3594)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4509 (1.4504)\tPrec@1 31.250 (48.630)\n",
      "EPOCH: 4 val Results: Prec@1 48.630 Loss: 1.4504\n",
      "Best Prec@1: 48.630\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [5][0/390]\tTime 0.003 (0.003)\tLoss 1.4619 (1.4619)\tPrec@1 47.656 (47.656)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.003)\tLoss 1.4717 (1.3846)\tPrec@1 47.656 (51.543)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.003)\tLoss 1.4495 (1.4065)\tPrec@1 47.656 (50.189)\n",
      "Epoch: [5][234/390]\tTime 0.003 (0.003)\tLoss 1.4093 (1.4037)\tPrec@1 55.469 (50.422)\n",
      "Epoch: [5][312/390]\tTime 0.002 (0.003)\tLoss 1.3782 (1.4040)\tPrec@1 53.125 (50.344)\n",
      "Epoch: [5][390/390]\tTime 0.002 (0.003)\tLoss 1.3891 (1.4050)\tPrec@1 51.250 (50.348)\n",
      "EPOCH: 5 train Results: Prec@1 50.348 Loss: 1.4050\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3071 (1.3071)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3425 (1.4056)\tPrec@1 43.750 (49.900)\n",
      "EPOCH: 5 val Results: Prec@1 49.900 Loss: 1.4056\n",
      "Best Prec@1: 49.900\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [6][0/390]\tTime 0.003 (0.003)\tLoss 1.3444 (1.3444)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [6][78/390]\tTime 0.003 (0.003)\tLoss 1.4717 (1.3373)\tPrec@1 39.062 (53.165)\n",
      "Epoch: [6][156/390]\tTime 0.003 (0.003)\tLoss 1.2266 (1.3434)\tPrec@1 50.000 (52.707)\n",
      "Epoch: [6][234/390]\tTime 0.010 (0.003)\tLoss 1.4006 (1.3498)\tPrec@1 52.344 (52.513)\n",
      "Epoch: [6][312/390]\tTime 0.003 (0.003)\tLoss 1.4502 (1.3558)\tPrec@1 44.531 (52.254)\n",
      "Epoch: [6][390/390]\tTime 0.001 (0.003)\tLoss 1.5659 (1.3576)\tPrec@1 43.750 (52.150)\n",
      "EPOCH: 6 train Results: Prec@1 52.150 Loss: 1.3576\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2831 (1.2831)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4295 (1.3733)\tPrec@1 37.500 (50.660)\n",
      "EPOCH: 6 val Results: Prec@1 50.660 Loss: 1.3733\n",
      "Best Prec@1: 50.660\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [7][0/390]\tTime 0.003 (0.003)\tLoss 1.3937 (1.3937)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [7][78/390]\tTime 0.003 (0.003)\tLoss 1.2876 (1.3021)\tPrec@1 54.688 (54.430)\n",
      "Epoch: [7][156/390]\tTime 0.005 (0.003)\tLoss 1.4036 (1.3105)\tPrec@1 46.094 (54.180)\n",
      "Epoch: [7][234/390]\tTime 0.002 (0.003)\tLoss 1.3640 (1.3149)\tPrec@1 46.094 (53.710)\n",
      "Epoch: [7][312/390]\tTime 0.003 (0.003)\tLoss 1.3659 (1.3164)\tPrec@1 51.562 (53.532)\n",
      "Epoch: [7][390/390]\tTime 0.003 (0.003)\tLoss 1.4069 (1.3222)\tPrec@1 48.750 (53.310)\n",
      "EPOCH: 7 train Results: Prec@1 53.310 Loss: 1.3222\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2567 (1.2567)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5106 (1.3442)\tPrec@1 25.000 (51.780)\n",
      "EPOCH: 7 val Results: Prec@1 51.780 Loss: 1.3442\n",
      "Best Prec@1: 51.780\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [8][0/390]\tTime 0.006 (0.006)\tLoss 1.2521 (1.2521)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [8][78/390]\tTime 0.002 (0.003)\tLoss 1.2677 (1.2753)\tPrec@1 56.250 (55.301)\n",
      "Epoch: [8][156/390]\tTime 0.004 (0.003)\tLoss 1.3657 (1.2896)\tPrec@1 48.438 (54.628)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.003)\tLoss 1.2914 (1.2893)\tPrec@1 57.812 (54.611)\n",
      "Epoch: [8][312/390]\tTime 0.002 (0.003)\tLoss 1.1786 (1.2906)\tPrec@1 58.594 (54.490)\n",
      "Epoch: [8][390/390]\tTime 0.002 (0.003)\tLoss 1.4216 (1.2954)\tPrec@1 46.250 (54.262)\n",
      "EPOCH: 8 train Results: Prec@1 54.262 Loss: 1.2954\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.2019 (1.2019)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.003)\tLoss 1.4144 (1.3269)\tPrec@1 31.250 (52.560)\n",
      "EPOCH: 8 val Results: Prec@1 52.560 Loss: 1.3269\n",
      "Best Prec@1: 52.560\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [9][0/390]\tTime 0.008 (0.008)\tLoss 1.2597 (1.2597)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [9][78/390]\tTime 0.002 (0.003)\tLoss 1.1861 (1.2413)\tPrec@1 62.500 (56.309)\n",
      "Epoch: [9][156/390]\tTime 0.004 (0.003)\tLoss 1.4311 (1.2519)\tPrec@1 49.219 (56.006)\n",
      "Epoch: [9][234/390]\tTime 0.005 (0.003)\tLoss 1.3548 (1.2669)\tPrec@1 42.188 (55.163)\n",
      "Epoch: [9][312/390]\tTime 0.003 (0.003)\tLoss 1.3723 (1.2733)\tPrec@1 54.688 (55.014)\n",
      "Epoch: [9][390/390]\tTime 0.002 (0.003)\tLoss 1.4765 (1.2808)\tPrec@1 48.750 (54.644)\n",
      "EPOCH: 9 train Results: Prec@1 54.644 Loss: 1.2808\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1798 (1.1798)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3817 (1.3140)\tPrec@1 37.500 (53.380)\n",
      "EPOCH: 9 val Results: Prec@1 53.380 Loss: 1.3140\n",
      "Best Prec@1: 53.380\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [10][0/390]\tTime 0.002 (0.002)\tLoss 1.2381 (1.2381)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [10][78/390]\tTime 0.004 (0.003)\tLoss 1.0791 (1.2236)\tPrec@1 62.500 (56.616)\n",
      "Epoch: [10][156/390]\tTime 0.007 (0.003)\tLoss 1.2796 (1.2442)\tPrec@1 57.812 (55.941)\n",
      "Epoch: [10][234/390]\tTime 0.002 (0.003)\tLoss 1.2975 (1.2551)\tPrec@1 56.250 (55.303)\n",
      "Epoch: [10][312/390]\tTime 0.005 (0.003)\tLoss 1.3320 (1.2607)\tPrec@1 51.562 (55.142)\n",
      "Epoch: [10][390/390]\tTime 0.001 (0.003)\tLoss 1.1078 (1.2659)\tPrec@1 63.750 (54.998)\n",
      "EPOCH: 10 train Results: Prec@1 54.998 Loss: 1.2659\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2036 (1.2036)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6117 (1.2958)\tPrec@1 25.000 (53.410)\n",
      "EPOCH: 10 val Results: Prec@1 53.410 Loss: 1.2958\n",
      "Best Prec@1: 53.410\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [11][0/390]\tTime 0.004 (0.004)\tLoss 1.1811 (1.1811)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [11][78/390]\tTime 0.002 (0.003)\tLoss 1.1507 (1.2217)\tPrec@1 59.375 (57.031)\n",
      "Epoch: [11][156/390]\tTime 0.002 (0.003)\tLoss 1.1143 (1.2330)\tPrec@1 60.156 (56.260)\n",
      "Epoch: [11][234/390]\tTime 0.004 (0.003)\tLoss 1.3425 (1.2403)\tPrec@1 52.344 (55.977)\n",
      "Epoch: [11][312/390]\tTime 0.002 (0.003)\tLoss 1.1065 (1.2456)\tPrec@1 61.719 (55.843)\n",
      "Epoch: [11][390/390]\tTime 0.002 (0.003)\tLoss 1.3126 (1.2531)\tPrec@1 55.000 (55.518)\n",
      "EPOCH: 11 train Results: Prec@1 55.518 Loss: 1.2531\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1333 (1.1333)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5294 (1.2960)\tPrec@1 25.000 (53.430)\n",
      "EPOCH: 11 val Results: Prec@1 53.430 Loss: 1.2960\n",
      "Best Prec@1: 53.430\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [12][0/390]\tTime 0.002 (0.002)\tLoss 1.1759 (1.1759)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [12][78/390]\tTime 0.002 (0.003)\tLoss 1.0957 (1.2001)\tPrec@1 61.719 (58.070)\n",
      "Epoch: [12][156/390]\tTime 0.002 (0.003)\tLoss 1.3876 (1.2142)\tPrec@1 51.562 (57.186)\n",
      "Epoch: [12][234/390]\tTime 0.004 (0.003)\tLoss 1.0957 (1.2265)\tPrec@1 66.406 (56.473)\n",
      "Epoch: [12][312/390]\tTime 0.003 (0.003)\tLoss 1.2848 (1.2350)\tPrec@1 54.688 (56.243)\n",
      "Epoch: [12][390/390]\tTime 0.003 (0.003)\tLoss 1.1339 (1.2418)\tPrec@1 58.750 (56.014)\n",
      "EPOCH: 12 train Results: Prec@1 56.014 Loss: 1.2418\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1276 (1.1276)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2622 (1.2977)\tPrec@1 50.000 (53.580)\n",
      "EPOCH: 12 val Results: Prec@1 53.580 Loss: 1.2977\n",
      "Best Prec@1: 53.580\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [13][0/390]\tTime 0.005 (0.005)\tLoss 1.2045 (1.2045)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.1051 (1.1913)\tPrec@1 59.375 (57.714)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.003)\tLoss 1.1128 (1.1970)\tPrec@1 64.062 (57.713)\n",
      "Epoch: [13][234/390]\tTime 0.006 (0.003)\tLoss 1.3036 (1.2135)\tPrec@1 51.562 (57.078)\n",
      "Epoch: [13][312/390]\tTime 0.003 (0.003)\tLoss 1.2044 (1.2249)\tPrec@1 56.250 (56.624)\n",
      "Epoch: [13][390/390]\tTime 0.007 (0.003)\tLoss 1.2259 (1.2308)\tPrec@1 52.500 (56.516)\n",
      "EPOCH: 13 train Results: Prec@1 56.516 Loss: 1.2308\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1564 (1.1564)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5085 (1.2821)\tPrec@1 25.000 (53.560)\n",
      "EPOCH: 13 val Results: Prec@1 53.560 Loss: 1.2821\n",
      "Best Prec@1: 53.580\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [14][0/390]\tTime 0.003 (0.003)\tLoss 1.0853 (1.0853)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [14][78/390]\tTime 0.002 (0.003)\tLoss 1.2653 (1.1843)\tPrec@1 56.250 (58.248)\n",
      "Epoch: [14][156/390]\tTime 0.011 (0.003)\tLoss 1.3218 (1.1941)\tPrec@1 54.688 (57.504)\n",
      "Epoch: [14][234/390]\tTime 0.003 (0.003)\tLoss 1.2504 (1.2075)\tPrec@1 55.469 (57.131)\n",
      "Epoch: [14][312/390]\tTime 0.002 (0.003)\tLoss 1.1624 (1.2134)\tPrec@1 51.562 (56.954)\n",
      "Epoch: [14][390/390]\tTime 0.004 (0.003)\tLoss 1.3774 (1.2198)\tPrec@1 42.500 (56.650)\n",
      "EPOCH: 14 train Results: Prec@1 56.650 Loss: 1.2198\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1397 (1.1397)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4574 (1.2760)\tPrec@1 31.250 (54.210)\n",
      "EPOCH: 14 val Results: Prec@1 54.210 Loss: 1.2760\n",
      "Best Prec@1: 54.210\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [15][0/390]\tTime 0.003 (0.003)\tLoss 1.0741 (1.0741)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [15][78/390]\tTime 0.005 (0.004)\tLoss 1.2751 (1.1667)\tPrec@1 54.688 (59.049)\n",
      "Epoch: [15][156/390]\tTime 0.003 (0.003)\tLoss 1.0538 (1.1817)\tPrec@1 65.625 (58.539)\n",
      "Epoch: [15][234/390]\tTime 0.008 (0.003)\tLoss 1.2180 (1.1929)\tPrec@1 57.031 (57.889)\n",
      "Epoch: [15][312/390]\tTime 0.002 (0.003)\tLoss 1.3535 (1.2026)\tPrec@1 53.906 (57.478)\n",
      "Epoch: [15][390/390]\tTime 0.003 (0.003)\tLoss 1.2095 (1.2083)\tPrec@1 50.000 (57.258)\n",
      "EPOCH: 15 train Results: Prec@1 57.258 Loss: 1.2083\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1067 (1.1067)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4245 (1.2746)\tPrec@1 31.250 (53.890)\n",
      "EPOCH: 15 val Results: Prec@1 53.890 Loss: 1.2746\n",
      "Best Prec@1: 54.210\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [16][0/390]\tTime 0.009 (0.009)\tLoss 1.0743 (1.0743)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [16][78/390]\tTime 0.005 (0.003)\tLoss 1.1126 (1.1383)\tPrec@1 64.844 (59.909)\n",
      "Epoch: [16][156/390]\tTime 0.002 (0.004)\tLoss 1.1793 (1.1640)\tPrec@1 57.812 (59.042)\n",
      "Epoch: [16][234/390]\tTime 0.005 (0.003)\tLoss 1.3140 (1.1823)\tPrec@1 53.906 (58.451)\n",
      "Epoch: [16][312/390]\tTime 0.002 (0.003)\tLoss 1.2074 (1.1947)\tPrec@1 60.156 (57.817)\n",
      "Epoch: [16][390/390]\tTime 0.001 (0.003)\tLoss 1.4048 (1.2029)\tPrec@1 53.750 (57.464)\n",
      "EPOCH: 16 train Results: Prec@1 57.464 Loss: 1.2029\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0526 (1.0526)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3694 (1.2652)\tPrec@1 43.750 (54.660)\n",
      "EPOCH: 16 val Results: Prec@1 54.660 Loss: 1.2652\n",
      "Best Prec@1: 54.660\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [17][0/390]\tTime 0.004 (0.004)\tLoss 1.1530 (1.1530)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [17][78/390]\tTime 0.002 (0.003)\tLoss 1.2210 (1.1490)\tPrec@1 57.812 (59.553)\n",
      "Epoch: [17][156/390]\tTime 0.004 (0.003)\tLoss 1.3452 (1.1609)\tPrec@1 54.688 (59.261)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.003)\tLoss 1.1725 (1.1761)\tPrec@1 55.469 (58.554)\n",
      "Epoch: [17][312/390]\tTime 0.004 (0.003)\tLoss 1.2177 (1.1831)\tPrec@1 59.375 (58.259)\n",
      "Epoch: [17][390/390]\tTime 0.002 (0.003)\tLoss 1.1847 (1.1917)\tPrec@1 65.000 (57.946)\n",
      "EPOCH: 17 train Results: Prec@1 57.946 Loss: 1.1917\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1621 (1.1621)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3653 (1.2608)\tPrec@1 31.250 (54.930)\n",
      "EPOCH: 17 val Results: Prec@1 54.930 Loss: 1.2608\n",
      "Best Prec@1: 54.930\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [18][0/390]\tTime 0.005 (0.005)\tLoss 0.9917 (0.9917)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [18][78/390]\tTime 0.002 (0.003)\tLoss 1.1807 (1.1414)\tPrec@1 57.812 (59.919)\n",
      "Epoch: [18][156/390]\tTime 0.002 (0.003)\tLoss 1.1015 (1.1594)\tPrec@1 67.188 (59.156)\n",
      "Epoch: [18][234/390]\tTime 0.002 (0.003)\tLoss 1.0437 (1.1805)\tPrec@1 62.500 (58.108)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.003)\tLoss 1.2282 (1.1855)\tPrec@1 58.594 (57.925)\n",
      "Epoch: [18][390/390]\tTime 0.001 (0.003)\tLoss 1.1907 (1.1913)\tPrec@1 58.750 (57.684)\n",
      "EPOCH: 18 train Results: Prec@1 57.684 Loss: 1.1913\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1219 (1.1219)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2898 (1.2579)\tPrec@1 56.250 (54.400)\n",
      "EPOCH: 18 val Results: Prec@1 54.400 Loss: 1.2579\n",
      "Best Prec@1: 54.930\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [19][0/390]\tTime 0.009 (0.009)\tLoss 1.1920 (1.1920)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [19][78/390]\tTime 0.002 (0.003)\tLoss 1.3000 (1.1200)\tPrec@1 57.812 (60.878)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.003)\tLoss 1.1093 (1.1499)\tPrec@1 60.156 (59.723)\n",
      "Epoch: [19][234/390]\tTime 0.002 (0.003)\tLoss 1.1471 (1.1606)\tPrec@1 59.375 (59.099)\n",
      "Epoch: [19][312/390]\tTime 0.003 (0.003)\tLoss 1.0211 (1.1731)\tPrec@1 61.719 (58.546)\n",
      "Epoch: [19][390/390]\tTime 0.001 (0.003)\tLoss 1.3700 (1.1808)\tPrec@1 47.500 (58.196)\n",
      "EPOCH: 19 train Results: Prec@1 58.196 Loss: 1.1808\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1898 (1.1898)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6806 (1.2561)\tPrec@1 31.250 (55.120)\n",
      "EPOCH: 19 val Results: Prec@1 55.120 Loss: 1.2561\n",
      "Best Prec@1: 55.120\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [20][0/390]\tTime 0.002 (0.002)\tLoss 1.0959 (1.0959)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [20][78/390]\tTime 0.005 (0.003)\tLoss 0.9877 (1.1346)\tPrec@1 65.625 (59.780)\n",
      "Epoch: [20][156/390]\tTime 0.002 (0.003)\tLoss 1.0324 (1.1468)\tPrec@1 64.062 (59.350)\n",
      "Epoch: [20][234/390]\tTime 0.003 (0.003)\tLoss 1.1109 (1.1667)\tPrec@1 59.375 (58.554)\n",
      "Epoch: [20][312/390]\tTime 0.002 (0.003)\tLoss 1.2375 (1.1712)\tPrec@1 55.469 (58.389)\n",
      "Epoch: [20][390/390]\tTime 0.006 (0.003)\tLoss 1.2961 (1.1760)\tPrec@1 46.250 (58.200)\n",
      "EPOCH: 20 train Results: Prec@1 58.200 Loss: 1.1760\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1113 (1.1113)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5844 (1.2454)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 20 val Results: Prec@1 55.610 Loss: 1.2454\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [21][0/390]\tTime 0.006 (0.006)\tLoss 1.2493 (1.2493)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [21][78/390]\tTime 0.002 (0.003)\tLoss 1.2604 (1.1140)\tPrec@1 51.562 (60.839)\n",
      "Epoch: [21][156/390]\tTime 0.002 (0.003)\tLoss 1.2338 (1.1381)\tPrec@1 50.781 (59.684)\n",
      "Epoch: [21][234/390]\tTime 0.004 (0.003)\tLoss 1.1532 (1.1492)\tPrec@1 61.719 (59.362)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.1588 (1.1631)\tPrec@1 59.375 (58.928)\n",
      "Epoch: [21][390/390]\tTime 0.003 (0.003)\tLoss 1.1099 (1.1726)\tPrec@1 61.250 (58.556)\n",
      "EPOCH: 21 train Results: Prec@1 58.556 Loss: 1.1726\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1381 (1.1381)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2190 (1.2416)\tPrec@1 43.750 (55.190)\n",
      "EPOCH: 21 val Results: Prec@1 55.190 Loss: 1.2416\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [22][0/390]\tTime 0.002 (0.002)\tLoss 1.1992 (1.1992)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [22][78/390]\tTime 0.004 (0.003)\tLoss 1.0870 (1.1286)\tPrec@1 57.812 (60.117)\n",
      "Epoch: [22][156/390]\tTime 0.005 (0.003)\tLoss 1.0664 (1.1400)\tPrec@1 59.375 (59.619)\n",
      "Epoch: [22][234/390]\tTime 0.002 (0.003)\tLoss 1.1539 (1.1497)\tPrec@1 61.719 (59.245)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.003)\tLoss 1.1234 (1.1607)\tPrec@1 60.938 (59.008)\n",
      "Epoch: [22][390/390]\tTime 0.002 (0.003)\tLoss 1.0674 (1.1685)\tPrec@1 70.000 (58.614)\n",
      "EPOCH: 22 train Results: Prec@1 58.614 Loss: 1.1685\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1167 (1.1167)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2703 (1.2517)\tPrec@1 56.250 (55.080)\n",
      "EPOCH: 22 val Results: Prec@1 55.080 Loss: 1.2517\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [23][0/390]\tTime 0.004 (0.004)\tLoss 1.1154 (1.1154)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [23][78/390]\tTime 0.002 (0.003)\tLoss 1.1565 (1.1141)\tPrec@1 65.625 (60.542)\n",
      "Epoch: [23][156/390]\tTime 0.003 (0.003)\tLoss 1.3532 (1.1387)\tPrec@1 49.219 (59.679)\n",
      "Epoch: [23][234/390]\tTime 0.002 (0.003)\tLoss 1.1430 (1.1452)\tPrec@1 60.938 (59.412)\n",
      "Epoch: [23][312/390]\tTime 0.007 (0.003)\tLoss 1.2615 (1.1569)\tPrec@1 54.688 (59.041)\n",
      "Epoch: [23][390/390]\tTime 0.001 (0.003)\tLoss 1.3571 (1.1668)\tPrec@1 51.250 (58.766)\n",
      "EPOCH: 23 train Results: Prec@1 58.766 Loss: 1.1668\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1269 (1.1269)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2848 (1.2481)\tPrec@1 50.000 (55.540)\n",
      "EPOCH: 23 val Results: Prec@1 55.540 Loss: 1.2481\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [24][0/390]\tTime 0.009 (0.009)\tLoss 1.1320 (1.1320)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.1888 (1.0970)\tPrec@1 58.594 (61.650)\n",
      "Epoch: [24][156/390]\tTime 0.002 (0.003)\tLoss 1.2143 (1.1244)\tPrec@1 58.594 (60.106)\n",
      "Epoch: [24][234/390]\tTime 0.004 (0.003)\tLoss 1.2408 (1.1448)\tPrec@1 58.594 (59.438)\n",
      "Epoch: [24][312/390]\tTime 0.002 (0.003)\tLoss 1.2849 (1.1524)\tPrec@1 57.812 (59.238)\n",
      "Epoch: [24][390/390]\tTime 0.001 (0.003)\tLoss 1.3164 (1.1592)\tPrec@1 56.250 (58.932)\n",
      "EPOCH: 24 train Results: Prec@1 58.932 Loss: 1.1592\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0830 (1.0830)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.2823 (1.2530)\tPrec@1 56.250 (55.010)\n",
      "EPOCH: 24 val Results: Prec@1 55.010 Loss: 1.2530\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [25][0/390]\tTime 0.002 (0.002)\tLoss 1.0732 (1.0732)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [25][78/390]\tTime 0.002 (0.003)\tLoss 1.0521 (1.1025)\tPrec@1 57.812 (61.046)\n",
      "Epoch: [25][156/390]\tTime 0.003 (0.003)\tLoss 1.2440 (1.1258)\tPrec@1 54.688 (60.206)\n",
      "Epoch: [25][234/390]\tTime 0.003 (0.003)\tLoss 1.2274 (1.1387)\tPrec@1 54.688 (59.588)\n",
      "Epoch: [25][312/390]\tTime 0.005 (0.003)\tLoss 1.2523 (1.1492)\tPrec@1 53.906 (59.215)\n",
      "Epoch: [25][390/390]\tTime 0.001 (0.003)\tLoss 1.3174 (1.1534)\tPrec@1 51.250 (59.074)\n",
      "EPOCH: 25 train Results: Prec@1 59.074 Loss: 1.1534\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0846 (1.0846)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4605 (1.2443)\tPrec@1 31.250 (55.310)\n",
      "EPOCH: 25 val Results: Prec@1 55.310 Loss: 1.2443\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [26][0/390]\tTime 0.004 (0.004)\tLoss 1.1093 (1.1093)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [26][78/390]\tTime 0.003 (0.003)\tLoss 1.1272 (1.0922)\tPrec@1 64.844 (61.561)\n",
      "Epoch: [26][156/390]\tTime 0.002 (0.003)\tLoss 1.1578 (1.1174)\tPrec@1 58.594 (60.793)\n",
      "Epoch: [26][234/390]\tTime 0.004 (0.003)\tLoss 1.0955 (1.1319)\tPrec@1 64.844 (60.116)\n",
      "Epoch: [26][312/390]\tTime 0.003 (0.003)\tLoss 1.1409 (1.1446)\tPrec@1 56.250 (59.535)\n",
      "Epoch: [26][390/390]\tTime 0.001 (0.003)\tLoss 1.1945 (1.1506)\tPrec@1 56.250 (59.272)\n",
      "EPOCH: 26 train Results: Prec@1 59.272 Loss: 1.1506\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1052 (1.1052)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5416 (1.2453)\tPrec@1 37.500 (55.350)\n",
      "EPOCH: 26 val Results: Prec@1 55.350 Loss: 1.2453\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [27][0/390]\tTime 0.003 (0.003)\tLoss 1.1186 (1.1186)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [27][78/390]\tTime 0.003 (0.003)\tLoss 0.9938 (1.0932)\tPrec@1 64.062 (61.343)\n",
      "Epoch: [27][156/390]\tTime 0.002 (0.003)\tLoss 0.9739 (1.1117)\tPrec@1 67.969 (60.495)\n",
      "Epoch: [27][234/390]\tTime 0.002 (0.003)\tLoss 0.9497 (1.1266)\tPrec@1 67.188 (59.930)\n",
      "Epoch: [27][312/390]\tTime 0.004 (0.003)\tLoss 1.0674 (1.1386)\tPrec@1 62.500 (59.592)\n",
      "Epoch: [27][390/390]\tTime 0.002 (0.003)\tLoss 1.5435 (1.1484)\tPrec@1 43.750 (59.242)\n",
      "EPOCH: 27 train Results: Prec@1 59.242 Loss: 1.1484\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0920 (1.0920)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4958 (1.2396)\tPrec@1 43.750 (55.560)\n",
      "EPOCH: 27 val Results: Prec@1 55.560 Loss: 1.2396\n",
      "Best Prec@1: 55.610\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [28][0/390]\tTime 0.004 (0.004)\tLoss 1.0635 (1.0635)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [28][78/390]\tTime 0.005 (0.003)\tLoss 0.9934 (1.0911)\tPrec@1 66.406 (61.669)\n",
      "Epoch: [28][156/390]\tTime 0.003 (0.003)\tLoss 1.1167 (1.1076)\tPrec@1 60.938 (61.082)\n",
      "Epoch: [28][234/390]\tTime 0.004 (0.003)\tLoss 1.1034 (1.1234)\tPrec@1 57.812 (60.416)\n",
      "Epoch: [28][312/390]\tTime 0.004 (0.003)\tLoss 1.2519 (1.1332)\tPrec@1 56.250 (60.059)\n",
      "Epoch: [28][390/390]\tTime 0.001 (0.003)\tLoss 1.2989 (1.1420)\tPrec@1 50.000 (59.740)\n",
      "EPOCH: 28 train Results: Prec@1 59.740 Loss: 1.1420\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0942 (1.0942)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4875 (1.2379)\tPrec@1 50.000 (56.030)\n",
      "EPOCH: 28 val Results: Prec@1 56.030 Loss: 1.2379\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [29][0/390]\tTime 0.002 (0.002)\tLoss 1.0146 (1.0146)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [29][78/390]\tTime 0.003 (0.003)\tLoss 1.1840 (1.1012)\tPrec@1 56.250 (60.809)\n",
      "Epoch: [29][156/390]\tTime 0.005 (0.003)\tLoss 1.1207 (1.1148)\tPrec@1 59.375 (60.594)\n",
      "Epoch: [29][234/390]\tTime 0.002 (0.003)\tLoss 0.9797 (1.1255)\tPrec@1 72.656 (60.223)\n",
      "Epoch: [29][312/390]\tTime 0.008 (0.003)\tLoss 1.1592 (1.1367)\tPrec@1 57.031 (59.802)\n",
      "Epoch: [29][390/390]\tTime 0.001 (0.003)\tLoss 1.2679 (1.1435)\tPrec@1 50.000 (59.612)\n",
      "EPOCH: 29 train Results: Prec@1 59.612 Loss: 1.1435\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0259 (1.0259)\tPrec@1 69.531 (69.531)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1773 (1.2469)\tPrec@1 56.250 (55.410)\n",
      "EPOCH: 29 val Results: Prec@1 55.410 Loss: 1.2469\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [30][0/390]\tTime 0.002 (0.002)\tLoss 1.0986 (1.0986)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.003)\tLoss 1.1054 (1.0869)\tPrec@1 58.594 (61.491)\n",
      "Epoch: [30][156/390]\tTime 0.006 (0.003)\tLoss 1.1313 (1.1052)\tPrec@1 63.281 (60.898)\n",
      "Epoch: [30][234/390]\tTime 0.003 (0.003)\tLoss 1.1090 (1.1189)\tPrec@1 56.250 (60.436)\n",
      "Epoch: [30][312/390]\tTime 0.003 (0.003)\tLoss 1.1544 (1.1302)\tPrec@1 59.375 (59.927)\n",
      "Epoch: [30][390/390]\tTime 0.003 (0.003)\tLoss 1.1632 (1.1399)\tPrec@1 57.500 (59.570)\n",
      "EPOCH: 30 train Results: Prec@1 59.570 Loss: 1.1399\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0440 (1.0440)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3766 (1.2475)\tPrec@1 37.500 (55.140)\n",
      "EPOCH: 30 val Results: Prec@1 55.140 Loss: 1.2475\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [31][0/390]\tTime 0.009 (0.009)\tLoss 1.1084 (1.1084)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [31][78/390]\tTime 0.002 (0.004)\tLoss 1.0360 (1.0886)\tPrec@1 60.156 (61.541)\n",
      "Epoch: [31][156/390]\tTime 0.002 (0.003)\tLoss 1.0970 (1.1084)\tPrec@1 57.812 (60.709)\n",
      "Epoch: [31][234/390]\tTime 0.003 (0.003)\tLoss 1.1676 (1.1207)\tPrec@1 60.156 (60.256)\n",
      "Epoch: [31][312/390]\tTime 0.003 (0.003)\tLoss 1.1111 (1.1287)\tPrec@1 62.500 (59.872)\n",
      "Epoch: [31][390/390]\tTime 0.002 (0.003)\tLoss 1.1985 (1.1387)\tPrec@1 58.750 (59.516)\n",
      "EPOCH: 31 train Results: Prec@1 59.516 Loss: 1.1387\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1271 (1.1271)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2666 (1.2445)\tPrec@1 37.500 (55.690)\n",
      "EPOCH: 31 val Results: Prec@1 55.690 Loss: 1.2445\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [32][0/390]\tTime 0.004 (0.004)\tLoss 1.0746 (1.0746)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [32][78/390]\tTime 0.005 (0.003)\tLoss 1.1052 (1.0882)\tPrec@1 58.594 (61.837)\n",
      "Epoch: [32][156/390]\tTime 0.002 (0.003)\tLoss 0.8864 (1.1008)\tPrec@1 66.406 (61.291)\n",
      "Epoch: [32][234/390]\tTime 0.003 (0.003)\tLoss 1.1043 (1.1154)\tPrec@1 60.156 (60.655)\n",
      "Epoch: [32][312/390]\tTime 0.002 (0.003)\tLoss 1.0891 (1.1241)\tPrec@1 60.156 (60.301)\n",
      "Epoch: [32][390/390]\tTime 0.006 (0.003)\tLoss 1.0233 (1.1346)\tPrec@1 68.750 (59.990)\n",
      "EPOCH: 32 train Results: Prec@1 59.990 Loss: 1.1346\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1170 (1.1170)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3565 (1.2444)\tPrec@1 50.000 (55.580)\n",
      "EPOCH: 32 val Results: Prec@1 55.580 Loss: 1.2444\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [33][0/390]\tTime 0.008 (0.008)\tLoss 1.1935 (1.1935)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [33][78/390]\tTime 0.002 (0.004)\tLoss 1.2894 (1.1008)\tPrec@1 55.469 (61.462)\n",
      "Epoch: [33][156/390]\tTime 0.002 (0.004)\tLoss 1.2633 (1.1108)\tPrec@1 54.688 (60.803)\n",
      "Epoch: [33][234/390]\tTime 0.005 (0.003)\tLoss 1.2622 (1.1154)\tPrec@1 56.250 (60.379)\n",
      "Epoch: [33][312/390]\tTime 0.003 (0.003)\tLoss 1.1789 (1.1253)\tPrec@1 57.812 (59.982)\n",
      "Epoch: [33][390/390]\tTime 0.001 (0.003)\tLoss 1.1605 (1.1321)\tPrec@1 56.250 (59.826)\n",
      "EPOCH: 33 train Results: Prec@1 59.826 Loss: 1.1321\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1105 (1.1105)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3731 (1.2408)\tPrec@1 43.750 (55.540)\n",
      "EPOCH: 33 val Results: Prec@1 55.540 Loss: 1.2408\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [34][0/390]\tTime 0.003 (0.003)\tLoss 1.0543 (1.0543)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [34][78/390]\tTime 0.002 (0.003)\tLoss 1.1212 (1.0719)\tPrec@1 62.500 (62.362)\n",
      "Epoch: [34][156/390]\tTime 0.003 (0.003)\tLoss 1.1158 (1.0990)\tPrec@1 61.719 (61.067)\n",
      "Epoch: [34][234/390]\tTime 0.009 (0.003)\tLoss 1.1210 (1.1090)\tPrec@1 62.500 (60.755)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.1343 (1.1204)\tPrec@1 64.844 (60.284)\n",
      "Epoch: [34][390/390]\tTime 0.002 (0.003)\tLoss 1.0579 (1.1304)\tPrec@1 65.000 (59.928)\n",
      "EPOCH: 34 train Results: Prec@1 59.928 Loss: 1.1304\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0616 (1.0616)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3219 (1.2385)\tPrec@1 43.750 (55.540)\n",
      "EPOCH: 34 val Results: Prec@1 55.540 Loss: 1.2385\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [35][0/390]\tTime 0.004 (0.004)\tLoss 1.1319 (1.1319)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [35][78/390]\tTime 0.002 (0.003)\tLoss 1.1950 (1.0746)\tPrec@1 57.031 (61.600)\n",
      "Epoch: [35][156/390]\tTime 0.002 (0.003)\tLoss 1.3534 (1.0933)\tPrec@1 48.438 (60.982)\n",
      "Epoch: [35][234/390]\tTime 0.002 (0.003)\tLoss 1.1661 (1.1127)\tPrec@1 63.281 (60.419)\n",
      "Epoch: [35][312/390]\tTime 0.004 (0.003)\tLoss 1.1302 (1.1223)\tPrec@1 61.719 (60.094)\n",
      "Epoch: [35][390/390]\tTime 0.001 (0.003)\tLoss 1.0637 (1.1279)\tPrec@1 60.000 (59.914)\n",
      "EPOCH: 35 train Results: Prec@1 59.914 Loss: 1.1279\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0851 (1.0851)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3533 (1.2411)\tPrec@1 43.750 (56.260)\n",
      "EPOCH: 35 val Results: Prec@1 56.260 Loss: 1.2411\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [36][0/390]\tTime 0.005 (0.005)\tLoss 1.2431 (1.2431)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [36][78/390]\tTime 0.006 (0.004)\tLoss 1.1088 (1.0737)\tPrec@1 64.062 (62.282)\n",
      "Epoch: [36][156/390]\tTime 0.007 (0.003)\tLoss 1.1542 (1.0923)\tPrec@1 60.938 (61.639)\n",
      "Epoch: [36][234/390]\tTime 0.004 (0.003)\tLoss 1.1186 (1.1033)\tPrec@1 65.625 (61.137)\n",
      "Epoch: [36][312/390]\tTime 0.005 (0.003)\tLoss 1.1454 (1.1131)\tPrec@1 62.500 (60.635)\n",
      "Epoch: [36][390/390]\tTime 0.003 (0.003)\tLoss 1.2185 (1.1229)\tPrec@1 60.000 (60.400)\n",
      "EPOCH: 36 train Results: Prec@1 60.400 Loss: 1.1229\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0986 (1.0986)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3459 (1.2395)\tPrec@1 43.750 (55.930)\n",
      "EPOCH: 36 val Results: Prec@1 55.930 Loss: 1.2395\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [37][0/390]\tTime 0.003 (0.003)\tLoss 0.9165 (0.9165)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 0.9462 (1.0602)\tPrec@1 69.531 (62.965)\n",
      "Epoch: [37][156/390]\tTime 0.002 (0.003)\tLoss 1.0731 (1.0908)\tPrec@1 58.594 (61.455)\n",
      "Epoch: [37][234/390]\tTime 0.002 (0.003)\tLoss 1.3014 (1.1074)\tPrec@1 52.344 (60.685)\n",
      "Epoch: [37][312/390]\tTime 0.002 (0.003)\tLoss 1.0386 (1.1216)\tPrec@1 62.500 (60.194)\n",
      "Epoch: [37][390/390]\tTime 0.003 (0.003)\tLoss 1.1650 (1.1273)\tPrec@1 57.500 (60.022)\n",
      "EPOCH: 37 train Results: Prec@1 60.022 Loss: 1.1273\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1171 (1.1171)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.004 (0.003)\tLoss 1.3028 (1.2452)\tPrec@1 50.000 (55.530)\n",
      "EPOCH: 37 val Results: Prec@1 55.530 Loss: 1.2452\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [38][0/390]\tTime 0.009 (0.009)\tLoss 1.0316 (1.0316)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.003)\tLoss 1.1614 (1.0794)\tPrec@1 60.938 (62.322)\n",
      "Epoch: [38][156/390]\tTime 0.003 (0.003)\tLoss 1.0105 (1.0925)\tPrec@1 61.719 (61.754)\n",
      "Epoch: [38][234/390]\tTime 0.004 (0.003)\tLoss 1.1016 (1.1047)\tPrec@1 60.938 (61.203)\n",
      "Epoch: [38][312/390]\tTime 0.003 (0.004)\tLoss 1.3448 (1.1113)\tPrec@1 49.219 (60.833)\n",
      "Epoch: [38][390/390]\tTime 0.001 (0.004)\tLoss 1.2034 (1.1200)\tPrec@1 60.000 (60.506)\n",
      "EPOCH: 38 train Results: Prec@1 60.506 Loss: 1.1200\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1075 (1.1075)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2866 (1.2310)\tPrec@1 50.000 (56.240)\n",
      "EPOCH: 38 val Results: Prec@1 56.240 Loss: 1.2310\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [39][0/390]\tTime 0.006 (0.006)\tLoss 1.0911 (1.0911)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [39][78/390]\tTime 0.004 (0.004)\tLoss 1.1914 (1.0532)\tPrec@1 60.156 (63.331)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.004)\tLoss 1.0883 (1.0823)\tPrec@1 59.375 (62.167)\n",
      "Epoch: [39][234/390]\tTime 0.005 (0.004)\tLoss 1.1433 (1.1004)\tPrec@1 60.156 (61.393)\n",
      "Epoch: [39][312/390]\tTime 0.003 (0.004)\tLoss 1.4549 (1.1087)\tPrec@1 46.094 (60.990)\n",
      "Epoch: [39][390/390]\tTime 0.004 (0.004)\tLoss 1.2186 (1.1202)\tPrec@1 52.500 (60.498)\n",
      "EPOCH: 39 train Results: Prec@1 60.498 Loss: 1.1202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0268 (1.0268)\tPrec@1 68.750 (68.750)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5476 (1.2347)\tPrec@1 50.000 (55.660)\n",
      "EPOCH: 39 val Results: Prec@1 55.660 Loss: 1.2347\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [40][0/390]\tTime 0.004 (0.004)\tLoss 1.0170 (1.0170)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.003)\tLoss 1.0337 (1.0625)\tPrec@1 64.062 (62.698)\n",
      "Epoch: [40][156/390]\tTime 0.005 (0.003)\tLoss 1.0657 (1.0736)\tPrec@1 60.938 (62.361)\n",
      "Epoch: [40][234/390]\tTime 0.005 (0.003)\tLoss 1.3863 (1.0888)\tPrec@1 46.875 (61.765)\n",
      "Epoch: [40][312/390]\tTime 0.004 (0.003)\tLoss 1.3729 (1.1037)\tPrec@1 50.000 (61.195)\n",
      "Epoch: [40][390/390]\tTime 0.004 (0.003)\tLoss 1.3379 (1.1171)\tPrec@1 55.000 (60.556)\n",
      "EPOCH: 40 train Results: Prec@1 60.556 Loss: 1.1171\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1092 (1.1092)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5726 (1.2327)\tPrec@1 43.750 (55.870)\n",
      "EPOCH: 40 val Results: Prec@1 55.870 Loss: 1.2327\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [41][0/390]\tTime 0.002 (0.002)\tLoss 1.1889 (1.1889)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [41][78/390]\tTime 0.002 (0.003)\tLoss 1.0530 (1.0520)\tPrec@1 65.625 (62.975)\n",
      "Epoch: [41][156/390]\tTime 0.002 (0.003)\tLoss 1.2933 (1.0792)\tPrec@1 53.906 (61.968)\n",
      "Epoch: [41][234/390]\tTime 0.002 (0.003)\tLoss 1.1498 (1.0914)\tPrec@1 60.156 (61.516)\n",
      "Epoch: [41][312/390]\tTime 0.004 (0.003)\tLoss 1.1189 (1.1061)\tPrec@1 65.625 (60.933)\n",
      "Epoch: [41][390/390]\tTime 0.003 (0.003)\tLoss 1.0799 (1.1120)\tPrec@1 62.500 (60.592)\n",
      "EPOCH: 41 train Results: Prec@1 60.592 Loss: 1.1120\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0359 (1.0359)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2788 (1.2296)\tPrec@1 56.250 (56.100)\n",
      "EPOCH: 41 val Results: Prec@1 56.100 Loss: 1.2296\n",
      "Best Prec@1: 56.260\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [42][0/390]\tTime 0.002 (0.002)\tLoss 1.0957 (1.0957)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [42][78/390]\tTime 0.004 (0.003)\tLoss 0.9927 (1.0620)\tPrec@1 60.156 (62.876)\n",
      "Epoch: [42][156/390]\tTime 0.004 (0.003)\tLoss 1.0492 (1.0740)\tPrec@1 61.719 (62.331)\n",
      "Epoch: [42][234/390]\tTime 0.002 (0.003)\tLoss 1.1259 (1.0886)\tPrec@1 63.281 (61.420)\n",
      "Epoch: [42][312/390]\tTime 0.002 (0.003)\tLoss 1.2214 (1.1015)\tPrec@1 53.906 (60.933)\n",
      "Epoch: [42][390/390]\tTime 0.001 (0.003)\tLoss 1.2360 (1.1124)\tPrec@1 53.750 (60.518)\n",
      "EPOCH: 42 train Results: Prec@1 60.518 Loss: 1.1124\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0219 (1.0219)\tPrec@1 70.312 (70.312)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2728 (1.2332)\tPrec@1 37.500 (56.350)\n",
      "EPOCH: 42 val Results: Prec@1 56.350 Loss: 1.2332\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [43][0/390]\tTime 0.004 (0.004)\tLoss 1.0696 (1.0696)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [43][78/390]\tTime 0.003 (0.003)\tLoss 1.0778 (1.0569)\tPrec@1 61.719 (62.451)\n",
      "Epoch: [43][156/390]\tTime 0.002 (0.003)\tLoss 1.2046 (1.0806)\tPrec@1 55.469 (61.769)\n",
      "Epoch: [43][234/390]\tTime 0.003 (0.003)\tLoss 1.1199 (1.0878)\tPrec@1 58.594 (61.353)\n",
      "Epoch: [43][312/390]\tTime 0.007 (0.003)\tLoss 1.0187 (1.1022)\tPrec@1 61.719 (60.845)\n",
      "Epoch: [43][390/390]\tTime 0.004 (0.003)\tLoss 1.0087 (1.1114)\tPrec@1 58.750 (60.486)\n",
      "EPOCH: 43 train Results: Prec@1 60.486 Loss: 1.1114\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1239 (1.1239)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2751 (1.2362)\tPrec@1 37.500 (55.940)\n",
      "EPOCH: 43 val Results: Prec@1 55.940 Loss: 1.2362\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [44][0/390]\tTime 0.008 (0.008)\tLoss 1.0896 (1.0896)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.003)\tLoss 0.9578 (1.0592)\tPrec@1 64.844 (63.143)\n",
      "Epoch: [44][156/390]\tTime 0.002 (0.003)\tLoss 1.0253 (1.0716)\tPrec@1 61.719 (62.376)\n",
      "Epoch: [44][234/390]\tTime 0.003 (0.003)\tLoss 1.0702 (1.0869)\tPrec@1 59.375 (61.872)\n",
      "Epoch: [44][312/390]\tTime 0.004 (0.003)\tLoss 1.1389 (1.0987)\tPrec@1 55.469 (61.297)\n",
      "Epoch: [44][390/390]\tTime 0.002 (0.003)\tLoss 1.1196 (1.1083)\tPrec@1 56.250 (60.810)\n",
      "EPOCH: 44 train Results: Prec@1 60.810 Loss: 1.1083\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0369 (1.0369)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3291 (1.2319)\tPrec@1 56.250 (56.280)\n",
      "EPOCH: 44 val Results: Prec@1 56.280 Loss: 1.2319\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [45][0/390]\tTime 0.002 (0.002)\tLoss 1.1315 (1.1315)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [45][78/390]\tTime 0.002 (0.003)\tLoss 1.1972 (1.0548)\tPrec@1 59.375 (62.579)\n",
      "Epoch: [45][156/390]\tTime 0.003 (0.002)\tLoss 1.1068 (1.0660)\tPrec@1 57.031 (62.346)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.003)\tLoss 1.1699 (1.0825)\tPrec@1 60.156 (61.785)\n",
      "Epoch: [45][312/390]\tTime 0.002 (0.003)\tLoss 1.0127 (1.0960)\tPrec@1 62.500 (61.362)\n",
      "Epoch: [45][390/390]\tTime 0.001 (0.003)\tLoss 1.2370 (1.1045)\tPrec@1 50.000 (61.100)\n",
      "EPOCH: 45 train Results: Prec@1 61.100 Loss: 1.1045\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1024 (1.1024)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3949 (1.2324)\tPrec@1 43.750 (55.950)\n",
      "EPOCH: 45 val Results: Prec@1 55.950 Loss: 1.2324\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [46][0/390]\tTime 0.002 (0.002)\tLoss 1.0781 (1.0781)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [46][78/390]\tTime 0.003 (0.003)\tLoss 1.1216 (1.0565)\tPrec@1 62.500 (62.816)\n",
      "Epoch: [46][156/390]\tTime 0.006 (0.003)\tLoss 1.2054 (1.0659)\tPrec@1 54.688 (62.420)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.0797 (1.0829)\tPrec@1 64.062 (61.755)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.004)\tLoss 1.1435 (1.0934)\tPrec@1 60.156 (61.324)\n",
      "Epoch: [46][390/390]\tTime 0.005 (0.004)\tLoss 1.0570 (1.1054)\tPrec@1 61.250 (60.902)\n",
      "EPOCH: 46 train Results: Prec@1 60.902 Loss: 1.1054\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1002 (1.1002)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3881 (1.2367)\tPrec@1 37.500 (55.930)\n",
      "EPOCH: 46 val Results: Prec@1 55.930 Loss: 1.2367\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [47][0/390]\tTime 0.003 (0.003)\tLoss 1.3072 (1.3072)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [47][78/390]\tTime 0.002 (0.003)\tLoss 1.2435 (1.0478)\tPrec@1 56.250 (62.935)\n",
      "Epoch: [47][156/390]\tTime 0.009 (0.003)\tLoss 1.0516 (1.0675)\tPrec@1 64.844 (62.311)\n",
      "Epoch: [47][234/390]\tTime 0.071 (0.004)\tLoss 1.0144 (1.0859)\tPrec@1 61.719 (61.469)\n",
      "Epoch: [47][312/390]\tTime 0.002 (0.004)\tLoss 1.1420 (1.0919)\tPrec@1 53.125 (61.170)\n",
      "Epoch: [47][390/390]\tTime 0.003 (0.004)\tLoss 1.1035 (1.1013)\tPrec@1 53.750 (60.872)\n",
      "EPOCH: 47 train Results: Prec@1 60.872 Loss: 1.1013\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0236 (1.0236)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2713 (1.2342)\tPrec@1 50.000 (56.180)\n",
      "EPOCH: 47 val Results: Prec@1 56.180 Loss: 1.2342\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [48][0/390]\tTime 0.003 (0.003)\tLoss 1.0313 (1.0313)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [48][78/390]\tTime 0.003 (0.003)\tLoss 1.1448 (1.0513)\tPrec@1 55.469 (63.548)\n",
      "Epoch: [48][156/390]\tTime 0.005 (0.003)\tLoss 1.1415 (1.0697)\tPrec@1 60.156 (62.580)\n",
      "Epoch: [48][234/390]\tTime 0.002 (0.003)\tLoss 1.0314 (1.0791)\tPrec@1 62.500 (61.988)\n",
      "Epoch: [48][312/390]\tTime 0.002 (0.003)\tLoss 1.1223 (1.0930)\tPrec@1 61.719 (61.477)\n",
      "Epoch: [48][390/390]\tTime 0.001 (0.003)\tLoss 1.3848 (1.1034)\tPrec@1 52.500 (61.012)\n",
      "EPOCH: 48 train Results: Prec@1 61.012 Loss: 1.1034\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0834 (1.0834)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2664 (1.2405)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 48 val Results: Prec@1 55.610 Loss: 1.2405\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [49][0/390]\tTime 0.002 (0.002)\tLoss 1.0140 (1.0140)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [49][78/390]\tTime 0.004 (0.003)\tLoss 1.1803 (1.0312)\tPrec@1 58.594 (64.349)\n",
      "Epoch: [49][156/390]\tTime 0.002 (0.003)\tLoss 1.1827 (1.0613)\tPrec@1 57.812 (62.863)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.003)\tLoss 1.0539 (1.0794)\tPrec@1 58.594 (62.081)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.1552 (1.0898)\tPrec@1 56.250 (61.586)\n",
      "Epoch: [49][390/390]\tTime 0.001 (0.003)\tLoss 1.2602 (1.1020)\tPrec@1 57.500 (61.062)\n",
      "EPOCH: 49 train Results: Prec@1 61.062 Loss: 1.1020\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0766 (1.0766)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0305 (1.2342)\tPrec@1 50.000 (55.450)\n",
      "EPOCH: 49 val Results: Prec@1 55.450 Loss: 1.2342\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [50][0/390]\tTime 0.009 (0.009)\tLoss 0.9991 (0.9991)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [50][78/390]\tTime 0.002 (0.003)\tLoss 1.0411 (1.0431)\tPrec@1 64.844 (63.331)\n",
      "Epoch: [50][156/390]\tTime 0.002 (0.003)\tLoss 1.0951 (1.0587)\tPrec@1 56.250 (62.376)\n",
      "Epoch: [50][234/390]\tTime 0.003 (0.003)\tLoss 1.2334 (1.0780)\tPrec@1 54.688 (61.809)\n",
      "Epoch: [50][312/390]\tTime 0.004 (0.003)\tLoss 1.0302 (1.0894)\tPrec@1 58.594 (61.364)\n",
      "Epoch: [50][390/390]\tTime 0.004 (0.003)\tLoss 1.0230 (1.1003)\tPrec@1 65.000 (60.878)\n",
      "EPOCH: 50 train Results: Prec@1 60.878 Loss: 1.1003\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0744 (1.0744)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3533 (1.2394)\tPrec@1 50.000 (55.330)\n",
      "EPOCH: 50 val Results: Prec@1 55.330 Loss: 1.2394\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [51][0/390]\tTime 0.002 (0.002)\tLoss 1.0875 (1.0875)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [51][78/390]\tTime 0.002 (0.003)\tLoss 1.1042 (1.0511)\tPrec@1 58.594 (62.401)\n",
      "Epoch: [51][156/390]\tTime 0.004 (0.003)\tLoss 1.1185 (1.0677)\tPrec@1 65.625 (61.848)\n",
      "Epoch: [51][234/390]\tTime 0.002 (0.003)\tLoss 1.1774 (1.0784)\tPrec@1 55.469 (61.549)\n",
      "Epoch: [51][312/390]\tTime 0.004 (0.003)\tLoss 1.0610 (1.0883)\tPrec@1 60.938 (61.277)\n",
      "Epoch: [51][390/390]\tTime 0.004 (0.003)\tLoss 1.1077 (1.0990)\tPrec@1 57.500 (60.978)\n",
      "EPOCH: 51 train Results: Prec@1 60.978 Loss: 1.0990\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0972 (1.0972)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2760 (1.2414)\tPrec@1 31.250 (56.240)\n",
      "EPOCH: 51 val Results: Prec@1 56.240 Loss: 1.2414\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [52][0/390]\tTime 0.006 (0.006)\tLoss 1.0283 (1.0283)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [52][78/390]\tTime 0.004 (0.003)\tLoss 1.1272 (1.0395)\tPrec@1 53.906 (63.489)\n",
      "Epoch: [52][156/390]\tTime 0.002 (0.003)\tLoss 1.1983 (1.0511)\tPrec@1 54.688 (63.037)\n",
      "Epoch: [52][234/390]\tTime 0.003 (0.003)\tLoss 0.9455 (1.0696)\tPrec@1 68.750 (62.134)\n",
      "Epoch: [52][312/390]\tTime 0.005 (0.003)\tLoss 1.2442 (1.0863)\tPrec@1 56.250 (61.532)\n",
      "Epoch: [52][390/390]\tTime 0.003 (0.003)\tLoss 1.2180 (1.0975)\tPrec@1 56.250 (61.118)\n",
      "EPOCH: 52 train Results: Prec@1 61.118 Loss: 1.0975\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0850 (1.0850)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2981 (1.2413)\tPrec@1 62.500 (55.470)\n",
      "EPOCH: 52 val Results: Prec@1 55.470 Loss: 1.2413\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [53][0/390]\tTime 0.006 (0.006)\tLoss 1.1355 (1.1355)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [53][78/390]\tTime 0.002 (0.003)\tLoss 1.0428 (1.0328)\tPrec@1 65.625 (63.252)\n",
      "Epoch: [53][156/390]\tTime 0.002 (0.003)\tLoss 1.0206 (1.0514)\tPrec@1 66.406 (62.809)\n",
      "Epoch: [53][234/390]\tTime 0.002 (0.003)\tLoss 1.1020 (1.0704)\tPrec@1 56.250 (62.035)\n",
      "Epoch: [53][312/390]\tTime 0.004 (0.003)\tLoss 1.4058 (1.0835)\tPrec@1 53.125 (61.616)\n",
      "Epoch: [53][390/390]\tTime 0.002 (0.003)\tLoss 1.0275 (1.0952)\tPrec@1 63.750 (61.200)\n",
      "EPOCH: 53 train Results: Prec@1 61.200 Loss: 1.0952\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1602 (1.1602)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3560 (1.2338)\tPrec@1 37.500 (55.980)\n",
      "EPOCH: 53 val Results: Prec@1 55.980 Loss: 1.2338\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [54][0/390]\tTime 0.005 (0.005)\tLoss 0.9865 (0.9865)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [54][78/390]\tTime 0.003 (0.003)\tLoss 0.9839 (1.0348)\tPrec@1 65.625 (63.825)\n",
      "Epoch: [54][156/390]\tTime 0.003 (0.003)\tLoss 0.9527 (1.0531)\tPrec@1 61.719 (63.032)\n",
      "Epoch: [54][234/390]\tTime 0.004 (0.003)\tLoss 1.0113 (1.0686)\tPrec@1 64.062 (62.414)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2670 (1.0825)\tPrec@1 53.906 (61.701)\n",
      "Epoch: [54][390/390]\tTime 0.004 (0.003)\tLoss 1.1946 (1.0960)\tPrec@1 53.750 (61.248)\n",
      "EPOCH: 54 train Results: Prec@1 61.248 Loss: 1.0960\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0463 (1.0463)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2251 (1.2345)\tPrec@1 43.750 (56.290)\n",
      "EPOCH: 54 val Results: Prec@1 56.290 Loss: 1.2345\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.1065 (1.1065)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.1141 (1.0299)\tPrec@1 64.062 (63.459)\n",
      "Epoch: [55][156/390]\tTime 0.003 (0.003)\tLoss 1.1611 (1.0543)\tPrec@1 55.469 (62.619)\n",
      "Epoch: [55][234/390]\tTime 0.003 (0.003)\tLoss 1.1703 (1.0737)\tPrec@1 59.375 (62.035)\n",
      "Epoch: [55][312/390]\tTime 0.005 (0.003)\tLoss 0.9202 (1.0825)\tPrec@1 71.875 (61.629)\n",
      "Epoch: [55][390/390]\tTime 0.005 (0.003)\tLoss 1.0324 (1.0926)\tPrec@1 63.750 (61.248)\n",
      "EPOCH: 55 train Results: Prec@1 61.248 Loss: 1.0926\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1257 (1.1257)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4026 (1.2321)\tPrec@1 43.750 (56.020)\n",
      "EPOCH: 55 val Results: Prec@1 56.020 Loss: 1.2321\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [56][0/390]\tTime 0.004 (0.004)\tLoss 0.9296 (0.9296)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [56][78/390]\tTime 0.004 (0.003)\tLoss 1.1026 (1.0504)\tPrec@1 62.500 (62.688)\n",
      "Epoch: [56][156/390]\tTime 0.003 (0.003)\tLoss 0.9807 (1.0585)\tPrec@1 67.188 (62.435)\n",
      "Epoch: [56][234/390]\tTime 0.002 (0.003)\tLoss 1.1657 (1.0697)\tPrec@1 60.156 (62.045)\n",
      "Epoch: [56][312/390]\tTime 0.002 (0.003)\tLoss 1.0364 (1.0832)\tPrec@1 67.188 (61.539)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.003)\tLoss 1.1828 (1.0927)\tPrec@1 63.750 (61.266)\n",
      "EPOCH: 56 train Results: Prec@1 61.266 Loss: 1.0927\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1371 (1.1371)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2733 (1.2419)\tPrec@1 37.500 (55.780)\n",
      "EPOCH: 56 val Results: Prec@1 55.780 Loss: 1.2419\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [57][0/390]\tTime 0.004 (0.004)\tLoss 1.0124 (1.0124)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [57][78/390]\tTime 0.002 (0.004)\tLoss 1.0503 (1.0407)\tPrec@1 65.625 (63.271)\n",
      "Epoch: [57][156/390]\tTime 0.003 (0.004)\tLoss 1.0563 (1.0520)\tPrec@1 63.281 (62.719)\n",
      "Epoch: [57][234/390]\tTime 0.002 (0.003)\tLoss 1.1362 (1.0666)\tPrec@1 61.719 (62.048)\n",
      "Epoch: [57][312/390]\tTime 0.003 (0.003)\tLoss 1.1462 (1.0780)\tPrec@1 58.594 (61.611)\n",
      "Epoch: [57][390/390]\tTime 0.003 (0.003)\tLoss 1.1255 (1.0915)\tPrec@1 58.750 (61.144)\n",
      "EPOCH: 57 train Results: Prec@1 61.144 Loss: 1.0915\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2126 (1.2126)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3973 (1.2366)\tPrec@1 37.500 (55.580)\n",
      "EPOCH: 57 val Results: Prec@1 55.580 Loss: 1.2366\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [58][0/390]\tTime 0.003 (0.003)\tLoss 1.0216 (1.0216)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [58][78/390]\tTime 0.002 (0.003)\tLoss 1.0640 (1.0286)\tPrec@1 62.500 (64.062)\n",
      "Epoch: [58][156/390]\tTime 0.003 (0.003)\tLoss 1.0973 (1.0501)\tPrec@1 60.938 (63.301)\n",
      "Epoch: [58][234/390]\tTime 0.003 (0.003)\tLoss 1.0911 (1.0669)\tPrec@1 60.156 (62.583)\n",
      "Epoch: [58][312/390]\tTime 0.007 (0.003)\tLoss 1.0146 (1.0797)\tPrec@1 64.062 (62.043)\n",
      "Epoch: [58][390/390]\tTime 0.001 (0.003)\tLoss 1.1772 (1.0905)\tPrec@1 60.000 (61.548)\n",
      "EPOCH: 58 train Results: Prec@1 61.548 Loss: 1.0905\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1740 (1.1740)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2721 (1.2270)\tPrec@1 37.500 (56.060)\n",
      "EPOCH: 58 val Results: Prec@1 56.060 Loss: 1.2270\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [59][0/390]\tTime 0.002 (0.002)\tLoss 1.0426 (1.0426)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [59][78/390]\tTime 0.002 (0.003)\tLoss 1.0337 (1.0279)\tPrec@1 61.719 (63.776)\n",
      "Epoch: [59][156/390]\tTime 0.003 (0.003)\tLoss 1.1640 (1.0439)\tPrec@1 60.938 (63.182)\n",
      "Epoch: [59][234/390]\tTime 0.002 (0.003)\tLoss 1.0846 (1.0647)\tPrec@1 57.812 (62.284)\n",
      "Epoch: [59][312/390]\tTime 0.002 (0.003)\tLoss 1.0547 (1.0767)\tPrec@1 64.844 (61.916)\n",
      "Epoch: [59][390/390]\tTime 0.001 (0.003)\tLoss 1.0782 (1.0875)\tPrec@1 63.750 (61.524)\n",
      "EPOCH: 59 train Results: Prec@1 61.524 Loss: 1.0875\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1489 (1.1489)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.2032 (1.2305)\tPrec@1 37.500 (56.000)\n",
      "EPOCH: 59 val Results: Prec@1 56.000 Loss: 1.2305\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [60][0/390]\tTime 0.006 (0.006)\tLoss 0.8979 (0.8979)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [60][78/390]\tTime 0.004 (0.003)\tLoss 1.0319 (1.0355)\tPrec@1 58.594 (63.716)\n",
      "Epoch: [60][156/390]\tTime 0.003 (0.003)\tLoss 0.9109 (1.0547)\tPrec@1 73.438 (63.037)\n",
      "Epoch: [60][234/390]\tTime 0.002 (0.003)\tLoss 1.1139 (1.0715)\tPrec@1 60.156 (62.131)\n",
      "Epoch: [60][312/390]\tTime 0.003 (0.003)\tLoss 1.0180 (1.0827)\tPrec@1 62.500 (61.562)\n",
      "Epoch: [60][390/390]\tTime 0.001 (0.003)\tLoss 1.2097 (1.0896)\tPrec@1 53.750 (61.302)\n",
      "EPOCH: 60 train Results: Prec@1 61.302 Loss: 1.0896\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.0965 (1.0965)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0512 (1.2450)\tPrec@1 56.250 (55.790)\n",
      "EPOCH: 60 val Results: Prec@1 55.790 Loss: 1.2450\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [61][0/390]\tTime 0.004 (0.004)\tLoss 0.9360 (0.9360)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [61][78/390]\tTime 0.002 (0.003)\tLoss 1.0558 (1.0281)\tPrec@1 65.625 (64.033)\n",
      "Epoch: [61][156/390]\tTime 0.004 (0.004)\tLoss 1.1664 (1.0476)\tPrec@1 59.375 (63.077)\n",
      "Epoch: [61][234/390]\tTime 0.003 (0.003)\tLoss 1.0781 (1.0646)\tPrec@1 59.375 (62.231)\n",
      "Epoch: [61][312/390]\tTime 0.003 (0.003)\tLoss 1.1760 (1.0777)\tPrec@1 53.125 (61.699)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.003)\tLoss 1.1206 (1.0881)\tPrec@1 63.750 (61.388)\n",
      "EPOCH: 61 train Results: Prec@1 61.388 Loss: 1.0881\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1447 (1.1447)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3405 (1.2303)\tPrec@1 50.000 (56.270)\n",
      "EPOCH: 61 val Results: Prec@1 56.270 Loss: 1.2303\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [62][0/390]\tTime 0.003 (0.003)\tLoss 1.0378 (1.0378)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.0573 (1.0222)\tPrec@1 60.156 (64.349)\n",
      "Epoch: [62][156/390]\tTime 0.002 (0.004)\tLoss 0.9991 (1.0456)\tPrec@1 67.188 (63.207)\n",
      "Epoch: [62][234/390]\tTime 0.002 (0.004)\tLoss 1.1409 (1.0620)\tPrec@1 63.281 (62.613)\n",
      "Epoch: [62][312/390]\tTime 0.004 (0.004)\tLoss 1.1850 (1.0726)\tPrec@1 59.375 (62.163)\n",
      "Epoch: [62][390/390]\tTime 0.002 (0.004)\tLoss 0.9192 (1.0841)\tPrec@1 70.000 (61.710)\n",
      "EPOCH: 62 train Results: Prec@1 61.710 Loss: 1.0841\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1730 (1.1730)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2816 (1.2330)\tPrec@1 43.750 (56.100)\n",
      "EPOCH: 62 val Results: Prec@1 56.100 Loss: 1.2330\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [63][0/390]\tTime 0.003 (0.003)\tLoss 1.0487 (1.0487)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [63][78/390]\tTime 0.004 (0.003)\tLoss 1.1332 (1.0265)\tPrec@1 60.156 (63.647)\n",
      "Epoch: [63][156/390]\tTime 0.007 (0.003)\tLoss 1.0808 (1.0518)\tPrec@1 61.719 (62.500)\n",
      "Epoch: [63][234/390]\tTime 0.004 (0.003)\tLoss 1.2219 (1.0668)\tPrec@1 57.812 (62.124)\n",
      "Epoch: [63][312/390]\tTime 0.002 (0.003)\tLoss 1.2787 (1.0806)\tPrec@1 56.250 (61.654)\n",
      "Epoch: [63][390/390]\tTime 0.001 (0.003)\tLoss 1.2105 (1.0890)\tPrec@1 58.750 (61.430)\n",
      "EPOCH: 63 train Results: Prec@1 61.430 Loss: 1.0890\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1446 (1.1446)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1445 (1.2326)\tPrec@1 43.750 (56.210)\n",
      "EPOCH: 63 val Results: Prec@1 56.210 Loss: 1.2326\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [64][0/390]\tTime 0.009 (0.009)\tLoss 0.9187 (0.9187)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [64][78/390]\tTime 0.002 (0.003)\tLoss 1.0358 (1.0141)\tPrec@1 59.375 (64.062)\n",
      "Epoch: [64][156/390]\tTime 0.004 (0.003)\tLoss 1.1527 (1.0421)\tPrec@1 57.812 (63.222)\n",
      "Epoch: [64][234/390]\tTime 0.005 (0.003)\tLoss 1.2237 (1.0589)\tPrec@1 57.031 (62.497)\n",
      "Epoch: [64][312/390]\tTime 0.003 (0.003)\tLoss 1.1959 (1.0771)\tPrec@1 60.938 (61.869)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.003)\tLoss 1.3159 (1.0888)\tPrec@1 55.000 (61.464)\n",
      "EPOCH: 64 train Results: Prec@1 61.464 Loss: 1.0888\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1564 (1.1564)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0662 (1.2434)\tPrec@1 62.500 (55.460)\n",
      "EPOCH: 64 val Results: Prec@1 55.460 Loss: 1.2434\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [65][0/390]\tTime 0.004 (0.004)\tLoss 1.0095 (1.0095)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.003)\tLoss 1.0935 (1.0319)\tPrec@1 61.719 (63.410)\n",
      "Epoch: [65][156/390]\tTime 0.009 (0.003)\tLoss 1.0382 (1.0445)\tPrec@1 61.719 (63.042)\n",
      "Epoch: [65][234/390]\tTime 0.005 (0.003)\tLoss 1.1054 (1.0623)\tPrec@1 58.594 (62.264)\n",
      "Epoch: [65][312/390]\tTime 0.002 (0.003)\tLoss 1.0879 (1.0753)\tPrec@1 62.500 (61.779)\n",
      "Epoch: [65][390/390]\tTime 0.003 (0.003)\tLoss 1.1173 (1.0852)\tPrec@1 60.000 (61.308)\n",
      "EPOCH: 65 train Results: Prec@1 61.308 Loss: 1.0852\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0969 (1.0969)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2174 (1.2317)\tPrec@1 56.250 (55.970)\n",
      "EPOCH: 65 val Results: Prec@1 55.970 Loss: 1.2317\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [66][0/390]\tTime 0.003 (0.003)\tLoss 0.9867 (0.9867)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [66][78/390]\tTime 0.004 (0.003)\tLoss 1.1542 (1.0192)\tPrec@1 57.812 (63.736)\n",
      "Epoch: [66][156/390]\tTime 0.002 (0.003)\tLoss 1.0888 (1.0396)\tPrec@1 60.938 (63.371)\n",
      "Epoch: [66][234/390]\tTime 0.002 (0.004)\tLoss 1.1667 (1.0576)\tPrec@1 59.375 (62.580)\n",
      "Epoch: [66][312/390]\tTime 0.004 (0.004)\tLoss 1.2817 (1.0760)\tPrec@1 57.031 (61.918)\n",
      "Epoch: [66][390/390]\tTime 0.002 (0.004)\tLoss 1.0876 (1.0836)\tPrec@1 70.000 (61.740)\n",
      "EPOCH: 66 train Results: Prec@1 61.740 Loss: 1.0836\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1119 (1.1119)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2524 (1.2448)\tPrec@1 50.000 (55.760)\n",
      "EPOCH: 66 val Results: Prec@1 55.760 Loss: 1.2448\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [67][0/390]\tTime 0.002 (0.002)\tLoss 1.1526 (1.1526)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [67][78/390]\tTime 0.006 (0.004)\tLoss 1.0402 (1.0151)\tPrec@1 59.375 (64.211)\n",
      "Epoch: [67][156/390]\tTime 0.002 (0.003)\tLoss 1.2241 (1.0394)\tPrec@1 55.469 (63.067)\n",
      "Epoch: [67][234/390]\tTime 0.002 (0.003)\tLoss 1.1799 (1.0609)\tPrec@1 63.281 (62.267)\n",
      "Epoch: [67][312/390]\tTime 0.002 (0.003)\tLoss 1.0219 (1.0735)\tPrec@1 64.062 (61.729)\n",
      "Epoch: [67][390/390]\tTime 0.007 (0.003)\tLoss 1.1286 (1.0830)\tPrec@1 58.750 (61.428)\n",
      "EPOCH: 67 train Results: Prec@1 61.428 Loss: 1.0830\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1119 (1.1119)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0656 (1.2348)\tPrec@1 62.500 (55.940)\n",
      "EPOCH: 67 val Results: Prec@1 55.940 Loss: 1.2348\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [68][0/390]\tTime 0.002 (0.002)\tLoss 0.9049 (0.9049)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [68][78/390]\tTime 0.006 (0.003)\tLoss 0.9261 (1.0131)\tPrec@1 70.312 (64.379)\n",
      "Epoch: [68][156/390]\tTime 0.002 (0.003)\tLoss 1.2402 (1.0497)\tPrec@1 60.156 (62.948)\n",
      "Epoch: [68][234/390]\tTime 0.008 (0.003)\tLoss 1.1922 (1.0635)\tPrec@1 58.594 (62.330)\n",
      "Epoch: [68][312/390]\tTime 0.004 (0.003)\tLoss 1.2907 (1.0763)\tPrec@1 52.344 (62.046)\n",
      "Epoch: [68][390/390]\tTime 0.002 (0.003)\tLoss 1.3545 (1.0862)\tPrec@1 53.750 (61.738)\n",
      "EPOCH: 68 train Results: Prec@1 61.738 Loss: 1.0862\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0647 (1.0647)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1184 (1.2308)\tPrec@1 56.250 (56.190)\n",
      "EPOCH: 68 val Results: Prec@1 56.190 Loss: 1.2308\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [69][0/390]\tTime 0.004 (0.004)\tLoss 1.0045 (1.0045)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.003)\tLoss 1.0491 (0.9986)\tPrec@1 57.812 (64.636)\n",
      "Epoch: [69][156/390]\tTime 0.003 (0.003)\tLoss 1.0644 (1.0352)\tPrec@1 60.938 (63.436)\n",
      "Epoch: [69][234/390]\tTime 0.003 (0.003)\tLoss 0.9091 (1.0531)\tPrec@1 74.219 (62.779)\n",
      "Epoch: [69][312/390]\tTime 0.009 (0.003)\tLoss 1.1261 (1.0684)\tPrec@1 59.375 (62.088)\n",
      "Epoch: [69][390/390]\tTime 0.001 (0.003)\tLoss 1.1266 (1.0798)\tPrec@1 61.250 (61.666)\n",
      "EPOCH: 69 train Results: Prec@1 61.666 Loss: 1.0798\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1335 (1.1335)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1652 (1.2336)\tPrec@1 37.500 (56.310)\n",
      "EPOCH: 69 val Results: Prec@1 56.310 Loss: 1.2336\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [70][0/390]\tTime 0.002 (0.002)\tLoss 0.9863 (0.9863)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [70][78/390]\tTime 0.002 (0.003)\tLoss 1.1255 (1.0115)\tPrec@1 57.812 (64.616)\n",
      "Epoch: [70][156/390]\tTime 0.003 (0.003)\tLoss 1.1236 (1.0325)\tPrec@1 58.594 (63.654)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.003)\tLoss 1.0181 (1.0530)\tPrec@1 58.594 (62.763)\n",
      "Epoch: [70][312/390]\tTime 0.007 (0.003)\tLoss 1.0408 (1.0749)\tPrec@1 67.188 (61.953)\n",
      "Epoch: [70][390/390]\tTime 0.001 (0.003)\tLoss 1.1138 (1.0814)\tPrec@1 66.250 (61.660)\n",
      "EPOCH: 70 train Results: Prec@1 61.660 Loss: 1.0814\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1433 (1.1433)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0267 (1.2322)\tPrec@1 43.750 (55.700)\n",
      "EPOCH: 70 val Results: Prec@1 55.700 Loss: 1.2322\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [71][0/390]\tTime 0.002 (0.002)\tLoss 0.9564 (0.9564)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [71][78/390]\tTime 0.002 (0.003)\tLoss 1.0672 (1.0138)\tPrec@1 57.031 (64.686)\n",
      "Epoch: [71][156/390]\tTime 0.002 (0.003)\tLoss 1.0100 (1.0379)\tPrec@1 60.938 (63.595)\n",
      "Epoch: [71][234/390]\tTime 0.004 (0.003)\tLoss 0.9083 (1.0613)\tPrec@1 67.188 (62.610)\n",
      "Epoch: [71][312/390]\tTime 0.007 (0.003)\tLoss 1.0563 (1.0689)\tPrec@1 55.469 (62.225)\n",
      "Epoch: [71][390/390]\tTime 0.002 (0.003)\tLoss 0.9471 (1.0800)\tPrec@1 62.500 (61.792)\n",
      "EPOCH: 71 train Results: Prec@1 61.792 Loss: 1.0800\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1280 (1.1280)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2665 (1.2370)\tPrec@1 31.250 (56.180)\n",
      "EPOCH: 71 val Results: Prec@1 56.180 Loss: 1.2370\n",
      "Best Prec@1: 56.350\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [72][0/390]\tTime 0.005 (0.005)\tLoss 1.0654 (1.0654)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [72][78/390]\tTime 0.006 (0.003)\tLoss 1.1557 (1.0285)\tPrec@1 55.469 (63.865)\n",
      "Epoch: [72][156/390]\tTime 0.005 (0.003)\tLoss 1.0539 (1.0441)\tPrec@1 57.031 (63.202)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.003)\tLoss 1.1334 (1.0610)\tPrec@1 57.812 (62.320)\n",
      "Epoch: [72][312/390]\tTime 0.003 (0.003)\tLoss 1.1536 (1.0718)\tPrec@1 61.719 (61.963)\n",
      "Epoch: [72][390/390]\tTime 0.001 (0.003)\tLoss 0.9524 (1.0820)\tPrec@1 63.750 (61.592)\n",
      "EPOCH: 72 train Results: Prec@1 61.592 Loss: 1.0820\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0525 (1.0525)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3311 (1.2210)\tPrec@1 43.750 (56.460)\n",
      "EPOCH: 72 val Results: Prec@1 56.460 Loss: 1.2210\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [73][0/390]\tTime 0.005 (0.005)\tLoss 0.9201 (0.9201)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.003)\tLoss 0.9306 (1.0251)\tPrec@1 67.969 (64.181)\n",
      "Epoch: [73][156/390]\tTime 0.004 (0.003)\tLoss 1.1798 (1.0354)\tPrec@1 59.375 (63.251)\n",
      "Epoch: [73][234/390]\tTime 0.012 (0.003)\tLoss 1.0079 (1.0500)\tPrec@1 63.281 (62.686)\n",
      "Epoch: [73][312/390]\tTime 0.002 (0.003)\tLoss 0.9693 (1.0657)\tPrec@1 65.625 (62.263)\n",
      "Epoch: [73][390/390]\tTime 0.001 (0.003)\tLoss 1.0785 (1.0763)\tPrec@1 58.750 (61.978)\n",
      "EPOCH: 73 train Results: Prec@1 61.978 Loss: 1.0763\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0673 (1.0673)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3216 (1.2304)\tPrec@1 37.500 (56.100)\n",
      "EPOCH: 73 val Results: Prec@1 56.100 Loss: 1.2304\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [74][0/390]\tTime 0.004 (0.004)\tLoss 1.1425 (1.1425)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [74][78/390]\tTime 0.005 (0.003)\tLoss 1.0721 (1.0175)\tPrec@1 60.938 (64.181)\n",
      "Epoch: [74][156/390]\tTime 0.002 (0.003)\tLoss 1.0361 (1.0417)\tPrec@1 62.500 (63.346)\n",
      "Epoch: [74][234/390]\tTime 0.002 (0.003)\tLoss 1.0657 (1.0567)\tPrec@1 61.719 (62.683)\n",
      "Epoch: [74][312/390]\tTime 0.003 (0.003)\tLoss 1.0789 (1.0666)\tPrec@1 64.062 (62.255)\n",
      "Epoch: [74][390/390]\tTime 0.003 (0.003)\tLoss 1.0676 (1.0783)\tPrec@1 67.500 (61.750)\n",
      "EPOCH: 74 train Results: Prec@1 61.750 Loss: 1.0783\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1527 (1.1527)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4157 (1.2434)\tPrec@1 25.000 (55.940)\n",
      "EPOCH: 74 val Results: Prec@1 55.940 Loss: 1.2434\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [75][0/390]\tTime 0.004 (0.004)\tLoss 1.0387 (1.0387)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [75][78/390]\tTime 0.004 (0.004)\tLoss 1.1215 (1.0026)\tPrec@1 60.938 (64.478)\n",
      "Epoch: [75][156/390]\tTime 0.002 (0.003)\tLoss 0.8985 (1.0386)\tPrec@1 71.094 (63.003)\n",
      "Epoch: [75][234/390]\tTime 0.008 (0.003)\tLoss 1.2172 (1.0539)\tPrec@1 52.344 (62.606)\n",
      "Epoch: [75][312/390]\tTime 0.008 (0.003)\tLoss 1.1745 (1.0665)\tPrec@1 58.594 (62.106)\n",
      "Epoch: [75][390/390]\tTime 0.003 (0.003)\tLoss 1.1175 (1.0791)\tPrec@1 61.250 (61.742)\n",
      "EPOCH: 75 train Results: Prec@1 61.742 Loss: 1.0791\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0860 (1.0860)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5978 (1.2440)\tPrec@1 37.500 (55.600)\n",
      "EPOCH: 75 val Results: Prec@1 55.600 Loss: 1.2440\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [76][0/390]\tTime 0.009 (0.009)\tLoss 1.1240 (1.1240)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.004)\tLoss 1.1117 (1.0241)\tPrec@1 60.156 (64.211)\n",
      "Epoch: [76][156/390]\tTime 0.002 (0.004)\tLoss 0.9429 (1.0407)\tPrec@1 70.312 (63.177)\n",
      "Epoch: [76][234/390]\tTime 0.002 (0.004)\tLoss 0.9829 (1.0566)\tPrec@1 64.062 (62.547)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 0.9658 (1.0658)\tPrec@1 63.281 (62.303)\n",
      "Epoch: [76][390/390]\tTime 0.001 (0.003)\tLoss 0.8827 (1.0770)\tPrec@1 70.000 (61.852)\n",
      "EPOCH: 76 train Results: Prec@1 61.852 Loss: 1.0770\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1511 (1.1511)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3325 (1.2269)\tPrec@1 43.750 (56.130)\n",
      "EPOCH: 76 val Results: Prec@1 56.130 Loss: 1.2269\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [77][0/390]\tTime 0.002 (0.002)\tLoss 0.9932 (0.9932)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [77][78/390]\tTime 0.003 (0.003)\tLoss 1.1392 (1.0224)\tPrec@1 60.156 (64.072)\n",
      "Epoch: [77][156/390]\tTime 0.008 (0.003)\tLoss 1.0964 (1.0391)\tPrec@1 57.812 (63.241)\n",
      "Epoch: [77][234/390]\tTime 0.002 (0.003)\tLoss 0.9342 (1.0533)\tPrec@1 67.188 (62.566)\n",
      "Epoch: [77][312/390]\tTime 0.002 (0.003)\tLoss 1.1298 (1.0652)\tPrec@1 60.938 (62.071)\n",
      "Epoch: [77][390/390]\tTime 0.001 (0.003)\tLoss 1.1164 (1.0772)\tPrec@1 56.250 (61.656)\n",
      "EPOCH: 77 train Results: Prec@1 61.656 Loss: 1.0772\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.0961 (1.0961)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3022 (1.2324)\tPrec@1 31.250 (56.050)\n",
      "EPOCH: 77 val Results: Prec@1 56.050 Loss: 1.2324\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [78][0/390]\tTime 0.002 (0.002)\tLoss 0.9781 (0.9781)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [78][78/390]\tTime 0.003 (0.003)\tLoss 1.0755 (0.9952)\tPrec@1 56.250 (65.002)\n",
      "Epoch: [78][156/390]\tTime 0.003 (0.003)\tLoss 1.0448 (1.0280)\tPrec@1 59.375 (63.560)\n",
      "Epoch: [78][234/390]\tTime 0.004 (0.003)\tLoss 1.0600 (1.0499)\tPrec@1 61.719 (62.799)\n",
      "Epoch: [78][312/390]\tTime 0.002 (0.003)\tLoss 1.0553 (1.0655)\tPrec@1 64.844 (62.061)\n",
      "Epoch: [78][390/390]\tTime 0.002 (0.003)\tLoss 1.0195 (1.0752)\tPrec@1 66.250 (61.658)\n",
      "EPOCH: 78 train Results: Prec@1 61.658 Loss: 1.0752\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0897 (1.0897)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0635 (1.2285)\tPrec@1 43.750 (56.120)\n",
      "EPOCH: 78 val Results: Prec@1 56.120 Loss: 1.2285\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [79][0/390]\tTime 0.006 (0.006)\tLoss 0.8830 (0.8830)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [79][78/390]\tTime 0.003 (0.004)\tLoss 1.0044 (0.9882)\tPrec@1 62.500 (65.121)\n",
      "Epoch: [79][156/390]\tTime 0.004 (0.003)\tLoss 0.9979 (1.0166)\tPrec@1 67.969 (64.137)\n",
      "Epoch: [79][234/390]\tTime 0.002 (0.003)\tLoss 1.1267 (1.0391)\tPrec@1 57.031 (63.201)\n",
      "Epoch: [79][312/390]\tTime 0.003 (0.003)\tLoss 1.1645 (1.0592)\tPrec@1 58.594 (62.480)\n",
      "Epoch: [79][390/390]\tTime 0.002 (0.003)\tLoss 1.0301 (1.0731)\tPrec@1 62.500 (61.838)\n",
      "EPOCH: 79 train Results: Prec@1 61.838 Loss: 1.0731\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1066 (1.1066)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2858 (1.2298)\tPrec@1 37.500 (55.960)\n",
      "EPOCH: 79 val Results: Prec@1 55.960 Loss: 1.2298\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [80][0/390]\tTime 0.003 (0.003)\tLoss 1.0215 (1.0215)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [80][78/390]\tTime 0.004 (0.003)\tLoss 1.0354 (0.9988)\tPrec@1 62.500 (65.427)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.1419 (1.0302)\tPrec@1 57.812 (63.530)\n",
      "Epoch: [80][234/390]\tTime 0.002 (0.003)\tLoss 1.0152 (1.0519)\tPrec@1 63.281 (62.869)\n",
      "Epoch: [80][312/390]\tTime 0.005 (0.003)\tLoss 1.3171 (1.0663)\tPrec@1 57.031 (62.293)\n",
      "Epoch: [80][390/390]\tTime 0.001 (0.003)\tLoss 1.1856 (1.0736)\tPrec@1 53.750 (61.926)\n",
      "EPOCH: 80 train Results: Prec@1 61.926 Loss: 1.0736\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1255 (1.1255)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4424 (1.2456)\tPrec@1 31.250 (55.850)\n",
      "EPOCH: 80 val Results: Prec@1 55.850 Loss: 1.2456\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [81][0/390]\tTime 0.002 (0.002)\tLoss 1.0009 (1.0009)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [81][78/390]\tTime 0.005 (0.003)\tLoss 1.0580 (1.0021)\tPrec@1 62.500 (64.676)\n",
      "Epoch: [81][156/390]\tTime 0.003 (0.003)\tLoss 0.9516 (1.0318)\tPrec@1 66.406 (63.510)\n",
      "Epoch: [81][234/390]\tTime 0.002 (0.003)\tLoss 1.0469 (1.0445)\tPrec@1 64.844 (62.985)\n",
      "Epoch: [81][312/390]\tTime 0.002 (0.003)\tLoss 1.3557 (1.0619)\tPrec@1 50.781 (62.385)\n",
      "Epoch: [81][390/390]\tTime 0.001 (0.003)\tLoss 1.0627 (1.0731)\tPrec@1 58.750 (61.978)\n",
      "EPOCH: 81 train Results: Prec@1 61.978 Loss: 1.0731\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1539 (1.1539)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1550 (1.2378)\tPrec@1 37.500 (56.060)\n",
      "EPOCH: 81 val Results: Prec@1 56.060 Loss: 1.2378\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [82][0/390]\tTime 0.006 (0.006)\tLoss 1.0540 (1.0540)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [82][78/390]\tTime 0.003 (0.003)\tLoss 0.8941 (1.0125)\tPrec@1 71.094 (64.181)\n",
      "Epoch: [82][156/390]\tTime 0.002 (0.003)\tLoss 1.1340 (1.0284)\tPrec@1 57.031 (63.356)\n",
      "Epoch: [82][234/390]\tTime 0.002 (0.003)\tLoss 1.1063 (1.0463)\tPrec@1 62.500 (62.709)\n",
      "Epoch: [82][312/390]\tTime 0.002 (0.003)\tLoss 1.0560 (1.0541)\tPrec@1 64.062 (62.465)\n",
      "Epoch: [82][390/390]\tTime 0.003 (0.003)\tLoss 1.0658 (1.0724)\tPrec@1 57.500 (61.862)\n",
      "EPOCH: 82 train Results: Prec@1 61.862 Loss: 1.0724\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1099 (1.1099)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1784 (1.2226)\tPrec@1 50.000 (56.390)\n",
      "EPOCH: 82 val Results: Prec@1 56.390 Loss: 1.2226\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [83][0/390]\tTime 0.004 (0.004)\tLoss 0.9328 (0.9328)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [83][78/390]\tTime 0.003 (0.003)\tLoss 1.2311 (1.0084)\tPrec@1 53.906 (64.686)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.1721 (1.0378)\tPrec@1 53.906 (63.361)\n",
      "Epoch: [83][234/390]\tTime 0.002 (0.003)\tLoss 0.9850 (1.0538)\tPrec@1 64.844 (62.736)\n",
      "Epoch: [83][312/390]\tTime 0.003 (0.003)\tLoss 1.1960 (1.0686)\tPrec@1 59.375 (62.268)\n",
      "Epoch: [83][390/390]\tTime 0.002 (0.003)\tLoss 1.1405 (1.0752)\tPrec@1 63.750 (61.874)\n",
      "EPOCH: 83 train Results: Prec@1 61.874 Loss: 1.0752\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1003 (1.1003)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1726 (1.2335)\tPrec@1 31.250 (56.020)\n",
      "EPOCH: 83 val Results: Prec@1 56.020 Loss: 1.2335\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [84][0/390]\tTime 0.003 (0.003)\tLoss 0.7884 (0.7884)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [84][78/390]\tTime 0.003 (0.003)\tLoss 0.8443 (1.0069)\tPrec@1 71.094 (64.913)\n",
      "Epoch: [84][156/390]\tTime 0.003 (0.003)\tLoss 1.0472 (1.0295)\tPrec@1 62.500 (63.729)\n",
      "Epoch: [84][234/390]\tTime 0.002 (0.003)\tLoss 1.0556 (1.0526)\tPrec@1 61.719 (62.856)\n",
      "Epoch: [84][312/390]\tTime 0.003 (0.003)\tLoss 1.0063 (1.0617)\tPrec@1 65.625 (62.453)\n",
      "Epoch: [84][390/390]\tTime 0.001 (0.003)\tLoss 1.3239 (1.0705)\tPrec@1 55.000 (62.150)\n",
      "EPOCH: 84 train Results: Prec@1 62.150 Loss: 1.0705\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0244 (1.0244)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1001 (1.2431)\tPrec@1 56.250 (56.010)\n",
      "EPOCH: 84 val Results: Prec@1 56.010 Loss: 1.2431\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [85][0/390]\tTime 0.002 (0.002)\tLoss 0.8598 (0.8598)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [85][78/390]\tTime 0.002 (0.004)\tLoss 0.9279 (1.0058)\tPrec@1 73.438 (64.943)\n",
      "Epoch: [85][156/390]\tTime 0.010 (0.004)\tLoss 0.9076 (1.0282)\tPrec@1 67.969 (63.913)\n",
      "Epoch: [85][234/390]\tTime 0.002 (0.004)\tLoss 1.2712 (1.0465)\tPrec@1 53.125 (63.165)\n",
      "Epoch: [85][312/390]\tTime 0.002 (0.004)\tLoss 1.0796 (1.0608)\tPrec@1 60.938 (62.617)\n",
      "Epoch: [85][390/390]\tTime 0.004 (0.003)\tLoss 1.1236 (1.0718)\tPrec@1 56.250 (62.006)\n",
      "EPOCH: 85 train Results: Prec@1 62.006 Loss: 1.0718\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0547 (1.0547)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5419 (1.2384)\tPrec@1 37.500 (55.470)\n",
      "EPOCH: 85 val Results: Prec@1 55.470 Loss: 1.2384\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [86][0/390]\tTime 0.002 (0.002)\tLoss 1.0032 (1.0032)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [86][78/390]\tTime 0.005 (0.003)\tLoss 1.0115 (1.0246)\tPrec@1 62.500 (63.657)\n",
      "Epoch: [86][156/390]\tTime 0.005 (0.003)\tLoss 0.9998 (1.0331)\tPrec@1 63.281 (63.326)\n",
      "Epoch: [86][234/390]\tTime 0.002 (0.003)\tLoss 1.1173 (1.0461)\tPrec@1 59.375 (63.032)\n",
      "Epoch: [86][312/390]\tTime 0.003 (0.003)\tLoss 1.2314 (1.0592)\tPrec@1 58.594 (62.512)\n",
      "Epoch: [86][390/390]\tTime 0.002 (0.003)\tLoss 1.1657 (1.0726)\tPrec@1 60.000 (61.986)\n",
      "EPOCH: 86 train Results: Prec@1 61.986 Loss: 1.0726\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1119 (1.1119)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2099 (1.2300)\tPrec@1 37.500 (55.850)\n",
      "EPOCH: 86 val Results: Prec@1 55.850 Loss: 1.2300\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [87][0/390]\tTime 0.002 (0.002)\tLoss 0.9718 (0.9718)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [87][78/390]\tTime 0.002 (0.003)\tLoss 0.9153 (0.9857)\tPrec@1 70.312 (65.239)\n",
      "Epoch: [87][156/390]\tTime 0.002 (0.003)\tLoss 1.1287 (1.0249)\tPrec@1 57.031 (63.669)\n",
      "Epoch: [87][234/390]\tTime 0.002 (0.003)\tLoss 1.0888 (1.0422)\tPrec@1 64.844 (62.936)\n",
      "Epoch: [87][312/390]\tTime 0.004 (0.003)\tLoss 1.2561 (1.0581)\tPrec@1 58.594 (62.542)\n",
      "Epoch: [87][390/390]\tTime 0.002 (0.003)\tLoss 1.2518 (1.0674)\tPrec@1 51.250 (62.178)\n",
      "EPOCH: 87 train Results: Prec@1 62.178 Loss: 1.0674\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0561 (1.0561)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0519 (1.2359)\tPrec@1 68.750 (56.340)\n",
      "EPOCH: 87 val Results: Prec@1 56.340 Loss: 1.2359\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [88][0/390]\tTime 0.002 (0.002)\tLoss 0.9976 (0.9976)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [88][78/390]\tTime 0.002 (0.003)\tLoss 0.8443 (0.9864)\tPrec@1 70.312 (65.783)\n",
      "Epoch: [88][156/390]\tTime 0.003 (0.003)\tLoss 1.1437 (1.0294)\tPrec@1 55.469 (63.844)\n",
      "Epoch: [88][234/390]\tTime 0.007 (0.003)\tLoss 1.0932 (1.0490)\tPrec@1 56.250 (62.959)\n",
      "Epoch: [88][312/390]\tTime 0.002 (0.003)\tLoss 1.0661 (1.0636)\tPrec@1 59.375 (62.450)\n",
      "Epoch: [88][390/390]\tTime 0.001 (0.003)\tLoss 1.3547 (1.0736)\tPrec@1 53.750 (62.052)\n",
      "EPOCH: 88 train Results: Prec@1 62.052 Loss: 1.0736\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1688 (1.1688)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0231 (1.2443)\tPrec@1 56.250 (56.230)\n",
      "EPOCH: 88 val Results: Prec@1 56.230 Loss: 1.2443\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [89][0/390]\tTime 0.004 (0.004)\tLoss 0.9632 (0.9632)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.003)\tLoss 1.0243 (1.0078)\tPrec@1 70.312 (64.458)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1083 (1.0242)\tPrec@1 60.938 (63.819)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 1.1079 (1.0459)\tPrec@1 62.500 (63.112)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.1021 (1.0631)\tPrec@1 63.281 (62.413)\n",
      "Epoch: [89][390/390]\tTime 0.003 (0.003)\tLoss 1.1967 (1.0739)\tPrec@1 57.500 (61.998)\n",
      "EPOCH: 89 train Results: Prec@1 61.998 Loss: 1.0739\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1094 (1.1094)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4829 (1.2416)\tPrec@1 50.000 (56.440)\n",
      "EPOCH: 89 val Results: Prec@1 56.440 Loss: 1.2416\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [90][0/390]\tTime 0.002 (0.002)\tLoss 1.0691 (1.0691)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [90][78/390]\tTime 0.002 (0.003)\tLoss 0.9765 (0.9965)\tPrec@1 64.062 (64.597)\n",
      "Epoch: [90][156/390]\tTime 0.004 (0.003)\tLoss 1.1726 (1.0235)\tPrec@1 54.688 (63.849)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.0814 (1.0453)\tPrec@1 61.719 (62.879)\n",
      "Epoch: [90][312/390]\tTime 0.009 (0.003)\tLoss 1.1460 (1.0551)\tPrec@1 59.375 (62.507)\n",
      "Epoch: [90][390/390]\tTime 0.003 (0.003)\tLoss 0.9204 (1.0689)\tPrec@1 67.500 (62.004)\n",
      "EPOCH: 90 train Results: Prec@1 62.004 Loss: 1.0689\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0234 (1.0234)\tPrec@1 71.094 (71.094)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5110 (1.2436)\tPrec@1 43.750 (55.430)\n",
      "EPOCH: 90 val Results: Prec@1 55.430 Loss: 1.2436\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [91][0/390]\tTime 0.003 (0.003)\tLoss 1.1458 (1.1458)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [91][78/390]\tTime 0.002 (0.003)\tLoss 0.9902 (1.0040)\tPrec@1 65.625 (64.992)\n",
      "Epoch: [91][156/390]\tTime 0.002 (0.003)\tLoss 1.1741 (1.0257)\tPrec@1 61.719 (63.893)\n",
      "Epoch: [91][234/390]\tTime 0.003 (0.003)\tLoss 1.1067 (1.0402)\tPrec@1 57.812 (63.285)\n",
      "Epoch: [91][312/390]\tTime 0.004 (0.003)\tLoss 1.1104 (1.0579)\tPrec@1 60.938 (62.627)\n",
      "Epoch: [91][390/390]\tTime 0.002 (0.003)\tLoss 1.0359 (1.0701)\tPrec@1 58.750 (62.098)\n",
      "EPOCH: 91 train Results: Prec@1 62.098 Loss: 1.0701\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1244 (1.1244)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2693 (1.2356)\tPrec@1 50.000 (55.930)\n",
      "EPOCH: 91 val Results: Prec@1 55.930 Loss: 1.2356\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [92][0/390]\tTime 0.002 (0.002)\tLoss 1.0518 (1.0518)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.003)\tLoss 1.0130 (1.0031)\tPrec@1 64.062 (64.201)\n",
      "Epoch: [92][156/390]\tTime 0.003 (0.003)\tLoss 1.0565 (1.0328)\tPrec@1 65.625 (63.361)\n",
      "Epoch: [92][234/390]\tTime 0.008 (0.003)\tLoss 1.0779 (1.0465)\tPrec@1 61.719 (62.723)\n",
      "Epoch: [92][312/390]\tTime 0.004 (0.003)\tLoss 1.1319 (1.0616)\tPrec@1 56.250 (62.081)\n",
      "Epoch: [92][390/390]\tTime 0.002 (0.003)\tLoss 1.0879 (1.0732)\tPrec@1 58.750 (61.662)\n",
      "EPOCH: 92 train Results: Prec@1 61.662 Loss: 1.0732\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0657 (1.0657)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3797 (1.2356)\tPrec@1 50.000 (56.170)\n",
      "EPOCH: 92 val Results: Prec@1 56.170 Loss: 1.2356\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 0.9675 (0.9675)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.003)\tLoss 0.9313 (0.9843)\tPrec@1 66.406 (65.131)\n",
      "Epoch: [93][156/390]\tTime 0.004 (0.003)\tLoss 1.0777 (1.0152)\tPrec@1 60.938 (64.167)\n",
      "Epoch: [93][234/390]\tTime 0.002 (0.003)\tLoss 1.1613 (1.0360)\tPrec@1 60.156 (63.308)\n",
      "Epoch: [93][312/390]\tTime 0.003 (0.003)\tLoss 1.2600 (1.0530)\tPrec@1 55.469 (62.527)\n",
      "Epoch: [93][390/390]\tTime 0.027 (0.004)\tLoss 0.9846 (1.0665)\tPrec@1 62.500 (62.060)\n",
      "EPOCH: 93 train Results: Prec@1 62.060 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1010 (1.1010)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5213 (1.2422)\tPrec@1 37.500 (56.070)\n",
      "EPOCH: 93 val Results: Prec@1 56.070 Loss: 1.2422\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [94][0/390]\tTime 0.002 (0.002)\tLoss 0.9380 (0.9380)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [94][78/390]\tTime 0.004 (0.003)\tLoss 0.9810 (0.9945)\tPrec@1 65.625 (65.042)\n",
      "Epoch: [94][156/390]\tTime 0.005 (0.003)\tLoss 1.0051 (1.0205)\tPrec@1 64.844 (63.873)\n",
      "Epoch: [94][234/390]\tTime 0.006 (0.003)\tLoss 0.9233 (1.0451)\tPrec@1 65.625 (62.972)\n",
      "Epoch: [94][312/390]\tTime 0.006 (0.003)\tLoss 1.1802 (1.0588)\tPrec@1 56.250 (62.410)\n",
      "Epoch: [94][390/390]\tTime 0.005 (0.003)\tLoss 1.4465 (1.0726)\tPrec@1 56.250 (61.934)\n",
      "EPOCH: 94 train Results: Prec@1 61.934 Loss: 1.0726\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1701 (1.1701)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4250 (1.2380)\tPrec@1 43.750 (56.030)\n",
      "EPOCH: 94 val Results: Prec@1 56.030 Loss: 1.2380\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [95][0/390]\tTime 0.004 (0.004)\tLoss 1.0202 (1.0202)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [95][78/390]\tTime 0.002 (0.003)\tLoss 1.0994 (1.0004)\tPrec@1 64.844 (64.962)\n",
      "Epoch: [95][156/390]\tTime 0.002 (0.003)\tLoss 0.9846 (1.0191)\tPrec@1 64.062 (64.157)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.003)\tLoss 1.1684 (1.0396)\tPrec@1 58.594 (63.198)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.003)\tLoss 1.0937 (1.0552)\tPrec@1 54.688 (62.587)\n",
      "Epoch: [95][390/390]\tTime 0.002 (0.003)\tLoss 1.1727 (1.0690)\tPrec@1 62.500 (62.140)\n",
      "EPOCH: 95 train Results: Prec@1 62.140 Loss: 1.0690\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.0476 (1.0476)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3418 (1.2477)\tPrec@1 43.750 (55.890)\n",
      "EPOCH: 95 val Results: Prec@1 55.890 Loss: 1.2477\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [96][0/390]\tTime 0.002 (0.002)\tLoss 1.0343 (1.0343)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.003)\tLoss 1.1241 (0.9985)\tPrec@1 55.469 (64.656)\n",
      "Epoch: [96][156/390]\tTime 0.002 (0.003)\tLoss 0.9821 (1.0231)\tPrec@1 69.531 (63.659)\n",
      "Epoch: [96][234/390]\tTime 0.002 (0.003)\tLoss 1.0412 (1.0462)\tPrec@1 67.188 (62.889)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 1.0121 (1.0551)\tPrec@1 65.625 (62.478)\n",
      "Epoch: [96][390/390]\tTime 0.002 (0.003)\tLoss 1.1976 (1.0690)\tPrec@1 58.750 (61.964)\n",
      "EPOCH: 96 train Results: Prec@1 61.964 Loss: 1.0690\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1595 (1.1595)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9926 (1.2406)\tPrec@1 62.500 (55.740)\n",
      "EPOCH: 96 val Results: Prec@1 55.740 Loss: 1.2406\n",
      "Best Prec@1: 56.460\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [97][0/390]\tTime 0.002 (0.002)\tLoss 0.9961 (0.9961)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [97][78/390]\tTime 0.004 (0.003)\tLoss 0.9661 (0.9961)\tPrec@1 64.062 (64.804)\n",
      "Epoch: [97][156/390]\tTime 0.002 (0.003)\tLoss 1.1600 (1.0231)\tPrec@1 58.594 (63.809)\n",
      "Epoch: [97][234/390]\tTime 0.008 (0.003)\tLoss 1.2710 (1.0378)\tPrec@1 57.031 (63.238)\n",
      "Epoch: [97][312/390]\tTime 0.002 (0.003)\tLoss 1.1476 (1.0522)\tPrec@1 64.844 (62.640)\n",
      "Epoch: [97][390/390]\tTime 0.002 (0.003)\tLoss 0.9369 (1.0658)\tPrec@1 72.500 (62.064)\n",
      "EPOCH: 97 train Results: Prec@1 62.064 Loss: 1.0658\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0652 (1.0652)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.2427 (1.2365)\tPrec@1 43.750 (56.650)\n",
      "EPOCH: 97 val Results: Prec@1 56.650 Loss: 1.2365\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [98][0/390]\tTime 0.004 (0.004)\tLoss 1.0234 (1.0234)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [98][78/390]\tTime 0.003 (0.003)\tLoss 0.9024 (0.9933)\tPrec@1 67.969 (65.032)\n",
      "Epoch: [98][156/390]\tTime 0.002 (0.003)\tLoss 1.2301 (1.0199)\tPrec@1 60.156 (64.152)\n",
      "Epoch: [98][234/390]\tTime 0.002 (0.003)\tLoss 0.9610 (1.0364)\tPrec@1 64.844 (63.368)\n",
      "Epoch: [98][312/390]\tTime 0.002 (0.003)\tLoss 1.1810 (1.0526)\tPrec@1 55.469 (62.712)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.003)\tLoss 1.0507 (1.0643)\tPrec@1 66.250 (62.358)\n",
      "EPOCH: 98 train Results: Prec@1 62.358 Loss: 1.0643\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1457 (1.1457)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2746 (1.2463)\tPrec@1 50.000 (55.600)\n",
      "EPOCH: 98 val Results: Prec@1 55.600 Loss: 1.2463\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [99][0/390]\tTime 0.007 (0.007)\tLoss 1.0034 (1.0034)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.003)\tLoss 0.8944 (1.0087)\tPrec@1 71.875 (64.557)\n",
      "Epoch: [99][156/390]\tTime 0.003 (0.003)\tLoss 1.0947 (1.0343)\tPrec@1 62.500 (63.605)\n",
      "Epoch: [99][234/390]\tTime 0.002 (0.003)\tLoss 1.3753 (1.0452)\tPrec@1 50.781 (63.029)\n",
      "Epoch: [99][312/390]\tTime 0.006 (0.003)\tLoss 1.0716 (1.0574)\tPrec@1 63.281 (62.607)\n",
      "Epoch: [99][390/390]\tTime 0.001 (0.003)\tLoss 0.9349 (1.0665)\tPrec@1 63.750 (62.212)\n",
      "EPOCH: 99 train Results: Prec@1 62.212 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1006 (1.1006)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5218 (1.2456)\tPrec@1 37.500 (55.680)\n",
      "EPOCH: 99 val Results: Prec@1 55.680 Loss: 1.2456\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [100][0/390]\tTime 0.002 (0.002)\tLoss 0.9029 (0.9029)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [100][78/390]\tTime 0.008 (0.003)\tLoss 1.1736 (1.0041)\tPrec@1 60.938 (64.399)\n",
      "Epoch: [100][156/390]\tTime 0.002 (0.003)\tLoss 0.9884 (1.0244)\tPrec@1 64.062 (63.659)\n",
      "Epoch: [100][234/390]\tTime 0.005 (0.003)\tLoss 1.1698 (1.0431)\tPrec@1 54.688 (62.939)\n",
      "Epoch: [100][312/390]\tTime 0.003 (0.003)\tLoss 1.0912 (1.0602)\tPrec@1 60.938 (62.343)\n",
      "Epoch: [100][390/390]\tTime 0.001 (0.003)\tLoss 1.2717 (1.0698)\tPrec@1 56.250 (62.114)\n",
      "EPOCH: 100 train Results: Prec@1 62.114 Loss: 1.0698\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1236 (1.1236)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4116 (1.2382)\tPrec@1 25.000 (55.930)\n",
      "EPOCH: 100 val Results: Prec@1 55.930 Loss: 1.2382\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [101][0/390]\tTime 0.002 (0.002)\tLoss 0.9714 (0.9714)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.003)\tLoss 1.0562 (1.0028)\tPrec@1 62.500 (64.725)\n",
      "Epoch: [101][156/390]\tTime 0.002 (0.003)\tLoss 1.1385 (1.0264)\tPrec@1 59.375 (63.754)\n",
      "Epoch: [101][234/390]\tTime 0.003 (0.003)\tLoss 1.0788 (1.0432)\tPrec@1 60.156 (62.952)\n",
      "Epoch: [101][312/390]\tTime 0.002 (0.003)\tLoss 1.0254 (1.0504)\tPrec@1 63.281 (62.652)\n",
      "Epoch: [101][390/390]\tTime 0.005 (0.003)\tLoss 1.2320 (1.0636)\tPrec@1 56.250 (62.218)\n",
      "EPOCH: 101 train Results: Prec@1 62.218 Loss: 1.0636\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1440 (1.1440)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2290 (1.2465)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 101 val Results: Prec@1 55.300 Loss: 1.2465\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [102][0/390]\tTime 0.002 (0.002)\tLoss 0.9416 (0.9416)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [102][78/390]\tTime 0.002 (0.003)\tLoss 1.0704 (0.9804)\tPrec@1 60.938 (65.477)\n",
      "Epoch: [102][156/390]\tTime 0.002 (0.003)\tLoss 1.0360 (1.0177)\tPrec@1 63.281 (64.058)\n",
      "Epoch: [102][234/390]\tTime 0.007 (0.003)\tLoss 1.1863 (1.0409)\tPrec@1 59.375 (63.085)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.003)\tLoss 1.0747 (1.0545)\tPrec@1 67.188 (62.625)\n",
      "Epoch: [102][390/390]\tTime 0.003 (0.003)\tLoss 1.0379 (1.0645)\tPrec@1 67.500 (62.230)\n",
      "EPOCH: 102 train Results: Prec@1 62.230 Loss: 1.0645\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1221 (1.1221)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3839 (1.2505)\tPrec@1 56.250 (55.650)\n",
      "EPOCH: 102 val Results: Prec@1 55.650 Loss: 1.2505\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [103][0/390]\tTime 0.002 (0.002)\tLoss 0.9924 (0.9924)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [103][78/390]\tTime 0.002 (0.003)\tLoss 1.0441 (1.0105)\tPrec@1 66.406 (63.973)\n",
      "Epoch: [103][156/390]\tTime 0.003 (0.003)\tLoss 1.0423 (1.0252)\tPrec@1 60.156 (63.500)\n",
      "Epoch: [103][234/390]\tTime 0.003 (0.003)\tLoss 1.1826 (1.0419)\tPrec@1 58.594 (62.896)\n",
      "Epoch: [103][312/390]\tTime 0.002 (0.003)\tLoss 1.1998 (1.0533)\tPrec@1 57.812 (62.560)\n",
      "Epoch: [103][390/390]\tTime 0.003 (0.003)\tLoss 1.2175 (1.0629)\tPrec@1 65.000 (62.258)\n",
      "EPOCH: 103 train Results: Prec@1 62.258 Loss: 1.0629\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1297 (1.1297)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3953 (1.2553)\tPrec@1 56.250 (55.360)\n",
      "EPOCH: 103 val Results: Prec@1 55.360 Loss: 1.2553\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [104][0/390]\tTime 0.003 (0.003)\tLoss 0.9585 (0.9585)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 0.9698 (0.9917)\tPrec@1 66.406 (64.893)\n",
      "Epoch: [104][156/390]\tTime 0.005 (0.003)\tLoss 1.0079 (1.0196)\tPrec@1 64.062 (63.694)\n",
      "Epoch: [104][234/390]\tTime 0.002 (0.003)\tLoss 0.9436 (1.0365)\tPrec@1 65.625 (63.135)\n",
      "Epoch: [104][312/390]\tTime 0.002 (0.003)\tLoss 0.9249 (1.0542)\tPrec@1 70.312 (62.485)\n",
      "Epoch: [104][390/390]\tTime 0.002 (0.003)\tLoss 1.0010 (1.0634)\tPrec@1 63.750 (62.182)\n",
      "EPOCH: 104 train Results: Prec@1 62.182 Loss: 1.0634\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1226 (1.1226)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2282 (1.2421)\tPrec@1 43.750 (55.670)\n",
      "EPOCH: 104 val Results: Prec@1 55.670 Loss: 1.2421\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [105][0/390]\tTime 0.002 (0.002)\tLoss 0.9050 (0.9050)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [105][78/390]\tTime 0.004 (0.003)\tLoss 1.0170 (0.9970)\tPrec@1 59.375 (64.923)\n",
      "Epoch: [105][156/390]\tTime 0.003 (0.003)\tLoss 1.1799 (1.0192)\tPrec@1 57.031 (64.092)\n",
      "Epoch: [105][234/390]\tTime 0.003 (0.003)\tLoss 1.2517 (1.0339)\tPrec@1 57.812 (63.517)\n",
      "Epoch: [105][312/390]\tTime 0.002 (0.003)\tLoss 1.1661 (1.0468)\tPrec@1 62.500 (62.999)\n",
      "Epoch: [105][390/390]\tTime 0.003 (0.003)\tLoss 1.0781 (1.0616)\tPrec@1 62.500 (62.504)\n",
      "EPOCH: 105 train Results: Prec@1 62.504 Loss: 1.0616\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1137 (1.1137)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4205 (1.2446)\tPrec@1 43.750 (55.910)\n",
      "EPOCH: 105 val Results: Prec@1 55.910 Loss: 1.2446\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [106][0/390]\tTime 0.003 (0.003)\tLoss 0.9277 (0.9277)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [106][78/390]\tTime 0.002 (0.003)\tLoss 0.8805 (0.9828)\tPrec@1 67.188 (65.526)\n",
      "Epoch: [106][156/390]\tTime 0.002 (0.003)\tLoss 1.1310 (1.0149)\tPrec@1 64.062 (64.142)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.003)\tLoss 1.0324 (1.0370)\tPrec@1 63.281 (63.145)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.003)\tLoss 1.0201 (1.0533)\tPrec@1 65.625 (62.510)\n",
      "Epoch: [106][390/390]\tTime 0.002 (0.003)\tLoss 1.0652 (1.0634)\tPrec@1 71.250 (62.118)\n",
      "EPOCH: 106 train Results: Prec@1 62.118 Loss: 1.0634\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0977 (1.0977)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3285 (1.2433)\tPrec@1 56.250 (55.910)\n",
      "EPOCH: 106 val Results: Prec@1 55.910 Loss: 1.2433\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [107][0/390]\tTime 0.003 (0.003)\tLoss 0.8380 (0.8380)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [107][78/390]\tTime 0.002 (0.003)\tLoss 1.0704 (0.9894)\tPrec@1 64.844 (65.042)\n",
      "Epoch: [107][156/390]\tTime 0.002 (0.004)\tLoss 1.2216 (1.0141)\tPrec@1 57.031 (64.072)\n",
      "Epoch: [107][234/390]\tTime 0.063 (0.004)\tLoss 1.1562 (1.0305)\tPrec@1 55.469 (63.461)\n",
      "Epoch: [107][312/390]\tTime 0.002 (0.003)\tLoss 1.1094 (1.0509)\tPrec@1 62.500 (62.742)\n",
      "Epoch: [107][390/390]\tTime 0.003 (0.004)\tLoss 0.9462 (1.0665)\tPrec@1 63.750 (62.120)\n",
      "EPOCH: 107 train Results: Prec@1 62.120 Loss: 1.0665\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1234 (1.1234)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4880 (1.2456)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 107 val Results: Prec@1 55.370 Loss: 1.2456\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [108][0/390]\tTime 0.002 (0.002)\tLoss 1.0178 (1.0178)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [108][78/390]\tTime 0.003 (0.004)\tLoss 1.1324 (0.9808)\tPrec@1 61.719 (65.051)\n",
      "Epoch: [108][156/390]\tTime 0.002 (0.004)\tLoss 1.0524 (1.0135)\tPrec@1 64.844 (64.162)\n",
      "Epoch: [108][234/390]\tTime 0.002 (0.004)\tLoss 1.1259 (1.0340)\tPrec@1 60.156 (63.295)\n",
      "Epoch: [108][312/390]\tTime 0.005 (0.004)\tLoss 1.0437 (1.0469)\tPrec@1 63.281 (62.672)\n",
      "Epoch: [108][390/390]\tTime 0.002 (0.004)\tLoss 1.1297 (1.0599)\tPrec@1 58.750 (62.226)\n",
      "EPOCH: 108 train Results: Prec@1 62.226 Loss: 1.0599\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1188 (1.1188)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4841 (1.2510)\tPrec@1 43.750 (54.750)\n",
      "EPOCH: 108 val Results: Prec@1 54.750 Loss: 1.2510\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [109][0/390]\tTime 0.003 (0.003)\tLoss 1.0805 (1.0805)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [109][78/390]\tTime 0.004 (0.003)\tLoss 1.0636 (0.9981)\tPrec@1 64.844 (64.775)\n",
      "Epoch: [109][156/390]\tTime 0.003 (0.003)\tLoss 1.1074 (1.0269)\tPrec@1 60.156 (63.620)\n",
      "Epoch: [109][234/390]\tTime 0.004 (0.003)\tLoss 0.9154 (1.0415)\tPrec@1 67.188 (62.959)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.003)\tLoss 1.1608 (1.0531)\tPrec@1 51.562 (62.512)\n",
      "Epoch: [109][390/390]\tTime 0.002 (0.003)\tLoss 1.1821 (1.0607)\tPrec@1 58.750 (62.216)\n",
      "EPOCH: 109 train Results: Prec@1 62.216 Loss: 1.0607\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0958 (1.0958)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2776 (1.2323)\tPrec@1 68.750 (56.310)\n",
      "EPOCH: 109 val Results: Prec@1 56.310 Loss: 1.2323\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [110][0/390]\tTime 0.005 (0.005)\tLoss 1.0152 (1.0152)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 1.0749 (0.9839)\tPrec@1 61.719 (65.042)\n",
      "Epoch: [110][156/390]\tTime 0.002 (0.003)\tLoss 0.9514 (1.0140)\tPrec@1 71.875 (64.023)\n",
      "Epoch: [110][234/390]\tTime 0.004 (0.003)\tLoss 1.1555 (1.0283)\tPrec@1 60.156 (63.614)\n",
      "Epoch: [110][312/390]\tTime 0.002 (0.003)\tLoss 1.0881 (1.0489)\tPrec@1 67.188 (62.844)\n",
      "Epoch: [110][390/390]\tTime 0.001 (0.003)\tLoss 1.2480 (1.0567)\tPrec@1 56.250 (62.424)\n",
      "EPOCH: 110 train Results: Prec@1 62.424 Loss: 1.0567\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0659 (1.0659)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4452 (1.2331)\tPrec@1 43.750 (56.200)\n",
      "EPOCH: 110 val Results: Prec@1 56.200 Loss: 1.2331\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [111][0/390]\tTime 0.002 (0.002)\tLoss 0.9671 (0.9671)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [111][78/390]\tTime 0.005 (0.004)\tLoss 1.0009 (0.9900)\tPrec@1 66.406 (64.389)\n",
      "Epoch: [111][156/390]\tTime 0.055 (0.004)\tLoss 1.0119 (1.0214)\tPrec@1 58.594 (63.241)\n",
      "Epoch: [111][234/390]\tTime 0.003 (0.004)\tLoss 1.2277 (1.0437)\tPrec@1 57.812 (62.776)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.003)\tLoss 1.0830 (1.0562)\tPrec@1 65.625 (62.400)\n",
      "Epoch: [111][390/390]\tTime 0.001 (0.003)\tLoss 1.1222 (1.0662)\tPrec@1 63.750 (62.042)\n",
      "EPOCH: 111 train Results: Prec@1 62.042 Loss: 1.0662\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1375 (1.1375)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2907 (1.2502)\tPrec@1 50.000 (55.630)\n",
      "EPOCH: 111 val Results: Prec@1 55.630 Loss: 1.2502\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [112][0/390]\tTime 0.005 (0.005)\tLoss 0.9740 (0.9740)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.1124 (0.9887)\tPrec@1 60.156 (65.051)\n",
      "Epoch: [112][156/390]\tTime 0.004 (0.003)\tLoss 1.0977 (1.0139)\tPrec@1 64.062 (64.092)\n",
      "Epoch: [112][234/390]\tTime 0.002 (0.003)\tLoss 1.1656 (1.0295)\tPrec@1 57.812 (63.551)\n",
      "Epoch: [112][312/390]\tTime 0.003 (0.003)\tLoss 1.0951 (1.0524)\tPrec@1 62.500 (62.707)\n",
      "Epoch: [112][390/390]\tTime 0.004 (0.003)\tLoss 1.2321 (1.0601)\tPrec@1 52.500 (62.440)\n",
      "EPOCH: 112 train Results: Prec@1 62.440 Loss: 1.0601\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0884 (1.0884)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1358 (1.2429)\tPrec@1 50.000 (55.860)\n",
      "EPOCH: 112 val Results: Prec@1 55.860 Loss: 1.2429\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 1.0080 (1.0080)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [113][78/390]\tTime 0.002 (0.003)\tLoss 1.0585 (0.9902)\tPrec@1 60.156 (65.635)\n",
      "Epoch: [113][156/390]\tTime 0.003 (0.003)\tLoss 0.9940 (1.0150)\tPrec@1 64.844 (64.421)\n",
      "Epoch: [113][234/390]\tTime 0.004 (0.003)\tLoss 0.9539 (1.0334)\tPrec@1 65.625 (63.707)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.003)\tLoss 1.0174 (1.0466)\tPrec@1 65.625 (63.159)\n",
      "Epoch: [113][390/390]\tTime 0.002 (0.003)\tLoss 1.2093 (1.0610)\tPrec@1 60.000 (62.588)\n",
      "EPOCH: 113 train Results: Prec@1 62.588 Loss: 1.0610\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1081 (1.1081)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2380 (1.2330)\tPrec@1 56.250 (56.080)\n",
      "EPOCH: 113 val Results: Prec@1 56.080 Loss: 1.2330\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [114][0/390]\tTime 0.006 (0.006)\tLoss 1.0444 (1.0444)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.003)\tLoss 0.8303 (0.9943)\tPrec@1 72.656 (65.121)\n",
      "Epoch: [114][156/390]\tTime 0.002 (0.003)\tLoss 1.0545 (1.0177)\tPrec@1 62.500 (64.192)\n",
      "Epoch: [114][234/390]\tTime 0.003 (0.003)\tLoss 1.1229 (1.0379)\tPrec@1 60.938 (63.381)\n",
      "Epoch: [114][312/390]\tTime 0.003 (0.003)\tLoss 0.9619 (1.0468)\tPrec@1 68.750 (63.007)\n",
      "Epoch: [114][390/390]\tTime 0.003 (0.003)\tLoss 1.2093 (1.0581)\tPrec@1 60.000 (62.544)\n",
      "EPOCH: 114 train Results: Prec@1 62.544 Loss: 1.0581\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0983 (1.0983)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4331 (1.2490)\tPrec@1 43.750 (55.530)\n",
      "EPOCH: 114 val Results: Prec@1 55.530 Loss: 1.2490\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [115][0/390]\tTime 0.003 (0.003)\tLoss 0.8017 (0.8017)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [115][78/390]\tTime 0.002 (0.003)\tLoss 0.9386 (0.9937)\tPrec@1 71.875 (64.636)\n",
      "Epoch: [115][156/390]\tTime 0.003 (0.003)\tLoss 1.1182 (1.0073)\tPrec@1 59.375 (63.953)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.003)\tLoss 1.1019 (1.0295)\tPrec@1 57.812 (63.398)\n",
      "Epoch: [115][312/390]\tTime 0.004 (0.003)\tLoss 1.1989 (1.0459)\tPrec@1 56.250 (62.782)\n",
      "Epoch: [115][390/390]\tTime 0.003 (0.003)\tLoss 1.3911 (1.0581)\tPrec@1 57.500 (62.348)\n",
      "EPOCH: 115 train Results: Prec@1 62.348 Loss: 1.0581\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1227 (1.1227)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2953 (1.2361)\tPrec@1 50.000 (56.430)\n",
      "EPOCH: 115 val Results: Prec@1 56.430 Loss: 1.2361\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [116][0/390]\tTime 0.003 (0.003)\tLoss 1.0399 (1.0399)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [116][78/390]\tTime 0.002 (0.003)\tLoss 1.0919 (0.9959)\tPrec@1 63.281 (65.002)\n",
      "Epoch: [116][156/390]\tTime 0.002 (0.003)\tLoss 1.1828 (1.0197)\tPrec@1 61.719 (63.883)\n",
      "Epoch: [116][234/390]\tTime 0.007 (0.003)\tLoss 1.0187 (1.0375)\tPrec@1 65.625 (63.142)\n",
      "Epoch: [116][312/390]\tTime 0.003 (0.003)\tLoss 1.2106 (1.0542)\tPrec@1 60.156 (62.488)\n",
      "Epoch: [116][390/390]\tTime 0.003 (0.003)\tLoss 1.2266 (1.0622)\tPrec@1 57.500 (62.160)\n",
      "EPOCH: 116 train Results: Prec@1 62.160 Loss: 1.0622\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0919 (1.0919)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2540 (1.2424)\tPrec@1 50.000 (55.640)\n",
      "EPOCH: 116 val Results: Prec@1 55.640 Loss: 1.2424\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [117][0/390]\tTime 0.003 (0.003)\tLoss 0.9586 (0.9586)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [117][78/390]\tTime 0.002 (0.003)\tLoss 1.1554 (0.9848)\tPrec@1 61.719 (65.467)\n",
      "Epoch: [117][156/390]\tTime 0.002 (0.003)\tLoss 0.9953 (1.0136)\tPrec@1 71.875 (64.043)\n",
      "Epoch: [117][234/390]\tTime 0.002 (0.003)\tLoss 1.1499 (1.0360)\tPrec@1 58.594 (63.108)\n",
      "Epoch: [117][312/390]\tTime 0.002 (0.003)\tLoss 1.0699 (1.0499)\tPrec@1 60.156 (62.615)\n",
      "Epoch: [117][390/390]\tTime 0.011 (0.003)\tLoss 1.0161 (1.0592)\tPrec@1 60.000 (62.312)\n",
      "EPOCH: 117 train Results: Prec@1 62.312 Loss: 1.0592\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0306 (1.0306)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3932 (1.2401)\tPrec@1 50.000 (56.030)\n",
      "EPOCH: 117 val Results: Prec@1 56.030 Loss: 1.2401\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [118][0/390]\tTime 0.005 (0.005)\tLoss 1.0416 (1.0416)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [118][78/390]\tTime 0.002 (0.003)\tLoss 1.0004 (0.9981)\tPrec@1 62.500 (64.864)\n",
      "Epoch: [118][156/390]\tTime 0.006 (0.003)\tLoss 1.0591 (1.0130)\tPrec@1 60.938 (64.336)\n",
      "Epoch: [118][234/390]\tTime 0.004 (0.003)\tLoss 1.0659 (1.0283)\tPrec@1 59.375 (63.607)\n",
      "Epoch: [118][312/390]\tTime 0.002 (0.003)\tLoss 1.0204 (1.0457)\tPrec@1 66.406 (62.777)\n",
      "Epoch: [118][390/390]\tTime 0.002 (0.003)\tLoss 1.0732 (1.0598)\tPrec@1 63.750 (62.294)\n",
      "EPOCH: 118 train Results: Prec@1 62.294 Loss: 1.0598\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0885 (1.0885)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1110 (1.2570)\tPrec@1 50.000 (55.630)\n",
      "EPOCH: 118 val Results: Prec@1 55.630 Loss: 1.2570\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [119][0/390]\tTime 0.005 (0.005)\tLoss 1.0058 (1.0058)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [119][78/390]\tTime 0.002 (0.002)\tLoss 1.0333 (0.9760)\tPrec@1 67.188 (66.199)\n",
      "Epoch: [119][156/390]\tTime 0.003 (0.003)\tLoss 1.1316 (1.0007)\tPrec@1 61.719 (64.948)\n",
      "Epoch: [119][234/390]\tTime 0.002 (0.003)\tLoss 1.0707 (1.0248)\tPrec@1 70.312 (63.956)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.003)\tLoss 1.0681 (1.0434)\tPrec@1 61.719 (63.309)\n",
      "Epoch: [119][390/390]\tTime 0.002 (0.003)\tLoss 1.2620 (1.0583)\tPrec@1 52.500 (62.686)\n",
      "EPOCH: 119 train Results: Prec@1 62.686 Loss: 1.0583\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1592 (1.1592)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1900 (1.2414)\tPrec@1 62.500 (56.500)\n",
      "EPOCH: 119 val Results: Prec@1 56.500 Loss: 1.2414\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [120][0/390]\tTime 0.010 (0.010)\tLoss 0.8465 (0.8465)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.003)\tLoss 1.1015 (1.0034)\tPrec@1 63.281 (63.934)\n",
      "Epoch: [120][156/390]\tTime 0.002 (0.003)\tLoss 0.9712 (1.0230)\tPrec@1 66.406 (63.411)\n",
      "Epoch: [120][234/390]\tTime 0.002 (0.003)\tLoss 1.1273 (1.0370)\tPrec@1 62.500 (63.085)\n",
      "Epoch: [120][312/390]\tTime 0.002 (0.003)\tLoss 1.1796 (1.0435)\tPrec@1 60.156 (62.869)\n",
      "Epoch: [120][390/390]\tTime 0.001 (0.003)\tLoss 1.0826 (1.0572)\tPrec@1 60.000 (62.422)\n",
      "EPOCH: 120 train Results: Prec@1 62.422 Loss: 1.0572\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1410 (1.1410)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1463 (1.2381)\tPrec@1 56.250 (56.320)\n",
      "EPOCH: 120 val Results: Prec@1 56.320 Loss: 1.2381\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [121][0/390]\tTime 0.004 (0.004)\tLoss 0.9176 (0.9176)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [121][78/390]\tTime 0.003 (0.003)\tLoss 1.0083 (0.9769)\tPrec@1 64.844 (65.170)\n",
      "Epoch: [121][156/390]\tTime 0.005 (0.003)\tLoss 1.0512 (1.0154)\tPrec@1 64.062 (63.878)\n",
      "Epoch: [121][234/390]\tTime 0.008 (0.003)\tLoss 1.1725 (1.0300)\tPrec@1 60.156 (63.428)\n",
      "Epoch: [121][312/390]\tTime 0.003 (0.003)\tLoss 0.9272 (1.0414)\tPrec@1 71.875 (62.969)\n",
      "Epoch: [121][390/390]\tTime 0.002 (0.003)\tLoss 0.7719 (1.0543)\tPrec@1 77.500 (62.634)\n",
      "EPOCH: 121 train Results: Prec@1 62.634 Loss: 1.0543\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1053 (1.1053)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2653 (1.2404)\tPrec@1 50.000 (55.830)\n",
      "EPOCH: 121 val Results: Prec@1 55.830 Loss: 1.2404\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [122][0/390]\tTime 0.004 (0.004)\tLoss 0.8587 (0.8587)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [122][78/390]\tTime 0.004 (0.003)\tLoss 0.9714 (0.9718)\tPrec@1 68.750 (65.783)\n",
      "Epoch: [122][156/390]\tTime 0.002 (0.003)\tLoss 0.9016 (1.0034)\tPrec@1 67.188 (64.769)\n",
      "Epoch: [122][234/390]\tTime 0.002 (0.003)\tLoss 1.2045 (1.0298)\tPrec@1 59.375 (63.684)\n",
      "Epoch: [122][312/390]\tTime 0.002 (0.003)\tLoss 1.1164 (1.0479)\tPrec@1 60.938 (62.857)\n",
      "Epoch: [122][390/390]\tTime 0.002 (0.003)\tLoss 1.2171 (1.0587)\tPrec@1 52.500 (62.412)\n",
      "EPOCH: 122 train Results: Prec@1 62.412 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1278 (1.1278)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1542 (1.2508)\tPrec@1 43.750 (55.820)\n",
      "EPOCH: 122 val Results: Prec@1 55.820 Loss: 1.2508\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [123][0/390]\tTime 0.005 (0.005)\tLoss 0.8843 (0.8843)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [123][78/390]\tTime 0.002 (0.003)\tLoss 1.1059 (0.9863)\tPrec@1 56.250 (64.933)\n",
      "Epoch: [123][156/390]\tTime 0.002 (0.003)\tLoss 1.0581 (1.0112)\tPrec@1 63.281 (64.112)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.003)\tLoss 1.1821 (1.0292)\tPrec@1 60.156 (63.547)\n",
      "Epoch: [123][312/390]\tTime 0.003 (0.003)\tLoss 1.1624 (1.0445)\tPrec@1 56.250 (62.934)\n",
      "Epoch: [123][390/390]\tTime 0.002 (0.003)\tLoss 0.9861 (1.0555)\tPrec@1 57.500 (62.426)\n",
      "EPOCH: 123 train Results: Prec@1 62.426 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1520 (1.1520)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5616 (1.2442)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 123 val Results: Prec@1 55.630 Loss: 1.2442\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [124][0/390]\tTime 0.003 (0.003)\tLoss 0.9526 (0.9526)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [124][78/390]\tTime 0.003 (0.003)\tLoss 1.1315 (0.9898)\tPrec@1 64.844 (64.982)\n",
      "Epoch: [124][156/390]\tTime 0.002 (0.003)\tLoss 1.0142 (1.0192)\tPrec@1 63.281 (63.938)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.1207 (1.0347)\tPrec@1 53.125 (63.361)\n",
      "Epoch: [124][312/390]\tTime 0.003 (0.003)\tLoss 1.1423 (1.0452)\tPrec@1 64.062 (62.992)\n",
      "Epoch: [124][390/390]\tTime 0.001 (0.003)\tLoss 1.2559 (1.0563)\tPrec@1 56.250 (62.590)\n",
      "EPOCH: 124 train Results: Prec@1 62.590 Loss: 1.0563\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1495 (1.1495)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3113 (1.2439)\tPrec@1 50.000 (55.430)\n",
      "EPOCH: 124 val Results: Prec@1 55.430 Loss: 1.2439\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [125][0/390]\tTime 0.010 (0.010)\tLoss 0.9665 (0.9665)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [125][78/390]\tTime 0.002 (0.003)\tLoss 0.8627 (0.9852)\tPrec@1 68.750 (65.447)\n",
      "Epoch: [125][156/390]\tTime 0.003 (0.003)\tLoss 1.0006 (1.0151)\tPrec@1 63.281 (63.824)\n",
      "Epoch: [125][234/390]\tTime 0.003 (0.003)\tLoss 1.1689 (1.0330)\tPrec@1 58.594 (63.278)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.0903 (1.0444)\tPrec@1 66.406 (62.984)\n",
      "Epoch: [125][390/390]\tTime 0.004 (0.003)\tLoss 0.9902 (1.0553)\tPrec@1 62.500 (62.578)\n",
      "EPOCH: 125 train Results: Prec@1 62.578 Loss: 1.0553\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0995 (1.0995)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1445 (1.2464)\tPrec@1 62.500 (55.350)\n",
      "EPOCH: 125 val Results: Prec@1 55.350 Loss: 1.2464\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [126][0/390]\tTime 0.003 (0.003)\tLoss 0.8440 (0.8440)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [126][78/390]\tTime 0.005 (0.003)\tLoss 0.8701 (0.9746)\tPrec@1 70.312 (66.060)\n",
      "Epoch: [126][156/390]\tTime 0.006 (0.003)\tLoss 1.1099 (1.0097)\tPrec@1 60.156 (64.590)\n",
      "Epoch: [126][234/390]\tTime 0.008 (0.003)\tLoss 1.1703 (1.0274)\tPrec@1 55.469 (63.883)\n",
      "Epoch: [126][312/390]\tTime 0.003 (0.003)\tLoss 1.0385 (1.0451)\tPrec@1 59.375 (63.092)\n",
      "Epoch: [126][390/390]\tTime 0.001 (0.003)\tLoss 1.1139 (1.0566)\tPrec@1 57.500 (62.648)\n",
      "EPOCH: 126 train Results: Prec@1 62.648 Loss: 1.0566\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1683 (1.1683)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5607 (1.2612)\tPrec@1 37.500 (55.270)\n",
      "EPOCH: 126 val Results: Prec@1 55.270 Loss: 1.2612\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [127][0/390]\tTime 0.004 (0.004)\tLoss 1.1059 (1.1059)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.004)\tLoss 0.8879 (0.9820)\tPrec@1 68.750 (65.625)\n",
      "Epoch: [127][156/390]\tTime 0.003 (0.004)\tLoss 0.9412 (1.0070)\tPrec@1 62.500 (64.590)\n",
      "Epoch: [127][234/390]\tTime 0.002 (0.004)\tLoss 1.2215 (1.0233)\tPrec@1 53.125 (63.916)\n",
      "Epoch: [127][312/390]\tTime 0.003 (0.004)\tLoss 1.1398 (1.0433)\tPrec@1 60.156 (63.082)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.003)\tLoss 1.0749 (1.0548)\tPrec@1 58.750 (62.734)\n",
      "EPOCH: 127 train Results: Prec@1 62.734 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0907 (1.0907)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5188 (1.2488)\tPrec@1 25.000 (56.110)\n",
      "EPOCH: 127 val Results: Prec@1 56.110 Loss: 1.2488\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [128][0/390]\tTime 0.002 (0.002)\tLoss 1.0835 (1.0835)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [128][78/390]\tTime 0.002 (0.003)\tLoss 1.0944 (0.9799)\tPrec@1 59.375 (65.348)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.003)\tLoss 1.0504 (1.0108)\tPrec@1 59.375 (64.291)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.003)\tLoss 1.0993 (1.0257)\tPrec@1 63.281 (63.813)\n",
      "Epoch: [128][312/390]\tTime 0.004 (0.003)\tLoss 0.9750 (1.0414)\tPrec@1 65.625 (63.219)\n",
      "Epoch: [128][390/390]\tTime 0.006 (0.003)\tLoss 1.2167 (1.0561)\tPrec@1 62.500 (62.710)\n",
      "EPOCH: 128 train Results: Prec@1 62.710 Loss: 1.0561\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1464 (1.1464)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0429 (1.2534)\tPrec@1 62.500 (56.020)\n",
      "EPOCH: 128 val Results: Prec@1 56.020 Loss: 1.2534\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [129][0/390]\tTime 0.003 (0.003)\tLoss 0.9921 (0.9921)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 0.9038 (0.9669)\tPrec@1 67.188 (65.576)\n",
      "Epoch: [129][156/390]\tTime 0.002 (0.003)\tLoss 1.1639 (0.9948)\tPrec@1 53.906 (64.565)\n",
      "Epoch: [129][234/390]\tTime 0.002 (0.003)\tLoss 1.2257 (1.0203)\tPrec@1 51.562 (63.630)\n",
      "Epoch: [129][312/390]\tTime 0.003 (0.003)\tLoss 0.9724 (1.0389)\tPrec@1 65.625 (62.942)\n",
      "Epoch: [129][390/390]\tTime 0.004 (0.003)\tLoss 1.0107 (1.0530)\tPrec@1 61.250 (62.494)\n",
      "EPOCH: 129 train Results: Prec@1 62.494 Loss: 1.0530\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1518 (1.1518)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5558 (1.2488)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 129 val Results: Prec@1 55.630 Loss: 1.2488\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [130][0/390]\tTime 0.005 (0.005)\tLoss 0.9196 (0.9196)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [130][78/390]\tTime 0.003 (0.003)\tLoss 0.8568 (0.9780)\tPrec@1 67.969 (65.318)\n",
      "Epoch: [130][156/390]\tTime 0.002 (0.003)\tLoss 1.1205 (0.9997)\tPrec@1 57.031 (64.610)\n",
      "Epoch: [130][234/390]\tTime 0.002 (0.003)\tLoss 1.2181 (1.0277)\tPrec@1 50.781 (63.647)\n",
      "Epoch: [130][312/390]\tTime 0.002 (0.003)\tLoss 1.0215 (1.0419)\tPrec@1 63.281 (63.062)\n",
      "Epoch: [130][390/390]\tTime 0.001 (0.003)\tLoss 1.2963 (1.0548)\tPrec@1 57.500 (62.690)\n",
      "EPOCH: 130 train Results: Prec@1 62.690 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1774 (1.1774)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2627 (1.2432)\tPrec@1 50.000 (55.650)\n",
      "EPOCH: 130 val Results: Prec@1 55.650 Loss: 1.2432\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [131][0/390]\tTime 0.006 (0.006)\tLoss 0.8136 (0.8136)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [131][78/390]\tTime 0.005 (0.003)\tLoss 0.9095 (0.9846)\tPrec@1 67.969 (64.903)\n",
      "Epoch: [131][156/390]\tTime 0.003 (0.003)\tLoss 1.1519 (1.0140)\tPrec@1 57.812 (63.431)\n",
      "Epoch: [131][234/390]\tTime 0.002 (0.003)\tLoss 1.0859 (1.0344)\tPrec@1 60.156 (62.965)\n",
      "Epoch: [131][312/390]\tTime 0.003 (0.003)\tLoss 1.3291 (1.0490)\tPrec@1 52.344 (62.542)\n",
      "Epoch: [131][390/390]\tTime 0.003 (0.003)\tLoss 0.9885 (1.0614)\tPrec@1 56.250 (62.164)\n",
      "EPOCH: 131 train Results: Prec@1 62.164 Loss: 1.0614\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1521 (1.1521)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4112 (1.2416)\tPrec@1 43.750 (56.310)\n",
      "EPOCH: 131 val Results: Prec@1 56.310 Loss: 1.2416\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [132][0/390]\tTime 0.003 (0.003)\tLoss 0.9764 (0.9764)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [132][78/390]\tTime 0.003 (0.003)\tLoss 0.9670 (0.9841)\tPrec@1 70.312 (65.477)\n",
      "Epoch: [132][156/390]\tTime 0.004 (0.004)\tLoss 1.1561 (1.0174)\tPrec@1 56.250 (63.888)\n",
      "Epoch: [132][234/390]\tTime 0.006 (0.003)\tLoss 1.0350 (1.0365)\tPrec@1 60.938 (63.108)\n",
      "Epoch: [132][312/390]\tTime 0.002 (0.003)\tLoss 1.3245 (1.0470)\tPrec@1 50.000 (62.822)\n",
      "Epoch: [132][390/390]\tTime 0.001 (0.003)\tLoss 1.1155 (1.0529)\tPrec@1 60.000 (62.556)\n",
      "EPOCH: 132 train Results: Prec@1 62.556 Loss: 1.0529\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1525 (1.1525)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1410 (1.2420)\tPrec@1 50.000 (55.350)\n",
      "EPOCH: 132 val Results: Prec@1 55.350 Loss: 1.2420\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [133][0/390]\tTime 0.004 (0.004)\tLoss 0.9031 (0.9031)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [133][78/390]\tTime 0.002 (0.004)\tLoss 1.0601 (0.9873)\tPrec@1 57.812 (64.814)\n",
      "Epoch: [133][156/390]\tTime 0.002 (0.004)\tLoss 0.9121 (1.0023)\tPrec@1 73.438 (64.694)\n",
      "Epoch: [133][234/390]\tTime 0.002 (0.003)\tLoss 1.1981 (1.0238)\tPrec@1 58.594 (63.803)\n",
      "Epoch: [133][312/390]\tTime 0.007 (0.003)\tLoss 1.1081 (1.0429)\tPrec@1 65.625 (63.067)\n",
      "Epoch: [133][390/390]\tTime 0.002 (0.003)\tLoss 1.1656 (1.0555)\tPrec@1 55.000 (62.606)\n",
      "EPOCH: 133 train Results: Prec@1 62.606 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1554 (1.1554)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2486 (1.2487)\tPrec@1 62.500 (55.740)\n",
      "EPOCH: 133 val Results: Prec@1 55.740 Loss: 1.2487\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [134][0/390]\tTime 0.002 (0.002)\tLoss 0.9774 (0.9774)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [134][78/390]\tTime 0.002 (0.003)\tLoss 0.9520 (0.9884)\tPrec@1 70.312 (65.032)\n",
      "Epoch: [134][156/390]\tTime 0.002 (0.003)\tLoss 1.0292 (1.0189)\tPrec@1 62.500 (64.122)\n",
      "Epoch: [134][234/390]\tTime 0.003 (0.003)\tLoss 1.0544 (1.0295)\tPrec@1 65.625 (63.670)\n",
      "Epoch: [134][312/390]\tTime 0.002 (0.003)\tLoss 0.9994 (1.0422)\tPrec@1 64.844 (63.112)\n",
      "Epoch: [134][390/390]\tTime 0.001 (0.003)\tLoss 1.1031 (1.0541)\tPrec@1 57.500 (62.632)\n",
      "EPOCH: 134 train Results: Prec@1 62.632 Loss: 1.0541\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0985 (1.0985)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0846 (1.2384)\tPrec@1 68.750 (56.230)\n",
      "EPOCH: 134 val Results: Prec@1 56.230 Loss: 1.2384\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [135][0/390]\tTime 0.005 (0.005)\tLoss 0.9843 (0.9843)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [135][78/390]\tTime 0.005 (0.003)\tLoss 0.9682 (0.9819)\tPrec@1 65.625 (65.437)\n",
      "Epoch: [135][156/390]\tTime 0.002 (0.003)\tLoss 1.0184 (1.0116)\tPrec@1 64.062 (64.386)\n",
      "Epoch: [135][234/390]\tTime 0.003 (0.003)\tLoss 1.1733 (1.0331)\tPrec@1 60.156 (63.477)\n",
      "Epoch: [135][312/390]\tTime 0.002 (0.003)\tLoss 1.0581 (1.0466)\tPrec@1 60.156 (63.092)\n",
      "Epoch: [135][390/390]\tTime 0.002 (0.003)\tLoss 1.1130 (1.0606)\tPrec@1 63.750 (62.520)\n",
      "EPOCH: 135 train Results: Prec@1 62.520 Loss: 1.0606\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1276 (1.1276)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1416 (1.2412)\tPrec@1 62.500 (56.260)\n",
      "EPOCH: 135 val Results: Prec@1 56.260 Loss: 1.2412\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [136][0/390]\tTime 0.003 (0.003)\tLoss 0.8898 (0.8898)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [136][78/390]\tTime 0.004 (0.003)\tLoss 0.9741 (0.9828)\tPrec@1 63.281 (65.556)\n",
      "Epoch: [136][156/390]\tTime 0.004 (0.003)\tLoss 0.9854 (1.0118)\tPrec@1 65.625 (64.406)\n",
      "Epoch: [136][234/390]\tTime 0.003 (0.003)\tLoss 1.0737 (1.0322)\tPrec@1 67.188 (63.610)\n",
      "Epoch: [136][312/390]\tTime 0.002 (0.003)\tLoss 1.0903 (1.0442)\tPrec@1 63.281 (63.039)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.0519 (1.0537)\tPrec@1 65.000 (62.498)\n",
      "EPOCH: 136 train Results: Prec@1 62.498 Loss: 1.0537\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0629 (1.0629)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3957 (1.2518)\tPrec@1 43.750 (55.760)\n",
      "EPOCH: 136 val Results: Prec@1 55.760 Loss: 1.2518\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [137][0/390]\tTime 0.004 (0.004)\tLoss 0.9844 (0.9844)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [137][78/390]\tTime 0.003 (0.003)\tLoss 1.0180 (1.0001)\tPrec@1 63.281 (64.705)\n",
      "Epoch: [137][156/390]\tTime 0.002 (0.003)\tLoss 1.0029 (1.0149)\tPrec@1 67.188 (64.062)\n",
      "Epoch: [137][234/390]\tTime 0.003 (0.003)\tLoss 0.9981 (1.0258)\tPrec@1 63.281 (63.481)\n",
      "Epoch: [137][312/390]\tTime 0.002 (0.004)\tLoss 1.1435 (1.0404)\tPrec@1 58.594 (62.932)\n",
      "Epoch: [137][390/390]\tTime 0.002 (0.003)\tLoss 1.1828 (1.0557)\tPrec@1 62.500 (62.410)\n",
      "EPOCH: 137 train Results: Prec@1 62.410 Loss: 1.0557\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0793 (1.0793)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1897 (1.2468)\tPrec@1 50.000 (55.920)\n",
      "EPOCH: 137 val Results: Prec@1 55.920 Loss: 1.2468\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [138][0/390]\tTime 0.004 (0.004)\tLoss 1.0197 (1.0197)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [138][78/390]\tTime 0.002 (0.003)\tLoss 1.0023 (0.9910)\tPrec@1 64.062 (64.982)\n",
      "Epoch: [138][156/390]\tTime 0.011 (0.004)\tLoss 1.0524 (1.0146)\tPrec@1 63.281 (64.192)\n",
      "Epoch: [138][234/390]\tTime 0.003 (0.004)\tLoss 1.2363 (1.0344)\tPrec@1 57.812 (63.600)\n",
      "Epoch: [138][312/390]\tTime 0.004 (0.004)\tLoss 1.3238 (1.0490)\tPrec@1 54.688 (63.029)\n",
      "Epoch: [138][390/390]\tTime 0.001 (0.004)\tLoss 1.2542 (1.0599)\tPrec@1 51.250 (62.606)\n",
      "EPOCH: 138 train Results: Prec@1 62.606 Loss: 1.0599\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1396 (1.1396)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1525 (1.2438)\tPrec@1 68.750 (55.730)\n",
      "EPOCH: 138 val Results: Prec@1 55.730 Loss: 1.2438\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [139][0/390]\tTime 0.004 (0.004)\tLoss 0.9400 (0.9400)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [139][78/390]\tTime 0.002 (0.003)\tLoss 0.9549 (0.9800)\tPrec@1 66.406 (64.972)\n",
      "Epoch: [139][156/390]\tTime 0.003 (0.003)\tLoss 0.9865 (1.0099)\tPrec@1 65.625 (64.271)\n",
      "Epoch: [139][234/390]\tTime 0.007 (0.003)\tLoss 1.0976 (1.0304)\tPrec@1 62.500 (63.471)\n",
      "Epoch: [139][312/390]\tTime 0.003 (0.004)\tLoss 1.0325 (1.0436)\tPrec@1 61.719 (62.967)\n",
      "Epoch: [139][390/390]\tTime 0.001 (0.004)\tLoss 1.1408 (1.0545)\tPrec@1 61.250 (62.500)\n",
      "EPOCH: 139 train Results: Prec@1 62.500 Loss: 1.0545\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2566 (1.2566)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2131 (1.2424)\tPrec@1 43.750 (55.900)\n",
      "EPOCH: 139 val Results: Prec@1 55.900 Loss: 1.2424\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [140][0/390]\tTime 0.004 (0.004)\tLoss 0.9393 (0.9393)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [140][78/390]\tTime 0.003 (0.003)\tLoss 1.0934 (0.9813)\tPrec@1 57.031 (65.684)\n",
      "Epoch: [140][156/390]\tTime 0.002 (0.004)\tLoss 1.0635 (1.0050)\tPrec@1 64.062 (64.431)\n",
      "Epoch: [140][234/390]\tTime 0.002 (0.005)\tLoss 0.9282 (1.0262)\tPrec@1 64.844 (63.381)\n",
      "Epoch: [140][312/390]\tTime 0.002 (0.005)\tLoss 1.0334 (1.0378)\tPrec@1 57.031 (63.114)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.005)\tLoss 1.2326 (1.0531)\tPrec@1 52.500 (62.532)\n",
      "EPOCH: 140 train Results: Prec@1 62.532 Loss: 1.0531\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1588 (1.1588)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6436 (1.2515)\tPrec@1 31.250 (55.730)\n",
      "EPOCH: 140 val Results: Prec@1 55.730 Loss: 1.2515\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [141][0/390]\tTime 0.003 (0.003)\tLoss 0.7695 (0.7695)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [141][78/390]\tTime 0.003 (0.003)\tLoss 1.0693 (0.9881)\tPrec@1 65.625 (64.903)\n",
      "Epoch: [141][156/390]\tTime 0.002 (0.003)\tLoss 1.0198 (1.0139)\tPrec@1 61.719 (63.769)\n",
      "Epoch: [141][234/390]\tTime 0.002 (0.003)\tLoss 1.0810 (1.0331)\tPrec@1 64.844 (63.305)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.003)\tLoss 1.1711 (1.0491)\tPrec@1 64.062 (62.670)\n",
      "Epoch: [141][390/390]\tTime 0.003 (0.004)\tLoss 1.1927 (1.0582)\tPrec@1 55.000 (62.378)\n",
      "EPOCH: 141 train Results: Prec@1 62.378 Loss: 1.0582\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1646 (1.1646)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4236 (1.2419)\tPrec@1 43.750 (56.170)\n",
      "EPOCH: 141 val Results: Prec@1 56.170 Loss: 1.2419\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [142][0/390]\tTime 0.006 (0.006)\tLoss 0.9188 (0.9188)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [142][78/390]\tTime 0.015 (0.005)\tLoss 0.9566 (0.9794)\tPrec@1 66.406 (65.615)\n",
      "Epoch: [142][156/390]\tTime 0.002 (0.004)\tLoss 1.0058 (1.0048)\tPrec@1 62.500 (64.262)\n",
      "Epoch: [142][234/390]\tTime 0.002 (0.004)\tLoss 1.0705 (1.0225)\tPrec@1 66.406 (63.700)\n",
      "Epoch: [142][312/390]\tTime 0.010 (0.004)\tLoss 1.1555 (1.0428)\tPrec@1 59.375 (62.934)\n",
      "Epoch: [142][390/390]\tTime 0.006 (0.004)\tLoss 1.1153 (1.0560)\tPrec@1 61.250 (62.444)\n",
      "EPOCH: 142 train Results: Prec@1 62.444 Loss: 1.0560\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1941 (1.1941)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9589 (1.2618)\tPrec@1 68.750 (55.720)\n",
      "EPOCH: 142 val Results: Prec@1 55.720 Loss: 1.2618\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [143][0/390]\tTime 0.005 (0.005)\tLoss 0.8287 (0.8287)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [143][78/390]\tTime 0.005 (0.003)\tLoss 1.0337 (0.9805)\tPrec@1 68.750 (66.011)\n",
      "Epoch: [143][156/390]\tTime 0.004 (0.003)\tLoss 0.9681 (1.0051)\tPrec@1 61.719 (64.749)\n",
      "Epoch: [143][234/390]\tTime 0.009 (0.003)\tLoss 1.0126 (1.0195)\tPrec@1 60.938 (64.142)\n",
      "Epoch: [143][312/390]\tTime 0.003 (0.003)\tLoss 1.1045 (1.0400)\tPrec@1 65.625 (63.219)\n",
      "Epoch: [143][390/390]\tTime 0.003 (0.003)\tLoss 1.0400 (1.0523)\tPrec@1 65.000 (62.762)\n",
      "EPOCH: 143 train Results: Prec@1 62.762 Loss: 1.0523\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1829 (1.1829)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1903 (1.2546)\tPrec@1 62.500 (55.800)\n",
      "EPOCH: 143 val Results: Prec@1 55.800 Loss: 1.2546\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [144][0/390]\tTime 0.006 (0.006)\tLoss 0.9555 (0.9555)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [144][78/390]\tTime 0.002 (0.006)\tLoss 1.0410 (0.9933)\tPrec@1 60.938 (64.735)\n",
      "Epoch: [144][156/390]\tTime 0.002 (0.005)\tLoss 1.3186 (1.0088)\tPrec@1 55.469 (63.993)\n",
      "Epoch: [144][234/390]\tTime 0.012 (0.004)\tLoss 1.0546 (1.0287)\tPrec@1 62.500 (63.424)\n",
      "Epoch: [144][312/390]\tTime 0.011 (0.004)\tLoss 0.9977 (1.0420)\tPrec@1 60.938 (62.864)\n",
      "Epoch: [144][390/390]\tTime 0.016 (0.005)\tLoss 0.9956 (1.0536)\tPrec@1 61.250 (62.482)\n",
      "EPOCH: 144 train Results: Prec@1 62.482 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1016 (1.1016)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3678 (1.2443)\tPrec@1 37.500 (55.850)\n",
      "EPOCH: 144 val Results: Prec@1 55.850 Loss: 1.2443\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [145][0/390]\tTime 0.002 (0.002)\tLoss 1.0386 (1.0386)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [145][78/390]\tTime 0.002 (0.006)\tLoss 0.8599 (0.9963)\tPrec@1 71.875 (64.705)\n",
      "Epoch: [145][156/390]\tTime 0.003 (0.005)\tLoss 1.1530 (1.0008)\tPrec@1 62.500 (64.456)\n",
      "Epoch: [145][234/390]\tTime 0.003 (0.004)\tLoss 1.0696 (1.0245)\tPrec@1 63.281 (63.634)\n",
      "Epoch: [145][312/390]\tTime 0.007 (0.004)\tLoss 1.2211 (1.0401)\tPrec@1 52.344 (63.042)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.004)\tLoss 1.0382 (1.0496)\tPrec@1 67.500 (62.714)\n",
      "EPOCH: 145 train Results: Prec@1 62.714 Loss: 1.0496\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1231 (1.1231)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5981 (1.2594)\tPrec@1 50.000 (56.050)\n",
      "EPOCH: 145 val Results: Prec@1 56.050 Loss: 1.2594\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [146][0/390]\tTime 0.003 (0.003)\tLoss 1.0175 (1.0175)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [146][78/390]\tTime 0.002 (0.005)\tLoss 0.9075 (0.9832)\tPrec@1 68.750 (65.318)\n",
      "Epoch: [146][156/390]\tTime 0.013 (0.005)\tLoss 0.8845 (1.0173)\tPrec@1 67.188 (63.898)\n",
      "Epoch: [146][234/390]\tTime 0.004 (0.004)\tLoss 0.8852 (1.0314)\tPrec@1 69.531 (63.408)\n",
      "Epoch: [146][312/390]\tTime 0.004 (0.004)\tLoss 0.9099 (1.0454)\tPrec@1 68.750 (62.904)\n",
      "Epoch: [146][390/390]\tTime 0.002 (0.004)\tLoss 0.9654 (1.0580)\tPrec@1 63.750 (62.444)\n",
      "EPOCH: 146 train Results: Prec@1 62.444 Loss: 1.0580\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1334 (1.1334)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1523 (1.2455)\tPrec@1 50.000 (56.190)\n",
      "EPOCH: 146 val Results: Prec@1 56.190 Loss: 1.2455\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [147][0/390]\tTime 0.004 (0.004)\tLoss 0.9262 (0.9262)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [147][78/390]\tTime 0.003 (0.004)\tLoss 1.0787 (0.9879)\tPrec@1 64.844 (65.526)\n",
      "Epoch: [147][156/390]\tTime 0.002 (0.005)\tLoss 1.1022 (1.0083)\tPrec@1 60.938 (64.376)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.004)\tLoss 1.0891 (1.0252)\tPrec@1 62.500 (63.614)\n",
      "Epoch: [147][312/390]\tTime 0.012 (0.004)\tLoss 1.0196 (1.0384)\tPrec@1 65.625 (63.211)\n",
      "Epoch: [147][390/390]\tTime 0.003 (0.004)\tLoss 1.2217 (1.0521)\tPrec@1 58.750 (62.764)\n",
      "EPOCH: 147 train Results: Prec@1 62.764 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1374 (1.1374)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3970 (1.2449)\tPrec@1 43.750 (56.020)\n",
      "EPOCH: 147 val Results: Prec@1 56.020 Loss: 1.2449\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [148][0/390]\tTime 0.003 (0.003)\tLoss 1.0187 (1.0187)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [148][78/390]\tTime 0.009 (0.005)\tLoss 0.9959 (0.9859)\tPrec@1 64.844 (65.694)\n",
      "Epoch: [148][156/390]\tTime 0.009 (0.005)\tLoss 1.0054 (1.0093)\tPrec@1 71.094 (64.565)\n",
      "Epoch: [148][234/390]\tTime 0.003 (0.005)\tLoss 0.9351 (1.0307)\tPrec@1 71.094 (63.561)\n",
      "Epoch: [148][312/390]\tTime 0.004 (0.005)\tLoss 0.9715 (1.0431)\tPrec@1 66.406 (63.064)\n",
      "Epoch: [148][390/390]\tTime 0.003 (0.004)\tLoss 1.0868 (1.0535)\tPrec@1 58.750 (62.722)\n",
      "EPOCH: 148 train Results: Prec@1 62.722 Loss: 1.0535\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1692 (1.1692)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3045 (1.2574)\tPrec@1 37.500 (55.920)\n",
      "EPOCH: 148 val Results: Prec@1 55.920 Loss: 1.2574\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [149][0/390]\tTime 0.003 (0.003)\tLoss 0.8989 (0.8989)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [149][78/390]\tTime 0.002 (0.004)\tLoss 1.0823 (0.9783)\tPrec@1 58.594 (65.526)\n",
      "Epoch: [149][156/390]\tTime 0.006 (0.003)\tLoss 1.0015 (1.0000)\tPrec@1 64.844 (64.550)\n",
      "Epoch: [149][234/390]\tTime 0.004 (0.004)\tLoss 0.9860 (1.0214)\tPrec@1 65.625 (63.600)\n",
      "Epoch: [149][312/390]\tTime 0.002 (0.004)\tLoss 1.1331 (1.0388)\tPrec@1 62.500 (63.074)\n",
      "Epoch: [149][390/390]\tTime 0.002 (0.003)\tLoss 1.1586 (1.0529)\tPrec@1 56.250 (62.614)\n",
      "EPOCH: 149 train Results: Prec@1 62.614 Loss: 1.0529\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1581 (1.1581)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2205 (1.2395)\tPrec@1 56.250 (55.950)\n",
      "EPOCH: 149 val Results: Prec@1 55.950 Loss: 1.2395\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [150][0/390]\tTime 0.005 (0.005)\tLoss 0.9954 (0.9954)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [150][78/390]\tTime 0.002 (0.003)\tLoss 1.1860 (0.9600)\tPrec@1 63.281 (66.525)\n",
      "Epoch: [150][156/390]\tTime 0.003 (0.003)\tLoss 1.1368 (0.9932)\tPrec@1 65.625 (65.107)\n",
      "Epoch: [150][234/390]\tTime 0.002 (0.003)\tLoss 1.1294 (1.0257)\tPrec@1 60.938 (63.747)\n",
      "Epoch: [150][312/390]\tTime 0.004 (0.003)\tLoss 1.1213 (1.0417)\tPrec@1 57.812 (63.107)\n",
      "Epoch: [150][390/390]\tTime 0.001 (0.003)\tLoss 1.1039 (1.0547)\tPrec@1 66.250 (62.682)\n",
      "EPOCH: 150 train Results: Prec@1 62.682 Loss: 1.0547\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.0899 (1.0899)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1477 (1.2386)\tPrec@1 50.000 (55.810)\n",
      "EPOCH: 150 val Results: Prec@1 55.810 Loss: 1.2386\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [151][0/390]\tTime 0.004 (0.004)\tLoss 0.8751 (0.8751)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [151][78/390]\tTime 0.002 (0.003)\tLoss 1.1739 (0.9745)\tPrec@1 58.594 (65.971)\n",
      "Epoch: [151][156/390]\tTime 0.003 (0.004)\tLoss 1.0461 (0.9960)\tPrec@1 64.844 (65.018)\n",
      "Epoch: [151][234/390]\tTime 0.003 (0.004)\tLoss 1.0950 (1.0184)\tPrec@1 54.688 (63.979)\n",
      "Epoch: [151][312/390]\tTime 0.002 (0.003)\tLoss 1.1098 (1.0343)\tPrec@1 62.500 (63.191)\n",
      "Epoch: [151][390/390]\tTime 0.001 (0.003)\tLoss 1.2381 (1.0494)\tPrec@1 58.750 (62.668)\n",
      "EPOCH: 151 train Results: Prec@1 62.668 Loss: 1.0494\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1274 (1.1274)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1526 (1.2491)\tPrec@1 56.250 (55.820)\n",
      "EPOCH: 151 val Results: Prec@1 55.820 Loss: 1.2491\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [152][0/390]\tTime 0.002 (0.002)\tLoss 0.8862 (0.8862)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [152][78/390]\tTime 0.013 (0.004)\tLoss 1.0201 (0.9859)\tPrec@1 64.844 (65.150)\n",
      "Epoch: [152][156/390]\tTime 0.003 (0.003)\tLoss 0.9114 (1.0118)\tPrec@1 71.094 (64.097)\n",
      "Epoch: [152][234/390]\tTime 0.002 (0.003)\tLoss 1.1243 (1.0270)\tPrec@1 65.625 (63.418)\n",
      "Epoch: [152][312/390]\tTime 0.002 (0.003)\tLoss 1.0971 (1.0390)\tPrec@1 60.156 (63.052)\n",
      "Epoch: [152][390/390]\tTime 0.001 (0.003)\tLoss 0.9769 (1.0504)\tPrec@1 67.500 (62.650)\n",
      "EPOCH: 152 train Results: Prec@1 62.650 Loss: 1.0504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0998 (1.0998)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1490 (1.2469)\tPrec@1 43.750 (55.460)\n",
      "EPOCH: 152 val Results: Prec@1 55.460 Loss: 1.2469\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [153][0/390]\tTime 0.004 (0.004)\tLoss 0.7881 (0.7881)\tPrec@1 78.125 (78.125)\n",
      "Epoch: [153][78/390]\tTime 0.003 (0.004)\tLoss 1.0317 (0.9816)\tPrec@1 63.281 (64.923)\n",
      "Epoch: [153][156/390]\tTime 0.003 (0.003)\tLoss 0.9838 (1.0099)\tPrec@1 66.406 (64.053)\n",
      "Epoch: [153][234/390]\tTime 0.003 (0.003)\tLoss 0.9992 (1.0240)\tPrec@1 67.188 (63.777)\n",
      "Epoch: [153][312/390]\tTime 0.002 (0.003)\tLoss 0.9122 (1.0358)\tPrec@1 66.406 (63.284)\n",
      "Epoch: [153][390/390]\tTime 0.006 (0.003)\tLoss 1.1043 (1.0488)\tPrec@1 61.250 (62.690)\n",
      "EPOCH: 153 train Results: Prec@1 62.690 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1536 (1.1536)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5127 (1.2395)\tPrec@1 31.250 (56.110)\n",
      "EPOCH: 153 val Results: Prec@1 56.110 Loss: 1.2395\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [154][0/390]\tTime 0.004 (0.004)\tLoss 0.8154 (0.8154)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.004)\tLoss 0.9076 (0.9809)\tPrec@1 66.406 (65.111)\n",
      "Epoch: [154][156/390]\tTime 0.002 (0.004)\tLoss 1.1076 (1.0031)\tPrec@1 58.594 (64.441)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.003)\tLoss 1.2239 (1.0206)\tPrec@1 53.125 (63.604)\n",
      "Epoch: [154][312/390]\tTime 0.003 (0.003)\tLoss 1.0830 (1.0397)\tPrec@1 54.688 (62.939)\n",
      "Epoch: [154][390/390]\tTime 0.001 (0.003)\tLoss 1.1593 (1.0523)\tPrec@1 57.500 (62.468)\n",
      "EPOCH: 154 train Results: Prec@1 62.468 Loss: 1.0523\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1460 (1.1460)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5095 (1.2499)\tPrec@1 62.500 (55.580)\n",
      "EPOCH: 154 val Results: Prec@1 55.580 Loss: 1.2499\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [155][0/390]\tTime 0.005 (0.005)\tLoss 0.9730 (0.9730)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [155][78/390]\tTime 0.002 (0.003)\tLoss 1.0117 (0.9669)\tPrec@1 62.500 (65.932)\n",
      "Epoch: [155][156/390]\tTime 0.003 (0.003)\tLoss 0.8956 (0.9903)\tPrec@1 65.625 (64.565)\n",
      "Epoch: [155][234/390]\tTime 0.004 (0.003)\tLoss 1.0822 (1.0114)\tPrec@1 57.031 (63.800)\n",
      "Epoch: [155][312/390]\tTime 0.005 (0.003)\tLoss 1.2484 (1.0324)\tPrec@1 55.469 (63.087)\n",
      "Epoch: [155][390/390]\tTime 0.001 (0.003)\tLoss 0.9424 (1.0482)\tPrec@1 67.500 (62.606)\n",
      "EPOCH: 155 train Results: Prec@1 62.606 Loss: 1.0482\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1167 (1.1167)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5456 (1.2592)\tPrec@1 31.250 (55.240)\n",
      "EPOCH: 155 val Results: Prec@1 55.240 Loss: 1.2592\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [156][0/390]\tTime 0.004 (0.004)\tLoss 0.9581 (0.9581)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [156][78/390]\tTime 0.002 (0.003)\tLoss 1.0681 (0.9710)\tPrec@1 60.938 (65.872)\n",
      "Epoch: [156][156/390]\tTime 0.004 (0.003)\tLoss 1.2052 (0.9985)\tPrec@1 56.250 (64.675)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.003)\tLoss 0.9933 (1.0244)\tPrec@1 65.625 (63.314)\n",
      "Epoch: [156][312/390]\tTime 0.002 (0.003)\tLoss 1.2215 (1.0414)\tPrec@1 57.812 (62.762)\n",
      "Epoch: [156][390/390]\tTime 0.002 (0.003)\tLoss 1.1265 (1.0527)\tPrec@1 60.000 (62.416)\n",
      "EPOCH: 156 train Results: Prec@1 62.416 Loss: 1.0527\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1416 (1.1416)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2897 (1.2408)\tPrec@1 50.000 (55.680)\n",
      "EPOCH: 156 val Results: Prec@1 55.680 Loss: 1.2408\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [157][0/390]\tTime 0.003 (0.003)\tLoss 1.0445 (1.0445)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [157][78/390]\tTime 0.002 (0.003)\tLoss 0.9362 (0.9707)\tPrec@1 61.719 (65.971)\n",
      "Epoch: [157][156/390]\tTime 0.002 (0.003)\tLoss 0.8165 (1.0000)\tPrec@1 67.969 (64.859)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.0113 (1.0236)\tPrec@1 61.719 (63.926)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.003)\tLoss 0.9982 (1.0383)\tPrec@1 70.312 (63.286)\n",
      "Epoch: [157][390/390]\tTime 0.002 (0.003)\tLoss 1.0817 (1.0474)\tPrec@1 57.500 (62.880)\n",
      "EPOCH: 157 train Results: Prec@1 62.880 Loss: 1.0474\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.0465 (1.0465)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1724 (1.2409)\tPrec@1 43.750 (55.920)\n",
      "EPOCH: 157 val Results: Prec@1 55.920 Loss: 1.2409\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [158][0/390]\tTime 0.003 (0.003)\tLoss 0.8930 (0.8930)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [158][78/390]\tTime 0.003 (0.003)\tLoss 1.1330 (0.9784)\tPrec@1 57.031 (65.922)\n",
      "Epoch: [158][156/390]\tTime 0.002 (0.003)\tLoss 1.0802 (1.0012)\tPrec@1 64.844 (64.724)\n",
      "Epoch: [158][234/390]\tTime 0.002 (0.003)\tLoss 1.0516 (1.0261)\tPrec@1 64.844 (63.830)\n",
      "Epoch: [158][312/390]\tTime 0.003 (0.003)\tLoss 1.1098 (1.0367)\tPrec@1 57.812 (63.131)\n",
      "Epoch: [158][390/390]\tTime 0.003 (0.003)\tLoss 1.1138 (1.0510)\tPrec@1 56.250 (62.610)\n",
      "EPOCH: 158 train Results: Prec@1 62.610 Loss: 1.0510\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0859 (1.0859)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4610 (1.2544)\tPrec@1 56.250 (55.570)\n",
      "EPOCH: 158 val Results: Prec@1 55.570 Loss: 1.2544\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [159][0/390]\tTime 0.005 (0.005)\tLoss 0.9582 (0.9582)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [159][78/390]\tTime 0.003 (0.003)\tLoss 0.9578 (0.9768)\tPrec@1 64.844 (65.595)\n",
      "Epoch: [159][156/390]\tTime 0.005 (0.003)\tLoss 0.8983 (0.9998)\tPrec@1 68.750 (64.620)\n",
      "Epoch: [159][234/390]\tTime 0.002 (0.003)\tLoss 0.9113 (1.0191)\tPrec@1 66.406 (63.903)\n",
      "Epoch: [159][312/390]\tTime 0.002 (0.003)\tLoss 1.0120 (1.0364)\tPrec@1 59.375 (63.261)\n",
      "Epoch: [159][390/390]\tTime 0.003 (0.003)\tLoss 1.2200 (1.0489)\tPrec@1 56.250 (62.818)\n",
      "EPOCH: 159 train Results: Prec@1 62.818 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1780 (1.1780)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4234 (1.2566)\tPrec@1 31.250 (54.970)\n",
      "EPOCH: 159 val Results: Prec@1 54.970 Loss: 1.2566\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [160][0/390]\tTime 0.005 (0.005)\tLoss 0.9319 (0.9319)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [160][78/390]\tTime 0.002 (0.003)\tLoss 1.0406 (1.0033)\tPrec@1 65.625 (64.705)\n",
      "Epoch: [160][156/390]\tTime 0.002 (0.003)\tLoss 1.0621 (1.0104)\tPrec@1 63.281 (64.097)\n",
      "Epoch: [160][234/390]\tTime 0.005 (0.003)\tLoss 1.0014 (1.0297)\tPrec@1 65.625 (63.328)\n",
      "Epoch: [160][312/390]\tTime 0.009 (0.003)\tLoss 1.0168 (1.0400)\tPrec@1 63.281 (63.037)\n",
      "Epoch: [160][390/390]\tTime 0.003 (0.003)\tLoss 0.9658 (1.0441)\tPrec@1 70.000 (62.862)\n",
      "EPOCH: 160 train Results: Prec@1 62.862 Loss: 1.0441\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1265 (1.1265)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2179 (1.2561)\tPrec@1 75.000 (54.660)\n",
      "EPOCH: 160 val Results: Prec@1 54.660 Loss: 1.2561\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [161][0/390]\tTime 0.006 (0.006)\tLoss 0.9399 (0.9399)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [161][78/390]\tTime 0.004 (0.003)\tLoss 0.9124 (0.9657)\tPrec@1 66.406 (65.318)\n",
      "Epoch: [161][156/390]\tTime 0.004 (0.003)\tLoss 1.0368 (0.9966)\tPrec@1 65.625 (64.625)\n",
      "Epoch: [161][234/390]\tTime 0.005 (0.003)\tLoss 1.0072 (1.0178)\tPrec@1 62.500 (63.840)\n",
      "Epoch: [161][312/390]\tTime 0.003 (0.003)\tLoss 1.0369 (1.0302)\tPrec@1 64.844 (63.448)\n",
      "Epoch: [161][390/390]\tTime 0.002 (0.003)\tLoss 1.5501 (1.0466)\tPrec@1 42.500 (62.820)\n",
      "EPOCH: 161 train Results: Prec@1 62.820 Loss: 1.0466\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2091 (1.2091)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2627 (1.2418)\tPrec@1 43.750 (55.630)\n",
      "EPOCH: 161 val Results: Prec@1 55.630 Loss: 1.2418\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.8447 (0.8447)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [162][78/390]\tTime 0.003 (0.003)\tLoss 1.0451 (0.9804)\tPrec@1 64.844 (65.556)\n",
      "Epoch: [162][156/390]\tTime 0.003 (0.003)\tLoss 0.9330 (1.0099)\tPrec@1 67.188 (64.446)\n",
      "Epoch: [162][234/390]\tTime 0.002 (0.003)\tLoss 1.0354 (1.0200)\tPrec@1 65.625 (63.886)\n",
      "Epoch: [162][312/390]\tTime 0.003 (0.003)\tLoss 1.0808 (1.0350)\tPrec@1 62.500 (63.254)\n",
      "Epoch: [162][390/390]\tTime 0.001 (0.003)\tLoss 1.1795 (1.0460)\tPrec@1 56.250 (62.766)\n",
      "EPOCH: 162 train Results: Prec@1 62.766 Loss: 1.0460\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1169 (1.1169)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.8570 (1.2518)\tPrec@1 31.250 (55.910)\n",
      "EPOCH: 162 val Results: Prec@1 55.910 Loss: 1.2518\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [163][0/390]\tTime 0.005 (0.005)\tLoss 1.0921 (1.0921)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [163][78/390]\tTime 0.003 (0.003)\tLoss 0.8628 (0.9728)\tPrec@1 69.531 (65.239)\n",
      "Epoch: [163][156/390]\tTime 0.002 (0.003)\tLoss 1.0246 (0.9923)\tPrec@1 64.844 (64.585)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.003)\tLoss 1.0071 (1.0191)\tPrec@1 64.062 (63.607)\n",
      "Epoch: [163][312/390]\tTime 0.002 (0.003)\tLoss 1.1336 (1.0375)\tPrec@1 58.594 (62.974)\n",
      "Epoch: [163][390/390]\tTime 0.002 (0.003)\tLoss 1.0741 (1.0496)\tPrec@1 63.750 (62.448)\n",
      "EPOCH: 163 train Results: Prec@1 62.448 Loss: 1.0496\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1129 (1.1129)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3005 (1.2428)\tPrec@1 50.000 (56.220)\n",
      "EPOCH: 163 val Results: Prec@1 56.220 Loss: 1.2428\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [164][0/390]\tTime 0.002 (0.002)\tLoss 0.9328 (0.9328)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [164][78/390]\tTime 0.003 (0.003)\tLoss 1.0149 (0.9597)\tPrec@1 68.750 (66.100)\n",
      "Epoch: [164][156/390]\tTime 0.002 (0.003)\tLoss 1.0066 (0.9903)\tPrec@1 62.500 (65.098)\n",
      "Epoch: [164][234/390]\tTime 0.004 (0.003)\tLoss 1.0540 (1.0122)\tPrec@1 64.062 (64.139)\n",
      "Epoch: [164][312/390]\tTime 0.003 (0.003)\tLoss 1.2027 (1.0340)\tPrec@1 59.375 (63.349)\n",
      "Epoch: [164][390/390]\tTime 0.004 (0.003)\tLoss 1.0162 (1.0437)\tPrec@1 66.250 (63.088)\n",
      "EPOCH: 164 train Results: Prec@1 63.088 Loss: 1.0437\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1073 (1.1073)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3308 (1.2488)\tPrec@1 25.000 (55.550)\n",
      "EPOCH: 164 val Results: Prec@1 55.550 Loss: 1.2488\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [165][0/390]\tTime 0.004 (0.004)\tLoss 1.0223 (1.0223)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [165][78/390]\tTime 0.002 (0.003)\tLoss 0.9978 (0.9540)\tPrec@1 64.844 (66.199)\n",
      "Epoch: [165][156/390]\tTime 0.003 (0.003)\tLoss 1.0810 (0.9931)\tPrec@1 61.719 (64.699)\n",
      "Epoch: [165][234/390]\tTime 0.003 (0.003)\tLoss 1.2983 (1.0112)\tPrec@1 61.719 (64.176)\n",
      "Epoch: [165][312/390]\tTime 0.003 (0.003)\tLoss 1.1559 (1.0304)\tPrec@1 57.812 (63.294)\n",
      "Epoch: [165][390/390]\tTime 0.003 (0.003)\tLoss 1.3753 (1.0469)\tPrec@1 53.750 (62.726)\n",
      "EPOCH: 165 train Results: Prec@1 62.726 Loss: 1.0469\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1087 (1.1087)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4870 (1.2437)\tPrec@1 31.250 (55.870)\n",
      "EPOCH: 165 val Results: Prec@1 55.870 Loss: 1.2437\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [166][0/390]\tTime 0.004 (0.004)\tLoss 0.9328 (0.9328)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 0.9897 (0.9785)\tPrec@1 68.750 (65.615)\n",
      "Epoch: [166][156/390]\tTime 0.004 (0.003)\tLoss 1.1603 (1.0011)\tPrec@1 61.719 (64.724)\n",
      "Epoch: [166][234/390]\tTime 0.002 (0.003)\tLoss 1.1360 (1.0233)\tPrec@1 60.938 (64.059)\n",
      "Epoch: [166][312/390]\tTime 0.004 (0.003)\tLoss 1.0954 (1.0351)\tPrec@1 65.625 (63.359)\n",
      "Epoch: [166][390/390]\tTime 0.002 (0.003)\tLoss 1.1400 (1.0488)\tPrec@1 66.250 (62.938)\n",
      "EPOCH: 166 train Results: Prec@1 62.938 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1253 (1.1253)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2427 (1.2560)\tPrec@1 43.750 (55.710)\n",
      "EPOCH: 166 val Results: Prec@1 55.710 Loss: 1.2560\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [167][0/390]\tTime 0.002 (0.002)\tLoss 1.0941 (1.0941)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [167][78/390]\tTime 0.003 (0.003)\tLoss 1.0380 (0.9762)\tPrec@1 58.594 (65.843)\n",
      "Epoch: [167][156/390]\tTime 0.003 (0.003)\tLoss 1.0179 (0.9985)\tPrec@1 67.188 (64.779)\n",
      "Epoch: [167][234/390]\tTime 0.003 (0.003)\tLoss 1.0262 (1.0223)\tPrec@1 62.500 (63.777)\n",
      "Epoch: [167][312/390]\tTime 0.004 (0.003)\tLoss 1.1427 (1.0341)\tPrec@1 57.031 (63.331)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.003)\tLoss 1.1687 (1.0489)\tPrec@1 61.250 (62.776)\n",
      "EPOCH: 167 train Results: Prec@1 62.776 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1245 (1.1245)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2168 (1.2527)\tPrec@1 50.000 (55.610)\n",
      "EPOCH: 167 val Results: Prec@1 55.610 Loss: 1.2527\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [168][0/390]\tTime 0.004 (0.004)\tLoss 0.9400 (0.9400)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [168][78/390]\tTime 0.003 (0.003)\tLoss 0.8851 (0.9661)\tPrec@1 71.094 (66.159)\n",
      "Epoch: [168][156/390]\tTime 0.003 (0.003)\tLoss 0.9360 (1.0031)\tPrec@1 69.531 (64.545)\n",
      "Epoch: [168][234/390]\tTime 0.003 (0.003)\tLoss 1.1173 (1.0238)\tPrec@1 61.719 (63.610)\n",
      "Epoch: [168][312/390]\tTime 0.002 (0.003)\tLoss 1.0270 (1.0412)\tPrec@1 63.281 (62.984)\n",
      "Epoch: [168][390/390]\tTime 0.002 (0.003)\tLoss 1.0879 (1.0525)\tPrec@1 62.500 (62.482)\n",
      "EPOCH: 168 train Results: Prec@1 62.482 Loss: 1.0525\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0878 (1.0878)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6343 (1.2464)\tPrec@1 37.500 (55.460)\n",
      "EPOCH: 168 val Results: Prec@1 55.460 Loss: 1.2464\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [169][0/390]\tTime 0.006 (0.006)\tLoss 0.9461 (0.9461)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [169][78/390]\tTime 0.002 (0.003)\tLoss 0.9573 (0.9737)\tPrec@1 63.281 (65.694)\n",
      "Epoch: [169][156/390]\tTime 0.003 (0.003)\tLoss 0.9511 (0.9937)\tPrec@1 64.844 (65.033)\n",
      "Epoch: [169][234/390]\tTime 0.002 (0.003)\tLoss 0.9718 (1.0150)\tPrec@1 64.844 (64.003)\n",
      "Epoch: [169][312/390]\tTime 0.004 (0.003)\tLoss 1.0193 (1.0331)\tPrec@1 60.156 (63.239)\n",
      "Epoch: [169][390/390]\tTime 0.001 (0.003)\tLoss 1.0216 (1.0459)\tPrec@1 65.000 (62.794)\n",
      "EPOCH: 169 train Results: Prec@1 62.794 Loss: 1.0459\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1412 (1.1412)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1248 (1.2568)\tPrec@1 56.250 (55.200)\n",
      "EPOCH: 169 val Results: Prec@1 55.200 Loss: 1.2568\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [170][0/390]\tTime 0.003 (0.003)\tLoss 0.9778 (0.9778)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [170][78/390]\tTime 0.003 (0.003)\tLoss 1.0730 (0.9698)\tPrec@1 57.031 (65.882)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.003)\tLoss 0.8725 (0.9961)\tPrec@1 68.750 (64.903)\n",
      "Epoch: [170][234/390]\tTime 0.003 (0.003)\tLoss 1.1281 (1.0198)\tPrec@1 62.500 (63.906)\n",
      "Epoch: [170][312/390]\tTime 0.003 (0.003)\tLoss 1.0620 (1.0392)\tPrec@1 61.719 (63.179)\n",
      "Epoch: [170][390/390]\tTime 0.001 (0.003)\tLoss 1.0222 (1.0468)\tPrec@1 63.750 (62.900)\n",
      "EPOCH: 170 train Results: Prec@1 62.900 Loss: 1.0468\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1587 (1.1587)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1593 (1.2432)\tPrec@1 56.250 (55.700)\n",
      "EPOCH: 170 val Results: Prec@1 55.700 Loss: 1.2432\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [171][0/390]\tTime 0.006 (0.006)\tLoss 0.8857 (0.8857)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [171][78/390]\tTime 0.003 (0.003)\tLoss 0.9510 (0.9719)\tPrec@1 68.750 (66.139)\n",
      "Epoch: [171][156/390]\tTime 0.004 (0.003)\tLoss 1.1038 (1.0048)\tPrec@1 66.406 (64.898)\n",
      "Epoch: [171][234/390]\tTime 0.004 (0.003)\tLoss 0.9085 (1.0224)\tPrec@1 64.062 (63.946)\n",
      "Epoch: [171][312/390]\tTime 0.003 (0.003)\tLoss 0.9581 (1.0340)\tPrec@1 71.875 (63.666)\n",
      "Epoch: [171][390/390]\tTime 0.001 (0.003)\tLoss 1.2376 (1.0464)\tPrec@1 51.250 (63.026)\n",
      "EPOCH: 171 train Results: Prec@1 63.026 Loss: 1.0464\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1020 (1.1020)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.008 (0.001)\tLoss 1.6248 (1.2537)\tPrec@1 31.250 (55.890)\n",
      "EPOCH: 171 val Results: Prec@1 55.890 Loss: 1.2537\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [172][0/390]\tTime 0.005 (0.005)\tLoss 0.8201 (0.8201)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [172][78/390]\tTime 0.003 (0.004)\tLoss 0.8731 (0.9614)\tPrec@1 64.062 (66.634)\n",
      "Epoch: [172][156/390]\tTime 0.002 (0.003)\tLoss 1.0522 (0.9907)\tPrec@1 64.844 (65.038)\n",
      "Epoch: [172][234/390]\tTime 0.004 (0.003)\tLoss 1.1010 (1.0200)\tPrec@1 65.625 (64.049)\n",
      "Epoch: [172][312/390]\tTime 0.003 (0.003)\tLoss 1.1422 (1.0349)\tPrec@1 60.156 (63.379)\n",
      "Epoch: [172][390/390]\tTime 0.002 (0.003)\tLoss 1.1942 (1.0472)\tPrec@1 62.500 (62.898)\n",
      "EPOCH: 172 train Results: Prec@1 62.898 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1276 (1.1276)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5709 (1.2412)\tPrec@1 43.750 (55.700)\n",
      "EPOCH: 172 val Results: Prec@1 55.700 Loss: 1.2412\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [173][0/390]\tTime 0.004 (0.004)\tLoss 1.0277 (1.0277)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [173][78/390]\tTime 0.002 (0.003)\tLoss 1.1934 (0.9924)\tPrec@1 60.156 (64.775)\n",
      "Epoch: [173][156/390]\tTime 0.002 (0.003)\tLoss 1.0398 (1.0049)\tPrec@1 65.625 (64.416)\n",
      "Epoch: [173][234/390]\tTime 0.003 (0.003)\tLoss 0.9578 (1.0158)\tPrec@1 66.406 (63.840)\n",
      "Epoch: [173][312/390]\tTime 0.002 (0.003)\tLoss 1.0942 (1.0307)\tPrec@1 62.500 (63.384)\n",
      "Epoch: [173][390/390]\tTime 0.001 (0.003)\tLoss 1.0194 (1.0420)\tPrec@1 66.250 (62.966)\n",
      "EPOCH: 173 train Results: Prec@1 62.966 Loss: 1.0420\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1224 (1.1224)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3836 (1.2625)\tPrec@1 56.250 (55.200)\n",
      "EPOCH: 173 val Results: Prec@1 55.200 Loss: 1.2625\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [174][0/390]\tTime 0.003 (0.003)\tLoss 0.9796 (0.9796)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [174][78/390]\tTime 0.002 (0.003)\tLoss 0.9132 (0.9665)\tPrec@1 64.062 (66.268)\n",
      "Epoch: [174][156/390]\tTime 0.005 (0.003)\tLoss 0.9287 (0.9938)\tPrec@1 67.969 (65.093)\n",
      "Epoch: [174][234/390]\tTime 0.004 (0.003)\tLoss 1.0511 (1.0114)\tPrec@1 67.969 (64.432)\n",
      "Epoch: [174][312/390]\tTime 0.003 (0.003)\tLoss 1.0833 (1.0297)\tPrec@1 64.844 (63.718)\n",
      "Epoch: [174][390/390]\tTime 0.006 (0.003)\tLoss 1.2148 (1.0454)\tPrec@1 60.000 (63.070)\n",
      "EPOCH: 174 train Results: Prec@1 63.070 Loss: 1.0454\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1013 (1.1013)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5803 (1.2563)\tPrec@1 43.750 (56.160)\n",
      "EPOCH: 174 val Results: Prec@1 56.160 Loss: 1.2563\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [175][0/390]\tTime 0.003 (0.003)\tLoss 0.9380 (0.9380)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [175][78/390]\tTime 0.003 (0.003)\tLoss 1.1188 (0.9728)\tPrec@1 59.375 (65.902)\n",
      "Epoch: [175][156/390]\tTime 0.002 (0.003)\tLoss 1.0672 (0.9933)\tPrec@1 64.062 (64.794)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.003)\tLoss 1.0997 (1.0163)\tPrec@1 59.375 (64.116)\n",
      "Epoch: [175][312/390]\tTime 0.004 (0.003)\tLoss 1.1252 (1.0325)\tPrec@1 60.156 (63.626)\n",
      "Epoch: [175][390/390]\tTime 0.001 (0.003)\tLoss 1.0761 (1.0472)\tPrec@1 61.250 (62.920)\n",
      "EPOCH: 175 train Results: Prec@1 62.920 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1492 (1.1492)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3397 (1.2638)\tPrec@1 43.750 (55.310)\n",
      "EPOCH: 175 val Results: Prec@1 55.310 Loss: 1.2638\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [176][0/390]\tTime 0.002 (0.002)\tLoss 1.0720 (1.0720)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [176][78/390]\tTime 0.003 (0.003)\tLoss 1.1627 (0.9914)\tPrec@1 61.719 (65.269)\n",
      "Epoch: [176][156/390]\tTime 0.003 (0.003)\tLoss 1.0145 (1.0125)\tPrec@1 61.719 (64.326)\n",
      "Epoch: [176][234/390]\tTime 0.005 (0.003)\tLoss 1.0383 (1.0296)\tPrec@1 61.719 (63.660)\n",
      "Epoch: [176][312/390]\tTime 0.010 (0.003)\tLoss 1.0174 (1.0396)\tPrec@1 62.500 (63.191)\n",
      "Epoch: [176][390/390]\tTime 0.002 (0.003)\tLoss 1.0484 (1.0488)\tPrec@1 57.500 (62.848)\n",
      "EPOCH: 176 train Results: Prec@1 62.848 Loss: 1.0488\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0743 (1.0743)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4634 (1.2524)\tPrec@1 37.500 (55.870)\n",
      "EPOCH: 176 val Results: Prec@1 55.870 Loss: 1.2524\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [177][0/390]\tTime 0.007 (0.007)\tLoss 1.0310 (1.0310)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [177][78/390]\tTime 0.002 (0.004)\tLoss 0.8249 (0.9678)\tPrec@1 68.750 (66.426)\n",
      "Epoch: [177][156/390]\tTime 0.002 (0.004)\tLoss 1.1410 (1.0010)\tPrec@1 55.469 (64.759)\n",
      "Epoch: [177][234/390]\tTime 0.003 (0.003)\tLoss 1.0725 (1.0182)\tPrec@1 56.250 (63.906)\n",
      "Epoch: [177][312/390]\tTime 0.003 (0.003)\tLoss 1.0462 (1.0326)\tPrec@1 63.281 (63.351)\n",
      "Epoch: [177][390/390]\tTime 0.001 (0.003)\tLoss 1.0632 (1.0445)\tPrec@1 67.500 (63.078)\n",
      "EPOCH: 177 train Results: Prec@1 63.078 Loss: 1.0445\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1761 (1.1761)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5163 (1.2609)\tPrec@1 25.000 (55.580)\n",
      "EPOCH: 177 val Results: Prec@1 55.580 Loss: 1.2609\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [178][0/390]\tTime 0.004 (0.004)\tLoss 1.0442 (1.0442)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [178][78/390]\tTime 0.002 (0.003)\tLoss 0.9195 (0.9842)\tPrec@1 64.844 (65.249)\n",
      "Epoch: [178][156/390]\tTime 0.002 (0.003)\tLoss 1.1793 (1.0092)\tPrec@1 58.594 (64.023)\n",
      "Epoch: [178][234/390]\tTime 0.003 (0.003)\tLoss 1.1790 (1.0274)\tPrec@1 59.375 (63.241)\n",
      "Epoch: [178][312/390]\tTime 0.003 (0.003)\tLoss 1.0554 (1.0408)\tPrec@1 57.031 (62.814)\n",
      "Epoch: [178][390/390]\tTime 0.002 (0.003)\tLoss 1.1016 (1.0512)\tPrec@1 62.500 (62.500)\n",
      "EPOCH: 178 train Results: Prec@1 62.500 Loss: 1.0512\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1458 (1.1458)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2778 (1.2588)\tPrec@1 50.000 (55.320)\n",
      "EPOCH: 178 val Results: Prec@1 55.320 Loss: 1.2588\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [179][0/390]\tTime 0.006 (0.006)\tLoss 1.1130 (1.1130)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [179][78/390]\tTime 0.002 (0.003)\tLoss 1.0834 (0.9741)\tPrec@1 62.500 (65.783)\n",
      "Epoch: [179][156/390]\tTime 0.003 (0.003)\tLoss 1.1344 (1.0016)\tPrec@1 62.500 (64.665)\n",
      "Epoch: [179][234/390]\tTime 0.006 (0.003)\tLoss 1.2396 (1.0285)\tPrec@1 51.562 (63.587)\n",
      "Epoch: [179][312/390]\tTime 0.005 (0.003)\tLoss 0.9430 (1.0386)\tPrec@1 60.156 (63.129)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.2663 (1.0497)\tPrec@1 60.000 (62.744)\n",
      "EPOCH: 179 train Results: Prec@1 62.744 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0966 (1.0966)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4226 (1.2616)\tPrec@1 43.750 (55.190)\n",
      "EPOCH: 179 val Results: Prec@1 55.190 Loss: 1.2616\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [180][0/390]\tTime 0.006 (0.006)\tLoss 0.9571 (0.9571)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [180][78/390]\tTime 0.002 (0.003)\tLoss 0.9628 (0.9794)\tPrec@1 64.844 (65.269)\n",
      "Epoch: [180][156/390]\tTime 0.002 (0.003)\tLoss 1.0035 (0.9967)\tPrec@1 61.719 (64.635)\n",
      "Epoch: [180][234/390]\tTime 0.002 (0.003)\tLoss 1.0097 (1.0198)\tPrec@1 60.938 (63.836)\n",
      "Epoch: [180][312/390]\tTime 0.003 (0.003)\tLoss 0.9995 (1.0356)\tPrec@1 62.500 (63.246)\n",
      "Epoch: [180][390/390]\tTime 0.008 (0.003)\tLoss 1.2200 (1.0472)\tPrec@1 51.250 (62.786)\n",
      "EPOCH: 180 train Results: Prec@1 62.786 Loss: 1.0472\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1056 (1.1056)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3170 (1.2598)\tPrec@1 56.250 (55.170)\n",
      "EPOCH: 180 val Results: Prec@1 55.170 Loss: 1.2598\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [181][0/390]\tTime 0.003 (0.003)\tLoss 0.9139 (0.9139)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [181][78/390]\tTime 0.002 (0.003)\tLoss 0.9497 (0.9729)\tPrec@1 67.969 (65.398)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 0.8538 (0.9989)\tPrec@1 70.312 (64.053)\n",
      "Epoch: [181][234/390]\tTime 0.003 (0.003)\tLoss 1.0173 (1.0230)\tPrec@1 64.844 (63.314)\n",
      "Epoch: [181][312/390]\tTime 0.002 (0.003)\tLoss 1.1749 (1.0367)\tPrec@1 63.281 (62.827)\n",
      "Epoch: [181][390/390]\tTime 0.001 (0.003)\tLoss 1.1375 (1.0481)\tPrec@1 58.750 (62.540)\n",
      "EPOCH: 181 train Results: Prec@1 62.540 Loss: 1.0481\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0585 (1.0585)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2523 (1.2504)\tPrec@1 37.500 (55.400)\n",
      "EPOCH: 181 val Results: Prec@1 55.400 Loss: 1.2504\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [182][0/390]\tTime 0.002 (0.002)\tLoss 1.0420 (1.0420)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [182][78/390]\tTime 0.003 (0.003)\tLoss 1.1415 (0.9616)\tPrec@1 57.812 (66.070)\n",
      "Epoch: [182][156/390]\tTime 0.003 (0.003)\tLoss 1.0642 (0.9933)\tPrec@1 66.406 (64.849)\n",
      "Epoch: [182][234/390]\tTime 0.004 (0.003)\tLoss 1.1778 (1.0185)\tPrec@1 55.469 (63.836)\n",
      "Epoch: [182][312/390]\tTime 0.002 (0.003)\tLoss 1.0654 (1.0327)\tPrec@1 59.375 (63.344)\n",
      "Epoch: [182][390/390]\tTime 0.002 (0.003)\tLoss 1.2314 (1.0425)\tPrec@1 50.000 (62.918)\n",
      "EPOCH: 182 train Results: Prec@1 62.918 Loss: 1.0425\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1319 (1.1319)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4710 (1.2556)\tPrec@1 37.500 (55.530)\n",
      "EPOCH: 182 val Results: Prec@1 55.530 Loss: 1.2556\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [183][0/390]\tTime 0.006 (0.006)\tLoss 0.9138 (0.9138)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [183][78/390]\tTime 0.002 (0.003)\tLoss 1.0230 (0.9693)\tPrec@1 59.375 (65.655)\n",
      "Epoch: [183][156/390]\tTime 0.003 (0.003)\tLoss 0.9735 (0.9914)\tPrec@1 67.969 (64.779)\n",
      "Epoch: [183][234/390]\tTime 0.004 (0.003)\tLoss 1.0751 (1.0152)\tPrec@1 60.156 (63.933)\n",
      "Epoch: [183][312/390]\tTime 0.002 (0.003)\tLoss 1.0940 (1.0297)\tPrec@1 59.375 (63.294)\n",
      "Epoch: [183][390/390]\tTime 0.002 (0.003)\tLoss 1.1205 (1.0461)\tPrec@1 60.000 (62.844)\n",
      "EPOCH: 183 train Results: Prec@1 62.844 Loss: 1.0461\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1055 (1.1055)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6359 (1.2553)\tPrec@1 25.000 (55.550)\n",
      "EPOCH: 183 val Results: Prec@1 55.550 Loss: 1.2553\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [184][0/390]\tTime 0.004 (0.004)\tLoss 0.8062 (0.8062)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [184][78/390]\tTime 0.003 (0.003)\tLoss 1.0619 (0.9591)\tPrec@1 65.625 (66.525)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.003)\tLoss 1.0596 (0.9905)\tPrec@1 64.062 (64.993)\n",
      "Epoch: [184][234/390]\tTime 0.002 (0.003)\tLoss 1.1303 (1.0189)\tPrec@1 58.594 (63.989)\n",
      "Epoch: [184][312/390]\tTime 0.002 (0.003)\tLoss 1.1243 (1.0371)\tPrec@1 57.031 (63.279)\n",
      "Epoch: [184][390/390]\tTime 0.003 (0.003)\tLoss 1.3938 (1.0474)\tPrec@1 57.500 (62.824)\n",
      "EPOCH: 184 train Results: Prec@1 62.824 Loss: 1.0474\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0825 (1.0825)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2809 (1.2525)\tPrec@1 37.500 (55.940)\n",
      "EPOCH: 184 val Results: Prec@1 55.940 Loss: 1.2525\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [185][0/390]\tTime 0.003 (0.003)\tLoss 0.9649 (0.9649)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [185][78/390]\tTime 0.005 (0.003)\tLoss 1.0796 (0.9818)\tPrec@1 57.031 (65.091)\n",
      "Epoch: [185][156/390]\tTime 0.002 (0.003)\tLoss 0.9345 (1.0005)\tPrec@1 70.312 (64.371)\n",
      "Epoch: [185][234/390]\tTime 0.003 (0.003)\tLoss 1.0679 (1.0111)\tPrec@1 62.500 (64.059)\n",
      "Epoch: [185][312/390]\tTime 0.003 (0.003)\tLoss 0.9294 (1.0268)\tPrec@1 65.625 (63.521)\n",
      "Epoch: [185][390/390]\tTime 0.007 (0.003)\tLoss 1.1245 (1.0470)\tPrec@1 56.250 (62.688)\n",
      "EPOCH: 185 train Results: Prec@1 62.688 Loss: 1.0470\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1204 (1.1204)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2590 (1.2654)\tPrec@1 50.000 (55.290)\n",
      "EPOCH: 185 val Results: Prec@1 55.290 Loss: 1.2654\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [186][0/390]\tTime 0.003 (0.003)\tLoss 0.9816 (0.9816)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [186][78/390]\tTime 0.003 (0.003)\tLoss 0.9760 (0.9799)\tPrec@1 66.406 (65.674)\n",
      "Epoch: [186][156/390]\tTime 0.002 (0.003)\tLoss 0.9215 (1.0024)\tPrec@1 62.500 (64.620)\n",
      "Epoch: [186][234/390]\tTime 0.002 (0.003)\tLoss 1.0623 (1.0230)\tPrec@1 60.938 (63.816)\n",
      "Epoch: [186][312/390]\tTime 0.009 (0.003)\tLoss 1.0597 (1.0402)\tPrec@1 65.625 (63.014)\n",
      "Epoch: [186][390/390]\tTime 0.003 (0.003)\tLoss 0.9753 (1.0470)\tPrec@1 66.250 (62.686)\n",
      "EPOCH: 186 train Results: Prec@1 62.686 Loss: 1.0470\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0657 (1.0657)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5094 (1.2667)\tPrec@1 37.500 (55.110)\n",
      "EPOCH: 186 val Results: Prec@1 55.110 Loss: 1.2667\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [187][0/390]\tTime 0.004 (0.004)\tLoss 1.0221 (1.0221)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [187][78/390]\tTime 0.003 (0.003)\tLoss 0.9606 (0.9698)\tPrec@1 70.312 (66.021)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.2455 (0.9916)\tPrec@1 50.000 (64.928)\n",
      "Epoch: [187][234/390]\tTime 0.003 (0.003)\tLoss 1.0200 (1.0167)\tPrec@1 65.625 (63.906)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.1157 (1.0356)\tPrec@1 60.938 (63.154)\n",
      "Epoch: [187][390/390]\tTime 0.003 (0.003)\tLoss 1.1646 (1.0478)\tPrec@1 43.750 (62.742)\n",
      "EPOCH: 187 train Results: Prec@1 62.742 Loss: 1.0478\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1042 (1.1042)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3300 (1.2547)\tPrec@1 50.000 (55.520)\n",
      "EPOCH: 187 val Results: Prec@1 55.520 Loss: 1.2547\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [188][0/390]\tTime 0.004 (0.004)\tLoss 0.8969 (0.8969)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [188][78/390]\tTime 0.005 (0.003)\tLoss 1.1906 (0.9580)\tPrec@1 57.031 (66.495)\n",
      "Epoch: [188][156/390]\tTime 0.004 (0.003)\tLoss 1.0506 (0.9908)\tPrec@1 64.844 (65.152)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.003)\tLoss 1.1124 (1.0102)\tPrec@1 56.250 (64.355)\n",
      "Epoch: [188][312/390]\tTime 0.005 (0.003)\tLoss 1.2388 (1.0274)\tPrec@1 60.156 (63.626)\n",
      "Epoch: [188][390/390]\tTime 0.003 (0.003)\tLoss 1.0935 (1.0428)\tPrec@1 61.250 (62.920)\n",
      "EPOCH: 188 train Results: Prec@1 62.920 Loss: 1.0428\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1313 (1.1313)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2982 (1.2540)\tPrec@1 50.000 (55.790)\n",
      "EPOCH: 188 val Results: Prec@1 55.790 Loss: 1.2540\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [189][0/390]\tTime 0.004 (0.004)\tLoss 1.0129 (1.0129)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.004)\tLoss 0.9230 (0.9637)\tPrec@1 72.656 (65.862)\n",
      "Epoch: [189][156/390]\tTime 0.003 (0.003)\tLoss 1.2959 (0.9982)\tPrec@1 55.469 (64.585)\n",
      "Epoch: [189][234/390]\tTime 0.003 (0.003)\tLoss 1.0847 (1.0120)\tPrec@1 63.281 (63.936)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.003)\tLoss 1.1934 (1.0294)\tPrec@1 56.250 (63.216)\n",
      "Epoch: [189][390/390]\tTime 0.001 (0.003)\tLoss 1.2625 (1.0437)\tPrec@1 51.250 (62.784)\n",
      "EPOCH: 189 train Results: Prec@1 62.784 Loss: 1.0437\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1279 (1.1279)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5225 (1.2483)\tPrec@1 50.000 (55.990)\n",
      "EPOCH: 189 val Results: Prec@1 55.990 Loss: 1.2483\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [190][0/390]\tTime 0.005 (0.005)\tLoss 0.8330 (0.8330)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.003)\tLoss 0.9152 (0.9669)\tPrec@1 64.062 (66.050)\n",
      "Epoch: [190][156/390]\tTime 0.002 (0.003)\tLoss 1.1347 (0.9924)\tPrec@1 58.594 (64.903)\n",
      "Epoch: [190][234/390]\tTime 0.003 (0.003)\tLoss 0.9945 (1.0181)\tPrec@1 67.188 (63.896)\n",
      "Epoch: [190][312/390]\tTime 0.002 (0.003)\tLoss 0.9400 (1.0351)\tPrec@1 64.062 (63.341)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.003)\tLoss 1.2552 (1.0497)\tPrec@1 55.000 (62.694)\n",
      "EPOCH: 190 train Results: Prec@1 62.694 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1591 (1.1591)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6958 (1.2511)\tPrec@1 37.500 (55.280)\n",
      "EPOCH: 190 val Results: Prec@1 55.280 Loss: 1.2511\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [191][0/390]\tTime 0.002 (0.002)\tLoss 0.9930 (0.9930)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [191][78/390]\tTime 0.004 (0.003)\tLoss 0.9274 (0.9745)\tPrec@1 69.531 (65.813)\n",
      "Epoch: [191][156/390]\tTime 0.002 (0.003)\tLoss 0.9955 (0.9990)\tPrec@1 64.844 (64.864)\n",
      "Epoch: [191][234/390]\tTime 0.008 (0.003)\tLoss 0.9437 (1.0219)\tPrec@1 65.625 (63.959)\n",
      "Epoch: [191][312/390]\tTime 0.003 (0.003)\tLoss 0.9932 (1.0340)\tPrec@1 62.500 (63.354)\n",
      "Epoch: [191][390/390]\tTime 0.010 (0.003)\tLoss 1.1576 (1.0478)\tPrec@1 57.500 (62.730)\n",
      "EPOCH: 191 train Results: Prec@1 62.730 Loss: 1.0478\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0531 (1.0531)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3600 (1.2481)\tPrec@1 56.250 (55.860)\n",
      "EPOCH: 191 val Results: Prec@1 55.860 Loss: 1.2481\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [192][0/390]\tTime 0.002 (0.002)\tLoss 0.7903 (0.7903)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [192][78/390]\tTime 0.002 (0.003)\tLoss 1.0245 (0.9830)\tPrec@1 60.938 (64.992)\n",
      "Epoch: [192][156/390]\tTime 0.003 (0.003)\tLoss 0.9877 (1.0004)\tPrec@1 65.625 (64.341)\n",
      "Epoch: [192][234/390]\tTime 0.002 (0.003)\tLoss 1.0540 (1.0242)\tPrec@1 64.062 (63.590)\n",
      "Epoch: [192][312/390]\tTime 0.004 (0.003)\tLoss 1.1390 (1.0338)\tPrec@1 54.688 (63.216)\n",
      "Epoch: [192][390/390]\tTime 0.002 (0.003)\tLoss 0.9822 (1.0444)\tPrec@1 65.000 (62.898)\n",
      "EPOCH: 192 train Results: Prec@1 62.898 Loss: 1.0444\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1119 (1.1119)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9618 (1.2557)\tPrec@1 75.000 (55.700)\n",
      "EPOCH: 192 val Results: Prec@1 55.700 Loss: 1.2557\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [193][0/390]\tTime 0.005 (0.005)\tLoss 0.8345 (0.8345)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [193][78/390]\tTime 0.003 (0.003)\tLoss 0.9295 (0.9751)\tPrec@1 64.844 (65.556)\n",
      "Epoch: [193][156/390]\tTime 0.002 (0.003)\tLoss 0.9883 (1.0006)\tPrec@1 64.062 (64.545)\n",
      "Epoch: [193][234/390]\tTime 0.003 (0.003)\tLoss 1.0185 (1.0195)\tPrec@1 64.844 (63.753)\n",
      "Epoch: [193][312/390]\tTime 0.009 (0.003)\tLoss 1.2772 (1.0356)\tPrec@1 57.031 (63.236)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.003)\tLoss 1.0590 (1.0447)\tPrec@1 58.750 (62.936)\n",
      "EPOCH: 193 train Results: Prec@1 62.936 Loss: 1.0447\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1199 (1.1199)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1907 (1.2537)\tPrec@1 37.500 (55.890)\n",
      "EPOCH: 193 val Results: Prec@1 55.890 Loss: 1.2537\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [194][0/390]\tTime 0.003 (0.003)\tLoss 0.9055 (0.9055)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [194][78/390]\tTime 0.002 (0.003)\tLoss 0.9322 (0.9796)\tPrec@1 67.188 (65.526)\n",
      "Epoch: [194][156/390]\tTime 0.004 (0.003)\tLoss 0.9899 (0.9995)\tPrec@1 62.500 (64.505)\n",
      "Epoch: [194][234/390]\tTime 0.002 (0.003)\tLoss 1.0399 (1.0203)\tPrec@1 64.844 (63.604)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.003)\tLoss 1.0345 (1.0347)\tPrec@1 59.375 (63.174)\n",
      "Epoch: [194][390/390]\tTime 0.002 (0.003)\tLoss 1.1787 (1.0445)\tPrec@1 58.750 (62.766)\n",
      "EPOCH: 194 train Results: Prec@1 62.766 Loss: 1.0445\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1332 (1.1332)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2050 (1.2472)\tPrec@1 50.000 (55.560)\n",
      "EPOCH: 194 val Results: Prec@1 55.560 Loss: 1.2472\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [195][0/390]\tTime 0.002 (0.002)\tLoss 0.9004 (0.9004)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [195][78/390]\tTime 0.002 (0.003)\tLoss 0.9925 (0.9635)\tPrec@1 62.500 (66.011)\n",
      "Epoch: [195][156/390]\tTime 0.002 (0.003)\tLoss 0.9735 (0.9900)\tPrec@1 67.969 (64.894)\n",
      "Epoch: [195][234/390]\tTime 0.002 (0.003)\tLoss 1.1091 (1.0108)\tPrec@1 62.500 (64.245)\n",
      "Epoch: [195][312/390]\tTime 0.002 (0.003)\tLoss 1.2993 (1.0287)\tPrec@1 57.812 (63.706)\n",
      "Epoch: [195][390/390]\tTime 0.001 (0.003)\tLoss 1.0762 (1.0370)\tPrec@1 65.000 (63.392)\n",
      "EPOCH: 195 train Results: Prec@1 63.392 Loss: 1.0370\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1353 (1.1353)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4918 (1.2603)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 195 val Results: Prec@1 55.150 Loss: 1.2603\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [196][0/390]\tTime 0.005 (0.005)\tLoss 0.9135 (0.9135)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [196][78/390]\tTime 0.009 (0.003)\tLoss 1.0043 (0.9699)\tPrec@1 60.156 (65.002)\n",
      "Epoch: [196][156/390]\tTime 0.005 (0.003)\tLoss 1.2323 (0.9926)\tPrec@1 59.375 (64.675)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.003)\tLoss 0.9840 (1.0238)\tPrec@1 64.062 (63.570)\n",
      "Epoch: [196][312/390]\tTime 0.004 (0.003)\tLoss 1.1704 (1.0368)\tPrec@1 57.031 (63.059)\n",
      "Epoch: [196][390/390]\tTime 0.003 (0.003)\tLoss 0.9349 (1.0443)\tPrec@1 62.500 (62.736)\n",
      "EPOCH: 196 train Results: Prec@1 62.736 Loss: 1.0443\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0959 (1.0959)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3160 (1.2418)\tPrec@1 43.750 (56.180)\n",
      "EPOCH: 196 val Results: Prec@1 56.180 Loss: 1.2418\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [197][0/390]\tTime 0.003 (0.003)\tLoss 0.8713 (0.8713)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [197][78/390]\tTime 0.002 (0.003)\tLoss 0.9768 (0.9515)\tPrec@1 66.406 (66.070)\n",
      "Epoch: [197][156/390]\tTime 0.004 (0.003)\tLoss 0.9895 (0.9817)\tPrec@1 64.062 (65.381)\n",
      "Epoch: [197][234/390]\tTime 0.002 (0.003)\tLoss 1.0439 (1.0109)\tPrec@1 67.969 (64.192)\n",
      "Epoch: [197][312/390]\tTime 0.002 (0.003)\tLoss 1.0722 (1.0338)\tPrec@1 57.812 (63.429)\n",
      "Epoch: [197][390/390]\tTime 0.001 (0.003)\tLoss 1.0184 (1.0464)\tPrec@1 56.250 (62.916)\n",
      "EPOCH: 197 train Results: Prec@1 62.916 Loss: 1.0464\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2252 (1.2252)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.005 (0.001)\tLoss 1.2355 (1.2554)\tPrec@1 43.750 (55.610)\n",
      "EPOCH: 197 val Results: Prec@1 55.610 Loss: 1.2554\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [198][0/390]\tTime 0.006 (0.006)\tLoss 0.9913 (0.9913)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [198][78/390]\tTime 0.004 (0.004)\tLoss 0.9574 (0.9690)\tPrec@1 65.625 (65.309)\n",
      "Epoch: [198][156/390]\tTime 0.003 (0.003)\tLoss 0.8438 (0.9934)\tPrec@1 68.750 (64.640)\n",
      "Epoch: [198][234/390]\tTime 0.002 (0.003)\tLoss 1.3072 (1.0149)\tPrec@1 59.375 (64.089)\n",
      "Epoch: [198][312/390]\tTime 0.003 (0.003)\tLoss 0.9062 (1.0295)\tPrec@1 67.969 (63.516)\n",
      "Epoch: [198][390/390]\tTime 0.001 (0.003)\tLoss 1.0523 (1.0414)\tPrec@1 58.750 (63.150)\n",
      "EPOCH: 198 train Results: Prec@1 63.150 Loss: 1.0414\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1853 (1.1853)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2713 (1.2585)\tPrec@1 50.000 (54.750)\n",
      "EPOCH: 198 val Results: Prec@1 54.750 Loss: 1.2585\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [199][0/390]\tTime 0.003 (0.003)\tLoss 0.9385 (0.9385)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [199][78/390]\tTime 0.002 (0.003)\tLoss 0.9224 (0.9600)\tPrec@1 66.406 (66.337)\n",
      "Epoch: [199][156/390]\tTime 0.003 (0.003)\tLoss 1.0251 (0.9838)\tPrec@1 64.062 (65.312)\n",
      "Epoch: [199][234/390]\tTime 0.007 (0.003)\tLoss 1.0470 (1.0087)\tPrec@1 60.156 (64.232)\n",
      "Epoch: [199][312/390]\tTime 0.003 (0.003)\tLoss 1.1340 (1.0278)\tPrec@1 61.719 (63.503)\n",
      "Epoch: [199][390/390]\tTime 0.001 (0.003)\tLoss 1.0075 (1.0396)\tPrec@1 67.500 (63.102)\n",
      "EPOCH: 199 train Results: Prec@1 63.102 Loss: 1.0396\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1861 (1.1861)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2950 (1.2502)\tPrec@1 43.750 (55.730)\n",
      "EPOCH: 199 val Results: Prec@1 55.730 Loss: 1.2502\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "current lr 5.00000e-03\n",
      "Epoch: [200][0/390]\tTime 0.004 (0.004)\tLoss 0.8784 (0.8784)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [200][78/390]\tTime 0.003 (0.003)\tLoss 0.9935 (0.9716)\tPrec@1 65.625 (65.724)\n",
      "Epoch: [200][156/390]\tTime 0.003 (0.003)\tLoss 0.8013 (1.0008)\tPrec@1 70.312 (64.366)\n",
      "Epoch: [200][234/390]\tTime 0.002 (0.003)\tLoss 1.0529 (1.0167)\tPrec@1 60.938 (63.747)\n",
      "Epoch: [200][312/390]\tTime 0.003 (0.003)\tLoss 1.2514 (1.0276)\tPrec@1 52.344 (63.321)\n",
      "Epoch: [200][390/390]\tTime 0.002 (0.003)\tLoss 1.2870 (1.0412)\tPrec@1 52.500 (62.850)\n",
      "EPOCH: 200 train Results: Prec@1 62.850 Loss: 1.0412\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1336 (1.1336)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0008 (1.2516)\tPrec@1 75.000 (55.740)\n",
      "EPOCH: 200 val Results: Prec@1 55.740 Loss: 1.2516\n",
      "Best Prec@1: 56.650\n",
      "\n",
      "End time:  Thu Apr  4 23:08:58 2024\n",
      "train executed in 258.5027 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.005, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer4 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:08:58 2024\n",
      "current lr 5.00000e-02\n",
      "Epoch: [1][0/390]\tTime 0.007 (0.007)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.002 (0.003)\tLoss 1.8188 (2.5696)\tPrec@1 35.938 (29.114)\n",
      "Epoch: [1][156/390]\tTime 0.002 (0.003)\tLoss 1.9599 (2.1665)\tPrec@1 29.688 (33.435)\n",
      "Epoch: [1][234/390]\tTime 0.003 (0.003)\tLoss 1.6756 (2.0000)\tPrec@1 46.094 (35.881)\n",
      "Epoch: [1][312/390]\tTime 0.006 (0.003)\tLoss 1.6016 (1.9032)\tPrec@1 42.188 (37.468)\n",
      "Epoch: [1][390/390]\tTime 0.001 (0.003)\tLoss 1.5402 (1.8414)\tPrec@1 41.250 (38.444)\n",
      "EPOCH: 1 train Results: Prec@1 38.444 Loss: 1.8414\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.3736 (1.3736)\tPrec@1 48.438 (48.438)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4550 (1.4873)\tPrec@1 37.500 (46.810)\n",
      "EPOCH: 1 val Results: Prec@1 46.810 Loss: 1.4873\n",
      "Best Prec@1: 46.810\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [2][0/390]\tTime 0.003 (0.003)\tLoss 1.5012 (1.5012)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [2][78/390]\tTime 0.002 (0.004)\tLoss 1.3811 (1.4727)\tPrec@1 52.344 (47.330)\n",
      "Epoch: [2][156/390]\tTime 0.002 (0.004)\tLoss 1.5013 (1.4606)\tPrec@1 43.750 (47.666)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.003)\tLoss 1.5439 (1.4596)\tPrec@1 44.531 (47.779)\n",
      "Epoch: [2][312/390]\tTime 0.002 (0.003)\tLoss 1.6764 (1.4544)\tPrec@1 36.719 (47.816)\n",
      "Epoch: [2][390/390]\tTime 0.001 (0.003)\tLoss 1.3668 (1.4469)\tPrec@1 48.750 (48.134)\n",
      "EPOCH: 2 train Results: Prec@1 48.134 Loss: 1.4469\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2507 (1.2507)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5007 (1.3926)\tPrec@1 25.000 (49.840)\n",
      "EPOCH: 2 val Results: Prec@1 49.840 Loss: 1.3926\n",
      "Best Prec@1: 49.840\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [3][0/390]\tTime 0.002 (0.002)\tLoss 1.3419 (1.3419)\tPrec@1 45.312 (45.312)\n",
      "Epoch: [3][78/390]\tTime 0.002 (0.003)\tLoss 1.3687 (1.3578)\tPrec@1 51.562 (51.315)\n",
      "Epoch: [3][156/390]\tTime 0.004 (0.003)\tLoss 1.3243 (1.3509)\tPrec@1 50.781 (51.602)\n",
      "Epoch: [3][234/390]\tTime 0.003 (0.003)\tLoss 1.4300 (1.3571)\tPrec@1 46.875 (51.556)\n",
      "Epoch: [3][312/390]\tTime 0.003 (0.003)\tLoss 1.4384 (1.3595)\tPrec@1 46.875 (51.398)\n",
      "Epoch: [3][390/390]\tTime 0.001 (0.003)\tLoss 1.4307 (1.3622)\tPrec@1 52.500 (51.314)\n",
      "EPOCH: 3 train Results: Prec@1 51.314 Loss: 1.3622\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1742 (1.1742)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2618 (1.3461)\tPrec@1 37.500 (51.470)\n",
      "EPOCH: 3 val Results: Prec@1 51.470 Loss: 1.3461\n",
      "Best Prec@1: 51.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [4][0/390]\tTime 0.003 (0.003)\tLoss 1.3356 (1.3356)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [4][78/390]\tTime 0.002 (0.003)\tLoss 1.3319 (1.2904)\tPrec@1 56.250 (53.995)\n",
      "Epoch: [4][156/390]\tTime 0.003 (0.003)\tLoss 1.2949 (1.2981)\tPrec@1 50.781 (53.414)\n",
      "Epoch: [4][234/390]\tTime 0.002 (0.003)\tLoss 1.3315 (1.3025)\tPrec@1 45.312 (53.208)\n",
      "Epoch: [4][312/390]\tTime 0.004 (0.003)\tLoss 1.4059 (1.3114)\tPrec@1 48.438 (52.880)\n",
      "Epoch: [4][390/390]\tTime 0.001 (0.003)\tLoss 1.3346 (1.3170)\tPrec@1 47.500 (52.644)\n",
      "EPOCH: 4 train Results: Prec@1 52.644 Loss: 1.3170\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1858 (1.1858)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2150 (1.3214)\tPrec@1 37.500 (52.190)\n",
      "EPOCH: 4 val Results: Prec@1 52.190 Loss: 1.3214\n",
      "Best Prec@1: 52.190\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [5][0/390]\tTime 0.002 (0.002)\tLoss 1.2751 (1.2751)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [5][78/390]\tTime 0.004 (0.003)\tLoss 1.3299 (1.2261)\tPrec@1 51.562 (56.102)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.003)\tLoss 1.2906 (1.2594)\tPrec@1 54.688 (54.991)\n",
      "Epoch: [5][234/390]\tTime 0.003 (0.003)\tLoss 1.3224 (1.2656)\tPrec@1 57.031 (54.727)\n",
      "Epoch: [5][312/390]\tTime 0.003 (0.003)\tLoss 1.2890 (1.2735)\tPrec@1 49.219 (54.301)\n",
      "Epoch: [5][390/390]\tTime 0.002 (0.003)\tLoss 1.2639 (1.2802)\tPrec@1 58.750 (54.114)\n",
      "EPOCH: 5 train Results: Prec@1 54.114 Loss: 1.2802\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1894 (1.1894)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3859 (1.3306)\tPrec@1 37.500 (52.350)\n",
      "EPOCH: 5 val Results: Prec@1 52.350 Loss: 1.3306\n",
      "Best Prec@1: 52.350\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [6][0/390]\tTime 0.004 (0.004)\tLoss 1.2816 (1.2816)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [6][78/390]\tTime 0.002 (0.003)\tLoss 1.3086 (1.2018)\tPrec@1 53.125 (57.239)\n",
      "Epoch: [6][156/390]\tTime 0.003 (0.003)\tLoss 1.1388 (1.2187)\tPrec@1 57.812 (56.568)\n",
      "Epoch: [6][234/390]\tTime 0.003 (0.003)\tLoss 1.3002 (1.2352)\tPrec@1 54.688 (55.901)\n",
      "Epoch: [6][312/390]\tTime 0.002 (0.003)\tLoss 1.4017 (1.2510)\tPrec@1 50.781 (55.399)\n",
      "Epoch: [6][390/390]\tTime 0.001 (0.003)\tLoss 1.4604 (1.2573)\tPrec@1 48.750 (55.220)\n",
      "EPOCH: 6 train Results: Prec@1 55.220 Loss: 1.2573\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1685 (1.1685)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3487 (1.3056)\tPrec@1 50.000 (53.150)\n",
      "EPOCH: 6 val Results: Prec@1 53.150 Loss: 1.3056\n",
      "Best Prec@1: 53.150\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [7][0/390]\tTime 0.005 (0.005)\tLoss 1.2931 (1.2931)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [7][78/390]\tTime 0.005 (0.003)\tLoss 1.2229 (1.1855)\tPrec@1 54.688 (57.991)\n",
      "Epoch: [7][156/390]\tTime 0.002 (0.003)\tLoss 1.4374 (1.2069)\tPrec@1 47.656 (56.867)\n",
      "Epoch: [7][234/390]\tTime 0.003 (0.003)\tLoss 1.2747 (1.2205)\tPrec@1 57.812 (56.247)\n",
      "Epoch: [7][312/390]\tTime 0.003 (0.003)\tLoss 1.3267 (1.2257)\tPrec@1 53.125 (56.118)\n",
      "Epoch: [7][390/390]\tTime 0.002 (0.003)\tLoss 1.3731 (1.2350)\tPrec@1 53.750 (55.938)\n",
      "EPOCH: 7 train Results: Prec@1 55.938 Loss: 1.2350\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1648 (1.1648)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1925 (1.2930)\tPrec@1 50.000 (53.720)\n",
      "EPOCH: 7 val Results: Prec@1 53.720 Loss: 1.2930\n",
      "Best Prec@1: 53.720\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [8][0/390]\tTime 0.007 (0.007)\tLoss 1.1494 (1.1494)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [8][78/390]\tTime 0.003 (0.003)\tLoss 1.1556 (1.1648)\tPrec@1 58.594 (58.732)\n",
      "Epoch: [8][156/390]\tTime 0.003 (0.003)\tLoss 1.3564 (1.1940)\tPrec@1 50.000 (56.981)\n",
      "Epoch: [8][234/390]\tTime 0.010 (0.003)\tLoss 1.1316 (1.2043)\tPrec@1 63.281 (56.636)\n",
      "Epoch: [8][312/390]\tTime 0.006 (0.003)\tLoss 1.0779 (1.2112)\tPrec@1 62.500 (56.555)\n",
      "Epoch: [8][390/390]\tTime 0.004 (0.003)\tLoss 1.2901 (1.2170)\tPrec@1 52.500 (56.308)\n",
      "EPOCH: 8 train Results: Prec@1 56.308 Loss: 1.2170\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1455 (1.1455)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4124 (1.2908)\tPrec@1 31.250 (53.590)\n",
      "EPOCH: 8 val Results: Prec@1 53.590 Loss: 1.2908\n",
      "Best Prec@1: 53.720\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [9][0/390]\tTime 0.003 (0.003)\tLoss 1.1328 (1.1328)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [9][78/390]\tTime 0.006 (0.003)\tLoss 1.1703 (1.1364)\tPrec@1 57.812 (59.464)\n",
      "Epoch: [9][156/390]\tTime 0.002 (0.003)\tLoss 1.4414 (1.1644)\tPrec@1 48.438 (58.544)\n",
      "Epoch: [9][234/390]\tTime 0.004 (0.003)\tLoss 1.3301 (1.1831)\tPrec@1 52.344 (57.746)\n",
      "Epoch: [9][312/390]\tTime 0.002 (0.003)\tLoss 1.2342 (1.1926)\tPrec@1 52.344 (57.448)\n",
      "Epoch: [9][390/390]\tTime 0.001 (0.003)\tLoss 1.4747 (1.2018)\tPrec@1 48.750 (57.028)\n",
      "EPOCH: 9 train Results: Prec@1 57.028 Loss: 1.2018\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1676 (1.1676)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2356 (1.2877)\tPrec@1 31.250 (53.420)\n",
      "EPOCH: 9 val Results: Prec@1 53.420 Loss: 1.2877\n",
      "Best Prec@1: 53.720\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [10][0/390]\tTime 0.003 (0.003)\tLoss 1.1789 (1.1789)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [10][78/390]\tTime 0.002 (0.003)\tLoss 0.9829 (1.1228)\tPrec@1 60.156 (59.553)\n",
      "Epoch: [10][156/390]\tTime 0.003 (0.003)\tLoss 1.1185 (1.1525)\tPrec@1 60.938 (58.609)\n",
      "Epoch: [10][234/390]\tTime 0.006 (0.003)\tLoss 1.1962 (1.1705)\tPrec@1 56.250 (57.793)\n",
      "Epoch: [10][312/390]\tTime 0.002 (0.003)\tLoss 1.2337 (1.1825)\tPrec@1 53.906 (57.565)\n",
      "Epoch: [10][390/390]\tTime 0.008 (0.003)\tLoss 1.0216 (1.1893)\tPrec@1 63.750 (57.344)\n",
      "EPOCH: 10 train Results: Prec@1 57.344 Loss: 1.1893\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1882 (1.1882)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4069 (1.2833)\tPrec@1 43.750 (53.600)\n",
      "EPOCH: 10 val Results: Prec@1 53.600 Loss: 1.2833\n",
      "Best Prec@1: 53.720\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [11][0/390]\tTime 0.003 (0.003)\tLoss 1.0512 (1.0512)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [11][78/390]\tTime 0.004 (0.003)\tLoss 1.0874 (1.1190)\tPrec@1 64.062 (59.869)\n",
      "Epoch: [11][156/390]\tTime 0.008 (0.003)\tLoss 1.0058 (1.1404)\tPrec@1 63.281 (59.012)\n",
      "Epoch: [11][234/390]\tTime 0.002 (0.003)\tLoss 1.2664 (1.1588)\tPrec@1 57.812 (58.255)\n",
      "Epoch: [11][312/390]\tTime 0.006 (0.003)\tLoss 1.0864 (1.1696)\tPrec@1 64.844 (57.837)\n",
      "Epoch: [11][390/390]\tTime 0.002 (0.003)\tLoss 1.1246 (1.1797)\tPrec@1 58.750 (57.526)\n",
      "EPOCH: 11 train Results: Prec@1 57.526 Loss: 1.1797\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1324 (1.1324)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5375 (1.2760)\tPrec@1 43.750 (54.250)\n",
      "EPOCH: 11 val Results: Prec@1 54.250 Loss: 1.2760\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [12][0/390]\tTime 0.002 (0.002)\tLoss 1.0133 (1.0133)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [12][78/390]\tTime 0.002 (0.004)\tLoss 1.0254 (1.0958)\tPrec@1 64.844 (61.175)\n",
      "Epoch: [12][156/390]\tTime 0.004 (0.003)\tLoss 1.2558 (1.1229)\tPrec@1 58.594 (60.231)\n",
      "Epoch: [12][234/390]\tTime 0.007 (0.003)\tLoss 1.0696 (1.1439)\tPrec@1 66.406 (59.122)\n",
      "Epoch: [12][312/390]\tTime 0.008 (0.003)\tLoss 1.1382 (1.1583)\tPrec@1 56.250 (58.776)\n",
      "Epoch: [12][390/390]\tTime 0.001 (0.003)\tLoss 0.9858 (1.1681)\tPrec@1 65.000 (58.508)\n",
      "EPOCH: 12 train Results: Prec@1 58.508 Loss: 1.1681\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1276 (1.1276)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3409 (1.2875)\tPrec@1 43.750 (53.800)\n",
      "EPOCH: 12 val Results: Prec@1 53.800 Loss: 1.2875\n",
      "Best Prec@1: 54.250\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [13][0/390]\tTime 0.003 (0.003)\tLoss 1.0728 (1.0728)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.0344 (1.0967)\tPrec@1 63.281 (60.848)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.004)\tLoss 1.0571 (1.1176)\tPrec@1 67.969 (60.111)\n",
      "Epoch: [13][234/390]\tTime 0.005 (0.003)\tLoss 1.3708 (1.1390)\tPrec@1 53.125 (59.318)\n",
      "Epoch: [13][312/390]\tTime 0.005 (0.003)\tLoss 1.2263 (1.1535)\tPrec@1 58.594 (58.761)\n",
      "Epoch: [13][390/390]\tTime 0.001 (0.003)\tLoss 1.0859 (1.1599)\tPrec@1 58.750 (58.500)\n",
      "EPOCH: 13 train Results: Prec@1 58.500 Loss: 1.1599\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1954 (1.1954)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5231 (1.2771)\tPrec@1 37.500 (54.290)\n",
      "EPOCH: 13 val Results: Prec@1 54.290 Loss: 1.2771\n",
      "Best Prec@1: 54.290\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [14][0/390]\tTime 0.003 (0.003)\tLoss 0.9510 (0.9510)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [14][78/390]\tTime 0.002 (0.003)\tLoss 1.1203 (1.0943)\tPrec@1 55.469 (60.532)\n",
      "Epoch: [14][156/390]\tTime 0.002 (0.003)\tLoss 1.1453 (1.1107)\tPrec@1 62.500 (59.932)\n",
      "Epoch: [14][234/390]\tTime 0.003 (0.003)\tLoss 1.2051 (1.1316)\tPrec@1 56.250 (59.555)\n",
      "Epoch: [14][312/390]\tTime 0.002 (0.003)\tLoss 1.0573 (1.1440)\tPrec@1 60.156 (59.140)\n",
      "Epoch: [14][390/390]\tTime 0.003 (0.003)\tLoss 1.2936 (1.1521)\tPrec@1 47.500 (58.810)\n",
      "EPOCH: 14 train Results: Prec@1 58.810 Loss: 1.1521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1218 (1.1218)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4726 (1.2685)\tPrec@1 37.500 (54.780)\n",
      "EPOCH: 14 val Results: Prec@1 54.780 Loss: 1.2685\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [15][0/390]\tTime 0.004 (0.004)\tLoss 1.0137 (1.0137)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [15][78/390]\tTime 0.003 (0.003)\tLoss 1.1851 (1.0733)\tPrec@1 58.594 (61.798)\n",
      "Epoch: [15][156/390]\tTime 0.004 (0.003)\tLoss 1.0415 (1.1065)\tPrec@1 60.156 (60.599)\n",
      "Epoch: [15][234/390]\tTime 0.002 (0.003)\tLoss 1.1780 (1.1225)\tPrec@1 55.469 (60.146)\n",
      "Epoch: [15][312/390]\tTime 0.005 (0.003)\tLoss 1.2798 (1.1362)\tPrec@1 57.031 (59.635)\n",
      "Epoch: [15][390/390]\tTime 0.002 (0.003)\tLoss 1.1789 (1.1431)\tPrec@1 58.750 (59.332)\n",
      "EPOCH: 15 train Results: Prec@1 59.332 Loss: 1.1431\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1364 (1.1364)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4928 (1.2801)\tPrec@1 37.500 (54.600)\n",
      "EPOCH: 15 val Results: Prec@1 54.600 Loss: 1.2801\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [16][0/390]\tTime 0.004 (0.004)\tLoss 1.0279 (1.0279)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.003)\tLoss 1.0034 (1.0588)\tPrec@1 65.625 (61.828)\n",
      "Epoch: [16][156/390]\tTime 0.009 (0.003)\tLoss 1.0568 (1.0894)\tPrec@1 60.156 (60.798)\n",
      "Epoch: [16][234/390]\tTime 0.002 (0.003)\tLoss 1.2371 (1.1144)\tPrec@1 57.812 (59.943)\n",
      "Epoch: [16][312/390]\tTime 0.002 (0.003)\tLoss 1.1640 (1.1289)\tPrec@1 56.250 (59.333)\n",
      "Epoch: [16][390/390]\tTime 0.002 (0.003)\tLoss 1.3219 (1.1405)\tPrec@1 51.250 (59.024)\n",
      "EPOCH: 16 train Results: Prec@1 59.024 Loss: 1.1405\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0991 (1.0991)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.0826 (1.2813)\tPrec@1 56.250 (54.240)\n",
      "EPOCH: 16 val Results: Prec@1 54.240 Loss: 1.2813\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [17][0/390]\tTime 0.005 (0.005)\tLoss 1.0846 (1.0846)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [17][78/390]\tTime 0.002 (0.003)\tLoss 1.1644 (1.0577)\tPrec@1 58.594 (62.381)\n",
      "Epoch: [17][156/390]\tTime 0.002 (0.003)\tLoss 1.2876 (1.0842)\tPrec@1 53.906 (61.445)\n",
      "Epoch: [17][234/390]\tTime 0.004 (0.003)\tLoss 1.1141 (1.1041)\tPrec@1 57.812 (60.409)\n",
      "Epoch: [17][312/390]\tTime 0.002 (0.003)\tLoss 1.1851 (1.1205)\tPrec@1 62.500 (59.917)\n",
      "Epoch: [17][390/390]\tTime 0.002 (0.003)\tLoss 1.1986 (1.1334)\tPrec@1 60.000 (59.402)\n",
      "EPOCH: 17 train Results: Prec@1 59.402 Loss: 1.1334\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1190 (1.1190)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2422 (1.2858)\tPrec@1 56.250 (53.750)\n",
      "EPOCH: 17 val Results: Prec@1 53.750 Loss: 1.2858\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [18][0/390]\tTime 0.008 (0.008)\tLoss 0.9528 (0.9528)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [18][78/390]\tTime 0.004 (0.003)\tLoss 1.0184 (1.0623)\tPrec@1 61.719 (62.203)\n",
      "Epoch: [18][156/390]\tTime 0.004 (0.003)\tLoss 1.0277 (1.0894)\tPrec@1 70.312 (61.341)\n",
      "Epoch: [18][234/390]\tTime 0.002 (0.003)\tLoss 0.9552 (1.1115)\tPrec@1 64.062 (60.469)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.003)\tLoss 1.1818 (1.1205)\tPrec@1 63.281 (60.084)\n",
      "Epoch: [18][390/390]\tTime 0.001 (0.003)\tLoss 1.1098 (1.1301)\tPrec@1 55.000 (59.732)\n",
      "EPOCH: 18 train Results: Prec@1 59.732 Loss: 1.1301\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1691 (1.1691)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1363 (1.2912)\tPrec@1 56.250 (53.380)\n",
      "EPOCH: 18 val Results: Prec@1 53.380 Loss: 1.2912\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [19][0/390]\tTime 0.005 (0.005)\tLoss 1.0479 (1.0479)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [19][78/390]\tTime 0.049 (0.003)\tLoss 1.2420 (1.0391)\tPrec@1 57.812 (62.698)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.003)\tLoss 1.2165 (1.0731)\tPrec@1 57.812 (61.560)\n",
      "Epoch: [19][234/390]\tTime 0.007 (0.003)\tLoss 1.1244 (1.0947)\tPrec@1 60.156 (60.957)\n",
      "Epoch: [19][312/390]\tTime 0.004 (0.003)\tLoss 1.0154 (1.1115)\tPrec@1 61.719 (60.358)\n",
      "Epoch: [19][390/390]\tTime 0.004 (0.003)\tLoss 1.3959 (1.1210)\tPrec@1 55.000 (59.978)\n",
      "EPOCH: 19 train Results: Prec@1 59.978 Loss: 1.1210\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1481 (1.1481)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.6266 (1.2794)\tPrec@1 37.500 (54.070)\n",
      "EPOCH: 19 val Results: Prec@1 54.070 Loss: 1.2794\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [20][0/390]\tTime 0.012 (0.012)\tLoss 1.0244 (1.0244)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [20][78/390]\tTime 0.002 (0.003)\tLoss 0.9763 (1.0609)\tPrec@1 63.281 (61.847)\n",
      "Epoch: [20][156/390]\tTime 0.006 (0.003)\tLoss 0.9938 (1.0772)\tPrec@1 63.281 (61.355)\n",
      "Epoch: [20][234/390]\tTime 0.002 (0.003)\tLoss 1.0891 (1.1000)\tPrec@1 64.062 (60.455)\n",
      "Epoch: [20][312/390]\tTime 0.004 (0.003)\tLoss 1.2540 (1.1088)\tPrec@1 51.562 (60.333)\n",
      "Epoch: [20][390/390]\tTime 0.002 (0.003)\tLoss 1.2734 (1.1189)\tPrec@1 55.000 (60.002)\n",
      "EPOCH: 20 train Results: Prec@1 60.002 Loss: 1.1189\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.1246 (1.1246)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2110 (1.2768)\tPrec@1 43.750 (54.660)\n",
      "EPOCH: 20 val Results: Prec@1 54.660 Loss: 1.2768\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [21][0/390]\tTime 0.002 (0.002)\tLoss 1.1326 (1.1326)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [21][78/390]\tTime 0.003 (0.003)\tLoss 1.0840 (1.0339)\tPrec@1 55.469 (62.945)\n",
      "Epoch: [21][156/390]\tTime 0.003 (0.003)\tLoss 1.0653 (1.0685)\tPrec@1 60.156 (61.893)\n",
      "Epoch: [21][234/390]\tTime 0.002 (0.003)\tLoss 1.1605 (1.0831)\tPrec@1 62.500 (61.576)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.0295 (1.0990)\tPrec@1 67.969 (60.833)\n",
      "Epoch: [21][390/390]\tTime 0.006 (0.003)\tLoss 1.3250 (1.1116)\tPrec@1 53.750 (60.328)\n",
      "EPOCH: 21 train Results: Prec@1 60.328 Loss: 1.1116\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1789 (1.1789)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2911 (1.2747)\tPrec@1 43.750 (54.670)\n",
      "EPOCH: 21 val Results: Prec@1 54.670 Loss: 1.2747\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [22][0/390]\tTime 0.005 (0.005)\tLoss 1.1545 (1.1545)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [22][78/390]\tTime 0.005 (0.003)\tLoss 1.0840 (1.0626)\tPrec@1 62.500 (62.144)\n",
      "Epoch: [22][156/390]\tTime 0.006 (0.004)\tLoss 1.0955 (1.0781)\tPrec@1 60.938 (61.375)\n",
      "Epoch: [22][234/390]\tTime 0.002 (0.003)\tLoss 1.0644 (1.0902)\tPrec@1 59.375 (60.798)\n",
      "Epoch: [22][312/390]\tTime 0.003 (0.003)\tLoss 1.1485 (1.1052)\tPrec@1 59.375 (60.191)\n",
      "Epoch: [22][390/390]\tTime 0.005 (0.003)\tLoss 1.1045 (1.1184)\tPrec@1 58.750 (59.746)\n",
      "EPOCH: 22 train Results: Prec@1 59.746 Loss: 1.1184\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1487 (1.1487)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2054 (1.2813)\tPrec@1 31.250 (54.300)\n",
      "EPOCH: 22 val Results: Prec@1 54.300 Loss: 1.2813\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [23][0/390]\tTime 0.002 (0.002)\tLoss 1.0619 (1.0619)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [23][78/390]\tTime 0.003 (0.003)\tLoss 1.1433 (1.0278)\tPrec@1 60.156 (63.608)\n",
      "Epoch: [23][156/390]\tTime 0.003 (0.003)\tLoss 1.2536 (1.0632)\tPrec@1 52.344 (62.206)\n",
      "Epoch: [23][234/390]\tTime 0.003 (0.003)\tLoss 1.2205 (1.0785)\tPrec@1 57.031 (61.499)\n",
      "Epoch: [23][312/390]\tTime 0.002 (0.003)\tLoss 1.3087 (1.0979)\tPrec@1 53.906 (60.868)\n",
      "Epoch: [23][390/390]\tTime 0.003 (0.003)\tLoss 1.3752 (1.1109)\tPrec@1 58.750 (60.372)\n",
      "EPOCH: 23 train Results: Prec@1 60.372 Loss: 1.1109\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1800 (1.1800)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5958 (1.2707)\tPrec@1 31.250 (54.420)\n",
      "EPOCH: 23 val Results: Prec@1 54.420 Loss: 1.2707\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [24][0/390]\tTime 0.005 (0.005)\tLoss 1.0435 (1.0435)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.1727 (1.0231)\tPrec@1 67.188 (63.944)\n",
      "Epoch: [24][156/390]\tTime 0.008 (0.003)\tLoss 1.1401 (1.0584)\tPrec@1 57.031 (62.246)\n",
      "Epoch: [24][234/390]\tTime 0.002 (0.003)\tLoss 1.2878 (1.0848)\tPrec@1 54.688 (61.237)\n",
      "Epoch: [24][312/390]\tTime 0.002 (0.004)\tLoss 1.3018 (1.0990)\tPrec@1 54.688 (60.693)\n",
      "Epoch: [24][390/390]\tTime 0.002 (0.004)\tLoss 1.3255 (1.1088)\tPrec@1 55.000 (60.296)\n",
      "EPOCH: 24 train Results: Prec@1 60.296 Loss: 1.1088\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2120 (1.2120)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5150 (1.2944)\tPrec@1 31.250 (54.010)\n",
      "EPOCH: 24 val Results: Prec@1 54.010 Loss: 1.2944\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [25][0/390]\tTime 0.008 (0.008)\tLoss 0.9278 (0.9278)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [25][78/390]\tTime 0.002 (0.003)\tLoss 1.0294 (1.0267)\tPrec@1 64.844 (63.271)\n",
      "Epoch: [25][156/390]\tTime 0.002 (0.004)\tLoss 1.1922 (1.0558)\tPrec@1 60.156 (62.361)\n",
      "Epoch: [25][234/390]\tTime 0.002 (0.004)\tLoss 1.2191 (1.0809)\tPrec@1 54.688 (61.406)\n",
      "Epoch: [25][312/390]\tTime 0.003 (0.004)\tLoss 1.2276 (1.0965)\tPrec@1 60.156 (60.945)\n",
      "Epoch: [25][390/390]\tTime 0.004 (0.003)\tLoss 1.3127 (1.1050)\tPrec@1 51.250 (60.598)\n",
      "EPOCH: 25 train Results: Prec@1 60.598 Loss: 1.1050\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1484 (1.1484)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3425 (1.2799)\tPrec@1 56.250 (54.290)\n",
      "EPOCH: 25 val Results: Prec@1 54.290 Loss: 1.2799\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [26][0/390]\tTime 0.006 (0.006)\tLoss 1.0847 (1.0847)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [26][78/390]\tTime 0.002 (0.003)\tLoss 1.0236 (1.0130)\tPrec@1 60.938 (63.973)\n",
      "Epoch: [26][156/390]\tTime 0.003 (0.003)\tLoss 1.1405 (1.0530)\tPrec@1 62.500 (62.794)\n",
      "Epoch: [26][234/390]\tTime 0.002 (0.003)\tLoss 1.0530 (1.0739)\tPrec@1 60.156 (61.941)\n",
      "Epoch: [26][312/390]\tTime 0.004 (0.003)\tLoss 1.0509 (1.0908)\tPrec@1 60.156 (61.324)\n",
      "Epoch: [26][390/390]\tTime 0.001 (0.003)\tLoss 1.2073 (1.1010)\tPrec@1 55.000 (60.860)\n",
      "EPOCH: 26 train Results: Prec@1 60.860 Loss: 1.1010\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1917 (1.1917)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2608 (1.2842)\tPrec@1 37.500 (54.460)\n",
      "EPOCH: 26 val Results: Prec@1 54.460 Loss: 1.2842\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [27][0/390]\tTime 0.004 (0.004)\tLoss 1.0757 (1.0757)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [27][78/390]\tTime 0.004 (0.003)\tLoss 0.9873 (1.0215)\tPrec@1 64.062 (63.617)\n",
      "Epoch: [27][156/390]\tTime 0.003 (0.003)\tLoss 0.8946 (1.0512)\tPrec@1 65.625 (62.475)\n",
      "Epoch: [27][234/390]\tTime 0.002 (0.003)\tLoss 0.9891 (1.0688)\tPrec@1 65.625 (61.729)\n",
      "Epoch: [27][312/390]\tTime 0.004 (0.003)\tLoss 1.1280 (1.0881)\tPrec@1 60.156 (60.972)\n",
      "Epoch: [27][390/390]\tTime 0.005 (0.003)\tLoss 1.3398 (1.1025)\tPrec@1 52.500 (60.594)\n",
      "EPOCH: 27 train Results: Prec@1 60.594 Loss: 1.1025\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1170 (1.1170)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4828 (1.2813)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 27 val Results: Prec@1 54.630 Loss: 1.2813\n",
      "Best Prec@1: 54.780\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [28][0/390]\tTime 0.004 (0.004)\tLoss 1.0308 (1.0308)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [28][78/390]\tTime 0.008 (0.003)\tLoss 1.0196 (1.0335)\tPrec@1 70.312 (63.311)\n",
      "Epoch: [28][156/390]\tTime 0.002 (0.004)\tLoss 1.1482 (1.0532)\tPrec@1 60.938 (62.296)\n",
      "Epoch: [28][234/390]\tTime 0.002 (0.003)\tLoss 1.0352 (1.0734)\tPrec@1 63.281 (61.589)\n",
      "Epoch: [28][312/390]\tTime 0.004 (0.003)\tLoss 1.1826 (1.0856)\tPrec@1 61.719 (61.097)\n",
      "Epoch: [28][390/390]\tTime 0.003 (0.003)\tLoss 1.2334 (1.0984)\tPrec@1 57.500 (60.688)\n",
      "EPOCH: 28 train Results: Prec@1 60.688 Loss: 1.0984\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2030 (1.2030)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6278 (1.2732)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 28 val Results: Prec@1 55.230 Loss: 1.2732\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [29][0/390]\tTime 0.003 (0.003)\tLoss 0.9857 (0.9857)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [29][78/390]\tTime 0.002 (0.003)\tLoss 1.0549 (1.0210)\tPrec@1 66.406 (63.261)\n",
      "Epoch: [29][156/390]\tTime 0.003 (0.003)\tLoss 1.0663 (1.0482)\tPrec@1 58.594 (62.530)\n",
      "Epoch: [29][234/390]\tTime 0.002 (0.003)\tLoss 1.0148 (1.0718)\tPrec@1 65.625 (61.692)\n",
      "Epoch: [29][312/390]\tTime 0.002 (0.003)\tLoss 1.0641 (1.0907)\tPrec@1 60.156 (60.967)\n",
      "Epoch: [29][390/390]\tTime 0.003 (0.003)\tLoss 1.2661 (1.1007)\tPrec@1 57.500 (60.612)\n",
      "EPOCH: 29 train Results: Prec@1 60.612 Loss: 1.1007\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1110 (1.1110)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1953 (1.2742)\tPrec@1 56.250 (54.450)\n",
      "EPOCH: 29 val Results: Prec@1 54.450 Loss: 1.2742\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [30][0/390]\tTime 0.005 (0.005)\tLoss 0.9370 (0.9370)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.003)\tLoss 1.0583 (1.0285)\tPrec@1 64.062 (63.360)\n",
      "Epoch: [30][156/390]\tTime 0.002 (0.003)\tLoss 1.0981 (1.0568)\tPrec@1 55.469 (62.371)\n",
      "Epoch: [30][234/390]\tTime 0.005 (0.003)\tLoss 1.1814 (1.0722)\tPrec@1 59.375 (61.872)\n",
      "Epoch: [30][312/390]\tTime 0.005 (0.003)\tLoss 1.0904 (1.0856)\tPrec@1 64.844 (61.392)\n",
      "Epoch: [30][390/390]\tTime 0.001 (0.003)\tLoss 1.0602 (1.0988)\tPrec@1 60.000 (60.984)\n",
      "EPOCH: 30 train Results: Prec@1 60.984 Loss: 1.0988\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1279 (1.1279)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2511 (1.2791)\tPrec@1 43.750 (54.790)\n",
      "EPOCH: 30 val Results: Prec@1 54.790 Loss: 1.2791\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [31][0/390]\tTime 0.002 (0.002)\tLoss 0.8981 (0.8981)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [31][78/390]\tTime 0.002 (0.003)\tLoss 0.9234 (1.0233)\tPrec@1 66.406 (63.281)\n",
      "Epoch: [31][156/390]\tTime 0.003 (0.003)\tLoss 1.0218 (1.0422)\tPrec@1 60.156 (62.545)\n",
      "Epoch: [31][234/390]\tTime 0.007 (0.003)\tLoss 1.1494 (1.0618)\tPrec@1 60.938 (61.885)\n",
      "Epoch: [31][312/390]\tTime 0.006 (0.003)\tLoss 1.0893 (1.0766)\tPrec@1 60.938 (61.479)\n",
      "Epoch: [31][390/390]\tTime 0.002 (0.003)\tLoss 1.2468 (1.0924)\tPrec@1 62.500 (60.964)\n",
      "EPOCH: 31 train Results: Prec@1 60.964 Loss: 1.0924\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1692 (1.1692)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5650 (1.2817)\tPrec@1 18.750 (54.700)\n",
      "EPOCH: 31 val Results: Prec@1 54.700 Loss: 1.2817\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [32][0/390]\tTime 0.004 (0.004)\tLoss 0.9850 (0.9850)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [32][78/390]\tTime 0.002 (0.004)\tLoss 0.9557 (1.0077)\tPrec@1 68.750 (64.359)\n",
      "Epoch: [32][156/390]\tTime 0.003 (0.004)\tLoss 0.9941 (1.0446)\tPrec@1 67.969 (62.898)\n",
      "Epoch: [32][234/390]\tTime 0.003 (0.003)\tLoss 1.1219 (1.0616)\tPrec@1 60.156 (62.031)\n",
      "Epoch: [32][312/390]\tTime 0.005 (0.003)\tLoss 0.9866 (1.0756)\tPrec@1 63.281 (61.502)\n",
      "Epoch: [32][390/390]\tTime 0.004 (0.003)\tLoss 0.9577 (1.0900)\tPrec@1 68.750 (61.034)\n",
      "EPOCH: 32 train Results: Prec@1 61.034 Loss: 1.0900\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2563 (1.2563)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6028 (1.2856)\tPrec@1 50.000 (54.580)\n",
      "EPOCH: 32 val Results: Prec@1 54.580 Loss: 1.2856\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [33][0/390]\tTime 0.004 (0.004)\tLoss 1.1115 (1.1115)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [33][78/390]\tTime 0.003 (0.003)\tLoss 1.1822 (1.0259)\tPrec@1 54.688 (62.994)\n",
      "Epoch: [33][156/390]\tTime 0.004 (0.003)\tLoss 1.1119 (1.0493)\tPrec@1 60.156 (62.316)\n",
      "Epoch: [33][234/390]\tTime 0.002 (0.003)\tLoss 1.1700 (1.0643)\tPrec@1 57.031 (61.765)\n",
      "Epoch: [33][312/390]\tTime 0.008 (0.003)\tLoss 1.1552 (1.0769)\tPrec@1 60.938 (61.389)\n",
      "Epoch: [33][390/390]\tTime 0.002 (0.003)\tLoss 1.2877 (1.0919)\tPrec@1 57.500 (60.950)\n",
      "EPOCH: 33 train Results: Prec@1 60.950 Loss: 1.0919\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1515 (1.1515)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6077 (1.2760)\tPrec@1 37.500 (54.950)\n",
      "EPOCH: 33 val Results: Prec@1 54.950 Loss: 1.2760\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [34][0/390]\tTime 0.002 (0.002)\tLoss 0.8998 (0.8998)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [34][78/390]\tTime 0.002 (0.003)\tLoss 1.0872 (1.0087)\tPrec@1 62.500 (64.715)\n",
      "Epoch: [34][156/390]\tTime 0.002 (0.003)\tLoss 1.1636 (1.0389)\tPrec@1 64.062 (63.391)\n",
      "Epoch: [34][234/390]\tTime 0.002 (0.003)\tLoss 1.1098 (1.0558)\tPrec@1 57.031 (62.497)\n",
      "Epoch: [34][312/390]\tTime 0.002 (0.003)\tLoss 1.1302 (1.0741)\tPrec@1 62.500 (61.903)\n",
      "Epoch: [34][390/390]\tTime 0.004 (0.003)\tLoss 1.0717 (1.0873)\tPrec@1 55.000 (61.324)\n",
      "EPOCH: 34 train Results: Prec@1 61.324 Loss: 1.0873\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1182 (1.1182)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2825 (1.2813)\tPrec@1 56.250 (54.040)\n",
      "EPOCH: 34 val Results: Prec@1 54.040 Loss: 1.2813\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [35][0/390]\tTime 0.002 (0.002)\tLoss 1.0572 (1.0572)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [35][78/390]\tTime 0.003 (0.004)\tLoss 1.1519 (1.0013)\tPrec@1 54.688 (64.508)\n",
      "Epoch: [35][156/390]\tTime 0.007 (0.004)\tLoss 1.3191 (1.0299)\tPrec@1 57.812 (63.251)\n",
      "Epoch: [35][234/390]\tTime 0.002 (0.003)\tLoss 1.1560 (1.0603)\tPrec@1 60.938 (62.098)\n",
      "Epoch: [35][312/390]\tTime 0.002 (0.003)\tLoss 1.1823 (1.0776)\tPrec@1 53.906 (61.552)\n",
      "Epoch: [35][390/390]\tTime 0.009 (0.003)\tLoss 1.0559 (1.0882)\tPrec@1 67.500 (61.202)\n",
      "EPOCH: 35 train Results: Prec@1 61.202 Loss: 1.0882\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1425 (1.1425)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.8031 (1.2783)\tPrec@1 25.000 (54.240)\n",
      "EPOCH: 35 val Results: Prec@1 54.240 Loss: 1.2783\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [36][0/390]\tTime 0.002 (0.002)\tLoss 1.1035 (1.1035)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [36][78/390]\tTime 0.004 (0.003)\tLoss 1.1714 (1.0058)\tPrec@1 56.250 (64.330)\n",
      "Epoch: [36][156/390]\tTime 0.007 (0.003)\tLoss 1.1166 (1.0426)\tPrec@1 60.938 (62.749)\n",
      "Epoch: [36][234/390]\tTime 0.003 (0.003)\tLoss 1.1041 (1.0589)\tPrec@1 64.062 (62.108)\n",
      "Epoch: [36][312/390]\tTime 0.003 (0.003)\tLoss 1.2327 (1.0718)\tPrec@1 60.156 (61.681)\n",
      "Epoch: [36][390/390]\tTime 0.001 (0.003)\tLoss 1.2852 (1.0842)\tPrec@1 48.750 (61.272)\n",
      "EPOCH: 36 train Results: Prec@1 61.272 Loss: 1.0842\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1404 (1.1404)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3955 (1.2735)\tPrec@1 43.750 (55.130)\n",
      "EPOCH: 36 val Results: Prec@1 55.130 Loss: 1.2735\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [37][0/390]\tTime 0.005 (0.005)\tLoss 0.8278 (0.8278)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 0.9678 (0.9957)\tPrec@1 65.625 (65.071)\n",
      "Epoch: [37][156/390]\tTime 0.002 (0.003)\tLoss 0.9042 (1.0326)\tPrec@1 67.969 (63.246)\n",
      "Epoch: [37][234/390]\tTime 0.002 (0.003)\tLoss 1.1706 (1.0590)\tPrec@1 56.250 (62.380)\n",
      "Epoch: [37][312/390]\tTime 0.003 (0.003)\tLoss 1.0533 (1.0817)\tPrec@1 69.531 (61.432)\n",
      "Epoch: [37][390/390]\tTime 0.005 (0.003)\tLoss 1.1111 (1.0890)\tPrec@1 56.250 (61.224)\n",
      "EPOCH: 37 train Results: Prec@1 61.224 Loss: 1.0890\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2157 (1.2157)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5334 (1.2759)\tPrec@1 37.500 (54.960)\n",
      "EPOCH: 37 val Results: Prec@1 54.960 Loss: 1.2759\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [38][0/390]\tTime 0.005 (0.005)\tLoss 0.9967 (0.9967)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [38][78/390]\tTime 0.003 (0.003)\tLoss 1.2499 (1.0245)\tPrec@1 57.031 (63.558)\n",
      "Epoch: [38][156/390]\tTime 0.012 (0.003)\tLoss 0.9926 (1.0437)\tPrec@1 67.188 (62.883)\n",
      "Epoch: [38][234/390]\tTime 0.003 (0.003)\tLoss 1.0457 (1.0668)\tPrec@1 65.625 (62.058)\n",
      "Epoch: [38][312/390]\tTime 0.002 (0.003)\tLoss 1.2319 (1.0769)\tPrec@1 56.250 (61.686)\n",
      "Epoch: [38][390/390]\tTime 0.002 (0.003)\tLoss 1.3319 (1.0873)\tPrec@1 60.000 (61.230)\n",
      "EPOCH: 38 train Results: Prec@1 61.230 Loss: 1.0873\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1426 (1.1426)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5026 (1.2743)\tPrec@1 50.000 (54.860)\n",
      "EPOCH: 38 val Results: Prec@1 54.860 Loss: 1.2743\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [39][0/390]\tTime 0.002 (0.002)\tLoss 0.9770 (0.9770)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.003)\tLoss 1.1584 (0.9991)\tPrec@1 61.719 (64.557)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.003)\tLoss 1.0875 (1.0332)\tPrec@1 63.281 (63.640)\n",
      "Epoch: [39][234/390]\tTime 0.004 (0.003)\tLoss 1.1842 (1.0573)\tPrec@1 59.375 (62.689)\n",
      "Epoch: [39][312/390]\tTime 0.002 (0.003)\tLoss 1.4436 (1.0705)\tPrec@1 38.281 (62.078)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2792 (1.0856)\tPrec@1 50.000 (61.524)\n",
      "EPOCH: 39 train Results: Prec@1 61.524 Loss: 1.0856\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1100 (1.1100)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7733 (1.2825)\tPrec@1 50.000 (54.740)\n",
      "EPOCH: 39 val Results: Prec@1 54.740 Loss: 1.2825\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [40][0/390]\tTime 0.002 (0.002)\tLoss 0.9824 (0.9824)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.003)\tLoss 1.0025 (1.0058)\tPrec@1 62.500 (64.517)\n",
      "Epoch: [40][156/390]\tTime 0.004 (0.003)\tLoss 1.0671 (1.0232)\tPrec@1 61.719 (63.565)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.003)\tLoss 1.1570 (1.0434)\tPrec@1 57.031 (62.766)\n",
      "Epoch: [40][312/390]\tTime 0.006 (0.003)\tLoss 1.2956 (1.0661)\tPrec@1 57.031 (61.948)\n",
      "Epoch: [40][390/390]\tTime 0.002 (0.003)\tLoss 1.3083 (1.0833)\tPrec@1 50.000 (61.230)\n",
      "EPOCH: 40 train Results: Prec@1 61.230 Loss: 1.0833\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1386 (1.1386)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6661 (1.2744)\tPrec@1 50.000 (54.440)\n",
      "EPOCH: 40 val Results: Prec@1 54.440 Loss: 1.2744\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [41][0/390]\tTime 0.003 (0.003)\tLoss 1.1466 (1.1466)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [41][78/390]\tTime 0.003 (0.003)\tLoss 0.9220 (0.9893)\tPrec@1 69.531 (64.666)\n",
      "Epoch: [41][156/390]\tTime 0.004 (0.003)\tLoss 1.1869 (1.0318)\tPrec@1 58.594 (63.087)\n",
      "Epoch: [41][234/390]\tTime 0.002 (0.003)\tLoss 1.1511 (1.0541)\tPrec@1 60.156 (62.204)\n",
      "Epoch: [41][312/390]\tTime 0.003 (0.003)\tLoss 1.2960 (1.0710)\tPrec@1 56.250 (61.487)\n",
      "Epoch: [41][390/390]\tTime 0.001 (0.003)\tLoss 1.3165 (1.0826)\tPrec@1 52.500 (61.164)\n",
      "EPOCH: 41 train Results: Prec@1 61.164 Loss: 1.0826\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1811 (1.1811)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1503 (1.2666)\tPrec@1 56.250 (55.210)\n",
      "EPOCH: 41 val Results: Prec@1 55.210 Loss: 1.2666\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [42][0/390]\tTime 0.007 (0.007)\tLoss 0.9582 (0.9582)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [42][78/390]\tTime 0.003 (0.003)\tLoss 0.9995 (1.0072)\tPrec@1 62.500 (64.300)\n",
      "Epoch: [42][156/390]\tTime 0.002 (0.003)\tLoss 1.0475 (1.0276)\tPrec@1 63.281 (63.630)\n",
      "Epoch: [42][234/390]\tTime 0.002 (0.003)\tLoss 1.1259 (1.0500)\tPrec@1 60.156 (62.726)\n",
      "Epoch: [42][312/390]\tTime 0.004 (0.003)\tLoss 1.1452 (1.0668)\tPrec@1 59.375 (62.013)\n",
      "Epoch: [42][390/390]\tTime 0.004 (0.003)\tLoss 1.2064 (1.0828)\tPrec@1 60.000 (61.388)\n",
      "EPOCH: 42 train Results: Prec@1 61.388 Loss: 1.0828\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2009 (1.2009)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2749 (1.2797)\tPrec@1 43.750 (54.790)\n",
      "EPOCH: 42 val Results: Prec@1 54.790 Loss: 1.2797\n",
      "Best Prec@1: 55.230\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [43][0/390]\tTime 0.002 (0.002)\tLoss 1.0460 (1.0460)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [43][78/390]\tTime 0.002 (0.003)\tLoss 1.0962 (1.0096)\tPrec@1 61.719 (63.726)\n",
      "Epoch: [43][156/390]\tTime 0.004 (0.003)\tLoss 1.2716 (1.0372)\tPrec@1 54.688 (62.809)\n",
      "Epoch: [43][234/390]\tTime 0.003 (0.003)\tLoss 1.1639 (1.0514)\tPrec@1 60.156 (62.337)\n",
      "Epoch: [43][312/390]\tTime 0.010 (0.003)\tLoss 1.0386 (1.0707)\tPrec@1 64.062 (61.751)\n",
      "Epoch: [43][390/390]\tTime 0.004 (0.003)\tLoss 0.9912 (1.0805)\tPrec@1 61.250 (61.426)\n",
      "EPOCH: 43 train Results: Prec@1 61.426 Loss: 1.0805\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1542 (1.1542)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4859 (1.2732)\tPrec@1 43.750 (55.470)\n",
      "EPOCH: 43 val Results: Prec@1 55.470 Loss: 1.2732\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [44][0/390]\tTime 0.002 (0.002)\tLoss 0.9624 (0.9624)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.003)\tLoss 0.9852 (1.0151)\tPrec@1 60.938 (63.934)\n",
      "Epoch: [44][156/390]\tTime 0.005 (0.003)\tLoss 1.0796 (1.0364)\tPrec@1 60.156 (63.231)\n",
      "Epoch: [44][234/390]\tTime 0.002 (0.003)\tLoss 0.9295 (1.0551)\tPrec@1 66.406 (62.560)\n",
      "Epoch: [44][312/390]\tTime 0.003 (0.003)\tLoss 1.0767 (1.0709)\tPrec@1 57.812 (61.791)\n",
      "Epoch: [44][390/390]\tTime 0.002 (0.003)\tLoss 1.1669 (1.0822)\tPrec@1 65.000 (61.286)\n",
      "EPOCH: 44 train Results: Prec@1 61.286 Loss: 1.0822\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2492 (1.2492)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5318 (1.2907)\tPrec@1 25.000 (53.950)\n",
      "EPOCH: 44 val Results: Prec@1 53.950 Loss: 1.2907\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [45][0/390]\tTime 0.002 (0.002)\tLoss 1.0854 (1.0854)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [45][78/390]\tTime 0.003 (0.003)\tLoss 1.1013 (1.0050)\tPrec@1 60.938 (64.033)\n",
      "Epoch: [45][156/390]\tTime 0.003 (0.003)\tLoss 1.0999 (1.0303)\tPrec@1 57.812 (63.182)\n",
      "Epoch: [45][234/390]\tTime 0.003 (0.003)\tLoss 1.1068 (1.0481)\tPrec@1 60.938 (62.407)\n",
      "Epoch: [45][312/390]\tTime 0.002 (0.003)\tLoss 1.1178 (1.0667)\tPrec@1 59.375 (61.886)\n",
      "Epoch: [45][390/390]\tTime 0.004 (0.003)\tLoss 1.1554 (1.0776)\tPrec@1 57.500 (61.524)\n",
      "EPOCH: 45 train Results: Prec@1 61.524 Loss: 1.0776\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2332 (1.2332)\tPrec@1 53.906 (53.906)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4293 (1.2866)\tPrec@1 56.250 (53.750)\n",
      "EPOCH: 45 val Results: Prec@1 53.750 Loss: 1.2866\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [46][0/390]\tTime 0.005 (0.005)\tLoss 1.0349 (1.0349)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [46][78/390]\tTime 0.002 (0.003)\tLoss 0.9916 (1.0064)\tPrec@1 64.062 (64.092)\n",
      "Epoch: [46][156/390]\tTime 0.003 (0.003)\tLoss 1.0570 (1.0236)\tPrec@1 60.938 (63.321)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.0947 (1.0453)\tPrec@1 58.594 (62.520)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.003)\tLoss 1.1043 (1.0574)\tPrec@1 60.156 (62.203)\n",
      "Epoch: [46][390/390]\tTime 0.001 (0.003)\tLoss 1.1682 (1.0740)\tPrec@1 60.000 (61.682)\n",
      "EPOCH: 46 train Results: Prec@1 61.682 Loss: 1.0740\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1684 (1.1684)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5721 (1.2974)\tPrec@1 31.250 (54.350)\n",
      "EPOCH: 46 val Results: Prec@1 54.350 Loss: 1.2974\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [47][0/390]\tTime 0.002 (0.002)\tLoss 1.2586 (1.2586)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [47][78/390]\tTime 0.003 (0.004)\tLoss 1.2039 (0.9863)\tPrec@1 59.375 (65.279)\n",
      "Epoch: [47][156/390]\tTime 0.005 (0.003)\tLoss 1.0902 (1.0176)\tPrec@1 62.500 (63.988)\n",
      "Epoch: [47][234/390]\tTime 0.004 (0.003)\tLoss 1.0539 (1.0473)\tPrec@1 64.844 (62.945)\n",
      "Epoch: [47][312/390]\tTime 0.005 (0.003)\tLoss 1.0108 (1.0612)\tPrec@1 65.625 (62.398)\n",
      "Epoch: [47][390/390]\tTime 0.001 (0.003)\tLoss 1.2024 (1.0744)\tPrec@1 51.250 (61.922)\n",
      "EPOCH: 47 train Results: Prec@1 61.922 Loss: 1.0744\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1427 (1.1427)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2208 (1.2789)\tPrec@1 56.250 (54.500)\n",
      "EPOCH: 47 val Results: Prec@1 54.500 Loss: 1.2789\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [48][0/390]\tTime 0.002 (0.002)\tLoss 0.9331 (0.9331)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [48][78/390]\tTime 0.002 (0.003)\tLoss 0.9649 (1.0086)\tPrec@1 67.969 (63.875)\n",
      "Epoch: [48][156/390]\tTime 0.002 (0.003)\tLoss 1.0866 (1.0295)\tPrec@1 60.938 (63.236)\n",
      "Epoch: [48][234/390]\tTime 0.003 (0.003)\tLoss 1.0060 (1.0442)\tPrec@1 65.625 (62.706)\n",
      "Epoch: [48][312/390]\tTime 0.003 (0.003)\tLoss 1.1125 (1.0597)\tPrec@1 59.375 (62.203)\n",
      "Epoch: [48][390/390]\tTime 0.001 (0.003)\tLoss 1.4226 (1.0718)\tPrec@1 56.250 (61.916)\n",
      "EPOCH: 48 train Results: Prec@1 61.916 Loss: 1.0718\n",
      "Test: [0/78]\tTime 0.009 (0.009)\tLoss 1.1313 (1.1313)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5982 (1.2819)\tPrec@1 43.750 (54.850)\n",
      "EPOCH: 48 val Results: Prec@1 54.850 Loss: 1.2819\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [49][0/390]\tTime 0.002 (0.002)\tLoss 0.9880 (0.9880)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [49][78/390]\tTime 0.002 (0.003)\tLoss 1.1662 (0.9812)\tPrec@1 58.594 (65.447)\n",
      "Epoch: [49][156/390]\tTime 0.005 (0.003)\tLoss 1.1849 (1.0238)\tPrec@1 62.500 (63.654)\n",
      "Epoch: [49][234/390]\tTime 0.008 (0.003)\tLoss 1.0452 (1.0426)\tPrec@1 67.188 (62.995)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.0764 (1.0575)\tPrec@1 62.500 (62.363)\n",
      "Epoch: [49][390/390]\tTime 0.003 (0.003)\tLoss 1.2710 (1.0731)\tPrec@1 53.750 (61.818)\n",
      "EPOCH: 49 train Results: Prec@1 61.818 Loss: 1.0731\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2430 (1.2430)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3481 (1.2727)\tPrec@1 50.000 (55.270)\n",
      "EPOCH: 49 val Results: Prec@1 55.270 Loss: 1.2727\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [50][0/390]\tTime 0.005 (0.005)\tLoss 1.0523 (1.0523)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [50][78/390]\tTime 0.002 (0.003)\tLoss 1.0600 (1.0076)\tPrec@1 64.844 (64.003)\n",
      "Epoch: [50][156/390]\tTime 0.005 (0.003)\tLoss 1.0913 (1.0293)\tPrec@1 58.594 (63.356)\n",
      "Epoch: [50][234/390]\tTime 0.002 (0.003)\tLoss 1.1260 (1.0528)\tPrec@1 59.375 (62.643)\n",
      "Epoch: [50][312/390]\tTime 0.004 (0.003)\tLoss 1.0610 (1.0659)\tPrec@1 64.844 (62.283)\n",
      "Epoch: [50][390/390]\tTime 0.001 (0.003)\tLoss 1.0818 (1.0796)\tPrec@1 62.500 (61.752)\n",
      "EPOCH: 50 train Results: Prec@1 61.752 Loss: 1.0796\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1587 (1.1587)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5379 (1.2765)\tPrec@1 31.250 (54.830)\n",
      "EPOCH: 50 val Results: Prec@1 54.830 Loss: 1.2765\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [51][0/390]\tTime 0.002 (0.002)\tLoss 1.0543 (1.0543)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [51][78/390]\tTime 0.005 (0.003)\tLoss 1.0220 (1.0064)\tPrec@1 63.281 (63.825)\n",
      "Epoch: [51][156/390]\tTime 0.007 (0.003)\tLoss 1.1263 (1.0293)\tPrec@1 61.719 (63.172)\n",
      "Epoch: [51][234/390]\tTime 0.002 (0.003)\tLoss 1.1837 (1.0437)\tPrec@1 61.719 (62.763)\n",
      "Epoch: [51][312/390]\tTime 0.008 (0.003)\tLoss 0.9957 (1.0586)\tPrec@1 68.750 (62.395)\n",
      "Epoch: [51][390/390]\tTime 0.003 (0.003)\tLoss 1.1301 (1.0730)\tPrec@1 53.750 (61.796)\n",
      "EPOCH: 51 train Results: Prec@1 61.796 Loss: 1.0730\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2996 (1.2996)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3418 (1.2808)\tPrec@1 50.000 (54.790)\n",
      "EPOCH: 51 val Results: Prec@1 54.790 Loss: 1.2808\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [52][0/390]\tTime 0.003 (0.003)\tLoss 0.9591 (0.9591)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [52][78/390]\tTime 0.003 (0.004)\tLoss 1.0336 (0.9907)\tPrec@1 61.719 (64.695)\n",
      "Epoch: [52][156/390]\tTime 0.005 (0.003)\tLoss 1.0796 (1.0160)\tPrec@1 57.031 (63.988)\n",
      "Epoch: [52][234/390]\tTime 0.008 (0.003)\tLoss 0.9910 (1.0397)\tPrec@1 65.625 (62.909)\n",
      "Epoch: [52][312/390]\tTime 0.005 (0.003)\tLoss 1.3333 (1.0592)\tPrec@1 46.875 (62.156)\n",
      "Epoch: [52][390/390]\tTime 0.001 (0.003)\tLoss 1.2540 (1.0713)\tPrec@1 60.000 (61.828)\n",
      "EPOCH: 52 train Results: Prec@1 61.828 Loss: 1.0713\n",
      "Test: [0/78]\tTime 0.014 (0.014)\tLoss 1.1736 (1.1736)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2819 (1.2800)\tPrec@1 43.750 (54.470)\n",
      "EPOCH: 52 val Results: Prec@1 54.470 Loss: 1.2800\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [53][0/390]\tTime 0.004 (0.004)\tLoss 1.0487 (1.0487)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [53][78/390]\tTime 0.002 (0.003)\tLoss 1.0103 (0.9852)\tPrec@1 62.500 (65.140)\n",
      "Epoch: [53][156/390]\tTime 0.002 (0.003)\tLoss 1.0360 (1.0180)\tPrec@1 59.375 (63.525)\n",
      "Epoch: [53][234/390]\tTime 0.004 (0.003)\tLoss 1.1756 (1.0397)\tPrec@1 60.156 (62.836)\n",
      "Epoch: [53][312/390]\tTime 0.004 (0.003)\tLoss 1.4020 (1.0564)\tPrec@1 50.781 (62.178)\n",
      "Epoch: [53][390/390]\tTime 0.001 (0.003)\tLoss 1.1508 (1.0723)\tPrec@1 65.000 (61.646)\n",
      "EPOCH: 53 train Results: Prec@1 61.646 Loss: 1.0723\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2483 (1.2483)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4761 (1.2736)\tPrec@1 31.250 (54.560)\n",
      "EPOCH: 53 val Results: Prec@1 54.560 Loss: 1.2736\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [54][0/390]\tTime 0.004 (0.004)\tLoss 0.9295 (0.9295)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [54][78/390]\tTime 0.002 (0.003)\tLoss 1.0369 (0.9938)\tPrec@1 64.844 (64.577)\n",
      "Epoch: [54][156/390]\tTime 0.004 (0.003)\tLoss 0.9917 (1.0212)\tPrec@1 63.281 (63.769)\n",
      "Epoch: [54][234/390]\tTime 0.002 (0.003)\tLoss 0.9871 (1.0446)\tPrec@1 67.188 (62.803)\n",
      "Epoch: [54][312/390]\tTime 0.004 (0.003)\tLoss 1.3116 (1.0605)\tPrec@1 63.281 (62.181)\n",
      "Epoch: [54][390/390]\tTime 0.001 (0.003)\tLoss 1.2400 (1.0751)\tPrec@1 60.000 (61.710)\n",
      "EPOCH: 54 train Results: Prec@1 61.710 Loss: 1.0751\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1561 (1.1561)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4297 (1.2817)\tPrec@1 56.250 (54.470)\n",
      "EPOCH: 54 val Results: Prec@1 54.470 Loss: 1.2817\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.0466 (1.0466)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.1490 (0.9927)\tPrec@1 62.500 (64.903)\n",
      "Epoch: [55][156/390]\tTime 0.002 (0.003)\tLoss 1.0967 (1.0176)\tPrec@1 61.719 (63.769)\n",
      "Epoch: [55][234/390]\tTime 0.002 (0.003)\tLoss 1.2043 (1.0430)\tPrec@1 58.594 (62.866)\n",
      "Epoch: [55][312/390]\tTime 0.003 (0.003)\tLoss 0.8842 (1.0578)\tPrec@1 70.312 (62.113)\n",
      "Epoch: [55][390/390]\tTime 0.001 (0.003)\tLoss 1.0986 (1.0723)\tPrec@1 55.000 (61.596)\n",
      "EPOCH: 55 train Results: Prec@1 61.596 Loss: 1.0723\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2139 (1.2139)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5488 (1.2769)\tPrec@1 43.750 (54.840)\n",
      "EPOCH: 55 val Results: Prec@1 54.840 Loss: 1.2769\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [56][0/390]\tTime 0.003 (0.003)\tLoss 0.8653 (0.8653)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [56][78/390]\tTime 0.004 (0.003)\tLoss 0.9948 (1.0162)\tPrec@1 60.938 (63.528)\n",
      "Epoch: [56][156/390]\tTime 0.004 (0.003)\tLoss 0.9221 (1.0204)\tPrec@1 71.094 (63.445)\n",
      "Epoch: [56][234/390]\tTime 0.002 (0.003)\tLoss 1.0643 (1.0386)\tPrec@1 59.375 (62.739)\n",
      "Epoch: [56][312/390]\tTime 0.008 (0.003)\tLoss 0.9309 (1.0538)\tPrec@1 66.406 (62.200)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.003)\tLoss 1.2578 (1.0701)\tPrec@1 53.750 (61.738)\n",
      "EPOCH: 56 train Results: Prec@1 61.738 Loss: 1.0701\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1455 (1.1455)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3831 (1.2717)\tPrec@1 50.000 (55.660)\n",
      "EPOCH: 56 val Results: Prec@1 55.660 Loss: 1.2717\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [57][0/390]\tTime 0.005 (0.005)\tLoss 1.0013 (1.0013)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [57][78/390]\tTime 0.002 (0.005)\tLoss 0.9728 (0.9912)\tPrec@1 65.625 (64.893)\n",
      "Epoch: [57][156/390]\tTime 0.004 (0.004)\tLoss 0.9530 (1.0264)\tPrec@1 67.188 (63.550)\n",
      "Epoch: [57][234/390]\tTime 0.037 (0.008)\tLoss 1.1064 (1.0438)\tPrec@1 63.281 (62.812)\n",
      "Epoch: [57][312/390]\tTime 0.022 (0.011)\tLoss 1.1458 (1.0574)\tPrec@1 57.812 (62.273)\n",
      "Epoch: [57][390/390]\tTime 0.021 (0.013)\tLoss 1.1211 (1.0720)\tPrec@1 58.750 (61.786)\n",
      "EPOCH: 57 train Results: Prec@1 61.786 Loss: 1.0720\n",
      "Test: [0/78]\tTime 0.018 (0.018)\tLoss 1.1589 (1.1589)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.011 (0.012)\tLoss 1.4586 (1.2799)\tPrec@1 37.500 (54.760)\n",
      "EPOCH: 57 val Results: Prec@1 54.760 Loss: 1.2799\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [58][0/390]\tTime 0.039 (0.039)\tLoss 0.9631 (0.9631)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [58][78/390]\tTime 0.040 (0.028)\tLoss 1.0176 (0.9865)\tPrec@1 67.969 (64.854)\n",
      "Epoch: [58][156/390]\tTime 0.029 (0.026)\tLoss 1.0685 (1.0209)\tPrec@1 62.500 (63.560)\n",
      "Epoch: [58][234/390]\tTime 0.005 (0.019)\tLoss 0.9947 (1.0411)\tPrec@1 61.719 (62.856)\n",
      "Epoch: [58][312/390]\tTime 0.008 (0.015)\tLoss 0.9573 (1.0528)\tPrec@1 66.406 (62.403)\n",
      "Epoch: [58][390/390]\tTime 0.001 (0.013)\tLoss 1.1471 (1.0662)\tPrec@1 61.250 (61.876)\n",
      "EPOCH: 58 train Results: Prec@1 61.876 Loss: 1.0662\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1580 (1.1580)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2817 (1.2697)\tPrec@1 43.750 (55.050)\n",
      "EPOCH: 58 val Results: Prec@1 55.050 Loss: 1.2697\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [59][0/390]\tTime 0.004 (0.004)\tLoss 0.9925 (0.9925)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [59][78/390]\tTime 0.002 (0.003)\tLoss 1.1198 (1.0002)\tPrec@1 61.719 (64.873)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.1455 (1.0194)\tPrec@1 59.375 (63.809)\n",
      "Epoch: [59][234/390]\tTime 0.004 (0.003)\tLoss 1.0412 (1.0432)\tPrec@1 61.719 (62.729)\n",
      "Epoch: [59][312/390]\tTime 0.002 (0.004)\tLoss 1.1666 (1.0599)\tPrec@1 60.156 (62.166)\n",
      "Epoch: [59][390/390]\tTime 0.003 (0.003)\tLoss 1.1380 (1.0706)\tPrec@1 56.250 (61.796)\n",
      "EPOCH: 59 train Results: Prec@1 61.796 Loss: 1.0706\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2382 (1.2382)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2687 (1.2976)\tPrec@1 50.000 (54.570)\n",
      "EPOCH: 59 val Results: Prec@1 54.570 Loss: 1.2976\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [60][0/390]\tTime 0.004 (0.004)\tLoss 0.9158 (0.9158)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [60][78/390]\tTime 0.004 (0.003)\tLoss 1.0133 (0.9948)\tPrec@1 58.594 (64.765)\n",
      "Epoch: [60][156/390]\tTime 0.005 (0.003)\tLoss 0.9862 (1.0248)\tPrec@1 60.156 (63.809)\n",
      "Epoch: [60][234/390]\tTime 0.003 (0.003)\tLoss 1.0858 (1.0432)\tPrec@1 60.938 (62.889)\n",
      "Epoch: [60][312/390]\tTime 0.002 (0.003)\tLoss 1.1822 (1.0595)\tPrec@1 56.250 (62.380)\n",
      "Epoch: [60][390/390]\tTime 0.002 (0.003)\tLoss 1.3171 (1.0704)\tPrec@1 50.000 (62.002)\n",
      "EPOCH: 60 train Results: Prec@1 62.002 Loss: 1.0704\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1514 (1.1514)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1446 (1.2835)\tPrec@1 43.750 (54.910)\n",
      "EPOCH: 60 val Results: Prec@1 54.910 Loss: 1.2835\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [61][0/390]\tTime 0.004 (0.004)\tLoss 0.9072 (0.9072)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [61][78/390]\tTime 0.011 (0.003)\tLoss 0.9910 (0.9858)\tPrec@1 63.281 (64.260)\n",
      "Epoch: [61][156/390]\tTime 0.002 (0.003)\tLoss 1.0420 (1.0129)\tPrec@1 65.625 (63.565)\n",
      "Epoch: [61][234/390]\tTime 0.003 (0.003)\tLoss 1.0848 (1.0374)\tPrec@1 55.469 (62.653)\n",
      "Epoch: [61][312/390]\tTime 0.004 (0.003)\tLoss 1.2083 (1.0551)\tPrec@1 59.375 (62.156)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.003)\tLoss 1.1005 (1.0655)\tPrec@1 63.750 (61.770)\n",
      "EPOCH: 61 train Results: Prec@1 61.770 Loss: 1.0655\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1895 (1.1895)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7674 (1.2631)\tPrec@1 25.000 (54.920)\n",
      "EPOCH: 61 val Results: Prec@1 54.920 Loss: 1.2631\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [62][0/390]\tTime 0.003 (0.003)\tLoss 0.9968 (0.9968)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.0617 (0.9917)\tPrec@1 66.406 (64.982)\n",
      "Epoch: [62][156/390]\tTime 0.003 (0.003)\tLoss 0.9058 (1.0198)\tPrec@1 67.969 (63.714)\n",
      "Epoch: [62][234/390]\tTime 0.002 (0.003)\tLoss 1.2886 (1.0417)\tPrec@1 56.250 (62.759)\n",
      "Epoch: [62][312/390]\tTime 0.002 (0.003)\tLoss 1.1485 (1.0577)\tPrec@1 55.469 (62.058)\n",
      "Epoch: [62][390/390]\tTime 0.002 (0.003)\tLoss 0.8766 (1.0707)\tPrec@1 67.500 (61.634)\n",
      "EPOCH: 62 train Results: Prec@1 61.634 Loss: 1.0707\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1791 (1.1791)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2939 (1.2685)\tPrec@1 37.500 (54.910)\n",
      "EPOCH: 62 val Results: Prec@1 54.910 Loss: 1.2685\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [63][0/390]\tTime 0.003 (0.003)\tLoss 0.9571 (0.9571)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [63][78/390]\tTime 0.004 (0.003)\tLoss 1.1089 (0.9909)\tPrec@1 58.594 (64.953)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.003)\tLoss 1.1768 (1.0210)\tPrec@1 61.719 (63.600)\n",
      "Epoch: [63][234/390]\tTime 0.003 (0.003)\tLoss 1.1468 (1.0466)\tPrec@1 57.031 (62.689)\n",
      "Epoch: [63][312/390]\tTime 0.003 (0.003)\tLoss 1.2555 (1.0625)\tPrec@1 53.125 (62.051)\n",
      "Epoch: [63][390/390]\tTime 0.001 (0.003)\tLoss 1.2378 (1.0736)\tPrec@1 58.750 (61.690)\n",
      "EPOCH: 63 train Results: Prec@1 61.690 Loss: 1.0736\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2051 (1.2051)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4459 (1.2618)\tPrec@1 31.250 (55.260)\n",
      "EPOCH: 63 val Results: Prec@1 55.260 Loss: 1.2618\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [64][0/390]\tTime 0.002 (0.002)\tLoss 0.9491 (0.9491)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [64][78/390]\tTime 0.002 (0.003)\tLoss 0.9531 (0.9738)\tPrec@1 70.312 (65.526)\n",
      "Epoch: [64][156/390]\tTime 0.003 (0.003)\tLoss 1.0994 (1.0128)\tPrec@1 60.938 (64.087)\n",
      "Epoch: [64][234/390]\tTime 0.002 (0.003)\tLoss 1.1740 (1.0373)\tPrec@1 59.375 (62.902)\n",
      "Epoch: [64][312/390]\tTime 0.003 (0.003)\tLoss 1.1752 (1.0548)\tPrec@1 56.250 (62.368)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.003)\tLoss 1.2526 (1.0703)\tPrec@1 52.500 (61.868)\n",
      "EPOCH: 64 train Results: Prec@1 61.868 Loss: 1.0703\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1988 (1.1988)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0134 (1.2810)\tPrec@1 43.750 (55.120)\n",
      "EPOCH: 64 val Results: Prec@1 55.120 Loss: 1.2810\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [65][0/390]\tTime 0.010 (0.010)\tLoss 0.9545 (0.9545)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.004)\tLoss 1.0078 (0.9953)\tPrec@1 61.719 (64.765)\n",
      "Epoch: [65][156/390]\tTime 0.004 (0.004)\tLoss 1.0430 (1.0184)\tPrec@1 64.062 (63.804)\n",
      "Epoch: [65][234/390]\tTime 0.002 (0.004)\tLoss 1.1034 (1.0412)\tPrec@1 59.375 (62.955)\n",
      "Epoch: [65][312/390]\tTime 0.002 (0.004)\tLoss 1.0684 (1.0565)\tPrec@1 60.156 (62.323)\n",
      "Epoch: [65][390/390]\tTime 0.003 (0.004)\tLoss 0.9935 (1.0668)\tPrec@1 68.750 (61.888)\n",
      "EPOCH: 65 train Results: Prec@1 61.888 Loss: 1.0668\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1839 (1.1839)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3617 (1.2647)\tPrec@1 37.500 (55.140)\n",
      "EPOCH: 65 val Results: Prec@1 55.140 Loss: 1.2647\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [66][0/390]\tTime 0.003 (0.003)\tLoss 0.9819 (0.9819)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [66][78/390]\tTime 0.002 (0.003)\tLoss 1.1325 (0.9878)\tPrec@1 55.469 (65.061)\n",
      "Epoch: [66][156/390]\tTime 0.004 (0.003)\tLoss 1.0181 (1.0181)\tPrec@1 66.406 (63.913)\n",
      "Epoch: [66][234/390]\tTime 0.003 (0.003)\tLoss 1.3402 (1.0407)\tPrec@1 54.688 (63.009)\n",
      "Epoch: [66][312/390]\tTime 0.005 (0.003)\tLoss 1.1578 (1.0603)\tPrec@1 62.500 (62.323)\n",
      "Epoch: [66][390/390]\tTime 0.002 (0.003)\tLoss 1.0183 (1.0706)\tPrec@1 67.500 (61.866)\n",
      "EPOCH: 66 train Results: Prec@1 61.866 Loss: 1.0706\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1992 (1.1992)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4401 (1.2696)\tPrec@1 43.750 (54.610)\n",
      "EPOCH: 66 val Results: Prec@1 54.610 Loss: 1.2696\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [67][0/390]\tTime 0.002 (0.002)\tLoss 1.0221 (1.0221)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [67][78/390]\tTime 0.004 (0.003)\tLoss 1.1872 (0.9884)\tPrec@1 57.812 (65.645)\n",
      "Epoch: [67][156/390]\tTime 0.002 (0.003)\tLoss 1.1171 (1.0127)\tPrec@1 61.719 (64.296)\n",
      "Epoch: [67][234/390]\tTime 0.002 (0.003)\tLoss 1.2195 (1.0395)\tPrec@1 55.469 (63.195)\n",
      "Epoch: [67][312/390]\tTime 0.005 (0.003)\tLoss 1.1266 (1.0542)\tPrec@1 60.938 (62.530)\n",
      "Epoch: [67][390/390]\tTime 0.002 (0.003)\tLoss 1.0659 (1.0661)\tPrec@1 66.250 (61.966)\n",
      "EPOCH: 67 train Results: Prec@1 61.966 Loss: 1.0661\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2016 (1.2016)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0030 (1.2718)\tPrec@1 62.500 (54.640)\n",
      "EPOCH: 67 val Results: Prec@1 54.640 Loss: 1.2718\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [68][0/390]\tTime 0.005 (0.005)\tLoss 0.8798 (0.8798)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [68][78/390]\tTime 0.002 (0.003)\tLoss 0.9083 (0.9747)\tPrec@1 67.188 (65.398)\n",
      "Epoch: [68][156/390]\tTime 0.002 (0.003)\tLoss 1.2222 (1.0190)\tPrec@1 60.156 (63.570)\n",
      "Epoch: [68][234/390]\tTime 0.005 (0.003)\tLoss 1.0991 (1.0375)\tPrec@1 61.719 (62.862)\n",
      "Epoch: [68][312/390]\tTime 0.002 (0.003)\tLoss 1.2106 (1.0541)\tPrec@1 55.469 (62.460)\n",
      "Epoch: [68][390/390]\tTime 0.003 (0.003)\tLoss 1.3580 (1.0659)\tPrec@1 53.750 (62.004)\n",
      "EPOCH: 68 train Results: Prec@1 62.004 Loss: 1.0659\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1050 (1.1050)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.8262 (1.2645)\tPrec@1 62.500 (54.920)\n",
      "EPOCH: 68 val Results: Prec@1 54.920 Loss: 1.2645\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [69][0/390]\tTime 0.004 (0.004)\tLoss 0.9525 (0.9525)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [69][78/390]\tTime 0.002 (0.003)\tLoss 1.0198 (0.9777)\tPrec@1 60.156 (65.833)\n",
      "Epoch: [69][156/390]\tTime 0.002 (0.003)\tLoss 1.0128 (1.0094)\tPrec@1 70.312 (64.476)\n",
      "Epoch: [69][234/390]\tTime 0.002 (0.004)\tLoss 0.9818 (1.0323)\tPrec@1 61.719 (63.561)\n",
      "Epoch: [69][312/390]\tTime 0.002 (0.004)\tLoss 1.1322 (1.0498)\tPrec@1 62.500 (62.690)\n",
      "Epoch: [69][390/390]\tTime 0.003 (0.003)\tLoss 1.2601 (1.0654)\tPrec@1 56.250 (62.114)\n",
      "EPOCH: 69 train Results: Prec@1 62.114 Loss: 1.0654\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2196 (1.2196)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3725 (1.2743)\tPrec@1 50.000 (55.430)\n",
      "EPOCH: 69 val Results: Prec@1 55.430 Loss: 1.2743\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [70][0/390]\tTime 0.003 (0.003)\tLoss 0.7541 (0.7541)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [70][78/390]\tTime 0.003 (0.003)\tLoss 1.2142 (0.9845)\tPrec@1 51.562 (64.893)\n",
      "Epoch: [70][156/390]\tTime 0.002 (0.003)\tLoss 1.1103 (1.0141)\tPrec@1 63.281 (63.709)\n",
      "Epoch: [70][234/390]\tTime 0.003 (0.003)\tLoss 1.0225 (1.0340)\tPrec@1 67.969 (63.331)\n",
      "Epoch: [70][312/390]\tTime 0.004 (0.004)\tLoss 1.0710 (1.0564)\tPrec@1 60.938 (62.507)\n",
      "Epoch: [70][390/390]\tTime 0.003 (0.004)\tLoss 1.1094 (1.0667)\tPrec@1 55.000 (62.032)\n",
      "EPOCH: 70 train Results: Prec@1 62.032 Loss: 1.0667\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2125 (1.2125)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4177 (1.2719)\tPrec@1 50.000 (54.550)\n",
      "EPOCH: 70 val Results: Prec@1 54.550 Loss: 1.2719\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [71][0/390]\tTime 0.002 (0.002)\tLoss 1.0286 (1.0286)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [71][78/390]\tTime 0.004 (0.003)\tLoss 1.0175 (0.9819)\tPrec@1 62.500 (65.378)\n",
      "Epoch: [71][156/390]\tTime 0.005 (0.003)\tLoss 1.0056 (1.0152)\tPrec@1 63.281 (64.107)\n",
      "Epoch: [71][234/390]\tTime 0.002 (0.003)\tLoss 0.9194 (1.0388)\tPrec@1 62.500 (63.108)\n",
      "Epoch: [71][312/390]\tTime 0.002 (0.003)\tLoss 1.0313 (1.0508)\tPrec@1 60.156 (62.632)\n",
      "Epoch: [71][390/390]\tTime 0.001 (0.003)\tLoss 0.9820 (1.0651)\tPrec@1 67.500 (61.920)\n",
      "EPOCH: 71 train Results: Prec@1 61.920 Loss: 1.0651\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2738 (1.2738)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2815 (1.2779)\tPrec@1 37.500 (54.970)\n",
      "EPOCH: 71 val Results: Prec@1 54.970 Loss: 1.2779\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [72][0/390]\tTime 0.004 (0.004)\tLoss 1.0390 (1.0390)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [72][78/390]\tTime 0.002 (0.003)\tLoss 1.2249 (0.9866)\tPrec@1 54.688 (65.487)\n",
      "Epoch: [72][156/390]\tTime 0.004 (0.003)\tLoss 1.1287 (1.0140)\tPrec@1 60.938 (64.157)\n",
      "Epoch: [72][234/390]\tTime 0.005 (0.003)\tLoss 1.1005 (1.0336)\tPrec@1 64.062 (63.314)\n",
      "Epoch: [72][312/390]\tTime 0.007 (0.003)\tLoss 1.2333 (1.0517)\tPrec@1 58.594 (62.680)\n",
      "Epoch: [72][390/390]\tTime 0.001 (0.003)\tLoss 0.8444 (1.0626)\tPrec@1 65.000 (62.222)\n",
      "EPOCH: 72 train Results: Prec@1 62.222 Loss: 1.0626\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1258 (1.1258)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4061 (1.2753)\tPrec@1 56.250 (54.810)\n",
      "EPOCH: 72 val Results: Prec@1 54.810 Loss: 1.2753\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [73][0/390]\tTime 0.003 (0.003)\tLoss 0.9689 (0.9689)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.003)\tLoss 0.9899 (0.9910)\tPrec@1 64.062 (64.784)\n",
      "Epoch: [73][156/390]\tTime 0.002 (0.004)\tLoss 1.0963 (1.0106)\tPrec@1 63.281 (63.993)\n",
      "Epoch: [73][234/390]\tTime 0.003 (0.004)\tLoss 0.9948 (1.0305)\tPrec@1 63.281 (63.075)\n",
      "Epoch: [73][312/390]\tTime 0.002 (0.004)\tLoss 1.0090 (1.0519)\tPrec@1 61.719 (62.288)\n",
      "Epoch: [73][390/390]\tTime 0.007 (0.004)\tLoss 1.1782 (1.0619)\tPrec@1 53.750 (61.948)\n",
      "EPOCH: 73 train Results: Prec@1 61.948 Loss: 1.0619\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1705 (1.1705)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 0.9942 (1.2790)\tPrec@1 68.750 (55.110)\n",
      "EPOCH: 73 val Results: Prec@1 55.110 Loss: 1.2790\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [74][0/390]\tTime 0.002 (0.002)\tLoss 1.0388 (1.0388)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [74][78/390]\tTime 0.002 (0.003)\tLoss 0.9061 (0.9887)\tPrec@1 66.406 (65.220)\n",
      "Epoch: [74][156/390]\tTime 0.004 (0.003)\tLoss 0.9261 (1.0207)\tPrec@1 64.844 (63.819)\n",
      "Epoch: [74][234/390]\tTime 0.003 (0.003)\tLoss 1.0667 (1.0404)\tPrec@1 66.406 (63.049)\n",
      "Epoch: [74][312/390]\tTime 0.002 (0.003)\tLoss 1.0523 (1.0523)\tPrec@1 60.156 (62.410)\n",
      "Epoch: [74][390/390]\tTime 0.001 (0.003)\tLoss 1.1253 (1.0637)\tPrec@1 63.750 (62.070)\n",
      "EPOCH: 74 train Results: Prec@1 62.070 Loss: 1.0637\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2022 (1.2022)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4141 (1.2841)\tPrec@1 43.750 (54.530)\n",
      "EPOCH: 74 val Results: Prec@1 54.530 Loss: 1.2841\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [75][0/390]\tTime 0.004 (0.004)\tLoss 0.9669 (0.9669)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [75][78/390]\tTime 0.003 (0.004)\tLoss 1.2110 (0.9807)\tPrec@1 60.938 (64.893)\n",
      "Epoch: [75][156/390]\tTime 0.002 (0.004)\tLoss 0.8740 (1.0158)\tPrec@1 69.531 (63.286)\n",
      "Epoch: [75][234/390]\tTime 0.004 (0.004)\tLoss 1.2140 (1.0411)\tPrec@1 55.469 (62.663)\n",
      "Epoch: [75][312/390]\tTime 0.004 (0.004)\tLoss 1.2055 (1.0575)\tPrec@1 55.469 (62.148)\n",
      "Epoch: [75][390/390]\tTime 0.002 (0.003)\tLoss 1.1497 (1.0701)\tPrec@1 57.500 (61.730)\n",
      "EPOCH: 75 train Results: Prec@1 61.730 Loss: 1.0701\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1655 (1.1655)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2079 (1.2671)\tPrec@1 43.750 (55.190)\n",
      "EPOCH: 75 val Results: Prec@1 55.190 Loss: 1.2671\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [76][0/390]\tTime 0.002 (0.002)\tLoss 1.1168 (1.1168)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.003)\tLoss 1.0235 (0.9857)\tPrec@1 61.719 (64.735)\n",
      "Epoch: [76][156/390]\tTime 0.002 (0.003)\tLoss 1.0222 (1.0168)\tPrec@1 67.969 (63.774)\n",
      "Epoch: [76][234/390]\tTime 0.002 (0.003)\tLoss 1.0875 (1.0409)\tPrec@1 65.625 (62.779)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 0.9003 (1.0536)\tPrec@1 65.625 (62.270)\n",
      "Epoch: [76][390/390]\tTime 0.006 (0.003)\tLoss 0.8558 (1.0664)\tPrec@1 66.250 (61.786)\n",
      "EPOCH: 76 train Results: Prec@1 61.786 Loss: 1.0664\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1925 (1.1925)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6263 (1.2688)\tPrec@1 43.750 (55.060)\n",
      "EPOCH: 76 val Results: Prec@1 55.060 Loss: 1.2688\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [77][0/390]\tTime 0.006 (0.006)\tLoss 0.9726 (0.9726)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [77][78/390]\tTime 0.002 (0.003)\tLoss 1.0122 (0.9954)\tPrec@1 64.844 (64.241)\n",
      "Epoch: [77][156/390]\tTime 0.003 (0.003)\tLoss 1.1409 (1.0157)\tPrec@1 57.812 (63.416)\n",
      "Epoch: [77][234/390]\tTime 0.002 (0.003)\tLoss 1.0163 (1.0361)\tPrec@1 65.625 (62.680)\n",
      "Epoch: [77][312/390]\tTime 0.007 (0.003)\tLoss 1.1320 (1.0508)\tPrec@1 57.812 (62.188)\n",
      "Epoch: [77][390/390]\tTime 0.001 (0.003)\tLoss 1.1676 (1.0652)\tPrec@1 60.000 (61.732)\n",
      "EPOCH: 77 train Results: Prec@1 61.732 Loss: 1.0652\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1290 (1.1290)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4022 (1.2615)\tPrec@1 50.000 (55.220)\n",
      "EPOCH: 77 val Results: Prec@1 55.220 Loss: 1.2615\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [78][0/390]\tTime 0.003 (0.003)\tLoss 0.8747 (0.8747)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [78][78/390]\tTime 0.002 (0.004)\tLoss 1.0990 (0.9601)\tPrec@1 61.719 (66.228)\n",
      "Epoch: [78][156/390]\tTime 0.002 (0.003)\tLoss 1.1035 (1.0067)\tPrec@1 60.156 (64.545)\n",
      "Epoch: [78][234/390]\tTime 0.003 (0.003)\tLoss 1.0249 (1.0348)\tPrec@1 60.156 (63.487)\n",
      "Epoch: [78][312/390]\tTime 0.005 (0.003)\tLoss 1.0723 (1.0536)\tPrec@1 65.625 (62.657)\n",
      "Epoch: [78][390/390]\tTime 0.001 (0.003)\tLoss 0.9720 (1.0640)\tPrec@1 65.000 (62.104)\n",
      "EPOCH: 78 train Results: Prec@1 62.104 Loss: 1.0640\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1391 (1.1391)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1357 (1.2646)\tPrec@1 56.250 (54.710)\n",
      "EPOCH: 78 val Results: Prec@1 54.710 Loss: 1.2646\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [79][0/390]\tTime 0.003 (0.003)\tLoss 0.8318 (0.8318)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [79][78/390]\tTime 0.003 (0.004)\tLoss 0.9445 (0.9632)\tPrec@1 60.938 (65.348)\n",
      "Epoch: [79][156/390]\tTime 0.002 (0.003)\tLoss 0.9798 (0.9932)\tPrec@1 65.625 (64.436)\n",
      "Epoch: [79][234/390]\tTime 0.006 (0.003)\tLoss 1.0303 (1.0233)\tPrec@1 64.844 (63.344)\n",
      "Epoch: [79][312/390]\tTime 0.002 (0.003)\tLoss 1.3173 (1.0500)\tPrec@1 53.125 (62.657)\n",
      "Epoch: [79][390/390]\tTime 0.001 (0.003)\tLoss 1.0160 (1.0650)\tPrec@1 61.250 (62.110)\n",
      "EPOCH: 79 train Results: Prec@1 62.110 Loss: 1.0650\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2207 (1.2207)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1655 (1.2620)\tPrec@1 62.500 (54.820)\n",
      "EPOCH: 79 val Results: Prec@1 54.820 Loss: 1.2620\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [80][0/390]\tTime 0.004 (0.004)\tLoss 1.0161 (1.0161)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [80][78/390]\tTime 0.004 (0.003)\tLoss 0.9587 (0.9852)\tPrec@1 64.062 (65.140)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.0728 (1.0156)\tPrec@1 57.031 (63.814)\n",
      "Epoch: [80][234/390]\tTime 0.003 (0.003)\tLoss 1.0965 (1.0367)\tPrec@1 55.469 (63.175)\n",
      "Epoch: [80][312/390]\tTime 0.002 (0.003)\tLoss 1.2717 (1.0535)\tPrec@1 53.125 (62.512)\n",
      "Epoch: [80][390/390]\tTime 0.002 (0.003)\tLoss 1.1460 (1.0649)\tPrec@1 58.750 (62.100)\n",
      "EPOCH: 80 train Results: Prec@1 62.100 Loss: 1.0649\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1631 (1.1631)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4021 (1.2693)\tPrec@1 43.750 (55.580)\n",
      "EPOCH: 80 val Results: Prec@1 55.580 Loss: 1.2693\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [81][0/390]\tTime 0.005 (0.005)\tLoss 0.9148 (0.9148)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [81][78/390]\tTime 0.002 (0.003)\tLoss 0.9205 (0.9692)\tPrec@1 64.062 (65.526)\n",
      "Epoch: [81][156/390]\tTime 0.007 (0.003)\tLoss 0.8922 (1.0061)\tPrec@1 66.406 (64.346)\n",
      "Epoch: [81][234/390]\tTime 0.002 (0.003)\tLoss 0.9361 (1.0326)\tPrec@1 68.750 (63.408)\n",
      "Epoch: [81][312/390]\tTime 0.002 (0.003)\tLoss 1.3069 (1.0521)\tPrec@1 52.344 (62.710)\n",
      "Epoch: [81][390/390]\tTime 0.001 (0.003)\tLoss 1.0195 (1.0663)\tPrec@1 66.250 (62.158)\n",
      "EPOCH: 81 train Results: Prec@1 62.158 Loss: 1.0663\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2828 (1.2828)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4324 (1.2698)\tPrec@1 37.500 (54.580)\n",
      "EPOCH: 81 val Results: Prec@1 54.580 Loss: 1.2698\n",
      "Best Prec@1: 55.660\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [82][0/390]\tTime 0.005 (0.005)\tLoss 0.9351 (0.9351)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [82][78/390]\tTime 0.002 (0.004)\tLoss 0.9028 (0.9757)\tPrec@1 66.406 (65.259)\n",
      "Epoch: [82][156/390]\tTime 0.003 (0.004)\tLoss 1.1004 (1.0059)\tPrec@1 64.062 (63.968)\n",
      "Epoch: [82][234/390]\tTime 0.002 (0.004)\tLoss 1.1099 (1.0302)\tPrec@1 57.812 (63.059)\n",
      "Epoch: [82][312/390]\tTime 0.002 (0.003)\tLoss 0.9926 (1.0446)\tPrec@1 63.281 (62.562)\n",
      "Epoch: [82][390/390]\tTime 0.001 (0.003)\tLoss 1.0918 (1.0622)\tPrec@1 56.250 (62.004)\n",
      "EPOCH: 82 train Results: Prec@1 62.004 Loss: 1.0622\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1971 (1.1971)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3234 (1.2552)\tPrec@1 25.000 (55.790)\n",
      "EPOCH: 82 val Results: Prec@1 55.790 Loss: 1.2552\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [83][0/390]\tTime 0.002 (0.002)\tLoss 0.9606 (0.9606)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [83][78/390]\tTime 0.002 (0.003)\tLoss 1.1387 (0.9895)\tPrec@1 60.938 (64.933)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.1759 (1.0202)\tPrec@1 56.250 (63.699)\n",
      "Epoch: [83][234/390]\tTime 0.002 (0.004)\tLoss 1.0148 (1.0391)\tPrec@1 64.062 (62.965)\n",
      "Epoch: [83][312/390]\tTime 0.003 (0.003)\tLoss 1.2163 (1.0593)\tPrec@1 56.250 (62.255)\n",
      "Epoch: [83][390/390]\tTime 0.003 (0.003)\tLoss 1.1949 (1.0683)\tPrec@1 56.250 (61.828)\n",
      "EPOCH: 83 train Results: Prec@1 61.828 Loss: 1.0683\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1398 (1.1398)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1878 (1.2673)\tPrec@1 43.750 (55.760)\n",
      "EPOCH: 83 val Results: Prec@1 55.760 Loss: 1.2673\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [84][0/390]\tTime 0.011 (0.011)\tLoss 0.8526 (0.8526)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [84][78/390]\tTime 0.002 (0.004)\tLoss 0.8975 (1.0057)\tPrec@1 70.312 (64.597)\n",
      "Epoch: [84][156/390]\tTime 0.002 (0.004)\tLoss 1.1596 (1.0194)\tPrec@1 59.375 (63.834)\n",
      "Epoch: [84][234/390]\tTime 0.004 (0.004)\tLoss 1.0848 (1.0425)\tPrec@1 67.188 (62.866)\n",
      "Epoch: [84][312/390]\tTime 0.003 (0.003)\tLoss 1.0593 (1.0540)\tPrec@1 61.719 (62.480)\n",
      "Epoch: [84][390/390]\tTime 0.008 (0.003)\tLoss 1.3506 (1.0591)\tPrec@1 55.000 (62.264)\n",
      "EPOCH: 84 train Results: Prec@1 62.264 Loss: 1.0591\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1658 (1.1658)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2707 (1.2713)\tPrec@1 37.500 (55.200)\n",
      "EPOCH: 84 val Results: Prec@1 55.200 Loss: 1.2713\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [85][0/390]\tTime 0.002 (0.002)\tLoss 0.8915 (0.8915)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [85][78/390]\tTime 0.003 (0.003)\tLoss 0.8670 (0.9719)\tPrec@1 67.969 (65.239)\n",
      "Epoch: [85][156/390]\tTime 0.004 (0.003)\tLoss 0.8077 (1.0031)\tPrec@1 71.875 (64.167)\n",
      "Epoch: [85][234/390]\tTime 0.007 (0.003)\tLoss 1.2353 (1.0308)\tPrec@1 57.031 (63.231)\n",
      "Epoch: [85][312/390]\tTime 0.002 (0.003)\tLoss 0.9834 (1.0491)\tPrec@1 67.188 (62.555)\n",
      "Epoch: [85][390/390]\tTime 0.003 (0.003)\tLoss 1.1295 (1.0632)\tPrec@1 52.500 (62.062)\n",
      "EPOCH: 85 train Results: Prec@1 62.062 Loss: 1.0632\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1307 (1.1307)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2019 (1.2602)\tPrec@1 56.250 (55.560)\n",
      "EPOCH: 85 val Results: Prec@1 55.560 Loss: 1.2602\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [86][0/390]\tTime 0.003 (0.003)\tLoss 0.9748 (0.9748)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [86][78/390]\tTime 0.002 (0.004)\tLoss 0.9400 (0.9892)\tPrec@1 65.625 (65.210)\n",
      "Epoch: [86][156/390]\tTime 0.003 (0.004)\tLoss 0.8549 (1.0071)\tPrec@1 65.625 (64.127)\n",
      "Epoch: [86][234/390]\tTime 0.002 (0.004)\tLoss 1.0029 (1.0281)\tPrec@1 64.062 (63.471)\n",
      "Epoch: [86][312/390]\tTime 0.003 (0.004)\tLoss 1.3038 (1.0443)\tPrec@1 53.906 (62.907)\n",
      "Epoch: [86][390/390]\tTime 0.002 (0.004)\tLoss 1.1256 (1.0595)\tPrec@1 60.000 (62.234)\n",
      "EPOCH: 86 train Results: Prec@1 62.234 Loss: 1.0595\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1927 (1.1927)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2393 (1.2746)\tPrec@1 43.750 (55.200)\n",
      "EPOCH: 86 val Results: Prec@1 55.200 Loss: 1.2746\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [87][0/390]\tTime 0.006 (0.006)\tLoss 1.0315 (1.0315)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [87][78/390]\tTime 0.004 (0.003)\tLoss 0.8143 (0.9665)\tPrec@1 71.875 (65.576)\n",
      "Epoch: [87][156/390]\tTime 0.006 (0.003)\tLoss 1.0230 (1.0046)\tPrec@1 60.156 (64.281)\n",
      "Epoch: [87][234/390]\tTime 0.006 (0.003)\tLoss 1.0919 (1.0286)\tPrec@1 65.625 (63.401)\n",
      "Epoch: [87][312/390]\tTime 0.002 (0.003)\tLoss 1.1148 (1.0491)\tPrec@1 58.594 (62.775)\n",
      "Epoch: [87][390/390]\tTime 0.002 (0.003)\tLoss 1.2686 (1.0586)\tPrec@1 51.250 (62.448)\n",
      "EPOCH: 87 train Results: Prec@1 62.448 Loss: 1.0586\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1693 (1.1693)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9613 (1.2784)\tPrec@1 50.000 (55.200)\n",
      "EPOCH: 87 val Results: Prec@1 55.200 Loss: 1.2784\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [88][0/390]\tTime 0.006 (0.006)\tLoss 0.9419 (0.9419)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [88][78/390]\tTime 0.003 (0.003)\tLoss 0.9799 (0.9628)\tPrec@1 65.625 (65.843)\n",
      "Epoch: [88][156/390]\tTime 0.002 (0.003)\tLoss 1.1192 (1.0097)\tPrec@1 60.938 (63.983)\n",
      "Epoch: [88][234/390]\tTime 0.003 (0.003)\tLoss 1.1366 (1.0305)\tPrec@1 59.375 (63.138)\n",
      "Epoch: [88][312/390]\tTime 0.003 (0.003)\tLoss 1.1030 (1.0516)\tPrec@1 58.594 (62.318)\n",
      "Epoch: [88][390/390]\tTime 0.002 (0.003)\tLoss 1.3147 (1.0635)\tPrec@1 58.750 (61.956)\n",
      "EPOCH: 88 train Results: Prec@1 61.956 Loss: 1.0635\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1932 (1.1932)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1640 (1.2678)\tPrec@1 43.750 (55.320)\n",
      "EPOCH: 88 val Results: Prec@1 55.320 Loss: 1.2678\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [89][0/390]\tTime 0.002 (0.002)\tLoss 0.8929 (0.8929)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.003)\tLoss 1.0076 (0.9927)\tPrec@1 65.625 (64.171)\n",
      "Epoch: [89][156/390]\tTime 0.002 (0.003)\tLoss 1.1859 (1.0159)\tPrec@1 57.031 (63.436)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.003)\tLoss 0.9890 (1.0385)\tPrec@1 67.969 (62.643)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.003)\tLoss 1.0659 (1.0542)\tPrec@1 63.281 (62.116)\n",
      "Epoch: [89][390/390]\tTime 0.006 (0.003)\tLoss 1.2881 (1.0670)\tPrec@1 52.500 (61.688)\n",
      "EPOCH: 89 train Results: Prec@1 61.688 Loss: 1.0670\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1691 (1.1691)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4950 (1.2795)\tPrec@1 43.750 (55.420)\n",
      "EPOCH: 89 val Results: Prec@1 55.420 Loss: 1.2795\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [90][0/390]\tTime 0.005 (0.005)\tLoss 0.9755 (0.9755)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [90][78/390]\tTime 0.003 (0.003)\tLoss 1.0221 (0.9705)\tPrec@1 66.406 (65.724)\n",
      "Epoch: [90][156/390]\tTime 0.004 (0.003)\tLoss 1.1651 (1.0061)\tPrec@1 60.938 (64.461)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.0531 (1.0315)\tPrec@1 60.938 (63.361)\n",
      "Epoch: [90][312/390]\tTime 0.003 (0.003)\tLoss 1.2831 (1.0453)\tPrec@1 57.812 (62.765)\n",
      "Epoch: [90][390/390]\tTime 0.004 (0.003)\tLoss 0.9745 (1.0591)\tPrec@1 62.500 (62.330)\n",
      "EPOCH: 90 train Results: Prec@1 62.330 Loss: 1.0591\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1122 (1.1122)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6762 (1.2926)\tPrec@1 37.500 (54.310)\n",
      "EPOCH: 90 val Results: Prec@1 54.310 Loss: 1.2926\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [91][0/390]\tTime 0.004 (0.004)\tLoss 1.0534 (1.0534)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [91][78/390]\tTime 0.002 (0.003)\tLoss 1.0326 (0.9701)\tPrec@1 64.844 (65.625)\n",
      "Epoch: [91][156/390]\tTime 0.011 (0.003)\tLoss 1.3308 (1.0072)\tPrec@1 53.125 (64.087)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.003)\tLoss 1.1132 (1.0250)\tPrec@1 60.938 (63.351)\n",
      "Epoch: [91][312/390]\tTime 0.002 (0.003)\tLoss 1.1395 (1.0462)\tPrec@1 59.375 (62.587)\n",
      "Epoch: [91][390/390]\tTime 0.002 (0.003)\tLoss 1.1996 (1.0616)\tPrec@1 56.250 (61.952)\n",
      "EPOCH: 91 train Results: Prec@1 61.952 Loss: 1.0616\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1136 (1.1136)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3611 (1.2696)\tPrec@1 56.250 (55.330)\n",
      "EPOCH: 91 val Results: Prec@1 55.330 Loss: 1.2696\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [92][0/390]\tTime 0.002 (0.002)\tLoss 1.1540 (1.1540)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [92][78/390]\tTime 0.005 (0.003)\tLoss 0.9639 (0.9795)\tPrec@1 63.281 (64.715)\n",
      "Epoch: [92][156/390]\tTime 0.005 (0.003)\tLoss 1.0628 (1.0138)\tPrec@1 63.281 (63.664)\n",
      "Epoch: [92][234/390]\tTime 0.002 (0.003)\tLoss 1.0614 (1.0341)\tPrec@1 65.625 (62.783)\n",
      "Epoch: [92][312/390]\tTime 0.002 (0.003)\tLoss 1.1652 (1.0501)\tPrec@1 55.469 (62.360)\n",
      "Epoch: [92][390/390]\tTime 0.001 (0.003)\tLoss 1.0475 (1.0648)\tPrec@1 61.250 (61.852)\n",
      "EPOCH: 92 train Results: Prec@1 61.852 Loss: 1.0648\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1955 (1.1955)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2089 (1.2803)\tPrec@1 50.000 (55.210)\n",
      "EPOCH: 92 val Results: Prec@1 55.210 Loss: 1.2803\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [93][0/390]\tTime 0.003 (0.003)\tLoss 0.9554 (0.9554)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.004)\tLoss 0.9655 (0.9542)\tPrec@1 64.844 (66.149)\n",
      "Epoch: [93][156/390]\tTime 0.005 (0.004)\tLoss 1.0139 (0.9930)\tPrec@1 64.062 (64.600)\n",
      "Epoch: [93][234/390]\tTime 0.002 (0.003)\tLoss 1.0462 (1.0198)\tPrec@1 62.500 (63.507)\n",
      "Epoch: [93][312/390]\tTime 0.002 (0.003)\tLoss 1.2201 (1.0435)\tPrec@1 60.156 (62.478)\n",
      "Epoch: [93][390/390]\tTime 0.001 (0.003)\tLoss 1.0452 (1.0592)\tPrec@1 65.000 (61.932)\n",
      "EPOCH: 93 train Results: Prec@1 61.932 Loss: 1.0592\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1378 (1.1378)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3352 (1.2745)\tPrec@1 25.000 (55.110)\n",
      "EPOCH: 93 val Results: Prec@1 55.110 Loss: 1.2745\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [94][0/390]\tTime 0.002 (0.002)\tLoss 0.8318 (0.8318)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [94][78/390]\tTime 0.003 (0.003)\tLoss 0.9812 (0.9638)\tPrec@1 68.750 (66.149)\n",
      "Epoch: [94][156/390]\tTime 0.002 (0.003)\tLoss 1.0180 (0.9973)\tPrec@1 63.281 (64.739)\n",
      "Epoch: [94][234/390]\tTime 0.002 (0.003)\tLoss 0.9694 (1.0314)\tPrec@1 67.969 (63.331)\n",
      "Epoch: [94][312/390]\tTime 0.009 (0.003)\tLoss 1.2382 (1.0503)\tPrec@1 59.375 (62.595)\n",
      "Epoch: [94][390/390]\tTime 0.001 (0.003)\tLoss 1.2668 (1.0658)\tPrec@1 61.250 (62.030)\n",
      "EPOCH: 94 train Results: Prec@1 62.030 Loss: 1.0658\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1911 (1.1911)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1627 (1.2737)\tPrec@1 37.500 (54.790)\n",
      "EPOCH: 94 val Results: Prec@1 54.790 Loss: 1.2737\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [95][0/390]\tTime 0.002 (0.002)\tLoss 1.0341 (1.0341)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [95][78/390]\tTime 0.002 (0.003)\tLoss 1.0791 (0.9717)\tPrec@1 66.406 (65.051)\n",
      "Epoch: [95][156/390]\tTime 0.004 (0.003)\tLoss 0.9830 (1.0079)\tPrec@1 63.281 (63.878)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.003)\tLoss 1.1847 (1.0332)\tPrec@1 57.031 (63.112)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.003)\tLoss 1.1329 (1.0490)\tPrec@1 59.375 (62.468)\n",
      "Epoch: [95][390/390]\tTime 0.006 (0.003)\tLoss 1.1293 (1.0626)\tPrec@1 62.500 (62.072)\n",
      "EPOCH: 95 train Results: Prec@1 62.072 Loss: 1.0626\n",
      "Test: [0/78]\tTime 0.040 (0.040)\tLoss 1.1364 (1.1364)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.002)\tLoss 1.1954 (1.2746)\tPrec@1 37.500 (55.200)\n",
      "EPOCH: 95 val Results: Prec@1 55.200 Loss: 1.2746\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [96][0/390]\tTime 0.006 (0.006)\tLoss 0.9241 (0.9241)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [96][78/390]\tTime 0.002 (0.003)\tLoss 0.9960 (0.9711)\tPrec@1 64.844 (65.378)\n",
      "Epoch: [96][156/390]\tTime 0.002 (0.003)\tLoss 0.8921 (1.0062)\tPrec@1 66.406 (64.321)\n",
      "Epoch: [96][234/390]\tTime 0.002 (0.003)\tLoss 1.0160 (1.0308)\tPrec@1 63.281 (63.288)\n",
      "Epoch: [96][312/390]\tTime 0.002 (0.003)\tLoss 0.9326 (1.0420)\tPrec@1 66.406 (62.867)\n",
      "Epoch: [96][390/390]\tTime 0.007 (0.003)\tLoss 1.1494 (1.0571)\tPrec@1 61.250 (62.254)\n",
      "EPOCH: 96 train Results: Prec@1 62.254 Loss: 1.0571\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.2040 (1.2040)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0675 (1.2755)\tPrec@1 50.000 (54.450)\n",
      "EPOCH: 96 val Results: Prec@1 54.450 Loss: 1.2755\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [97][0/390]\tTime 0.006 (0.006)\tLoss 0.9709 (0.9709)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [97][78/390]\tTime 0.005 (0.003)\tLoss 0.9844 (0.9898)\tPrec@1 60.156 (64.537)\n",
      "Epoch: [97][156/390]\tTime 0.002 (0.003)\tLoss 1.2118 (1.0171)\tPrec@1 59.375 (63.844)\n",
      "Epoch: [97][234/390]\tTime 0.002 (0.003)\tLoss 1.2107 (1.0367)\tPrec@1 56.250 (62.975)\n",
      "Epoch: [97][312/390]\tTime 0.003 (0.003)\tLoss 1.0523 (1.0517)\tPrec@1 61.719 (62.450)\n",
      "Epoch: [97][390/390]\tTime 0.001 (0.003)\tLoss 1.0625 (1.0650)\tPrec@1 65.000 (61.992)\n",
      "EPOCH: 97 train Results: Prec@1 61.992 Loss: 1.0650\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1975 (1.1975)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.0948 (1.2903)\tPrec@1 43.750 (54.680)\n",
      "EPOCH: 97 val Results: Prec@1 54.680 Loss: 1.2903\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [98][0/390]\tTime 0.004 (0.004)\tLoss 1.0418 (1.0418)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [98][78/390]\tTime 0.004 (0.003)\tLoss 0.8694 (0.9697)\tPrec@1 69.531 (65.309)\n",
      "Epoch: [98][156/390]\tTime 0.002 (0.003)\tLoss 1.2069 (1.0060)\tPrec@1 60.156 (64.013)\n",
      "Epoch: [98][234/390]\tTime 0.002 (0.003)\tLoss 0.9936 (1.0280)\tPrec@1 68.750 (63.195)\n",
      "Epoch: [98][312/390]\tTime 0.003 (0.003)\tLoss 1.2980 (1.0461)\tPrec@1 52.344 (62.595)\n",
      "Epoch: [98][390/390]\tTime 0.003 (0.003)\tLoss 1.1057 (1.0620)\tPrec@1 60.000 (62.048)\n",
      "EPOCH: 98 train Results: Prec@1 62.048 Loss: 1.0620\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2034 (1.2034)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6856 (1.2892)\tPrec@1 18.750 (54.260)\n",
      "EPOCH: 98 val Results: Prec@1 54.260 Loss: 1.2892\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [99][0/390]\tTime 0.002 (0.002)\tLoss 0.9254 (0.9254)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.003)\tLoss 0.9087 (0.9962)\tPrec@1 70.312 (65.111)\n",
      "Epoch: [99][156/390]\tTime 0.006 (0.003)\tLoss 1.0571 (1.0168)\tPrec@1 64.062 (64.147)\n",
      "Epoch: [99][234/390]\tTime 0.003 (0.003)\tLoss 1.3252 (1.0309)\tPrec@1 53.906 (63.491)\n",
      "Epoch: [99][312/390]\tTime 0.006 (0.003)\tLoss 1.0726 (1.0487)\tPrec@1 61.719 (62.720)\n",
      "Epoch: [99][390/390]\tTime 0.003 (0.003)\tLoss 0.8657 (1.0613)\tPrec@1 72.500 (62.138)\n",
      "EPOCH: 99 train Results: Prec@1 62.138 Loss: 1.0613\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1817 (1.1817)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6488 (1.2657)\tPrec@1 31.250 (55.650)\n",
      "EPOCH: 99 val Results: Prec@1 55.650 Loss: 1.2657\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [100][0/390]\tTime 0.008 (0.008)\tLoss 0.8834 (0.8834)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [100][78/390]\tTime 0.002 (0.003)\tLoss 1.0262 (0.9761)\tPrec@1 64.062 (65.170)\n",
      "Epoch: [100][156/390]\tTime 0.002 (0.003)\tLoss 1.0628 (1.0028)\tPrec@1 61.719 (64.331)\n",
      "Epoch: [100][234/390]\tTime 0.003 (0.003)\tLoss 1.0907 (1.0306)\tPrec@1 60.938 (63.401)\n",
      "Epoch: [100][312/390]\tTime 0.013 (0.003)\tLoss 1.0557 (1.0516)\tPrec@1 65.625 (62.730)\n",
      "Epoch: [100][390/390]\tTime 0.003 (0.003)\tLoss 1.3214 (1.0626)\tPrec@1 55.000 (62.342)\n",
      "EPOCH: 100 train Results: Prec@1 62.342 Loss: 1.0626\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1823 (1.1823)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5954 (1.2666)\tPrec@1 31.250 (55.300)\n",
      "EPOCH: 100 val Results: Prec@1 55.300 Loss: 1.2666\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [101][0/390]\tTime 0.004 (0.004)\tLoss 1.0176 (1.0176)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [101][78/390]\tTime 0.002 (0.003)\tLoss 0.9925 (0.9918)\tPrec@1 62.500 (64.953)\n",
      "Epoch: [101][156/390]\tTime 0.007 (0.003)\tLoss 1.1441 (1.0132)\tPrec@1 57.031 (64.087)\n",
      "Epoch: [101][234/390]\tTime 0.002 (0.003)\tLoss 1.0471 (1.0356)\tPrec@1 61.719 (63.115)\n",
      "Epoch: [101][312/390]\tTime 0.002 (0.003)\tLoss 1.0731 (1.0459)\tPrec@1 60.156 (62.590)\n",
      "Epoch: [101][390/390]\tTime 0.002 (0.003)\tLoss 1.0754 (1.0613)\tPrec@1 57.500 (62.104)\n",
      "EPOCH: 101 train Results: Prec@1 62.104 Loss: 1.0613\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2287 (1.2287)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3841 (1.2959)\tPrec@1 43.750 (54.150)\n",
      "EPOCH: 101 val Results: Prec@1 54.150 Loss: 1.2959\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [102][0/390]\tTime 0.006 (0.006)\tLoss 1.0303 (1.0303)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [102][78/390]\tTime 0.002 (0.003)\tLoss 1.0467 (0.9781)\tPrec@1 57.031 (64.814)\n",
      "Epoch: [102][156/390]\tTime 0.003 (0.003)\tLoss 0.9211 (1.0124)\tPrec@1 70.312 (63.898)\n",
      "Epoch: [102][234/390]\tTime 0.003 (0.003)\tLoss 1.0540 (1.0272)\tPrec@1 66.406 (63.441)\n",
      "Epoch: [102][312/390]\tTime 0.003 (0.003)\tLoss 1.1175 (1.0453)\tPrec@1 60.156 (62.872)\n",
      "Epoch: [102][390/390]\tTime 0.004 (0.003)\tLoss 1.0492 (1.0575)\tPrec@1 56.250 (62.446)\n",
      "EPOCH: 102 train Results: Prec@1 62.446 Loss: 1.0575\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2126 (1.2126)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5630 (1.2717)\tPrec@1 50.000 (54.870)\n",
      "EPOCH: 102 val Results: Prec@1 54.870 Loss: 1.2717\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [103][0/390]\tTime 0.002 (0.002)\tLoss 0.9580 (0.9580)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [103][78/390]\tTime 0.006 (0.003)\tLoss 1.0571 (0.9869)\tPrec@1 63.281 (64.597)\n",
      "Epoch: [103][156/390]\tTime 0.003 (0.003)\tLoss 1.1439 (1.0170)\tPrec@1 55.469 (63.649)\n",
      "Epoch: [103][234/390]\tTime 0.002 (0.003)\tLoss 1.1372 (1.0344)\tPrec@1 59.375 (63.019)\n",
      "Epoch: [103][312/390]\tTime 0.003 (0.003)\tLoss 1.2933 (1.0484)\tPrec@1 57.031 (62.597)\n",
      "Epoch: [103][390/390]\tTime 0.002 (0.003)\tLoss 1.2758 (1.0572)\tPrec@1 61.250 (62.324)\n",
      "EPOCH: 103 train Results: Prec@1 62.324 Loss: 1.0572\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1071 (1.1071)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3865 (1.2853)\tPrec@1 56.250 (54.880)\n",
      "EPOCH: 103 val Results: Prec@1 54.880 Loss: 1.2853\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [104][0/390]\tTime 0.002 (0.002)\tLoss 1.0036 (1.0036)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [104][78/390]\tTime 0.002 (0.003)\tLoss 0.9430 (0.9773)\tPrec@1 68.750 (65.526)\n",
      "Epoch: [104][156/390]\tTime 0.003 (0.003)\tLoss 0.9602 (1.0062)\tPrec@1 66.406 (64.092)\n",
      "Epoch: [104][234/390]\tTime 0.006 (0.003)\tLoss 0.9605 (1.0226)\tPrec@1 65.625 (63.547)\n",
      "Epoch: [104][312/390]\tTime 0.003 (0.003)\tLoss 1.0733 (1.0459)\tPrec@1 63.281 (62.745)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.003)\tLoss 0.9493 (1.0579)\tPrec@1 68.750 (62.364)\n",
      "EPOCH: 104 train Results: Prec@1 62.364 Loss: 1.0579\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1840 (1.1840)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0926 (1.2637)\tPrec@1 50.000 (55.160)\n",
      "EPOCH: 104 val Results: Prec@1 55.160 Loss: 1.2637\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [105][0/390]\tTime 0.006 (0.006)\tLoss 0.9371 (0.9371)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [105][78/390]\tTime 0.006 (0.003)\tLoss 1.0111 (0.9932)\tPrec@1 66.406 (64.043)\n",
      "Epoch: [105][156/390]\tTime 0.002 (0.003)\tLoss 1.1201 (1.0129)\tPrec@1 53.125 (63.495)\n",
      "Epoch: [105][234/390]\tTime 0.006 (0.003)\tLoss 1.2016 (1.0286)\tPrec@1 63.281 (62.919)\n",
      "Epoch: [105][312/390]\tTime 0.002 (0.003)\tLoss 1.1179 (1.0452)\tPrec@1 58.594 (62.340)\n",
      "Epoch: [105][390/390]\tTime 0.003 (0.003)\tLoss 1.0560 (1.0612)\tPrec@1 63.750 (61.994)\n",
      "EPOCH: 105 train Results: Prec@1 61.994 Loss: 1.0612\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1052 (1.1052)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3546 (1.2881)\tPrec@1 43.750 (55.220)\n",
      "EPOCH: 105 val Results: Prec@1 55.220 Loss: 1.2881\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [106][0/390]\tTime 0.002 (0.002)\tLoss 0.9487 (0.9487)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [106][78/390]\tTime 0.003 (0.003)\tLoss 0.9786 (0.9678)\tPrec@1 58.594 (65.427)\n",
      "Epoch: [106][156/390]\tTime 0.002 (0.003)\tLoss 1.0089 (1.0019)\tPrec@1 64.844 (64.092)\n",
      "Epoch: [106][234/390]\tTime 0.007 (0.003)\tLoss 0.9840 (1.0263)\tPrec@1 64.062 (63.351)\n",
      "Epoch: [106][312/390]\tTime 0.004 (0.003)\tLoss 1.0701 (1.0474)\tPrec@1 56.250 (62.605)\n",
      "Epoch: [106][390/390]\tTime 0.002 (0.003)\tLoss 1.0552 (1.0587)\tPrec@1 60.000 (62.278)\n",
      "EPOCH: 106 train Results: Prec@1 62.278 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0953 (1.0953)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3044 (1.2732)\tPrec@1 43.750 (54.850)\n",
      "EPOCH: 106 val Results: Prec@1 54.850 Loss: 1.2732\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [107][0/390]\tTime 0.003 (0.003)\tLoss 0.8581 (0.8581)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [107][78/390]\tTime 0.005 (0.004)\tLoss 1.1520 (0.9746)\tPrec@1 60.156 (65.150)\n",
      "Epoch: [107][156/390]\tTime 0.003 (0.003)\tLoss 1.0159 (1.0058)\tPrec@1 64.062 (63.903)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.003)\tLoss 1.1200 (1.0200)\tPrec@1 62.500 (63.401)\n",
      "Epoch: [107][312/390]\tTime 0.004 (0.003)\tLoss 1.1613 (1.0436)\tPrec@1 59.375 (62.705)\n",
      "Epoch: [107][390/390]\tTime 0.001 (0.003)\tLoss 0.9627 (1.0586)\tPrec@1 66.250 (62.230)\n",
      "EPOCH: 107 train Results: Prec@1 62.230 Loss: 1.0586\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2309 (1.2309)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3505 (1.2829)\tPrec@1 50.000 (54.590)\n",
      "EPOCH: 107 val Results: Prec@1 54.590 Loss: 1.2829\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [108][0/390]\tTime 0.002 (0.002)\tLoss 1.0024 (1.0024)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [108][78/390]\tTime 0.008 (0.003)\tLoss 1.0461 (0.9675)\tPrec@1 66.406 (65.566)\n",
      "Epoch: [108][156/390]\tTime 0.002 (0.003)\tLoss 0.8661 (1.0039)\tPrec@1 72.656 (64.072)\n",
      "Epoch: [108][234/390]\tTime 0.002 (0.003)\tLoss 1.2185 (1.0285)\tPrec@1 50.000 (63.298)\n",
      "Epoch: [108][312/390]\tTime 0.002 (0.003)\tLoss 0.9968 (1.0443)\tPrec@1 62.500 (62.732)\n",
      "Epoch: [108][390/390]\tTime 0.001 (0.003)\tLoss 1.1056 (1.0579)\tPrec@1 62.500 (62.292)\n",
      "EPOCH: 108 train Results: Prec@1 62.292 Loss: 1.0579\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2168 (1.2168)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6365 (1.2773)\tPrec@1 50.000 (54.870)\n",
      "EPOCH: 108 val Results: Prec@1 54.870 Loss: 1.2773\n",
      "Best Prec@1: 55.790\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [109][0/390]\tTime 0.002 (0.002)\tLoss 1.0235 (1.0235)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [109][78/390]\tTime 0.002 (0.003)\tLoss 1.0247 (0.9876)\tPrec@1 62.500 (64.695)\n",
      "Epoch: [109][156/390]\tTime 0.002 (0.003)\tLoss 1.0659 (1.0140)\tPrec@1 63.281 (63.709)\n",
      "Epoch: [109][234/390]\tTime 0.002 (0.003)\tLoss 0.9203 (1.0314)\tPrec@1 63.281 (63.132)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.003)\tLoss 1.0573 (1.0441)\tPrec@1 57.812 (62.535)\n",
      "Epoch: [109][390/390]\tTime 0.002 (0.003)\tLoss 1.1416 (1.0564)\tPrec@1 60.000 (62.088)\n",
      "EPOCH: 109 train Results: Prec@1 62.088 Loss: 1.0564\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1210 (1.1210)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6092 (1.2575)\tPrec@1 37.500 (55.880)\n",
      "EPOCH: 109 val Results: Prec@1 55.880 Loss: 1.2575\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [110][0/390]\tTime 0.002 (0.002)\tLoss 1.0046 (1.0046)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.003)\tLoss 0.9736 (0.9835)\tPrec@1 65.625 (64.903)\n",
      "Epoch: [110][156/390]\tTime 0.002 (0.003)\tLoss 1.0018 (1.0114)\tPrec@1 64.062 (63.625)\n",
      "Epoch: [110][234/390]\tTime 0.002 (0.003)\tLoss 1.1512 (1.0261)\tPrec@1 58.594 (63.381)\n",
      "Epoch: [110][312/390]\tTime 0.004 (0.003)\tLoss 1.0916 (1.0474)\tPrec@1 60.156 (62.635)\n",
      "Epoch: [110][390/390]\tTime 0.001 (0.003)\tLoss 1.1858 (1.0569)\tPrec@1 62.500 (62.152)\n",
      "EPOCH: 110 train Results: Prec@1 62.152 Loss: 1.0569\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0670 (1.0670)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.3937 (1.2703)\tPrec@1 43.750 (55.550)\n",
      "EPOCH: 110 val Results: Prec@1 55.550 Loss: 1.2703\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [111][0/390]\tTime 0.012 (0.012)\tLoss 0.9272 (0.9272)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [111][78/390]\tTime 0.002 (0.003)\tLoss 1.0260 (0.9681)\tPrec@1 63.281 (65.279)\n",
      "Epoch: [111][156/390]\tTime 0.002 (0.003)\tLoss 1.0470 (1.0159)\tPrec@1 60.156 (63.515)\n",
      "Epoch: [111][234/390]\tTime 0.003 (0.003)\tLoss 1.2730 (1.0414)\tPrec@1 59.375 (62.653)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.003)\tLoss 1.0421 (1.0539)\tPrec@1 64.844 (62.116)\n",
      "Epoch: [111][390/390]\tTime 0.002 (0.003)\tLoss 1.1862 (1.0618)\tPrec@1 67.500 (61.836)\n",
      "EPOCH: 111 train Results: Prec@1 61.836 Loss: 1.0618\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1972 (1.1972)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4879 (1.2844)\tPrec@1 43.750 (55.130)\n",
      "EPOCH: 111 val Results: Prec@1 55.130 Loss: 1.2844\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [112][0/390]\tTime 0.004 (0.004)\tLoss 0.9057 (0.9057)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [112][78/390]\tTime 0.003 (0.003)\tLoss 1.0465 (0.9725)\tPrec@1 60.156 (65.378)\n",
      "Epoch: [112][156/390]\tTime 0.004 (0.003)\tLoss 1.2159 (1.0000)\tPrec@1 57.031 (64.331)\n",
      "Epoch: [112][234/390]\tTime 0.003 (0.003)\tLoss 1.2305 (1.0268)\tPrec@1 58.594 (63.451)\n",
      "Epoch: [112][312/390]\tTime 0.003 (0.003)\tLoss 0.9786 (1.0514)\tPrec@1 64.844 (62.455)\n",
      "Epoch: [112][390/390]\tTime 0.001 (0.003)\tLoss 1.4319 (1.0603)\tPrec@1 57.500 (62.260)\n",
      "EPOCH: 112 train Results: Prec@1 62.260 Loss: 1.0603\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0975 (1.0975)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5973 (1.2794)\tPrec@1 31.250 (55.220)\n",
      "EPOCH: 112 val Results: Prec@1 55.220 Loss: 1.2794\n",
      "Best Prec@1: 55.880\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 0.9061 (0.9061)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [113][78/390]\tTime 0.003 (0.003)\tLoss 0.9264 (0.9776)\tPrec@1 65.625 (65.160)\n",
      "Epoch: [113][156/390]\tTime 0.003 (0.003)\tLoss 1.0399 (1.0006)\tPrec@1 61.719 (63.933)\n",
      "Epoch: [113][234/390]\tTime 0.002 (0.003)\tLoss 0.9718 (1.0236)\tPrec@1 66.406 (63.152)\n",
      "Epoch: [113][312/390]\tTime 0.003 (0.003)\tLoss 1.0770 (1.0410)\tPrec@1 60.938 (62.582)\n",
      "Epoch: [113][390/390]\tTime 0.001 (0.003)\tLoss 1.0925 (1.0556)\tPrec@1 63.750 (62.198)\n",
      "EPOCH: 113 train Results: Prec@1 62.198 Loss: 1.0556\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1543 (1.1543)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4604 (1.2559)\tPrec@1 43.750 (55.920)\n",
      "EPOCH: 113 val Results: Prec@1 55.920 Loss: 1.2559\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [114][0/390]\tTime 0.002 (0.002)\tLoss 1.0181 (1.0181)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [114][78/390]\tTime 0.003 (0.002)\tLoss 0.6639 (0.9674)\tPrec@1 78.906 (65.724)\n",
      "Epoch: [114][156/390]\tTime 0.002 (0.003)\tLoss 1.1183 (1.0000)\tPrec@1 59.375 (64.500)\n",
      "Epoch: [114][234/390]\tTime 0.010 (0.003)\tLoss 1.1297 (1.0300)\tPrec@1 59.375 (63.321)\n",
      "Epoch: [114][312/390]\tTime 0.002 (0.003)\tLoss 0.9720 (1.0438)\tPrec@1 70.312 (62.887)\n",
      "Epoch: [114][390/390]\tTime 0.005 (0.003)\tLoss 1.1674 (1.0596)\tPrec@1 53.750 (62.268)\n",
      "EPOCH: 114 train Results: Prec@1 62.268 Loss: 1.0596\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1211 (1.1211)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7723 (1.2880)\tPrec@1 25.000 (54.090)\n",
      "EPOCH: 114 val Results: Prec@1 54.090 Loss: 1.2880\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [115][0/390]\tTime 0.003 (0.003)\tLoss 0.7921 (0.7921)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [115][78/390]\tTime 0.002 (0.003)\tLoss 0.8813 (0.9750)\tPrec@1 66.406 (65.101)\n",
      "Epoch: [115][156/390]\tTime 0.002 (0.003)\tLoss 1.1162 (0.9951)\tPrec@1 60.938 (64.222)\n",
      "Epoch: [115][234/390]\tTime 0.002 (0.003)\tLoss 1.2006 (1.0242)\tPrec@1 54.688 (63.185)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.003)\tLoss 1.3279 (1.0426)\tPrec@1 52.344 (62.615)\n",
      "Epoch: [115][390/390]\tTime 0.001 (0.003)\tLoss 1.2542 (1.0565)\tPrec@1 57.500 (62.088)\n",
      "EPOCH: 115 train Results: Prec@1 62.088 Loss: 1.0565\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1686 (1.1686)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3198 (1.2715)\tPrec@1 43.750 (55.580)\n",
      "EPOCH: 115 val Results: Prec@1 55.580 Loss: 1.2715\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [116][0/390]\tTime 0.002 (0.002)\tLoss 1.0630 (1.0630)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [116][78/390]\tTime 0.003 (0.003)\tLoss 0.8889 (0.9791)\tPrec@1 67.969 (65.042)\n",
      "Epoch: [116][156/390]\tTime 0.003 (0.003)\tLoss 1.1596 (1.0116)\tPrec@1 61.719 (63.804)\n",
      "Epoch: [116][234/390]\tTime 0.002 (0.004)\tLoss 1.0472 (1.0345)\tPrec@1 62.500 (63.025)\n",
      "Epoch: [116][312/390]\tTime 0.005 (0.003)\tLoss 1.1260 (1.0491)\tPrec@1 62.500 (62.595)\n",
      "Epoch: [116][390/390]\tTime 0.003 (0.003)\tLoss 1.2652 (1.0585)\tPrec@1 60.000 (62.230)\n",
      "EPOCH: 116 train Results: Prec@1 62.230 Loss: 1.0585\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1470 (1.1470)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4220 (1.2663)\tPrec@1 50.000 (55.590)\n",
      "EPOCH: 116 val Results: Prec@1 55.590 Loss: 1.2663\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [117][0/390]\tTime 0.002 (0.002)\tLoss 0.9047 (0.9047)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [117][78/390]\tTime 0.002 (0.003)\tLoss 1.1406 (0.9695)\tPrec@1 59.375 (65.427)\n",
      "Epoch: [117][156/390]\tTime 0.002 (0.003)\tLoss 1.1875 (1.0029)\tPrec@1 61.719 (64.391)\n",
      "Epoch: [117][234/390]\tTime 0.003 (0.003)\tLoss 1.0517 (1.0264)\tPrec@1 67.969 (63.511)\n",
      "Epoch: [117][312/390]\tTime 0.006 (0.003)\tLoss 1.0378 (1.0461)\tPrec@1 61.719 (62.829)\n",
      "Epoch: [117][390/390]\tTime 0.002 (0.003)\tLoss 1.0624 (1.0564)\tPrec@1 57.500 (62.480)\n",
      "EPOCH: 117 train Results: Prec@1 62.480 Loss: 1.0564\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0790 (1.0790)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4085 (1.2639)\tPrec@1 37.500 (55.310)\n",
      "EPOCH: 117 val Results: Prec@1 55.310 Loss: 1.2639\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [118][0/390]\tTime 0.004 (0.004)\tLoss 0.9370 (0.9370)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [118][78/390]\tTime 0.005 (0.003)\tLoss 1.0378 (0.9881)\tPrec@1 60.938 (64.735)\n",
      "Epoch: [118][156/390]\tTime 0.003 (0.003)\tLoss 0.9752 (1.0106)\tPrec@1 62.500 (64.038)\n",
      "Epoch: [118][234/390]\tTime 0.002 (0.003)\tLoss 1.0124 (1.0247)\tPrec@1 57.812 (63.541)\n",
      "Epoch: [118][312/390]\tTime 0.014 (0.003)\tLoss 1.0739 (1.0415)\tPrec@1 64.062 (62.902)\n",
      "Epoch: [118][390/390]\tTime 0.002 (0.003)\tLoss 1.2203 (1.0587)\tPrec@1 57.500 (62.310)\n",
      "EPOCH: 118 train Results: Prec@1 62.310 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1874 (1.1874)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2505 (1.2829)\tPrec@1 25.000 (54.350)\n",
      "EPOCH: 118 val Results: Prec@1 54.350 Loss: 1.2829\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [119][0/390]\tTime 0.002 (0.002)\tLoss 0.8914 (0.8914)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [119][78/390]\tTime 0.005 (0.003)\tLoss 1.1185 (0.9589)\tPrec@1 60.938 (65.961)\n",
      "Epoch: [119][156/390]\tTime 0.002 (0.003)\tLoss 1.1781 (0.9981)\tPrec@1 59.375 (64.615)\n",
      "Epoch: [119][234/390]\tTime 0.003 (0.003)\tLoss 0.9921 (1.0224)\tPrec@1 62.500 (63.597)\n",
      "Epoch: [119][312/390]\tTime 0.002 (0.004)\tLoss 1.1443 (1.0411)\tPrec@1 57.812 (62.889)\n",
      "Epoch: [119][390/390]\tTime 0.005 (0.004)\tLoss 1.1475 (1.0557)\tPrec@1 65.000 (62.434)\n",
      "EPOCH: 119 train Results: Prec@1 62.434 Loss: 1.0557\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1863 (1.1863)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4040 (1.2804)\tPrec@1 50.000 (55.160)\n",
      "EPOCH: 119 val Results: Prec@1 55.160 Loss: 1.2804\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [120][0/390]\tTime 0.003 (0.003)\tLoss 0.7583 (0.7583)\tPrec@1 78.125 (78.125)\n",
      "Epoch: [120][78/390]\tTime 0.002 (0.003)\tLoss 1.0494 (0.9773)\tPrec@1 55.469 (65.032)\n",
      "Epoch: [120][156/390]\tTime 0.002 (0.004)\tLoss 0.9370 (1.0095)\tPrec@1 64.062 (63.938)\n",
      "Epoch: [120][234/390]\tTime 0.005 (0.003)\tLoss 1.0619 (1.0297)\tPrec@1 61.719 (63.208)\n",
      "Epoch: [120][312/390]\tTime 0.002 (0.003)\tLoss 1.2868 (1.0448)\tPrec@1 53.125 (62.590)\n",
      "Epoch: [120][390/390]\tTime 0.002 (0.003)\tLoss 1.0729 (1.0587)\tPrec@1 65.000 (62.026)\n",
      "EPOCH: 120 train Results: Prec@1 62.026 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1100 (1.1100)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2774 (1.2656)\tPrec@1 43.750 (55.460)\n",
      "EPOCH: 120 val Results: Prec@1 55.460 Loss: 1.2656\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [121][0/390]\tTime 0.005 (0.005)\tLoss 0.8959 (0.8959)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [121][78/390]\tTime 0.002 (0.003)\tLoss 0.9726 (0.9562)\tPrec@1 59.375 (65.625)\n",
      "Epoch: [121][156/390]\tTime 0.002 (0.003)\tLoss 0.9311 (1.0080)\tPrec@1 70.312 (63.814)\n",
      "Epoch: [121][234/390]\tTime 0.003 (0.003)\tLoss 1.1400 (1.0284)\tPrec@1 60.156 (63.271)\n",
      "Epoch: [121][312/390]\tTime 0.003 (0.003)\tLoss 0.9345 (1.0405)\tPrec@1 67.188 (62.814)\n",
      "Epoch: [121][390/390]\tTime 0.001 (0.003)\tLoss 0.9419 (1.0548)\tPrec@1 61.250 (62.314)\n",
      "EPOCH: 121 train Results: Prec@1 62.314 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1802 (1.1802)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5896 (1.2822)\tPrec@1 43.750 (55.090)\n",
      "EPOCH: 121 val Results: Prec@1 55.090 Loss: 1.2822\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [122][0/390]\tTime 0.004 (0.004)\tLoss 0.9090 (0.9090)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [122][78/390]\tTime 0.002 (0.003)\tLoss 1.0022 (0.9731)\tPrec@1 61.719 (65.506)\n",
      "Epoch: [122][156/390]\tTime 0.004 (0.003)\tLoss 1.0020 (1.0078)\tPrec@1 64.844 (64.436)\n",
      "Epoch: [122][234/390]\tTime 0.007 (0.003)\tLoss 1.1966 (1.0291)\tPrec@1 57.812 (63.398)\n",
      "Epoch: [122][312/390]\tTime 0.003 (0.003)\tLoss 1.0570 (1.0457)\tPrec@1 58.594 (62.692)\n",
      "Epoch: [122][390/390]\tTime 0.001 (0.003)\tLoss 1.1433 (1.0587)\tPrec@1 57.500 (62.310)\n",
      "EPOCH: 122 train Results: Prec@1 62.310 Loss: 1.0587\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1627 (1.1627)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5027 (1.2815)\tPrec@1 43.750 (55.660)\n",
      "EPOCH: 122 val Results: Prec@1 55.660 Loss: 1.2815\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [123][0/390]\tTime 0.002 (0.002)\tLoss 0.9790 (0.9790)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [123][78/390]\tTime 0.007 (0.003)\tLoss 0.9149 (0.9817)\tPrec@1 67.969 (64.636)\n",
      "Epoch: [123][156/390]\tTime 0.002 (0.003)\tLoss 1.1353 (1.0076)\tPrec@1 61.719 (63.560)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.003)\tLoss 1.2845 (1.0295)\tPrec@1 61.719 (62.982)\n",
      "Epoch: [123][312/390]\tTime 0.005 (0.003)\tLoss 1.1044 (1.0447)\tPrec@1 61.719 (62.572)\n",
      "Epoch: [123][390/390]\tTime 0.001 (0.003)\tLoss 1.0842 (1.0589)\tPrec@1 60.000 (62.072)\n",
      "EPOCH: 123 train Results: Prec@1 62.072 Loss: 1.0589\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2142 (1.2142)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7815 (1.2734)\tPrec@1 43.750 (55.370)\n",
      "EPOCH: 123 val Results: Prec@1 55.370 Loss: 1.2734\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [124][0/390]\tTime 0.002 (0.002)\tLoss 0.8392 (0.8392)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [124][78/390]\tTime 0.004 (0.003)\tLoss 0.9400 (0.9751)\tPrec@1 67.188 (65.259)\n",
      "Epoch: [124][156/390]\tTime 0.008 (0.003)\tLoss 1.0112 (1.0143)\tPrec@1 64.844 (63.878)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.003)\tLoss 1.0604 (1.0300)\tPrec@1 57.812 (63.281)\n",
      "Epoch: [124][312/390]\tTime 0.002 (0.003)\tLoss 1.1325 (1.0407)\tPrec@1 62.500 (62.977)\n",
      "Epoch: [124][390/390]\tTime 0.001 (0.003)\tLoss 1.1941 (1.0550)\tPrec@1 53.750 (62.520)\n",
      "EPOCH: 124 train Results: Prec@1 62.520 Loss: 1.0550\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1685 (1.1685)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4352 (1.2757)\tPrec@1 50.000 (55.270)\n",
      "EPOCH: 124 val Results: Prec@1 55.270 Loss: 1.2757\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [125][0/390]\tTime 0.004 (0.004)\tLoss 1.0182 (1.0182)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [125][78/390]\tTime 0.003 (0.004)\tLoss 0.9252 (0.9738)\tPrec@1 67.188 (65.200)\n",
      "Epoch: [125][156/390]\tTime 0.002 (0.003)\tLoss 0.9221 (0.9993)\tPrec@1 68.750 (64.431)\n",
      "Epoch: [125][234/390]\tTime 0.002 (0.003)\tLoss 1.1496 (1.0252)\tPrec@1 59.375 (63.514)\n",
      "Epoch: [125][312/390]\tTime 0.002 (0.003)\tLoss 1.0632 (1.0392)\tPrec@1 60.938 (62.904)\n",
      "Epoch: [125][390/390]\tTime 0.001 (0.003)\tLoss 1.0824 (1.0525)\tPrec@1 60.000 (62.406)\n",
      "EPOCH: 125 train Results: Prec@1 62.406 Loss: 1.0525\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1412 (1.1412)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0219 (1.2709)\tPrec@1 56.250 (55.580)\n",
      "EPOCH: 125 val Results: Prec@1 55.580 Loss: 1.2709\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [126][0/390]\tTime 0.002 (0.002)\tLoss 0.8516 (0.8516)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [126][78/390]\tTime 0.003 (0.003)\tLoss 0.9495 (0.9678)\tPrec@1 66.406 (65.348)\n",
      "Epoch: [126][156/390]\tTime 0.004 (0.004)\tLoss 1.0913 (1.0080)\tPrec@1 64.844 (64.152)\n",
      "Epoch: [126][234/390]\tTime 0.016 (0.004)\tLoss 1.2227 (1.0262)\tPrec@1 57.031 (63.371)\n",
      "Epoch: [126][312/390]\tTime 0.006 (0.003)\tLoss 1.0892 (1.0451)\tPrec@1 56.250 (62.782)\n",
      "Epoch: [126][390/390]\tTime 0.001 (0.003)\tLoss 1.0182 (1.0570)\tPrec@1 58.750 (62.506)\n",
      "EPOCH: 126 train Results: Prec@1 62.506 Loss: 1.0570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1650 (1.1650)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5126 (1.2862)\tPrec@1 43.750 (54.960)\n",
      "EPOCH: 126 val Results: Prec@1 54.960 Loss: 1.2862\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [127][0/390]\tTime 0.002 (0.002)\tLoss 1.0068 (1.0068)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [127][78/390]\tTime 0.002 (0.003)\tLoss 0.8789 (0.9775)\tPrec@1 69.531 (65.210)\n",
      "Epoch: [127][156/390]\tTime 0.003 (0.003)\tLoss 0.9612 (0.9985)\tPrec@1 62.500 (64.406)\n",
      "Epoch: [127][234/390]\tTime 0.002 (0.004)\tLoss 1.2703 (1.0235)\tPrec@1 55.469 (63.544)\n",
      "Epoch: [127][312/390]\tTime 0.002 (0.004)\tLoss 1.1859 (1.0448)\tPrec@1 55.469 (62.672)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.004)\tLoss 1.1352 (1.0575)\tPrec@1 62.500 (62.292)\n",
      "EPOCH: 127 train Results: Prec@1 62.292 Loss: 1.0575\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1415 (1.1415)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0750 (1.2656)\tPrec@1 56.250 (55.670)\n",
      "EPOCH: 127 val Results: Prec@1 55.670 Loss: 1.2656\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [128][0/390]\tTime 0.002 (0.002)\tLoss 0.9762 (0.9762)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [128][78/390]\tTime 0.004 (0.003)\tLoss 1.0724 (0.9754)\tPrec@1 63.281 (65.299)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.003)\tLoss 0.9882 (1.0082)\tPrec@1 67.969 (64.072)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.003)\tLoss 1.0450 (1.0252)\tPrec@1 62.500 (63.467)\n",
      "Epoch: [128][312/390]\tTime 0.003 (0.003)\tLoss 1.0905 (1.0415)\tPrec@1 60.938 (62.834)\n",
      "Epoch: [128][390/390]\tTime 0.001 (0.003)\tLoss 1.2652 (1.0555)\tPrec@1 55.000 (62.472)\n",
      "EPOCH: 128 train Results: Prec@1 62.472 Loss: 1.0555\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0864 (1.0864)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3620 (1.2803)\tPrec@1 43.750 (55.280)\n",
      "EPOCH: 128 val Results: Prec@1 55.280 Loss: 1.2803\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [129][0/390]\tTime 0.004 (0.004)\tLoss 1.1178 (1.1178)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 0.9524 (0.9793)\tPrec@1 69.531 (64.962)\n",
      "Epoch: [129][156/390]\tTime 0.017 (0.003)\tLoss 1.0972 (0.9995)\tPrec@1 58.594 (64.286)\n",
      "Epoch: [129][234/390]\tTime 0.002 (0.004)\tLoss 1.1651 (1.0234)\tPrec@1 59.375 (63.457)\n",
      "Epoch: [129][312/390]\tTime 0.002 (0.004)\tLoss 0.9217 (1.0412)\tPrec@1 64.062 (62.857)\n",
      "Epoch: [129][390/390]\tTime 0.002 (0.004)\tLoss 1.1019 (1.0559)\tPrec@1 56.250 (62.276)\n",
      "EPOCH: 129 train Results: Prec@1 62.276 Loss: 1.0559\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1469 (1.1469)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2512 (1.2620)\tPrec@1 31.250 (55.440)\n",
      "EPOCH: 129 val Results: Prec@1 55.440 Loss: 1.2620\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [130][0/390]\tTime 0.002 (0.002)\tLoss 0.8411 (0.8411)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [130][78/390]\tTime 0.002 (0.003)\tLoss 0.8896 (0.9731)\tPrec@1 64.062 (65.417)\n",
      "Epoch: [130][156/390]\tTime 0.002 (0.003)\tLoss 1.1477 (1.0005)\tPrec@1 59.375 (64.316)\n",
      "Epoch: [130][234/390]\tTime 0.004 (0.003)\tLoss 1.1516 (1.0262)\tPrec@1 59.375 (63.298)\n",
      "Epoch: [130][312/390]\tTime 0.002 (0.003)\tLoss 1.1629 (1.0440)\tPrec@1 58.594 (62.800)\n",
      "Epoch: [130][390/390]\tTime 0.001 (0.003)\tLoss 1.3576 (1.0586)\tPrec@1 57.500 (62.286)\n",
      "EPOCH: 130 train Results: Prec@1 62.286 Loss: 1.0586\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2686 (1.2686)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2197 (1.2847)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 130 val Results: Prec@1 54.630 Loss: 1.2847\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [131][0/390]\tTime 0.005 (0.005)\tLoss 0.8687 (0.8687)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [131][78/390]\tTime 0.002 (0.004)\tLoss 0.9167 (0.9755)\tPrec@1 67.188 (65.131)\n",
      "Epoch: [131][156/390]\tTime 0.019 (0.004)\tLoss 1.1968 (1.0022)\tPrec@1 57.031 (64.013)\n",
      "Epoch: [131][234/390]\tTime 0.008 (0.004)\tLoss 1.1939 (1.0250)\tPrec@1 57.031 (63.321)\n",
      "Epoch: [131][312/390]\tTime 0.005 (0.004)\tLoss 1.3674 (1.0440)\tPrec@1 50.781 (62.712)\n",
      "Epoch: [131][390/390]\tTime 0.003 (0.004)\tLoss 1.0260 (1.0576)\tPrec@1 60.000 (62.340)\n",
      "EPOCH: 131 train Results: Prec@1 62.340 Loss: 1.0576\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2590 (1.2590)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4127 (1.2783)\tPrec@1 43.750 (55.660)\n",
      "EPOCH: 131 val Results: Prec@1 55.660 Loss: 1.2783\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [132][0/390]\tTime 0.006 (0.006)\tLoss 0.9295 (0.9295)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [132][78/390]\tTime 0.002 (0.003)\tLoss 0.9584 (0.9716)\tPrec@1 66.406 (65.773)\n",
      "Epoch: [132][156/390]\tTime 0.002 (0.003)\tLoss 1.0773 (1.0175)\tPrec@1 60.156 (63.913)\n",
      "Epoch: [132][234/390]\tTime 0.007 (0.004)\tLoss 1.0384 (1.0396)\tPrec@1 67.969 (63.198)\n",
      "Epoch: [132][312/390]\tTime 0.004 (0.003)\tLoss 1.2507 (1.0496)\tPrec@1 52.344 (62.745)\n",
      "Epoch: [132][390/390]\tTime 0.002 (0.004)\tLoss 1.0426 (1.0567)\tPrec@1 63.750 (62.416)\n",
      "EPOCH: 132 train Results: Prec@1 62.416 Loss: 1.0567\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1601 (1.1601)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1963 (1.2712)\tPrec@1 43.750 (55.680)\n",
      "EPOCH: 132 val Results: Prec@1 55.680 Loss: 1.2712\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [133][0/390]\tTime 0.004 (0.004)\tLoss 0.9084 (0.9084)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [133][78/390]\tTime 0.002 (0.004)\tLoss 0.9303 (0.9719)\tPrec@1 66.406 (65.309)\n",
      "Epoch: [133][156/390]\tTime 0.004 (0.004)\tLoss 0.9852 (0.9931)\tPrec@1 63.281 (64.525)\n",
      "Epoch: [133][234/390]\tTime 0.003 (0.004)\tLoss 1.1468 (1.0221)\tPrec@1 59.375 (63.318)\n",
      "Epoch: [133][312/390]\tTime 0.002 (0.003)\tLoss 1.0194 (1.0436)\tPrec@1 66.406 (62.530)\n",
      "Epoch: [133][390/390]\tTime 0.006 (0.003)\tLoss 1.0452 (1.0539)\tPrec@1 62.500 (62.218)\n",
      "EPOCH: 133 train Results: Prec@1 62.218 Loss: 1.0539\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2785 (1.2785)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4063 (1.2824)\tPrec@1 43.750 (55.200)\n",
      "EPOCH: 133 val Results: Prec@1 55.200 Loss: 1.2824\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [134][0/390]\tTime 0.002 (0.002)\tLoss 0.9994 (0.9994)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [134][78/390]\tTime 0.002 (0.004)\tLoss 1.0019 (0.9734)\tPrec@1 67.188 (66.110)\n",
      "Epoch: [134][156/390]\tTime 0.002 (0.004)\tLoss 1.0233 (1.0144)\tPrec@1 64.844 (64.291)\n",
      "Epoch: [134][234/390]\tTime 0.005 (0.004)\tLoss 1.1444 (1.0318)\tPrec@1 61.719 (63.501)\n",
      "Epoch: [134][312/390]\tTime 0.008 (0.003)\tLoss 1.0489 (1.0438)\tPrec@1 63.281 (63.044)\n",
      "Epoch: [134][390/390]\tTime 0.014 (0.003)\tLoss 1.1543 (1.0562)\tPrec@1 55.000 (62.580)\n",
      "EPOCH: 134 train Results: Prec@1 62.580 Loss: 1.0562\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1828 (1.1828)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2819 (1.2673)\tPrec@1 43.750 (55.810)\n",
      "EPOCH: 134 val Results: Prec@1 55.810 Loss: 1.2673\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [135][0/390]\tTime 0.005 (0.005)\tLoss 0.9706 (0.9706)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [135][78/390]\tTime 0.007 (0.003)\tLoss 1.0065 (0.9840)\tPrec@1 67.969 (64.715)\n",
      "Epoch: [135][156/390]\tTime 0.008 (0.003)\tLoss 1.0629 (1.0066)\tPrec@1 64.844 (64.008)\n",
      "Epoch: [135][234/390]\tTime 0.002 (0.003)\tLoss 1.2625 (1.0292)\tPrec@1 53.906 (63.075)\n",
      "Epoch: [135][312/390]\tTime 0.003 (0.003)\tLoss 0.9436 (1.0465)\tPrec@1 64.062 (62.527)\n",
      "Epoch: [135][390/390]\tTime 0.001 (0.003)\tLoss 1.1159 (1.0594)\tPrec@1 60.000 (62.188)\n",
      "EPOCH: 135 train Results: Prec@1 62.188 Loss: 1.0594\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2432 (1.2432)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1610 (1.2677)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 135 val Results: Prec@1 55.230 Loss: 1.2677\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [136][0/390]\tTime 0.003 (0.003)\tLoss 0.8243 (0.8243)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [136][78/390]\tTime 0.002 (0.003)\tLoss 1.1068 (0.9801)\tPrec@1 62.500 (65.546)\n",
      "Epoch: [136][156/390]\tTime 0.002 (0.003)\tLoss 0.8672 (1.0030)\tPrec@1 71.875 (64.739)\n",
      "Epoch: [136][234/390]\tTime 0.003 (0.003)\tLoss 1.0238 (1.0240)\tPrec@1 62.500 (63.687)\n",
      "Epoch: [136][312/390]\tTime 0.002 (0.003)\tLoss 1.1397 (1.0386)\tPrec@1 62.500 (63.084)\n",
      "Epoch: [136][390/390]\tTime 0.003 (0.003)\tLoss 1.1839 (1.0492)\tPrec@1 57.500 (62.758)\n",
      "EPOCH: 136 train Results: Prec@1 62.758 Loss: 1.0492\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2107 (1.2107)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1389 (1.2795)\tPrec@1 68.750 (55.500)\n",
      "EPOCH: 136 val Results: Prec@1 55.500 Loss: 1.2795\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [137][0/390]\tTime 0.005 (0.005)\tLoss 1.0124 (1.0124)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [137][78/390]\tTime 0.002 (0.003)\tLoss 1.0741 (0.9864)\tPrec@1 60.156 (64.903)\n",
      "Epoch: [137][156/390]\tTime 0.002 (0.003)\tLoss 0.9216 (1.0034)\tPrec@1 68.750 (64.510)\n",
      "Epoch: [137][234/390]\tTime 0.006 (0.003)\tLoss 0.9174 (1.0216)\tPrec@1 66.406 (63.481)\n",
      "Epoch: [137][312/390]\tTime 0.002 (0.004)\tLoss 0.9866 (1.0385)\tPrec@1 68.750 (62.879)\n",
      "Epoch: [137][390/390]\tTime 0.009 (0.005)\tLoss 1.2039 (1.0551)\tPrec@1 60.000 (62.384)\n",
      "EPOCH: 137 train Results: Prec@1 62.384 Loss: 1.0551\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1189 (1.1189)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3363 (1.2738)\tPrec@1 43.750 (55.560)\n",
      "EPOCH: 137 val Results: Prec@1 55.560 Loss: 1.2738\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [138][0/390]\tTime 0.005 (0.005)\tLoss 1.0291 (1.0291)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [138][78/390]\tTime 0.002 (0.005)\tLoss 0.9423 (0.9646)\tPrec@1 64.844 (65.724)\n",
      "Epoch: [138][156/390]\tTime 0.009 (0.004)\tLoss 0.9918 (1.0057)\tPrec@1 60.938 (64.117)\n",
      "Epoch: [138][234/390]\tTime 0.004 (0.005)\tLoss 1.1888 (1.0271)\tPrec@1 57.812 (63.374)\n",
      "Epoch: [138][312/390]\tTime 0.002 (0.005)\tLoss 1.1460 (1.0424)\tPrec@1 63.281 (62.750)\n",
      "Epoch: [138][390/390]\tTime 0.002 (0.005)\tLoss 1.1539 (1.0557)\tPrec@1 57.500 (62.274)\n",
      "EPOCH: 138 train Results: Prec@1 62.274 Loss: 1.0557\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1866 (1.1866)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5186 (1.2796)\tPrec@1 37.500 (55.140)\n",
      "EPOCH: 138 val Results: Prec@1 55.140 Loss: 1.2796\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [139][0/390]\tTime 0.003 (0.003)\tLoss 0.9915 (0.9915)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [139][78/390]\tTime 0.004 (0.003)\tLoss 0.9201 (0.9818)\tPrec@1 68.750 (64.854)\n",
      "Epoch: [139][156/390]\tTime 0.002 (0.003)\tLoss 1.0426 (1.0087)\tPrec@1 64.844 (64.222)\n",
      "Epoch: [139][234/390]\tTime 0.004 (0.003)\tLoss 0.9975 (1.0276)\tPrec@1 66.406 (63.411)\n",
      "Epoch: [139][312/390]\tTime 0.002 (0.003)\tLoss 1.0904 (1.0424)\tPrec@1 58.594 (62.797)\n",
      "Epoch: [139][390/390]\tTime 0.005 (0.003)\tLoss 1.0182 (1.0546)\tPrec@1 63.750 (62.312)\n",
      "EPOCH: 139 train Results: Prec@1 62.312 Loss: 1.0546\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1983 (1.1983)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.3045 (1.2905)\tPrec@1 37.500 (54.740)\n",
      "EPOCH: 139 val Results: Prec@1 54.740 Loss: 1.2905\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [140][0/390]\tTime 0.003 (0.003)\tLoss 1.0560 (1.0560)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [140][78/390]\tTime 0.002 (0.003)\tLoss 0.9958 (0.9716)\tPrec@1 67.969 (66.446)\n",
      "Epoch: [140][156/390]\tTime 0.003 (0.003)\tLoss 0.9786 (0.9985)\tPrec@1 64.844 (64.675)\n",
      "Epoch: [140][234/390]\tTime 0.004 (0.003)\tLoss 0.9594 (1.0215)\tPrec@1 62.500 (63.783)\n",
      "Epoch: [140][312/390]\tTime 0.002 (0.003)\tLoss 0.9797 (1.0382)\tPrec@1 60.938 (63.024)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.003)\tLoss 1.2719 (1.0552)\tPrec@1 52.500 (62.416)\n",
      "EPOCH: 140 train Results: Prec@1 62.416 Loss: 1.0552\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2126 (1.2126)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.4619 (1.2797)\tPrec@1 37.500 (54.960)\n",
      "EPOCH: 140 val Results: Prec@1 54.960 Loss: 1.2797\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [141][0/390]\tTime 0.005 (0.005)\tLoss 0.6640 (0.6640)\tPrec@1 78.906 (78.906)\n",
      "Epoch: [141][78/390]\tTime 0.002 (0.006)\tLoss 1.1954 (0.9732)\tPrec@1 57.812 (65.338)\n",
      "Epoch: [141][156/390]\tTime 0.002 (0.006)\tLoss 1.0877 (1.0072)\tPrec@1 57.031 (63.858)\n",
      "Epoch: [141][234/390]\tTime 0.004 (0.006)\tLoss 1.1971 (1.0296)\tPrec@1 61.719 (63.098)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.006)\tLoss 1.1613 (1.0437)\tPrec@1 62.500 (62.517)\n",
      "Epoch: [141][390/390]\tTime 0.002 (0.005)\tLoss 1.2001 (1.0540)\tPrec@1 56.250 (62.234)\n",
      "EPOCH: 141 train Results: Prec@1 62.234 Loss: 1.0540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2176 (1.2176)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2271 (1.2754)\tPrec@1 37.500 (55.000)\n",
      "EPOCH: 141 val Results: Prec@1 55.000 Loss: 1.2754\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [142][0/390]\tTime 0.004 (0.004)\tLoss 0.9739 (0.9739)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [142][78/390]\tTime 0.002 (0.003)\tLoss 0.9589 (0.9787)\tPrec@1 63.281 (65.279)\n",
      "Epoch: [142][156/390]\tTime 0.003 (0.003)\tLoss 0.9702 (1.0037)\tPrec@1 70.312 (64.356)\n",
      "Epoch: [142][234/390]\tTime 0.009 (0.003)\tLoss 1.0196 (1.0175)\tPrec@1 64.062 (63.750)\n",
      "Epoch: [142][312/390]\tTime 0.004 (0.004)\tLoss 1.2466 (1.0357)\tPrec@1 54.688 (62.994)\n",
      "Epoch: [142][390/390]\tTime 0.002 (0.004)\tLoss 1.0765 (1.0498)\tPrec@1 63.750 (62.438)\n",
      "EPOCH: 142 train Results: Prec@1 62.438 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3533 (1.3533)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2340 (1.2858)\tPrec@1 37.500 (55.080)\n",
      "EPOCH: 142 val Results: Prec@1 55.080 Loss: 1.2858\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [143][0/390]\tTime 0.003 (0.003)\tLoss 0.8759 (0.8759)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [143][78/390]\tTime 0.002 (0.003)\tLoss 1.0551 (0.9653)\tPrec@1 63.281 (65.961)\n",
      "Epoch: [143][156/390]\tTime 0.002 (0.004)\tLoss 1.0174 (1.0025)\tPrec@1 63.281 (64.267)\n",
      "Epoch: [143][234/390]\tTime 0.017 (0.003)\tLoss 1.0090 (1.0229)\tPrec@1 64.844 (63.620)\n",
      "Epoch: [143][312/390]\tTime 0.003 (0.003)\tLoss 1.1809 (1.0379)\tPrec@1 60.156 (63.007)\n",
      "Epoch: [143][390/390]\tTime 0.001 (0.004)\tLoss 1.0902 (1.0520)\tPrec@1 58.750 (62.446)\n",
      "EPOCH: 143 train Results: Prec@1 62.446 Loss: 1.0520\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1666 (1.1666)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5304 (1.2861)\tPrec@1 43.750 (55.070)\n",
      "EPOCH: 143 val Results: Prec@1 55.070 Loss: 1.2861\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [144][0/390]\tTime 0.002 (0.002)\tLoss 0.8754 (0.8754)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [144][78/390]\tTime 0.002 (0.005)\tLoss 1.0114 (0.9766)\tPrec@1 67.969 (64.953)\n",
      "Epoch: [144][156/390]\tTime 0.002 (0.006)\tLoss 1.2154 (1.0054)\tPrec@1 55.469 (64.152)\n",
      "Epoch: [144][234/390]\tTime 0.002 (0.006)\tLoss 1.1210 (1.0291)\tPrec@1 60.156 (63.275)\n",
      "Epoch: [144][312/390]\tTime 0.003 (0.005)\tLoss 1.1045 (1.0406)\tPrec@1 62.500 (62.802)\n",
      "Epoch: [144][390/390]\tTime 0.005 (0.005)\tLoss 1.1109 (1.0540)\tPrec@1 56.250 (62.280)\n",
      "EPOCH: 144 train Results: Prec@1 62.280 Loss: 1.0540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0783 (1.0783)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3885 (1.2691)\tPrec@1 50.000 (55.190)\n",
      "EPOCH: 144 val Results: Prec@1 55.190 Loss: 1.2691\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [145][0/390]\tTime 0.005 (0.005)\tLoss 0.9592 (0.9592)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [145][78/390]\tTime 0.002 (0.004)\tLoss 0.9436 (0.9803)\tPrec@1 64.844 (64.715)\n",
      "Epoch: [145][156/390]\tTime 0.009 (0.004)\tLoss 1.1899 (1.0002)\tPrec@1 57.812 (64.341)\n",
      "Epoch: [145][234/390]\tTime 0.004 (0.005)\tLoss 0.9033 (1.0264)\tPrec@1 66.406 (63.404)\n",
      "Epoch: [145][312/390]\tTime 0.003 (0.004)\tLoss 1.2107 (1.0461)\tPrec@1 56.250 (62.615)\n",
      "Epoch: [145][390/390]\tTime 0.002 (0.004)\tLoss 1.1190 (1.0570)\tPrec@1 62.500 (62.316)\n",
      "EPOCH: 145 train Results: Prec@1 62.316 Loss: 1.0570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1698 (1.1698)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4618 (1.2749)\tPrec@1 43.750 (55.090)\n",
      "EPOCH: 145 val Results: Prec@1 55.090 Loss: 1.2749\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [146][0/390]\tTime 0.002 (0.002)\tLoss 1.0142 (1.0142)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [146][78/390]\tTime 0.003 (0.003)\tLoss 0.9875 (0.9765)\tPrec@1 64.062 (65.091)\n",
      "Epoch: [146][156/390]\tTime 0.003 (0.004)\tLoss 0.9657 (1.0124)\tPrec@1 67.188 (63.829)\n",
      "Epoch: [146][234/390]\tTime 0.003 (0.005)\tLoss 0.9167 (1.0300)\tPrec@1 68.750 (63.228)\n",
      "Epoch: [146][312/390]\tTime 0.003 (0.006)\tLoss 0.9189 (1.0453)\tPrec@1 66.406 (62.715)\n",
      "Epoch: [146][390/390]\tTime 0.006 (0.006)\tLoss 0.9842 (1.0583)\tPrec@1 62.500 (62.204)\n",
      "EPOCH: 146 train Results: Prec@1 62.204 Loss: 1.0583\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1554 (1.1554)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2210 (1.2823)\tPrec@1 50.000 (54.690)\n",
      "EPOCH: 146 val Results: Prec@1 54.690 Loss: 1.2823\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [147][0/390]\tTime 0.010 (0.010)\tLoss 0.8892 (0.8892)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [147][78/390]\tTime 0.008 (0.006)\tLoss 0.9920 (0.9799)\tPrec@1 64.844 (65.150)\n",
      "Epoch: [147][156/390]\tTime 0.005 (0.006)\tLoss 1.1061 (1.0057)\tPrec@1 62.500 (64.097)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.006)\tLoss 1.1256 (1.0273)\tPrec@1 62.500 (63.338)\n",
      "Epoch: [147][312/390]\tTime 0.003 (0.005)\tLoss 1.1484 (1.0442)\tPrec@1 64.844 (62.620)\n",
      "Epoch: [147][390/390]\tTime 0.002 (0.005)\tLoss 1.1600 (1.0557)\tPrec@1 56.250 (62.284)\n",
      "EPOCH: 147 train Results: Prec@1 62.284 Loss: 1.0557\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1474 (1.1474)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5218 (1.2847)\tPrec@1 37.500 (54.490)\n",
      "EPOCH: 147 val Results: Prec@1 54.490 Loss: 1.2847\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [148][0/390]\tTime 0.003 (0.003)\tLoss 0.9431 (0.9431)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [148][78/390]\tTime 0.004 (0.004)\tLoss 1.0363 (0.9796)\tPrec@1 60.938 (65.140)\n",
      "Epoch: [148][156/390]\tTime 0.003 (0.004)\tLoss 1.0889 (1.0081)\tPrec@1 58.594 (63.973)\n",
      "Epoch: [148][234/390]\tTime 0.002 (0.005)\tLoss 1.0857 (1.0325)\tPrec@1 55.469 (63.112)\n",
      "Epoch: [148][312/390]\tTime 0.005 (0.004)\tLoss 1.1014 (1.0435)\tPrec@1 63.281 (62.837)\n",
      "Epoch: [148][390/390]\tTime 0.001 (0.004)\tLoss 0.8781 (1.0540)\tPrec@1 65.000 (62.338)\n",
      "EPOCH: 148 train Results: Prec@1 62.338 Loss: 1.0540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1848 (1.1848)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4664 (1.2815)\tPrec@1 50.000 (55.370)\n",
      "EPOCH: 148 val Results: Prec@1 55.370 Loss: 1.2815\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [149][0/390]\tTime 0.004 (0.004)\tLoss 0.8937 (0.8937)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [149][78/390]\tTime 0.004 (0.003)\tLoss 0.9870 (0.9639)\tPrec@1 67.969 (65.922)\n",
      "Epoch: [149][156/390]\tTime 0.008 (0.003)\tLoss 0.9871 (0.9974)\tPrec@1 64.844 (64.436)\n",
      "Epoch: [149][234/390]\tTime 0.004 (0.003)\tLoss 1.0101 (1.0245)\tPrec@1 64.844 (63.441)\n",
      "Epoch: [149][312/390]\tTime 0.002 (0.003)\tLoss 1.1388 (1.0391)\tPrec@1 55.469 (62.792)\n",
      "Epoch: [149][390/390]\tTime 0.003 (0.003)\tLoss 1.1773 (1.0537)\tPrec@1 57.500 (62.218)\n",
      "EPOCH: 149 train Results: Prec@1 62.218 Loss: 1.0537\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1329 (1.1329)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3726 (1.2654)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 149 val Results: Prec@1 55.150 Loss: 1.2654\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [150][0/390]\tTime 0.002 (0.002)\tLoss 1.0329 (1.0329)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [150][78/390]\tTime 0.005 (0.004)\tLoss 1.2202 (0.9547)\tPrec@1 57.812 (66.208)\n",
      "Epoch: [150][156/390]\tTime 0.002 (0.004)\tLoss 1.1726 (0.9919)\tPrec@1 59.375 (64.734)\n",
      "Epoch: [150][234/390]\tTime 0.002 (0.004)\tLoss 1.2929 (1.0245)\tPrec@1 54.688 (63.344)\n",
      "Epoch: [150][312/390]\tTime 0.002 (0.004)\tLoss 1.1972 (1.0413)\tPrec@1 57.812 (62.797)\n",
      "Epoch: [150][390/390]\tTime 0.006 (0.004)\tLoss 1.0898 (1.0517)\tPrec@1 62.500 (62.364)\n",
      "EPOCH: 150 train Results: Prec@1 62.364 Loss: 1.0517\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1993 (1.1993)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3348 (1.2688)\tPrec@1 37.500 (55.630)\n",
      "EPOCH: 150 val Results: Prec@1 55.630 Loss: 1.2688\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [151][0/390]\tTime 0.005 (0.005)\tLoss 0.9282 (0.9282)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [151][78/390]\tTime 0.003 (0.004)\tLoss 1.1566 (0.9652)\tPrec@1 53.906 (65.477)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.005)\tLoss 0.9782 (0.9939)\tPrec@1 66.406 (64.456)\n",
      "Epoch: [151][234/390]\tTime 0.003 (0.004)\tLoss 1.1344 (1.0222)\tPrec@1 54.688 (63.295)\n",
      "Epoch: [151][312/390]\tTime 0.004 (0.004)\tLoss 1.1533 (1.0376)\tPrec@1 59.375 (62.705)\n",
      "Epoch: [151][390/390]\tTime 0.002 (0.004)\tLoss 1.2783 (1.0563)\tPrec@1 57.500 (62.182)\n",
      "EPOCH: 151 train Results: Prec@1 62.182 Loss: 1.0563\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2193 (1.2193)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3854 (1.2657)\tPrec@1 50.000 (55.300)\n",
      "EPOCH: 151 val Results: Prec@1 55.300 Loss: 1.2657\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [152][0/390]\tTime 0.004 (0.004)\tLoss 0.8986 (0.8986)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [152][78/390]\tTime 0.003 (0.003)\tLoss 1.0596 (0.9748)\tPrec@1 57.812 (65.338)\n",
      "Epoch: [152][156/390]\tTime 0.002 (0.003)\tLoss 1.0129 (1.0089)\tPrec@1 62.500 (64.003)\n",
      "Epoch: [152][234/390]\tTime 0.002 (0.003)\tLoss 1.0813 (1.0229)\tPrec@1 64.844 (63.627)\n",
      "Epoch: [152][312/390]\tTime 0.002 (0.003)\tLoss 1.2338 (1.0372)\tPrec@1 48.438 (63.064)\n",
      "Epoch: [152][390/390]\tTime 0.003 (0.003)\tLoss 0.9794 (1.0489)\tPrec@1 71.250 (62.574)\n",
      "EPOCH: 152 train Results: Prec@1 62.574 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1441 (1.1441)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4594 (1.2726)\tPrec@1 37.500 (55.070)\n",
      "EPOCH: 152 val Results: Prec@1 55.070 Loss: 1.2726\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [153][0/390]\tTime 0.002 (0.002)\tLoss 0.8083 (0.8083)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [153][78/390]\tTime 0.002 (0.004)\tLoss 0.9372 (0.9632)\tPrec@1 67.969 (65.734)\n",
      "Epoch: [153][156/390]\tTime 0.004 (0.003)\tLoss 0.9652 (0.9964)\tPrec@1 67.188 (64.675)\n",
      "Epoch: [153][234/390]\tTime 0.002 (0.004)\tLoss 1.0995 (1.0186)\tPrec@1 61.719 (63.890)\n",
      "Epoch: [153][312/390]\tTime 0.008 (0.004)\tLoss 0.9770 (1.0360)\tPrec@1 67.188 (63.264)\n",
      "Epoch: [153][390/390]\tTime 0.010 (0.003)\tLoss 1.1738 (1.0512)\tPrec@1 58.750 (62.586)\n",
      "EPOCH: 153 train Results: Prec@1 62.586 Loss: 1.0512\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1974 (1.1974)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2853 (1.2674)\tPrec@1 50.000 (55.400)\n",
      "EPOCH: 153 val Results: Prec@1 55.400 Loss: 1.2674\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [154][0/390]\tTime 0.006 (0.006)\tLoss 0.8441 (0.8441)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.003)\tLoss 0.9333 (0.9666)\tPrec@1 64.062 (65.655)\n",
      "Epoch: [154][156/390]\tTime 0.002 (0.004)\tLoss 1.1208 (0.9972)\tPrec@1 59.375 (64.281)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.004)\tLoss 1.2625 (1.0172)\tPrec@1 56.250 (63.614)\n",
      "Epoch: [154][312/390]\tTime 0.002 (0.004)\tLoss 1.1499 (1.0392)\tPrec@1 55.469 (62.919)\n",
      "Epoch: [154][390/390]\tTime 0.001 (0.004)\tLoss 1.2449 (1.0553)\tPrec@1 57.500 (62.268)\n",
      "EPOCH: 154 train Results: Prec@1 62.268 Loss: 1.0553\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2100 (1.2100)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.1720 (1.2699)\tPrec@1 43.750 (54.990)\n",
      "EPOCH: 154 val Results: Prec@1 54.990 Loss: 1.2699\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [155][0/390]\tTime 0.004 (0.004)\tLoss 1.0149 (1.0149)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [155][78/390]\tTime 0.005 (0.003)\tLoss 1.0837 (0.9690)\tPrec@1 64.844 (65.783)\n",
      "Epoch: [155][156/390]\tTime 0.004 (0.003)\tLoss 0.9518 (0.9906)\tPrec@1 64.062 (64.953)\n",
      "Epoch: [155][234/390]\tTime 0.004 (0.003)\tLoss 1.2940 (1.0159)\tPrec@1 50.781 (64.146)\n",
      "Epoch: [155][312/390]\tTime 0.008 (0.003)\tLoss 1.0599 (1.0312)\tPrec@1 67.969 (63.673)\n",
      "Epoch: [155][390/390]\tTime 0.003 (0.003)\tLoss 1.0135 (1.0500)\tPrec@1 62.500 (62.756)\n",
      "EPOCH: 155 train Results: Prec@1 62.756 Loss: 1.0500\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1842 (1.1842)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3321 (1.2830)\tPrec@1 43.750 (54.740)\n",
      "EPOCH: 155 val Results: Prec@1 54.740 Loss: 1.2830\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [156][0/390]\tTime 0.002 (0.002)\tLoss 0.9788 (0.9788)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [156][78/390]\tTime 0.003 (0.003)\tLoss 0.9788 (0.9516)\tPrec@1 60.156 (66.159)\n",
      "Epoch: [156][156/390]\tTime 0.004 (0.003)\tLoss 1.2629 (0.9963)\tPrec@1 53.906 (64.257)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.003)\tLoss 1.1077 (1.0231)\tPrec@1 60.938 (63.351)\n",
      "Epoch: [156][312/390]\tTime 0.002 (0.003)\tLoss 1.2801 (1.0412)\tPrec@1 53.125 (62.707)\n",
      "Epoch: [156][390/390]\tTime 0.013 (0.003)\tLoss 1.1941 (1.0509)\tPrec@1 56.250 (62.368)\n",
      "EPOCH: 156 train Results: Prec@1 62.368 Loss: 1.0509\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2574 (1.2574)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7317 (1.2787)\tPrec@1 31.250 (55.130)\n",
      "EPOCH: 156 val Results: Prec@1 55.130 Loss: 1.2787\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [157][0/390]\tTime 0.006 (0.006)\tLoss 1.0983 (1.0983)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [157][78/390]\tTime 0.002 (0.003)\tLoss 0.9867 (0.9814)\tPrec@1 61.719 (64.814)\n",
      "Epoch: [157][156/390]\tTime 0.065 (0.003)\tLoss 0.9547 (1.0041)\tPrec@1 67.188 (63.699)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.0239 (1.0278)\tPrec@1 67.969 (63.042)\n",
      "Epoch: [157][312/390]\tTime 0.002 (0.003)\tLoss 1.1458 (1.0441)\tPrec@1 60.156 (62.665)\n",
      "Epoch: [157][390/390]\tTime 0.001 (0.003)\tLoss 1.1487 (1.0528)\tPrec@1 61.250 (62.408)\n",
      "EPOCH: 157 train Results: Prec@1 62.408 Loss: 1.0528\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.1763 (1.1763)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9522 (1.2784)\tPrec@1 50.000 (55.160)\n",
      "EPOCH: 157 val Results: Prec@1 55.160 Loss: 1.2784\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [158][0/390]\tTime 0.002 (0.002)\tLoss 0.9057 (0.9057)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [158][78/390]\tTime 0.003 (0.003)\tLoss 1.1486 (0.9622)\tPrec@1 60.938 (65.991)\n",
      "Epoch: [158][156/390]\tTime 0.002 (0.004)\tLoss 1.0785 (0.9894)\tPrec@1 57.812 (64.560)\n",
      "Epoch: [158][234/390]\tTime 0.002 (0.003)\tLoss 1.0586 (1.0153)\tPrec@1 59.375 (63.733)\n",
      "Epoch: [158][312/390]\tTime 0.003 (0.003)\tLoss 1.1659 (1.0328)\tPrec@1 60.938 (63.032)\n",
      "Epoch: [158][390/390]\tTime 0.002 (0.004)\tLoss 1.1654 (1.0476)\tPrec@1 60.000 (62.466)\n",
      "EPOCH: 158 train Results: Prec@1 62.466 Loss: 1.0476\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1554 (1.1554)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1961 (1.2789)\tPrec@1 56.250 (55.450)\n",
      "EPOCH: 158 val Results: Prec@1 55.450 Loss: 1.2789\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [159][0/390]\tTime 0.003 (0.003)\tLoss 0.8849 (0.8849)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [159][78/390]\tTime 0.003 (0.003)\tLoss 0.9523 (0.9701)\tPrec@1 67.188 (65.773)\n",
      "Epoch: [159][156/390]\tTime 0.002 (0.004)\tLoss 0.9254 (1.0064)\tPrec@1 66.406 (64.217)\n",
      "Epoch: [159][234/390]\tTime 0.002 (0.004)\tLoss 0.9947 (1.0276)\tPrec@1 66.406 (63.467)\n",
      "Epoch: [159][312/390]\tTime 0.002 (0.004)\tLoss 0.9721 (1.0434)\tPrec@1 63.281 (62.735)\n",
      "Epoch: [159][390/390]\tTime 0.001 (0.004)\tLoss 1.2444 (1.0551)\tPrec@1 51.250 (62.258)\n",
      "EPOCH: 159 train Results: Prec@1 62.258 Loss: 1.0551\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1506 (1.1506)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5919 (1.2747)\tPrec@1 31.250 (55.250)\n",
      "EPOCH: 159 val Results: Prec@1 55.250 Loss: 1.2747\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [160][0/390]\tTime 0.006 (0.006)\tLoss 0.9350 (0.9350)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [160][78/390]\tTime 0.003 (0.003)\tLoss 1.0617 (1.0034)\tPrec@1 64.844 (64.428)\n",
      "Epoch: [160][156/390]\tTime 0.002 (0.003)\tLoss 1.0270 (1.0134)\tPrec@1 64.062 (63.928)\n",
      "Epoch: [160][234/390]\tTime 0.004 (0.004)\tLoss 0.9794 (1.0292)\tPrec@1 67.969 (63.351)\n",
      "Epoch: [160][312/390]\tTime 0.003 (0.004)\tLoss 1.0046 (1.0415)\tPrec@1 67.188 (63.014)\n",
      "Epoch: [160][390/390]\tTime 0.002 (0.003)\tLoss 0.9708 (1.0470)\tPrec@1 70.000 (62.790)\n",
      "EPOCH: 160 train Results: Prec@1 62.790 Loss: 1.0470\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1477 (1.1477)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1484 (1.2714)\tPrec@1 50.000 (55.340)\n",
      "EPOCH: 160 val Results: Prec@1 55.340 Loss: 1.2714\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [161][0/390]\tTime 0.007 (0.007)\tLoss 0.9777 (0.9777)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [161][78/390]\tTime 0.003 (0.003)\tLoss 0.9600 (0.9564)\tPrec@1 68.750 (65.971)\n",
      "Epoch: [161][156/390]\tTime 0.004 (0.003)\tLoss 1.0059 (0.9920)\tPrec@1 63.281 (64.854)\n",
      "Epoch: [161][234/390]\tTime 0.003 (0.003)\tLoss 0.9870 (1.0186)\tPrec@1 66.406 (63.813)\n",
      "Epoch: [161][312/390]\tTime 0.002 (0.003)\tLoss 0.9858 (1.0341)\tPrec@1 67.969 (63.284)\n",
      "Epoch: [161][390/390]\tTime 0.002 (0.003)\tLoss 1.2690 (1.0500)\tPrec@1 51.250 (62.740)\n",
      "EPOCH: 161 train Results: Prec@1 62.740 Loss: 1.0500\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2367 (1.2367)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2709 (1.2649)\tPrec@1 50.000 (55.130)\n",
      "EPOCH: 161 val Results: Prec@1 55.130 Loss: 1.2649\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 0.8036 (0.8036)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [162][78/390]\tTime 0.002 (0.003)\tLoss 1.0772 (0.9716)\tPrec@1 63.281 (65.220)\n",
      "Epoch: [162][156/390]\tTime 0.005 (0.003)\tLoss 0.9170 (0.9998)\tPrec@1 67.188 (64.466)\n",
      "Epoch: [162][234/390]\tTime 0.002 (0.003)\tLoss 0.9683 (1.0200)\tPrec@1 67.969 (63.664)\n",
      "Epoch: [162][312/390]\tTime 0.005 (0.003)\tLoss 1.1521 (1.0380)\tPrec@1 61.719 (63.059)\n",
      "Epoch: [162][390/390]\tTime 0.007 (0.003)\tLoss 1.0920 (1.0516)\tPrec@1 66.250 (62.646)\n",
      "EPOCH: 162 train Results: Prec@1 62.646 Loss: 1.0516\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2669 (1.2669)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6285 (1.2859)\tPrec@1 43.750 (54.750)\n",
      "EPOCH: 162 val Results: Prec@1 54.750 Loss: 1.2859\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [163][0/390]\tTime 0.004 (0.004)\tLoss 1.0660 (1.0660)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [163][78/390]\tTime 0.002 (0.004)\tLoss 1.0678 (0.9728)\tPrec@1 58.594 (65.210)\n",
      "Epoch: [163][156/390]\tTime 0.002 (0.004)\tLoss 1.0335 (0.9971)\tPrec@1 63.281 (64.550)\n",
      "Epoch: [163][234/390]\tTime 0.002 (0.004)\tLoss 1.0391 (1.0233)\tPrec@1 65.625 (63.674)\n",
      "Epoch: [163][312/390]\tTime 0.012 (0.003)\tLoss 1.0980 (1.0383)\tPrec@1 56.250 (63.236)\n",
      "Epoch: [163][390/390]\tTime 0.003 (0.003)\tLoss 1.0405 (1.0512)\tPrec@1 62.500 (62.664)\n",
      "EPOCH: 163 train Results: Prec@1 62.664 Loss: 1.0512\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1462 (1.1462)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1379 (1.2701)\tPrec@1 56.250 (55.660)\n",
      "EPOCH: 163 val Results: Prec@1 55.660 Loss: 1.2701\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [164][0/390]\tTime 0.003 (0.003)\tLoss 0.9560 (0.9560)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [164][78/390]\tTime 0.002 (0.003)\tLoss 1.1102 (0.9647)\tPrec@1 61.719 (65.398)\n",
      "Epoch: [164][156/390]\tTime 0.003 (0.003)\tLoss 1.0740 (1.0015)\tPrec@1 57.031 (64.142)\n",
      "Epoch: [164][234/390]\tTime 0.011 (0.003)\tLoss 1.0299 (1.0263)\tPrec@1 60.938 (63.082)\n",
      "Epoch: [164][312/390]\tTime 0.003 (0.004)\tLoss 1.2488 (1.0449)\tPrec@1 57.812 (62.460)\n",
      "Epoch: [164][390/390]\tTime 0.001 (0.003)\tLoss 1.0010 (1.0548)\tPrec@1 61.250 (62.062)\n",
      "EPOCH: 164 train Results: Prec@1 62.062 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.1130 (1.1130)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0342 (1.2695)\tPrec@1 50.000 (55.530)\n",
      "EPOCH: 164 val Results: Prec@1 55.530 Loss: 1.2695\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [165][0/390]\tTime 0.004 (0.004)\tLoss 1.0631 (1.0631)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [165][78/390]\tTime 0.002 (0.003)\tLoss 0.9483 (0.9485)\tPrec@1 65.625 (66.584)\n",
      "Epoch: [165][156/390]\tTime 0.003 (0.003)\tLoss 1.0360 (0.9939)\tPrec@1 64.844 (64.719)\n",
      "Epoch: [165][234/390]\tTime 0.006 (0.003)\tLoss 1.2476 (1.0186)\tPrec@1 60.156 (63.813)\n",
      "Epoch: [165][312/390]\tTime 0.003 (0.003)\tLoss 1.1353 (1.0381)\tPrec@1 60.156 (63.079)\n",
      "Epoch: [165][390/390]\tTime 0.003 (0.003)\tLoss 1.1594 (1.0497)\tPrec@1 63.750 (62.564)\n",
      "EPOCH: 165 train Results: Prec@1 62.564 Loss: 1.0497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1508 (1.1508)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0310 (1.2886)\tPrec@1 43.750 (54.810)\n",
      "EPOCH: 165 val Results: Prec@1 54.810 Loss: 1.2886\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [166][0/390]\tTime 0.005 (0.005)\tLoss 1.0151 (1.0151)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.004)\tLoss 0.9569 (0.9599)\tPrec@1 67.188 (65.605)\n",
      "Epoch: [166][156/390]\tTime 0.009 (0.003)\tLoss 1.0884 (0.9916)\tPrec@1 63.281 (64.510)\n",
      "Epoch: [166][234/390]\tTime 0.003 (0.003)\tLoss 1.2448 (1.0263)\tPrec@1 53.906 (63.025)\n",
      "Epoch: [166][312/390]\tTime 0.002 (0.003)\tLoss 1.0379 (1.0391)\tPrec@1 63.281 (62.657)\n",
      "Epoch: [166][390/390]\tTime 0.002 (0.004)\tLoss 0.9054 (1.0531)\tPrec@1 70.000 (62.324)\n",
      "EPOCH: 166 train Results: Prec@1 62.324 Loss: 1.0531\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1791 (1.1791)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2267 (1.2762)\tPrec@1 43.750 (55.560)\n",
      "EPOCH: 166 val Results: Prec@1 55.560 Loss: 1.2762\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [167][0/390]\tTime 0.004 (0.004)\tLoss 1.0987 (1.0987)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [167][78/390]\tTime 0.005 (0.004)\tLoss 1.0524 (0.9703)\tPrec@1 60.938 (65.407)\n",
      "Epoch: [167][156/390]\tTime 0.004 (0.004)\tLoss 0.9838 (0.9944)\tPrec@1 66.406 (64.217)\n",
      "Epoch: [167][234/390]\tTime 0.002 (0.003)\tLoss 1.0791 (1.0224)\tPrec@1 60.156 (63.447)\n",
      "Epoch: [167][312/390]\tTime 0.003 (0.003)\tLoss 1.2113 (1.0356)\tPrec@1 61.719 (62.954)\n",
      "Epoch: [167][390/390]\tTime 0.001 (0.003)\tLoss 1.1419 (1.0494)\tPrec@1 58.750 (62.482)\n",
      "EPOCH: 167 train Results: Prec@1 62.482 Loss: 1.0494\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1709 (1.1709)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2743 (1.2681)\tPrec@1 56.250 (55.670)\n",
      "EPOCH: 167 val Results: Prec@1 55.670 Loss: 1.2681\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [168][0/390]\tTime 0.004 (0.004)\tLoss 0.8502 (0.8502)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [168][78/390]\tTime 0.002 (0.003)\tLoss 0.9077 (0.9581)\tPrec@1 69.531 (65.813)\n",
      "Epoch: [168][156/390]\tTime 0.003 (0.003)\tLoss 1.0789 (0.9923)\tPrec@1 64.844 (64.719)\n",
      "Epoch: [168][234/390]\tTime 0.002 (0.003)\tLoss 1.0492 (1.0146)\tPrec@1 57.812 (63.707)\n",
      "Epoch: [168][312/390]\tTime 0.002 (0.003)\tLoss 1.0290 (1.0358)\tPrec@1 60.156 (62.892)\n",
      "Epoch: [168][390/390]\tTime 0.001 (0.003)\tLoss 1.0820 (1.0489)\tPrec@1 63.750 (62.444)\n",
      "EPOCH: 168 train Results: Prec@1 62.444 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.2360 (1.2360)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4459 (1.2775)\tPrec@1 37.500 (55.010)\n",
      "EPOCH: 168 val Results: Prec@1 55.010 Loss: 1.2775\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [169][0/390]\tTime 0.002 (0.002)\tLoss 0.9207 (0.9207)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [169][78/390]\tTime 0.006 (0.004)\tLoss 1.0043 (0.9770)\tPrec@1 62.500 (65.793)\n",
      "Epoch: [169][156/390]\tTime 0.005 (0.004)\tLoss 0.9166 (1.0012)\tPrec@1 66.406 (64.530)\n",
      "Epoch: [169][234/390]\tTime 0.002 (0.003)\tLoss 0.9935 (1.0273)\tPrec@1 67.188 (63.481)\n",
      "Epoch: [169][312/390]\tTime 0.003 (0.003)\tLoss 1.0345 (1.0435)\tPrec@1 62.500 (62.879)\n",
      "Epoch: [169][390/390]\tTime 0.001 (0.003)\tLoss 1.0339 (1.0570)\tPrec@1 58.750 (62.330)\n",
      "EPOCH: 169 train Results: Prec@1 62.330 Loss: 1.0570\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2630 (1.2630)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1171 (1.2872)\tPrec@1 43.750 (54.680)\n",
      "EPOCH: 169 val Results: Prec@1 54.680 Loss: 1.2872\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [170][0/390]\tTime 0.003 (0.003)\tLoss 0.9438 (0.9438)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [170][78/390]\tTime 0.002 (0.003)\tLoss 1.1096 (0.9542)\tPrec@1 61.719 (66.377)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.003)\tLoss 0.9324 (0.9865)\tPrec@1 67.969 (64.809)\n",
      "Epoch: [170][234/390]\tTime 0.005 (0.003)\tLoss 1.1591 (1.0117)\tPrec@1 63.281 (63.770)\n",
      "Epoch: [170][312/390]\tTime 0.002 (0.003)\tLoss 1.1080 (1.0352)\tPrec@1 55.469 (62.844)\n",
      "Epoch: [170][390/390]\tTime 0.001 (0.003)\tLoss 0.9915 (1.0476)\tPrec@1 63.750 (62.454)\n",
      "EPOCH: 170 train Results: Prec@1 62.454 Loss: 1.0476\n",
      "Test: [0/78]\tTime 0.015 (0.015)\tLoss 1.1559 (1.1559)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.0393 (1.2681)\tPrec@1 43.750 (55.490)\n",
      "EPOCH: 170 val Results: Prec@1 55.490 Loss: 1.2681\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [171][0/390]\tTime 0.002 (0.002)\tLoss 0.8451 (0.8451)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [171][78/390]\tTime 0.002 (0.003)\tLoss 1.0146 (0.9829)\tPrec@1 66.406 (65.091)\n",
      "Epoch: [171][156/390]\tTime 0.002 (0.003)\tLoss 1.1900 (1.0102)\tPrec@1 60.938 (63.983)\n",
      "Epoch: [171][234/390]\tTime 0.003 (0.003)\tLoss 0.8682 (1.0235)\tPrec@1 68.750 (63.624)\n",
      "Epoch: [171][312/390]\tTime 0.004 (0.003)\tLoss 0.9438 (1.0369)\tPrec@1 70.312 (63.191)\n",
      "Epoch: [171][390/390]\tTime 0.002 (0.003)\tLoss 1.3729 (1.0525)\tPrec@1 57.500 (62.562)\n",
      "EPOCH: 171 train Results: Prec@1 62.562 Loss: 1.0525\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2874 (1.2860)\tPrec@1 31.250 (54.550)\n",
      "EPOCH: 171 val Results: Prec@1 54.550 Loss: 1.2860\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [172][0/390]\tTime 0.002 (0.002)\tLoss 0.8504 (0.8504)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [172][78/390]\tTime 0.003 (0.003)\tLoss 0.9169 (0.9549)\tPrec@1 66.406 (66.070)\n",
      "Epoch: [172][156/390]\tTime 0.003 (0.003)\tLoss 1.0534 (0.9905)\tPrec@1 61.719 (64.689)\n",
      "Epoch: [172][234/390]\tTime 0.003 (0.004)\tLoss 0.9967 (1.0206)\tPrec@1 67.188 (63.557)\n",
      "Epoch: [172][312/390]\tTime 0.005 (0.004)\tLoss 1.1219 (1.0378)\tPrec@1 60.938 (62.992)\n",
      "Epoch: [172][390/390]\tTime 0.013 (0.004)\tLoss 1.1309 (1.0498)\tPrec@1 58.750 (62.604)\n",
      "EPOCH: 172 train Results: Prec@1 62.604 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1841 (1.1841)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3541 (1.2742)\tPrec@1 50.000 (55.480)\n",
      "EPOCH: 172 val Results: Prec@1 55.480 Loss: 1.2742\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [173][0/390]\tTime 0.005 (0.005)\tLoss 0.9739 (0.9739)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [173][78/390]\tTime 0.002 (0.003)\tLoss 1.2207 (0.9692)\tPrec@1 56.250 (65.071)\n",
      "Epoch: [173][156/390]\tTime 0.004 (0.003)\tLoss 1.0249 (1.0032)\tPrec@1 63.281 (63.694)\n",
      "Epoch: [173][234/390]\tTime 0.002 (0.003)\tLoss 0.9059 (1.0179)\tPrec@1 67.969 (63.258)\n",
      "Epoch: [173][312/390]\tTime 0.008 (0.003)\tLoss 1.0716 (1.0385)\tPrec@1 64.062 (62.587)\n",
      "Epoch: [173][390/390]\tTime 0.001 (0.003)\tLoss 1.0667 (1.0540)\tPrec@1 56.250 (62.232)\n",
      "EPOCH: 173 train Results: Prec@1 62.232 Loss: 1.0540\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2192 (1.2192)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4511 (1.2900)\tPrec@1 31.250 (55.180)\n",
      "EPOCH: 173 val Results: Prec@1 55.180 Loss: 1.2900\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [174][0/390]\tTime 0.002 (0.002)\tLoss 1.0283 (1.0283)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [174][78/390]\tTime 0.002 (0.003)\tLoss 0.8644 (0.9530)\tPrec@1 62.500 (66.139)\n",
      "Epoch: [174][156/390]\tTime 0.003 (0.003)\tLoss 1.0219 (0.9935)\tPrec@1 60.938 (64.630)\n",
      "Epoch: [174][234/390]\tTime 0.004 (0.003)\tLoss 1.0697 (1.0160)\tPrec@1 60.938 (63.767)\n",
      "Epoch: [174][312/390]\tTime 0.002 (0.003)\tLoss 0.9763 (1.0339)\tPrec@1 65.625 (63.112)\n",
      "Epoch: [174][390/390]\tTime 0.003 (0.003)\tLoss 0.9747 (1.0496)\tPrec@1 58.750 (62.514)\n",
      "EPOCH: 174 train Results: Prec@1 62.514 Loss: 1.0496\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1622 (1.1622)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2613 (1.2788)\tPrec@1 50.000 (55.230)\n",
      "EPOCH: 174 val Results: Prec@1 55.230 Loss: 1.2788\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [175][0/390]\tTime 0.003 (0.003)\tLoss 0.9966 (0.9966)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [175][78/390]\tTime 0.004 (0.005)\tLoss 1.1348 (0.9680)\tPrec@1 59.375 (65.655)\n",
      "Epoch: [175][156/390]\tTime 0.003 (0.004)\tLoss 0.9629 (0.9917)\tPrec@1 67.969 (64.739)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.004)\tLoss 1.1205 (1.0144)\tPrec@1 58.594 (63.963)\n",
      "Epoch: [175][312/390]\tTime 0.002 (0.004)\tLoss 1.1321 (1.0337)\tPrec@1 59.375 (63.274)\n",
      "Epoch: [175][390/390]\tTime 0.004 (0.003)\tLoss 1.2169 (1.0486)\tPrec@1 48.750 (62.672)\n",
      "EPOCH: 175 train Results: Prec@1 62.672 Loss: 1.0486\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2115 (1.2115)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4381 (1.2762)\tPrec@1 43.750 (54.730)\n",
      "EPOCH: 175 val Results: Prec@1 54.730 Loss: 1.2762\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [176][0/390]\tTime 0.003 (0.003)\tLoss 1.0673 (1.0673)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [176][78/390]\tTime 0.003 (0.003)\tLoss 1.1339 (0.9829)\tPrec@1 62.500 (65.378)\n",
      "Epoch: [176][156/390]\tTime 0.002 (0.004)\tLoss 1.0696 (1.0050)\tPrec@1 53.125 (64.097)\n",
      "Epoch: [176][234/390]\tTime 0.004 (0.004)\tLoss 1.2216 (1.0265)\tPrec@1 58.594 (63.418)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.004)\tLoss 1.0274 (1.0392)\tPrec@1 63.281 (62.917)\n",
      "Epoch: [176][390/390]\tTime 0.005 (0.004)\tLoss 0.9526 (1.0504)\tPrec@1 68.750 (62.472)\n",
      "EPOCH: 176 train Results: Prec@1 62.472 Loss: 1.0504\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1260 (1.1260)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3751 (1.2726)\tPrec@1 43.750 (55.400)\n",
      "EPOCH: 176 val Results: Prec@1 55.400 Loss: 1.2726\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [177][0/390]\tTime 0.004 (0.004)\tLoss 1.0030 (1.0030)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [177][78/390]\tTime 0.002 (0.004)\tLoss 0.9079 (0.9681)\tPrec@1 67.969 (66.129)\n",
      "Epoch: [177][156/390]\tTime 0.002 (0.003)\tLoss 1.1086 (1.0067)\tPrec@1 59.375 (64.107)\n",
      "Epoch: [177][234/390]\tTime 0.003 (0.004)\tLoss 1.0480 (1.0223)\tPrec@1 58.594 (63.434)\n",
      "Epoch: [177][312/390]\tTime 0.019 (0.004)\tLoss 1.0877 (1.0398)\tPrec@1 57.031 (63.007)\n",
      "Epoch: [177][390/390]\tTime 0.003 (0.004)\tLoss 1.1559 (1.0493)\tPrec@1 61.250 (62.588)\n",
      "EPOCH: 177 train Results: Prec@1 62.588 Loss: 1.0493\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2586 (1.2586)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4494 (1.2935)\tPrec@1 31.250 (54.920)\n",
      "EPOCH: 177 val Results: Prec@1 54.920 Loss: 1.2935\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [178][0/390]\tTime 0.002 (0.002)\tLoss 1.0602 (1.0602)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [178][78/390]\tTime 0.003 (0.003)\tLoss 1.0035 (0.9746)\tPrec@1 68.750 (65.200)\n",
      "Epoch: [178][156/390]\tTime 0.002 (0.003)\tLoss 1.0493 (1.0026)\tPrec@1 66.406 (64.257)\n",
      "Epoch: [178][234/390]\tTime 0.004 (0.003)\tLoss 1.1247 (1.0237)\tPrec@1 60.938 (63.408)\n",
      "Epoch: [178][312/390]\tTime 0.002 (0.003)\tLoss 1.0997 (1.0382)\tPrec@1 58.594 (62.947)\n",
      "Epoch: [178][390/390]\tTime 0.001 (0.003)\tLoss 1.0498 (1.0507)\tPrec@1 62.500 (62.548)\n",
      "EPOCH: 178 train Results: Prec@1 62.548 Loss: 1.0507\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1768 (1.1768)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2206 (1.2954)\tPrec@1 37.500 (54.900)\n",
      "EPOCH: 178 val Results: Prec@1 54.900 Loss: 1.2954\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [179][0/390]\tTime 0.002 (0.002)\tLoss 1.0416 (1.0416)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [179][78/390]\tTime 0.003 (0.003)\tLoss 1.0088 (0.9530)\tPrec@1 69.531 (66.159)\n",
      "Epoch: [179][156/390]\tTime 0.024 (0.003)\tLoss 1.0910 (0.9951)\tPrec@1 67.969 (64.476)\n",
      "Epoch: [179][234/390]\tTime 0.002 (0.003)\tLoss 1.2298 (1.0261)\tPrec@1 57.031 (63.341)\n",
      "Epoch: [179][312/390]\tTime 0.005 (0.003)\tLoss 1.0640 (1.0413)\tPrec@1 60.938 (62.785)\n",
      "Epoch: [179][390/390]\tTime 0.001 (0.003)\tLoss 1.3608 (1.0546)\tPrec@1 52.500 (62.344)\n",
      "EPOCH: 179 train Results: Prec@1 62.344 Loss: 1.0546\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0921 (1.0921)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1258 (1.2954)\tPrec@1 50.000 (54.430)\n",
      "EPOCH: 179 val Results: Prec@1 54.430 Loss: 1.2954\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [180][0/390]\tTime 0.010 (0.010)\tLoss 0.9902 (0.9902)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [180][78/390]\tTime 0.003 (0.003)\tLoss 1.0207 (0.9731)\tPrec@1 62.500 (65.309)\n",
      "Epoch: [180][156/390]\tTime 0.004 (0.003)\tLoss 1.1116 (0.9972)\tPrec@1 66.406 (64.262)\n",
      "Epoch: [180][234/390]\tTime 0.003 (0.003)\tLoss 1.0380 (1.0253)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [180][312/390]\tTime 0.005 (0.003)\tLoss 1.0292 (1.0434)\tPrec@1 67.188 (62.597)\n",
      "Epoch: [180][390/390]\tTime 0.005 (0.003)\tLoss 1.2511 (1.0521)\tPrec@1 51.250 (62.300)\n",
      "EPOCH: 180 train Results: Prec@1 62.300 Loss: 1.0521\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1538 (1.1538)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2570 (1.2942)\tPrec@1 43.750 (54.490)\n",
      "EPOCH: 180 val Results: Prec@1 54.490 Loss: 1.2942\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [181][0/390]\tTime 0.012 (0.012)\tLoss 1.0288 (1.0288)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [181][78/390]\tTime 0.003 (0.004)\tLoss 0.8773 (0.9618)\tPrec@1 66.406 (65.220)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 0.9303 (0.9909)\tPrec@1 67.188 (64.276)\n",
      "Epoch: [181][234/390]\tTime 0.002 (0.003)\tLoss 1.0420 (1.0204)\tPrec@1 63.281 (63.331)\n",
      "Epoch: [181][312/390]\tTime 0.002 (0.003)\tLoss 1.0994 (1.0383)\tPrec@1 63.281 (62.660)\n",
      "Epoch: [181][390/390]\tTime 0.001 (0.003)\tLoss 1.2904 (1.0500)\tPrec@1 50.000 (62.268)\n",
      "EPOCH: 181 train Results: Prec@1 62.268 Loss: 1.0500\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1270 (1.1270)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9782 (1.2892)\tPrec@1 56.250 (55.120)\n",
      "EPOCH: 181 val Results: Prec@1 55.120 Loss: 1.2892\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [182][0/390]\tTime 0.002 (0.002)\tLoss 1.0553 (1.0553)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [182][78/390]\tTime 0.003 (0.003)\tLoss 1.0987 (0.9636)\tPrec@1 58.594 (65.417)\n",
      "Epoch: [182][156/390]\tTime 0.002 (0.003)\tLoss 0.9091 (0.9967)\tPrec@1 73.438 (64.172)\n",
      "Epoch: [182][234/390]\tTime 0.004 (0.004)\tLoss 1.2803 (1.0257)\tPrec@1 57.031 (63.175)\n",
      "Epoch: [182][312/390]\tTime 0.009 (0.003)\tLoss 1.0573 (1.0405)\tPrec@1 65.625 (62.710)\n",
      "Epoch: [182][390/390]\tTime 0.004 (0.003)\tLoss 1.2152 (1.0498)\tPrec@1 56.250 (62.414)\n",
      "EPOCH: 182 train Results: Prec@1 62.414 Loss: 1.0498\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2434 (1.2906)\tPrec@1 56.250 (54.920)\n",
      "EPOCH: 182 val Results: Prec@1 54.920 Loss: 1.2906\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [183][0/390]\tTime 0.003 (0.003)\tLoss 0.8174 (0.8174)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [183][78/390]\tTime 0.002 (0.003)\tLoss 0.9482 (0.9501)\tPrec@1 66.406 (66.485)\n",
      "Epoch: [183][156/390]\tTime 0.003 (0.003)\tLoss 0.9455 (0.9917)\tPrec@1 70.312 (64.764)\n",
      "Epoch: [183][234/390]\tTime 0.002 (0.003)\tLoss 1.0783 (1.0210)\tPrec@1 65.625 (63.541)\n",
      "Epoch: [183][312/390]\tTime 0.006 (0.004)\tLoss 1.0477 (1.0408)\tPrec@1 64.844 (62.877)\n",
      "Epoch: [183][390/390]\tTime 0.002 (0.004)\tLoss 1.2305 (1.0545)\tPrec@1 58.750 (62.358)\n",
      "EPOCH: 183 train Results: Prec@1 62.358 Loss: 1.0545\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1769 (1.1769)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6685 (1.2790)\tPrec@1 37.500 (55.150)\n",
      "EPOCH: 183 val Results: Prec@1 55.150 Loss: 1.2790\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [184][0/390]\tTime 0.003 (0.003)\tLoss 0.8851 (0.8851)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [184][78/390]\tTime 0.004 (0.005)\tLoss 1.0891 (0.9615)\tPrec@1 60.938 (66.040)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.004)\tLoss 1.1144 (0.9929)\tPrec@1 63.281 (64.774)\n",
      "Epoch: [184][234/390]\tTime 0.014 (0.004)\tLoss 1.2643 (1.0205)\tPrec@1 57.031 (63.634)\n",
      "Epoch: [184][312/390]\tTime 0.002 (0.004)\tLoss 1.0575 (1.0384)\tPrec@1 58.594 (62.869)\n",
      "Epoch: [184][390/390]\tTime 0.003 (0.004)\tLoss 1.3520 (1.0516)\tPrec@1 53.750 (62.402)\n",
      "EPOCH: 184 train Results: Prec@1 62.402 Loss: 1.0516\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.0542 (1.0542)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2534 (1.2766)\tPrec@1 50.000 (55.770)\n",
      "EPOCH: 184 val Results: Prec@1 55.770 Loss: 1.2766\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [185][0/390]\tTime 0.002 (0.002)\tLoss 1.0624 (1.0624)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [185][78/390]\tTime 0.005 (0.003)\tLoss 1.1761 (0.9844)\tPrec@1 51.562 (64.695)\n",
      "Epoch: [185][156/390]\tTime 0.002 (0.003)\tLoss 0.9582 (1.0000)\tPrec@1 64.062 (64.048)\n",
      "Epoch: [185][234/390]\tTime 0.003 (0.003)\tLoss 1.1197 (1.0152)\tPrec@1 60.156 (63.693)\n",
      "Epoch: [185][312/390]\tTime 0.002 (0.003)\tLoss 1.0257 (1.0292)\tPrec@1 62.500 (63.161)\n",
      "Epoch: [185][390/390]\tTime 0.001 (0.003)\tLoss 1.1298 (1.0486)\tPrec@1 56.250 (62.508)\n",
      "EPOCH: 185 train Results: Prec@1 62.508 Loss: 1.0486\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.0972 (1.0972)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.4375 (1.2890)\tPrec@1 31.250 (54.760)\n",
      "EPOCH: 185 val Results: Prec@1 54.760 Loss: 1.2890\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [186][0/390]\tTime 0.005 (0.005)\tLoss 0.9534 (0.9534)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [186][78/390]\tTime 0.003 (0.003)\tLoss 1.0136 (0.9700)\tPrec@1 60.938 (65.655)\n",
      "Epoch: [186][156/390]\tTime 0.085 (0.004)\tLoss 1.0166 (0.9999)\tPrec@1 64.844 (64.485)\n",
      "Epoch: [186][234/390]\tTime 0.017 (0.004)\tLoss 1.1401 (1.0243)\tPrec@1 56.250 (63.551)\n",
      "Epoch: [186][312/390]\tTime 0.002 (0.004)\tLoss 1.0591 (1.0443)\tPrec@1 64.844 (62.899)\n",
      "Epoch: [186][390/390]\tTime 0.008 (0.004)\tLoss 0.9597 (1.0526)\tPrec@1 66.250 (62.542)\n",
      "EPOCH: 186 train Results: Prec@1 62.542 Loss: 1.0526\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1185 (1.1185)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4281 (1.2962)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 186 val Results: Prec@1 54.630 Loss: 1.2962\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [187][0/390]\tTime 0.002 (0.002)\tLoss 1.0670 (1.0670)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [187][78/390]\tTime 0.004 (0.004)\tLoss 0.8741 (0.9634)\tPrec@1 70.312 (65.348)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.2432 (0.9884)\tPrec@1 53.125 (64.565)\n",
      "Epoch: [187][234/390]\tTime 0.002 (0.003)\tLoss 1.0166 (1.0142)\tPrec@1 67.188 (63.720)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.0702 (1.0342)\tPrec@1 66.406 (63.094)\n",
      "Epoch: [187][390/390]\tTime 0.001 (0.003)\tLoss 1.1837 (1.0484)\tPrec@1 57.500 (62.588)\n",
      "EPOCH: 187 train Results: Prec@1 62.588 Loss: 1.0484\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1446 (1.1446)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6352 (1.2930)\tPrec@1 43.750 (54.760)\n",
      "EPOCH: 187 val Results: Prec@1 54.760 Loss: 1.2930\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [188][0/390]\tTime 0.002 (0.002)\tLoss 0.8107 (0.8107)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [188][78/390]\tTime 0.002 (0.003)\tLoss 1.1783 (0.9644)\tPrec@1 62.500 (66.208)\n",
      "Epoch: [188][156/390]\tTime 0.007 (0.003)\tLoss 1.0560 (0.9882)\tPrec@1 54.688 (65.088)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.003)\tLoss 1.3150 (1.0127)\tPrec@1 50.781 (64.116)\n",
      "Epoch: [188][312/390]\tTime 0.002 (0.003)\tLoss 1.3795 (1.0313)\tPrec@1 60.156 (63.463)\n",
      "Epoch: [188][390/390]\tTime 0.002 (0.003)\tLoss 1.3469 (1.0489)\tPrec@1 48.750 (62.620)\n",
      "EPOCH: 188 train Results: Prec@1 62.620 Loss: 1.0489\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1901 (1.1901)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2590 (1.2907)\tPrec@1 37.500 (54.820)\n",
      "EPOCH: 188 val Results: Prec@1 54.820 Loss: 1.2907\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [189][0/390]\tTime 0.006 (0.006)\tLoss 0.9439 (0.9439)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.002)\tLoss 0.9524 (0.9596)\tPrec@1 68.750 (65.595)\n",
      "Epoch: [189][156/390]\tTime 0.002 (0.003)\tLoss 1.2134 (0.9966)\tPrec@1 57.812 (64.356)\n",
      "Epoch: [189][234/390]\tTime 0.006 (0.003)\tLoss 1.2292 (1.0164)\tPrec@1 53.906 (63.504)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.003)\tLoss 1.1375 (1.0358)\tPrec@1 58.594 (62.635)\n",
      "Epoch: [189][390/390]\tTime 0.002 (0.003)\tLoss 1.3582 (1.0506)\tPrec@1 48.750 (62.200)\n",
      "EPOCH: 189 train Results: Prec@1 62.200 Loss: 1.0506\n",
      "Test: [0/78]\tTime 0.013 (0.013)\tLoss 1.1495 (1.1495)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4607 (1.2835)\tPrec@1 43.750 (54.460)\n",
      "EPOCH: 189 val Results: Prec@1 54.460 Loss: 1.2835\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [190][0/390]\tTime 0.005 (0.005)\tLoss 0.9696 (0.9696)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.003)\tLoss 1.0165 (0.9770)\tPrec@1 63.281 (65.022)\n",
      "Epoch: [190][156/390]\tTime 0.010 (0.003)\tLoss 0.9380 (1.0007)\tPrec@1 70.312 (64.067)\n",
      "Epoch: [190][234/390]\tTime 0.002 (0.003)\tLoss 1.0972 (1.0272)\tPrec@1 60.156 (62.975)\n",
      "Epoch: [190][312/390]\tTime 0.003 (0.004)\tLoss 0.9632 (1.0446)\tPrec@1 64.844 (62.455)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.003)\tLoss 1.1989 (1.0577)\tPrec@1 60.000 (62.094)\n",
      "EPOCH: 190 train Results: Prec@1 62.094 Loss: 1.0577\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1271 (1.1271)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4742 (1.2877)\tPrec@1 50.000 (54.700)\n",
      "EPOCH: 190 val Results: Prec@1 54.700 Loss: 1.2877\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [191][0/390]\tTime 0.004 (0.004)\tLoss 0.8493 (0.8493)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.003)\tLoss 0.8819 (0.9654)\tPrec@1 75.000 (65.912)\n",
      "Epoch: [191][156/390]\tTime 0.005 (0.004)\tLoss 1.0677 (1.0006)\tPrec@1 61.719 (64.192)\n",
      "Epoch: [191][234/390]\tTime 0.004 (0.004)\tLoss 0.9720 (1.0255)\tPrec@1 67.969 (63.354)\n",
      "Epoch: [191][312/390]\tTime 0.004 (0.004)\tLoss 1.0280 (1.0404)\tPrec@1 60.156 (62.829)\n",
      "Epoch: [191][390/390]\tTime 0.003 (0.004)\tLoss 1.2119 (1.0548)\tPrec@1 55.000 (62.282)\n",
      "EPOCH: 191 train Results: Prec@1 62.282 Loss: 1.0548\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1684 (1.1684)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4545 (1.2672)\tPrec@1 37.500 (55.610)\n",
      "EPOCH: 191 val Results: Prec@1 55.610 Loss: 1.2672\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [192][0/390]\tTime 0.003 (0.003)\tLoss 0.8439 (0.8439)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [192][78/390]\tTime 0.002 (0.004)\tLoss 0.9842 (0.9679)\tPrec@1 63.281 (66.090)\n",
      "Epoch: [192][156/390]\tTime 0.002 (0.004)\tLoss 1.1003 (0.9996)\tPrec@1 64.844 (64.570)\n",
      "Epoch: [192][234/390]\tTime 0.002 (0.004)\tLoss 0.9874 (1.0194)\tPrec@1 59.375 (63.886)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.003)\tLoss 1.1154 (1.0343)\tPrec@1 52.344 (63.384)\n",
      "Epoch: [192][390/390]\tTime 0.010 (0.004)\tLoss 1.0439 (1.0487)\tPrec@1 60.000 (62.846)\n",
      "EPOCH: 192 train Results: Prec@1 62.846 Loss: 1.0487\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1298 (1.1298)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1542 (1.2683)\tPrec@1 56.250 (55.670)\n",
      "EPOCH: 192 val Results: Prec@1 55.670 Loss: 1.2683\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [193][0/390]\tTime 0.003 (0.003)\tLoss 0.8510 (0.8510)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [193][78/390]\tTime 0.006 (0.003)\tLoss 0.9306 (0.9661)\tPrec@1 66.406 (66.228)\n",
      "Epoch: [193][156/390]\tTime 0.015 (0.003)\tLoss 1.0519 (0.9997)\tPrec@1 59.375 (64.471)\n",
      "Epoch: [193][234/390]\tTime 0.002 (0.003)\tLoss 1.0842 (1.0181)\tPrec@1 66.406 (63.873)\n",
      "Epoch: [193][312/390]\tTime 0.003 (0.004)\tLoss 1.1010 (1.0358)\tPrec@1 62.500 (63.304)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.004)\tLoss 1.0003 (1.0465)\tPrec@1 63.750 (62.832)\n",
      "EPOCH: 193 train Results: Prec@1 62.832 Loss: 1.0465\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1363 (1.1363)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3392 (1.2797)\tPrec@1 31.250 (54.730)\n",
      "EPOCH: 193 val Results: Prec@1 54.730 Loss: 1.2797\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [194][0/390]\tTime 0.004 (0.004)\tLoss 0.8793 (0.8793)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [194][78/390]\tTime 0.002 (0.004)\tLoss 1.0017 (0.9867)\tPrec@1 60.938 (64.577)\n",
      "Epoch: [194][156/390]\tTime 0.002 (0.004)\tLoss 0.9124 (1.0059)\tPrec@1 64.062 (63.938)\n",
      "Epoch: [194][234/390]\tTime 0.007 (0.003)\tLoss 1.1256 (1.0232)\tPrec@1 59.375 (63.211)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.003)\tLoss 1.2009 (1.0378)\tPrec@1 57.812 (62.755)\n",
      "Epoch: [194][390/390]\tTime 0.002 (0.003)\tLoss 1.2495 (1.0536)\tPrec@1 52.500 (62.270)\n",
      "EPOCH: 194 train Results: Prec@1 62.270 Loss: 1.0536\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2613 (1.2613)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 0.9177 (1.2736)\tPrec@1 62.500 (55.260)\n",
      "EPOCH: 194 val Results: Prec@1 55.260 Loss: 1.2736\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [195][0/390]\tTime 0.005 (0.005)\tLoss 0.7983 (0.7983)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [195][78/390]\tTime 0.002 (0.003)\tLoss 0.9430 (0.9736)\tPrec@1 68.750 (65.388)\n",
      "Epoch: [195][156/390]\tTime 0.003 (0.003)\tLoss 1.0029 (0.9975)\tPrec@1 61.719 (64.097)\n",
      "Epoch: [195][234/390]\tTime 0.002 (0.003)\tLoss 1.0986 (1.0180)\tPrec@1 58.594 (63.584)\n",
      "Epoch: [195][312/390]\tTime 0.002 (0.003)\tLoss 1.2609 (1.0374)\tPrec@1 52.344 (62.944)\n",
      "Epoch: [195][390/390]\tTime 0.002 (0.003)\tLoss 1.1798 (1.0475)\tPrec@1 57.500 (62.514)\n",
      "EPOCH: 195 train Results: Prec@1 62.514 Loss: 1.0475\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1814 (1.1814)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3546 (1.2995)\tPrec@1 56.250 (54.630)\n",
      "EPOCH: 195 val Results: Prec@1 54.630 Loss: 1.2995\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [196][0/390]\tTime 0.004 (0.004)\tLoss 0.8933 (0.8933)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 1.0879 (0.9626)\tPrec@1 65.625 (65.704)\n",
      "Epoch: [196][156/390]\tTime 0.002 (0.003)\tLoss 1.0811 (0.9926)\tPrec@1 59.375 (64.441)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.004)\tLoss 1.0134 (1.0256)\tPrec@1 67.969 (63.191)\n",
      "Epoch: [196][312/390]\tTime 0.003 (0.003)\tLoss 1.1686 (1.0396)\tPrec@1 57.812 (62.710)\n",
      "Epoch: [196][390/390]\tTime 0.002 (0.003)\tLoss 0.9945 (1.0508)\tPrec@1 63.750 (62.288)\n",
      "EPOCH: 196 train Results: Prec@1 62.288 Loss: 1.0508\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2030 (1.2030)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.1823 (1.2815)\tPrec@1 50.000 (55.060)\n",
      "EPOCH: 196 val Results: Prec@1 55.060 Loss: 1.2815\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [197][0/390]\tTime 0.005 (0.005)\tLoss 0.8775 (0.8775)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [197][78/390]\tTime 0.003 (0.004)\tLoss 1.0880 (0.9498)\tPrec@1 65.625 (66.297)\n",
      "Epoch: [197][156/390]\tTime 0.009 (0.004)\tLoss 1.0555 (0.9869)\tPrec@1 64.844 (64.874)\n",
      "Epoch: [197][234/390]\tTime 0.003 (0.004)\tLoss 0.9627 (1.0194)\tPrec@1 67.188 (63.697)\n",
      "Epoch: [197][312/390]\tTime 0.006 (0.004)\tLoss 0.9931 (1.0408)\tPrec@1 63.281 (62.805)\n",
      "Epoch: [197][390/390]\tTime 0.006 (0.004)\tLoss 0.8275 (1.0544)\tPrec@1 80.000 (62.412)\n",
      "EPOCH: 197 train Results: Prec@1 62.412 Loss: 1.0544\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1605 (1.1605)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5509 (1.2846)\tPrec@1 56.250 (54.890)\n",
      "EPOCH: 197 val Results: Prec@1 54.890 Loss: 1.2846\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [198][0/390]\tTime 0.003 (0.003)\tLoss 1.0150 (1.0150)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [198][78/390]\tTime 0.002 (0.004)\tLoss 1.0224 (0.9739)\tPrec@1 65.625 (65.180)\n",
      "Epoch: [198][156/390]\tTime 0.006 (0.004)\tLoss 1.0505 (0.9959)\tPrec@1 60.156 (64.476)\n",
      "Epoch: [198][234/390]\tTime 0.006 (0.004)\tLoss 1.2706 (1.0219)\tPrec@1 53.906 (63.418)\n",
      "Epoch: [198][312/390]\tTime 0.004 (0.003)\tLoss 1.0281 (1.0372)\tPrec@1 62.500 (62.814)\n",
      "Epoch: [198][390/390]\tTime 0.003 (0.004)\tLoss 1.0056 (1.0512)\tPrec@1 58.750 (62.348)\n",
      "EPOCH: 198 train Results: Prec@1 62.348 Loss: 1.0512\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1018 (1.1018)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4544 (1.2988)\tPrec@1 50.000 (54.130)\n",
      "EPOCH: 198 val Results: Prec@1 54.130 Loss: 1.2988\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [199][0/390]\tTime 0.002 (0.002)\tLoss 1.1413 (1.1413)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [199][78/390]\tTime 0.005 (0.003)\tLoss 1.0338 (0.9565)\tPrec@1 65.625 (66.208)\n",
      "Epoch: [199][156/390]\tTime 0.003 (0.003)\tLoss 1.0599 (0.9834)\tPrec@1 60.156 (65.028)\n",
      "Epoch: [199][234/390]\tTime 0.002 (0.003)\tLoss 0.9818 (1.0138)\tPrec@1 68.750 (63.866)\n",
      "Epoch: [199][312/390]\tTime 0.003 (0.003)\tLoss 1.1839 (1.0316)\tPrec@1 53.125 (62.917)\n",
      "Epoch: [199][390/390]\tTime 0.003 (0.003)\tLoss 1.1163 (1.0477)\tPrec@1 57.500 (62.402)\n",
      "EPOCH: 199 train Results: Prec@1 62.402 Loss: 1.0477\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1605 (1.1605)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1917 (1.2977)\tPrec@1 50.000 (54.400)\n",
      "EPOCH: 199 val Results: Prec@1 54.400 Loss: 1.2977\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "current lr 5.00000e-02\n",
      "Epoch: [200][0/390]\tTime 0.004 (0.004)\tLoss 0.9240 (0.9240)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [200][78/390]\tTime 0.003 (0.003)\tLoss 1.0649 (0.9824)\tPrec@1 57.812 (65.002)\n",
      "Epoch: [200][156/390]\tTime 0.002 (0.003)\tLoss 0.7854 (1.0077)\tPrec@1 71.875 (64.092)\n",
      "Epoch: [200][234/390]\tTime 0.004 (0.003)\tLoss 1.0138 (1.0293)\tPrec@1 65.625 (63.338)\n",
      "Epoch: [200][312/390]\tTime 0.002 (0.003)\tLoss 1.2991 (1.0392)\tPrec@1 53.125 (63.039)\n",
      "Epoch: [200][390/390]\tTime 0.003 (0.003)\tLoss 1.2498 (1.0537)\tPrec@1 53.750 (62.384)\n",
      "EPOCH: 200 train Results: Prec@1 62.384 Loss: 1.0537\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.0851 (1.0851)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1298 (1.2711)\tPrec@1 56.250 (55.420)\n",
      "EPOCH: 200 val Results: Prec@1 55.420 Loss: 1.2711\n",
      "Best Prec@1: 55.920\n",
      "\n",
      "End time:  Thu Apr  4 23:13:39 2024\n",
      "train executed in 281.4510 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.05, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer5 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:13:40 2024\n",
      "current lr 5.00000e-04\n",
      "Epoch: [1][0/390]\tTime 0.006 (0.006)\tLoss 5.5346 (5.5346)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][78/390]\tTime 0.003 (0.004)\tLoss 4.8175 (5.1444)\tPrec@1 11.719 (10.987)\n",
      "Epoch: [1][156/390]\tTime 0.002 (0.003)\tLoss 4.2685 (4.6881)\tPrec@1 11.719 (12.371)\n",
      "Epoch: [1][234/390]\tTime 0.002 (0.003)\tLoss 3.3635 (4.3636)\tPrec@1 19.531 (13.461)\n",
      "Epoch: [1][312/390]\tTime 0.002 (0.003)\tLoss 3.3746 (4.1154)\tPrec@1 19.531 (14.569)\n",
      "Epoch: [1][390/390]\tTime 0.001 (0.003)\tLoss 2.7901 (3.9102)\tPrec@1 21.250 (15.806)\n",
      "EPOCH: 1 train Results: Prec@1 15.806 Loss: 3.9102\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 2.6787 (2.6787)\tPrec@1 20.312 (20.312)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 2.6365 (2.5733)\tPrec@1 25.000 (23.550)\n",
      "EPOCH: 1 val Results: Prec@1 23.550 Loss: 2.5733\n",
      "Best Prec@1: 23.550\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [2][0/390]\tTime 0.004 (0.004)\tLoss 2.9218 (2.9218)\tPrec@1 19.531 (19.531)\n",
      "Epoch: [2][78/390]\tTime 0.004 (0.003)\tLoss 2.4449 (2.8344)\tPrec@1 23.438 (21.727)\n",
      "Epoch: [2][156/390]\tTime 0.008 (0.003)\tLoss 2.6524 (2.7534)\tPrec@1 19.531 (22.930)\n",
      "Epoch: [2][234/390]\tTime 0.002 (0.003)\tLoss 2.8675 (2.6834)\tPrec@1 17.969 (23.594)\n",
      "Epoch: [2][312/390]\tTime 0.009 (0.003)\tLoss 2.6599 (2.6218)\tPrec@1 21.875 (24.321)\n",
      "Epoch: [2][390/390]\tTime 0.007 (0.003)\tLoss 2.2045 (2.5664)\tPrec@1 25.000 (24.878)\n",
      "EPOCH: 2 train Results: Prec@1 24.878 Loss: 2.5664\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 2.1176 (2.1176)\tPrec@1 30.469 (30.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.9654 (2.0576)\tPrec@1 18.750 (31.040)\n",
      "EPOCH: 2 val Results: Prec@1 31.040 Loss: 2.0576\n",
      "Best Prec@1: 31.040\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [3][0/390]\tTime 0.002 (0.002)\tLoss 2.1558 (2.1558)\tPrec@1 28.125 (28.125)\n",
      "Epoch: [3][78/390]\tTime 0.002 (0.003)\tLoss 2.1571 (2.2477)\tPrec@1 32.031 (28.708)\n",
      "Epoch: [3][156/390]\tTime 0.003 (0.004)\tLoss 2.0899 (2.2072)\tPrec@1 25.781 (29.265)\n",
      "Epoch: [3][234/390]\tTime 0.008 (0.003)\tLoss 2.0944 (2.1789)\tPrec@1 27.344 (29.644)\n",
      "Epoch: [3][312/390]\tTime 0.003 (0.004)\tLoss 2.0893 (2.1553)\tPrec@1 30.469 (30.034)\n",
      "Epoch: [3][390/390]\tTime 0.001 (0.003)\tLoss 1.8981 (2.1285)\tPrec@1 30.000 (30.380)\n",
      "EPOCH: 3 train Results: Prec@1 30.380 Loss: 2.1285\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.8987 (1.8987)\tPrec@1 32.031 (32.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7110 (1.8509)\tPrec@1 18.750 (35.400)\n",
      "EPOCH: 3 val Results: Prec@1 35.400 Loss: 1.8509\n",
      "Best Prec@1: 35.400\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [4][0/390]\tTime 0.002 (0.002)\tLoss 2.0733 (2.0733)\tPrec@1 21.875 (21.875)\n",
      "Epoch: [4][78/390]\tTime 0.003 (0.003)\tLoss 2.0473 (1.9675)\tPrec@1 35.156 (32.605)\n",
      "Epoch: [4][156/390]\tTime 0.002 (0.003)\tLoss 1.9737 (1.9443)\tPrec@1 34.375 (33.181)\n",
      "Epoch: [4][234/390]\tTime 0.003 (0.003)\tLoss 2.0308 (1.9294)\tPrec@1 28.125 (33.358)\n",
      "Epoch: [4][312/390]\tTime 0.007 (0.003)\tLoss 1.7379 (1.9162)\tPrec@1 37.500 (33.681)\n",
      "Epoch: [4][390/390]\tTime 0.005 (0.003)\tLoss 1.7338 (1.9012)\tPrec@1 35.000 (34.008)\n",
      "EPOCH: 4 train Results: Prec@1 34.008 Loss: 1.9012\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.7558 (1.7558)\tPrec@1 35.156 (35.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6075 (1.7430)\tPrec@1 31.250 (38.480)\n",
      "EPOCH: 4 val Results: Prec@1 38.480 Loss: 1.7430\n",
      "Best Prec@1: 38.480\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [5][0/390]\tTime 0.004 (0.004)\tLoss 1.9966 (1.9966)\tPrec@1 25.781 (25.781)\n",
      "Epoch: [5][78/390]\tTime 0.002 (0.003)\tLoss 1.9753 (1.7861)\tPrec@1 36.719 (36.689)\n",
      "Epoch: [5][156/390]\tTime 0.002 (0.003)\tLoss 1.8870 (1.7971)\tPrec@1 34.375 (36.385)\n",
      "Epoch: [5][234/390]\tTime 0.004 (0.003)\tLoss 1.7252 (1.7840)\tPrec@1 36.719 (36.905)\n",
      "Epoch: [5][312/390]\tTime 0.003 (0.003)\tLoss 1.6975 (1.7773)\tPrec@1 34.375 (37.245)\n",
      "Epoch: [5][390/390]\tTime 0.001 (0.003)\tLoss 1.7003 (1.7683)\tPrec@1 42.500 (37.586)\n",
      "EPOCH: 5 train Results: Prec@1 37.586 Loss: 1.7683\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.6671 (1.6671)\tPrec@1 38.281 (38.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5541 (1.6777)\tPrec@1 31.250 (40.820)\n",
      "EPOCH: 5 val Results: Prec@1 40.820 Loss: 1.6777\n",
      "Best Prec@1: 40.820\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [6][0/390]\tTime 0.002 (0.002)\tLoss 1.7795 (1.7795)\tPrec@1 40.625 (40.625)\n",
      "Epoch: [6][78/390]\tTime 0.004 (0.003)\tLoss 1.7975 (1.6895)\tPrec@1 35.938 (40.882)\n",
      "Epoch: [6][156/390]\tTime 0.007 (0.003)\tLoss 1.5600 (1.6888)\tPrec@1 40.625 (40.779)\n",
      "Epoch: [6][234/390]\tTime 0.006 (0.003)\tLoss 1.7876 (1.6873)\tPrec@1 37.500 (40.682)\n",
      "Epoch: [6][312/390]\tTime 0.004 (0.003)\tLoss 1.7899 (1.6860)\tPrec@1 35.938 (40.695)\n",
      "Epoch: [6][390/390]\tTime 0.001 (0.004)\tLoss 1.8947 (1.6824)\tPrec@1 27.500 (40.832)\n",
      "EPOCH: 6 train Results: Prec@1 40.832 Loss: 1.6824\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.6071 (1.6071)\tPrec@1 46.875 (46.875)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5382 (1.6356)\tPrec@1 31.250 (42.810)\n",
      "EPOCH: 6 val Results: Prec@1 42.810 Loss: 1.6356\n",
      "Best Prec@1: 42.810\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [7][0/390]\tTime 0.004 (0.004)\tLoss 1.6896 (1.6896)\tPrec@1 39.062 (39.062)\n",
      "Epoch: [7][78/390]\tTime 0.002 (0.004)\tLoss 1.6855 (1.6414)\tPrec@1 35.938 (42.534)\n",
      "Epoch: [7][156/390]\tTime 0.004 (0.004)\tLoss 1.6619 (1.6369)\tPrec@1 43.750 (43.004)\n",
      "Epoch: [7][234/390]\tTime 0.005 (0.004)\tLoss 1.6211 (1.6315)\tPrec@1 43.750 (42.759)\n",
      "Epoch: [7][312/390]\tTime 0.003 (0.004)\tLoss 1.5375 (1.6275)\tPrec@1 48.438 (42.789)\n",
      "Epoch: [7][390/390]\tTime 0.003 (0.004)\tLoss 1.6448 (1.6271)\tPrec@1 51.250 (42.850)\n",
      "EPOCH: 7 train Results: Prec@1 42.850 Loss: 1.6271\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.5640 (1.5640)\tPrec@1 46.875 (46.875)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5290 (1.6051)\tPrec@1 31.250 (43.980)\n",
      "EPOCH: 7 val Results: Prec@1 43.980 Loss: 1.6051\n",
      "Best Prec@1: 43.980\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [8][0/390]\tTime 0.006 (0.006)\tLoss 1.6069 (1.6069)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [8][78/390]\tTime 0.003 (0.003)\tLoss 1.6141 (1.6008)\tPrec@1 44.531 (44.610)\n",
      "Epoch: [8][156/390]\tTime 0.004 (0.004)\tLoss 1.6429 (1.6006)\tPrec@1 36.719 (44.357)\n",
      "Epoch: [8][234/390]\tTime 0.002 (0.004)\tLoss 1.5426 (1.5956)\tPrec@1 44.531 (44.561)\n",
      "Epoch: [8][312/390]\tTime 0.009 (0.004)\tLoss 1.4690 (1.5909)\tPrec@1 49.219 (44.893)\n",
      "Epoch: [8][390/390]\tTime 0.004 (0.003)\tLoss 1.7086 (1.5901)\tPrec@1 36.250 (44.962)\n",
      "EPOCH: 8 train Results: Prec@1 44.962 Loss: 1.5901\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.5364 (1.5364)\tPrec@1 46.875 (46.875)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5017 (1.5803)\tPrec@1 31.250 (45.360)\n",
      "EPOCH: 8 val Results: Prec@1 45.360 Loss: 1.5803\n",
      "Best Prec@1: 45.360\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [9][0/390]\tTime 0.003 (0.003)\tLoss 1.5893 (1.5893)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [9][78/390]\tTime 0.011 (0.004)\tLoss 1.5062 (1.5544)\tPrec@1 50.000 (46.855)\n",
      "Epoch: [9][156/390]\tTime 0.002 (0.004)\tLoss 1.7130 (1.5528)\tPrec@1 39.844 (46.835)\n",
      "Epoch: [9][234/390]\tTime 0.003 (0.004)\tLoss 1.6062 (1.5595)\tPrec@1 43.750 (46.366)\n",
      "Epoch: [9][312/390]\tTime 0.002 (0.004)\tLoss 1.5954 (1.5586)\tPrec@1 44.531 (46.413)\n",
      "Epoch: [9][390/390]\tTime 0.006 (0.004)\tLoss 1.7099 (1.5593)\tPrec@1 45.000 (46.332)\n",
      "EPOCH: 9 train Results: Prec@1 46.332 Loss: 1.5593\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.5118 (1.5118)\tPrec@1 51.562 (51.562)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5166 (1.5599)\tPrec@1 18.750 (46.420)\n",
      "EPOCH: 9 val Results: Prec@1 46.420 Loss: 1.5599\n",
      "Best Prec@1: 46.420\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [10][0/390]\tTime 0.005 (0.005)\tLoss 1.5762 (1.5762)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [10][78/390]\tTime 0.004 (0.003)\tLoss 1.5180 (1.5301)\tPrec@1 52.344 (48.042)\n",
      "Epoch: [10][156/390]\tTime 0.004 (0.003)\tLoss 1.5919 (1.5417)\tPrec@1 46.875 (47.169)\n",
      "Epoch: [10][234/390]\tTime 0.002 (0.003)\tLoss 1.5746 (1.5414)\tPrec@1 39.844 (47.071)\n",
      "Epoch: [10][312/390]\tTime 0.003 (0.003)\tLoss 1.5687 (1.5385)\tPrec@1 50.000 (47.267)\n",
      "Epoch: [10][390/390]\tTime 0.003 (0.003)\tLoss 1.3883 (1.5375)\tPrec@1 51.250 (47.384)\n",
      "EPOCH: 10 train Results: Prec@1 47.384 Loss: 1.5375\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4993 (1.4993)\tPrec@1 47.656 (47.656)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5652 (1.5337)\tPrec@1 18.750 (46.920)\n",
      "EPOCH: 10 val Results: Prec@1 46.920 Loss: 1.5337\n",
      "Best Prec@1: 46.920\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [11][0/390]\tTime 0.002 (0.002)\tLoss 1.4812 (1.4812)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [11][78/390]\tTime 0.002 (0.003)\tLoss 1.4978 (1.5062)\tPrec@1 48.438 (48.655)\n",
      "Epoch: [11][156/390]\tTime 0.002 (0.003)\tLoss 1.4570 (1.5085)\tPrec@1 52.344 (48.423)\n",
      "Epoch: [11][234/390]\tTime 0.003 (0.003)\tLoss 1.5654 (1.5117)\tPrec@1 47.656 (48.258)\n",
      "Epoch: [11][312/390]\tTime 0.003 (0.003)\tLoss 1.4224 (1.5097)\tPrec@1 54.688 (48.333)\n",
      "Epoch: [11][390/390]\tTime 0.001 (0.003)\tLoss 1.5814 (1.5098)\tPrec@1 47.500 (48.240)\n",
      "EPOCH: 11 train Results: Prec@1 48.240 Loss: 1.5098\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4702 (1.4702)\tPrec@1 49.219 (49.219)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5406 (1.5104)\tPrec@1 18.750 (47.950)\n",
      "EPOCH: 11 val Results: Prec@1 47.950 Loss: 1.5104\n",
      "Best Prec@1: 47.950\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [12][0/390]\tTime 0.002 (0.002)\tLoss 1.4779 (1.4779)\tPrec@1 50.781 (50.781)\n",
      "Epoch: [12][78/390]\tTime 0.003 (0.003)\tLoss 1.4438 (1.4816)\tPrec@1 49.219 (50.020)\n",
      "Epoch: [12][156/390]\tTime 0.002 (0.003)\tLoss 1.5597 (1.4816)\tPrec@1 42.188 (49.746)\n",
      "Epoch: [12][234/390]\tTime 0.002 (0.003)\tLoss 1.3459 (1.4792)\tPrec@1 55.469 (49.727)\n",
      "Epoch: [12][312/390]\tTime 0.002 (0.003)\tLoss 1.4837 (1.4789)\tPrec@1 52.344 (49.643)\n",
      "Epoch: [12][390/390]\tTime 0.005 (0.003)\tLoss 1.3271 (1.4788)\tPrec@1 56.250 (49.672)\n",
      "EPOCH: 12 train Results: Prec@1 49.672 Loss: 1.4788\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.4223 (1.4223)\tPrec@1 51.562 (51.562)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4382 (1.4756)\tPrec@1 37.500 (48.560)\n",
      "EPOCH: 12 val Results: Prec@1 48.560 Loss: 1.4756\n",
      "Best Prec@1: 48.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [13][0/390]\tTime 0.002 (0.002)\tLoss 1.4517 (1.4517)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [13][78/390]\tTime 0.002 (0.003)\tLoss 1.4005 (1.4485)\tPrec@1 55.469 (50.455)\n",
      "Epoch: [13][156/390]\tTime 0.002 (0.003)\tLoss 1.3852 (1.4433)\tPrec@1 52.344 (50.995)\n",
      "Epoch: [13][234/390]\tTime 0.003 (0.003)\tLoss 1.5293 (1.4463)\tPrec@1 49.219 (50.878)\n",
      "Epoch: [13][312/390]\tTime 0.002 (0.003)\tLoss 1.4393 (1.4484)\tPrec@1 48.438 (50.599)\n",
      "Epoch: [13][390/390]\tTime 0.002 (0.003)\tLoss 1.4006 (1.4477)\tPrec@1 47.500 (50.696)\n",
      "EPOCH: 13 train Results: Prec@1 50.696 Loss: 1.4477\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.3893 (1.3893)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5100 (1.4463)\tPrec@1 31.250 (49.610)\n",
      "EPOCH: 13 val Results: Prec@1 49.610 Loss: 1.4463\n",
      "Best Prec@1: 49.610\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [14][0/390]\tTime 0.002 (0.002)\tLoss 1.3367 (1.3367)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [14][78/390]\tTime 0.002 (0.003)\tLoss 1.3904 (1.4139)\tPrec@1 55.469 (51.998)\n",
      "Epoch: [14][156/390]\tTime 0.002 (0.003)\tLoss 1.4554 (1.4135)\tPrec@1 51.562 (51.876)\n",
      "Epoch: [14][234/390]\tTime 0.002 (0.003)\tLoss 1.4012 (1.4177)\tPrec@1 52.344 (51.679)\n",
      "Epoch: [14][312/390]\tTime 0.002 (0.003)\tLoss 1.3118 (1.4171)\tPrec@1 50.781 (51.655)\n",
      "Epoch: [14][390/390]\tTime 0.001 (0.003)\tLoss 1.5682 (1.4174)\tPrec@1 45.000 (51.600)\n",
      "EPOCH: 14 train Results: Prec@1 51.600 Loss: 1.4174\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.3433 (1.3433)\tPrec@1 53.125 (53.125)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4291 (1.4204)\tPrec@1 37.500 (51.140)\n",
      "EPOCH: 14 val Results: Prec@1 51.140 Loss: 1.4204\n",
      "Best Prec@1: 51.140\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [15][0/390]\tTime 0.009 (0.009)\tLoss 1.3124 (1.3124)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [15][78/390]\tTime 0.002 (0.003)\tLoss 1.4701 (1.3763)\tPrec@1 49.219 (53.204)\n",
      "Epoch: [15][156/390]\tTime 0.002 (0.003)\tLoss 1.2889 (1.3829)\tPrec@1 52.344 (53.110)\n",
      "Epoch: [15][234/390]\tTime 0.003 (0.003)\tLoss 1.3548 (1.3867)\tPrec@1 57.812 (52.590)\n",
      "Epoch: [15][312/390]\tTime 0.010 (0.003)\tLoss 1.5340 (1.3884)\tPrec@1 50.781 (52.441)\n",
      "Epoch: [15][390/390]\tTime 0.001 (0.003)\tLoss 1.3691 (1.3882)\tPrec@1 53.750 (52.478)\n",
      "EPOCH: 15 train Results: Prec@1 52.478 Loss: 1.3882\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2691 (1.2691)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2573 (1.3908)\tPrec@1 31.250 (51.630)\n",
      "EPOCH: 15 val Results: Prec@1 51.630 Loss: 1.3908\n",
      "Best Prec@1: 51.630\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [16][0/390]\tTime 0.005 (0.005)\tLoss 1.2812 (1.2812)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [16][78/390]\tTime 0.002 (0.003)\tLoss 1.2880 (1.3322)\tPrec@1 56.250 (54.420)\n",
      "Epoch: [16][156/390]\tTime 0.003 (0.003)\tLoss 1.3715 (1.3490)\tPrec@1 50.781 (53.608)\n",
      "Epoch: [16][234/390]\tTime 0.006 (0.003)\tLoss 1.5338 (1.3583)\tPrec@1 48.438 (53.444)\n",
      "Epoch: [16][312/390]\tTime 0.004 (0.003)\tLoss 1.3443 (1.3638)\tPrec@1 52.344 (53.225)\n",
      "Epoch: [16][390/390]\tTime 0.001 (0.003)\tLoss 1.5525 (1.3681)\tPrec@1 47.500 (53.046)\n",
      "EPOCH: 16 train Results: Prec@1 53.046 Loss: 1.3681\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2525 (1.2525)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2814 (1.3807)\tPrec@1 50.000 (52.080)\n",
      "EPOCH: 16 val Results: Prec@1 52.080 Loss: 1.3807\n",
      "Best Prec@1: 52.080\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [17][0/390]\tTime 0.005 (0.005)\tLoss 1.3545 (1.3545)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [17][78/390]\tTime 0.004 (0.003)\tLoss 1.4050 (1.3284)\tPrec@1 47.656 (54.509)\n",
      "Epoch: [17][156/390]\tTime 0.003 (0.003)\tLoss 1.4489 (1.3356)\tPrec@1 48.438 (54.260)\n",
      "Epoch: [17][234/390]\tTime 0.002 (0.003)\tLoss 1.3742 (1.3423)\tPrec@1 50.781 (53.896)\n",
      "Epoch: [17][312/390]\tTime 0.004 (0.003)\tLoss 1.3717 (1.3448)\tPrec@1 52.344 (53.869)\n",
      "Epoch: [17][390/390]\tTime 0.004 (0.003)\tLoss 1.2755 (1.3497)\tPrec@1 60.000 (53.726)\n",
      "EPOCH: 17 train Results: Prec@1 53.726 Loss: 1.3497\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2557 (1.2557)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2048 (1.3666)\tPrec@1 62.500 (52.360)\n",
      "EPOCH: 17 val Results: Prec@1 52.360 Loss: 1.3666\n",
      "Best Prec@1: 52.360\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [18][0/390]\tTime 0.003 (0.003)\tLoss 1.2693 (1.2693)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [18][78/390]\tTime 0.005 (0.003)\tLoss 1.3496 (1.3132)\tPrec@1 52.344 (55.014)\n",
      "Epoch: [18][156/390]\tTime 0.003 (0.003)\tLoss 1.2631 (1.3224)\tPrec@1 57.812 (54.747)\n",
      "Epoch: [18][234/390]\tTime 0.003 (0.004)\tLoss 1.2049 (1.3354)\tPrec@1 57.812 (54.092)\n",
      "Epoch: [18][312/390]\tTime 0.002 (0.004)\tLoss 1.3427 (1.3367)\tPrec@1 55.469 (54.044)\n",
      "Epoch: [18][390/390]\tTime 0.001 (0.003)\tLoss 1.2166 (1.3379)\tPrec@1 58.750 (53.956)\n",
      "EPOCH: 18 train Results: Prec@1 53.956 Loss: 1.3379\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2245 (1.2245)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2927 (1.3532)\tPrec@1 50.000 (53.160)\n",
      "EPOCH: 18 val Results: Prec@1 53.160 Loss: 1.3532\n",
      "Best Prec@1: 53.160\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [19][0/390]\tTime 0.003 (0.003)\tLoss 1.3007 (1.3007)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [19][78/390]\tTime 0.003 (0.003)\tLoss 1.4261 (1.2807)\tPrec@1 50.781 (56.804)\n",
      "Epoch: [19][156/390]\tTime 0.002 (0.003)\tLoss 1.2704 (1.3020)\tPrec@1 56.250 (55.797)\n",
      "Epoch: [19][234/390]\tTime 0.005 (0.003)\tLoss 1.3132 (1.3064)\tPrec@1 53.125 (55.436)\n",
      "Epoch: [19][312/390]\tTime 0.005 (0.003)\tLoss 1.2001 (1.3178)\tPrec@1 62.500 (54.762)\n",
      "Epoch: [19][390/390]\tTime 0.002 (0.003)\tLoss 1.5276 (1.3216)\tPrec@1 45.000 (54.578)\n",
      "EPOCH: 19 train Results: Prec@1 54.578 Loss: 1.3216\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2494 (1.2494)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5206 (1.3451)\tPrec@1 25.000 (53.310)\n",
      "EPOCH: 19 val Results: Prec@1 53.310 Loss: 1.3451\n",
      "Best Prec@1: 53.310\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [20][0/390]\tTime 0.004 (0.004)\tLoss 1.2568 (1.2568)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [20][78/390]\tTime 0.005 (0.003)\tLoss 1.1280 (1.2944)\tPrec@1 59.375 (56.013)\n",
      "Epoch: [20][156/390]\tTime 0.003 (0.003)\tLoss 1.2142 (1.2967)\tPrec@1 54.688 (55.862)\n",
      "Epoch: [20][234/390]\tTime 0.002 (0.003)\tLoss 1.3250 (1.3078)\tPrec@1 56.250 (55.083)\n",
      "Epoch: [20][312/390]\tTime 0.002 (0.003)\tLoss 1.3619 (1.3099)\tPrec@1 55.469 (54.957)\n",
      "Epoch: [20][390/390]\tTime 0.002 (0.003)\tLoss 1.3924 (1.3121)\tPrec@1 51.250 (54.858)\n",
      "EPOCH: 20 train Results: Prec@1 54.858 Loss: 1.3121\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2322 (1.2322)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3543 (1.3368)\tPrec@1 50.000 (53.340)\n",
      "EPOCH: 20 val Results: Prec@1 53.340 Loss: 1.3368\n",
      "Best Prec@1: 53.340\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [21][0/390]\tTime 0.003 (0.003)\tLoss 1.3590 (1.3590)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [21][78/390]\tTime 0.002 (0.003)\tLoss 1.4001 (1.2653)\tPrec@1 52.344 (56.883)\n",
      "Epoch: [21][156/390]\tTime 0.003 (0.003)\tLoss 1.4362 (1.2808)\tPrec@1 48.438 (56.220)\n",
      "Epoch: [21][234/390]\tTime 0.002 (0.003)\tLoss 1.2988 (1.2902)\tPrec@1 54.688 (55.924)\n",
      "Epoch: [21][312/390]\tTime 0.002 (0.003)\tLoss 1.2414 (1.3001)\tPrec@1 54.688 (55.549)\n",
      "Epoch: [21][390/390]\tTime 0.003 (0.003)\tLoss 1.3374 (1.3066)\tPrec@1 56.250 (55.172)\n",
      "EPOCH: 21 train Results: Prec@1 55.172 Loss: 1.3066\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2689 (1.2689)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2487 (1.3327)\tPrec@1 43.750 (54.030)\n",
      "EPOCH: 21 val Results: Prec@1 54.030 Loss: 1.3327\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [22][0/390]\tTime 0.002 (0.002)\tLoss 1.2814 (1.2814)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [22][78/390]\tTime 0.003 (0.003)\tLoss 1.2571 (1.2760)\tPrec@1 53.906 (56.794)\n",
      "Epoch: [22][156/390]\tTime 0.002 (0.003)\tLoss 1.2092 (1.2824)\tPrec@1 57.812 (56.369)\n",
      "Epoch: [22][234/390]\tTime 0.003 (0.003)\tLoss 1.2733 (1.2872)\tPrec@1 54.688 (55.997)\n",
      "Epoch: [22][312/390]\tTime 0.002 (0.003)\tLoss 1.2672 (1.2947)\tPrec@1 53.906 (55.576)\n",
      "Epoch: [22][390/390]\tTime 0.007 (0.003)\tLoss 1.2212 (1.3009)\tPrec@1 65.000 (55.330)\n",
      "EPOCH: 22 train Results: Prec@1 55.330 Loss: 1.3009\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2343 (1.2343)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1572 (1.3286)\tPrec@1 56.250 (53.950)\n",
      "EPOCH: 22 val Results: Prec@1 53.950 Loss: 1.3286\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [23][0/390]\tTime 0.004 (0.004)\tLoss 1.2254 (1.2254)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [23][78/390]\tTime 0.004 (0.003)\tLoss 1.4177 (1.2693)\tPrec@1 51.562 (57.002)\n",
      "Epoch: [23][156/390]\tTime 0.002 (0.003)\tLoss 1.4447 (1.2819)\tPrec@1 54.688 (56.160)\n",
      "Epoch: [23][234/390]\tTime 0.002 (0.003)\tLoss 1.2347 (1.2818)\tPrec@1 60.156 (56.107)\n",
      "Epoch: [23][312/390]\tTime 0.002 (0.003)\tLoss 1.3901 (1.2888)\tPrec@1 50.000 (55.858)\n",
      "Epoch: [23][390/390]\tTime 0.002 (0.003)\tLoss 1.4754 (1.2957)\tPrec@1 53.750 (55.618)\n",
      "EPOCH: 23 train Results: Prec@1 55.618 Loss: 1.2957\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2027 (1.2027)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3861 (1.3374)\tPrec@1 37.500 (53.320)\n",
      "EPOCH: 23 val Results: Prec@1 53.320 Loss: 1.3374\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [24][0/390]\tTime 0.003 (0.003)\tLoss 1.2541 (1.2541)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [24][78/390]\tTime 0.002 (0.003)\tLoss 1.2974 (1.2398)\tPrec@1 58.594 (58.722)\n",
      "Epoch: [24][156/390]\tTime 0.002 (0.003)\tLoss 1.3168 (1.2620)\tPrec@1 57.031 (57.305)\n",
      "Epoch: [24][234/390]\tTime 0.004 (0.003)\tLoss 1.3782 (1.2786)\tPrec@1 51.562 (56.579)\n",
      "Epoch: [24][312/390]\tTime 0.004 (0.003)\tLoss 1.3844 (1.2841)\tPrec@1 50.000 (56.377)\n",
      "Epoch: [24][390/390]\tTime 0.001 (0.003)\tLoss 1.3160 (1.2875)\tPrec@1 52.500 (56.156)\n",
      "EPOCH: 24 train Results: Prec@1 56.156 Loss: 1.2875\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2241 (1.2241)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2286 (1.3253)\tPrec@1 43.750 (53.610)\n",
      "EPOCH: 24 val Results: Prec@1 53.610 Loss: 1.3253\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [25][0/390]\tTime 0.002 (0.002)\tLoss 1.2641 (1.2641)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [25][78/390]\tTime 0.003 (0.003)\tLoss 1.2123 (1.2577)\tPrec@1 57.812 (57.387)\n",
      "Epoch: [25][156/390]\tTime 0.003 (0.003)\tLoss 1.2666 (1.2697)\tPrec@1 57.031 (56.932)\n",
      "Epoch: [25][234/390]\tTime 0.011 (0.003)\tLoss 1.3335 (1.2774)\tPrec@1 52.344 (56.287)\n",
      "Epoch: [25][312/390]\tTime 0.002 (0.003)\tLoss 1.3970 (1.2833)\tPrec@1 50.000 (55.960)\n",
      "Epoch: [25][390/390]\tTime 0.004 (0.003)\tLoss 1.4233 (1.2861)\tPrec@1 45.000 (55.860)\n",
      "EPOCH: 25 train Results: Prec@1 55.860 Loss: 1.2861\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2205 (1.2205)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4438 (1.3246)\tPrec@1 31.250 (53.510)\n",
      "EPOCH: 25 val Results: Prec@1 53.510 Loss: 1.3246\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [26][0/390]\tTime 0.003 (0.003)\tLoss 1.2810 (1.2810)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [26][78/390]\tTime 0.002 (0.003)\tLoss 1.2420 (1.2373)\tPrec@1 61.719 (58.366)\n",
      "Epoch: [26][156/390]\tTime 0.007 (0.003)\tLoss 1.2752 (1.2572)\tPrec@1 53.906 (57.718)\n",
      "Epoch: [26][234/390]\tTime 0.003 (0.003)\tLoss 1.2180 (1.2658)\tPrec@1 57.812 (56.995)\n",
      "Epoch: [26][312/390]\tTime 0.002 (0.003)\tLoss 1.2585 (1.2754)\tPrec@1 57.031 (56.382)\n",
      "Epoch: [26][390/390]\tTime 0.010 (0.003)\tLoss 1.3419 (1.2796)\tPrec@1 50.000 (56.212)\n",
      "EPOCH: 26 train Results: Prec@1 56.212 Loss: 1.2796\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2149 (1.2149)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3660 (1.3258)\tPrec@1 25.000 (54.030)\n",
      "EPOCH: 26 val Results: Prec@1 54.030 Loss: 1.3258\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [27][0/390]\tTime 0.002 (0.002)\tLoss 1.3688 (1.3688)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [27][78/390]\tTime 0.003 (0.003)\tLoss 1.2031 (1.2418)\tPrec@1 60.156 (57.694)\n",
      "Epoch: [27][156/390]\tTime 0.003 (0.003)\tLoss 1.1492 (1.2541)\tPrec@1 59.375 (57.171)\n",
      "Epoch: [27][234/390]\tTime 0.003 (0.003)\tLoss 1.1679 (1.2626)\tPrec@1 61.719 (56.699)\n",
      "Epoch: [27][312/390]\tTime 0.002 (0.003)\tLoss 1.2486 (1.2718)\tPrec@1 60.938 (56.355)\n",
      "Epoch: [27][390/390]\tTime 0.003 (0.003)\tLoss 1.5213 (1.2792)\tPrec@1 48.750 (56.128)\n",
      "EPOCH: 27 train Results: Prec@1 56.128 Loss: 1.2792\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1781 (1.1781)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.004 (0.001)\tLoss 1.3718 (1.3234)\tPrec@1 50.000 (53.910)\n",
      "EPOCH: 27 val Results: Prec@1 53.910 Loss: 1.3234\n",
      "Best Prec@1: 54.030\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [28][0/390]\tTime 0.005 (0.005)\tLoss 1.2218 (1.2218)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [28][78/390]\tTime 0.002 (0.003)\tLoss 1.1611 (1.2433)\tPrec@1 66.406 (57.981)\n",
      "Epoch: [28][156/390]\tTime 0.002 (0.003)\tLoss 1.2403 (1.2505)\tPrec@1 60.938 (57.618)\n",
      "Epoch: [28][234/390]\tTime 0.004 (0.003)\tLoss 1.2478 (1.2596)\tPrec@1 56.250 (57.221)\n",
      "Epoch: [28][312/390]\tTime 0.006 (0.003)\tLoss 1.3227 (1.2666)\tPrec@1 51.562 (56.904)\n",
      "Epoch: [28][390/390]\tTime 0.002 (0.003)\tLoss 1.4296 (1.2733)\tPrec@1 42.500 (56.570)\n",
      "EPOCH: 28 train Results: Prec@1 56.570 Loss: 1.2733\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2230 (1.2230)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3409 (1.3152)\tPrec@1 43.750 (54.310)\n",
      "EPOCH: 28 val Results: Prec@1 54.310 Loss: 1.3152\n",
      "Best Prec@1: 54.310\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [29][0/390]\tTime 0.006 (0.006)\tLoss 1.1836 (1.1836)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [29][78/390]\tTime 0.004 (0.003)\tLoss 1.3463 (1.2502)\tPrec@1 52.344 (57.358)\n",
      "Epoch: [29][156/390]\tTime 0.002 (0.003)\tLoss 1.2039 (1.2564)\tPrec@1 59.375 (57.404)\n",
      "Epoch: [29][234/390]\tTime 0.005 (0.003)\tLoss 1.1700 (1.2646)\tPrec@1 65.625 (57.118)\n",
      "Epoch: [29][312/390]\tTime 0.002 (0.003)\tLoss 1.2730 (1.2711)\tPrec@1 53.906 (56.804)\n",
      "Epoch: [29][390/390]\tTime 0.004 (0.003)\tLoss 1.3716 (1.2747)\tPrec@1 48.750 (56.462)\n",
      "EPOCH: 29 train Results: Prec@1 56.462 Loss: 1.2747\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1526 (1.1526)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1597 (1.3167)\tPrec@1 50.000 (54.700)\n",
      "EPOCH: 29 val Results: Prec@1 54.700 Loss: 1.3167\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [30][0/390]\tTime 0.010 (0.010)\tLoss 1.2591 (1.2591)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [30][78/390]\tTime 0.002 (0.003)\tLoss 1.2796 (1.2343)\tPrec@1 56.250 (58.258)\n",
      "Epoch: [30][156/390]\tTime 0.004 (0.003)\tLoss 1.2549 (1.2494)\tPrec@1 57.031 (57.972)\n",
      "Epoch: [30][234/390]\tTime 0.003 (0.003)\tLoss 1.2572 (1.2552)\tPrec@1 53.906 (57.350)\n",
      "Epoch: [30][312/390]\tTime 0.004 (0.003)\tLoss 1.2226 (1.2607)\tPrec@1 58.594 (56.916)\n",
      "Epoch: [30][390/390]\tTime 0.001 (0.003)\tLoss 1.2421 (1.2677)\tPrec@1 61.250 (56.642)\n",
      "EPOCH: 30 train Results: Prec@1 56.642 Loss: 1.2677\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1910 (1.1910)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2805 (1.3166)\tPrec@1 43.750 (53.950)\n",
      "EPOCH: 30 val Results: Prec@1 53.950 Loss: 1.3166\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [31][0/390]\tTime 0.002 (0.002)\tLoss 1.2185 (1.2185)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [31][78/390]\tTime 0.003 (0.003)\tLoss 1.2112 (1.2376)\tPrec@1 56.250 (58.139)\n",
      "Epoch: [31][156/390]\tTime 0.003 (0.003)\tLoss 1.2326 (1.2475)\tPrec@1 58.594 (57.947)\n",
      "Epoch: [31][234/390]\tTime 0.002 (0.003)\tLoss 1.2922 (1.2549)\tPrec@1 55.469 (57.550)\n",
      "Epoch: [31][312/390]\tTime 0.007 (0.003)\tLoss 1.2123 (1.2610)\tPrec@1 61.719 (57.116)\n",
      "Epoch: [31][390/390]\tTime 0.003 (0.003)\tLoss 1.2920 (1.2676)\tPrec@1 58.750 (56.722)\n",
      "EPOCH: 31 train Results: Prec@1 56.722 Loss: 1.2676\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2578 (1.2578)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4832 (1.3220)\tPrec@1 31.250 (53.870)\n",
      "EPOCH: 31 val Results: Prec@1 53.870 Loss: 1.3220\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [32][0/390]\tTime 0.005 (0.005)\tLoss 1.2493 (1.2493)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [32][78/390]\tTime 0.004 (0.003)\tLoss 1.1590 (1.2386)\tPrec@1 60.156 (58.030)\n",
      "Epoch: [32][156/390]\tTime 0.002 (0.003)\tLoss 1.0421 (1.2432)\tPrec@1 69.531 (57.444)\n",
      "Epoch: [32][234/390]\tTime 0.004 (0.003)\tLoss 1.2497 (1.2533)\tPrec@1 57.031 (57.064)\n",
      "Epoch: [32][312/390]\tTime 0.002 (0.003)\tLoss 1.2187 (1.2581)\tPrec@1 59.375 (56.931)\n",
      "Epoch: [32][390/390]\tTime 0.002 (0.003)\tLoss 1.2152 (1.2659)\tPrec@1 55.000 (56.648)\n",
      "EPOCH: 32 train Results: Prec@1 56.648 Loss: 1.2659\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.2129 (1.2129)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3419 (1.3118)\tPrec@1 37.500 (54.440)\n",
      "EPOCH: 32 val Results: Prec@1 54.440 Loss: 1.3118\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [33][0/390]\tTime 0.005 (0.005)\tLoss 1.3007 (1.3007)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [33][78/390]\tTime 0.002 (0.003)\tLoss 1.3719 (1.2385)\tPrec@1 53.125 (57.902)\n",
      "Epoch: [33][156/390]\tTime 0.002 (0.003)\tLoss 1.3224 (1.2489)\tPrec@1 50.781 (57.275)\n",
      "Epoch: [33][234/390]\tTime 0.002 (0.003)\tLoss 1.3035 (1.2507)\tPrec@1 54.688 (57.251)\n",
      "Epoch: [33][312/390]\tTime 0.002 (0.003)\tLoss 1.3085 (1.2598)\tPrec@1 54.688 (56.896)\n",
      "Epoch: [33][390/390]\tTime 0.001 (0.003)\tLoss 1.2818 (1.2638)\tPrec@1 53.750 (56.788)\n",
      "EPOCH: 33 train Results: Prec@1 56.788 Loss: 1.2638\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2086 (1.2086)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4200 (1.3122)\tPrec@1 37.500 (54.410)\n",
      "EPOCH: 33 val Results: Prec@1 54.410 Loss: 1.3122\n",
      "Best Prec@1: 54.700\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [34][0/390]\tTime 0.002 (0.002)\tLoss 1.1757 (1.1757)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [34][78/390]\tTime 0.005 (0.003)\tLoss 1.3173 (1.2209)\tPrec@1 50.000 (59.326)\n",
      "Epoch: [34][156/390]\tTime 0.002 (0.003)\tLoss 1.2840 (1.2357)\tPrec@1 55.469 (58.260)\n",
      "Epoch: [34][234/390]\tTime 0.002 (0.003)\tLoss 1.2302 (1.2416)\tPrec@1 56.250 (57.846)\n",
      "Epoch: [34][312/390]\tTime 0.003 (0.003)\tLoss 1.2348 (1.2508)\tPrec@1 62.500 (57.403)\n",
      "Epoch: [34][390/390]\tTime 0.003 (0.003)\tLoss 1.2295 (1.2582)\tPrec@1 57.500 (57.008)\n",
      "EPOCH: 34 train Results: Prec@1 57.008 Loss: 1.2582\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1591 (1.1591)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3237 (1.3084)\tPrec@1 37.500 (55.110)\n",
      "EPOCH: 34 val Results: Prec@1 55.110 Loss: 1.3084\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [35][0/390]\tTime 0.004 (0.004)\tLoss 1.2261 (1.2261)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [35][78/390]\tTime 0.002 (0.004)\tLoss 1.2894 (1.2255)\tPrec@1 56.250 (58.960)\n",
      "Epoch: [35][156/390]\tTime 0.002 (0.004)\tLoss 1.4749 (1.2321)\tPrec@1 52.344 (58.181)\n",
      "Epoch: [35][234/390]\tTime 0.003 (0.003)\tLoss 1.2239 (1.2455)\tPrec@1 61.719 (57.593)\n",
      "Epoch: [35][312/390]\tTime 0.003 (0.003)\tLoss 1.3121 (1.2516)\tPrec@1 54.688 (57.311)\n",
      "Epoch: [35][390/390]\tTime 0.001 (0.003)\tLoss 1.2571 (1.2566)\tPrec@1 55.000 (57.086)\n",
      "EPOCH: 35 train Results: Prec@1 57.086 Loss: 1.2566\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1990 (1.1990)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2926 (1.3124)\tPrec@1 37.500 (54.570)\n",
      "EPOCH: 35 val Results: Prec@1 54.570 Loss: 1.3124\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [36][0/390]\tTime 0.003 (0.003)\tLoss 1.3486 (1.3486)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [36][78/390]\tTime 0.002 (0.003)\tLoss 1.2624 (1.2262)\tPrec@1 53.125 (58.465)\n",
      "Epoch: [36][156/390]\tTime 0.003 (0.004)\tLoss 1.3448 (1.2390)\tPrec@1 51.562 (58.076)\n",
      "Epoch: [36][234/390]\tTime 0.002 (0.003)\tLoss 1.2939 (1.2442)\tPrec@1 56.250 (57.563)\n",
      "Epoch: [36][312/390]\tTime 0.002 (0.003)\tLoss 1.2865 (1.2506)\tPrec@1 61.719 (57.353)\n",
      "Epoch: [36][390/390]\tTime 0.001 (0.003)\tLoss 1.3571 (1.2565)\tPrec@1 52.500 (56.976)\n",
      "EPOCH: 36 train Results: Prec@1 56.976 Loss: 1.2565\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2064 (1.2064)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3555 (1.3022)\tPrec@1 37.500 (54.070)\n",
      "EPOCH: 36 val Results: Prec@1 54.070 Loss: 1.3022\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [37][0/390]\tTime 0.002 (0.002)\tLoss 1.1116 (1.1116)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [37][78/390]\tTime 0.002 (0.003)\tLoss 1.1197 (1.2052)\tPrec@1 64.844 (59.424)\n",
      "Epoch: [37][156/390]\tTime 0.004 (0.003)\tLoss 1.1836 (1.2278)\tPrec@1 55.469 (58.370)\n",
      "Epoch: [37][234/390]\tTime 0.003 (0.003)\tLoss 1.4228 (1.2449)\tPrec@1 45.312 (57.370)\n",
      "Epoch: [37][312/390]\tTime 0.002 (0.003)\tLoss 1.2434 (1.2552)\tPrec@1 58.594 (56.884)\n",
      "Epoch: [37][390/390]\tTime 0.002 (0.003)\tLoss 1.3003 (1.2578)\tPrec@1 56.250 (56.886)\n",
      "EPOCH: 37 train Results: Prec@1 56.886 Loss: 1.2578\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2314 (1.2314)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.001 (0.002)\tLoss 1.3739 (1.3093)\tPrec@1 43.750 (54.420)\n",
      "EPOCH: 37 val Results: Prec@1 54.420 Loss: 1.3093\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [38][0/390]\tTime 0.015 (0.015)\tLoss 1.1985 (1.1985)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [38][78/390]\tTime 0.002 (0.004)\tLoss 1.2787 (1.2355)\tPrec@1 54.688 (57.981)\n",
      "Epoch: [38][156/390]\tTime 0.002 (0.004)\tLoss 1.2223 (1.2414)\tPrec@1 56.250 (57.703)\n",
      "Epoch: [38][234/390]\tTime 0.002 (0.003)\tLoss 1.2186 (1.2459)\tPrec@1 56.250 (57.367)\n",
      "Epoch: [38][312/390]\tTime 0.020 (0.003)\tLoss 1.3667 (1.2490)\tPrec@1 47.656 (57.139)\n",
      "Epoch: [38][390/390]\tTime 0.001 (0.004)\tLoss 1.2024 (1.2555)\tPrec@1 61.250 (56.928)\n",
      "EPOCH: 38 train Results: Prec@1 56.928 Loss: 1.2555\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.2325 (1.2325)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2935 (1.3104)\tPrec@1 56.250 (54.130)\n",
      "EPOCH: 38 val Results: Prec@1 54.130 Loss: 1.3104\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [39][0/390]\tTime 0.007 (0.007)\tLoss 1.2960 (1.2960)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [39][78/390]\tTime 0.002 (0.004)\tLoss 1.2939 (1.2049)\tPrec@1 55.469 (59.612)\n",
      "Epoch: [39][156/390]\tTime 0.003 (0.003)\tLoss 1.2399 (1.2251)\tPrec@1 54.688 (58.484)\n",
      "Epoch: [39][234/390]\tTime 0.002 (0.003)\tLoss 1.2925 (1.2368)\tPrec@1 53.906 (57.975)\n",
      "Epoch: [39][312/390]\tTime 0.006 (0.003)\tLoss 1.5444 (1.2426)\tPrec@1 46.094 (57.790)\n",
      "Epoch: [39][390/390]\tTime 0.001 (0.003)\tLoss 1.2783 (1.2511)\tPrec@1 50.000 (57.442)\n",
      "EPOCH: 39 train Results: Prec@1 57.442 Loss: 1.2511\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1769 (1.1769)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5843 (1.3060)\tPrec@1 43.750 (54.270)\n",
      "EPOCH: 39 val Results: Prec@1 54.270 Loss: 1.3060\n",
      "Best Prec@1: 55.110\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [40][0/390]\tTime 0.002 (0.002)\tLoss 1.2156 (1.2156)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [40][78/390]\tTime 0.002 (0.003)\tLoss 1.2453 (1.2178)\tPrec@1 56.250 (59.009)\n",
      "Epoch: [40][156/390]\tTime 0.002 (0.003)\tLoss 1.3282 (1.2225)\tPrec@1 57.812 (58.549)\n",
      "Epoch: [40][234/390]\tTime 0.002 (0.003)\tLoss 1.3875 (1.2320)\tPrec@1 53.125 (58.231)\n",
      "Epoch: [40][312/390]\tTime 0.002 (0.003)\tLoss 1.4029 (1.2435)\tPrec@1 50.781 (57.610)\n",
      "Epoch: [40][390/390]\tTime 0.002 (0.003)\tLoss 1.3277 (1.2522)\tPrec@1 56.250 (57.244)\n",
      "EPOCH: 40 train Results: Prec@1 57.244 Loss: 1.2522\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.2519 (1.2519)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5657 (1.3002)\tPrec@1 31.250 (55.130)\n",
      "EPOCH: 40 val Results: Prec@1 55.130 Loss: 1.3002\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [41][0/390]\tTime 0.005 (0.005)\tLoss 1.3309 (1.3309)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [41][78/390]\tTime 0.002 (0.003)\tLoss 1.1843 (1.2062)\tPrec@1 62.500 (59.217)\n",
      "Epoch: [41][156/390]\tTime 0.004 (0.003)\tLoss 1.3480 (1.2282)\tPrec@1 53.906 (58.370)\n",
      "Epoch: [41][234/390]\tTime 0.002 (0.003)\tLoss 1.3492 (1.2354)\tPrec@1 52.344 (58.115)\n",
      "Epoch: [41][312/390]\tTime 0.002 (0.004)\tLoss 1.2867 (1.2437)\tPrec@1 57.031 (57.713)\n",
      "Epoch: [41][390/390]\tTime 0.002 (0.003)\tLoss 1.2076 (1.2502)\tPrec@1 62.500 (57.366)\n",
      "EPOCH: 41 train Results: Prec@1 57.366 Loss: 1.2502\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1424 (1.1424)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3816 (1.3001)\tPrec@1 43.750 (54.690)\n",
      "EPOCH: 41 val Results: Prec@1 54.690 Loss: 1.3001\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [42][0/390]\tTime 0.004 (0.004)\tLoss 1.2853 (1.2853)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [42][78/390]\tTime 0.002 (0.003)\tLoss 1.2669 (1.2091)\tPrec@1 57.812 (59.227)\n",
      "Epoch: [42][156/390]\tTime 0.002 (0.003)\tLoss 1.1931 (1.2167)\tPrec@1 63.281 (58.783)\n",
      "Epoch: [42][234/390]\tTime 0.002 (0.004)\tLoss 1.2398 (1.2344)\tPrec@1 57.812 (57.995)\n",
      "Epoch: [42][312/390]\tTime 0.002 (0.004)\tLoss 1.3484 (1.2437)\tPrec@1 57.031 (57.770)\n",
      "Epoch: [42][390/390]\tTime 0.008 (0.004)\tLoss 1.1955 (1.2506)\tPrec@1 61.250 (57.356)\n",
      "EPOCH: 42 train Results: Prec@1 57.356 Loss: 1.2506\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.2000 (1.2000)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3955 (1.3004)\tPrec@1 37.500 (55.100)\n",
      "EPOCH: 42 val Results: Prec@1 55.100 Loss: 1.3004\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [43][0/390]\tTime 0.005 (0.005)\tLoss 1.2683 (1.2683)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [43][78/390]\tTime 0.005 (0.003)\tLoss 1.2526 (1.2194)\tPrec@1 54.688 (58.455)\n",
      "Epoch: [43][156/390]\tTime 0.002 (0.003)\tLoss 1.2656 (1.2307)\tPrec@1 55.469 (57.837)\n",
      "Epoch: [43][234/390]\tTime 0.002 (0.003)\tLoss 1.2134 (1.2356)\tPrec@1 59.375 (57.693)\n",
      "Epoch: [43][312/390]\tTime 0.003 (0.003)\tLoss 1.1297 (1.2459)\tPrec@1 62.500 (57.268)\n",
      "Epoch: [43][390/390]\tTime 0.002 (0.003)\tLoss 1.0839 (1.2496)\tPrec@1 71.250 (57.064)\n",
      "EPOCH: 43 train Results: Prec@1 57.064 Loss: 1.2496\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2376 (1.2376)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3304 (1.3063)\tPrec@1 37.500 (54.840)\n",
      "EPOCH: 43 val Results: Prec@1 54.840 Loss: 1.3063\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [44][0/390]\tTime 0.005 (0.005)\tLoss 1.2049 (1.2049)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [44][78/390]\tTime 0.002 (0.004)\tLoss 1.1138 (1.2117)\tPrec@1 57.812 (59.217)\n",
      "Epoch: [44][156/390]\tTime 0.004 (0.004)\tLoss 1.1252 (1.2208)\tPrec@1 59.375 (58.549)\n",
      "Epoch: [44][234/390]\tTime 0.003 (0.004)\tLoss 1.1956 (1.2349)\tPrec@1 57.031 (57.985)\n",
      "Epoch: [44][312/390]\tTime 0.002 (0.004)\tLoss 1.1947 (1.2425)\tPrec@1 60.156 (57.481)\n",
      "Epoch: [44][390/390]\tTime 0.001 (0.004)\tLoss 1.2281 (1.2479)\tPrec@1 55.000 (57.168)\n",
      "EPOCH: 44 train Results: Prec@1 57.168 Loss: 1.2479\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1712 (1.1712)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4385 (1.3010)\tPrec@1 43.750 (55.130)\n",
      "EPOCH: 44 val Results: Prec@1 55.130 Loss: 1.3010\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [45][0/390]\tTime 0.002 (0.002)\tLoss 1.2343 (1.2343)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [45][78/390]\tTime 0.003 (0.003)\tLoss 1.2803 (1.2121)\tPrec@1 57.812 (58.564)\n",
      "Epoch: [45][156/390]\tTime 0.003 (0.003)\tLoss 1.2652 (1.2182)\tPrec@1 60.938 (58.529)\n",
      "Epoch: [45][234/390]\tTime 0.010 (0.003)\tLoss 1.3072 (1.2289)\tPrec@1 58.594 (58.052)\n",
      "Epoch: [45][312/390]\tTime 0.007 (0.003)\tLoss 1.2091 (1.2379)\tPrec@1 57.031 (57.718)\n",
      "Epoch: [45][390/390]\tTime 0.001 (0.003)\tLoss 1.3963 (1.2431)\tPrec@1 51.250 (57.516)\n",
      "EPOCH: 45 train Results: Prec@1 57.516 Loss: 1.2431\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1776 (1.1776)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3150 (1.2958)\tPrec@1 37.500 (55.120)\n",
      "EPOCH: 45 val Results: Prec@1 55.120 Loss: 1.2958\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [46][0/390]\tTime 0.002 (0.002)\tLoss 1.2737 (1.2737)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [46][78/390]\tTime 0.002 (0.003)\tLoss 1.2852 (1.2190)\tPrec@1 52.344 (58.604)\n",
      "Epoch: [46][156/390]\tTime 0.002 (0.003)\tLoss 1.2893 (1.2230)\tPrec@1 50.000 (58.305)\n",
      "Epoch: [46][234/390]\tTime 0.002 (0.003)\tLoss 1.2730 (1.2330)\tPrec@1 52.344 (57.869)\n",
      "Epoch: [46][312/390]\tTime 0.002 (0.003)\tLoss 1.2577 (1.2400)\tPrec@1 56.250 (57.593)\n",
      "Epoch: [46][390/390]\tTime 0.003 (0.003)\tLoss 1.2135 (1.2479)\tPrec@1 53.750 (57.280)\n",
      "EPOCH: 46 train Results: Prec@1 57.280 Loss: 1.2479\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2283 (1.2283)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5113 (1.3019)\tPrec@1 37.500 (54.780)\n",
      "EPOCH: 46 val Results: Prec@1 54.780 Loss: 1.3019\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [47][0/390]\tTime 0.004 (0.004)\tLoss 1.4856 (1.4856)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [47][78/390]\tTime 0.003 (0.003)\tLoss 1.3421 (1.2046)\tPrec@1 57.031 (59.167)\n",
      "Epoch: [47][156/390]\tTime 0.003 (0.003)\tLoss 1.2535 (1.2215)\tPrec@1 57.812 (58.474)\n",
      "Epoch: [47][234/390]\tTime 0.002 (0.003)\tLoss 1.1153 (1.2298)\tPrec@1 61.719 (57.945)\n",
      "Epoch: [47][312/390]\tTime 0.004 (0.003)\tLoss 1.2903 (1.2326)\tPrec@1 50.000 (57.745)\n",
      "Epoch: [47][390/390]\tTime 0.002 (0.003)\tLoss 1.2556 (1.2407)\tPrec@1 55.000 (57.448)\n",
      "EPOCH: 47 train Results: Prec@1 57.448 Loss: 1.2407\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1360 (1.1360)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3283 (1.2954)\tPrec@1 43.750 (54.930)\n",
      "EPOCH: 47 val Results: Prec@1 54.930 Loss: 1.2954\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [48][0/390]\tTime 0.003 (0.003)\tLoss 1.1786 (1.1786)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [48][78/390]\tTime 0.005 (0.003)\tLoss 1.2821 (1.2217)\tPrec@1 53.125 (58.495)\n",
      "Epoch: [48][156/390]\tTime 0.002 (0.003)\tLoss 1.3176 (1.2280)\tPrec@1 54.688 (58.136)\n",
      "Epoch: [48][234/390]\tTime 0.002 (0.003)\tLoss 1.2665 (1.2318)\tPrec@1 57.031 (57.892)\n",
      "Epoch: [48][312/390]\tTime 0.003 (0.003)\tLoss 1.2216 (1.2394)\tPrec@1 63.281 (57.488)\n",
      "Epoch: [48][390/390]\tTime 0.004 (0.003)\tLoss 1.3957 (1.2458)\tPrec@1 47.500 (57.142)\n",
      "EPOCH: 48 train Results: Prec@1 57.142 Loss: 1.2458\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2483 (1.2483)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6881 (1.3058)\tPrec@1 31.250 (54.260)\n",
      "EPOCH: 48 val Results: Prec@1 54.260 Loss: 1.3058\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [49][0/390]\tTime 0.004 (0.004)\tLoss 1.1757 (1.1757)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [49][78/390]\tTime 0.003 (0.003)\tLoss 1.2363 (1.1959)\tPrec@1 59.375 (59.355)\n",
      "Epoch: [49][156/390]\tTime 0.004 (0.004)\tLoss 1.2959 (1.2184)\tPrec@1 52.344 (58.549)\n",
      "Epoch: [49][234/390]\tTime 0.002 (0.004)\tLoss 1.2653 (1.2310)\tPrec@1 60.938 (57.919)\n",
      "Epoch: [49][312/390]\tTime 0.002 (0.003)\tLoss 1.2610 (1.2371)\tPrec@1 50.000 (57.725)\n",
      "Epoch: [49][390/390]\tTime 0.001 (0.003)\tLoss 1.4012 (1.2451)\tPrec@1 51.250 (57.342)\n",
      "EPOCH: 49 train Results: Prec@1 57.342 Loss: 1.2451\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2078 (1.2078)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3580 (1.2999)\tPrec@1 31.250 (54.860)\n",
      "EPOCH: 49 val Results: Prec@1 54.860 Loss: 1.2999\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [50][0/390]\tTime 0.002 (0.002)\tLoss 1.2196 (1.2196)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [50][78/390]\tTime 0.003 (0.003)\tLoss 1.2498 (1.2052)\tPrec@1 60.156 (59.405)\n",
      "Epoch: [50][156/390]\tTime 0.003 (0.003)\tLoss 1.2259 (1.2174)\tPrec@1 52.344 (58.514)\n",
      "Epoch: [50][234/390]\tTime 0.003 (0.004)\tLoss 1.2719 (1.2284)\tPrec@1 53.125 (58.165)\n",
      "Epoch: [50][312/390]\tTime 0.004 (0.004)\tLoss 1.2362 (1.2355)\tPrec@1 53.906 (57.773)\n",
      "Epoch: [50][390/390]\tTime 0.003 (0.004)\tLoss 1.2123 (1.2443)\tPrec@1 57.500 (57.460)\n",
      "EPOCH: 50 train Results: Prec@1 57.460 Loss: 1.2443\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2172 (1.2172)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4909 (1.3046)\tPrec@1 43.750 (54.430)\n",
      "EPOCH: 50 val Results: Prec@1 54.430 Loss: 1.3046\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [51][0/390]\tTime 0.002 (0.002)\tLoss 1.2474 (1.2474)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [51][78/390]\tTime 0.009 (0.004)\tLoss 1.1722 (1.2064)\tPrec@1 63.281 (59.049)\n",
      "Epoch: [51][156/390]\tTime 0.002 (0.004)\tLoss 1.2227 (1.2199)\tPrec@1 60.156 (58.464)\n",
      "Epoch: [51][234/390]\tTime 0.002 (0.004)\tLoss 1.3278 (1.2257)\tPrec@1 51.562 (58.175)\n",
      "Epoch: [51][312/390]\tTime 0.004 (0.004)\tLoss 1.2109 (1.2322)\tPrec@1 60.156 (57.920)\n",
      "Epoch: [51][390/390]\tTime 0.002 (0.004)\tLoss 1.1640 (1.2396)\tPrec@1 66.250 (57.644)\n",
      "EPOCH: 51 train Results: Prec@1 57.644 Loss: 1.2396\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2163 (1.2163)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3444 (1.2975)\tPrec@1 43.750 (54.870)\n",
      "EPOCH: 51 val Results: Prec@1 54.870 Loss: 1.2975\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [52][0/390]\tTime 0.004 (0.004)\tLoss 1.1486 (1.1486)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [52][78/390]\tTime 0.002 (0.003)\tLoss 1.2147 (1.1983)\tPrec@1 62.500 (59.593)\n",
      "Epoch: [52][156/390]\tTime 0.003 (0.003)\tLoss 1.2564 (1.2112)\tPrec@1 52.344 (58.803)\n",
      "Epoch: [52][234/390]\tTime 0.002 (0.003)\tLoss 1.1174 (1.2240)\tPrec@1 62.500 (58.125)\n",
      "Epoch: [52][312/390]\tTime 0.003 (0.003)\tLoss 1.3444 (1.2349)\tPrec@1 56.250 (57.673)\n",
      "Epoch: [52][390/390]\tTime 0.003 (0.004)\tLoss 1.4885 (1.2432)\tPrec@1 48.750 (57.292)\n",
      "EPOCH: 52 train Results: Prec@1 57.292 Loss: 1.2432\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2205 (1.2205)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3592 (1.3127)\tPrec@1 50.000 (53.780)\n",
      "EPOCH: 52 val Results: Prec@1 53.780 Loss: 1.3127\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [53][0/390]\tTime 0.007 (0.007)\tLoss 1.2875 (1.2875)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [53][78/390]\tTime 0.004 (0.004)\tLoss 1.2073 (1.1971)\tPrec@1 64.062 (59.909)\n",
      "Epoch: [53][156/390]\tTime 0.003 (0.004)\tLoss 1.2353 (1.2065)\tPrec@1 60.938 (59.365)\n",
      "Epoch: [53][234/390]\tTime 0.003 (0.004)\tLoss 1.2369 (1.2196)\tPrec@1 59.375 (58.703)\n",
      "Epoch: [53][312/390]\tTime 0.002 (0.004)\tLoss 1.5088 (1.2275)\tPrec@1 43.750 (58.384)\n",
      "Epoch: [53][390/390]\tTime 0.008 (0.004)\tLoss 1.2529 (1.2353)\tPrec@1 62.500 (57.920)\n",
      "EPOCH: 53 train Results: Prec@1 57.920 Loss: 1.2353\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2395 (1.2395)\tPrec@1 54.688 (54.688)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4323 (1.3009)\tPrec@1 37.500 (54.130)\n",
      "EPOCH: 53 val Results: Prec@1 54.130 Loss: 1.3009\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [54][0/390]\tTime 0.004 (0.004)\tLoss 1.1302 (1.1302)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [54][78/390]\tTime 0.003 (0.003)\tLoss 1.2045 (1.2037)\tPrec@1 59.375 (59.286)\n",
      "Epoch: [54][156/390]\tTime 0.003 (0.003)\tLoss 1.1747 (1.2182)\tPrec@1 60.156 (58.290)\n",
      "Epoch: [54][234/390]\tTime 0.002 (0.003)\tLoss 1.2229 (1.2246)\tPrec@1 64.062 (58.148)\n",
      "Epoch: [54][312/390]\tTime 0.002 (0.003)\tLoss 1.2752 (1.2312)\tPrec@1 58.594 (57.820)\n",
      "Epoch: [54][390/390]\tTime 0.002 (0.003)\tLoss 1.3251 (1.2406)\tPrec@1 52.500 (57.484)\n",
      "EPOCH: 54 train Results: Prec@1 57.484 Loss: 1.2406\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1841 (1.1841)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3184 (1.3012)\tPrec@1 50.000 (54.440)\n",
      "EPOCH: 54 val Results: Prec@1 54.440 Loss: 1.3012\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [55][0/390]\tTime 0.002 (0.002)\tLoss 1.2667 (1.2667)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [55][78/390]\tTime 0.002 (0.003)\tLoss 1.2766 (1.1953)\tPrec@1 58.594 (59.998)\n",
      "Epoch: [55][156/390]\tTime 0.002 (0.003)\tLoss 1.2912 (1.2156)\tPrec@1 52.344 (58.614)\n",
      "Epoch: [55][234/390]\tTime 0.003 (0.003)\tLoss 1.2508 (1.2278)\tPrec@1 58.594 (58.042)\n",
      "Epoch: [55][312/390]\tTime 0.004 (0.003)\tLoss 1.1350 (1.2313)\tPrec@1 65.625 (57.780)\n",
      "Epoch: [55][390/390]\tTime 0.003 (0.003)\tLoss 1.2304 (1.2376)\tPrec@1 63.750 (57.426)\n",
      "EPOCH: 55 train Results: Prec@1 57.426 Loss: 1.2376\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1853 (1.1853)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4375 (1.2977)\tPrec@1 31.250 (54.980)\n",
      "EPOCH: 55 val Results: Prec@1 54.980 Loss: 1.2977\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [56][0/390]\tTime 0.005 (0.005)\tLoss 1.1342 (1.1342)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [56][78/390]\tTime 0.004 (0.003)\tLoss 1.1990 (1.2183)\tPrec@1 58.594 (58.406)\n",
      "Epoch: [56][156/390]\tTime 0.008 (0.003)\tLoss 1.1407 (1.2189)\tPrec@1 65.625 (58.390)\n",
      "Epoch: [56][234/390]\tTime 0.005 (0.003)\tLoss 1.2267 (1.2226)\tPrec@1 57.812 (58.268)\n",
      "Epoch: [56][312/390]\tTime 0.004 (0.003)\tLoss 1.1483 (1.2299)\tPrec@1 60.156 (58.042)\n",
      "Epoch: [56][390/390]\tTime 0.001 (0.003)\tLoss 1.3605 (1.2357)\tPrec@1 55.000 (57.782)\n",
      "EPOCH: 56 train Results: Prec@1 57.782 Loss: 1.2357\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1291 (1.1291)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3535 (1.3008)\tPrec@1 37.500 (54.280)\n",
      "EPOCH: 56 val Results: Prec@1 54.280 Loss: 1.3008\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [57][0/390]\tTime 0.002 (0.002)\tLoss 1.2079 (1.2079)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [57][78/390]\tTime 0.003 (0.003)\tLoss 1.2382 (1.1907)\tPrec@1 54.688 (59.365)\n",
      "Epoch: [57][156/390]\tTime 0.002 (0.003)\tLoss 1.3007 (1.2082)\tPrec@1 57.812 (58.758)\n",
      "Epoch: [57][234/390]\tTime 0.007 (0.003)\tLoss 1.3300 (1.2183)\tPrec@1 54.688 (58.218)\n",
      "Epoch: [57][312/390]\tTime 0.004 (0.003)\tLoss 1.2608 (1.2250)\tPrec@1 57.031 (57.897)\n",
      "Epoch: [57][390/390]\tTime 0.005 (0.003)\tLoss 1.2431 (1.2366)\tPrec@1 53.750 (57.584)\n",
      "EPOCH: 57 train Results: Prec@1 57.584 Loss: 1.2366\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2114 (1.2114)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4071 (1.3064)\tPrec@1 12.500 (54.410)\n",
      "EPOCH: 57 val Results: Prec@1 54.410 Loss: 1.3064\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [58][0/390]\tTime 0.002 (0.002)\tLoss 1.1656 (1.1656)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [58][78/390]\tTime 0.004 (0.003)\tLoss 1.2329 (1.1952)\tPrec@1 56.250 (59.207)\n",
      "Epoch: [58][156/390]\tTime 0.002 (0.003)\tLoss 1.2349 (1.2108)\tPrec@1 56.250 (58.474)\n",
      "Epoch: [58][234/390]\tTime 0.002 (0.003)\tLoss 1.2213 (1.2207)\tPrec@1 61.719 (58.195)\n",
      "Epoch: [58][312/390]\tTime 0.002 (0.003)\tLoss 1.2051 (1.2266)\tPrec@1 60.156 (58.022)\n",
      "Epoch: [58][390/390]\tTime 0.003 (0.003)\tLoss 1.2753 (1.2332)\tPrec@1 57.500 (57.656)\n",
      "EPOCH: 58 train Results: Prec@1 57.656 Loss: 1.2332\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2225 (1.2225)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4099 (1.2922)\tPrec@1 25.000 (54.770)\n",
      "EPOCH: 58 val Results: Prec@1 54.770 Loss: 1.2922\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [59][0/390]\tTime 0.002 (0.002)\tLoss 1.2743 (1.2743)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [59][78/390]\tTime 0.002 (0.003)\tLoss 1.1848 (1.1966)\tPrec@1 53.125 (58.979)\n",
      "Epoch: [59][156/390]\tTime 0.002 (0.003)\tLoss 1.3003 (1.2018)\tPrec@1 52.344 (58.947)\n",
      "Epoch: [59][234/390]\tTime 0.007 (0.003)\tLoss 1.2039 (1.2191)\tPrec@1 57.812 (58.301)\n",
      "Epoch: [59][312/390]\tTime 0.004 (0.003)\tLoss 1.2583 (1.2265)\tPrec@1 58.594 (58.045)\n",
      "Epoch: [59][390/390]\tTime 0.001 (0.003)\tLoss 1.1802 (1.2349)\tPrec@1 62.500 (57.654)\n",
      "EPOCH: 59 train Results: Prec@1 57.654 Loss: 1.2349\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2302 (1.2302)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2846 (1.3040)\tPrec@1 37.500 (54.510)\n",
      "EPOCH: 59 val Results: Prec@1 54.510 Loss: 1.3040\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [60][0/390]\tTime 0.002 (0.002)\tLoss 1.1739 (1.1739)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [60][78/390]\tTime 0.002 (0.003)\tLoss 1.2054 (1.2094)\tPrec@1 57.812 (59.078)\n",
      "Epoch: [60][156/390]\tTime 0.007 (0.003)\tLoss 1.1671 (1.2186)\tPrec@1 60.938 (58.698)\n",
      "Epoch: [60][234/390]\tTime 0.002 (0.003)\tLoss 1.2821 (1.2251)\tPrec@1 57.031 (58.188)\n",
      "Epoch: [60][312/390]\tTime 0.003 (0.003)\tLoss 1.1856 (1.2323)\tPrec@1 60.156 (57.763)\n",
      "Epoch: [60][390/390]\tTime 0.002 (0.004)\tLoss 1.3013 (1.2364)\tPrec@1 55.000 (57.546)\n",
      "EPOCH: 60 train Results: Prec@1 57.546 Loss: 1.2364\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1743 (1.1743)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.2830 (1.3027)\tPrec@1 43.750 (54.820)\n",
      "EPOCH: 60 val Results: Prec@1 54.820 Loss: 1.3027\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [61][0/390]\tTime 0.005 (0.005)\tLoss 1.0819 (1.0819)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [61][78/390]\tTime 0.004 (0.004)\tLoss 1.2872 (1.2056)\tPrec@1 56.250 (58.841)\n",
      "Epoch: [61][156/390]\tTime 0.002 (0.004)\tLoss 1.3365 (1.2100)\tPrec@1 50.781 (58.828)\n",
      "Epoch: [61][234/390]\tTime 0.002 (0.003)\tLoss 1.1244 (1.2187)\tPrec@1 60.938 (58.305)\n",
      "Epoch: [61][312/390]\tTime 0.004 (0.003)\tLoss 1.2611 (1.2276)\tPrec@1 59.375 (58.007)\n",
      "Epoch: [61][390/390]\tTime 0.001 (0.003)\tLoss 1.2613 (1.2341)\tPrec@1 56.250 (57.694)\n",
      "EPOCH: 61 train Results: Prec@1 57.694 Loss: 1.2341\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1703 (1.1703)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3856 (1.2952)\tPrec@1 31.250 (54.990)\n",
      "EPOCH: 61 val Results: Prec@1 54.990 Loss: 1.2952\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [62][0/390]\tTime 0.002 (0.002)\tLoss 1.1888 (1.1888)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [62][78/390]\tTime 0.002 (0.003)\tLoss 1.2240 (1.1945)\tPrec@1 58.594 (59.652)\n",
      "Epoch: [62][156/390]\tTime 0.005 (0.003)\tLoss 1.1187 (1.2063)\tPrec@1 64.062 (58.813)\n",
      "Epoch: [62][234/390]\tTime 0.003 (0.003)\tLoss 1.3004 (1.2168)\tPrec@1 57.031 (58.348)\n",
      "Epoch: [62][312/390]\tTime 0.006 (0.003)\tLoss 1.3652 (1.2222)\tPrec@1 52.344 (57.995)\n",
      "Epoch: [62][390/390]\tTime 0.002 (0.003)\tLoss 1.0522 (1.2297)\tPrec@1 68.750 (57.748)\n",
      "EPOCH: 62 train Results: Prec@1 57.748 Loss: 1.2297\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1856 (1.1856)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4506 (1.2911)\tPrec@1 37.500 (54.940)\n",
      "EPOCH: 62 val Results: Prec@1 54.940 Loss: 1.2911\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [63][0/390]\tTime 0.005 (0.005)\tLoss 1.1443 (1.1443)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [63][78/390]\tTime 0.002 (0.003)\tLoss 1.2842 (1.1943)\tPrec@1 53.906 (59.484)\n",
      "Epoch: [63][156/390]\tTime 0.002 (0.003)\tLoss 1.2345 (1.2091)\tPrec@1 55.469 (58.823)\n",
      "Epoch: [63][234/390]\tTime 0.003 (0.003)\tLoss 1.2657 (1.2198)\tPrec@1 56.250 (58.308)\n",
      "Epoch: [63][312/390]\tTime 0.003 (0.003)\tLoss 1.3789 (1.2266)\tPrec@1 53.906 (58.187)\n",
      "Epoch: [63][390/390]\tTime 0.003 (0.003)\tLoss 1.2939 (1.2308)\tPrec@1 57.500 (58.040)\n",
      "EPOCH: 63 train Results: Prec@1 58.040 Loss: 1.2308\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2087 (1.2087)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3659 (1.3068)\tPrec@1 37.500 (54.270)\n",
      "EPOCH: 63 val Results: Prec@1 54.270 Loss: 1.3068\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [64][0/390]\tTime 0.003 (0.003)\tLoss 1.1599 (1.1599)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [64][78/390]\tTime 0.002 (0.004)\tLoss 1.1325 (1.1753)\tPrec@1 57.812 (59.662)\n",
      "Epoch: [64][156/390]\tTime 0.004 (0.003)\tLoss 1.3963 (1.1988)\tPrec@1 53.125 (59.136)\n",
      "Epoch: [64][234/390]\tTime 0.002 (0.004)\tLoss 1.3711 (1.2129)\tPrec@1 54.688 (58.477)\n",
      "Epoch: [64][312/390]\tTime 0.003 (0.005)\tLoss 1.3275 (1.2240)\tPrec@1 49.219 (57.965)\n",
      "Epoch: [64][390/390]\tTime 0.001 (0.005)\tLoss 1.3455 (1.2319)\tPrec@1 50.000 (57.632)\n",
      "EPOCH: 64 train Results: Prec@1 57.632 Loss: 1.2319\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1967 (1.1967)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2339 (1.2975)\tPrec@1 43.750 (54.300)\n",
      "EPOCH: 64 val Results: Prec@1 54.300 Loss: 1.2975\n",
      "Best Prec@1: 55.130\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [65][0/390]\tTime 0.005 (0.005)\tLoss 1.2075 (1.2075)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [65][78/390]\tTime 0.002 (0.004)\tLoss 1.2381 (1.1884)\tPrec@1 57.031 (59.642)\n",
      "Epoch: [65][156/390]\tTime 0.003 (0.004)\tLoss 1.1908 (1.1971)\tPrec@1 62.500 (59.549)\n",
      "Epoch: [65][234/390]\tTime 0.006 (0.004)\tLoss 1.2720 (1.2125)\tPrec@1 55.469 (58.753)\n",
      "Epoch: [65][312/390]\tTime 0.003 (0.003)\tLoss 1.2390 (1.2227)\tPrec@1 53.125 (58.284)\n",
      "Epoch: [65][390/390]\tTime 0.004 (0.003)\tLoss 1.1067 (1.2285)\tPrec@1 65.000 (57.984)\n",
      "EPOCH: 65 train Results: Prec@1 57.984 Loss: 1.2285\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2234 (1.2234)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1642 (1.2930)\tPrec@1 50.000 (55.220)\n",
      "EPOCH: 65 val Results: Prec@1 55.220 Loss: 1.2930\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [66][0/390]\tTime 0.004 (0.004)\tLoss 1.1706 (1.1706)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [66][78/390]\tTime 0.003 (0.003)\tLoss 1.2490 (1.1937)\tPrec@1 53.906 (59.573)\n",
      "Epoch: [66][156/390]\tTime 0.004 (0.003)\tLoss 1.1865 (1.2042)\tPrec@1 61.719 (58.962)\n",
      "Epoch: [66][234/390]\tTime 0.002 (0.003)\tLoss 1.3049 (1.2135)\tPrec@1 53.906 (58.477)\n",
      "Epoch: [66][312/390]\tTime 0.005 (0.003)\tLoss 1.3521 (1.2245)\tPrec@1 56.250 (57.852)\n",
      "Epoch: [66][390/390]\tTime 0.001 (0.003)\tLoss 1.1626 (1.2299)\tPrec@1 66.250 (57.698)\n",
      "EPOCH: 66 train Results: Prec@1 57.698 Loss: 1.2299\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.2218 (1.2218)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4992 (1.2954)\tPrec@1 31.250 (54.670)\n",
      "EPOCH: 66 val Results: Prec@1 54.670 Loss: 1.2954\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [67][0/390]\tTime 0.002 (0.002)\tLoss 1.2580 (1.2580)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [67][78/390]\tTime 0.002 (0.003)\tLoss 1.2986 (1.1943)\tPrec@1 53.906 (58.970)\n",
      "Epoch: [67][156/390]\tTime 0.003 (0.003)\tLoss 1.4069 (1.2024)\tPrec@1 51.562 (58.733)\n",
      "Epoch: [67][234/390]\tTime 0.003 (0.004)\tLoss 1.2333 (1.2163)\tPrec@1 60.156 (58.341)\n",
      "Epoch: [67][312/390]\tTime 0.008 (0.004)\tLoss 1.2516 (1.2223)\tPrec@1 54.688 (58.164)\n",
      "Epoch: [67][390/390]\tTime 0.001 (0.003)\tLoss 1.3073 (1.2292)\tPrec@1 53.750 (57.882)\n",
      "EPOCH: 67 train Results: Prec@1 57.882 Loss: 1.2292\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.2240 (1.2240)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2378 (1.2961)\tPrec@1 50.000 (54.720)\n",
      "EPOCH: 67 val Results: Prec@1 54.720 Loss: 1.2961\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [68][0/390]\tTime 0.002 (0.002)\tLoss 1.1511 (1.1511)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [68][78/390]\tTime 0.025 (0.004)\tLoss 1.1591 (1.1807)\tPrec@1 58.594 (60.047)\n",
      "Epoch: [68][156/390]\tTime 0.002 (0.004)\tLoss 1.3251 (1.2069)\tPrec@1 53.906 (58.738)\n",
      "Epoch: [68][234/390]\tTime 0.002 (0.004)\tLoss 1.3166 (1.2175)\tPrec@1 53.906 (58.118)\n",
      "Epoch: [68][312/390]\tTime 0.010 (0.004)\tLoss 1.4600 (1.2268)\tPrec@1 49.219 (57.733)\n",
      "Epoch: [68][390/390]\tTime 0.001 (0.004)\tLoss 1.4884 (1.2314)\tPrec@1 43.750 (57.668)\n",
      "EPOCH: 68 train Results: Prec@1 57.668 Loss: 1.2314\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1416 (1.1416)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1554 (1.2914)\tPrec@1 50.000 (54.560)\n",
      "EPOCH: 68 val Results: Prec@1 54.560 Loss: 1.2914\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [69][0/390]\tTime 0.002 (0.002)\tLoss 1.1648 (1.1648)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [69][78/390]\tTime 0.003 (0.003)\tLoss 1.1946 (1.1756)\tPrec@1 56.250 (60.087)\n",
      "Epoch: [69][156/390]\tTime 0.007 (0.003)\tLoss 1.2722 (1.1940)\tPrec@1 56.250 (59.330)\n",
      "Epoch: [69][234/390]\tTime 0.002 (0.003)\tLoss 1.0600 (1.2078)\tPrec@1 64.844 (58.797)\n",
      "Epoch: [69][312/390]\tTime 0.002 (0.003)\tLoss 1.2904 (1.2172)\tPrec@1 57.031 (58.142)\n",
      "Epoch: [69][390/390]\tTime 0.002 (0.003)\tLoss 1.2346 (1.2252)\tPrec@1 57.500 (57.824)\n",
      "EPOCH: 69 train Results: Prec@1 57.824 Loss: 1.2252\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2064 (1.2064)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4380 (1.2938)\tPrec@1 31.250 (54.600)\n",
      "EPOCH: 69 val Results: Prec@1 54.600 Loss: 1.2938\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [70][0/390]\tTime 0.004 (0.004)\tLoss 1.0495 (1.0495)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [70][78/390]\tTime 0.002 (0.003)\tLoss 1.3439 (1.1864)\tPrec@1 48.438 (59.385)\n",
      "Epoch: [70][156/390]\tTime 0.002 (0.003)\tLoss 1.2567 (1.2036)\tPrec@1 57.031 (59.017)\n",
      "Epoch: [70][234/390]\tTime 0.002 (0.003)\tLoss 1.1594 (1.2101)\tPrec@1 59.375 (58.700)\n",
      "Epoch: [70][312/390]\tTime 0.005 (0.003)\tLoss 1.1276 (1.2233)\tPrec@1 67.188 (58.007)\n",
      "Epoch: [70][390/390]\tTime 0.004 (0.003)\tLoss 1.2858 (1.2278)\tPrec@1 53.750 (57.738)\n",
      "EPOCH: 70 train Results: Prec@1 57.738 Loss: 1.2278\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.2214 (1.2214)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2905 (1.2918)\tPrec@1 43.750 (54.690)\n",
      "EPOCH: 70 val Results: Prec@1 54.690 Loss: 1.2918\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [71][0/390]\tTime 0.004 (0.004)\tLoss 1.1782 (1.1782)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [71][78/390]\tTime 0.003 (0.003)\tLoss 1.1873 (1.1858)\tPrec@1 62.500 (59.840)\n",
      "Epoch: [71][156/390]\tTime 0.004 (0.004)\tLoss 1.2378 (1.2019)\tPrec@1 55.469 (59.151)\n",
      "Epoch: [71][234/390]\tTime 0.003 (0.004)\tLoss 1.0527 (1.2145)\tPrec@1 64.844 (58.547)\n",
      "Epoch: [71][312/390]\tTime 0.007 (0.004)\tLoss 1.2451 (1.2195)\tPrec@1 55.469 (58.234)\n",
      "Epoch: [71][390/390]\tTime 0.003 (0.004)\tLoss 1.1555 (1.2288)\tPrec@1 60.000 (57.770)\n",
      "EPOCH: 71 train Results: Prec@1 57.770 Loss: 1.2288\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2302 (1.2302)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2502 (1.3033)\tPrec@1 43.750 (54.550)\n",
      "EPOCH: 71 val Results: Prec@1 54.550 Loss: 1.3033\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [72][0/390]\tTime 0.002 (0.002)\tLoss 1.1687 (1.1687)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [72][78/390]\tTime 0.002 (0.004)\tLoss 1.3478 (1.1881)\tPrec@1 50.781 (60.097)\n",
      "Epoch: [72][156/390]\tTime 0.002 (0.004)\tLoss 1.2222 (1.1955)\tPrec@1 55.469 (59.280)\n",
      "Epoch: [72][234/390]\tTime 0.002 (0.004)\tLoss 1.3382 (1.2093)\tPrec@1 53.125 (58.531)\n",
      "Epoch: [72][312/390]\tTime 0.011 (0.005)\tLoss 1.2511 (1.2189)\tPrec@1 50.781 (58.194)\n",
      "Epoch: [72][390/390]\tTime 0.001 (0.005)\tLoss 1.0940 (1.2238)\tPrec@1 61.250 (57.996)\n",
      "EPOCH: 72 train Results: Prec@1 57.996 Loss: 1.2238\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1702 (1.1702)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2689 (1.2881)\tPrec@1 50.000 (54.780)\n",
      "EPOCH: 72 val Results: Prec@1 54.780 Loss: 1.2881\n",
      "Best Prec@1: 55.220\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [73][0/390]\tTime 0.003 (0.003)\tLoss 1.1047 (1.1047)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [73][78/390]\tTime 0.002 (0.004)\tLoss 1.1179 (1.1967)\tPrec@1 58.594 (59.434)\n",
      "Epoch: [73][156/390]\tTime 0.007 (0.003)\tLoss 1.2971 (1.2023)\tPrec@1 55.469 (58.544)\n",
      "Epoch: [73][234/390]\tTime 0.003 (0.004)\tLoss 1.2115 (1.2095)\tPrec@1 55.469 (58.404)\n",
      "Epoch: [73][312/390]\tTime 0.003 (0.004)\tLoss 1.1030 (1.2215)\tPrec@1 63.281 (57.945)\n",
      "Epoch: [73][390/390]\tTime 0.024 (0.004)\tLoss 1.1738 (1.2262)\tPrec@1 62.500 (57.774)\n",
      "EPOCH: 73 train Results: Prec@1 57.774 Loss: 1.2262\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1664 (1.1664)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3387 (1.2839)\tPrec@1 31.250 (55.540)\n",
      "EPOCH: 73 val Results: Prec@1 55.540 Loss: 1.2839\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [74][0/390]\tTime 0.012 (0.012)\tLoss 1.3350 (1.3350)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [74][78/390]\tTime 0.008 (0.005)\tLoss 1.1513 (1.1892)\tPrec@1 66.406 (59.909)\n",
      "Epoch: [74][156/390]\tTime 0.006 (0.005)\tLoss 1.2174 (1.2033)\tPrec@1 53.125 (59.310)\n",
      "Epoch: [74][234/390]\tTime 0.002 (0.005)\tLoss 1.2450 (1.2099)\tPrec@1 55.469 (58.880)\n",
      "Epoch: [74][312/390]\tTime 0.003 (0.004)\tLoss 1.1970 (1.2148)\tPrec@1 60.156 (58.494)\n",
      "Epoch: [74][390/390]\tTime 0.001 (0.004)\tLoss 1.2838 (1.2216)\tPrec@1 55.000 (58.164)\n",
      "EPOCH: 74 train Results: Prec@1 58.164 Loss: 1.2216\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2021 (1.2021)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6074 (1.2964)\tPrec@1 25.000 (54.790)\n",
      "EPOCH: 74 val Results: Prec@1 54.790 Loss: 1.2964\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [75][0/390]\tTime 0.002 (0.002)\tLoss 1.2052 (1.2052)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [75][78/390]\tTime 0.004 (0.003)\tLoss 1.3277 (1.1762)\tPrec@1 57.031 (59.474)\n",
      "Epoch: [75][156/390]\tTime 0.008 (0.003)\tLoss 0.9793 (1.2011)\tPrec@1 70.312 (58.624)\n",
      "Epoch: [75][234/390]\tTime 0.004 (0.003)\tLoss 1.2931 (1.2096)\tPrec@1 57.031 (58.481)\n",
      "Epoch: [75][312/390]\tTime 0.005 (0.003)\tLoss 1.2736 (1.2140)\tPrec@1 53.125 (58.347)\n",
      "Epoch: [75][390/390]\tTime 0.007 (0.003)\tLoss 1.2670 (1.2227)\tPrec@1 52.500 (58.048)\n",
      "EPOCH: 75 train Results: Prec@1 58.048 Loss: 1.2227\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2122 (1.2122)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4500 (1.3020)\tPrec@1 43.750 (54.440)\n",
      "EPOCH: 75 val Results: Prec@1 54.440 Loss: 1.3020\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [76][0/390]\tTime 0.004 (0.004)\tLoss 1.1677 (1.1677)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [76][78/390]\tTime 0.002 (0.003)\tLoss 1.3865 (1.1932)\tPrec@1 53.906 (59.563)\n",
      "Epoch: [76][156/390]\tTime 0.003 (0.003)\tLoss 1.0837 (1.2046)\tPrec@1 63.281 (58.877)\n",
      "Epoch: [76][234/390]\tTime 0.002 (0.003)\tLoss 1.1929 (1.2124)\tPrec@1 57.812 (58.394)\n",
      "Epoch: [76][312/390]\tTime 0.002 (0.003)\tLoss 1.0945 (1.2157)\tPrec@1 64.062 (58.334)\n",
      "Epoch: [76][390/390]\tTime 0.003 (0.003)\tLoss 1.0114 (1.2206)\tPrec@1 70.000 (58.116)\n",
      "EPOCH: 76 train Results: Prec@1 58.116 Loss: 1.2206\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2375 (1.2375)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4898 (1.2979)\tPrec@1 31.250 (54.750)\n",
      "EPOCH: 76 val Results: Prec@1 54.750 Loss: 1.2979\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [77][0/390]\tTime 0.002 (0.002)\tLoss 1.1870 (1.1870)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [77][78/390]\tTime 0.002 (0.004)\tLoss 1.2368 (1.1859)\tPrec@1 57.031 (59.494)\n",
      "Epoch: [77][156/390]\tTime 0.011 (0.004)\tLoss 1.2279 (1.2013)\tPrec@1 57.812 (58.808)\n",
      "Epoch: [77][234/390]\tTime 0.005 (0.005)\tLoss 1.1296 (1.2109)\tPrec@1 60.938 (58.461)\n",
      "Epoch: [77][312/390]\tTime 0.002 (0.005)\tLoss 1.2171 (1.2190)\tPrec@1 57.031 (58.122)\n",
      "Epoch: [77][390/390]\tTime 0.003 (0.005)\tLoss 1.3326 (1.2261)\tPrec@1 57.500 (57.864)\n",
      "EPOCH: 77 train Results: Prec@1 57.864 Loss: 1.2261\n",
      "Test: [0/78]\tTime 0.019 (0.019)\tLoss 1.1571 (1.1571)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4196 (1.2940)\tPrec@1 43.750 (54.440)\n",
      "EPOCH: 77 val Results: Prec@1 54.440 Loss: 1.2940\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [78][0/390]\tTime 0.006 (0.006)\tLoss 1.1271 (1.1271)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [78][78/390]\tTime 0.008 (0.006)\tLoss 1.2221 (1.1773)\tPrec@1 55.469 (60.364)\n",
      "Epoch: [78][156/390]\tTime 0.004 (0.005)\tLoss 1.2387 (1.1961)\tPrec@1 60.156 (59.410)\n",
      "Epoch: [78][234/390]\tTime 0.002 (0.004)\tLoss 1.2183 (1.2099)\tPrec@1 60.156 (58.816)\n",
      "Epoch: [78][312/390]\tTime 0.003 (0.004)\tLoss 1.2467 (1.2202)\tPrec@1 53.125 (58.122)\n",
      "Epoch: [78][390/390]\tTime 0.002 (0.004)\tLoss 1.2231 (1.2247)\tPrec@1 57.500 (57.890)\n",
      "EPOCH: 78 train Results: Prec@1 57.890 Loss: 1.2247\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1574 (1.1574)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2957 (1.2783)\tPrec@1 50.000 (55.330)\n",
      "EPOCH: 78 val Results: Prec@1 55.330 Loss: 1.2783\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [79][0/390]\tTime 0.004 (0.004)\tLoss 1.1086 (1.1086)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [79][78/390]\tTime 0.005 (0.003)\tLoss 1.0988 (1.1570)\tPrec@1 64.844 (61.402)\n",
      "Epoch: [79][156/390]\tTime 0.002 (0.003)\tLoss 1.1651 (1.1828)\tPrec@1 61.719 (60.231)\n",
      "Epoch: [79][234/390]\tTime 0.002 (0.003)\tLoss 1.2097 (1.1970)\tPrec@1 63.281 (59.495)\n",
      "Epoch: [79][312/390]\tTime 0.003 (0.003)\tLoss 1.2736 (1.2104)\tPrec@1 57.031 (58.893)\n",
      "Epoch: [79][390/390]\tTime 0.003 (0.003)\tLoss 1.2353 (1.2213)\tPrec@1 60.000 (58.220)\n",
      "EPOCH: 79 train Results: Prec@1 58.220 Loss: 1.2213\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1906 (1.1906)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4219 (1.2851)\tPrec@1 31.250 (55.030)\n",
      "EPOCH: 79 val Results: Prec@1 55.030 Loss: 1.2851\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [80][0/390]\tTime 0.004 (0.004)\tLoss 1.1269 (1.1269)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [80][78/390]\tTime 0.002 (0.003)\tLoss 1.1436 (1.1810)\tPrec@1 60.938 (60.018)\n",
      "Epoch: [80][156/390]\tTime 0.002 (0.003)\tLoss 1.1525 (1.1984)\tPrec@1 60.156 (58.952)\n",
      "Epoch: [80][234/390]\tTime 0.003 (0.003)\tLoss 1.1254 (1.2095)\tPrec@1 60.156 (58.703)\n",
      "Epoch: [80][312/390]\tTime 0.005 (0.004)\tLoss 1.3319 (1.2173)\tPrec@1 52.344 (58.432)\n",
      "Epoch: [80][390/390]\tTime 0.001 (0.004)\tLoss 1.2650 (1.2223)\tPrec@1 57.500 (58.222)\n",
      "EPOCH: 80 train Results: Prec@1 58.222 Loss: 1.2223\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2100 (1.2100)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4843 (1.2966)\tPrec@1 37.500 (54.600)\n",
      "EPOCH: 80 val Results: Prec@1 54.600 Loss: 1.2966\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [81][0/390]\tTime 0.003 (0.003)\tLoss 1.1159 (1.1159)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [81][78/390]\tTime 0.005 (0.004)\tLoss 1.1413 (1.1767)\tPrec@1 60.156 (60.127)\n",
      "Epoch: [81][156/390]\tTime 0.002 (0.004)\tLoss 1.0764 (1.1920)\tPrec@1 66.406 (59.733)\n",
      "Epoch: [81][234/390]\tTime 0.006 (0.004)\tLoss 1.1674 (1.1997)\tPrec@1 63.281 (59.302)\n",
      "Epoch: [81][312/390]\tTime 0.002 (0.004)\tLoss 1.3966 (1.2135)\tPrec@1 48.438 (58.579)\n",
      "Epoch: [81][390/390]\tTime 0.003 (0.004)\tLoss 1.1650 (1.2210)\tPrec@1 61.250 (58.150)\n",
      "EPOCH: 81 train Results: Prec@1 58.150 Loss: 1.2210\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1670 (1.1670)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4011 (1.2910)\tPrec@1 25.000 (54.880)\n",
      "EPOCH: 81 val Results: Prec@1 54.880 Loss: 1.2910\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [82][0/390]\tTime 0.005 (0.005)\tLoss 1.2545 (1.2545)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [82][78/390]\tTime 0.003 (0.003)\tLoss 1.1658 (1.1773)\tPrec@1 64.844 (60.443)\n",
      "Epoch: [82][156/390]\tTime 0.003 (0.003)\tLoss 1.3088 (1.1890)\tPrec@1 57.031 (59.559)\n",
      "Epoch: [82][234/390]\tTime 0.004 (0.003)\tLoss 1.2613 (1.2013)\tPrec@1 55.469 (58.893)\n",
      "Epoch: [82][312/390]\tTime 0.003 (0.003)\tLoss 1.1494 (1.2077)\tPrec@1 62.500 (58.751)\n",
      "Epoch: [82][390/390]\tTime 0.001 (0.003)\tLoss 1.1570 (1.2202)\tPrec@1 58.750 (58.260)\n",
      "EPOCH: 82 train Results: Prec@1 58.260 Loss: 1.2202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1984 (1.1984)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5185 (1.2835)\tPrec@1 31.250 (54.630)\n",
      "EPOCH: 82 val Results: Prec@1 54.630 Loss: 1.2835\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [83][0/390]\tTime 0.003 (0.003)\tLoss 1.1720 (1.1720)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [83][78/390]\tTime 0.002 (0.004)\tLoss 1.3529 (1.1829)\tPrec@1 46.094 (60.216)\n",
      "Epoch: [83][156/390]\tTime 0.002 (0.003)\tLoss 1.3733 (1.1985)\tPrec@1 48.438 (59.171)\n",
      "Epoch: [83][234/390]\tTime 0.002 (0.003)\tLoss 1.1533 (1.2067)\tPrec@1 61.719 (58.607)\n",
      "Epoch: [83][312/390]\tTime 0.002 (0.003)\tLoss 1.2086 (1.2136)\tPrec@1 58.594 (58.382)\n",
      "Epoch: [83][390/390]\tTime 0.002 (0.004)\tLoss 1.3812 (1.2198)\tPrec@1 47.500 (58.044)\n",
      "EPOCH: 83 train Results: Prec@1 58.044 Loss: 1.2198\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1565 (1.1565)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6991 (1.2897)\tPrec@1 31.250 (55.010)\n",
      "EPOCH: 83 val Results: Prec@1 55.010 Loss: 1.2897\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [84][0/390]\tTime 0.002 (0.002)\tLoss 1.0178 (1.0178)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [84][78/390]\tTime 0.011 (0.005)\tLoss 1.0432 (1.1856)\tPrec@1 71.094 (60.403)\n",
      "Epoch: [84][156/390]\tTime 0.004 (0.007)\tLoss 1.2356 (1.2021)\tPrec@1 58.594 (59.261)\n",
      "Epoch: [84][234/390]\tTime 0.012 (0.006)\tLoss 1.1898 (1.2134)\tPrec@1 59.375 (58.507)\n",
      "Epoch: [84][312/390]\tTime 0.006 (0.006)\tLoss 1.1620 (1.2166)\tPrec@1 60.938 (58.419)\n",
      "Epoch: [84][390/390]\tTime 0.001 (0.006)\tLoss 1.4053 (1.2185)\tPrec@1 51.250 (58.360)\n",
      "EPOCH: 84 train Results: Prec@1 58.360 Loss: 1.2185\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1384 (1.1384)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3985 (1.2960)\tPrec@1 31.250 (55.000)\n",
      "EPOCH: 84 val Results: Prec@1 55.000 Loss: 1.2960\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [85][0/390]\tTime 0.005 (0.005)\tLoss 1.1238 (1.1238)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [85][78/390]\tTime 0.002 (0.003)\tLoss 1.0400 (1.1745)\tPrec@1 68.750 (60.038)\n",
      "Epoch: [85][156/390]\tTime 0.004 (0.003)\tLoss 1.0419 (1.1937)\tPrec@1 64.062 (59.012)\n",
      "Epoch: [85][234/390]\tTime 0.002 (0.003)\tLoss 1.3643 (1.2030)\tPrec@1 50.000 (58.620)\n",
      "Epoch: [85][312/390]\tTime 0.003 (0.003)\tLoss 1.1963 (1.2140)\tPrec@1 59.375 (58.232)\n",
      "Epoch: [85][390/390]\tTime 0.001 (0.003)\tLoss 1.3064 (1.2215)\tPrec@1 60.000 (57.906)\n",
      "EPOCH: 85 train Results: Prec@1 57.906 Loss: 1.2215\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1801 (1.1801)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6422 (1.2856)\tPrec@1 18.750 (55.060)\n",
      "EPOCH: 85 val Results: Prec@1 55.060 Loss: 1.2856\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [86][0/390]\tTime 0.004 (0.004)\tLoss 1.1246 (1.1246)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [86][78/390]\tTime 0.004 (0.003)\tLoss 1.1671 (1.1889)\tPrec@1 63.281 (60.107)\n",
      "Epoch: [86][156/390]\tTime 0.003 (0.003)\tLoss 1.1395 (1.1938)\tPrec@1 63.281 (59.465)\n",
      "Epoch: [86][234/390]\tTime 0.002 (0.003)\tLoss 1.1793 (1.2037)\tPrec@1 57.031 (58.823)\n",
      "Epoch: [86][312/390]\tTime 0.002 (0.003)\tLoss 1.3778 (1.2123)\tPrec@1 50.000 (58.362)\n",
      "Epoch: [86][390/390]\tTime 0.001 (0.003)\tLoss 1.3843 (1.2217)\tPrec@1 52.500 (57.844)\n",
      "EPOCH: 86 train Results: Prec@1 57.844 Loss: 1.2217\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2027 (1.2027)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4265 (1.2825)\tPrec@1 37.500 (54.800)\n",
      "EPOCH: 86 val Results: Prec@1 54.800 Loss: 1.2825\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [87][0/390]\tTime 0.002 (0.002)\tLoss 1.0880 (1.0880)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [87][78/390]\tTime 0.003 (0.003)\tLoss 1.0650 (1.1710)\tPrec@1 66.406 (60.779)\n",
      "Epoch: [87][156/390]\tTime 0.004 (0.004)\tLoss 1.1953 (1.1949)\tPrec@1 64.844 (59.400)\n",
      "Epoch: [87][234/390]\tTime 0.002 (0.004)\tLoss 1.1286 (1.2071)\tPrec@1 62.500 (58.703)\n",
      "Epoch: [87][312/390]\tTime 0.004 (0.003)\tLoss 1.2432 (1.2156)\tPrec@1 56.250 (58.449)\n",
      "Epoch: [87][390/390]\tTime 0.003 (0.003)\tLoss 1.3201 (1.2208)\tPrec@1 51.250 (58.134)\n",
      "EPOCH: 87 train Results: Prec@1 58.134 Loss: 1.2208\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1682 (1.1682)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3881 (1.2864)\tPrec@1 50.000 (54.980)\n",
      "EPOCH: 87 val Results: Prec@1 54.980 Loss: 1.2864\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [88][0/390]\tTime 0.002 (0.002)\tLoss 1.1467 (1.1467)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [88][78/390]\tTime 0.005 (0.003)\tLoss 1.0675 (1.1653)\tPrec@1 64.062 (61.155)\n",
      "Epoch: [88][156/390]\tTime 0.002 (0.004)\tLoss 1.2534 (1.1900)\tPrec@1 54.688 (59.758)\n",
      "Epoch: [88][234/390]\tTime 0.007 (0.004)\tLoss 1.2550 (1.2028)\tPrec@1 59.375 (59.053)\n",
      "Epoch: [88][312/390]\tTime 0.008 (0.005)\tLoss 1.1637 (1.2134)\tPrec@1 57.812 (58.491)\n",
      "Epoch: [88][390/390]\tTime 0.002 (0.005)\tLoss 1.5171 (1.2200)\tPrec@1 47.500 (58.186)\n",
      "EPOCH: 88 train Results: Prec@1 58.186 Loss: 1.2200\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1984 (1.1984)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.003)\tLoss 1.3685 (1.2830)\tPrec@1 50.000 (55.130)\n",
      "EPOCH: 88 val Results: Prec@1 55.130 Loss: 1.2830\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [89][0/390]\tTime 0.015 (0.015)\tLoss 1.1554 (1.1554)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [89][78/390]\tTime 0.002 (0.006)\tLoss 1.1667 (1.1796)\tPrec@1 66.406 (59.840)\n",
      "Epoch: [89][156/390]\tTime 0.005 (0.006)\tLoss 1.2060 (1.1919)\tPrec@1 60.938 (59.285)\n",
      "Epoch: [89][234/390]\tTime 0.002 (0.006)\tLoss 1.2310 (1.2033)\tPrec@1 60.938 (58.850)\n",
      "Epoch: [89][312/390]\tTime 0.002 (0.006)\tLoss 1.2539 (1.2134)\tPrec@1 59.375 (58.484)\n",
      "Epoch: [89][390/390]\tTime 0.003 (0.005)\tLoss 1.3624 (1.2202)\tPrec@1 52.500 (58.188)\n",
      "EPOCH: 89 train Results: Prec@1 58.188 Loss: 1.2202\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1947 (1.1947)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5457 (1.2910)\tPrec@1 31.250 (54.570)\n",
      "EPOCH: 89 val Results: Prec@1 54.570 Loss: 1.2910\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [90][0/390]\tTime 0.002 (0.002)\tLoss 1.1688 (1.1688)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [90][78/390]\tTime 0.002 (0.004)\tLoss 1.1404 (1.1681)\tPrec@1 62.500 (60.117)\n",
      "Epoch: [90][156/390]\tTime 0.002 (0.003)\tLoss 1.2879 (1.1881)\tPrec@1 56.250 (59.400)\n",
      "Epoch: [90][234/390]\tTime 0.002 (0.003)\tLoss 1.2679 (1.2014)\tPrec@1 57.031 (58.624)\n",
      "Epoch: [90][312/390]\tTime 0.003 (0.003)\tLoss 1.2457 (1.2071)\tPrec@1 56.250 (58.499)\n",
      "Epoch: [90][390/390]\tTime 0.002 (0.003)\tLoss 1.1781 (1.2161)\tPrec@1 58.750 (58.164)\n",
      "EPOCH: 90 train Results: Prec@1 58.164 Loss: 1.2161\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1212 (1.1212)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.003 (0.001)\tLoss 1.7320 (1.2916)\tPrec@1 37.500 (54.850)\n",
      "EPOCH: 90 val Results: Prec@1 54.850 Loss: 1.2916\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [91][0/390]\tTime 0.002 (0.002)\tLoss 1.3601 (1.3601)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [91][78/390]\tTime 0.006 (0.003)\tLoss 1.1413 (1.1704)\tPrec@1 63.281 (60.225)\n",
      "Epoch: [91][156/390]\tTime 0.003 (0.003)\tLoss 1.3410 (1.1874)\tPrec@1 52.344 (59.420)\n",
      "Epoch: [91][234/390]\tTime 0.002 (0.004)\tLoss 1.1894 (1.1957)\tPrec@1 53.906 (58.969)\n",
      "Epoch: [91][312/390]\tTime 0.002 (0.004)\tLoss 1.1773 (1.2065)\tPrec@1 57.812 (58.664)\n",
      "Epoch: [91][390/390]\tTime 0.002 (0.004)\tLoss 1.2531 (1.2166)\tPrec@1 57.500 (58.144)\n",
      "EPOCH: 91 train Results: Prec@1 58.144 Loss: 1.2166\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1439 (1.1439)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3233 (1.2897)\tPrec@1 37.500 (54.690)\n",
      "EPOCH: 91 val Results: Prec@1 54.690 Loss: 1.2897\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [92][0/390]\tTime 0.002 (0.002)\tLoss 1.2117 (1.2117)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [92][78/390]\tTime 0.002 (0.005)\tLoss 1.2140 (1.1739)\tPrec@1 57.031 (59.593)\n",
      "Epoch: [92][156/390]\tTime 0.003 (0.005)\tLoss 1.1708 (1.1937)\tPrec@1 63.281 (59.221)\n",
      "Epoch: [92][234/390]\tTime 0.006 (0.004)\tLoss 1.2140 (1.2047)\tPrec@1 55.469 (58.627)\n",
      "Epoch: [92][312/390]\tTime 0.004 (0.005)\tLoss 1.2680 (1.2114)\tPrec@1 55.469 (58.164)\n",
      "Epoch: [92][390/390]\tTime 0.002 (0.004)\tLoss 1.2565 (1.2186)\tPrec@1 52.500 (57.902)\n",
      "EPOCH: 92 train Results: Prec@1 57.902 Loss: 1.2186\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1620 (1.1620)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4954 (1.2877)\tPrec@1 43.750 (54.790)\n",
      "EPOCH: 92 val Results: Prec@1 54.790 Loss: 1.2877\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [93][0/390]\tTime 0.002 (0.002)\tLoss 1.2096 (1.2096)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [93][78/390]\tTime 0.002 (0.004)\tLoss 1.1519 (1.1561)\tPrec@1 61.719 (60.581)\n",
      "Epoch: [93][156/390]\tTime 0.002 (0.004)\tLoss 1.1538 (1.1768)\tPrec@1 58.594 (59.688)\n",
      "Epoch: [93][234/390]\tTime 0.002 (0.005)\tLoss 1.3539 (1.1960)\tPrec@1 53.125 (58.916)\n",
      "Epoch: [93][312/390]\tTime 0.020 (0.006)\tLoss 1.3825 (1.2039)\tPrec@1 53.125 (58.432)\n",
      "Epoch: [93][390/390]\tTime 0.004 (0.006)\tLoss 1.2267 (1.2139)\tPrec@1 61.250 (57.976)\n",
      "EPOCH: 93 train Results: Prec@1 57.976 Loss: 1.2139\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1565 (1.1565)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.004)\tLoss 1.3425 (1.2910)\tPrec@1 37.500 (54.610)\n",
      "EPOCH: 93 val Results: Prec@1 54.610 Loss: 1.2910\n",
      "Best Prec@1: 55.540\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [94][0/390]\tTime 0.007 (0.007)\tLoss 1.0561 (1.0561)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [94][78/390]\tTime 0.007 (0.008)\tLoss 1.2078 (1.1661)\tPrec@1 62.500 (60.235)\n",
      "Epoch: [94][156/390]\tTime 0.004 (0.007)\tLoss 1.1455 (1.1855)\tPrec@1 64.062 (59.435)\n",
      "Epoch: [94][234/390]\tTime 0.002 (0.007)\tLoss 1.1099 (1.2012)\tPrec@1 61.719 (58.680)\n",
      "Epoch: [94][312/390]\tTime 0.005 (0.007)\tLoss 1.2954 (1.2103)\tPrec@1 55.469 (58.302)\n",
      "Epoch: [94][390/390]\tTime 0.047 (0.007)\tLoss 1.4397 (1.2179)\tPrec@1 56.250 (58.100)\n",
      "EPOCH: 94 train Results: Prec@1 58.100 Loss: 1.2179\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1726 (1.1726)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3337 (1.2802)\tPrec@1 50.000 (55.560)\n",
      "EPOCH: 94 val Results: Prec@1 55.560 Loss: 1.2802\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [95][0/390]\tTime 0.005 (0.005)\tLoss 1.1395 (1.1395)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [95][78/390]\tTime 0.002 (0.005)\tLoss 1.2526 (1.1622)\tPrec@1 50.781 (60.651)\n",
      "Epoch: [95][156/390]\tTime 0.002 (0.006)\tLoss 1.0843 (1.1821)\tPrec@1 64.062 (59.619)\n",
      "Epoch: [95][234/390]\tTime 0.002 (0.006)\tLoss 1.2068 (1.1939)\tPrec@1 59.375 (59.212)\n",
      "Epoch: [95][312/390]\tTime 0.002 (0.007)\tLoss 1.1470 (1.2064)\tPrec@1 63.281 (58.631)\n",
      "Epoch: [95][390/390]\tTime 0.003 (0.007)\tLoss 1.2551 (1.2160)\tPrec@1 58.750 (58.178)\n",
      "EPOCH: 95 train Results: Prec@1 58.178 Loss: 1.2160\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1500 (1.1500)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4995 (1.2897)\tPrec@1 37.500 (54.860)\n",
      "EPOCH: 95 val Results: Prec@1 54.860 Loss: 1.2897\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [96][0/390]\tTime 0.003 (0.003)\tLoss 1.1806 (1.1806)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [96][78/390]\tTime 0.004 (0.005)\tLoss 1.2334 (1.1792)\tPrec@1 60.156 (59.919)\n",
      "Epoch: [96][156/390]\tTime 0.011 (0.005)\tLoss 1.1243 (1.1963)\tPrec@1 63.281 (59.440)\n",
      "Epoch: [96][234/390]\tTime 0.009 (0.006)\tLoss 1.2533 (1.2092)\tPrec@1 52.344 (58.693)\n",
      "Epoch: [96][312/390]\tTime 0.008 (0.005)\tLoss 1.1252 (1.2113)\tPrec@1 61.719 (58.594)\n",
      "Epoch: [96][390/390]\tTime 0.003 (0.005)\tLoss 1.2761 (1.2168)\tPrec@1 57.500 (58.288)\n",
      "EPOCH: 96 train Results: Prec@1 58.288 Loss: 1.2168\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1495 (1.1495)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2610 (1.2847)\tPrec@1 56.250 (55.250)\n",
      "EPOCH: 96 val Results: Prec@1 55.250 Loss: 1.2847\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [97][0/390]\tTime 0.006 (0.006)\tLoss 1.1519 (1.1519)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [97][78/390]\tTime 0.009 (0.004)\tLoss 1.2309 (1.1729)\tPrec@1 57.812 (60.156)\n",
      "Epoch: [97][156/390]\tTime 0.008 (0.005)\tLoss 1.3189 (1.1907)\tPrec@1 58.594 (59.390)\n",
      "Epoch: [97][234/390]\tTime 0.002 (0.005)\tLoss 1.3471 (1.2006)\tPrec@1 50.781 (58.936)\n",
      "Epoch: [97][312/390]\tTime 0.008 (0.005)\tLoss 1.2999 (1.2081)\tPrec@1 52.344 (58.519)\n",
      "Epoch: [97][390/390]\tTime 0.001 (0.005)\tLoss 1.0910 (1.2160)\tPrec@1 63.750 (58.142)\n",
      "EPOCH: 97 train Results: Prec@1 58.142 Loss: 1.2160\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1978 (1.1978)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3354 (1.2855)\tPrec@1 37.500 (55.250)\n",
      "EPOCH: 97 val Results: Prec@1 55.250 Loss: 1.2855\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [98][0/390]\tTime 0.011 (0.011)\tLoss 1.1680 (1.1680)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [98][78/390]\tTime 0.005 (0.005)\tLoss 1.0599 (1.1732)\tPrec@1 66.406 (59.939)\n",
      "Epoch: [98][156/390]\tTime 0.004 (0.006)\tLoss 1.2767 (1.1893)\tPrec@1 57.812 (59.479)\n",
      "Epoch: [98][234/390]\tTime 0.005 (0.005)\tLoss 1.1609 (1.1964)\tPrec@1 58.594 (59.069)\n",
      "Epoch: [98][312/390]\tTime 0.003 (0.005)\tLoss 1.4437 (1.2049)\tPrec@1 43.750 (58.691)\n",
      "Epoch: [98][390/390]\tTime 0.001 (0.004)\tLoss 1.2146 (1.2120)\tPrec@1 58.750 (58.408)\n",
      "EPOCH: 98 train Results: Prec@1 58.408 Loss: 1.2120\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1382 (1.1382)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.5755 (1.2884)\tPrec@1 31.250 (54.830)\n",
      "EPOCH: 98 val Results: Prec@1 54.830 Loss: 1.2884\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [99][0/390]\tTime 0.004 (0.004)\tLoss 1.1804 (1.1804)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [99][78/390]\tTime 0.002 (0.005)\tLoss 1.0946 (1.1746)\tPrec@1 61.719 (60.750)\n",
      "Epoch: [99][156/390]\tTime 0.003 (0.004)\tLoss 1.1861 (1.1891)\tPrec@1 61.719 (59.564)\n",
      "Epoch: [99][234/390]\tTime 0.003 (0.004)\tLoss 1.4044 (1.1942)\tPrec@1 45.312 (59.099)\n",
      "Epoch: [99][312/390]\tTime 0.004 (0.004)\tLoss 1.2363 (1.2044)\tPrec@1 53.906 (58.654)\n",
      "Epoch: [99][390/390]\tTime 0.010 (0.004)\tLoss 1.0471 (1.2122)\tPrec@1 62.500 (58.210)\n",
      "EPOCH: 99 train Results: Prec@1 58.210 Loss: 1.2122\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1731 (1.1731)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.006 (0.002)\tLoss 1.5863 (1.2824)\tPrec@1 37.500 (54.820)\n",
      "EPOCH: 99 val Results: Prec@1 54.820 Loss: 1.2824\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [100][0/390]\tTime 0.005 (0.005)\tLoss 1.0897 (1.0897)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [100][78/390]\tTime 0.015 (0.007)\tLoss 1.2757 (1.1747)\tPrec@1 58.594 (59.504)\n",
      "Epoch: [100][156/390]\tTime 0.003 (0.007)\tLoss 1.2149 (1.1887)\tPrec@1 60.156 (59.256)\n",
      "Epoch: [100][234/390]\tTime 0.002 (0.006)\tLoss 1.2852 (1.2008)\tPrec@1 55.469 (58.743)\n",
      "Epoch: [100][312/390]\tTime 0.015 (0.006)\tLoss 1.1761 (1.2096)\tPrec@1 58.594 (58.364)\n",
      "Epoch: [100][390/390]\tTime 0.001 (0.006)\tLoss 1.2319 (1.2123)\tPrec@1 62.500 (58.268)\n",
      "EPOCH: 100 train Results: Prec@1 58.268 Loss: 1.2123\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1338 (1.1338)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4831 (1.2759)\tPrec@1 31.250 (55.140)\n",
      "EPOCH: 100 val Results: Prec@1 55.140 Loss: 1.2759\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [101][0/390]\tTime 0.003 (0.003)\tLoss 1.1786 (1.1786)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [101][78/390]\tTime 0.008 (0.006)\tLoss 1.1918 (1.1785)\tPrec@1 54.688 (59.553)\n",
      "Epoch: [101][156/390]\tTime 0.002 (0.005)\tLoss 1.2465 (1.1945)\tPrec@1 58.594 (58.644)\n",
      "Epoch: [101][234/390]\tTime 0.003 (0.005)\tLoss 1.2875 (1.2030)\tPrec@1 57.812 (58.441)\n",
      "Epoch: [101][312/390]\tTime 0.004 (0.005)\tLoss 1.1731 (1.2049)\tPrec@1 60.938 (58.227)\n",
      "Epoch: [101][390/390]\tTime 0.003 (0.005)\tLoss 1.2245 (1.2135)\tPrec@1 61.250 (58.034)\n",
      "EPOCH: 101 train Results: Prec@1 58.034 Loss: 1.2135\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2265 (1.2265)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2440 (1.2928)\tPrec@1 43.750 (54.540)\n",
      "EPOCH: 101 val Results: Prec@1 54.540 Loss: 1.2928\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [102][0/390]\tTime 0.008 (0.008)\tLoss 1.1109 (1.1109)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [102][78/390]\tTime 0.004 (0.007)\tLoss 1.1554 (1.1608)\tPrec@1 54.688 (59.939)\n",
      "Epoch: [102][156/390]\tTime 0.004 (0.006)\tLoss 1.1650 (1.1805)\tPrec@1 61.719 (59.275)\n",
      "Epoch: [102][234/390]\tTime 0.007 (0.006)\tLoss 1.2473 (1.1908)\tPrec@1 60.156 (58.916)\n",
      "Epoch: [102][312/390]\tTime 0.002 (0.006)\tLoss 1.2196 (1.2017)\tPrec@1 57.812 (58.686)\n",
      "Epoch: [102][390/390]\tTime 0.002 (0.005)\tLoss 1.1497 (1.2074)\tPrec@1 62.500 (58.426)\n",
      "EPOCH: 102 train Results: Prec@1 58.426 Loss: 1.2074\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1640 (1.1640)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4851 (1.2788)\tPrec@1 37.500 (55.200)\n",
      "EPOCH: 102 val Results: Prec@1 55.200 Loss: 1.2788\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [103][0/390]\tTime 0.006 (0.006)\tLoss 1.1306 (1.1306)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [103][78/390]\tTime 0.008 (0.007)\tLoss 1.1751 (1.1753)\tPrec@1 58.594 (60.176)\n",
      "Epoch: [103][156/390]\tTime 0.005 (0.006)\tLoss 1.2385 (1.1881)\tPrec@1 57.812 (59.410)\n",
      "Epoch: [103][234/390]\tTime 0.006 (0.006)\tLoss 1.3004 (1.1993)\tPrec@1 60.156 (58.840)\n",
      "Epoch: [103][312/390]\tTime 0.016 (0.006)\tLoss 1.4081 (1.2064)\tPrec@1 50.000 (58.476)\n",
      "Epoch: [103][390/390]\tTime 0.002 (0.006)\tLoss 1.3046 (1.2112)\tPrec@1 57.500 (58.222)\n",
      "EPOCH: 103 train Results: Prec@1 58.222 Loss: 1.2112\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1998 (1.1998)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.4556 (1.2911)\tPrec@1 43.750 (54.640)\n",
      "EPOCH: 103 val Results: Prec@1 54.640 Loss: 1.2911\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [104][0/390]\tTime 0.022 (0.022)\tLoss 1.1953 (1.1953)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [104][78/390]\tTime 0.003 (0.005)\tLoss 1.2286 (1.1608)\tPrec@1 61.719 (60.591)\n",
      "Epoch: [104][156/390]\tTime 0.002 (0.004)\tLoss 1.2484 (1.1812)\tPrec@1 54.688 (59.440)\n",
      "Epoch: [104][234/390]\tTime 0.003 (0.004)\tLoss 1.2059 (1.1917)\tPrec@1 59.375 (59.109)\n",
      "Epoch: [104][312/390]\tTime 0.003 (0.004)\tLoss 1.1870 (1.2045)\tPrec@1 59.375 (58.494)\n",
      "Epoch: [104][390/390]\tTime 0.001 (0.004)\tLoss 1.1691 (1.2117)\tPrec@1 55.000 (58.224)\n",
      "EPOCH: 104 train Results: Prec@1 58.224 Loss: 1.2117\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1930 (1.1930)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3628 (1.2780)\tPrec@1 31.250 (55.490)\n",
      "EPOCH: 104 val Results: Prec@1 55.490 Loss: 1.2780\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [105][0/390]\tTime 0.007 (0.007)\tLoss 1.1170 (1.1170)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [105][78/390]\tTime 0.007 (0.005)\tLoss 1.2164 (1.1766)\tPrec@1 53.125 (59.830)\n",
      "Epoch: [105][156/390]\tTime 0.007 (0.005)\tLoss 1.2105 (1.1818)\tPrec@1 53.125 (59.415)\n",
      "Epoch: [105][234/390]\tTime 0.006 (0.005)\tLoss 1.3626 (1.1914)\tPrec@1 52.344 (58.860)\n",
      "Epoch: [105][312/390]\tTime 0.003 (0.004)\tLoss 1.2260 (1.1985)\tPrec@1 55.469 (58.546)\n",
      "Epoch: [105][390/390]\tTime 0.002 (0.004)\tLoss 1.2246 (1.2107)\tPrec@1 60.000 (58.118)\n",
      "EPOCH: 105 train Results: Prec@1 58.118 Loss: 1.2107\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1446 (1.1446)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.006 (0.001)\tLoss 1.3497 (1.2858)\tPrec@1 43.750 (54.510)\n",
      "EPOCH: 105 val Results: Prec@1 54.510 Loss: 1.2858\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [106][0/390]\tTime 0.021 (0.021)\tLoss 1.1035 (1.1035)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [106][78/390]\tTime 0.002 (0.004)\tLoss 1.0591 (1.1582)\tPrec@1 67.969 (61.086)\n",
      "Epoch: [106][156/390]\tTime 0.004 (0.004)\tLoss 1.2177 (1.1782)\tPrec@1 57.031 (59.788)\n",
      "Epoch: [106][234/390]\tTime 0.002 (0.003)\tLoss 1.1462 (1.1954)\tPrec@1 60.938 (58.926)\n",
      "Epoch: [106][312/390]\tTime 0.002 (0.004)\tLoss 1.1887 (1.2071)\tPrec@1 60.938 (58.456)\n",
      "Epoch: [106][390/390]\tTime 0.013 (0.004)\tLoss 1.2763 (1.2135)\tPrec@1 56.250 (58.208)\n",
      "EPOCH: 106 train Results: Prec@1 58.208 Loss: 1.2135\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1758 (1.1758)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.002)\tLoss 1.5214 (1.2892)\tPrec@1 25.000 (54.650)\n",
      "EPOCH: 106 val Results: Prec@1 54.650 Loss: 1.2892\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [107][0/390]\tTime 0.005 (0.005)\tLoss 1.0744 (1.0744)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [107][78/390]\tTime 0.008 (0.004)\tLoss 1.2539 (1.1682)\tPrec@1 57.812 (60.502)\n",
      "Epoch: [107][156/390]\tTime 0.009 (0.005)\tLoss 1.3784 (1.1767)\tPrec@1 50.000 (59.748)\n",
      "Epoch: [107][234/390]\tTime 0.002 (0.004)\tLoss 1.2151 (1.1863)\tPrec@1 60.938 (59.418)\n",
      "Epoch: [107][312/390]\tTime 0.002 (0.004)\tLoss 1.3098 (1.2020)\tPrec@1 55.469 (58.823)\n",
      "Epoch: [107][390/390]\tTime 0.001 (0.004)\tLoss 1.0614 (1.2128)\tPrec@1 66.250 (58.252)\n",
      "EPOCH: 107 train Results: Prec@1 58.252 Loss: 1.2128\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1393 (1.1393)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6151 (1.2814)\tPrec@1 31.250 (55.130)\n",
      "EPOCH: 107 val Results: Prec@1 55.130 Loss: 1.2814\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [108][0/390]\tTime 0.003 (0.003)\tLoss 1.2009 (1.2009)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [108][78/390]\tTime 0.004 (0.004)\tLoss 1.2021 (1.1520)\tPrec@1 60.938 (61.116)\n",
      "Epoch: [108][156/390]\tTime 0.004 (0.004)\tLoss 1.2443 (1.1737)\tPrec@1 59.375 (60.186)\n",
      "Epoch: [108][234/390]\tTime 0.010 (0.004)\tLoss 1.2571 (1.1873)\tPrec@1 52.344 (59.661)\n",
      "Epoch: [108][312/390]\tTime 0.002 (0.004)\tLoss 1.2378 (1.1971)\tPrec@1 55.469 (59.053)\n",
      "Epoch: [108][390/390]\tTime 0.002 (0.004)\tLoss 1.1940 (1.2078)\tPrec@1 62.500 (58.622)\n",
      "EPOCH: 108 train Results: Prec@1 58.622 Loss: 1.2078\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1840 (1.1840)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5774 (1.2979)\tPrec@1 31.250 (54.150)\n",
      "EPOCH: 108 val Results: Prec@1 54.150 Loss: 1.2979\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [109][0/390]\tTime 0.002 (0.002)\tLoss 1.2194 (1.2194)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [109][78/390]\tTime 0.002 (0.004)\tLoss 1.2487 (1.1745)\tPrec@1 55.469 (60.275)\n",
      "Epoch: [109][156/390]\tTime 0.002 (0.003)\tLoss 1.2402 (1.1928)\tPrec@1 61.719 (59.440)\n",
      "Epoch: [109][234/390]\tTime 0.007 (0.004)\tLoss 1.0633 (1.1989)\tPrec@1 65.625 (58.993)\n",
      "Epoch: [109][312/390]\tTime 0.002 (0.004)\tLoss 1.1703 (1.2043)\tPrec@1 61.719 (58.589)\n",
      "Epoch: [109][390/390]\tTime 0.014 (0.006)\tLoss 1.2947 (1.2092)\tPrec@1 51.250 (58.374)\n",
      "EPOCH: 109 train Results: Prec@1 58.374 Loss: 1.2092\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1520 (1.1520)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.3713 (1.2788)\tPrec@1 37.500 (55.190)\n",
      "EPOCH: 109 val Results: Prec@1 55.190 Loss: 1.2788\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [110][0/390]\tTime 0.004 (0.004)\tLoss 1.2609 (1.2609)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [110][78/390]\tTime 0.002 (0.014)\tLoss 1.0998 (1.1624)\tPrec@1 58.594 (60.433)\n",
      "Epoch: [110][156/390]\tTime 0.002 (0.010)\tLoss 1.0840 (1.1805)\tPrec@1 61.719 (59.693)\n",
      "Epoch: [110][234/390]\tTime 0.002 (0.009)\tLoss 1.2838 (1.1875)\tPrec@1 59.375 (59.345)\n",
      "Epoch: [110][312/390]\tTime 0.011 (0.007)\tLoss 1.2851 (1.2025)\tPrec@1 55.469 (58.758)\n",
      "Epoch: [110][390/390]\tTime 0.006 (0.007)\tLoss 1.2996 (1.2063)\tPrec@1 58.750 (58.446)\n",
      "EPOCH: 110 train Results: Prec@1 58.446 Loss: 1.2063\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1593 (1.1593)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5975 (1.2762)\tPrec@1 37.500 (55.290)\n",
      "EPOCH: 110 val Results: Prec@1 55.290 Loss: 1.2762\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [111][0/390]\tTime 0.002 (0.002)\tLoss 1.1834 (1.1834)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [111][78/390]\tTime 0.004 (0.005)\tLoss 1.2223 (1.1708)\tPrec@1 56.250 (60.265)\n",
      "Epoch: [111][156/390]\tTime 0.002 (0.005)\tLoss 1.2315 (1.1891)\tPrec@1 59.375 (59.186)\n",
      "Epoch: [111][234/390]\tTime 0.004 (0.005)\tLoss 1.4086 (1.2031)\tPrec@1 53.906 (58.787)\n",
      "Epoch: [111][312/390]\tTime 0.002 (0.005)\tLoss 1.2178 (1.2103)\tPrec@1 57.812 (58.402)\n",
      "Epoch: [111][390/390]\tTime 0.002 (0.005)\tLoss 1.1089 (1.2134)\tPrec@1 66.250 (58.282)\n",
      "EPOCH: 111 train Results: Prec@1 58.282 Loss: 1.2134\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1972 (1.1972)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2500 (1.2900)\tPrec@1 62.500 (54.300)\n",
      "EPOCH: 111 val Results: Prec@1 54.300 Loss: 1.2900\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [112][0/390]\tTime 0.002 (0.002)\tLoss 1.1485 (1.1485)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [112][78/390]\tTime 0.002 (0.003)\tLoss 1.1659 (1.1684)\tPrec@1 57.031 (59.939)\n",
      "Epoch: [112][156/390]\tTime 0.007 (0.004)\tLoss 1.2339 (1.1837)\tPrec@1 58.594 (59.534)\n",
      "Epoch: [112][234/390]\tTime 0.003 (0.004)\tLoss 1.1911 (1.1896)\tPrec@1 60.156 (59.325)\n",
      "Epoch: [112][312/390]\tTime 0.002 (0.004)\tLoss 1.1596 (1.2051)\tPrec@1 57.812 (58.669)\n",
      "Epoch: [112][390/390]\tTime 0.002 (0.004)\tLoss 1.4530 (1.2098)\tPrec@1 50.000 (58.402)\n",
      "EPOCH: 112 train Results: Prec@1 58.402 Loss: 1.2098\n",
      "Test: [0/78]\tTime 0.010 (0.010)\tLoss 1.1140 (1.1140)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3902 (1.2767)\tPrec@1 25.000 (54.990)\n",
      "EPOCH: 112 val Results: Prec@1 54.990 Loss: 1.2767\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [113][0/390]\tTime 0.003 (0.003)\tLoss 1.0978 (1.0978)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [113][78/390]\tTime 0.003 (0.004)\tLoss 1.2095 (1.1621)\tPrec@1 60.938 (60.453)\n",
      "Epoch: [113][156/390]\tTime 0.006 (0.004)\tLoss 1.1946 (1.1809)\tPrec@1 57.031 (59.698)\n",
      "Epoch: [113][234/390]\tTime 0.008 (0.004)\tLoss 1.1084 (1.1924)\tPrec@1 64.062 (59.382)\n",
      "Epoch: [113][312/390]\tTime 0.002 (0.004)\tLoss 1.1468 (1.2000)\tPrec@1 59.375 (58.983)\n",
      "Epoch: [113][390/390]\tTime 0.002 (0.004)\tLoss 1.2126 (1.2067)\tPrec@1 58.750 (58.650)\n",
      "EPOCH: 113 train Results: Prec@1 58.650 Loss: 1.2067\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2039 (1.2039)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4391 (1.2719)\tPrec@1 43.750 (55.340)\n",
      "EPOCH: 113 val Results: Prec@1 55.340 Loss: 1.2719\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [114][0/390]\tTime 0.002 (0.002)\tLoss 1.2575 (1.2575)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [114][78/390]\tTime 0.002 (0.005)\tLoss 1.0029 (1.1680)\tPrec@1 71.875 (60.463)\n",
      "Epoch: [114][156/390]\tTime 0.003 (0.005)\tLoss 1.1632 (1.1794)\tPrec@1 59.375 (59.947)\n",
      "Epoch: [114][234/390]\tTime 0.002 (0.005)\tLoss 1.1550 (1.1943)\tPrec@1 53.906 (59.249)\n",
      "Epoch: [114][312/390]\tTime 0.002 (0.005)\tLoss 1.1038 (1.1999)\tPrec@1 66.406 (59.031)\n",
      "Epoch: [114][390/390]\tTime 0.011 (0.005)\tLoss 1.2363 (1.2079)\tPrec@1 51.250 (58.604)\n",
      "EPOCH: 114 train Results: Prec@1 58.604 Loss: 1.2079\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1261 (1.1261)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4950 (1.2791)\tPrec@1 43.750 (55.350)\n",
      "EPOCH: 114 val Results: Prec@1 55.350 Loss: 1.2791\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [115][0/390]\tTime 0.004 (0.004)\tLoss 0.9876 (0.9876)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [115][78/390]\tTime 0.003 (0.004)\tLoss 1.1082 (1.1586)\tPrec@1 61.719 (60.799)\n",
      "Epoch: [115][156/390]\tTime 0.005 (0.003)\tLoss 1.3632 (1.1817)\tPrec@1 49.219 (59.564)\n",
      "Epoch: [115][234/390]\tTime 0.004 (0.003)\tLoss 1.3343 (1.1962)\tPrec@1 58.594 (58.996)\n",
      "Epoch: [115][312/390]\tTime 0.003 (0.004)\tLoss 1.3628 (1.2037)\tPrec@1 51.562 (58.621)\n",
      "Epoch: [115][390/390]\tTime 0.002 (0.004)\tLoss 1.3926 (1.2077)\tPrec@1 56.250 (58.472)\n",
      "EPOCH: 115 train Results: Prec@1 58.472 Loss: 1.2077\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1793 (1.1793)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5571 (1.2825)\tPrec@1 43.750 (55.040)\n",
      "EPOCH: 115 val Results: Prec@1 55.040 Loss: 1.2825\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [116][0/390]\tTime 0.003 (0.003)\tLoss 1.1699 (1.1699)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [116][78/390]\tTime 0.002 (0.003)\tLoss 1.1726 (1.1681)\tPrec@1 58.594 (59.810)\n",
      "Epoch: [116][156/390]\tTime 0.008 (0.004)\tLoss 1.3291 (1.1841)\tPrec@1 54.688 (59.086)\n",
      "Epoch: [116][234/390]\tTime 0.003 (0.004)\tLoss 1.1984 (1.1937)\tPrec@1 59.375 (58.697)\n",
      "Epoch: [116][312/390]\tTime 0.003 (0.004)\tLoss 1.3400 (1.2037)\tPrec@1 47.656 (58.389)\n",
      "Epoch: [116][390/390]\tTime 0.001 (0.004)\tLoss 1.3752 (1.2097)\tPrec@1 52.500 (58.118)\n",
      "EPOCH: 116 train Results: Prec@1 58.118 Loss: 1.2097\n",
      "Test: [0/78]\tTime 0.012 (0.012)\tLoss 1.1179 (1.1179)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3485 (1.2825)\tPrec@1 25.000 (54.920)\n",
      "EPOCH: 116 val Results: Prec@1 54.920 Loss: 1.2825\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [117][0/390]\tTime 0.003 (0.003)\tLoss 1.2371 (1.2371)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [117][78/390]\tTime 0.003 (0.005)\tLoss 1.2362 (1.1739)\tPrec@1 53.906 (59.800)\n",
      "Epoch: [117][156/390]\tTime 0.007 (0.004)\tLoss 1.2479 (1.1837)\tPrec@1 60.156 (59.340)\n",
      "Epoch: [117][234/390]\tTime 0.004 (0.004)\tLoss 1.2697 (1.1946)\tPrec@1 57.031 (59.023)\n",
      "Epoch: [117][312/390]\tTime 0.002 (0.004)\tLoss 1.1833 (1.2044)\tPrec@1 53.906 (58.549)\n",
      "Epoch: [117][390/390]\tTime 0.001 (0.004)\tLoss 1.2284 (1.2078)\tPrec@1 53.750 (58.436)\n",
      "EPOCH: 117 train Results: Prec@1 58.436 Loss: 1.2078\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1309 (1.1309)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4488 (1.2777)\tPrec@1 37.500 (55.530)\n",
      "EPOCH: 117 val Results: Prec@1 55.530 Loss: 1.2777\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [118][0/390]\tTime 0.004 (0.004)\tLoss 1.1159 (1.1159)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [118][78/390]\tTime 0.012 (0.005)\tLoss 1.1409 (1.1724)\tPrec@1 57.812 (59.840)\n",
      "Epoch: [118][156/390]\tTime 0.010 (0.004)\tLoss 1.1670 (1.1787)\tPrec@1 60.156 (59.698)\n",
      "Epoch: [118][234/390]\tTime 0.003 (0.004)\tLoss 1.2340 (1.1868)\tPrec@1 57.812 (59.219)\n",
      "Epoch: [118][312/390]\tTime 0.004 (0.004)\tLoss 1.2299 (1.1965)\tPrec@1 61.719 (58.766)\n",
      "Epoch: [118][390/390]\tTime 0.002 (0.004)\tLoss 1.3262 (1.2062)\tPrec@1 52.500 (58.334)\n",
      "EPOCH: 118 train Results: Prec@1 58.334 Loss: 1.2062\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1318 (1.1318)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4823 (1.2931)\tPrec@1 43.750 (54.530)\n",
      "EPOCH: 118 val Results: Prec@1 54.530 Loss: 1.2931\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [119][0/390]\tTime 0.002 (0.002)\tLoss 1.2227 (1.2227)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [119][78/390]\tTime 0.003 (0.004)\tLoss 1.2470 (1.1567)\tPrec@1 57.031 (60.710)\n",
      "Epoch: [119][156/390]\tTime 0.003 (0.003)\tLoss 1.2946 (1.1780)\tPrec@1 57.031 (59.460)\n",
      "Epoch: [119][234/390]\tTime 0.002 (0.004)\tLoss 1.2063 (1.1887)\tPrec@1 62.500 (59.082)\n",
      "Epoch: [119][312/390]\tTime 0.003 (0.004)\tLoss 1.2392 (1.1966)\tPrec@1 59.375 (58.686)\n",
      "Epoch: [119][390/390]\tTime 0.002 (0.004)\tLoss 1.3096 (1.2061)\tPrec@1 47.500 (58.338)\n",
      "EPOCH: 119 train Results: Prec@1 58.338 Loss: 1.2061\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1622 (1.1622)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3971 (1.2812)\tPrec@1 37.500 (55.000)\n",
      "EPOCH: 119 val Results: Prec@1 55.000 Loss: 1.2812\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [120][0/390]\tTime 0.003 (0.003)\tLoss 1.0997 (1.0997)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [120][78/390]\tTime 0.010 (0.004)\tLoss 1.2504 (1.1716)\tPrec@1 52.344 (59.968)\n",
      "Epoch: [120][156/390]\tTime 0.002 (0.003)\tLoss 1.0959 (1.1884)\tPrec@1 58.594 (59.350)\n",
      "Epoch: [120][234/390]\tTime 0.004 (0.004)\tLoss 1.2703 (1.1956)\tPrec@1 57.031 (59.082)\n",
      "Epoch: [120][312/390]\tTime 0.004 (0.004)\tLoss 1.3086 (1.1995)\tPrec@1 51.562 (58.858)\n",
      "Epoch: [120][390/390]\tTime 0.003 (0.005)\tLoss 1.1433 (1.2066)\tPrec@1 63.750 (58.516)\n",
      "EPOCH: 120 train Results: Prec@1 58.516 Loss: 1.2066\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1259 (1.1259)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4125 (1.2674)\tPrec@1 43.750 (55.360)\n",
      "EPOCH: 120 val Results: Prec@1 55.360 Loss: 1.2674\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [121][0/390]\tTime 0.003 (0.003)\tLoss 1.0720 (1.0720)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [121][78/390]\tTime 0.004 (0.007)\tLoss 1.1824 (1.1519)\tPrec@1 57.031 (60.601)\n",
      "Epoch: [121][156/390]\tTime 0.018 (0.007)\tLoss 1.1406 (1.1807)\tPrec@1 61.719 (59.544)\n",
      "Epoch: [121][234/390]\tTime 0.005 (0.008)\tLoss 1.2377 (1.1890)\tPrec@1 48.438 (59.016)\n",
      "Epoch: [121][312/390]\tTime 0.003 (0.011)\tLoss 1.1130 (1.1972)\tPrec@1 65.625 (58.739)\n",
      "Epoch: [121][390/390]\tTime 0.001 (0.009)\tLoss 1.0292 (1.2040)\tPrec@1 60.000 (58.486)\n",
      "EPOCH: 121 train Results: Prec@1 58.486 Loss: 1.2040\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1004 (1.1004)\tPrec@1 67.188 (67.188)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4177 (1.2794)\tPrec@1 31.250 (54.910)\n",
      "EPOCH: 121 val Results: Prec@1 54.910 Loss: 1.2794\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [122][0/390]\tTime 0.010 (0.010)\tLoss 1.0978 (1.0978)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [122][78/390]\tTime 0.002 (0.006)\tLoss 1.0575 (1.1467)\tPrec@1 61.719 (61.116)\n",
      "Epoch: [122][156/390]\tTime 0.006 (0.006)\tLoss 1.1319 (1.1648)\tPrec@1 59.375 (60.390)\n",
      "Epoch: [122][234/390]\tTime 0.004 (0.006)\tLoss 1.3618 (1.1831)\tPrec@1 55.469 (59.648)\n",
      "Epoch: [122][312/390]\tTime 0.003 (0.006)\tLoss 1.1904 (1.1953)\tPrec@1 64.062 (59.023)\n",
      "Epoch: [122][390/390]\tTime 0.008 (0.006)\tLoss 1.3973 (1.2047)\tPrec@1 48.750 (58.684)\n",
      "EPOCH: 122 train Results: Prec@1 58.684 Loss: 1.2047\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1437 (1.1437)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4233 (1.2885)\tPrec@1 31.250 (54.960)\n",
      "EPOCH: 122 val Results: Prec@1 54.960 Loss: 1.2885\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [123][0/390]\tTime 0.003 (0.003)\tLoss 1.0802 (1.0802)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [123][78/390]\tTime 0.003 (0.005)\tLoss 1.1892 (1.1560)\tPrec@1 53.125 (60.443)\n",
      "Epoch: [123][156/390]\tTime 0.007 (0.004)\tLoss 1.2306 (1.1741)\tPrec@1 62.500 (59.808)\n",
      "Epoch: [123][234/390]\tTime 0.002 (0.005)\tLoss 1.3480 (1.1876)\tPrec@1 56.250 (59.299)\n",
      "Epoch: [123][312/390]\tTime 0.009 (0.005)\tLoss 1.2681 (1.1986)\tPrec@1 53.125 (58.936)\n",
      "Epoch: [123][390/390]\tTime 0.003 (0.005)\tLoss 1.2630 (1.2064)\tPrec@1 53.750 (58.614)\n",
      "EPOCH: 123 train Results: Prec@1 58.614 Loss: 1.2064\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2283 (1.2283)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.4307 (1.2891)\tPrec@1 43.750 (54.680)\n",
      "EPOCH: 123 val Results: Prec@1 54.680 Loss: 1.2891\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [124][0/390]\tTime 0.003 (0.003)\tLoss 1.1104 (1.1104)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [124][78/390]\tTime 0.003 (0.004)\tLoss 1.2378 (1.1758)\tPrec@1 55.469 (59.583)\n",
      "Epoch: [124][156/390]\tTime 0.010 (0.004)\tLoss 1.1826 (1.1868)\tPrec@1 60.938 (59.141)\n",
      "Epoch: [124][234/390]\tTime 0.002 (0.004)\tLoss 1.2262 (1.1972)\tPrec@1 55.469 (58.607)\n",
      "Epoch: [124][312/390]\tTime 0.002 (0.005)\tLoss 1.2127 (1.1990)\tPrec@1 62.500 (58.459)\n",
      "Epoch: [124][390/390]\tTime 0.001 (0.005)\tLoss 1.2418 (1.2039)\tPrec@1 51.250 (58.378)\n",
      "EPOCH: 124 train Results: Prec@1 58.378 Loss: 1.2039\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2144 (1.2144)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5948 (1.2807)\tPrec@1 31.250 (54.740)\n",
      "EPOCH: 124 val Results: Prec@1 54.740 Loss: 1.2807\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [125][0/390]\tTime 0.005 (0.005)\tLoss 1.1709 (1.1709)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [125][78/390]\tTime 0.010 (0.005)\tLoss 1.0086 (1.1648)\tPrec@1 64.844 (60.285)\n",
      "Epoch: [125][156/390]\tTime 0.002 (0.007)\tLoss 1.1372 (1.1825)\tPrec@1 60.938 (59.484)\n",
      "Epoch: [125][234/390]\tTime 0.002 (0.006)\tLoss 1.2629 (1.1916)\tPrec@1 53.906 (59.139)\n",
      "Epoch: [125][312/390]\tTime 0.009 (0.005)\tLoss 1.2893 (1.1983)\tPrec@1 57.812 (58.881)\n",
      "Epoch: [125][390/390]\tTime 0.004 (0.006)\tLoss 1.0765 (1.2036)\tPrec@1 66.250 (58.598)\n",
      "EPOCH: 125 train Results: Prec@1 58.598 Loss: 1.2036\n",
      "Test: [0/78]\tTime 0.011 (0.011)\tLoss 1.1603 (1.1603)\tPrec@1 59.375 (59.375)\n",
      "Test: [78/78]\tTime 0.000 (0.002)\tLoss 1.5389 (1.2895)\tPrec@1 25.000 (54.260)\n",
      "EPOCH: 125 val Results: Prec@1 54.260 Loss: 1.2895\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [126][0/390]\tTime 0.003 (0.003)\tLoss 1.0430 (1.0430)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [126][78/390]\tTime 0.002 (0.005)\tLoss 1.0104 (1.1499)\tPrec@1 67.188 (60.967)\n",
      "Epoch: [126][156/390]\tTime 0.002 (0.005)\tLoss 1.3279 (1.1744)\tPrec@1 53.906 (59.907)\n",
      "Epoch: [126][234/390]\tTime 0.003 (0.004)\tLoss 1.3180 (1.1859)\tPrec@1 53.906 (59.222)\n",
      "Epoch: [126][312/390]\tTime 0.002 (0.004)\tLoss 1.2186 (1.1971)\tPrec@1 52.344 (58.679)\n",
      "Epoch: [126][390/390]\tTime 0.005 (0.004)\tLoss 1.1459 (1.2047)\tPrec@1 61.250 (58.378)\n",
      "EPOCH: 126 train Results: Prec@1 58.378 Loss: 1.2047\n",
      "Test: [0/78]\tTime 0.007 (0.007)\tLoss 1.1900 (1.1900)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5380 (1.2813)\tPrec@1 31.250 (55.010)\n",
      "EPOCH: 126 val Results: Prec@1 55.010 Loss: 1.2813\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [127][0/390]\tTime 0.004 (0.004)\tLoss 1.2302 (1.2302)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [127][78/390]\tTime 0.003 (0.003)\tLoss 0.9891 (1.1586)\tPrec@1 71.875 (60.581)\n",
      "Epoch: [127][156/390]\tTime 0.002 (0.003)\tLoss 1.1796 (1.1763)\tPrec@1 57.031 (59.390)\n",
      "Epoch: [127][234/390]\tTime 0.003 (0.003)\tLoss 1.4046 (1.1863)\tPrec@1 50.000 (59.179)\n",
      "Epoch: [127][312/390]\tTime 0.012 (0.003)\tLoss 1.3060 (1.2002)\tPrec@1 53.125 (58.669)\n",
      "Epoch: [127][390/390]\tTime 0.001 (0.004)\tLoss 1.2304 (1.2049)\tPrec@1 57.500 (58.426)\n",
      "EPOCH: 127 train Results: Prec@1 58.426 Loss: 1.2049\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1471 (1.1471)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4881 (1.2794)\tPrec@1 31.250 (55.430)\n",
      "EPOCH: 127 val Results: Prec@1 55.430 Loss: 1.2794\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [128][0/390]\tTime 0.003 (0.003)\tLoss 1.1506 (1.1506)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [128][78/390]\tTime 0.002 (0.003)\tLoss 1.1967 (1.1560)\tPrec@1 59.375 (60.305)\n",
      "Epoch: [128][156/390]\tTime 0.002 (0.004)\tLoss 1.1544 (1.1790)\tPrec@1 60.156 (59.136)\n",
      "Epoch: [128][234/390]\tTime 0.002 (0.004)\tLoss 1.1417 (1.1852)\tPrec@1 65.625 (58.983)\n",
      "Epoch: [128][312/390]\tTime 0.004 (0.004)\tLoss 1.1670 (1.1938)\tPrec@1 61.719 (58.664)\n",
      "Epoch: [128][390/390]\tTime 0.001 (0.004)\tLoss 1.4488 (1.2027)\tPrec@1 56.250 (58.368)\n",
      "EPOCH: 128 train Results: Prec@1 58.368 Loss: 1.2027\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1637 (1.1637)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3958 (1.2886)\tPrec@1 37.500 (54.140)\n",
      "EPOCH: 128 val Results: Prec@1 54.140 Loss: 1.2886\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [129][0/390]\tTime 0.002 (0.002)\tLoss 1.2304 (1.2304)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [129][78/390]\tTime 0.002 (0.003)\tLoss 1.1180 (1.1470)\tPrec@1 60.156 (60.690)\n",
      "Epoch: [129][156/390]\tTime 0.004 (0.003)\tLoss 1.2023 (1.1626)\tPrec@1 59.375 (60.062)\n",
      "Epoch: [129][234/390]\tTime 0.003 (0.003)\tLoss 1.3615 (1.1794)\tPrec@1 50.000 (59.511)\n",
      "Epoch: [129][312/390]\tTime 0.002 (0.003)\tLoss 1.1328 (1.1904)\tPrec@1 61.719 (58.996)\n",
      "Epoch: [129][390/390]\tTime 0.001 (0.003)\tLoss 1.1776 (1.1989)\tPrec@1 57.500 (58.610)\n",
      "EPOCH: 129 train Results: Prec@1 58.610 Loss: 1.1989\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.2080 (1.2080)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4877 (1.2953)\tPrec@1 31.250 (54.620)\n",
      "EPOCH: 129 val Results: Prec@1 54.620 Loss: 1.2953\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [130][0/390]\tTime 0.005 (0.005)\tLoss 1.1054 (1.1054)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [130][78/390]\tTime 0.002 (0.003)\tLoss 1.0423 (1.1457)\tPrec@1 66.406 (61.284)\n",
      "Epoch: [130][156/390]\tTime 0.002 (0.003)\tLoss 1.3085 (1.1616)\tPrec@1 53.125 (60.156)\n",
      "Epoch: [130][234/390]\tTime 0.002 (0.003)\tLoss 1.3526 (1.1811)\tPrec@1 51.562 (59.405)\n",
      "Epoch: [130][312/390]\tTime 0.002 (0.003)\tLoss 1.2336 (1.1931)\tPrec@1 57.812 (58.893)\n",
      "Epoch: [130][390/390]\tTime 0.002 (0.003)\tLoss 1.4229 (1.2006)\tPrec@1 43.750 (58.612)\n",
      "EPOCH: 130 train Results: Prec@1 58.612 Loss: 1.2006\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1865 (1.1865)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2256 (1.2847)\tPrec@1 43.750 (55.040)\n",
      "EPOCH: 130 val Results: Prec@1 55.040 Loss: 1.2847\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [131][0/390]\tTime 0.002 (0.002)\tLoss 1.0178 (1.0178)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [131][78/390]\tTime 0.002 (0.003)\tLoss 1.1810 (1.1618)\tPrec@1 57.812 (59.919)\n",
      "Epoch: [131][156/390]\tTime 0.005 (0.003)\tLoss 1.2269 (1.1772)\tPrec@1 54.688 (59.290)\n",
      "Epoch: [131][234/390]\tTime 0.004 (0.003)\tLoss 1.2223 (1.1866)\tPrec@1 55.469 (58.886)\n",
      "Epoch: [131][312/390]\tTime 0.007 (0.003)\tLoss 1.3933 (1.1936)\tPrec@1 51.562 (58.753)\n",
      "Epoch: [131][390/390]\tTime 0.002 (0.004)\tLoss 1.1228 (1.2009)\tPrec@1 62.500 (58.488)\n",
      "EPOCH: 131 train Results: Prec@1 58.488 Loss: 1.2009\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1653 (1.1653)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6300 (1.2721)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 131 val Results: Prec@1 55.300 Loss: 1.2721\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [132][0/390]\tTime 0.002 (0.002)\tLoss 1.1634 (1.1634)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [132][78/390]\tTime 0.006 (0.003)\tLoss 1.1639 (1.1588)\tPrec@1 57.812 (60.384)\n",
      "Epoch: [132][156/390]\tTime 0.003 (0.003)\tLoss 1.2185 (1.1824)\tPrec@1 59.375 (59.340)\n",
      "Epoch: [132][234/390]\tTime 0.008 (0.003)\tLoss 1.1305 (1.1952)\tPrec@1 61.719 (58.793)\n",
      "Epoch: [132][312/390]\tTime 0.004 (0.003)\tLoss 1.4145 (1.2002)\tPrec@1 49.219 (58.654)\n",
      "Epoch: [132][390/390]\tTime 0.003 (0.003)\tLoss 1.1756 (1.2005)\tPrec@1 66.250 (58.628)\n",
      "EPOCH: 132 train Results: Prec@1 58.628 Loss: 1.2005\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1456 (1.1456)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.4911 (1.2799)\tPrec@1 37.500 (54.660)\n",
      "EPOCH: 132 val Results: Prec@1 54.660 Loss: 1.2799\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [133][0/390]\tTime 0.005 (0.005)\tLoss 1.0308 (1.0308)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [133][78/390]\tTime 0.003 (0.003)\tLoss 1.1455 (1.1579)\tPrec@1 60.938 (60.977)\n",
      "Epoch: [133][156/390]\tTime 0.003 (0.003)\tLoss 1.0751 (1.1661)\tPrec@1 65.625 (60.266)\n",
      "Epoch: [133][234/390]\tTime 0.006 (0.003)\tLoss 1.3449 (1.1813)\tPrec@1 52.344 (59.714)\n",
      "Epoch: [133][312/390]\tTime 0.004 (0.003)\tLoss 1.2420 (1.1937)\tPrec@1 55.469 (59.225)\n",
      "Epoch: [133][390/390]\tTime 0.003 (0.003)\tLoss 1.2279 (1.1988)\tPrec@1 58.750 (58.908)\n",
      "EPOCH: 133 train Results: Prec@1 58.908 Loss: 1.1988\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1803 (1.1803)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3064 (1.2878)\tPrec@1 56.250 (54.760)\n",
      "EPOCH: 133 val Results: Prec@1 54.760 Loss: 1.2878\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [134][0/390]\tTime 0.004 (0.004)\tLoss 1.1936 (1.1936)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [134][78/390]\tTime 0.002 (0.003)\tLoss 1.1917 (1.1695)\tPrec@1 59.375 (60.028)\n",
      "Epoch: [134][156/390]\tTime 0.002 (0.003)\tLoss 1.1679 (1.1874)\tPrec@1 58.594 (59.231)\n",
      "Epoch: [134][234/390]\tTime 0.003 (0.003)\tLoss 1.1688 (1.1943)\tPrec@1 54.688 (58.667)\n",
      "Epoch: [134][312/390]\tTime 0.004 (0.003)\tLoss 1.1910 (1.1999)\tPrec@1 60.938 (58.491)\n",
      "Epoch: [134][390/390]\tTime 0.005 (0.003)\tLoss 1.2577 (1.2046)\tPrec@1 55.000 (58.300)\n",
      "EPOCH: 134 train Results: Prec@1 58.300 Loss: 1.2046\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1164 (1.1164)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4784 (1.2759)\tPrec@1 37.500 (55.130)\n",
      "EPOCH: 134 val Results: Prec@1 55.130 Loss: 1.2759\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [135][0/390]\tTime 0.003 (0.003)\tLoss 1.1557 (1.1557)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [135][78/390]\tTime 0.002 (0.003)\tLoss 1.1030 (1.1559)\tPrec@1 67.969 (60.502)\n",
      "Epoch: [135][156/390]\tTime 0.006 (0.003)\tLoss 1.1661 (1.1705)\tPrec@1 59.375 (59.982)\n",
      "Epoch: [135][234/390]\tTime 0.003 (0.003)\tLoss 1.3214 (1.1829)\tPrec@1 54.688 (59.242)\n",
      "Epoch: [135][312/390]\tTime 0.002 (0.003)\tLoss 1.1623 (1.1923)\tPrec@1 59.375 (58.936)\n",
      "Epoch: [135][390/390]\tTime 0.003 (0.003)\tLoss 1.2521 (1.2012)\tPrec@1 55.000 (58.620)\n",
      "EPOCH: 135 train Results: Prec@1 58.620 Loss: 1.2012\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1857 (1.1857)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2644 (1.2787)\tPrec@1 56.250 (54.900)\n",
      "EPOCH: 135 val Results: Prec@1 54.900 Loss: 1.2787\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [136][0/390]\tTime 0.005 (0.005)\tLoss 0.9731 (0.9731)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [136][78/390]\tTime 0.004 (0.004)\tLoss 1.3694 (1.1545)\tPrec@1 48.438 (60.572)\n",
      "Epoch: [136][156/390]\tTime 0.004 (0.004)\tLoss 1.0757 (1.1666)\tPrec@1 63.281 (59.992)\n",
      "Epoch: [136][234/390]\tTime 0.002 (0.003)\tLoss 1.1348 (1.1828)\tPrec@1 65.625 (59.352)\n",
      "Epoch: [136][312/390]\tTime 0.002 (0.003)\tLoss 1.2614 (1.1915)\tPrec@1 54.688 (58.961)\n",
      "Epoch: [136][390/390]\tTime 0.001 (0.003)\tLoss 1.2922 (1.1990)\tPrec@1 52.500 (58.674)\n",
      "EPOCH: 136 train Results: Prec@1 58.674 Loss: 1.1990\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1156 (1.1156)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4554 (1.2746)\tPrec@1 37.500 (55.380)\n",
      "EPOCH: 136 val Results: Prec@1 55.380 Loss: 1.2746\n",
      "Best Prec@1: 55.560\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [137][0/390]\tTime 0.002 (0.002)\tLoss 1.1450 (1.1450)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [137][78/390]\tTime 0.003 (0.003)\tLoss 1.3352 (1.1663)\tPrec@1 56.250 (60.047)\n",
      "Epoch: [137][156/390]\tTime 0.007 (0.003)\tLoss 1.0721 (1.1755)\tPrec@1 66.406 (59.574)\n",
      "Epoch: [137][234/390]\tTime 0.004 (0.003)\tLoss 1.1404 (1.1817)\tPrec@1 60.156 (59.172)\n",
      "Epoch: [137][312/390]\tTime 0.003 (0.003)\tLoss 1.2110 (1.1909)\tPrec@1 60.156 (58.856)\n",
      "Epoch: [137][390/390]\tTime 0.003 (0.003)\tLoss 1.3640 (1.2016)\tPrec@1 53.750 (58.452)\n",
      "EPOCH: 137 train Results: Prec@1 58.452 Loss: 1.2016\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1436 (1.1436)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5461 (1.2751)\tPrec@1 37.500 (55.570)\n",
      "EPOCH: 137 val Results: Prec@1 55.570 Loss: 1.2751\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [138][0/390]\tTime 0.003 (0.003)\tLoss 1.2414 (1.2414)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [138][78/390]\tTime 0.004 (0.004)\tLoss 1.0855 (1.1618)\tPrec@1 67.188 (60.067)\n",
      "Epoch: [138][156/390]\tTime 0.004 (0.004)\tLoss 1.1934 (1.1787)\tPrec@1 58.594 (59.614)\n",
      "Epoch: [138][234/390]\tTime 0.007 (0.003)\tLoss 1.2739 (1.1892)\tPrec@1 58.594 (59.129)\n",
      "Epoch: [138][312/390]\tTime 0.006 (0.004)\tLoss 1.3400 (1.1972)\tPrec@1 50.781 (58.828)\n",
      "Epoch: [138][390/390]\tTime 0.001 (0.004)\tLoss 1.3267 (1.2027)\tPrec@1 57.500 (58.616)\n",
      "EPOCH: 138 train Results: Prec@1 58.616 Loss: 1.2027\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1210 (1.1210)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3186 (1.2709)\tPrec@1 43.750 (54.970)\n",
      "EPOCH: 138 val Results: Prec@1 54.970 Loss: 1.2709\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [139][0/390]\tTime 0.003 (0.003)\tLoss 1.1908 (1.1908)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [139][78/390]\tTime 0.002 (0.003)\tLoss 1.0084 (1.1504)\tPrec@1 67.188 (60.779)\n",
      "Epoch: [139][156/390]\tTime 0.003 (0.003)\tLoss 1.1461 (1.1738)\tPrec@1 64.062 (59.888)\n",
      "Epoch: [139][234/390]\tTime 0.006 (0.003)\tLoss 1.2245 (1.1853)\tPrec@1 59.375 (59.428)\n",
      "Epoch: [139][312/390]\tTime 0.013 (0.003)\tLoss 1.1918 (1.1925)\tPrec@1 57.031 (58.963)\n",
      "Epoch: [139][390/390]\tTime 0.003 (0.004)\tLoss 1.2161 (1.1992)\tPrec@1 58.750 (58.808)\n",
      "EPOCH: 139 train Results: Prec@1 58.808 Loss: 1.1992\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2067 (1.2067)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4348 (1.2769)\tPrec@1 37.500 (54.640)\n",
      "EPOCH: 139 val Results: Prec@1 54.640 Loss: 1.2769\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [140][0/390]\tTime 0.004 (0.004)\tLoss 1.1531 (1.1531)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [140][78/390]\tTime 0.004 (0.003)\tLoss 1.2319 (1.1481)\tPrec@1 55.469 (61.432)\n",
      "Epoch: [140][156/390]\tTime 0.002 (0.003)\tLoss 1.1432 (1.1664)\tPrec@1 57.031 (60.052)\n",
      "Epoch: [140][234/390]\tTime 0.004 (0.003)\tLoss 1.1078 (1.1767)\tPrec@1 59.375 (59.628)\n",
      "Epoch: [140][312/390]\tTime 0.004 (0.003)\tLoss 1.2915 (1.1870)\tPrec@1 53.125 (59.238)\n",
      "Epoch: [140][390/390]\tTime 0.002 (0.004)\tLoss 1.3592 (1.1969)\tPrec@1 51.250 (58.784)\n",
      "EPOCH: 140 train Results: Prec@1 58.784 Loss: 1.1969\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1624 (1.1624)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4059 (1.2808)\tPrec@1 37.500 (55.360)\n",
      "EPOCH: 140 val Results: Prec@1 55.360 Loss: 1.2808\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [141][0/390]\tTime 0.008 (0.008)\tLoss 0.9233 (0.9233)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [141][78/390]\tTime 0.003 (0.004)\tLoss 1.3136 (1.1546)\tPrec@1 56.250 (60.473)\n",
      "Epoch: [141][156/390]\tTime 0.009 (0.004)\tLoss 1.2111 (1.1758)\tPrec@1 53.906 (59.584)\n",
      "Epoch: [141][234/390]\tTime 0.004 (0.003)\tLoss 1.3117 (1.1855)\tPrec@1 52.344 (59.209)\n",
      "Epoch: [141][312/390]\tTime 0.002 (0.004)\tLoss 1.2038 (1.1942)\tPrec@1 59.375 (58.726)\n",
      "Epoch: [141][390/390]\tTime 0.003 (0.003)\tLoss 1.3467 (1.1976)\tPrec@1 52.500 (58.616)\n",
      "EPOCH: 141 train Results: Prec@1 58.616 Loss: 1.1976\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1825 (1.1825)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2663 (1.2805)\tPrec@1 62.500 (54.700)\n",
      "EPOCH: 141 val Results: Prec@1 54.700 Loss: 1.2805\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [142][0/390]\tTime 0.005 (0.005)\tLoss 1.1429 (1.1429)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [142][78/390]\tTime 0.002 (0.003)\tLoss 1.1858 (1.1500)\tPrec@1 61.719 (60.829)\n",
      "Epoch: [142][156/390]\tTime 0.002 (0.003)\tLoss 1.1325 (1.1707)\tPrec@1 59.375 (59.808)\n",
      "Epoch: [142][234/390]\tTime 0.002 (0.003)\tLoss 1.1850 (1.1776)\tPrec@1 60.156 (59.545)\n",
      "Epoch: [142][312/390]\tTime 0.002 (0.003)\tLoss 1.1936 (1.1897)\tPrec@1 59.375 (59.128)\n",
      "Epoch: [142][390/390]\tTime 0.002 (0.003)\tLoss 1.3134 (1.1980)\tPrec@1 57.500 (58.634)\n",
      "EPOCH: 142 train Results: Prec@1 58.634 Loss: 1.1980\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.2107 (1.2107)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1373 (1.2862)\tPrec@1 56.250 (55.210)\n",
      "EPOCH: 142 val Results: Prec@1 55.210 Loss: 1.2862\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [143][0/390]\tTime 0.003 (0.003)\tLoss 1.0445 (1.0445)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [143][78/390]\tTime 0.002 (0.003)\tLoss 1.1752 (1.1614)\tPrec@1 60.156 (61.056)\n",
      "Epoch: [143][156/390]\tTime 0.002 (0.003)\tLoss 1.2243 (1.1760)\tPrec@1 53.906 (60.281)\n",
      "Epoch: [143][234/390]\tTime 0.002 (0.003)\tLoss 1.1617 (1.1830)\tPrec@1 61.719 (59.774)\n",
      "Epoch: [143][312/390]\tTime 0.003 (0.003)\tLoss 1.2191 (1.1928)\tPrec@1 57.031 (59.258)\n",
      "Epoch: [143][390/390]\tTime 0.002 (0.003)\tLoss 1.1902 (1.1976)\tPrec@1 60.000 (59.002)\n",
      "EPOCH: 143 train Results: Prec@1 59.002 Loss: 1.1976\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1855 (1.1855)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1966 (1.2835)\tPrec@1 43.750 (55.100)\n",
      "EPOCH: 143 val Results: Prec@1 55.100 Loss: 1.2835\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [144][0/390]\tTime 0.002 (0.002)\tLoss 1.0974 (1.0974)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [144][78/390]\tTime 0.007 (0.003)\tLoss 1.2297 (1.1651)\tPrec@1 59.375 (60.275)\n",
      "Epoch: [144][156/390]\tTime 0.004 (0.003)\tLoss 1.3750 (1.1724)\tPrec@1 54.688 (60.017)\n",
      "Epoch: [144][234/390]\tTime 0.002 (0.003)\tLoss 1.1804 (1.1872)\tPrec@1 65.625 (59.435)\n",
      "Epoch: [144][312/390]\tTime 0.003 (0.003)\tLoss 1.1905 (1.1910)\tPrec@1 55.469 (59.170)\n",
      "Epoch: [144][390/390]\tTime 0.001 (0.003)\tLoss 1.2115 (1.1966)\tPrec@1 56.250 (58.924)\n",
      "EPOCH: 144 train Results: Prec@1 58.924 Loss: 1.1966\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1090 (1.1090)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6583 (1.2920)\tPrec@1 37.500 (54.010)\n",
      "EPOCH: 144 val Results: Prec@1 54.010 Loss: 1.2920\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [145][0/390]\tTime 0.003 (0.003)\tLoss 1.0880 (1.0880)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [145][78/390]\tTime 0.010 (0.003)\tLoss 1.0933 (1.1646)\tPrec@1 66.406 (60.235)\n",
      "Epoch: [145][156/390]\tTime 0.003 (0.003)\tLoss 1.1689 (1.1672)\tPrec@1 59.375 (60.121)\n",
      "Epoch: [145][234/390]\tTime 0.002 (0.003)\tLoss 1.1718 (1.1793)\tPrec@1 57.812 (59.694)\n",
      "Epoch: [145][312/390]\tTime 0.005 (0.003)\tLoss 1.2047 (1.1907)\tPrec@1 58.594 (58.996)\n",
      "Epoch: [145][390/390]\tTime 0.001 (0.003)\tLoss 1.1826 (1.1958)\tPrec@1 60.000 (58.834)\n",
      "EPOCH: 145 train Results: Prec@1 58.834 Loss: 1.1958\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1639 (1.1639)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5587 (1.2913)\tPrec@1 31.250 (54.570)\n",
      "EPOCH: 145 val Results: Prec@1 54.570 Loss: 1.2913\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [146][0/390]\tTime 0.006 (0.006)\tLoss 1.1676 (1.1676)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [146][78/390]\tTime 0.002 (0.003)\tLoss 1.1188 (1.1554)\tPrec@1 59.375 (60.324)\n",
      "Epoch: [146][156/390]\tTime 0.002 (0.003)\tLoss 1.0965 (1.1750)\tPrec@1 60.156 (59.644)\n",
      "Epoch: [146][234/390]\tTime 0.007 (0.003)\tLoss 1.0913 (1.1821)\tPrec@1 66.406 (59.385)\n",
      "Epoch: [146][312/390]\tTime 0.011 (0.003)\tLoss 1.1315 (1.1926)\tPrec@1 63.281 (59.080)\n",
      "Epoch: [146][390/390]\tTime 0.003 (0.003)\tLoss 1.0876 (1.1996)\tPrec@1 66.250 (58.666)\n",
      "EPOCH: 146 train Results: Prec@1 58.666 Loss: 1.1996\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1426 (1.1426)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2742 (1.2834)\tPrec@1 37.500 (55.030)\n",
      "EPOCH: 146 val Results: Prec@1 55.030 Loss: 1.2834\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [147][0/390]\tTime 0.003 (0.003)\tLoss 1.0627 (1.0627)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [147][78/390]\tTime 0.002 (0.003)\tLoss 1.1312 (1.1506)\tPrec@1 64.062 (60.502)\n",
      "Epoch: [147][156/390]\tTime 0.005 (0.006)\tLoss 1.2188 (1.1652)\tPrec@1 54.688 (59.763)\n",
      "Epoch: [147][234/390]\tTime 0.002 (0.006)\tLoss 1.2009 (1.1729)\tPrec@1 60.938 (59.491)\n",
      "Epoch: [147][312/390]\tTime 0.003 (0.005)\tLoss 1.2676 (1.1844)\tPrec@1 59.375 (59.093)\n",
      "Epoch: [147][390/390]\tTime 0.004 (0.005)\tLoss 1.2928 (1.1935)\tPrec@1 57.500 (58.766)\n",
      "EPOCH: 147 train Results: Prec@1 58.766 Loss: 1.1935\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1723 (1.1723)\tPrec@1 65.625 (65.625)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.6793 (1.2864)\tPrec@1 25.000 (54.680)\n",
      "EPOCH: 147 val Results: Prec@1 54.680 Loss: 1.2864\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [148][0/390]\tTime 0.003 (0.003)\tLoss 1.1767 (1.1767)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [148][78/390]\tTime 0.002 (0.003)\tLoss 1.2187 (1.1536)\tPrec@1 52.344 (60.700)\n",
      "Epoch: [148][156/390]\tTime 0.002 (0.003)\tLoss 1.2390 (1.1671)\tPrec@1 64.844 (60.057)\n",
      "Epoch: [148][234/390]\tTime 0.002 (0.004)\tLoss 1.2401 (1.1815)\tPrec@1 54.688 (59.348)\n",
      "Epoch: [148][312/390]\tTime 0.004 (0.004)\tLoss 1.0799 (1.1896)\tPrec@1 64.844 (59.008)\n",
      "Epoch: [148][390/390]\tTime 0.009 (0.004)\tLoss 1.1132 (1.1968)\tPrec@1 62.500 (58.700)\n",
      "EPOCH: 148 train Results: Prec@1 58.700 Loss: 1.1968\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2214 (1.2214)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4028 (1.2843)\tPrec@1 31.250 (54.990)\n",
      "EPOCH: 148 val Results: Prec@1 54.990 Loss: 1.2843\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [149][0/390]\tTime 0.004 (0.004)\tLoss 0.9784 (0.9784)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [149][78/390]\tTime 0.003 (0.004)\tLoss 1.2011 (1.1402)\tPrec@1 53.125 (61.333)\n",
      "Epoch: [149][156/390]\tTime 0.011 (0.004)\tLoss 1.1792 (1.1574)\tPrec@1 57.031 (60.181)\n",
      "Epoch: [149][234/390]\tTime 0.002 (0.004)\tLoss 1.0975 (1.1735)\tPrec@1 64.844 (59.428)\n",
      "Epoch: [149][312/390]\tTime 0.003 (0.004)\tLoss 1.1643 (1.1845)\tPrec@1 59.375 (59.083)\n",
      "Epoch: [149][390/390]\tTime 0.001 (0.004)\tLoss 1.2776 (1.1945)\tPrec@1 55.000 (58.656)\n",
      "EPOCH: 149 train Results: Prec@1 58.656 Loss: 1.1945\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1421 (1.1421)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4084 (1.2876)\tPrec@1 43.750 (54.450)\n",
      "EPOCH: 149 val Results: Prec@1 54.450 Loss: 1.2876\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [150][0/390]\tTime 0.003 (0.003)\tLoss 1.1954 (1.1954)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [150][78/390]\tTime 0.004 (0.005)\tLoss 1.2941 (1.1362)\tPrec@1 60.938 (61.600)\n",
      "Epoch: [150][156/390]\tTime 0.021 (0.018)\tLoss 1.2596 (1.1529)\tPrec@1 60.156 (60.410)\n",
      "Epoch: [150][234/390]\tTime 0.010 (0.018)\tLoss 1.3604 (1.1776)\tPrec@1 55.469 (59.368)\n",
      "Epoch: [150][312/390]\tTime 0.002 (0.019)\tLoss 1.2458 (1.1888)\tPrec@1 58.594 (58.891)\n",
      "Epoch: [150][390/390]\tTime 0.019 (0.022)\tLoss 1.2237 (1.1953)\tPrec@1 61.250 (58.572)\n",
      "EPOCH: 150 train Results: Prec@1 58.572 Loss: 1.1953\n",
      "Test: [0/78]\tTime 0.043 (0.043)\tLoss 1.1436 (1.1436)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.001 (0.008)\tLoss 1.4120 (1.2739)\tPrec@1 37.500 (55.110)\n",
      "EPOCH: 150 val Results: Prec@1 55.110 Loss: 1.2739\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [151][0/390]\tTime 0.030 (0.030)\tLoss 1.1144 (1.1144)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [151][78/390]\tTime 0.007 (0.024)\tLoss 1.2432 (1.1570)\tPrec@1 56.250 (60.473)\n",
      "Epoch: [151][156/390]\tTime 0.002 (0.016)\tLoss 1.2648 (1.1685)\tPrec@1 53.125 (59.838)\n",
      "Epoch: [151][234/390]\tTime 0.004 (0.012)\tLoss 1.1938 (1.1759)\tPrec@1 56.250 (59.422)\n",
      "Epoch: [151][312/390]\tTime 0.005 (0.011)\tLoss 1.2513 (1.1851)\tPrec@1 55.469 (59.038)\n",
      "Epoch: [151][390/390]\tTime 0.003 (0.009)\tLoss 1.2975 (1.1959)\tPrec@1 56.250 (58.576)\n",
      "EPOCH: 151 train Results: Prec@1 58.576 Loss: 1.1959\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1488 (1.1488)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.001 (0.001)\tLoss 1.3697 (1.2874)\tPrec@1 50.000 (54.760)\n",
      "EPOCH: 151 val Results: Prec@1 54.760 Loss: 1.2874\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [152][0/390]\tTime 0.006 (0.006)\tLoss 1.0341 (1.0341)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [152][78/390]\tTime 0.002 (0.006)\tLoss 1.1079 (1.1612)\tPrec@1 56.250 (60.107)\n",
      "Epoch: [152][156/390]\tTime 0.002 (0.005)\tLoss 1.1686 (1.1721)\tPrec@1 60.156 (60.022)\n",
      "Epoch: [152][234/390]\tTime 0.002 (0.005)\tLoss 1.2189 (1.1801)\tPrec@1 60.938 (59.621)\n",
      "Epoch: [152][312/390]\tTime 0.003 (0.005)\tLoss 1.2638 (1.1879)\tPrec@1 55.469 (59.240)\n",
      "Epoch: [152][390/390]\tTime 0.001 (0.005)\tLoss 1.0643 (1.1935)\tPrec@1 65.000 (59.004)\n",
      "EPOCH: 152 train Results: Prec@1 59.004 Loss: 1.1935\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1611 (1.1611)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3419 (1.2765)\tPrec@1 37.500 (54.870)\n",
      "EPOCH: 152 val Results: Prec@1 54.870 Loss: 1.2765\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [153][0/390]\tTime 0.004 (0.004)\tLoss 1.0755 (1.0755)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [153][78/390]\tTime 0.004 (0.004)\tLoss 1.0755 (1.1565)\tPrec@1 62.500 (60.572)\n",
      "Epoch: [153][156/390]\tTime 0.002 (0.004)\tLoss 1.1644 (1.1692)\tPrec@1 61.719 (59.977)\n",
      "Epoch: [153][234/390]\tTime 0.010 (0.004)\tLoss 1.0985 (1.1758)\tPrec@1 66.406 (59.697)\n",
      "Epoch: [153][312/390]\tTime 0.008 (0.005)\tLoss 1.1197 (1.1858)\tPrec@1 62.500 (59.165)\n",
      "Epoch: [153][390/390]\tTime 0.007 (0.004)\tLoss 1.2896 (1.1938)\tPrec@1 56.250 (58.714)\n",
      "EPOCH: 153 train Results: Prec@1 58.714 Loss: 1.1938\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1555 (1.1555)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4694 (1.2714)\tPrec@1 37.500 (55.090)\n",
      "EPOCH: 153 val Results: Prec@1 55.090 Loss: 1.2714\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [154][0/390]\tTime 0.002 (0.002)\tLoss 1.0731 (1.0731)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [154][78/390]\tTime 0.002 (0.003)\tLoss 1.1341 (1.1491)\tPrec@1 58.594 (61.017)\n",
      "Epoch: [154][156/390]\tTime 0.005 (0.003)\tLoss 1.2353 (1.1670)\tPrec@1 55.469 (60.121)\n",
      "Epoch: [154][234/390]\tTime 0.002 (0.003)\tLoss 1.2792 (1.1771)\tPrec@1 56.250 (59.485)\n",
      "Epoch: [154][312/390]\tTime 0.004 (0.003)\tLoss 1.2572 (1.1882)\tPrec@1 53.125 (59.073)\n",
      "Epoch: [154][390/390]\tTime 0.005 (0.004)\tLoss 1.3194 (1.1966)\tPrec@1 53.750 (58.690)\n",
      "EPOCH: 154 train Results: Prec@1 58.690 Loss: 1.1966\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1737 (1.1737)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4013 (1.2754)\tPrec@1 43.750 (54.870)\n",
      "EPOCH: 154 val Results: Prec@1 54.870 Loss: 1.2754\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [155][0/390]\tTime 0.002 (0.002)\tLoss 1.2421 (1.2421)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [155][78/390]\tTime 0.003 (0.004)\tLoss 1.2202 (1.1372)\tPrec@1 57.812 (61.214)\n",
      "Epoch: [155][156/390]\tTime 0.002 (0.004)\tLoss 1.1100 (1.1536)\tPrec@1 59.375 (60.340)\n",
      "Epoch: [155][234/390]\tTime 0.009 (0.004)\tLoss 1.2778 (1.1705)\tPrec@1 53.125 (59.432)\n",
      "Epoch: [155][312/390]\tTime 0.003 (0.004)\tLoss 1.2402 (1.1831)\tPrec@1 62.500 (59.048)\n",
      "Epoch: [155][390/390]\tTime 0.001 (0.004)\tLoss 1.0426 (1.1909)\tPrec@1 62.500 (58.742)\n",
      "EPOCH: 155 train Results: Prec@1 58.742 Loss: 1.1909\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1523 (1.1523)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5072 (1.2854)\tPrec@1 31.250 (54.500)\n",
      "EPOCH: 155 val Results: Prec@1 54.500 Loss: 1.2854\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [156][0/390]\tTime 0.003 (0.003)\tLoss 1.0753 (1.0753)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [156][78/390]\tTime 0.004 (0.003)\tLoss 1.1632 (1.1511)\tPrec@1 57.031 (60.463)\n",
      "Epoch: [156][156/390]\tTime 0.002 (0.003)\tLoss 1.3203 (1.1686)\tPrec@1 56.250 (59.957)\n",
      "Epoch: [156][234/390]\tTime 0.002 (0.003)\tLoss 1.1613 (1.1815)\tPrec@1 66.406 (59.342)\n",
      "Epoch: [156][312/390]\tTime 0.009 (0.003)\tLoss 1.3358 (1.1914)\tPrec@1 50.000 (59.058)\n",
      "Epoch: [156][390/390]\tTime 0.001 (0.003)\tLoss 1.3548 (1.1972)\tPrec@1 47.500 (58.764)\n",
      "EPOCH: 156 train Results: Prec@1 58.764 Loss: 1.1972\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1676 (1.1676)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6096 (1.2768)\tPrec@1 43.750 (55.280)\n",
      "EPOCH: 156 val Results: Prec@1 55.280 Loss: 1.2768\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [157][0/390]\tTime 0.003 (0.003)\tLoss 1.2566 (1.2566)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [157][78/390]\tTime 0.003 (0.003)\tLoss 1.0681 (1.1456)\tPrec@1 60.938 (61.224)\n",
      "Epoch: [157][156/390]\tTime 0.004 (0.003)\tLoss 1.1191 (1.1653)\tPrec@1 65.625 (60.276)\n",
      "Epoch: [157][234/390]\tTime 0.002 (0.003)\tLoss 1.1377 (1.1803)\tPrec@1 61.719 (59.408)\n",
      "Epoch: [157][312/390]\tTime 0.003 (0.003)\tLoss 1.2156 (1.1879)\tPrec@1 57.031 (59.001)\n",
      "Epoch: [157][390/390]\tTime 0.001 (0.003)\tLoss 1.2021 (1.1919)\tPrec@1 57.500 (58.852)\n",
      "EPOCH: 157 train Results: Prec@1 58.852 Loss: 1.1919\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1186 (1.1186)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3655 (1.2742)\tPrec@1 31.250 (55.370)\n",
      "EPOCH: 157 val Results: Prec@1 55.370 Loss: 1.2742\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [158][0/390]\tTime 0.004 (0.004)\tLoss 1.0466 (1.0466)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [158][78/390]\tTime 0.004 (0.003)\tLoss 1.2793 (1.1454)\tPrec@1 47.656 (60.690)\n",
      "Epoch: [158][156/390]\tTime 0.002 (0.003)\tLoss 1.2375 (1.1590)\tPrec@1 55.469 (59.987)\n",
      "Epoch: [158][234/390]\tTime 0.004 (0.003)\tLoss 1.2116 (1.1742)\tPrec@1 53.125 (59.422)\n",
      "Epoch: [158][312/390]\tTime 0.003 (0.004)\tLoss 1.1782 (1.1818)\tPrec@1 58.594 (59.185)\n",
      "Epoch: [158][390/390]\tTime 0.008 (0.004)\tLoss 1.1886 (1.1893)\tPrec@1 63.750 (58.938)\n",
      "EPOCH: 158 train Results: Prec@1 58.938 Loss: 1.1893\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1480 (1.1480)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4765 (1.2761)\tPrec@1 43.750 (54.920)\n",
      "EPOCH: 158 val Results: Prec@1 54.920 Loss: 1.2761\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [159][0/390]\tTime 0.005 (0.005)\tLoss 1.0335 (1.0335)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [159][78/390]\tTime 0.004 (0.004)\tLoss 1.1898 (1.1564)\tPrec@1 57.812 (60.255)\n",
      "Epoch: [159][156/390]\tTime 0.005 (0.003)\tLoss 1.0913 (1.1681)\tPrec@1 67.188 (59.728)\n",
      "Epoch: [159][234/390]\tTime 0.004 (0.003)\tLoss 1.0618 (1.1761)\tPrec@1 66.406 (59.448)\n",
      "Epoch: [159][312/390]\tTime 0.004 (0.003)\tLoss 1.1702 (1.1855)\tPrec@1 60.156 (59.120)\n",
      "Epoch: [159][390/390]\tTime 0.001 (0.003)\tLoss 1.2693 (1.1928)\tPrec@1 55.000 (58.806)\n",
      "EPOCH: 159 train Results: Prec@1 58.806 Loss: 1.1928\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1558 (1.1558)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4804 (1.2822)\tPrec@1 37.500 (54.930)\n",
      "EPOCH: 159 val Results: Prec@1 54.930 Loss: 1.2822\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [160][0/390]\tTime 0.004 (0.004)\tLoss 1.0515 (1.0515)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [160][78/390]\tTime 0.002 (0.005)\tLoss 1.2302 (1.1810)\tPrec@1 57.031 (59.632)\n",
      "Epoch: [160][156/390]\tTime 0.002 (0.004)\tLoss 1.2323 (1.1770)\tPrec@1 57.812 (59.460)\n",
      "Epoch: [160][234/390]\tTime 0.003 (0.004)\tLoss 1.1382 (1.1867)\tPrec@1 59.375 (58.973)\n",
      "Epoch: [160][312/390]\tTime 0.002 (0.004)\tLoss 1.1986 (1.1922)\tPrec@1 63.281 (58.791)\n",
      "Epoch: [160][390/390]\tTime 0.004 (0.004)\tLoss 1.1276 (1.1932)\tPrec@1 61.250 (58.780)\n",
      "EPOCH: 160 train Results: Prec@1 58.780 Loss: 1.1932\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1667 (1.1667)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2689 (1.2825)\tPrec@1 43.750 (55.350)\n",
      "EPOCH: 160 val Results: Prec@1 55.350 Loss: 1.2825\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [161][0/390]\tTime 0.003 (0.003)\tLoss 1.1817 (1.1817)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [161][78/390]\tTime 0.002 (0.003)\tLoss 1.0645 (1.1477)\tPrec@1 66.406 (60.384)\n",
      "Epoch: [161][156/390]\tTime 0.017 (0.003)\tLoss 1.1450 (1.1649)\tPrec@1 60.156 (59.733)\n",
      "Epoch: [161][234/390]\tTime 0.007 (0.003)\tLoss 1.2205 (1.1771)\tPrec@1 54.688 (59.275)\n",
      "Epoch: [161][312/390]\tTime 0.002 (0.003)\tLoss 1.2117 (1.1843)\tPrec@1 57.031 (59.073)\n",
      "Epoch: [161][390/390]\tTime 0.001 (0.003)\tLoss 1.4431 (1.1926)\tPrec@1 50.000 (58.640)\n",
      "EPOCH: 161 train Results: Prec@1 58.640 Loss: 1.1926\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1483 (1.1483)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4200 (1.2660)\tPrec@1 37.500 (55.410)\n",
      "EPOCH: 161 val Results: Prec@1 55.410 Loss: 1.2660\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [162][0/390]\tTime 0.003 (0.003)\tLoss 1.0160 (1.0160)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [162][78/390]\tTime 0.003 (0.004)\tLoss 1.1970 (1.1378)\tPrec@1 61.719 (60.750)\n",
      "Epoch: [162][156/390]\tTime 0.004 (0.004)\tLoss 1.0549 (1.1659)\tPrec@1 64.844 (59.743)\n",
      "Epoch: [162][234/390]\tTime 0.004 (0.004)\tLoss 1.1652 (1.1742)\tPrec@1 57.031 (59.528)\n",
      "Epoch: [162][312/390]\tTime 0.002 (0.004)\tLoss 1.1639 (1.1814)\tPrec@1 60.156 (59.285)\n",
      "Epoch: [162][390/390]\tTime 0.005 (0.004)\tLoss 1.2235 (1.1888)\tPrec@1 62.500 (58.930)\n",
      "EPOCH: 162 train Results: Prec@1 58.930 Loss: 1.1888\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1794 (1.1794)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6512 (1.2870)\tPrec@1 31.250 (54.340)\n",
      "EPOCH: 162 val Results: Prec@1 54.340 Loss: 1.2870\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [163][0/390]\tTime 0.005 (0.005)\tLoss 1.2740 (1.2740)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [163][78/390]\tTime 0.007 (0.003)\tLoss 1.0837 (1.1555)\tPrec@1 61.719 (60.572)\n",
      "Epoch: [163][156/390]\tTime 0.002 (0.003)\tLoss 1.1445 (1.1623)\tPrec@1 60.938 (60.221)\n",
      "Epoch: [163][234/390]\tTime 0.003 (0.003)\tLoss 1.1437 (1.1789)\tPrec@1 63.281 (59.508)\n",
      "Epoch: [163][312/390]\tTime 0.004 (0.004)\tLoss 1.1777 (1.1895)\tPrec@1 64.062 (59.008)\n",
      "Epoch: [163][390/390]\tTime 0.001 (0.003)\tLoss 1.2645 (1.1946)\tPrec@1 52.500 (58.840)\n",
      "EPOCH: 163 train Results: Prec@1 58.840 Loss: 1.1946\n",
      "Test: [0/78]\tTime 0.008 (0.008)\tLoss 1.1625 (1.1625)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3672 (1.2654)\tPrec@1 43.750 (55.400)\n",
      "EPOCH: 163 val Results: Prec@1 55.400 Loss: 1.2654\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [164][0/390]\tTime 0.002 (0.002)\tLoss 1.1930 (1.1930)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [164][78/390]\tTime 0.002 (0.004)\tLoss 1.1718 (1.1264)\tPrec@1 60.938 (61.679)\n",
      "Epoch: [164][156/390]\tTime 0.002 (0.003)\tLoss 1.2481 (1.1551)\tPrec@1 50.781 (60.534)\n",
      "Epoch: [164][234/390]\tTime 0.004 (0.003)\tLoss 1.2326 (1.1660)\tPrec@1 52.344 (60.013)\n",
      "Epoch: [164][312/390]\tTime 0.005 (0.003)\tLoss 1.2996 (1.1800)\tPrec@1 53.906 (59.330)\n",
      "Epoch: [164][390/390]\tTime 0.003 (0.003)\tLoss 1.1835 (1.1868)\tPrec@1 61.250 (59.000)\n",
      "EPOCH: 164 train Results: Prec@1 59.000 Loss: 1.1868\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1114 (1.1114)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4176 (1.2739)\tPrec@1 25.000 (55.570)\n",
      "EPOCH: 164 val Results: Prec@1 55.570 Loss: 1.2739\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [165][0/390]\tTime 0.004 (0.004)\tLoss 1.2400 (1.2400)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [165][78/390]\tTime 0.004 (0.003)\tLoss 1.0661 (1.1225)\tPrec@1 62.500 (61.808)\n",
      "Epoch: [165][156/390]\tTime 0.002 (0.003)\tLoss 1.1721 (1.1532)\tPrec@1 56.250 (60.460)\n",
      "Epoch: [165][234/390]\tTime 0.004 (0.003)\tLoss 1.3053 (1.1653)\tPrec@1 57.812 (60.106)\n",
      "Epoch: [165][312/390]\tTime 0.003 (0.003)\tLoss 1.2192 (1.1782)\tPrec@1 61.719 (59.432)\n",
      "Epoch: [165][390/390]\tTime 0.007 (0.003)\tLoss 1.4007 (1.1866)\tPrec@1 52.500 (59.018)\n",
      "EPOCH: 165 train Results: Prec@1 59.018 Loss: 1.1866\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1838 (1.1838)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3316 (1.2758)\tPrec@1 50.000 (54.990)\n",
      "EPOCH: 165 val Results: Prec@1 54.990 Loss: 1.2758\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [166][0/390]\tTime 0.011 (0.011)\tLoss 1.1995 (1.1995)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [166][78/390]\tTime 0.002 (0.003)\tLoss 1.0545 (1.1315)\tPrec@1 70.312 (61.689)\n",
      "Epoch: [166][156/390]\tTime 0.029 (0.004)\tLoss 1.2369 (1.1542)\tPrec@1 57.812 (60.435)\n",
      "Epoch: [166][234/390]\tTime 0.003 (0.004)\tLoss 1.2350 (1.1696)\tPrec@1 58.594 (59.814)\n",
      "Epoch: [166][312/390]\tTime 0.005 (0.004)\tLoss 1.2414 (1.1765)\tPrec@1 55.469 (59.560)\n",
      "Epoch: [166][390/390]\tTime 0.003 (0.004)\tLoss 1.1800 (1.1866)\tPrec@1 55.000 (59.136)\n",
      "EPOCH: 166 train Results: Prec@1 59.136 Loss: 1.1866\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1790 (1.1790)\tPrec@1 55.469 (55.469)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3142 (1.2884)\tPrec@1 43.750 (54.670)\n",
      "EPOCH: 166 val Results: Prec@1 54.670 Loss: 1.2884\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [167][0/390]\tTime 0.004 (0.004)\tLoss 1.2531 (1.2531)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [167][78/390]\tTime 0.003 (0.004)\tLoss 1.2523 (1.1505)\tPrec@1 53.906 (61.135)\n",
      "Epoch: [167][156/390]\tTime 0.002 (0.003)\tLoss 1.1063 (1.1648)\tPrec@1 64.062 (60.057)\n",
      "Epoch: [167][234/390]\tTime 0.003 (0.003)\tLoss 1.1361 (1.1798)\tPrec@1 57.031 (59.305)\n",
      "Epoch: [167][312/390]\tTime 0.014 (0.003)\tLoss 1.3370 (1.1830)\tPrec@1 53.125 (59.213)\n",
      "Epoch: [167][390/390]\tTime 0.002 (0.003)\tLoss 1.2811 (1.1909)\tPrec@1 57.500 (58.886)\n",
      "EPOCH: 167 train Results: Prec@1 58.886 Loss: 1.1909\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1374 (1.1374)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3367 (1.2785)\tPrec@1 37.500 (54.930)\n",
      "EPOCH: 167 val Results: Prec@1 54.930 Loss: 1.2785\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [168][0/390]\tTime 0.002 (0.002)\tLoss 1.0900 (1.0900)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [168][78/390]\tTime 0.002 (0.003)\tLoss 1.0113 (1.1345)\tPrec@1 72.656 (61.185)\n",
      "Epoch: [168][156/390]\tTime 0.006 (0.003)\tLoss 1.1099 (1.1629)\tPrec@1 61.719 (59.922)\n",
      "Epoch: [168][234/390]\tTime 0.002 (0.003)\tLoss 1.1578 (1.1756)\tPrec@1 60.156 (59.511)\n",
      "Epoch: [168][312/390]\tTime 0.002 (0.003)\tLoss 1.1125 (1.1850)\tPrec@1 60.156 (59.113)\n",
      "Epoch: [168][390/390]\tTime 0.004 (0.003)\tLoss 1.1935 (1.1918)\tPrec@1 52.500 (58.888)\n",
      "EPOCH: 168 train Results: Prec@1 58.888 Loss: 1.1918\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1294 (1.1294)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.7179 (1.2767)\tPrec@1 25.000 (55.140)\n",
      "EPOCH: 168 val Results: Prec@1 55.140 Loss: 1.2767\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [169][0/390]\tTime 0.002 (0.002)\tLoss 1.1293 (1.1293)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [169][78/390]\tTime 0.002 (0.003)\tLoss 1.1488 (1.1508)\tPrec@1 62.500 (60.730)\n",
      "Epoch: [169][156/390]\tTime 0.004 (0.003)\tLoss 1.0910 (1.1581)\tPrec@1 60.156 (60.241)\n",
      "Epoch: [169][234/390]\tTime 0.002 (0.004)\tLoss 1.1356 (1.1697)\tPrec@1 62.500 (59.674)\n",
      "Epoch: [169][312/390]\tTime 0.006 (0.004)\tLoss 1.1932 (1.1792)\tPrec@1 60.938 (59.260)\n",
      "Epoch: [169][390/390]\tTime 0.001 (0.004)\tLoss 1.2392 (1.1854)\tPrec@1 63.750 (59.070)\n",
      "EPOCH: 169 train Results: Prec@1 59.070 Loss: 1.1854\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1510 (1.1510)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3839 (1.2817)\tPrec@1 43.750 (54.990)\n",
      "EPOCH: 169 val Results: Prec@1 54.990 Loss: 1.2817\n",
      "Best Prec@1: 55.570\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [170][0/390]\tTime 0.004 (0.004)\tLoss 1.1337 (1.1337)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [170][78/390]\tTime 0.002 (0.003)\tLoss 1.1711 (1.1463)\tPrec@1 57.812 (60.997)\n",
      "Epoch: [170][156/390]\tTime 0.004 (0.003)\tLoss 1.0328 (1.1620)\tPrec@1 66.406 (59.843)\n",
      "Epoch: [170][234/390]\tTime 0.005 (0.003)\tLoss 1.1765 (1.1743)\tPrec@1 57.031 (59.295)\n",
      "Epoch: [170][312/390]\tTime 0.002 (0.003)\tLoss 1.1460 (1.1848)\tPrec@1 62.500 (58.903)\n",
      "Epoch: [170][390/390]\tTime 0.001 (0.003)\tLoss 1.2034 (1.1887)\tPrec@1 55.000 (58.770)\n",
      "EPOCH: 170 train Results: Prec@1 58.770 Loss: 1.1887\n",
      "Test: [0/78]\tTime 0.006 (0.006)\tLoss 1.1366 (1.1366)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.2424 (1.2698)\tPrec@1 37.500 (55.620)\n",
      "EPOCH: 170 val Results: Prec@1 55.620 Loss: 1.2698\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [171][0/390]\tTime 0.005 (0.005)\tLoss 1.0710 (1.0710)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [171][78/390]\tTime 0.007 (0.003)\tLoss 1.1086 (1.1417)\tPrec@1 65.625 (60.789)\n",
      "Epoch: [171][156/390]\tTime 0.002 (0.003)\tLoss 1.2235 (1.1607)\tPrec@1 59.375 (60.062)\n",
      "Epoch: [171][234/390]\tTime 0.002 (0.003)\tLoss 1.1266 (1.1710)\tPrec@1 65.625 (59.787)\n",
      "Epoch: [171][312/390]\tTime 0.002 (0.003)\tLoss 1.1562 (1.1767)\tPrec@1 63.281 (59.512)\n",
      "Epoch: [171][390/390]\tTime 0.003 (0.003)\tLoss 1.3606 (1.1849)\tPrec@1 52.500 (58.966)\n",
      "EPOCH: 171 train Results: Prec@1 58.966 Loss: 1.1849\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1902 (1.1902)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3257 (1.2744)\tPrec@1 31.250 (54.970)\n",
      "EPOCH: 171 val Results: Prec@1 54.970 Loss: 1.2744\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [172][0/390]\tTime 0.003 (0.003)\tLoss 0.9601 (0.9601)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [172][78/390]\tTime 0.002 (0.004)\tLoss 1.1697 (1.1404)\tPrec@1 57.812 (61.027)\n",
      "Epoch: [172][156/390]\tTime 0.002 (0.004)\tLoss 1.1050 (1.1572)\tPrec@1 58.594 (60.320)\n",
      "Epoch: [172][234/390]\tTime 0.002 (0.003)\tLoss 1.1808 (1.1710)\tPrec@1 57.812 (59.894)\n",
      "Epoch: [172][312/390]\tTime 0.002 (0.003)\tLoss 1.3384 (1.1827)\tPrec@1 53.125 (59.250)\n",
      "Epoch: [172][390/390]\tTime 0.003 (0.003)\tLoss 1.2105 (1.1892)\tPrec@1 61.250 (59.094)\n",
      "EPOCH: 172 train Results: Prec@1 59.094 Loss: 1.1892\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1956 (1.1956)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5681 (1.2741)\tPrec@1 43.750 (55.080)\n",
      "EPOCH: 172 val Results: Prec@1 55.080 Loss: 1.2741\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [173][0/390]\tTime 0.005 (0.005)\tLoss 1.1366 (1.1366)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [173][78/390]\tTime 0.004 (0.003)\tLoss 1.2871 (1.1532)\tPrec@1 51.562 (60.522)\n",
      "Epoch: [173][156/390]\tTime 0.002 (0.004)\tLoss 1.2466 (1.1660)\tPrec@1 58.594 (59.763)\n",
      "Epoch: [173][234/390]\tTime 0.002 (0.005)\tLoss 1.1574 (1.1698)\tPrec@1 63.281 (59.751)\n",
      "Epoch: [173][312/390]\tTime 0.002 (0.004)\tLoss 1.1480 (1.1791)\tPrec@1 60.156 (59.290)\n",
      "Epoch: [173][390/390]\tTime 0.004 (0.004)\tLoss 1.1233 (1.1870)\tPrec@1 58.750 (59.000)\n",
      "EPOCH: 173 train Results: Prec@1 59.000 Loss: 1.1870\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1353 (1.1353)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4368 (1.2756)\tPrec@1 43.750 (54.780)\n",
      "EPOCH: 173 val Results: Prec@1 54.780 Loss: 1.2756\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [174][0/390]\tTime 0.004 (0.004)\tLoss 1.1733 (1.1733)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [174][78/390]\tTime 0.004 (0.004)\tLoss 1.0670 (1.1437)\tPrec@1 63.281 (60.473)\n",
      "Epoch: [174][156/390]\tTime 0.002 (0.004)\tLoss 1.0978 (1.1643)\tPrec@1 63.281 (59.708)\n",
      "Epoch: [174][234/390]\tTime 0.002 (0.004)\tLoss 1.1848 (1.1712)\tPrec@1 64.844 (59.654)\n",
      "Epoch: [174][312/390]\tTime 0.003 (0.004)\tLoss 1.2141 (1.1836)\tPrec@1 54.688 (59.163)\n",
      "Epoch: [174][390/390]\tTime 0.002 (0.004)\tLoss 1.1937 (1.1925)\tPrec@1 56.250 (58.700)\n",
      "EPOCH: 174 train Results: Prec@1 58.700 Loss: 1.1925\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1529 (1.1529)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4807 (1.2747)\tPrec@1 37.500 (55.050)\n",
      "EPOCH: 174 val Results: Prec@1 55.050 Loss: 1.2747\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [175][0/390]\tTime 0.003 (0.003)\tLoss 1.1025 (1.1025)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [175][78/390]\tTime 0.005 (0.003)\tLoss 1.1936 (1.1363)\tPrec@1 55.469 (61.313)\n",
      "Epoch: [175][156/390]\tTime 0.002 (0.003)\tLoss 1.1947 (1.1536)\tPrec@1 60.156 (60.380)\n",
      "Epoch: [175][234/390]\tTime 0.002 (0.004)\tLoss 1.2693 (1.1667)\tPrec@1 57.031 (59.830)\n",
      "Epoch: [175][312/390]\tTime 0.002 (0.003)\tLoss 1.3023 (1.1769)\tPrec@1 55.469 (59.540)\n",
      "Epoch: [175][390/390]\tTime 0.001 (0.003)\tLoss 1.2722 (1.1872)\tPrec@1 52.500 (59.084)\n",
      "EPOCH: 175 train Results: Prec@1 59.084 Loss: 1.1872\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2127 (1.2127)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3512 (1.2782)\tPrec@1 50.000 (55.100)\n",
      "EPOCH: 175 val Results: Prec@1 55.100 Loss: 1.2782\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [176][0/390]\tTime 0.004 (0.004)\tLoss 1.1329 (1.1329)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [176][78/390]\tTime 0.004 (0.003)\tLoss 1.3300 (1.1538)\tPrec@1 54.688 (60.522)\n",
      "Epoch: [176][156/390]\tTime 0.002 (0.003)\tLoss 1.1466 (1.1666)\tPrec@1 60.156 (60.062)\n",
      "Epoch: [176][234/390]\tTime 0.008 (0.003)\tLoss 1.1640 (1.1731)\tPrec@1 59.375 (59.601)\n",
      "Epoch: [176][312/390]\tTime 0.002 (0.003)\tLoss 1.1271 (1.1797)\tPrec@1 65.625 (59.365)\n",
      "Epoch: [176][390/390]\tTime 0.006 (0.003)\tLoss 1.2139 (1.1865)\tPrec@1 51.250 (59.130)\n",
      "EPOCH: 176 train Results: Prec@1 59.130 Loss: 1.1865\n",
      "Test: [0/78]\tTime 0.020 (0.020)\tLoss 1.1466 (1.1466)\tPrec@1 66.406 (66.406)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3977 (1.2783)\tPrec@1 43.750 (54.850)\n",
      "EPOCH: 176 val Results: Prec@1 54.850 Loss: 1.2783\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [177][0/390]\tTime 0.005 (0.005)\tLoss 1.2694 (1.2694)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [177][78/390]\tTime 0.002 (0.003)\tLoss 1.0336 (1.1415)\tPrec@1 64.844 (61.135)\n",
      "Epoch: [177][156/390]\tTime 0.003 (0.004)\tLoss 1.1800 (1.1645)\tPrec@1 60.938 (60.047)\n",
      "Epoch: [177][234/390]\tTime 0.002 (0.004)\tLoss 1.2029 (1.1726)\tPrec@1 59.375 (59.684)\n",
      "Epoch: [177][312/390]\tTime 0.002 (0.004)\tLoss 1.1664 (1.1795)\tPrec@1 60.156 (59.415)\n",
      "Epoch: [177][390/390]\tTime 0.002 (0.003)\tLoss 1.2101 (1.1847)\tPrec@1 62.500 (59.182)\n",
      "EPOCH: 177 train Results: Prec@1 59.182 Loss: 1.1847\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2016 (1.2016)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.6524 (1.2705)\tPrec@1 31.250 (55.580)\n",
      "EPOCH: 177 val Results: Prec@1 55.580 Loss: 1.2705\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [178][0/390]\tTime 0.004 (0.004)\tLoss 1.2041 (1.2041)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [178][78/390]\tTime 0.002 (0.003)\tLoss 1.0893 (1.1432)\tPrec@1 63.281 (60.740)\n",
      "Epoch: [178][156/390]\tTime 0.002 (0.003)\tLoss 1.1617 (1.1632)\tPrec@1 62.500 (60.136)\n",
      "Epoch: [178][234/390]\tTime 0.005 (0.003)\tLoss 1.2647 (1.1767)\tPrec@1 56.250 (59.239)\n",
      "Epoch: [178][312/390]\tTime 0.004 (0.003)\tLoss 1.2585 (1.1827)\tPrec@1 53.125 (59.155)\n",
      "Epoch: [178][390/390]\tTime 0.001 (0.003)\tLoss 1.2672 (1.1881)\tPrec@1 55.000 (58.912)\n",
      "EPOCH: 178 train Results: Prec@1 58.912 Loss: 1.1881\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1578 (1.1578)\tPrec@1 62.500 (62.500)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3895 (1.2777)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 178 val Results: Prec@1 55.150 Loss: 1.2777\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [179][0/390]\tTime 0.004 (0.004)\tLoss 1.2557 (1.2557)\tPrec@1 53.906 (53.906)\n",
      "Epoch: [179][78/390]\tTime 0.008 (0.003)\tLoss 1.3219 (1.1349)\tPrec@1 51.562 (61.531)\n",
      "Epoch: [179][156/390]\tTime 0.012 (0.004)\tLoss 1.2202 (1.1589)\tPrec@1 58.594 (60.226)\n",
      "Epoch: [179][234/390]\tTime 0.023 (0.004)\tLoss 1.2422 (1.1761)\tPrec@1 56.250 (59.564)\n",
      "Epoch: [179][312/390]\tTime 0.003 (0.004)\tLoss 1.0418 (1.1824)\tPrec@1 63.281 (59.275)\n",
      "Epoch: [179][390/390]\tTime 0.003 (0.004)\tLoss 1.3483 (1.1901)\tPrec@1 57.500 (58.912)\n",
      "EPOCH: 179 train Results: Prec@1 58.912 Loss: 1.1901\n",
      "Test: [0/78]\tTime 0.000 (0.000)\tLoss 1.1777 (1.1777)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3544 (1.2788)\tPrec@1 43.750 (54.710)\n",
      "EPOCH: 179 val Results: Prec@1 54.710 Loss: 1.2788\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [180][0/390]\tTime 0.003 (0.003)\tLoss 1.2058 (1.2058)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [180][78/390]\tTime 0.002 (0.003)\tLoss 1.1382 (1.1525)\tPrec@1 65.625 (60.127)\n",
      "Epoch: [180][156/390]\tTime 0.004 (0.003)\tLoss 1.1497 (1.1606)\tPrec@1 59.375 (60.126)\n",
      "Epoch: [180][234/390]\tTime 0.002 (0.003)\tLoss 1.1735 (1.1727)\tPrec@1 56.250 (59.581)\n",
      "Epoch: [180][312/390]\tTime 0.004 (0.003)\tLoss 1.0777 (1.1823)\tPrec@1 61.719 (59.233)\n",
      "Epoch: [180][390/390]\tTime 0.001 (0.003)\tLoss 1.2923 (1.1888)\tPrec@1 52.500 (58.884)\n",
      "EPOCH: 180 train Results: Prec@1 58.884 Loss: 1.1888\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1527 (1.1527)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3929 (1.2741)\tPrec@1 43.750 (54.630)\n",
      "EPOCH: 180 val Results: Prec@1 54.630 Loss: 1.2741\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [181][0/390]\tTime 0.004 (0.004)\tLoss 1.1340 (1.1340)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [181][78/390]\tTime 0.004 (0.004)\tLoss 1.1405 (1.1289)\tPrec@1 63.281 (60.918)\n",
      "Epoch: [181][156/390]\tTime 0.002 (0.003)\tLoss 1.0958 (1.1540)\tPrec@1 60.156 (59.888)\n",
      "Epoch: [181][234/390]\tTime 0.002 (0.003)\tLoss 1.1981 (1.1680)\tPrec@1 64.062 (59.485)\n",
      "Epoch: [181][312/390]\tTime 0.004 (0.003)\tLoss 1.2110 (1.1765)\tPrec@1 57.812 (59.210)\n",
      "Epoch: [181][390/390]\tTime 0.003 (0.003)\tLoss 1.3059 (1.1830)\tPrec@1 48.750 (58.988)\n",
      "EPOCH: 181 train Results: Prec@1 58.988 Loss: 1.1830\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1341 (1.1341)\tPrec@1 64.844 (64.844)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4692 (1.2746)\tPrec@1 31.250 (55.250)\n",
      "EPOCH: 181 val Results: Prec@1 55.250 Loss: 1.2746\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [182][0/390]\tTime 0.002 (0.002)\tLoss 1.1806 (1.1806)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [182][78/390]\tTime 0.002 (0.004)\tLoss 1.2954 (1.1409)\tPrec@1 50.781 (60.769)\n",
      "Epoch: [182][156/390]\tTime 0.002 (0.004)\tLoss 1.0875 (1.1616)\tPrec@1 68.750 (59.932)\n",
      "Epoch: [182][234/390]\tTime 0.002 (0.004)\tLoss 1.3600 (1.1768)\tPrec@1 50.781 (59.348)\n",
      "Epoch: [182][312/390]\tTime 0.004 (0.003)\tLoss 1.1777 (1.1821)\tPrec@1 60.156 (59.265)\n",
      "Epoch: [182][390/390]\tTime 0.004 (0.003)\tLoss 1.2742 (1.1878)\tPrec@1 57.500 (59.002)\n",
      "EPOCH: 182 train Results: Prec@1 59.002 Loss: 1.1878\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1682 (1.1682)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5756 (1.2670)\tPrec@1 37.500 (55.090)\n",
      "EPOCH: 182 val Results: Prec@1 55.090 Loss: 1.2670\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [183][0/390]\tTime 0.004 (0.004)\tLoss 1.0906 (1.0906)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [183][78/390]\tTime 0.003 (0.003)\tLoss 1.0995 (1.1354)\tPrec@1 57.812 (60.799)\n",
      "Epoch: [183][156/390]\tTime 0.002 (0.004)\tLoss 1.1438 (1.1526)\tPrec@1 61.719 (60.216)\n",
      "Epoch: [183][234/390]\tTime 0.005 (0.004)\tLoss 1.1596 (1.1663)\tPrec@1 60.156 (59.688)\n",
      "Epoch: [183][312/390]\tTime 0.003 (0.004)\tLoss 1.1696 (1.1752)\tPrec@1 55.469 (59.163)\n",
      "Epoch: [183][390/390]\tTime 0.001 (0.004)\tLoss 1.3079 (1.1866)\tPrec@1 55.000 (58.774)\n",
      "EPOCH: 183 train Results: Prec@1 58.774 Loss: 1.1866\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1939 (1.1939)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5760 (1.2778)\tPrec@1 25.000 (54.980)\n",
      "EPOCH: 183 val Results: Prec@1 54.980 Loss: 1.2778\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [184][0/390]\tTime 0.008 (0.008)\tLoss 1.0511 (1.0511)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [184][78/390]\tTime 0.003 (0.004)\tLoss 1.1843 (1.1337)\tPrec@1 60.156 (61.066)\n",
      "Epoch: [184][156/390]\tTime 0.002 (0.004)\tLoss 1.2084 (1.1511)\tPrec@1 56.250 (60.425)\n",
      "Epoch: [184][234/390]\tTime 0.004 (0.004)\tLoss 1.2891 (1.1701)\tPrec@1 51.562 (59.767)\n",
      "Epoch: [184][312/390]\tTime 0.002 (0.005)\tLoss 1.2418 (1.1811)\tPrec@1 53.906 (59.220)\n",
      "Epoch: [184][390/390]\tTime 0.001 (0.004)\tLoss 1.3133 (1.1863)\tPrec@1 53.750 (59.068)\n",
      "EPOCH: 184 train Results: Prec@1 59.068 Loss: 1.1863\n",
      "Test: [0/78]\tTime 0.016 (0.016)\tLoss 1.1310 (1.1310)\tPrec@1 57.812 (57.812)\n",
      "Test: [78/78]\tTime 0.002 (0.001)\tLoss 1.3143 (1.2731)\tPrec@1 43.750 (54.700)\n",
      "EPOCH: 184 val Results: Prec@1 54.700 Loss: 1.2731\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [185][0/390]\tTime 0.003 (0.003)\tLoss 1.0912 (1.0912)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [185][78/390]\tTime 0.003 (0.004)\tLoss 1.2724 (1.1447)\tPrec@1 57.812 (60.799)\n",
      "Epoch: [185][156/390]\tTime 0.006 (0.004)\tLoss 1.1114 (1.1550)\tPrec@1 66.406 (60.012)\n",
      "Epoch: [185][234/390]\tTime 0.002 (0.004)\tLoss 1.3293 (1.1603)\tPrec@1 56.250 (60.020)\n",
      "Epoch: [185][312/390]\tTime 0.002 (0.004)\tLoss 1.0659 (1.1692)\tPrec@1 64.062 (59.565)\n",
      "Epoch: [185][390/390]\tTime 0.003 (0.004)\tLoss 1.3208 (1.1844)\tPrec@1 50.000 (58.924)\n",
      "EPOCH: 185 train Results: Prec@1 58.924 Loss: 1.1844\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1570 (1.1570)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3113 (1.2853)\tPrec@1 37.500 (54.880)\n",
      "EPOCH: 185 val Results: Prec@1 54.880 Loss: 1.2853\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [186][0/390]\tTime 0.004 (0.004)\tLoss 1.1774 (1.1774)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [186][78/390]\tTime 0.002 (0.004)\tLoss 1.1306 (1.1412)\tPrec@1 63.281 (61.116)\n",
      "Epoch: [186][156/390]\tTime 0.003 (0.004)\tLoss 1.1596 (1.1604)\tPrec@1 57.812 (60.047)\n",
      "Epoch: [186][234/390]\tTime 0.002 (0.004)\tLoss 1.2635 (1.1711)\tPrec@1 57.812 (59.658)\n",
      "Epoch: [186][312/390]\tTime 0.002 (0.004)\tLoss 1.1492 (1.1810)\tPrec@1 59.375 (59.193)\n",
      "Epoch: [186][390/390]\tTime 0.003 (0.004)\tLoss 1.0818 (1.1853)\tPrec@1 65.000 (58.970)\n",
      "EPOCH: 186 train Results: Prec@1 58.970 Loss: 1.1853\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1206 (1.1206)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3919 (1.2799)\tPrec@1 56.250 (54.910)\n",
      "EPOCH: 186 val Results: Prec@1 54.910 Loss: 1.2799\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [187][0/390]\tTime 0.003 (0.003)\tLoss 1.2402 (1.2402)\tPrec@1 57.031 (57.031)\n",
      "Epoch: [187][78/390]\tTime 0.003 (0.003)\tLoss 1.0992 (1.1297)\tPrec@1 63.281 (61.363)\n",
      "Epoch: [187][156/390]\tTime 0.002 (0.003)\tLoss 1.3337 (1.1508)\tPrec@1 55.469 (60.524)\n",
      "Epoch: [187][234/390]\tTime 0.008 (0.003)\tLoss 1.2727 (1.1643)\tPrec@1 53.906 (59.860)\n",
      "Epoch: [187][312/390]\tTime 0.002 (0.003)\tLoss 1.2941 (1.1780)\tPrec@1 60.156 (59.185)\n",
      "Epoch: [187][390/390]\tTime 0.003 (0.003)\tLoss 1.3519 (1.1872)\tPrec@1 60.000 (58.824)\n",
      "EPOCH: 187 train Results: Prec@1 58.824 Loss: 1.1872\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2061 (1.2061)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4775 (1.2677)\tPrec@1 43.750 (55.250)\n",
      "EPOCH: 187 val Results: Prec@1 55.250 Loss: 1.2677\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [188][0/390]\tTime 0.008 (0.008)\tLoss 1.0771 (1.0771)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [188][78/390]\tTime 0.007 (0.004)\tLoss 1.3635 (1.1380)\tPrec@1 53.125 (61.096)\n",
      "Epoch: [188][156/390]\tTime 0.009 (0.006)\tLoss 1.1611 (1.1540)\tPrec@1 57.812 (60.216)\n",
      "Epoch: [188][234/390]\tTime 0.002 (0.005)\tLoss 1.2497 (1.1641)\tPrec@1 58.594 (59.827)\n",
      "Epoch: [188][312/390]\tTime 0.005 (0.005)\tLoss 1.2466 (1.1740)\tPrec@1 57.031 (59.425)\n",
      "Epoch: [188][390/390]\tTime 0.002 (0.005)\tLoss 1.2856 (1.1848)\tPrec@1 53.750 (59.020)\n",
      "EPOCH: 188 train Results: Prec@1 59.020 Loss: 1.1848\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1777 (1.1777)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3740 (1.2697)\tPrec@1 43.750 (55.320)\n",
      "EPOCH: 188 val Results: Prec@1 55.320 Loss: 1.2697\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [189][0/390]\tTime 0.012 (0.012)\tLoss 1.2466 (1.2466)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [189][78/390]\tTime 0.002 (0.005)\tLoss 1.0638 (1.1288)\tPrec@1 64.062 (61.343)\n",
      "Epoch: [189][156/390]\tTime 0.002 (0.005)\tLoss 1.3282 (1.1527)\tPrec@1 57.812 (60.395)\n",
      "Epoch: [189][234/390]\tTime 0.005 (0.004)\tLoss 1.2160 (1.1628)\tPrec@1 53.906 (59.900)\n",
      "Epoch: [189][312/390]\tTime 0.002 (0.004)\tLoss 1.2448 (1.1743)\tPrec@1 54.688 (59.235)\n",
      "Epoch: [189][390/390]\tTime 0.015 (0.004)\tLoss 1.3231 (1.1841)\tPrec@1 53.750 (58.962)\n",
      "EPOCH: 189 train Results: Prec@1 58.962 Loss: 1.1841\n",
      "Test: [0/78]\tTime 0.005 (0.005)\tLoss 1.1686 (1.1686)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3144 (1.2713)\tPrec@1 43.750 (54.880)\n",
      "EPOCH: 189 val Results: Prec@1 54.880 Loss: 1.2713\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [190][0/390]\tTime 0.003 (0.003)\tLoss 1.0743 (1.0743)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [190][78/390]\tTime 0.003 (0.007)\tLoss 1.1130 (1.1385)\tPrec@1 61.719 (61.165)\n",
      "Epoch: [190][156/390]\tTime 0.002 (0.007)\tLoss 1.2271 (1.1504)\tPrec@1 56.250 (60.340)\n",
      "Epoch: [190][234/390]\tTime 0.004 (0.006)\tLoss 1.1442 (1.1661)\tPrec@1 59.375 (59.591)\n",
      "Epoch: [190][312/390]\tTime 0.002 (0.005)\tLoss 1.2125 (1.1766)\tPrec@1 59.375 (59.382)\n",
      "Epoch: [190][390/390]\tTime 0.001 (0.005)\tLoss 1.2334 (1.1866)\tPrec@1 52.500 (59.028)\n",
      "EPOCH: 190 train Results: Prec@1 59.028 Loss: 1.1866\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1747 (1.1747)\tPrec@1 63.281 (63.281)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5665 (1.2705)\tPrec@1 43.750 (55.160)\n",
      "EPOCH: 190 val Results: Prec@1 55.160 Loss: 1.2705\n",
      "Best Prec@1: 55.620\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [191][0/390]\tTime 0.009 (0.009)\tLoss 1.1637 (1.1637)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [191][78/390]\tTime 0.002 (0.005)\tLoss 1.0557 (1.1414)\tPrec@1 67.969 (61.145)\n",
      "Epoch: [191][156/390]\tTime 0.005 (0.004)\tLoss 1.1607 (1.1596)\tPrec@1 57.031 (60.301)\n",
      "Epoch: [191][234/390]\tTime 0.008 (0.005)\tLoss 1.1294 (1.1682)\tPrec@1 60.156 (59.910)\n",
      "Epoch: [191][312/390]\tTime 0.002 (0.005)\tLoss 1.0790 (1.1757)\tPrec@1 64.844 (59.495)\n",
      "Epoch: [191][390/390]\tTime 0.002 (0.005)\tLoss 1.3157 (1.1876)\tPrec@1 50.000 (58.996)\n",
      "EPOCH: 191 train Results: Prec@1 58.996 Loss: 1.1876\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1470 (1.1470)\tPrec@1 67.969 (67.969)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3913 (1.2620)\tPrec@1 43.750 (55.740)\n",
      "EPOCH: 191 val Results: Prec@1 55.740 Loss: 1.2620\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [192][0/390]\tTime 0.012 (0.012)\tLoss 1.0552 (1.0552)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [192][78/390]\tTime 0.004 (0.007)\tLoss 1.1254 (1.1486)\tPrec@1 57.031 (61.056)\n",
      "Epoch: [192][156/390]\tTime 0.010 (0.006)\tLoss 1.1876 (1.1596)\tPrec@1 60.938 (60.241)\n",
      "Epoch: [192][234/390]\tTime 0.002 (0.005)\tLoss 1.1339 (1.1715)\tPrec@1 61.719 (59.621)\n",
      "Epoch: [192][312/390]\tTime 0.002 (0.006)\tLoss 1.3129 (1.1743)\tPrec@1 53.125 (59.412)\n",
      "Epoch: [192][390/390]\tTime 0.001 (0.006)\tLoss 1.1490 (1.1821)\tPrec@1 62.500 (59.054)\n",
      "EPOCH: 192 train Results: Prec@1 59.054 Loss: 1.1821\n",
      "Test: [0/78]\tTime 0.003 (0.003)\tLoss 1.1368 (1.1368)\tPrec@1 64.062 (64.062)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4099 (1.2647)\tPrec@1 43.750 (55.240)\n",
      "EPOCH: 192 val Results: Prec@1 55.240 Loss: 1.2647\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [193][0/390]\tTime 0.004 (0.004)\tLoss 1.0520 (1.0520)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [193][78/390]\tTime 0.002 (0.004)\tLoss 1.1079 (1.1411)\tPrec@1 59.375 (60.858)\n",
      "Epoch: [193][156/390]\tTime 0.007 (0.004)\tLoss 1.1184 (1.1593)\tPrec@1 61.719 (60.311)\n",
      "Epoch: [193][234/390]\tTime 0.002 (0.004)\tLoss 1.1957 (1.1739)\tPrec@1 55.469 (59.475)\n",
      "Epoch: [193][312/390]\tTime 0.004 (0.004)\tLoss 1.2933 (1.1825)\tPrec@1 59.375 (59.188)\n",
      "Epoch: [193][390/390]\tTime 0.001 (0.004)\tLoss 1.2150 (1.1859)\tPrec@1 61.250 (59.052)\n",
      "EPOCH: 193 train Results: Prec@1 59.052 Loss: 1.1859\n",
      "Test: [0/78]\tTime 0.002 (0.002)\tLoss 1.1487 (1.1487)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4895 (1.2706)\tPrec@1 43.750 (55.270)\n",
      "EPOCH: 193 val Results: Prec@1 55.270 Loss: 1.2706\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [194][0/390]\tTime 0.005 (0.005)\tLoss 1.0935 (1.0935)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [194][78/390]\tTime 0.004 (0.003)\tLoss 1.0841 (1.1480)\tPrec@1 60.156 (61.007)\n",
      "Epoch: [194][156/390]\tTime 0.002 (0.003)\tLoss 1.0319 (1.1617)\tPrec@1 61.719 (60.106)\n",
      "Epoch: [194][234/390]\tTime 0.005 (0.004)\tLoss 1.1712 (1.1708)\tPrec@1 56.250 (59.754)\n",
      "Epoch: [194][312/390]\tTime 0.002 (0.004)\tLoss 1.2796 (1.1783)\tPrec@1 52.344 (59.477)\n",
      "Epoch: [194][390/390]\tTime 0.003 (0.004)\tLoss 1.3376 (1.1828)\tPrec@1 52.500 (59.176)\n",
      "EPOCH: 194 train Results: Prec@1 59.176 Loss: 1.1828\n",
      "Test: [0/78]\tTime 0.004 (0.004)\tLoss 1.1656 (1.1656)\tPrec@1 56.250 (56.250)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3816 (1.2666)\tPrec@1 50.000 (55.100)\n",
      "EPOCH: 194 val Results: Prec@1 55.100 Loss: 1.2666\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [195][0/390]\tTime 0.012 (0.012)\tLoss 1.0714 (1.0714)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [195][78/390]\tTime 0.002 (0.004)\tLoss 1.1404 (1.1296)\tPrec@1 64.844 (61.521)\n",
      "Epoch: [195][156/390]\tTime 0.002 (0.003)\tLoss 1.0928 (1.1497)\tPrec@1 63.281 (60.465)\n",
      "Epoch: [195][234/390]\tTime 0.002 (0.003)\tLoss 1.1996 (1.1603)\tPrec@1 53.906 (60.083)\n",
      "Epoch: [195][312/390]\tTime 0.002 (0.003)\tLoss 1.3859 (1.1739)\tPrec@1 52.344 (59.555)\n",
      "Epoch: [195][390/390]\tTime 0.002 (0.003)\tLoss 1.2962 (1.1805)\tPrec@1 56.250 (59.340)\n",
      "EPOCH: 195 train Results: Prec@1 59.340 Loss: 1.1805\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1712 (1.1712)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4911 (1.2695)\tPrec@1 31.250 (55.100)\n",
      "EPOCH: 195 val Results: Prec@1 55.100 Loss: 1.2695\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [196][0/390]\tTime 0.003 (0.003)\tLoss 1.0672 (1.0672)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [196][78/390]\tTime 0.002 (0.003)\tLoss 1.1500 (1.1313)\tPrec@1 60.938 (61.640)\n",
      "Epoch: [196][156/390]\tTime 0.003 (0.003)\tLoss 1.2373 (1.1481)\tPrec@1 59.375 (60.729)\n",
      "Epoch: [196][234/390]\tTime 0.002 (0.003)\tLoss 1.1112 (1.1721)\tPrec@1 60.938 (59.707)\n",
      "Epoch: [196][312/390]\tTime 0.002 (0.003)\tLoss 1.2204 (1.1764)\tPrec@1 51.562 (59.472)\n",
      "Epoch: [196][390/390]\tTime 0.005 (0.003)\tLoss 1.1674 (1.1824)\tPrec@1 62.500 (59.098)\n",
      "EPOCH: 196 train Results: Prec@1 59.098 Loss: 1.1824\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1347 (1.1347)\tPrec@1 61.719 (61.719)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.4848 (1.2634)\tPrec@1 31.250 (55.490)\n",
      "EPOCH: 196 val Results: Prec@1 55.490 Loss: 1.2634\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [197][0/390]\tTime 0.003 (0.003)\tLoss 1.0441 (1.0441)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [197][78/390]\tTime 0.002 (0.003)\tLoss 1.1960 (1.1165)\tPrec@1 52.344 (61.818)\n",
      "Epoch: [197][156/390]\tTime 0.002 (0.003)\tLoss 1.1993 (1.1396)\tPrec@1 59.375 (61.047)\n",
      "Epoch: [197][234/390]\tTime 0.002 (0.003)\tLoss 1.1067 (1.1566)\tPrec@1 66.406 (60.339)\n",
      "Epoch: [197][312/390]\tTime 0.003 (0.003)\tLoss 1.1900 (1.1739)\tPrec@1 58.594 (59.532)\n",
      "Epoch: [197][390/390]\tTime 0.007 (0.003)\tLoss 0.8923 (1.1819)\tPrec@1 71.250 (59.296)\n",
      "EPOCH: 197 train Results: Prec@1 59.296 Loss: 1.1819\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1929 (1.1929)\tPrec@1 60.938 (60.938)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1942 (1.2682)\tPrec@1 37.500 (54.970)\n",
      "EPOCH: 197 val Results: Prec@1 54.970 Loss: 1.2682\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [198][0/390]\tTime 0.002 (0.002)\tLoss 1.0860 (1.0860)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [198][78/390]\tTime 0.003 (0.003)\tLoss 1.1750 (1.1267)\tPrec@1 59.375 (61.323)\n",
      "Epoch: [198][156/390]\tTime 0.002 (0.003)\tLoss 1.1748 (1.1526)\tPrec@1 60.156 (60.236)\n",
      "Epoch: [198][234/390]\tTime 0.003 (0.003)\tLoss 1.3476 (1.1659)\tPrec@1 53.906 (59.781)\n",
      "Epoch: [198][312/390]\tTime 0.004 (0.004)\tLoss 1.0905 (1.1739)\tPrec@1 61.719 (59.462)\n",
      "Epoch: [198][390/390]\tTime 0.003 (0.004)\tLoss 1.3181 (1.1818)\tPrec@1 50.000 (59.190)\n",
      "EPOCH: 198 train Results: Prec@1 59.190 Loss: 1.1818\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.2053 (1.2053)\tPrec@1 57.031 (57.031)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.5579 (1.2769)\tPrec@1 37.500 (54.670)\n",
      "EPOCH: 198 val Results: Prec@1 54.670 Loss: 1.2769\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [199][0/390]\tTime 0.003 (0.003)\tLoss 1.2040 (1.2040)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [199][78/390]\tTime 0.002 (0.003)\tLoss 1.1427 (1.1223)\tPrec@1 60.156 (61.729)\n",
      "Epoch: [199][156/390]\tTime 0.002 (0.003)\tLoss 1.2468 (1.1451)\tPrec@1 53.906 (60.793)\n",
      "Epoch: [199][234/390]\tTime 0.002 (0.003)\tLoss 1.1699 (1.1625)\tPrec@1 58.594 (60.033)\n",
      "Epoch: [199][312/390]\tTime 0.007 (0.003)\tLoss 1.2219 (1.1740)\tPrec@1 61.719 (59.557)\n",
      "Epoch: [199][390/390]\tTime 0.002 (0.003)\tLoss 1.2168 (1.1795)\tPrec@1 57.500 (59.424)\n",
      "EPOCH: 199 train Results: Prec@1 59.424 Loss: 1.1795\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1458 (1.1458)\tPrec@1 60.156 (60.156)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.3556 (1.2712)\tPrec@1 43.750 (54.930)\n",
      "EPOCH: 199 val Results: Prec@1 54.930 Loss: 1.2712\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "current lr 5.00000e-04\n",
      "Epoch: [200][0/390]\tTime 0.002 (0.002)\tLoss 1.0840 (1.0840)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [200][78/390]\tTime 0.002 (0.003)\tLoss 1.1696 (1.1471)\tPrec@1 58.594 (60.661)\n",
      "Epoch: [200][156/390]\tTime 0.008 (0.003)\tLoss 0.9888 (1.1620)\tPrec@1 71.875 (60.216)\n",
      "Epoch: [200][234/390]\tTime 0.005 (0.003)\tLoss 1.1119 (1.1677)\tPrec@1 61.719 (59.794)\n",
      "Epoch: [200][312/390]\tTime 0.003 (0.003)\tLoss 1.2753 (1.1729)\tPrec@1 57.031 (59.625)\n",
      "Epoch: [200][390/390]\tTime 0.001 (0.003)\tLoss 1.4209 (1.1816)\tPrec@1 50.000 (59.182)\n",
      "EPOCH: 200 train Results: Prec@1 59.182 Loss: 1.1816\n",
      "Test: [0/78]\tTime 0.001 (0.001)\tLoss 1.1840 (1.1840)\tPrec@1 58.594 (58.594)\n",
      "Test: [78/78]\tTime 0.000 (0.001)\tLoss 1.1576 (1.2666)\tPrec@1 43.750 (55.100)\n",
      "EPOCH: 200 val Results: Prec@1 55.100 Loss: 1.2666\n",
      "Best Prec@1: 55.740\n",
      "\n",
      "End time:  Thu Apr  4 23:18:58 2024\n",
      "train executed in 318.0764 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 128\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.0005, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer6 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer6.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainers):\n",
    "    train_accu = [trainer.train_precs for trainer in trainers]\n",
    "    test_accu = [trainer.test_precs for trainer in trainers]\n",
    "    lrs = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005]\n",
    "    c = ['r-', 'b-', 'g-', 'y-', 'k-', 'm-']\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "\n",
    "    for i, y1 in enumerate(train_accu):\n",
    "        ax1.plot(x, y1, c[i], label='lr ' + str(lrs[i])) \n",
    "    \n",
    "    for i, y2 in enumerate(test_accu):\n",
    "        ax2.plot(x, y2, c[i], label='lr ' + str(lrs[i])) \n",
    "\n",
    "    ax1.set_title('Training Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "\n",
    "    ax2.set_title('Testing Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax2.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy([trainer, trainer5, trainer2, trainer4, trainer3, trainer6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainers):\n",
    "    train_losses = [trainer.train_losses for trainer in trainers]\n",
    "    test_losses = [trainer.test_losses for trainer in trainers]\n",
    "    lrs = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005]\n",
    "    c = ['r-', 'b-', 'g-', 'y-', 'k-', 'm-']\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "\n",
    "    for i, y1 in enumerate(train_losses):\n",
    "        ax1.plot(x, y1, c[i], label='lr ' + str(lrs[i])) \n",
    "    \n",
    "    for i, y2 in enumerate(test_losses):\n",
    "        ax2.plot(x, y2, c[i], label='lr ' + str(lrs[i])) \n",
    "\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    ax2.set_title('Testing Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy([trainer, trainer5, trainer2, trainer4, trainer3, trainer6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:44:39 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/3124]\tTime 0.006 (0.006)\tLoss 3.8532 (3.8532)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [1][625/3124]\tTime 0.001 (0.002)\tLoss 1.8422 (2.2802)\tPrec@1 25.000 (29.243)\n",
      "Epoch: [1][1250/3124]\tTime 0.002 (0.002)\tLoss 1.6495 (2.0239)\tPrec@1 31.250 (33.054)\n",
      "Epoch: [1][1875/3124]\tTime 0.001 (0.002)\tLoss 1.8696 (1.9184)\tPrec@1 43.750 (35.015)\n",
      "Epoch: [1][2500/3124]\tTime 0.001 (0.002)\tLoss 1.6753 (1.8606)\tPrec@1 56.250 (36.096)\n",
      "Epoch: [1][3124/3124]\tTime 0.001 (0.002)\tLoss 1.8750 (1.8275)\tPrec@1 37.500 (36.638)\n",
      "EPOCH: 1 train Results: Prec@1 36.638 Loss: 1.8275\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1028 (1.1028)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7012 (1.5592)\tPrec@1 25.000 (44.200)\n",
      "EPOCH: 1 val Results: Prec@1 44.200 Loss: 1.5592\n",
      "Best Prec@1: 44.200\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/3124]\tTime 0.004 (0.004)\tLoss 1.6005 (1.6005)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [2][625/3124]\tTime 0.001 (0.002)\tLoss 1.6348 (1.6470)\tPrec@1 31.250 (41.184)\n",
      "Epoch: [2][1250/3124]\tTime 0.001 (0.002)\tLoss 1.7875 (1.6507)\tPrec@1 31.250 (40.827)\n",
      "Epoch: [2][1875/3124]\tTime 0.001 (0.002)\tLoss 1.3278 (1.6539)\tPrec@1 43.750 (40.588)\n",
      "Epoch: [2][2500/3124]\tTime 0.001 (0.002)\tLoss 1.6788 (1.6589)\tPrec@1 37.500 (40.494)\n",
      "Epoch: [2][3124/3124]\tTime 0.001 (0.002)\tLoss 1.4558 (1.6551)\tPrec@1 62.500 (40.720)\n",
      "EPOCH: 2 train Results: Prec@1 40.720 Loss: 1.6551\n",
      "Test: [0/624]\tTime 0.006 (0.006)\tLoss 1.0530 (1.0530)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7072 (1.5450)\tPrec@1 25.000 (44.020)\n",
      "EPOCH: 2 val Results: Prec@1 44.020 Loss: 1.5450\n",
      "Best Prec@1: 44.200\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/3124]\tTime 0.004 (0.004)\tLoss 1.5545 (1.5545)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [3][625/3124]\tTime 0.001 (0.002)\tLoss 1.7838 (1.6372)\tPrec@1 43.750 (41.973)\n",
      "Epoch: [3][1250/3124]\tTime 0.001 (0.002)\tLoss 2.0583 (1.6351)\tPrec@1 25.000 (41.742)\n",
      "Epoch: [3][1875/3124]\tTime 0.002 (0.002)\tLoss 1.7948 (1.6369)\tPrec@1 31.250 (41.644)\n",
      "Epoch: [3][2500/3124]\tTime 0.001 (0.002)\tLoss 1.5931 (1.6395)\tPrec@1 56.250 (41.498)\n",
      "Epoch: [3][3124/3124]\tTime 0.001 (0.002)\tLoss 1.8767 (1.6406)\tPrec@1 25.000 (41.410)\n",
      "EPOCH: 3 train Results: Prec@1 41.410 Loss: 1.6406\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0560 (1.0560)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4886 (1.5201)\tPrec@1 37.500 (45.500)\n",
      "EPOCH: 3 val Results: Prec@1 45.500 Loss: 1.5201\n",
      "Best Prec@1: 45.500\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/3124]\tTime 0.001 (0.001)\tLoss 2.0712 (2.0712)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [4][625/3124]\tTime 0.001 (0.001)\tLoss 1.3504 (1.6269)\tPrec@1 43.750 (41.923)\n",
      "Epoch: [4][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5257 (1.6348)\tPrec@1 43.750 (41.427)\n",
      "Epoch: [4][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7302 (1.6329)\tPrec@1 43.750 (41.598)\n",
      "Epoch: [4][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8324 (1.6373)\tPrec@1 25.000 (41.478)\n",
      "Epoch: [4][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5310 (1.6383)\tPrec@1 50.000 (41.408)\n",
      "EPOCH: 4 train Results: Prec@1 41.408 Loss: 1.6383\n",
      "Test: [0/624]\tTime 0.004 (0.004)\tLoss 1.2156 (1.2156)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3706 (1.5239)\tPrec@1 37.500 (45.570)\n",
      "EPOCH: 4 val Results: Prec@1 45.570 Loss: 1.5239\n",
      "Best Prec@1: 45.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/3124]\tTime 0.002 (0.002)\tLoss 1.7084 (1.7084)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [5][625/3124]\tTime 0.001 (0.001)\tLoss 1.9697 (1.6125)\tPrec@1 31.250 (42.232)\n",
      "Epoch: [5][1250/3124]\tTime 0.001 (0.001)\tLoss 2.0567 (1.6315)\tPrec@1 31.250 (41.522)\n",
      "Epoch: [5][1875/3124]\tTime 0.002 (0.001)\tLoss 1.4447 (1.6313)\tPrec@1 50.000 (41.604)\n",
      "Epoch: [5][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6530 (1.6313)\tPrec@1 18.750 (41.561)\n",
      "Epoch: [5][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7408 (1.6319)\tPrec@1 43.750 (41.530)\n",
      "EPOCH: 5 train Results: Prec@1 41.530 Loss: 1.6319\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0066 (1.0066)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3038 (1.5257)\tPrec@1 50.000 (45.280)\n",
      "EPOCH: 5 val Results: Prec@1 45.280 Loss: 1.5257\n",
      "Best Prec@1: 45.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/3124]\tTime 0.002 (0.002)\tLoss 1.5509 (1.5509)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [6][625/3124]\tTime 0.001 (0.001)\tLoss 2.0817 (1.5987)\tPrec@1 31.250 (43.061)\n",
      "Epoch: [6][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4957 (1.6077)\tPrec@1 50.000 (42.561)\n",
      "Epoch: [6][1875/3124]\tTime 0.003 (0.001)\tLoss 1.2969 (1.6159)\tPrec@1 68.750 (42.074)\n",
      "Epoch: [6][2500/3124]\tTime 0.001 (0.001)\tLoss 2.2006 (1.6210)\tPrec@1 31.250 (41.756)\n",
      "Epoch: [6][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5299 (1.6208)\tPrec@1 56.250 (41.846)\n",
      "EPOCH: 6 train Results: Prec@1 41.846 Loss: 1.6208\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1385 (1.1385)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4298 (1.5034)\tPrec@1 37.500 (46.270)\n",
      "EPOCH: 6 val Results: Prec@1 46.270 Loss: 1.5034\n",
      "Best Prec@1: 46.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/3124]\tTime 0.002 (0.002)\tLoss 1.4741 (1.4741)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [7][625/3124]\tTime 0.001 (0.001)\tLoss 1.8933 (1.6186)\tPrec@1 25.000 (42.632)\n",
      "Epoch: [7][1250/3124]\tTime 0.002 (0.001)\tLoss 1.5112 (1.6244)\tPrec@1 56.250 (42.321)\n",
      "Epoch: [7][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6986 (1.6266)\tPrec@1 37.500 (42.091)\n",
      "Epoch: [7][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4680 (1.6245)\tPrec@1 56.250 (41.971)\n",
      "Epoch: [7][3124/3124]\tTime 0.001 (0.001)\tLoss 1.0956 (1.6281)\tPrec@1 68.750 (41.804)\n",
      "EPOCH: 7 train Results: Prec@1 41.804 Loss: 1.6281\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1445 (1.1445)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2111 (1.5214)\tPrec@1 43.750 (44.880)\n",
      "EPOCH: 7 val Results: Prec@1 44.880 Loss: 1.5214\n",
      "Best Prec@1: 46.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/3124]\tTime 0.001 (0.001)\tLoss 1.4512 (1.4512)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [8][625/3124]\tTime 0.001 (0.001)\tLoss 1.6070 (1.6144)\tPrec@1 50.000 (42.812)\n",
      "Epoch: [8][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7033 (1.6278)\tPrec@1 37.500 (42.176)\n",
      "Epoch: [8][1875/3124]\tTime 0.001 (0.001)\tLoss 1.2566 (1.6239)\tPrec@1 56.250 (42.234)\n",
      "Epoch: [8][2500/3124]\tTime 0.001 (0.001)\tLoss 1.1989 (1.6208)\tPrec@1 62.500 (42.283)\n",
      "Epoch: [8][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7394 (1.6244)\tPrec@1 37.500 (42.042)\n",
      "EPOCH: 8 train Results: Prec@1 42.042 Loss: 1.6244\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2007 (1.2007)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2762 (1.4980)\tPrec@1 56.250 (45.710)\n",
      "EPOCH: 8 val Results: Prec@1 45.710 Loss: 1.4980\n",
      "Best Prec@1: 46.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/3124]\tTime 0.001 (0.001)\tLoss 2.1514 (2.1514)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [9][625/3124]\tTime 0.001 (0.001)\tLoss 1.5783 (1.6062)\tPrec@1 37.500 (42.242)\n",
      "Epoch: [9][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8176 (1.6170)\tPrec@1 18.750 (42.186)\n",
      "Epoch: [9][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7793 (1.6251)\tPrec@1 37.500 (41.921)\n",
      "Epoch: [9][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6043 (1.6256)\tPrec@1 43.750 (41.843)\n",
      "Epoch: [9][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6495 (1.6302)\tPrec@1 50.000 (41.640)\n",
      "EPOCH: 9 train Results: Prec@1 41.640 Loss: 1.6302\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9857 (0.9857)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3602 (1.5167)\tPrec@1 25.000 (45.600)\n",
      "EPOCH: 9 val Results: Prec@1 45.600 Loss: 1.5167\n",
      "Best Prec@1: 46.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/3124]\tTime 0.001 (0.001)\tLoss 2.0235 (2.0235)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [10][625/3124]\tTime 0.001 (0.001)\tLoss 0.9924 (1.5908)\tPrec@1 81.250 (42.851)\n",
      "Epoch: [10][1250/3124]\tTime 0.001 (0.001)\tLoss 2.3059 (1.6156)\tPrec@1 25.000 (42.096)\n",
      "Epoch: [10][1875/3124]\tTime 0.001 (0.001)\tLoss 1.2989 (1.6202)\tPrec@1 43.750 (41.848)\n",
      "Epoch: [10][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4979 (1.6234)\tPrec@1 43.750 (41.833)\n",
      "Epoch: [10][3124/3124]\tTime 0.001 (0.001)\tLoss 1.1279 (1.6256)\tPrec@1 68.750 (41.896)\n",
      "EPOCH: 10 train Results: Prec@1 41.896 Loss: 1.6256\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0936 (1.0936)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6550 (1.5143)\tPrec@1 31.250 (46.580)\n",
      "EPOCH: 10 val Results: Prec@1 46.580 Loss: 1.5143\n",
      "Best Prec@1: 46.580\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/3124]\tTime 0.001 (0.001)\tLoss 1.5201 (1.5201)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [11][625/3124]\tTime 0.001 (0.001)\tLoss 1.3917 (1.5965)\tPrec@1 50.000 (43.381)\n",
      "Epoch: [11][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9710 (1.6070)\tPrec@1 31.250 (42.556)\n",
      "Epoch: [11][1875/3124]\tTime 0.001 (0.001)\tLoss 2.1925 (1.6188)\tPrec@1 31.250 (42.154)\n",
      "Epoch: [11][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4394 (1.6186)\tPrec@1 50.000 (42.048)\n",
      "Epoch: [11][3124/3124]\tTime 0.001 (0.001)\tLoss 1.0601 (1.6229)\tPrec@1 75.000 (41.996)\n",
      "EPOCH: 11 train Results: Prec@1 41.996 Loss: 1.6229\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1138 (1.1138)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2660 (1.5163)\tPrec@1 43.750 (45.770)\n",
      "EPOCH: 11 val Results: Prec@1 45.770 Loss: 1.5163\n",
      "Best Prec@1: 46.580\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/3124]\tTime 0.001 (0.001)\tLoss 1.7768 (1.7768)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [12][625/3124]\tTime 0.003 (0.001)\tLoss 1.7114 (1.5994)\tPrec@1 31.250 (42.981)\n",
      "Epoch: [12][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4679 (1.6125)\tPrec@1 43.750 (42.256)\n",
      "Epoch: [12][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5244 (1.6168)\tPrec@1 43.750 (42.104)\n",
      "Epoch: [12][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7931 (1.6198)\tPrec@1 25.000 (41.981)\n",
      "Epoch: [12][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5152 (1.6236)\tPrec@1 43.750 (41.916)\n",
      "EPOCH: 12 train Results: Prec@1 41.916 Loss: 1.6236\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0222 (1.0222)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4979 (1.5104)\tPrec@1 31.250 (46.170)\n",
      "EPOCH: 12 val Results: Prec@1 46.170 Loss: 1.5104\n",
      "Best Prec@1: 46.580\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/3124]\tTime 0.002 (0.002)\tLoss 1.3944 (1.3944)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [13][625/3124]\tTime 0.001 (0.001)\tLoss 1.2166 (1.6136)\tPrec@1 68.750 (41.783)\n",
      "Epoch: [13][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2976 (1.6173)\tPrec@1 62.500 (41.792)\n",
      "Epoch: [13][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4658 (1.6240)\tPrec@1 43.750 (41.724)\n",
      "Epoch: [13][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6938 (1.6258)\tPrec@1 50.000 (41.606)\n",
      "Epoch: [13][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7118 (1.6259)\tPrec@1 37.500 (41.572)\n",
      "EPOCH: 13 train Results: Prec@1 41.572 Loss: 1.6259\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9755 (0.9755)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5492 (1.5106)\tPrec@1 43.750 (45.810)\n",
      "EPOCH: 13 val Results: Prec@1 45.810 Loss: 1.5106\n",
      "Best Prec@1: 46.580\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/3124]\tTime 0.001 (0.001)\tLoss 1.5819 (1.5819)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [14][625/3124]\tTime 0.001 (0.001)\tLoss 1.3191 (1.6054)\tPrec@1 37.500 (43.161)\n",
      "Epoch: [14][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8046 (1.6148)\tPrec@1 12.500 (42.671)\n",
      "Epoch: [14][1875/3124]\tTime 0.001 (0.001)\tLoss 1.2654 (1.6222)\tPrec@1 62.500 (42.131)\n",
      "Epoch: [14][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4311 (1.6221)\tPrec@1 62.500 (42.021)\n",
      "Epoch: [14][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7957 (1.6255)\tPrec@1 25.000 (41.856)\n",
      "EPOCH: 14 train Results: Prec@1 41.856 Loss: 1.6255\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9668 (0.9668)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4801 (1.4937)\tPrec@1 31.250 (46.130)\n",
      "EPOCH: 14 val Results: Prec@1 46.130 Loss: 1.4937\n",
      "Best Prec@1: 46.580\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/3124]\tTime 0.001 (0.001)\tLoss 1.3521 (1.3521)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [15][625/3124]\tTime 0.001 (0.001)\tLoss 2.3611 (1.6001)\tPrec@1 25.000 (42.642)\n",
      "Epoch: [15][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4363 (1.6215)\tPrec@1 50.000 (42.021)\n",
      "Epoch: [15][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5454 (1.6224)\tPrec@1 43.750 (42.094)\n",
      "Epoch: [15][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6659 (1.6254)\tPrec@1 62.500 (41.941)\n",
      "Epoch: [15][3124/3124]\tTime 0.001 (0.001)\tLoss 2.0686 (1.6242)\tPrec@1 12.500 (42.016)\n",
      "EPOCH: 15 train Results: Prec@1 42.016 Loss: 1.6242\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1655 (1.1655)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4435 (1.5383)\tPrec@1 37.500 (44.850)\n",
      "EPOCH: 15 val Results: Prec@1 44.850 Loss: 1.5383\n",
      "Best Prec@1: 46.580\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/3124]\tTime 0.003 (0.003)\tLoss 1.3019 (1.3019)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [16][625/3124]\tTime 0.001 (0.001)\tLoss 1.5657 (1.5920)\tPrec@1 50.000 (43.081)\n",
      "Epoch: [16][1250/3124]\tTime 0.002 (0.001)\tLoss 1.6418 (1.6092)\tPrec@1 37.500 (42.181)\n",
      "Epoch: [16][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4483 (1.6192)\tPrec@1 56.250 (42.001)\n",
      "Epoch: [16][2500/3124]\tTime 0.001 (0.001)\tLoss 1.2221 (1.6236)\tPrec@1 56.250 (41.963)\n",
      "Epoch: [16][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4744 (1.6268)\tPrec@1 43.750 (41.910)\n",
      "EPOCH: 16 train Results: Prec@1 41.910 Loss: 1.6268\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0917 (1.0917)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5431 (1.4979)\tPrec@1 43.750 (46.670)\n",
      "EPOCH: 16 val Results: Prec@1 46.670 Loss: 1.4979\n",
      "Best Prec@1: 46.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/3124]\tTime 0.001 (0.001)\tLoss 1.2201 (1.2201)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [17][625/3124]\tTime 0.002 (0.001)\tLoss 1.9092 (1.6173)\tPrec@1 25.000 (41.623)\n",
      "Epoch: [17][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3226 (1.6227)\tPrec@1 43.750 (41.377)\n",
      "Epoch: [17][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7786 (1.6235)\tPrec@1 37.500 (41.455)\n",
      "Epoch: [17][2500/3124]\tTime 0.001 (0.001)\tLoss 2.0096 (1.6216)\tPrec@1 25.000 (41.693)\n",
      "Epoch: [17][3124/3124]\tTime 0.004 (0.001)\tLoss 1.3540 (1.6237)\tPrec@1 62.500 (41.598)\n",
      "EPOCH: 17 train Results: Prec@1 41.598 Loss: 1.6237\n",
      "Test: [0/624]\tTime 0.005 (0.005)\tLoss 1.1451 (1.1451)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4705 (1.5147)\tPrec@1 25.000 (46.180)\n",
      "EPOCH: 17 val Results: Prec@1 46.180 Loss: 1.5147\n",
      "Best Prec@1: 46.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/3124]\tTime 0.001 (0.001)\tLoss 1.6569 (1.6569)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [18][625/3124]\tTime 0.001 (0.001)\tLoss 1.9823 (1.6139)\tPrec@1 43.750 (42.173)\n",
      "Epoch: [18][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5698 (1.6189)\tPrec@1 43.750 (41.672)\n",
      "Epoch: [18][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4296 (1.6244)\tPrec@1 37.500 (41.405)\n",
      "Epoch: [18][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9297 (1.6241)\tPrec@1 37.500 (41.373)\n",
      "Epoch: [18][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3384 (1.6247)\tPrec@1 56.250 (41.580)\n",
      "EPOCH: 18 train Results: Prec@1 41.580 Loss: 1.6247\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9851 (0.9851)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5827 (1.5100)\tPrec@1 31.250 (45.220)\n",
      "EPOCH: 18 val Results: Prec@1 45.220 Loss: 1.5100\n",
      "Best Prec@1: 46.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/3124]\tTime 0.001 (0.001)\tLoss 1.8458 (1.8458)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [19][625/3124]\tTime 0.001 (0.001)\tLoss 1.3482 (1.5843)\tPrec@1 56.250 (43.261)\n",
      "Epoch: [19][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5760 (1.6094)\tPrec@1 37.500 (42.401)\n",
      "Epoch: [19][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9761 (1.6154)\tPrec@1 25.000 (42.214)\n",
      "Epoch: [19][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7149 (1.6224)\tPrec@1 31.250 (41.896)\n",
      "Epoch: [19][3124/3124]\tTime 0.001 (0.001)\tLoss 2.0047 (1.6227)\tPrec@1 25.000 (41.838)\n",
      "EPOCH: 19 train Results: Prec@1 41.838 Loss: 1.6227\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.2761 (1.2761)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.8686 (1.5088)\tPrec@1 31.250 (45.410)\n",
      "EPOCH: 19 val Results: Prec@1 45.410 Loss: 1.5088\n",
      "Best Prec@1: 46.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/3124]\tTime 0.001 (0.001)\tLoss 2.1127 (2.1127)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [20][625/3124]\tTime 0.003 (0.001)\tLoss 1.4146 (1.6213)\tPrec@1 50.000 (41.573)\n",
      "Epoch: [20][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4410 (1.6206)\tPrec@1 56.250 (41.797)\n",
      "Epoch: [20][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8337 (1.6255)\tPrec@1 31.250 (41.658)\n",
      "Epoch: [20][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4724 (1.6234)\tPrec@1 50.000 (41.843)\n",
      "Epoch: [20][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7435 (1.6240)\tPrec@1 25.000 (41.766)\n",
      "EPOCH: 20 train Results: Prec@1 41.766 Loss: 1.6240\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0236 (1.0236)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4358 (1.4993)\tPrec@1 31.250 (45.560)\n",
      "EPOCH: 20 val Results: Prec@1 45.560 Loss: 1.4993\n",
      "Best Prec@1: 46.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/3124]\tTime 0.003 (0.003)\tLoss 2.2391 (2.2391)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [21][625/3124]\tTime 0.001 (0.001)\tLoss 1.1487 (1.6058)\tPrec@1 62.500 (42.472)\n",
      "Epoch: [21][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5728 (1.6162)\tPrec@1 25.000 (42.071)\n",
      "Epoch: [21][1875/3124]\tTime 0.001 (0.001)\tLoss 2.0617 (1.6185)\tPrec@1 25.000 (42.081)\n",
      "Epoch: [21][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4145 (1.6239)\tPrec@1 43.750 (41.818)\n",
      "Epoch: [21][3124/3124]\tTime 0.004 (0.001)\tLoss 1.3561 (1.6272)\tPrec@1 50.000 (41.584)\n",
      "EPOCH: 21 train Results: Prec@1 41.584 Loss: 1.6272\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0071 (1.0071)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2669 (1.5010)\tPrec@1 31.250 (46.940)\n",
      "EPOCH: 21 val Results: Prec@1 46.940 Loss: 1.5010\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/3124]\tTime 0.001 (0.001)\tLoss 1.9258 (1.9258)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [22][625/3124]\tTime 0.001 (0.001)\tLoss 1.9965 (1.6163)\tPrec@1 18.750 (42.662)\n",
      "Epoch: [22][1250/3124]\tTime 0.008 (0.001)\tLoss 1.7874 (1.6179)\tPrec@1 37.500 (42.651)\n",
      "Epoch: [22][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8218 (1.6214)\tPrec@1 25.000 (42.357)\n",
      "Epoch: [22][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8317 (1.6248)\tPrec@1 25.000 (42.168)\n",
      "Epoch: [22][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3457 (1.6278)\tPrec@1 50.000 (41.956)\n",
      "EPOCH: 22 train Results: Prec@1 41.956 Loss: 1.6278\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9582 (0.9582)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3629 (1.5076)\tPrec@1 37.500 (45.360)\n",
      "EPOCH: 22 val Results: Prec@1 45.360 Loss: 1.5076\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/3124]\tTime 0.001 (0.001)\tLoss 1.4888 (1.4888)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [23][625/3124]\tTime 0.001 (0.001)\tLoss 1.8308 (1.6086)\tPrec@1 37.500 (41.773)\n",
      "Epoch: [23][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6820 (1.6202)\tPrec@1 43.750 (41.527)\n",
      "Epoch: [23][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4254 (1.6222)\tPrec@1 62.500 (41.428)\n",
      "Epoch: [23][2500/3124]\tTime 0.001 (0.001)\tLoss 2.0058 (1.6260)\tPrec@1 31.250 (41.271)\n",
      "Epoch: [23][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6317 (1.6258)\tPrec@1 31.250 (41.304)\n",
      "EPOCH: 23 train Results: Prec@1 41.304 Loss: 1.6258\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.3420 (1.3420)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2580 (1.5032)\tPrec@1 43.750 (46.150)\n",
      "EPOCH: 23 val Results: Prec@1 46.150 Loss: 1.5032\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/3124]\tTime 0.001 (0.001)\tLoss 1.7930 (1.7930)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [24][625/3124]\tTime 0.017 (0.002)\tLoss 1.8183 (1.5944)\tPrec@1 37.500 (43.331)\n",
      "Epoch: [24][1250/3124]\tTime 0.001 (0.002)\tLoss 1.7545 (1.6107)\tPrec@1 25.000 (42.181)\n",
      "Epoch: [24][1875/3124]\tTime 0.001 (0.002)\tLoss 2.0035 (1.6203)\tPrec@1 31.250 (41.948)\n",
      "Epoch: [24][2500/3124]\tTime 0.001 (0.002)\tLoss 1.6758 (1.6208)\tPrec@1 43.750 (41.866)\n",
      "Epoch: [24][3124/3124]\tTime 0.001 (0.002)\tLoss 1.7892 (1.6240)\tPrec@1 31.250 (41.804)\n",
      "EPOCH: 24 train Results: Prec@1 41.804 Loss: 1.6240\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0210 (1.0210)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3733 (1.5105)\tPrec@1 37.500 (45.600)\n",
      "EPOCH: 24 val Results: Prec@1 45.600 Loss: 1.5105\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/3124]\tTime 0.002 (0.002)\tLoss 1.7895 (1.7895)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [25][625/3124]\tTime 0.001 (0.002)\tLoss 1.4414 (1.6050)\tPrec@1 43.750 (42.212)\n",
      "Epoch: [25][1250/3124]\tTime 0.001 (0.002)\tLoss 1.6713 (1.6127)\tPrec@1 25.000 (42.431)\n",
      "Epoch: [25][1875/3124]\tTime 0.001 (0.002)\tLoss 1.5210 (1.6188)\tPrec@1 37.500 (42.054)\n",
      "Epoch: [25][2500/3124]\tTime 0.002 (0.002)\tLoss 1.7076 (1.6243)\tPrec@1 56.250 (41.726)\n",
      "Epoch: [25][3124/3124]\tTime 0.001 (0.002)\tLoss 1.6701 (1.6251)\tPrec@1 37.500 (41.840)\n",
      "EPOCH: 25 train Results: Prec@1 41.840 Loss: 1.6251\n",
      "Test: [0/624]\tTime 0.010 (0.010)\tLoss 1.1594 (1.1594)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4285 (1.5124)\tPrec@1 31.250 (45.860)\n",
      "EPOCH: 25 val Results: Prec@1 45.860 Loss: 1.5124\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/3124]\tTime 0.003 (0.003)\tLoss 1.4145 (1.4145)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [26][625/3124]\tTime 0.001 (0.003)\tLoss 1.7339 (1.6060)\tPrec@1 37.500 (42.362)\n",
      "Epoch: [26][1250/3124]\tTime 0.001 (0.003)\tLoss 1.5058 (1.6176)\tPrec@1 43.750 (42.316)\n",
      "Epoch: [26][1875/3124]\tTime 0.001 (0.003)\tLoss 1.1512 (1.6220)\tPrec@1 56.250 (42.084)\n",
      "Epoch: [26][2500/3124]\tTime 0.006 (0.002)\tLoss 1.2324 (1.6258)\tPrec@1 68.750 (41.811)\n",
      "Epoch: [26][3124/3124]\tTime 0.001 (0.002)\tLoss 2.1279 (1.6266)\tPrec@1 12.500 (41.744)\n",
      "EPOCH: 26 train Results: Prec@1 41.744 Loss: 1.6266\n",
      "Test: [0/624]\tTime 0.004 (0.004)\tLoss 1.3241 (1.3241)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.001)\tLoss 1.7731 (1.5286)\tPrec@1 25.000 (44.430)\n",
      "EPOCH: 26 val Results: Prec@1 44.430 Loss: 1.5286\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/3124]\tTime 0.004 (0.004)\tLoss 2.1320 (2.1320)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [27][625/3124]\tTime 0.001 (0.002)\tLoss 1.4897 (1.5897)\tPrec@1 50.000 (42.442)\n",
      "Epoch: [27][1250/3124]\tTime 0.001 (0.002)\tLoss 1.2431 (1.6051)\tPrec@1 50.000 (42.176)\n",
      "Epoch: [27][1875/3124]\tTime 0.002 (0.002)\tLoss 1.7764 (1.6130)\tPrec@1 43.750 (41.851)\n",
      "Epoch: [27][2500/3124]\tTime 0.002 (0.002)\tLoss 1.7890 (1.6223)\tPrec@1 43.750 (41.431)\n",
      "Epoch: [27][3124/3124]\tTime 0.004 (0.002)\tLoss 1.9069 (1.6263)\tPrec@1 43.750 (41.382)\n",
      "EPOCH: 27 train Results: Prec@1 41.382 Loss: 1.6263\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9178 (0.9178)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6209 (1.5072)\tPrec@1 18.750 (45.880)\n",
      "EPOCH: 27 val Results: Prec@1 45.880 Loss: 1.5072\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/3124]\tTime 0.005 (0.005)\tLoss 1.2791 (1.2791)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [28][625/3124]\tTime 0.002 (0.002)\tLoss 1.1710 (1.6196)\tPrec@1 56.250 (41.613)\n",
      "Epoch: [28][1250/3124]\tTime 0.001 (0.002)\tLoss 1.5106 (1.6212)\tPrec@1 43.750 (41.402)\n",
      "Epoch: [28][1875/3124]\tTime 0.001 (0.002)\tLoss 1.4483 (1.6226)\tPrec@1 50.000 (41.455)\n",
      "Epoch: [28][2500/3124]\tTime 0.001 (0.002)\tLoss 1.4363 (1.6223)\tPrec@1 56.250 (41.826)\n",
      "Epoch: [28][3124/3124]\tTime 0.001 (0.002)\tLoss 1.8212 (1.6256)\tPrec@1 50.000 (41.544)\n",
      "EPOCH: 28 train Results: Prec@1 41.544 Loss: 1.6256\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0131 (1.0131)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.001)\tLoss 1.3685 (1.4964)\tPrec@1 56.250 (46.640)\n",
      "EPOCH: 28 val Results: Prec@1 46.640 Loss: 1.4964\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/3124]\tTime 0.003 (0.003)\tLoss 1.4617 (1.4617)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [29][625/3124]\tTime 0.001 (0.001)\tLoss 1.8097 (1.6153)\tPrec@1 37.500 (42.642)\n",
      "Epoch: [29][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5315 (1.6186)\tPrec@1 43.750 (42.261)\n",
      "Epoch: [29][1875/3124]\tTime 0.007 (0.001)\tLoss 1.3019 (1.6203)\tPrec@1 56.250 (41.894)\n",
      "Epoch: [29][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5680 (1.6233)\tPrec@1 50.000 (41.881)\n",
      "Epoch: [29][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4343 (1.6259)\tPrec@1 43.750 (41.862)\n",
      "EPOCH: 29 train Results: Prec@1 41.862 Loss: 1.6259\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.1162 (1.1162)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3863 (1.5048)\tPrec@1 43.750 (45.820)\n",
      "EPOCH: 29 val Results: Prec@1 45.820 Loss: 1.5048\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/3124]\tTime 0.004 (0.004)\tLoss 1.5892 (1.5892)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [30][625/3124]\tTime 0.001 (0.002)\tLoss 1.7698 (1.6148)\tPrec@1 37.500 (41.743)\n",
      "Epoch: [30][1250/3124]\tTime 0.001 (0.002)\tLoss 1.4411 (1.6171)\tPrec@1 31.250 (41.961)\n",
      "Epoch: [30][1875/3124]\tTime 0.001 (0.002)\tLoss 1.6331 (1.6201)\tPrec@1 37.500 (41.941)\n",
      "Epoch: [30][2500/3124]\tTime 0.001 (0.002)\tLoss 1.8530 (1.6226)\tPrec@1 50.000 (41.961)\n",
      "Epoch: [30][3124/3124]\tTime 0.001 (0.002)\tLoss 1.4399 (1.6237)\tPrec@1 50.000 (41.834)\n",
      "EPOCH: 30 train Results: Prec@1 41.834 Loss: 1.6237\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0522 (1.0522)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.001)\tLoss 1.4034 (1.5096)\tPrec@1 43.750 (45.930)\n",
      "EPOCH: 30 val Results: Prec@1 45.930 Loss: 1.5096\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/3124]\tTime 0.002 (0.002)\tLoss 1.1741 (1.1741)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [31][625/3124]\tTime 0.001 (0.002)\tLoss 1.8195 (1.6111)\tPrec@1 25.000 (42.512)\n",
      "Epoch: [31][1250/3124]\tTime 0.001 (0.002)\tLoss 1.7032 (1.6122)\tPrec@1 56.250 (42.336)\n",
      "Epoch: [31][1875/3124]\tTime 0.004 (0.002)\tLoss 1.8951 (1.6190)\tPrec@1 31.250 (42.051)\n",
      "Epoch: [31][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9113 (1.6232)\tPrec@1 31.250 (41.803)\n",
      "Epoch: [31][3124/3124]\tTime 0.001 (0.002)\tLoss 1.2853 (1.6263)\tPrec@1 43.750 (41.760)\n",
      "EPOCH: 31 train Results: Prec@1 41.760 Loss: 1.6263\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 0.9817 (0.9817)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.001)\tLoss 1.3834 (1.5144)\tPrec@1 31.250 (46.290)\n",
      "EPOCH: 31 val Results: Prec@1 46.290 Loss: 1.5144\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/3124]\tTime 0.006 (0.006)\tLoss 1.8069 (1.8069)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [32][625/3124]\tTime 0.005 (0.001)\tLoss 1.2177 (1.6170)\tPrec@1 62.500 (41.554)\n",
      "Epoch: [32][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7230 (1.6206)\tPrec@1 56.250 (41.522)\n",
      "Epoch: [32][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8214 (1.6243)\tPrec@1 37.500 (41.631)\n",
      "Epoch: [32][2500/3124]\tTime 0.001 (0.001)\tLoss 1.2709 (1.6219)\tPrec@1 50.000 (41.743)\n",
      "Epoch: [32][3124/3124]\tTime 0.001 (0.001)\tLoss 1.2092 (1.6259)\tPrec@1 68.750 (41.604)\n",
      "EPOCH: 32 train Results: Prec@1 41.604 Loss: 1.6259\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0746 (1.0746)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5150 (1.5108)\tPrec@1 43.750 (45.850)\n",
      "EPOCH: 32 val Results: Prec@1 45.850 Loss: 1.5108\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/3124]\tTime 0.001 (0.001)\tLoss 1.8258 (1.8258)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [33][625/3124]\tTime 0.001 (0.001)\tLoss 1.5331 (1.6143)\tPrec@1 37.500 (42.682)\n",
      "Epoch: [33][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5654 (1.6168)\tPrec@1 50.000 (42.431)\n",
      "Epoch: [33][1875/3124]\tTime 0.002 (0.001)\tLoss 1.5267 (1.6209)\tPrec@1 62.500 (42.114)\n",
      "Epoch: [33][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7168 (1.6282)\tPrec@1 43.750 (41.841)\n",
      "Epoch: [33][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4744 (1.6281)\tPrec@1 43.750 (41.866)\n",
      "EPOCH: 33 train Results: Prec@1 41.866 Loss: 1.6281\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1627 (1.1627)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4018 (1.5191)\tPrec@1 37.500 (45.550)\n",
      "EPOCH: 33 val Results: Prec@1 45.550 Loss: 1.5191\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/3124]\tTime 0.002 (0.002)\tLoss 1.5778 (1.5778)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [34][625/3124]\tTime 0.001 (0.001)\tLoss 1.9033 (1.6087)\tPrec@1 25.000 (42.782)\n",
      "Epoch: [34][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3958 (1.6184)\tPrec@1 43.750 (42.096)\n",
      "Epoch: [34][1875/3124]\tTime 0.001 (0.001)\tLoss 2.1129 (1.6175)\tPrec@1 25.000 (41.978)\n",
      "Epoch: [34][2500/3124]\tTime 0.001 (0.001)\tLoss 1.1752 (1.6222)\tPrec@1 56.250 (41.748)\n",
      "Epoch: [34][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7696 (1.6244)\tPrec@1 37.500 (41.724)\n",
      "EPOCH: 34 train Results: Prec@1 41.724 Loss: 1.6244\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.3324 (1.3324)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4691 (1.4947)\tPrec@1 31.250 (46.120)\n",
      "EPOCH: 34 val Results: Prec@1 46.120 Loss: 1.4947\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/3124]\tTime 0.001 (0.001)\tLoss 1.4383 (1.4383)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [35][625/3124]\tTime 0.001 (0.001)\tLoss 1.5067 (1.6267)\tPrec@1 56.250 (41.803)\n",
      "Epoch: [35][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9687 (1.6185)\tPrec@1 25.000 (42.021)\n",
      "Epoch: [35][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3696 (1.6259)\tPrec@1 50.000 (41.601)\n",
      "Epoch: [35][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8289 (1.6249)\tPrec@1 31.250 (41.608)\n",
      "Epoch: [35][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5246 (1.6266)\tPrec@1 37.500 (41.526)\n",
      "EPOCH: 35 train Results: Prec@1 41.526 Loss: 1.6266\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.8384 (0.8384)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6859 (1.5374)\tPrec@1 37.500 (45.050)\n",
      "EPOCH: 35 val Results: Prec@1 45.050 Loss: 1.5374\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/3124]\tTime 0.002 (0.002)\tLoss 1.3515 (1.3515)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [36][625/3124]\tTime 0.001 (0.001)\tLoss 1.3623 (1.6096)\tPrec@1 50.000 (41.703)\n",
      "Epoch: [36][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3732 (1.6220)\tPrec@1 56.250 (41.667)\n",
      "Epoch: [36][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6381 (1.6231)\tPrec@1 37.500 (41.778)\n",
      "Epoch: [36][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7254 (1.6254)\tPrec@1 37.500 (41.583)\n",
      "Epoch: [36][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8627 (1.6269)\tPrec@1 37.500 (41.570)\n",
      "EPOCH: 36 train Results: Prec@1 41.570 Loss: 1.6269\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1052 (1.1052)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6146 (1.5164)\tPrec@1 31.250 (45.600)\n",
      "EPOCH: 36 val Results: Prec@1 45.600 Loss: 1.5164\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/3124]\tTime 0.001 (0.001)\tLoss 1.5678 (1.5678)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [37][625/3124]\tTime 0.001 (0.001)\tLoss 1.7172 (1.5993)\tPrec@1 18.750 (42.831)\n",
      "Epoch: [37][1250/3124]\tTime 0.001 (0.001)\tLoss 2.0307 (1.6135)\tPrec@1 25.000 (42.151)\n",
      "Epoch: [37][1875/3124]\tTime 0.002 (0.001)\tLoss 1.8926 (1.6228)\tPrec@1 37.500 (41.788)\n",
      "Epoch: [37][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9806 (1.6256)\tPrec@1 18.750 (41.608)\n",
      "Epoch: [37][3124/3124]\tTime 0.007 (0.001)\tLoss 1.3458 (1.6267)\tPrec@1 56.250 (41.640)\n",
      "EPOCH: 37 train Results: Prec@1 41.640 Loss: 1.6267\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0526 (1.0526)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7782 (1.5193)\tPrec@1 31.250 (45.260)\n",
      "EPOCH: 37 val Results: Prec@1 45.260 Loss: 1.5193\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/3124]\tTime 0.001 (0.001)\tLoss 1.4605 (1.4605)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [38][625/3124]\tTime 0.001 (0.001)\tLoss 1.7022 (1.6329)\tPrec@1 31.250 (41.304)\n",
      "Epoch: [38][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4252 (1.6247)\tPrec@1 56.250 (41.702)\n",
      "Epoch: [38][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6256 (1.6243)\tPrec@1 37.500 (41.721)\n",
      "Epoch: [38][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6567 (1.6209)\tPrec@1 50.000 (41.858)\n",
      "Epoch: [38][3124/3124]\tTime 0.002 (0.001)\tLoss 1.3906 (1.6268)\tPrec@1 62.500 (41.580)\n",
      "EPOCH: 38 train Results: Prec@1 41.580 Loss: 1.6268\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.1143 (1.1143)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5799 (1.5197)\tPrec@1 37.500 (44.940)\n",
      "EPOCH: 38 val Results: Prec@1 44.940 Loss: 1.5197\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/3124]\tTime 0.001 (0.001)\tLoss 1.9024 (1.9024)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [39][625/3124]\tTime 0.001 (0.001)\tLoss 1.3889 (1.6073)\tPrec@1 43.750 (42.772)\n",
      "Epoch: [39][1250/3124]\tTime 0.006 (0.001)\tLoss 1.8795 (1.6240)\tPrec@1 31.250 (41.911)\n",
      "Epoch: [39][1875/3124]\tTime 0.001 (0.001)\tLoss 2.0483 (1.6289)\tPrec@1 31.250 (41.664)\n",
      "Epoch: [39][2500/3124]\tTime 0.001 (0.001)\tLoss 2.0201 (1.6247)\tPrec@1 18.750 (41.813)\n",
      "Epoch: [39][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5360 (1.6271)\tPrec@1 37.500 (41.738)\n",
      "EPOCH: 39 train Results: Prec@1 41.738 Loss: 1.6271\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0493 (1.0493)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4954 (1.5167)\tPrec@1 37.500 (45.850)\n",
      "EPOCH: 39 val Results: Prec@1 45.850 Loss: 1.5167\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/3124]\tTime 0.007 (0.007)\tLoss 1.7541 (1.7541)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [40][625/3124]\tTime 0.001 (0.001)\tLoss 1.6580 (1.6141)\tPrec@1 37.500 (42.173)\n",
      "Epoch: [40][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4455 (1.6164)\tPrec@1 50.000 (42.126)\n",
      "Epoch: [40][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7389 (1.6206)\tPrec@1 31.250 (42.028)\n",
      "Epoch: [40][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4291 (1.6209)\tPrec@1 50.000 (41.936)\n",
      "Epoch: [40][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7500 (1.6263)\tPrec@1 37.500 (41.706)\n",
      "EPOCH: 40 train Results: Prec@1 41.706 Loss: 1.6263\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1035 (1.1035)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7838 (1.5198)\tPrec@1 37.500 (45.850)\n",
      "EPOCH: 40 val Results: Prec@1 45.850 Loss: 1.5198\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/3124]\tTime 0.001 (0.001)\tLoss 1.4409 (1.4409)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [41][625/3124]\tTime 0.001 (0.001)\tLoss 1.7432 (1.6032)\tPrec@1 31.250 (42.153)\n",
      "Epoch: [41][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6280 (1.6150)\tPrec@1 50.000 (42.021)\n",
      "Epoch: [41][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7420 (1.6171)\tPrec@1 31.250 (42.121)\n",
      "Epoch: [41][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6292 (1.6202)\tPrec@1 37.500 (41.971)\n",
      "Epoch: [41][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9460 (1.6243)\tPrec@1 18.750 (41.814)\n",
      "EPOCH: 41 train Results: Prec@1 41.814 Loss: 1.6243\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9743 (0.9743)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.001)\tLoss 1.3380 (1.5000)\tPrec@1 43.750 (46.180)\n",
      "EPOCH: 41 val Results: Prec@1 46.180 Loss: 1.5000\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/3124]\tTime 0.004 (0.004)\tLoss 1.6666 (1.6666)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [42][625/3124]\tTime 0.001 (0.001)\tLoss 2.2469 (1.6014)\tPrec@1 18.750 (42.941)\n",
      "Epoch: [42][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3930 (1.6156)\tPrec@1 56.250 (42.321)\n",
      "Epoch: [42][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4851 (1.6236)\tPrec@1 43.750 (41.954)\n",
      "Epoch: [42][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6136 (1.6284)\tPrec@1 43.750 (41.628)\n",
      "Epoch: [42][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4685 (1.6295)\tPrec@1 31.250 (41.614)\n",
      "EPOCH: 42 train Results: Prec@1 41.614 Loss: 1.6295\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0701 (1.0701)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5270 (1.5197)\tPrec@1 31.250 (44.850)\n",
      "EPOCH: 42 val Results: Prec@1 44.850 Loss: 1.5197\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/3124]\tTime 0.003 (0.003)\tLoss 1.5407 (1.5407)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [43][625/3124]\tTime 0.001 (0.001)\tLoss 1.8127 (1.6122)\tPrec@1 31.250 (42.262)\n",
      "Epoch: [43][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4036 (1.6225)\tPrec@1 50.000 (41.752)\n",
      "Epoch: [43][1875/3124]\tTime 0.001 (0.002)\tLoss 1.4622 (1.6218)\tPrec@1 43.750 (41.888)\n",
      "Epoch: [43][2500/3124]\tTime 0.001 (0.002)\tLoss 1.6611 (1.6280)\tPrec@1 37.500 (41.733)\n",
      "Epoch: [43][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3945 (1.6276)\tPrec@1 56.250 (41.684)\n",
      "EPOCH: 43 train Results: Prec@1 41.684 Loss: 1.6276\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1459 (1.1459)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5186 (1.5084)\tPrec@1 25.000 (45.490)\n",
      "EPOCH: 43 val Results: Prec@1 45.490 Loss: 1.5084\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/3124]\tTime 0.001 (0.001)\tLoss 1.6975 (1.6975)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [44][625/3124]\tTime 0.001 (0.001)\tLoss 1.5537 (1.6147)\tPrec@1 56.250 (42.612)\n",
      "Epoch: [44][1250/3124]\tTime 0.002 (0.001)\tLoss 1.4097 (1.6109)\tPrec@1 50.000 (42.471)\n",
      "Epoch: [44][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6606 (1.6190)\tPrec@1 37.500 (42.128)\n",
      "Epoch: [44][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5635 (1.6238)\tPrec@1 50.000 (41.833)\n",
      "Epoch: [44][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7903 (1.6260)\tPrec@1 43.750 (41.782)\n",
      "EPOCH: 44 train Results: Prec@1 41.782 Loss: 1.6260\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0574 (1.0574)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4671 (1.5006)\tPrec@1 31.250 (46.140)\n",
      "EPOCH: 44 val Results: Prec@1 46.140 Loss: 1.5006\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/3124]\tTime 0.001 (0.001)\tLoss 1.4282 (1.4282)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [45][625/3124]\tTime 0.001 (0.001)\tLoss 1.3742 (1.6065)\tPrec@1 50.000 (42.093)\n",
      "Epoch: [45][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8129 (1.6185)\tPrec@1 37.500 (41.562)\n",
      "Epoch: [45][1875/3124]\tTime 0.004 (0.001)\tLoss 1.7067 (1.6190)\tPrec@1 43.750 (41.508)\n",
      "Epoch: [45][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8488 (1.6218)\tPrec@1 43.750 (41.453)\n",
      "Epoch: [45][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7215 (1.6247)\tPrec@1 31.250 (41.496)\n",
      "EPOCH: 45 train Results: Prec@1 41.496 Loss: 1.6247\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9866 (0.9866)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4115 (1.4931)\tPrec@1 43.750 (46.530)\n",
      "EPOCH: 45 val Results: Prec@1 46.530 Loss: 1.4931\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/3124]\tTime 0.002 (0.002)\tLoss 1.5733 (1.5733)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [46][625/3124]\tTime 0.001 (0.001)\tLoss 1.4632 (1.6092)\tPrec@1 37.500 (42.252)\n",
      "Epoch: [46][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4238 (1.6149)\tPrec@1 31.250 (42.261)\n",
      "Epoch: [46][1875/3124]\tTime 0.002 (0.001)\tLoss 1.5714 (1.6209)\tPrec@1 56.250 (42.044)\n",
      "Epoch: [46][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8005 (1.6229)\tPrec@1 25.000 (41.866)\n",
      "Epoch: [46][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6060 (1.6232)\tPrec@1 43.750 (41.912)\n",
      "EPOCH: 46 train Results: Prec@1 41.912 Loss: 1.6232\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.0811 (1.0811)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2982 (1.5335)\tPrec@1 37.500 (45.420)\n",
      "EPOCH: 46 val Results: Prec@1 45.420 Loss: 1.5335\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/3124]\tTime 0.001 (0.001)\tLoss 1.6751 (1.6751)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [47][625/3124]\tTime 0.001 (0.001)\tLoss 1.7923 (1.6149)\tPrec@1 37.500 (42.332)\n",
      "Epoch: [47][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9337 (1.6176)\tPrec@1 43.750 (42.091)\n",
      "Epoch: [47][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8267 (1.6225)\tPrec@1 25.000 (41.884)\n",
      "Epoch: [47][2500/3124]\tTime 0.001 (0.001)\tLoss 2.0959 (1.6263)\tPrec@1 18.750 (41.721)\n",
      "Epoch: [47][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6411 (1.6284)\tPrec@1 50.000 (41.648)\n",
      "EPOCH: 47 train Results: Prec@1 41.648 Loss: 1.6284\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9709 (0.9709)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4335 (1.4912)\tPrec@1 18.750 (45.850)\n",
      "EPOCH: 47 val Results: Prec@1 45.850 Loss: 1.4912\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/3124]\tTime 0.001 (0.001)\tLoss 1.6811 (1.6811)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [48][625/3124]\tTime 0.001 (0.001)\tLoss 1.7027 (1.6150)\tPrec@1 43.750 (42.292)\n",
      "Epoch: [48][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7818 (1.6180)\tPrec@1 31.250 (42.221)\n",
      "Epoch: [48][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9619 (1.6199)\tPrec@1 25.000 (42.014)\n",
      "Epoch: [48][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3941 (1.6246)\tPrec@1 50.000 (41.636)\n",
      "Epoch: [48][3124/3124]\tTime 0.003 (0.001)\tLoss 1.7776 (1.6261)\tPrec@1 37.500 (41.592)\n",
      "EPOCH: 48 train Results: Prec@1 41.592 Loss: 1.6261\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1349 (1.1349)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5159 (1.5356)\tPrec@1 25.000 (44.540)\n",
      "EPOCH: 48 val Results: Prec@1 44.540 Loss: 1.5356\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/3124]\tTime 0.003 (0.003)\tLoss 2.1813 (2.1813)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [49][625/3124]\tTime 0.001 (0.001)\tLoss 1.5390 (1.6036)\tPrec@1 31.250 (42.702)\n",
      "Epoch: [49][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3863 (1.6147)\tPrec@1 62.500 (42.096)\n",
      "Epoch: [49][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4376 (1.6203)\tPrec@1 37.500 (41.914)\n",
      "Epoch: [49][2500/3124]\tTime 0.002 (0.001)\tLoss 1.7565 (1.6222)\tPrec@1 31.250 (41.976)\n",
      "Epoch: [49][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5041 (1.6252)\tPrec@1 43.750 (41.774)\n",
      "EPOCH: 49 train Results: Prec@1 41.774 Loss: 1.6252\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1180 (1.1180)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5240 (1.5113)\tPrec@1 31.250 (44.840)\n",
      "EPOCH: 49 val Results: Prec@1 44.840 Loss: 1.5113\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/3124]\tTime 0.002 (0.002)\tLoss 1.6734 (1.6734)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [50][625/3124]\tTime 0.001 (0.001)\tLoss 1.2769 (1.6116)\tPrec@1 56.250 (42.113)\n",
      "Epoch: [50][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3734 (1.6118)\tPrec@1 43.750 (42.176)\n",
      "Epoch: [50][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8850 (1.6175)\tPrec@1 50.000 (42.098)\n",
      "Epoch: [50][2500/3124]\tTime 0.003 (0.001)\tLoss 1.3404 (1.6231)\tPrec@1 50.000 (41.838)\n",
      "Epoch: [50][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5472 (1.6261)\tPrec@1 50.000 (41.732)\n",
      "EPOCH: 50 train Results: Prec@1 41.732 Loss: 1.6261\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.8825 (0.8825)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6592 (1.5099)\tPrec@1 37.500 (45.260)\n",
      "EPOCH: 50 val Results: Prec@1 45.260 Loss: 1.5099\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/3124]\tTime 0.004 (0.004)\tLoss 1.6330 (1.6330)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [51][625/3124]\tTime 0.001 (0.001)\tLoss 2.2015 (1.6118)\tPrec@1 18.750 (42.083)\n",
      "Epoch: [51][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6204 (1.6263)\tPrec@1 37.500 (41.772)\n",
      "Epoch: [51][1875/3124]\tTime 0.002 (0.001)\tLoss 1.8874 (1.6235)\tPrec@1 43.750 (41.828)\n",
      "Epoch: [51][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6249 (1.6232)\tPrec@1 56.250 (41.838)\n",
      "Epoch: [51][3124/3124]\tTime 0.006 (0.001)\tLoss 1.8382 (1.6277)\tPrec@1 25.000 (41.736)\n",
      "EPOCH: 51 train Results: Prec@1 41.736 Loss: 1.6277\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1893 (1.1893)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4936 (1.5308)\tPrec@1 18.750 (44.190)\n",
      "EPOCH: 51 val Results: Prec@1 44.190 Loss: 1.5308\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/3124]\tTime 0.001 (0.001)\tLoss 1.3608 (1.3608)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [52][625/3124]\tTime 0.001 (0.001)\tLoss 1.5662 (1.6029)\tPrec@1 43.750 (43.231)\n",
      "Epoch: [52][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6208 (1.6094)\tPrec@1 37.500 (42.536)\n",
      "Epoch: [52][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7639 (1.6130)\tPrec@1 25.000 (42.371)\n",
      "Epoch: [52][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7149 (1.6161)\tPrec@1 43.750 (42.273)\n",
      "Epoch: [52][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6608 (1.6226)\tPrec@1 37.500 (42.004)\n",
      "EPOCH: 52 train Results: Prec@1 42.004 Loss: 1.6226\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2815 (1.2815)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5831 (1.5428)\tPrec@1 56.250 (44.330)\n",
      "EPOCH: 52 val Results: Prec@1 44.330 Loss: 1.5428\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/3124]\tTime 0.001 (0.001)\tLoss 1.2417 (1.2417)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [53][625/3124]\tTime 0.001 (0.001)\tLoss 2.0773 (1.6128)\tPrec@1 25.000 (42.752)\n",
      "Epoch: [53][1250/3124]\tTime 0.001 (0.001)\tLoss 2.0546 (1.6196)\tPrec@1 37.500 (42.201)\n",
      "Epoch: [53][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6005 (1.6243)\tPrec@1 43.750 (41.921)\n",
      "Epoch: [53][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8000 (1.6275)\tPrec@1 25.000 (41.821)\n",
      "Epoch: [53][3124/3124]\tTime 0.001 (0.001)\tLoss 2.1203 (1.6301)\tPrec@1 18.750 (41.808)\n",
      "EPOCH: 53 train Results: Prec@1 41.808 Loss: 1.6301\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2736 (1.2736)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3982 (1.5170)\tPrec@1 37.500 (45.070)\n",
      "EPOCH: 53 val Results: Prec@1 45.070 Loss: 1.5170\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/3124]\tTime 0.002 (0.002)\tLoss 1.5827 (1.5827)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [54][625/3124]\tTime 0.001 (0.001)\tLoss 1.1797 (1.6207)\tPrec@1 68.750 (41.633)\n",
      "Epoch: [54][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3685 (1.6233)\tPrec@1 62.500 (41.467)\n",
      "Epoch: [54][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8705 (1.6235)\tPrec@1 37.500 (41.498)\n",
      "Epoch: [54][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9417 (1.6273)\tPrec@1 31.250 (41.378)\n",
      "Epoch: [54][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4014 (1.6294)\tPrec@1 31.250 (41.338)\n",
      "EPOCH: 54 train Results: Prec@1 41.338 Loss: 1.6294\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9345 (0.9345)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3187 (1.5291)\tPrec@1 37.500 (45.250)\n",
      "EPOCH: 54 val Results: Prec@1 45.250 Loss: 1.5291\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/3124]\tTime 0.002 (0.002)\tLoss 1.7888 (1.7888)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [55][625/3124]\tTime 0.001 (0.001)\tLoss 1.8794 (1.6213)\tPrec@1 50.000 (42.053)\n",
      "Epoch: [55][1250/3124]\tTime 0.002 (0.001)\tLoss 1.9429 (1.6267)\tPrec@1 31.250 (41.847)\n",
      "Epoch: [55][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5809 (1.6234)\tPrec@1 43.750 (42.068)\n",
      "Epoch: [55][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7631 (1.6225)\tPrec@1 37.500 (42.033)\n",
      "Epoch: [55][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4561 (1.6246)\tPrec@1 31.250 (41.942)\n",
      "EPOCH: 55 train Results: Prec@1 41.942 Loss: 1.6246\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.4732 (1.4732)\tPrec@1 37.500 (37.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5963 (1.5537)\tPrec@1 18.750 (44.060)\n",
      "EPOCH: 55 val Results: Prec@1 44.060 Loss: 1.5537\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/3124]\tTime 0.004 (0.004)\tLoss 1.6051 (1.6051)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [56][625/3124]\tTime 0.002 (0.001)\tLoss 1.2284 (1.6285)\tPrec@1 50.000 (40.954)\n",
      "Epoch: [56][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3391 (1.6223)\tPrec@1 56.250 (41.407)\n",
      "Epoch: [56][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4810 (1.6176)\tPrec@1 37.500 (41.564)\n",
      "Epoch: [56][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8772 (1.6202)\tPrec@1 31.250 (41.586)\n",
      "Epoch: [56][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5733 (1.6237)\tPrec@1 50.000 (41.560)\n",
      "EPOCH: 56 train Results: Prec@1 41.560 Loss: 1.6237\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.1847 (1.1847)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5046 (1.5137)\tPrec@1 31.250 (45.640)\n",
      "EPOCH: 56 val Results: Prec@1 45.640 Loss: 1.5137\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/3124]\tTime 0.003 (0.003)\tLoss 2.0113 (2.0113)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [57][625/3124]\tTime 0.001 (0.001)\tLoss 1.3460 (1.6057)\tPrec@1 68.750 (42.262)\n",
      "Epoch: [57][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4720 (1.6142)\tPrec@1 37.500 (41.867)\n",
      "Epoch: [57][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3472 (1.6171)\tPrec@1 43.750 (41.811)\n",
      "Epoch: [57][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3376 (1.6222)\tPrec@1 43.750 (41.741)\n",
      "Epoch: [57][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5684 (1.6256)\tPrec@1 43.750 (41.666)\n",
      "EPOCH: 57 train Results: Prec@1 41.666 Loss: 1.6256\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1572 (1.1572)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4388 (1.5212)\tPrec@1 43.750 (44.870)\n",
      "EPOCH: 57 val Results: Prec@1 44.870 Loss: 1.5212\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/3124]\tTime 0.001 (0.001)\tLoss 1.9017 (1.9017)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [58][625/3124]\tTime 0.002 (0.001)\tLoss 1.4976 (1.6108)\tPrec@1 37.500 (42.083)\n",
      "Epoch: [58][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8242 (1.6230)\tPrec@1 31.250 (41.941)\n",
      "Epoch: [58][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7428 (1.6291)\tPrec@1 43.750 (41.801)\n",
      "Epoch: [58][2500/3124]\tTime 0.001 (0.001)\tLoss 1.0392 (1.6273)\tPrec@1 68.750 (41.923)\n",
      "Epoch: [58][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5266 (1.6317)\tPrec@1 37.500 (41.698)\n",
      "EPOCH: 58 train Results: Prec@1 41.698 Loss: 1.6317\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2130 (1.2130)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3227 (1.5060)\tPrec@1 31.250 (46.000)\n",
      "EPOCH: 58 val Results: Prec@1 46.000 Loss: 1.5060\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/3124]\tTime 0.002 (0.002)\tLoss 1.5872 (1.5872)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [59][625/3124]\tTime 0.001 (0.001)\tLoss 2.0529 (1.6059)\tPrec@1 18.750 (42.113)\n",
      "Epoch: [59][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7062 (1.6090)\tPrec@1 37.500 (41.857)\n",
      "Epoch: [59][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4958 (1.6181)\tPrec@1 50.000 (41.688)\n",
      "Epoch: [59][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7638 (1.6226)\tPrec@1 37.500 (41.663)\n",
      "Epoch: [59][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7020 (1.6266)\tPrec@1 37.500 (41.574)\n",
      "EPOCH: 59 train Results: Prec@1 41.574 Loss: 1.6266\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0076 (1.0076)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3026 (1.5164)\tPrec@1 50.000 (45.760)\n",
      "EPOCH: 59 val Results: Prec@1 45.760 Loss: 1.5164\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/3124]\tTime 0.003 (0.003)\tLoss 1.2942 (1.2942)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [60][625/3124]\tTime 0.001 (0.001)\tLoss 1.7284 (1.6162)\tPrec@1 37.500 (42.612)\n",
      "Epoch: [60][1250/3124]\tTime 0.002 (0.001)\tLoss 1.8363 (1.6207)\tPrec@1 43.750 (42.181)\n",
      "Epoch: [60][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4837 (1.6248)\tPrec@1 43.750 (41.974)\n",
      "Epoch: [60][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3738 (1.6280)\tPrec@1 50.000 (41.701)\n",
      "Epoch: [60][3124/3124]\tTime 0.001 (0.001)\tLoss 1.1423 (1.6265)\tPrec@1 56.250 (41.760)\n",
      "EPOCH: 60 train Results: Prec@1 41.760 Loss: 1.6265\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2382 (1.2382)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3692 (1.4974)\tPrec@1 43.750 (46.450)\n",
      "EPOCH: 60 val Results: Prec@1 46.450 Loss: 1.4974\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/3124]\tTime 0.001 (0.001)\tLoss 1.5062 (1.5062)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [61][625/3124]\tTime 0.001 (0.001)\tLoss 1.7858 (1.6067)\tPrec@1 31.250 (41.833)\n",
      "Epoch: [61][1250/3124]\tTime 0.010 (0.001)\tLoss 1.5307 (1.6145)\tPrec@1 50.000 (41.881)\n",
      "Epoch: [61][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6261 (1.6230)\tPrec@1 50.000 (41.748)\n",
      "Epoch: [61][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9415 (1.6289)\tPrec@1 25.000 (41.456)\n",
      "Epoch: [61][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5088 (1.6298)\tPrec@1 31.250 (41.438)\n",
      "EPOCH: 61 train Results: Prec@1 41.438 Loss: 1.6298\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0684 (1.0684)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3607 (1.5055)\tPrec@1 31.250 (45.930)\n",
      "EPOCH: 61 val Results: Prec@1 45.930 Loss: 1.5055\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/3124]\tTime 0.001 (0.001)\tLoss 1.7799 (1.7799)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [62][625/3124]\tTime 0.001 (0.002)\tLoss 1.8864 (1.6322)\tPrec@1 37.500 (41.314)\n",
      "Epoch: [62][1250/3124]\tTime 0.004 (0.002)\tLoss 1.5908 (1.6282)\tPrec@1 50.000 (41.722)\n",
      "Epoch: [62][1875/3124]\tTime 0.001 (0.001)\tLoss 1.1519 (1.6232)\tPrec@1 62.500 (41.864)\n",
      "Epoch: [62][2500/3124]\tTime 0.002 (0.001)\tLoss 1.7211 (1.6240)\tPrec@1 37.500 (41.743)\n",
      "Epoch: [62][3124/3124]\tTime 0.001 (0.001)\tLoss 1.0439 (1.6252)\tPrec@1 68.750 (41.666)\n",
      "EPOCH: 62 train Results: Prec@1 41.666 Loss: 1.6252\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.8710 (0.8710)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3632 (1.5056)\tPrec@1 43.750 (45.480)\n",
      "EPOCH: 62 val Results: Prec@1 45.480 Loss: 1.5056\n",
      "Best Prec@1: 46.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/3124]\tTime 0.003 (0.003)\tLoss 1.5051 (1.5051)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [63][625/3124]\tTime 0.001 (0.001)\tLoss 1.6500 (1.6130)\tPrec@1 31.250 (42.202)\n",
      "Epoch: [63][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2871 (1.6237)\tPrec@1 37.500 (41.872)\n",
      "Epoch: [63][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6461 (1.6273)\tPrec@1 37.500 (41.858)\n",
      "Epoch: [63][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8731 (1.6298)\tPrec@1 18.750 (41.698)\n",
      "Epoch: [63][3124/3124]\tTime 0.002 (0.001)\tLoss 1.8458 (1.6307)\tPrec@1 31.250 (41.610)\n",
      "EPOCH: 63 train Results: Prec@1 41.610 Loss: 1.6307\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1224 (1.1224)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5461 (1.5051)\tPrec@1 37.500 (46.950)\n",
      "EPOCH: 63 val Results: Prec@1 46.950 Loss: 1.5051\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/3124]\tTime 0.002 (0.002)\tLoss 1.7356 (1.7356)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [64][625/3124]\tTime 0.001 (0.001)\tLoss 1.3283 (1.5936)\tPrec@1 62.500 (42.802)\n",
      "Epoch: [64][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2788 (1.6099)\tPrec@1 56.250 (42.441)\n",
      "Epoch: [64][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9828 (1.6164)\tPrec@1 37.500 (42.041)\n",
      "Epoch: [64][2500/3124]\tTime 0.001 (0.001)\tLoss 1.1646 (1.6193)\tPrec@1 62.500 (41.898)\n",
      "Epoch: [64][3124/3124]\tTime 0.003 (0.001)\tLoss 2.0159 (1.6218)\tPrec@1 18.750 (41.920)\n",
      "EPOCH: 64 train Results: Prec@1 41.920 Loss: 1.6218\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0292 (1.0292)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2712 (1.5209)\tPrec@1 50.000 (45.840)\n",
      "EPOCH: 64 val Results: Prec@1 45.840 Loss: 1.5209\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/3124]\tTime 0.002 (0.002)\tLoss 1.5147 (1.5147)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [65][625/3124]\tTime 0.002 (0.001)\tLoss 2.1578 (1.6071)\tPrec@1 25.000 (42.013)\n",
      "Epoch: [65][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3379 (1.6123)\tPrec@1 68.750 (42.276)\n",
      "Epoch: [65][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5330 (1.6145)\tPrec@1 43.750 (42.121)\n",
      "Epoch: [65][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3369 (1.6200)\tPrec@1 56.250 (41.841)\n",
      "Epoch: [65][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5507 (1.6234)\tPrec@1 50.000 (41.820)\n",
      "EPOCH: 65 train Results: Prec@1 41.820 Loss: 1.6234\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1664 (1.1664)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5812 (1.5167)\tPrec@1 31.250 (45.870)\n",
      "EPOCH: 65 val Results: Prec@1 45.870 Loss: 1.5167\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/3124]\tTime 0.002 (0.002)\tLoss 1.5707 (1.5707)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [66][625/3124]\tTime 0.001 (0.001)\tLoss 2.2240 (1.6048)\tPrec@1 12.500 (42.262)\n",
      "Epoch: [66][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9229 (1.6117)\tPrec@1 37.500 (42.381)\n",
      "Epoch: [66][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3597 (1.6187)\tPrec@1 43.750 (42.028)\n",
      "Epoch: [66][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3784 (1.6230)\tPrec@1 56.250 (41.966)\n",
      "Epoch: [66][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6154 (1.6241)\tPrec@1 43.750 (41.958)\n",
      "EPOCH: 66 train Results: Prec@1 41.958 Loss: 1.6241\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2136 (1.2136)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5220 (1.5107)\tPrec@1 31.250 (46.070)\n",
      "EPOCH: 66 val Results: Prec@1 46.070 Loss: 1.5107\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/3124]\tTime 0.008 (0.008)\tLoss 1.9659 (1.9659)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [67][625/3124]\tTime 0.001 (0.001)\tLoss 1.5544 (1.5992)\tPrec@1 50.000 (42.452)\n",
      "Epoch: [67][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6723 (1.6090)\tPrec@1 31.250 (42.081)\n",
      "Epoch: [67][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8448 (1.6219)\tPrec@1 43.750 (41.708)\n",
      "Epoch: [67][2500/3124]\tTime 0.002 (0.001)\tLoss 1.3901 (1.6246)\tPrec@1 31.250 (41.681)\n",
      "Epoch: [67][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5573 (1.6257)\tPrec@1 31.250 (41.584)\n",
      "EPOCH: 67 train Results: Prec@1 41.584 Loss: 1.6257\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0916 (1.0916)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2052 (1.5180)\tPrec@1 68.750 (45.320)\n",
      "EPOCH: 67 val Results: Prec@1 45.320 Loss: 1.5180\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/3124]\tTime 0.002 (0.002)\tLoss 1.8138 (1.8138)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [68][625/3124]\tTime 0.001 (0.001)\tLoss 1.5224 (1.6009)\tPrec@1 50.000 (42.422)\n",
      "Epoch: [68][1250/3124]\tTime 0.001 (0.004)\tLoss 1.4317 (1.6271)\tPrec@1 62.500 (41.262)\n",
      "Epoch: [68][1875/3124]\tTime 0.001 (0.006)\tLoss 1.7669 (1.6297)\tPrec@1 43.750 (41.335)\n",
      "Epoch: [68][2500/3124]\tTime 0.001 (0.005)\tLoss 1.8355 (1.6283)\tPrec@1 31.250 (41.488)\n",
      "Epoch: [68][3124/3124]\tTime 0.001 (0.004)\tLoss 1.2712 (1.6312)\tPrec@1 62.500 (41.474)\n",
      "EPOCH: 68 train Results: Prec@1 41.474 Loss: 1.6312\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9450 (0.9450)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4286 (1.5097)\tPrec@1 31.250 (45.110)\n",
      "EPOCH: 68 val Results: Prec@1 45.110 Loss: 1.5097\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/3124]\tTime 0.001 (0.001)\tLoss 1.7642 (1.7642)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [69][625/3124]\tTime 0.001 (0.002)\tLoss 1.3415 (1.5944)\tPrec@1 56.250 (43.391)\n",
      "Epoch: [69][1250/3124]\tTime 0.002 (0.002)\tLoss 1.8241 (1.6142)\tPrec@1 31.250 (42.646)\n",
      "Epoch: [69][1875/3124]\tTime 0.001 (0.002)\tLoss 1.2512 (1.6203)\tPrec@1 56.250 (42.254)\n",
      "Epoch: [69][2500/3124]\tTime 0.001 (0.002)\tLoss 1.8803 (1.6241)\tPrec@1 31.250 (41.908)\n",
      "Epoch: [69][3124/3124]\tTime 0.001 (0.002)\tLoss 1.4628 (1.6265)\tPrec@1 56.250 (41.702)\n",
      "EPOCH: 69 train Results: Prec@1 41.702 Loss: 1.6265\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9657 (0.9657)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4774 (1.4986)\tPrec@1 31.250 (46.040)\n",
      "EPOCH: 69 val Results: Prec@1 46.040 Loss: 1.4986\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/3124]\tTime 0.003 (0.003)\tLoss 1.1995 (1.1995)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [70][625/3124]\tTime 0.001 (0.001)\tLoss 1.9028 (1.6066)\tPrec@1 25.000 (43.371)\n",
      "Epoch: [70][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6851 (1.6220)\tPrec@1 37.500 (42.666)\n",
      "Epoch: [70][1875/3124]\tTime 0.005 (0.001)\tLoss 1.6947 (1.6215)\tPrec@1 37.500 (42.367)\n",
      "Epoch: [70][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5011 (1.6299)\tPrec@1 50.000 (41.953)\n",
      "Epoch: [70][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6270 (1.6285)\tPrec@1 50.000 (41.870)\n",
      "EPOCH: 70 train Results: Prec@1 41.870 Loss: 1.6285\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1946 (1.1946)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.1904 (1.5122)\tPrec@1 43.750 (45.530)\n",
      "EPOCH: 70 val Results: Prec@1 45.530 Loss: 1.5122\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/3124]\tTime 0.002 (0.002)\tLoss 1.3544 (1.3544)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [71][625/3124]\tTime 0.001 (0.001)\tLoss 1.7180 (1.6150)\tPrec@1 37.500 (41.673)\n",
      "Epoch: [71][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8322 (1.6227)\tPrec@1 50.000 (41.891)\n",
      "Epoch: [71][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6698 (1.6269)\tPrec@1 37.500 (41.734)\n",
      "Epoch: [71][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4053 (1.6253)\tPrec@1 43.750 (41.916)\n",
      "Epoch: [71][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3969 (1.6275)\tPrec@1 68.750 (41.702)\n",
      "EPOCH: 71 train Results: Prec@1 41.702 Loss: 1.6275\n",
      "Test: [0/624]\tTime 0.005 (0.005)\tLoss 1.2019 (1.2019)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4246 (1.5376)\tPrec@1 50.000 (43.670)\n",
      "EPOCH: 71 val Results: Prec@1 43.670 Loss: 1.5376\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/3124]\tTime 0.001 (0.001)\tLoss 1.7347 (1.7347)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [72][625/3124]\tTime 0.001 (0.001)\tLoss 1.6114 (1.6071)\tPrec@1 50.000 (42.682)\n",
      "Epoch: [72][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4216 (1.6148)\tPrec@1 56.250 (41.911)\n",
      "Epoch: [72][1875/3124]\tTime 0.001 (0.001)\tLoss 2.0769 (1.6244)\tPrec@1 31.250 (41.525)\n",
      "Epoch: [72][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6710 (1.6257)\tPrec@1 43.750 (41.543)\n",
      "Epoch: [72][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9109 (1.6273)\tPrec@1 31.250 (41.580)\n",
      "EPOCH: 72 train Results: Prec@1 41.580 Loss: 1.6273\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0659 (1.0659)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6472 (1.5067)\tPrec@1 25.000 (45.620)\n",
      "EPOCH: 72 val Results: Prec@1 45.620 Loss: 1.5067\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/3124]\tTime 0.001 (0.001)\tLoss 1.7083 (1.7083)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [73][625/3124]\tTime 0.002 (0.001)\tLoss 1.6659 (1.6254)\tPrec@1 56.250 (42.003)\n",
      "Epoch: [73][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9820 (1.6240)\tPrec@1 37.500 (41.867)\n",
      "Epoch: [73][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6547 (1.6223)\tPrec@1 43.750 (42.101)\n",
      "Epoch: [73][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3931 (1.6261)\tPrec@1 50.000 (41.921)\n",
      "Epoch: [73][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8126 (1.6275)\tPrec@1 31.250 (41.812)\n",
      "EPOCH: 73 train Results: Prec@1 41.812 Loss: 1.6275\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0227 (1.0227)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5619 (1.5192)\tPrec@1 25.000 (45.930)\n",
      "EPOCH: 73 val Results: Prec@1 45.930 Loss: 1.5192\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/3124]\tTime 0.003 (0.003)\tLoss 1.5253 (1.5253)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [74][625/3124]\tTime 0.002 (0.001)\tLoss 2.2293 (1.6120)\tPrec@1 18.750 (42.652)\n",
      "Epoch: [74][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5648 (1.6196)\tPrec@1 31.250 (42.481)\n",
      "Epoch: [74][1875/3124]\tTime 0.001 (0.001)\tLoss 1.0986 (1.6236)\tPrec@1 68.750 (42.197)\n",
      "Epoch: [74][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6238 (1.6243)\tPrec@1 31.250 (42.063)\n",
      "Epoch: [74][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5005 (1.6231)\tPrec@1 43.750 (41.978)\n",
      "EPOCH: 74 train Results: Prec@1 41.978 Loss: 1.6231\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0272 (1.0272)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4007 (1.5342)\tPrec@1 31.250 (44.960)\n",
      "EPOCH: 74 val Results: Prec@1 44.960 Loss: 1.5342\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/3124]\tTime 0.003 (0.003)\tLoss 1.6520 (1.6520)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [75][625/3124]\tTime 0.001 (0.001)\tLoss 1.9404 (1.5972)\tPrec@1 31.250 (42.742)\n",
      "Epoch: [75][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3154 (1.6145)\tPrec@1 50.000 (42.051)\n",
      "Epoch: [75][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6563 (1.6194)\tPrec@1 37.500 (41.878)\n",
      "Epoch: [75][2500/3124]\tTime 0.001 (0.001)\tLoss 2.2140 (1.6186)\tPrec@1 25.000 (41.903)\n",
      "Epoch: [75][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5726 (1.6230)\tPrec@1 37.500 (41.710)\n",
      "EPOCH: 75 train Results: Prec@1 41.710 Loss: 1.6230\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 0.9885 (0.9885)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6426 (1.5109)\tPrec@1 37.500 (46.020)\n",
      "EPOCH: 75 val Results: Prec@1 46.020 Loss: 1.5109\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/3124]\tTime 0.011 (0.011)\tLoss 1.5415 (1.5415)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [76][625/3124]\tTime 0.001 (0.001)\tLoss 1.1897 (1.6180)\tPrec@1 50.000 (42.472)\n",
      "Epoch: [76][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6367 (1.6222)\tPrec@1 43.750 (42.036)\n",
      "Epoch: [76][1875/3124]\tTime 0.001 (0.001)\tLoss 1.1466 (1.6209)\tPrec@1 68.750 (41.948)\n",
      "Epoch: [76][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3695 (1.6203)\tPrec@1 56.250 (42.006)\n",
      "Epoch: [76][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6728 (1.6234)\tPrec@1 43.750 (41.934)\n",
      "EPOCH: 76 train Results: Prec@1 41.934 Loss: 1.6234\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2838 (1.2838)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4536 (1.5296)\tPrec@1 37.500 (44.640)\n",
      "EPOCH: 76 val Results: Prec@1 44.640 Loss: 1.5296\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/3124]\tTime 0.001 (0.001)\tLoss 1.6534 (1.6534)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [77][625/3124]\tTime 0.006 (0.001)\tLoss 1.4366 (1.6181)\tPrec@1 43.750 (41.843)\n",
      "Epoch: [77][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8213 (1.6270)\tPrec@1 31.250 (41.562)\n",
      "Epoch: [77][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8969 (1.6271)\tPrec@1 25.000 (41.708)\n",
      "Epoch: [77][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8055 (1.6253)\tPrec@1 31.250 (41.758)\n",
      "Epoch: [77][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8590 (1.6255)\tPrec@1 31.250 (41.700)\n",
      "EPOCH: 77 train Results: Prec@1 41.700 Loss: 1.6255\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0561 (1.0561)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3957 (1.5050)\tPrec@1 18.750 (46.280)\n",
      "EPOCH: 77 val Results: Prec@1 46.280 Loss: 1.5050\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/3124]\tTime 0.001 (0.001)\tLoss 1.7042 (1.7042)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [78][625/3124]\tTime 0.001 (0.001)\tLoss 1.8709 (1.5921)\tPrec@1 31.250 (43.460)\n",
      "Epoch: [78][1250/3124]\tTime 0.003 (0.001)\tLoss 1.4436 (1.6081)\tPrec@1 31.250 (42.971)\n",
      "Epoch: [78][1875/3124]\tTime 0.003 (0.001)\tLoss 1.8892 (1.6181)\tPrec@1 37.500 (42.477)\n",
      "Epoch: [78][2500/3124]\tTime 0.005 (0.001)\tLoss 1.5754 (1.6250)\tPrec@1 56.250 (42.083)\n",
      "Epoch: [78][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3632 (1.6265)\tPrec@1 50.000 (41.932)\n",
      "EPOCH: 78 train Results: Prec@1 41.932 Loss: 1.6265\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0350 (1.0350)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2050 (1.5008)\tPrec@1 37.500 (45.770)\n",
      "EPOCH: 78 val Results: Prec@1 45.770 Loss: 1.5008\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/3124]\tTime 0.002 (0.002)\tLoss 1.5769 (1.5769)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [79][625/3124]\tTime 0.001 (0.001)\tLoss 1.4303 (1.5958)\tPrec@1 50.000 (42.602)\n",
      "Epoch: [79][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5280 (1.6079)\tPrec@1 43.750 (42.361)\n",
      "Epoch: [79][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3666 (1.6144)\tPrec@1 50.000 (42.021)\n",
      "Epoch: [79][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5515 (1.6210)\tPrec@1 37.500 (41.923)\n",
      "Epoch: [79][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8099 (1.6267)\tPrec@1 37.500 (41.674)\n",
      "EPOCH: 79 train Results: Prec@1 41.674 Loss: 1.6267\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0824 (1.0824)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4205 (1.5114)\tPrec@1 25.000 (45.540)\n",
      "EPOCH: 79 val Results: Prec@1 45.540 Loss: 1.5114\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/3124]\tTime 0.003 (0.003)\tLoss 1.8105 (1.8105)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [80][625/3124]\tTime 0.001 (0.001)\tLoss 1.5684 (1.6219)\tPrec@1 43.750 (42.492)\n",
      "Epoch: [80][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2583 (1.6243)\tPrec@1 62.500 (42.081)\n",
      "Epoch: [80][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6567 (1.6325)\tPrec@1 37.500 (41.738)\n",
      "Epoch: [80][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9149 (1.6308)\tPrec@1 37.500 (41.818)\n",
      "Epoch: [80][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9269 (1.6325)\tPrec@1 37.500 (41.776)\n",
      "EPOCH: 80 train Results: Prec@1 41.776 Loss: 1.6325\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1270 (1.1270)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.8182 (1.5179)\tPrec@1 25.000 (45.220)\n",
      "EPOCH: 80 val Results: Prec@1 45.220 Loss: 1.5179\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/3124]\tTime 0.002 (0.002)\tLoss 1.6359 (1.6359)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [81][625/3124]\tTime 0.001 (0.001)\tLoss 1.5844 (1.6069)\tPrec@1 25.000 (42.242)\n",
      "Epoch: [81][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3737 (1.6208)\tPrec@1 56.250 (41.827)\n",
      "Epoch: [81][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3708 (1.6205)\tPrec@1 43.750 (41.898)\n",
      "Epoch: [81][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4535 (1.6235)\tPrec@1 62.500 (41.846)\n",
      "Epoch: [81][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6290 (1.6228)\tPrec@1 43.750 (41.782)\n",
      "EPOCH: 81 train Results: Prec@1 41.782 Loss: 1.6228\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0206 (1.0206)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4948 (1.5076)\tPrec@1 31.250 (46.250)\n",
      "EPOCH: 81 val Results: Prec@1 46.250 Loss: 1.5076\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/3124]\tTime 0.003 (0.003)\tLoss 1.5089 (1.5089)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [82][625/3124]\tTime 0.001 (0.001)\tLoss 1.7485 (1.5972)\tPrec@1 43.750 (42.931)\n",
      "Epoch: [82][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7062 (1.6119)\tPrec@1 25.000 (42.291)\n",
      "Epoch: [82][1875/3124]\tTime 0.001 (0.001)\tLoss 2.1988 (1.6184)\tPrec@1 18.750 (42.048)\n",
      "Epoch: [82][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3139 (1.6189)\tPrec@1 50.000 (42.063)\n",
      "Epoch: [82][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4498 (1.6265)\tPrec@1 56.250 (41.722)\n",
      "EPOCH: 82 train Results: Prec@1 41.722 Loss: 1.6265\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1903 (1.1903)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4231 (1.5153)\tPrec@1 31.250 (45.560)\n",
      "EPOCH: 82 val Results: Prec@1 45.560 Loss: 1.5153\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/3124]\tTime 0.001 (0.001)\tLoss 1.5056 (1.5056)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [83][625/3124]\tTime 0.001 (0.001)\tLoss 1.6768 (1.6038)\tPrec@1 50.000 (43.311)\n",
      "Epoch: [83][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9945 (1.6178)\tPrec@1 18.750 (42.551)\n",
      "Epoch: [83][1875/3124]\tTime 0.003 (0.001)\tLoss 1.5240 (1.6169)\tPrec@1 43.750 (42.427)\n",
      "Epoch: [83][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5206 (1.6243)\tPrec@1 43.750 (42.188)\n",
      "Epoch: [83][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8247 (1.6259)\tPrec@1 43.750 (42.068)\n",
      "EPOCH: 83 train Results: Prec@1 42.068 Loss: 1.6259\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0262 (1.0262)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5609 (1.5122)\tPrec@1 50.000 (44.480)\n",
      "EPOCH: 83 val Results: Prec@1 44.480 Loss: 1.5122\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/3124]\tTime 0.002 (0.002)\tLoss 1.6326 (1.6326)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [84][625/3124]\tTime 0.001 (0.001)\tLoss 1.7566 (1.6251)\tPrec@1 31.250 (41.563)\n",
      "Epoch: [84][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6603 (1.6255)\tPrec@1 25.000 (41.317)\n",
      "Epoch: [84][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6675 (1.6280)\tPrec@1 31.250 (41.468)\n",
      "Epoch: [84][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4407 (1.6279)\tPrec@1 37.500 (41.348)\n",
      "Epoch: [84][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3374 (1.6246)\tPrec@1 50.000 (41.654)\n",
      "EPOCH: 84 train Results: Prec@1 41.654 Loss: 1.6246\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1325 (1.1325)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3845 (1.5331)\tPrec@1 31.250 (44.690)\n",
      "EPOCH: 84 val Results: Prec@1 44.690 Loss: 1.5331\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/3124]\tTime 0.001 (0.001)\tLoss 1.5884 (1.5884)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [85][625/3124]\tTime 0.001 (0.001)\tLoss 1.7626 (1.6041)\tPrec@1 50.000 (43.081)\n",
      "Epoch: [85][1250/3124]\tTime 0.001 (0.001)\tLoss 1.0859 (1.6121)\tPrec@1 62.500 (42.631)\n",
      "Epoch: [85][1875/3124]\tTime 0.001 (0.001)\tLoss 1.1740 (1.6188)\tPrec@1 68.750 (42.351)\n",
      "Epoch: [85][2500/3124]\tTime 0.001 (0.001)\tLoss 1.2747 (1.6245)\tPrec@1 56.250 (41.971)\n",
      "Epoch: [85][3124/3124]\tTime 0.001 (0.001)\tLoss 1.1887 (1.6276)\tPrec@1 62.500 (41.730)\n",
      "EPOCH: 85 train Results: Prec@1 41.730 Loss: 1.6276\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0385 (1.0385)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3658 (1.5218)\tPrec@1 31.250 (44.970)\n",
      "EPOCH: 85 val Results: Prec@1 44.970 Loss: 1.5218\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/3124]\tTime 0.003 (0.003)\tLoss 1.4807 (1.4807)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [86][625/3124]\tTime 0.001 (0.001)\tLoss 1.5128 (1.6369)\tPrec@1 43.750 (41.224)\n",
      "Epoch: [86][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6922 (1.6200)\tPrec@1 31.250 (41.772)\n",
      "Epoch: [86][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5449 (1.6242)\tPrec@1 56.250 (41.648)\n",
      "Epoch: [86][2500/3124]\tTime 0.003 (0.001)\tLoss 1.6973 (1.6275)\tPrec@1 43.750 (41.386)\n",
      "Epoch: [86][3124/3124]\tTime 0.001 (0.001)\tLoss 2.0540 (1.6296)\tPrec@1 37.500 (41.280)\n",
      "EPOCH: 86 train Results: Prec@1 41.280 Loss: 1.6296\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9098 (0.9098)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5019 (1.5072)\tPrec@1 25.000 (46.300)\n",
      "EPOCH: 86 val Results: Prec@1 46.300 Loss: 1.5072\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/3124]\tTime 0.003 (0.003)\tLoss 1.6867 (1.6867)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [87][625/3124]\tTime 0.001 (0.001)\tLoss 1.4162 (1.6109)\tPrec@1 56.250 (42.911)\n",
      "Epoch: [87][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8747 (1.6131)\tPrec@1 31.250 (42.656)\n",
      "Epoch: [87][1875/3124]\tTime 0.002 (0.001)\tLoss 1.4828 (1.6171)\tPrec@1 25.000 (42.234)\n",
      "Epoch: [87][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7232 (1.6238)\tPrec@1 50.000 (41.911)\n",
      "Epoch: [87][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9978 (1.6262)\tPrec@1 56.250 (41.722)\n",
      "EPOCH: 87 train Results: Prec@1 41.722 Loss: 1.6262\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1088 (1.1088)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.1902 (1.5272)\tPrec@1 50.000 (45.420)\n",
      "EPOCH: 87 val Results: Prec@1 45.420 Loss: 1.5272\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/3124]\tTime 0.001 (0.001)\tLoss 1.8531 (1.8531)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [88][625/3124]\tTime 0.001 (0.001)\tLoss 1.6041 (1.6045)\tPrec@1 37.500 (42.362)\n",
      "Epoch: [88][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9665 (1.6126)\tPrec@1 31.250 (42.351)\n",
      "Epoch: [88][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6407 (1.6162)\tPrec@1 37.500 (42.131)\n",
      "Epoch: [88][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6782 (1.6247)\tPrec@1 25.000 (41.733)\n",
      "Epoch: [88][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4017 (1.6296)\tPrec@1 37.500 (41.594)\n",
      "EPOCH: 88 train Results: Prec@1 41.594 Loss: 1.6296\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9076 (0.9076)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6882 (1.4994)\tPrec@1 25.000 (46.660)\n",
      "EPOCH: 88 val Results: Prec@1 46.660 Loss: 1.4994\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/3124]\tTime 0.001 (0.001)\tLoss 1.5901 (1.5901)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [89][625/3124]\tTime 0.001 (0.001)\tLoss 1.6245 (1.6016)\tPrec@1 50.000 (42.053)\n",
      "Epoch: [89][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6347 (1.6110)\tPrec@1 31.250 (41.857)\n",
      "Epoch: [89][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8978 (1.6156)\tPrec@1 37.500 (41.968)\n",
      "Epoch: [89][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6205 (1.6212)\tPrec@1 37.500 (41.846)\n",
      "Epoch: [89][3124/3124]\tTime 0.001 (0.001)\tLoss 2.0475 (1.6251)\tPrec@1 31.250 (41.790)\n",
      "EPOCH: 89 train Results: Prec@1 41.790 Loss: 1.6251\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2401 (1.2401)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6647 (1.5222)\tPrec@1 25.000 (45.290)\n",
      "EPOCH: 89 val Results: Prec@1 45.290 Loss: 1.5222\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/3124]\tTime 0.004 (0.004)\tLoss 1.6796 (1.6796)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [90][625/3124]\tTime 0.001 (0.001)\tLoss 1.4266 (1.6036)\tPrec@1 50.000 (43.301)\n",
      "Epoch: [90][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4137 (1.6170)\tPrec@1 56.250 (42.726)\n",
      "Epoch: [90][1875/3124]\tTime 0.002 (0.001)\tLoss 1.7471 (1.6231)\tPrec@1 25.000 (42.297)\n",
      "Epoch: [90][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7513 (1.6231)\tPrec@1 31.250 (42.173)\n",
      "Epoch: [90][3124/3124]\tTime 0.001 (0.001)\tLoss 1.1051 (1.6289)\tPrec@1 62.500 (41.960)\n",
      "EPOCH: 90 train Results: Prec@1 41.960 Loss: 1.6289\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1978 (1.1978)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7691 (1.5322)\tPrec@1 31.250 (45.080)\n",
      "EPOCH: 90 val Results: Prec@1 45.080 Loss: 1.5322\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/3124]\tTime 0.003 (0.003)\tLoss 1.7400 (1.7400)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [91][625/3124]\tTime 0.002 (0.002)\tLoss 1.6145 (1.5978)\tPrec@1 31.250 (42.502)\n",
      "Epoch: [91][1250/3124]\tTime 0.001 (0.003)\tLoss 1.7394 (1.6123)\tPrec@1 43.750 (42.136)\n",
      "Epoch: [91][1875/3124]\tTime 0.001 (0.003)\tLoss 1.6946 (1.6169)\tPrec@1 43.750 (41.878)\n",
      "Epoch: [91][2500/3124]\tTime 0.001 (0.002)\tLoss 2.3662 (1.6194)\tPrec@1 18.750 (41.793)\n",
      "Epoch: [91][3124/3124]\tTime 0.001 (0.002)\tLoss 1.8373 (1.6268)\tPrec@1 37.500 (41.538)\n",
      "EPOCH: 91 train Results: Prec@1 41.538 Loss: 1.6268\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9548 (0.9548)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.1845 (1.5124)\tPrec@1 50.000 (45.660)\n",
      "EPOCH: 91 val Results: Prec@1 45.660 Loss: 1.5124\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/3124]\tTime 0.003 (0.003)\tLoss 1.6191 (1.6191)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [92][625/3124]\tTime 0.001 (0.001)\tLoss 1.4716 (1.6218)\tPrec@1 50.000 (42.063)\n",
      "Epoch: [92][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6583 (1.6282)\tPrec@1 37.500 (41.767)\n",
      "Epoch: [92][1875/3124]\tTime 0.003 (0.001)\tLoss 1.3429 (1.6280)\tPrec@1 50.000 (41.804)\n",
      "Epoch: [92][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3093 (1.6292)\tPrec@1 62.500 (41.776)\n",
      "Epoch: [92][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4998 (1.6296)\tPrec@1 31.250 (41.746)\n",
      "EPOCH: 92 train Results: Prec@1 41.746 Loss: 1.6296\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1430 (1.1430)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4471 (1.5240)\tPrec@1 31.250 (44.930)\n",
      "EPOCH: 92 val Results: Prec@1 44.930 Loss: 1.5240\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/3124]\tTime 0.004 (0.004)\tLoss 1.4883 (1.4883)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [93][625/3124]\tTime 0.001 (0.001)\tLoss 1.3903 (1.5925)\tPrec@1 50.000 (42.662)\n",
      "Epoch: [93][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9157 (1.6161)\tPrec@1 31.250 (42.371)\n",
      "Epoch: [93][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5015 (1.6227)\tPrec@1 43.750 (42.068)\n",
      "Epoch: [93][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4250 (1.6262)\tPrec@1 37.500 (41.758)\n",
      "Epoch: [93][3124/3124]\tTime 0.001 (0.001)\tLoss 1.2279 (1.6320)\tPrec@1 50.000 (41.510)\n",
      "EPOCH: 93 train Results: Prec@1 41.510 Loss: 1.6320\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1899 (1.1899)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3317 (1.5444)\tPrec@1 37.500 (44.690)\n",
      "EPOCH: 93 val Results: Prec@1 44.690 Loss: 1.5444\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/3124]\tTime 0.002 (0.002)\tLoss 1.3924 (1.3924)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [94][625/3124]\tTime 0.001 (0.001)\tLoss 1.6601 (1.5956)\tPrec@1 37.500 (42.901)\n",
      "Epoch: [94][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5605 (1.6110)\tPrec@1 43.750 (42.216)\n",
      "Epoch: [94][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6794 (1.6180)\tPrec@1 18.750 (41.891)\n",
      "Epoch: [94][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4632 (1.6229)\tPrec@1 56.250 (41.708)\n",
      "Epoch: [94][3124/3124]\tTime 0.001 (0.001)\tLoss 2.0071 (1.6245)\tPrec@1 25.000 (41.756)\n",
      "EPOCH: 94 train Results: Prec@1 41.756 Loss: 1.6245\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0355 (1.0355)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.0538 (1.5248)\tPrec@1 50.000 (45.790)\n",
      "EPOCH: 94 val Results: Prec@1 45.790 Loss: 1.5248\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/3124]\tTime 0.001 (0.001)\tLoss 1.7422 (1.7422)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [95][625/3124]\tTime 0.001 (0.001)\tLoss 1.6118 (1.6065)\tPrec@1 43.750 (42.183)\n",
      "Epoch: [95][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5677 (1.6155)\tPrec@1 43.750 (42.311)\n",
      "Epoch: [95][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4567 (1.6205)\tPrec@1 43.750 (42.008)\n",
      "Epoch: [95][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4182 (1.6233)\tPrec@1 43.750 (41.863)\n",
      "Epoch: [95][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7126 (1.6244)\tPrec@1 37.500 (41.938)\n",
      "EPOCH: 95 train Results: Prec@1 41.938 Loss: 1.6244\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1903 (1.1903)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4176 (1.5144)\tPrec@1 43.750 (45.430)\n",
      "EPOCH: 95 val Results: Prec@1 45.430 Loss: 1.5144\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/3124]\tTime 0.001 (0.001)\tLoss 2.0841 (2.0841)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [96][625/3124]\tTime 0.001 (0.001)\tLoss 1.5996 (1.6156)\tPrec@1 37.500 (42.173)\n",
      "Epoch: [96][1250/3124]\tTime 0.001 (0.001)\tLoss 1.0454 (1.6248)\tPrec@1 62.500 (42.126)\n",
      "Epoch: [96][1875/3124]\tTime 0.002 (0.001)\tLoss 1.8923 (1.6280)\tPrec@1 31.250 (42.134)\n",
      "Epoch: [96][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4555 (1.6278)\tPrec@1 37.500 (41.963)\n",
      "Epoch: [96][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6268 (1.6308)\tPrec@1 25.000 (41.794)\n",
      "EPOCH: 96 train Results: Prec@1 41.794 Loss: 1.6308\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9090 (0.9090)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.1731 (1.5131)\tPrec@1 43.750 (45.760)\n",
      "EPOCH: 96 val Results: Prec@1 45.760 Loss: 1.5131\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/3124]\tTime 0.006 (0.006)\tLoss 1.7411 (1.7411)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [97][625/3124]\tTime 0.001 (0.001)\tLoss 1.5140 (1.6043)\tPrec@1 37.500 (42.232)\n",
      "Epoch: [97][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7556 (1.6158)\tPrec@1 43.750 (41.742)\n",
      "Epoch: [97][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4386 (1.6184)\tPrec@1 43.750 (41.891)\n",
      "Epoch: [97][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3003 (1.6211)\tPrec@1 43.750 (41.868)\n",
      "Epoch: [97][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5894 (1.6244)\tPrec@1 37.500 (41.850)\n",
      "EPOCH: 97 train Results: Prec@1 41.850 Loss: 1.6244\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.2607 (1.2607)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6919 (1.5039)\tPrec@1 31.250 (45.840)\n",
      "EPOCH: 97 val Results: Prec@1 45.840 Loss: 1.5039\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/3124]\tTime 0.002 (0.002)\tLoss 1.7651 (1.7651)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [98][625/3124]\tTime 0.001 (0.001)\tLoss 1.7553 (1.6243)\tPrec@1 37.500 (42.242)\n",
      "Epoch: [98][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7247 (1.6215)\tPrec@1 43.750 (42.436)\n",
      "Epoch: [98][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6446 (1.6234)\tPrec@1 43.750 (42.188)\n",
      "Epoch: [98][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4989 (1.6255)\tPrec@1 56.250 (41.788)\n",
      "Epoch: [98][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5584 (1.6256)\tPrec@1 37.500 (41.772)\n",
      "EPOCH: 98 train Results: Prec@1 41.772 Loss: 1.6256\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0100 (1.0100)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4816 (1.5274)\tPrec@1 25.000 (45.390)\n",
      "EPOCH: 98 val Results: Prec@1 45.390 Loss: 1.5274\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/3124]\tTime 0.002 (0.002)\tLoss 2.0289 (2.0289)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [99][625/3124]\tTime 0.001 (0.001)\tLoss 1.5975 (1.6135)\tPrec@1 37.500 (42.362)\n",
      "Epoch: [99][1250/3124]\tTime 0.002 (0.001)\tLoss 1.4496 (1.6316)\tPrec@1 43.750 (41.352)\n",
      "Epoch: [99][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5588 (1.6269)\tPrec@1 43.750 (41.591)\n",
      "Epoch: [99][2500/3124]\tTime 0.002 (0.001)\tLoss 1.5880 (1.6270)\tPrec@1 50.000 (41.631)\n",
      "Epoch: [99][3124/3124]\tTime 0.001 (0.001)\tLoss 1.2800 (1.6274)\tPrec@1 68.750 (41.682)\n",
      "EPOCH: 99 train Results: Prec@1 41.682 Loss: 1.6274\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0489 (1.0489)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5176 (1.5143)\tPrec@1 37.500 (45.540)\n",
      "EPOCH: 99 val Results: Prec@1 45.540 Loss: 1.5143\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/3124]\tTime 0.002 (0.002)\tLoss 1.6067 (1.6067)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [100][625/3124]\tTime 0.002 (0.001)\tLoss 1.2516 (1.6131)\tPrec@1 62.500 (42.133)\n",
      "Epoch: [100][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3410 (1.6218)\tPrec@1 43.750 (41.817)\n",
      "Epoch: [100][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7240 (1.6255)\tPrec@1 18.750 (41.721)\n",
      "Epoch: [100][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9810 (1.6299)\tPrec@1 25.000 (41.451)\n",
      "Epoch: [100][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5761 (1.6277)\tPrec@1 62.500 (41.470)\n",
      "EPOCH: 100 train Results: Prec@1 41.470 Loss: 1.6277\n",
      "Test: [0/624]\tTime 0.006 (0.006)\tLoss 1.1540 (1.1540)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4056 (1.5140)\tPrec@1 25.000 (46.150)\n",
      "EPOCH: 100 val Results: Prec@1 46.150 Loss: 1.5140\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/3124]\tTime 0.002 (0.002)\tLoss 1.5447 (1.5447)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [101][625/3124]\tTime 0.001 (0.001)\tLoss 1.3323 (1.6292)\tPrec@1 50.000 (41.414)\n",
      "Epoch: [101][1250/3124]\tTime 0.002 (0.001)\tLoss 1.6426 (1.6307)\tPrec@1 25.000 (41.257)\n",
      "Epoch: [101][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6945 (1.6289)\tPrec@1 31.250 (41.261)\n",
      "Epoch: [101][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8247 (1.6255)\tPrec@1 37.500 (41.398)\n",
      "Epoch: [101][3124/3124]\tTime 0.001 (0.001)\tLoss 2.0091 (1.6283)\tPrec@1 25.000 (41.320)\n",
      "EPOCH: 101 train Results: Prec@1 41.320 Loss: 1.6283\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1609 (1.1609)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.001 (0.000)\tLoss 1.5240 (1.5170)\tPrec@1 31.250 (45.550)\n",
      "EPOCH: 101 val Results: Prec@1 45.550 Loss: 1.5170\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/3124]\tTime 0.002 (0.002)\tLoss 1.9106 (1.9106)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [102][625/3124]\tTime 0.001 (0.001)\tLoss 1.3166 (1.6046)\tPrec@1 43.750 (42.692)\n",
      "Epoch: [102][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4690 (1.6189)\tPrec@1 50.000 (41.891)\n",
      "Epoch: [102][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7922 (1.6262)\tPrec@1 25.000 (41.814)\n",
      "Epoch: [102][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7607 (1.6275)\tPrec@1 18.750 (41.783)\n",
      "Epoch: [102][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6149 (1.6277)\tPrec@1 43.750 (41.804)\n",
      "EPOCH: 102 train Results: Prec@1 41.804 Loss: 1.6277\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2694 (1.2694)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5308 (1.5158)\tPrec@1 25.000 (45.910)\n",
      "EPOCH: 102 val Results: Prec@1 45.910 Loss: 1.5158\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/3124]\tTime 0.002 (0.002)\tLoss 1.8540 (1.8540)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [103][625/3124]\tTime 0.001 (0.001)\tLoss 1.2407 (1.6186)\tPrec@1 56.250 (42.113)\n",
      "Epoch: [103][1250/3124]\tTime 0.001 (0.001)\tLoss 2.3258 (1.6219)\tPrec@1 25.000 (41.797)\n",
      "Epoch: [103][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6592 (1.6264)\tPrec@1 43.750 (41.784)\n",
      "Epoch: [103][2500/3124]\tTime 0.003 (0.001)\tLoss 1.7917 (1.6295)\tPrec@1 31.250 (41.736)\n",
      "Epoch: [103][3124/3124]\tTime 0.003 (0.001)\tLoss 1.7845 (1.6269)\tPrec@1 31.250 (41.876)\n",
      "EPOCH: 103 train Results: Prec@1 41.876 Loss: 1.6269\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1922 (1.1922)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.001 (0.000)\tLoss 1.5344 (1.5360)\tPrec@1 25.000 (45.080)\n",
      "EPOCH: 103 val Results: Prec@1 45.080 Loss: 1.5360\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/3124]\tTime 0.003 (0.003)\tLoss 1.8035 (1.8035)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [104][625/3124]\tTime 0.001 (0.001)\tLoss 1.1832 (1.6021)\tPrec@1 62.500 (42.542)\n",
      "Epoch: [104][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4757 (1.6163)\tPrec@1 37.500 (42.066)\n",
      "Epoch: [104][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5286 (1.6209)\tPrec@1 50.000 (41.921)\n",
      "Epoch: [104][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7599 (1.6256)\tPrec@1 37.500 (41.668)\n",
      "Epoch: [104][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5612 (1.6282)\tPrec@1 37.500 (41.522)\n",
      "EPOCH: 104 train Results: Prec@1 41.522 Loss: 1.6282\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1792 (1.1792)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3555 (1.5337)\tPrec@1 31.250 (45.210)\n",
      "EPOCH: 104 val Results: Prec@1 45.210 Loss: 1.5337\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/3124]\tTime 0.007 (0.007)\tLoss 1.7336 (1.7336)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [105][625/3124]\tTime 0.001 (0.001)\tLoss 1.6773 (1.6063)\tPrec@1 43.750 (42.622)\n",
      "Epoch: [105][1250/3124]\tTime 0.001 (0.001)\tLoss 2.1719 (1.6121)\tPrec@1 25.000 (42.231)\n",
      "Epoch: [105][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6487 (1.6089)\tPrec@1 56.250 (42.188)\n",
      "Epoch: [105][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6627 (1.6105)\tPrec@1 50.000 (42.053)\n",
      "Epoch: [105][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5419 (1.6190)\tPrec@1 50.000 (41.904)\n",
      "EPOCH: 105 train Results: Prec@1 41.904 Loss: 1.6190\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1328 (1.1328)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5045 (1.5224)\tPrec@1 25.000 (45.610)\n",
      "EPOCH: 105 val Results: Prec@1 45.610 Loss: 1.5224\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/3124]\tTime 0.001 (0.001)\tLoss 1.7119 (1.7119)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [106][625/3124]\tTime 0.001 (0.001)\tLoss 1.5809 (1.6062)\tPrec@1 50.000 (41.683)\n",
      "Epoch: [106][1250/3124]\tTime 0.005 (0.001)\tLoss 1.5327 (1.6181)\tPrec@1 25.000 (41.572)\n",
      "Epoch: [106][1875/3124]\tTime 0.002 (0.001)\tLoss 1.7446 (1.6220)\tPrec@1 37.500 (41.558)\n",
      "Epoch: [106][2500/3124]\tTime 0.003 (0.001)\tLoss 1.6797 (1.6241)\tPrec@1 37.500 (41.616)\n",
      "Epoch: [106][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5398 (1.6265)\tPrec@1 50.000 (41.642)\n",
      "EPOCH: 106 train Results: Prec@1 41.642 Loss: 1.6265\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1937 (1.1937)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6118 (1.5111)\tPrec@1 37.500 (45.800)\n",
      "EPOCH: 106 val Results: Prec@1 45.800 Loss: 1.5111\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/3124]\tTime 0.001 (0.001)\tLoss 1.3653 (1.3653)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [107][625/3124]\tTime 0.001 (0.001)\tLoss 1.5171 (1.6097)\tPrec@1 43.750 (42.442)\n",
      "Epoch: [107][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7633 (1.6119)\tPrec@1 50.000 (42.256)\n",
      "Epoch: [107][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5710 (1.6151)\tPrec@1 43.750 (41.971)\n",
      "Epoch: [107][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4750 (1.6208)\tPrec@1 43.750 (41.816)\n",
      "Epoch: [107][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5481 (1.6263)\tPrec@1 43.750 (41.616)\n",
      "EPOCH: 107 train Results: Prec@1 41.616 Loss: 1.6263\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 0.9792 (0.9792)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4785 (1.5117)\tPrec@1 25.000 (45.560)\n",
      "EPOCH: 107 val Results: Prec@1 45.560 Loss: 1.5117\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/3124]\tTime 0.001 (0.001)\tLoss 1.4612 (1.4612)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [108][625/3124]\tTime 0.002 (0.001)\tLoss 1.5038 (1.6044)\tPrec@1 56.250 (42.722)\n",
      "Epoch: [108][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6121 (1.6184)\tPrec@1 50.000 (42.041)\n",
      "Epoch: [108][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4419 (1.6245)\tPrec@1 56.250 (41.918)\n",
      "Epoch: [108][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7472 (1.6277)\tPrec@1 37.500 (41.781)\n",
      "Epoch: [108][3124/3124]\tTime 0.002 (0.001)\tLoss 1.3397 (1.6293)\tPrec@1 43.750 (41.774)\n",
      "EPOCH: 108 train Results: Prec@1 41.774 Loss: 1.6293\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.3597 (1.3597)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.8481 (1.5075)\tPrec@1 31.250 (45.900)\n",
      "EPOCH: 108 val Results: Prec@1 45.900 Loss: 1.5075\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/3124]\tTime 0.001 (0.001)\tLoss 1.5804 (1.5804)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [109][625/3124]\tTime 0.002 (0.001)\tLoss 1.7222 (1.6104)\tPrec@1 43.750 (42.652)\n",
      "Epoch: [109][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2823 (1.6221)\tPrec@1 68.750 (42.046)\n",
      "Epoch: [109][1875/3124]\tTime 0.003 (0.001)\tLoss 1.6709 (1.6242)\tPrec@1 37.500 (41.934)\n",
      "Epoch: [109][2500/3124]\tTime 0.007 (0.001)\tLoss 1.6426 (1.6234)\tPrec@1 37.500 (41.951)\n",
      "Epoch: [109][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8750 (1.6251)\tPrec@1 31.250 (41.860)\n",
      "EPOCH: 109 train Results: Prec@1 41.860 Loss: 1.6251\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1579 (1.1579)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3852 (1.5020)\tPrec@1 50.000 (46.270)\n",
      "EPOCH: 109 val Results: Prec@1 46.270 Loss: 1.5020\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/3124]\tTime 0.001 (0.001)\tLoss 1.4716 (1.4716)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [110][625/3124]\tTime 0.001 (0.001)\tLoss 1.4650 (1.6050)\tPrec@1 43.750 (42.392)\n",
      "Epoch: [110][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2902 (1.6175)\tPrec@1 43.750 (42.076)\n",
      "Epoch: [110][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8474 (1.6217)\tPrec@1 31.250 (41.968)\n",
      "Epoch: [110][2500/3124]\tTime 0.002 (0.001)\tLoss 1.7815 (1.6270)\tPrec@1 56.250 (41.811)\n",
      "Epoch: [110][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8594 (1.6278)\tPrec@1 37.500 (41.804)\n",
      "EPOCH: 110 train Results: Prec@1 41.804 Loss: 1.6278\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1117 (1.1117)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5357 (1.5080)\tPrec@1 18.750 (45.890)\n",
      "EPOCH: 110 val Results: Prec@1 45.890 Loss: 1.5080\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/3124]\tTime 0.003 (0.003)\tLoss 1.6706 (1.6706)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [111][625/3124]\tTime 0.001 (0.001)\tLoss 1.8013 (1.6094)\tPrec@1 25.000 (42.722)\n",
      "Epoch: [111][1250/3124]\tTime 0.004 (0.001)\tLoss 1.6061 (1.6186)\tPrec@1 43.750 (42.276)\n",
      "Epoch: [111][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4730 (1.6270)\tPrec@1 50.000 (41.944)\n",
      "Epoch: [111][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6237 (1.6303)\tPrec@1 50.000 (41.803)\n",
      "Epoch: [111][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6709 (1.6287)\tPrec@1 43.750 (41.890)\n",
      "EPOCH: 111 train Results: Prec@1 41.890 Loss: 1.6287\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.3177 (1.3177)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5625 (1.5292)\tPrec@1 25.000 (44.980)\n",
      "EPOCH: 111 val Results: Prec@1 44.980 Loss: 1.5292\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/3124]\tTime 0.002 (0.002)\tLoss 1.6288 (1.6288)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [112][625/3124]\tTime 0.001 (0.001)\tLoss 1.6086 (1.6086)\tPrec@1 56.250 (42.502)\n",
      "Epoch: [112][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2305 (1.6177)\tPrec@1 68.750 (42.116)\n",
      "Epoch: [112][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3121 (1.6165)\tPrec@1 56.250 (42.231)\n",
      "Epoch: [112][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3924 (1.6261)\tPrec@1 50.000 (41.711)\n",
      "Epoch: [112][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7498 (1.6236)\tPrec@1 37.500 (41.792)\n",
      "EPOCH: 112 train Results: Prec@1 41.792 Loss: 1.6236\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1889 (1.1889)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2217 (1.5232)\tPrec@1 56.250 (45.140)\n",
      "EPOCH: 112 val Results: Prec@1 45.140 Loss: 1.5232\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/3124]\tTime 0.001 (0.001)\tLoss 1.6656 (1.6656)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [113][625/3124]\tTime 0.001 (0.001)\tLoss 1.4805 (1.6013)\tPrec@1 50.000 (42.502)\n",
      "Epoch: [113][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9890 (1.6171)\tPrec@1 31.250 (41.732)\n",
      "Epoch: [113][1875/3124]\tTime 0.002 (0.001)\tLoss 1.5261 (1.6202)\tPrec@1 56.250 (41.668)\n",
      "Epoch: [113][2500/3124]\tTime 0.002 (0.001)\tLoss 1.8515 (1.6229)\tPrec@1 43.750 (41.583)\n",
      "Epoch: [113][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6268 (1.6244)\tPrec@1 37.500 (41.642)\n",
      "EPOCH: 113 train Results: Prec@1 41.642 Loss: 1.6244\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1271 (1.1271)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6290 (1.5272)\tPrec@1 25.000 (44.940)\n",
      "EPOCH: 113 val Results: Prec@1 44.940 Loss: 1.5272\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/3124]\tTime 0.001 (0.001)\tLoss 1.3874 (1.3874)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [114][625/3124]\tTime 0.001 (0.001)\tLoss 1.2929 (1.6058)\tPrec@1 50.000 (43.111)\n",
      "Epoch: [114][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3999 (1.6204)\tPrec@1 50.000 (42.016)\n",
      "Epoch: [114][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7444 (1.6224)\tPrec@1 31.250 (41.971)\n",
      "Epoch: [114][2500/3124]\tTime 0.001 (0.001)\tLoss 1.2340 (1.6214)\tPrec@1 56.250 (41.971)\n",
      "Epoch: [114][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8731 (1.6248)\tPrec@1 37.500 (41.812)\n",
      "EPOCH: 114 train Results: Prec@1 41.812 Loss: 1.6248\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2395 (1.2395)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6032 (1.5114)\tPrec@1 37.500 (45.620)\n",
      "EPOCH: 114 val Results: Prec@1 45.620 Loss: 1.5114\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/3124]\tTime 0.001 (0.001)\tLoss 1.2576 (1.2576)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [115][625/3124]\tTime 0.001 (0.001)\tLoss 2.0440 (1.6010)\tPrec@1 18.750 (42.562)\n",
      "Epoch: [115][1250/3124]\tTime 0.003 (0.001)\tLoss 1.3906 (1.6078)\tPrec@1 50.000 (42.101)\n",
      "Epoch: [115][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9066 (1.6224)\tPrec@1 31.250 (41.714)\n",
      "Epoch: [115][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5858 (1.6277)\tPrec@1 43.750 (41.546)\n",
      "Epoch: [115][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4562 (1.6288)\tPrec@1 37.500 (41.410)\n",
      "EPOCH: 115 train Results: Prec@1 41.410 Loss: 1.6288\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0251 (1.0251)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2877 (1.5216)\tPrec@1 43.750 (45.560)\n",
      "EPOCH: 115 val Results: Prec@1 45.560 Loss: 1.5216\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/3124]\tTime 0.002 (0.002)\tLoss 1.5584 (1.5584)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [116][625/3124]\tTime 0.001 (0.001)\tLoss 1.8330 (1.6239)\tPrec@1 18.750 (41.683)\n",
      "Epoch: [116][1250/3124]\tTime 0.002 (0.001)\tLoss 1.7561 (1.6223)\tPrec@1 31.250 (41.916)\n",
      "Epoch: [116][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5278 (1.6237)\tPrec@1 43.750 (41.714)\n",
      "Epoch: [116][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4872 (1.6255)\tPrec@1 43.750 (41.678)\n",
      "Epoch: [116][3124/3124]\tTime 0.001 (0.001)\tLoss 2.0153 (1.6260)\tPrec@1 18.750 (41.644)\n",
      "EPOCH: 116 train Results: Prec@1 41.644 Loss: 1.6260\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0699 (1.0699)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2937 (1.5061)\tPrec@1 43.750 (45.750)\n",
      "EPOCH: 116 val Results: Prec@1 45.750 Loss: 1.5061\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/3124]\tTime 0.003 (0.003)\tLoss 1.1412 (1.1412)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [117][625/3124]\tTime 0.002 (0.001)\tLoss 1.6871 (1.6240)\tPrec@1 37.500 (41.673)\n",
      "Epoch: [117][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8021 (1.6277)\tPrec@1 37.500 (41.572)\n",
      "Epoch: [117][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6219 (1.6286)\tPrec@1 37.500 (41.375)\n",
      "Epoch: [117][2500/3124]\tTime 0.001 (0.001)\tLoss 1.1183 (1.6323)\tPrec@1 62.500 (41.308)\n",
      "Epoch: [117][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7115 (1.6312)\tPrec@1 37.500 (41.526)\n",
      "EPOCH: 117 train Results: Prec@1 41.526 Loss: 1.6312\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 0.7713 (0.7713)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3801 (1.5180)\tPrec@1 43.750 (45.820)\n",
      "EPOCH: 117 val Results: Prec@1 45.820 Loss: 1.5180\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/3124]\tTime 0.001 (0.001)\tLoss 1.7979 (1.7979)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [118][625/3124]\tTime 0.001 (0.001)\tLoss 1.9315 (1.6124)\tPrec@1 43.750 (42.262)\n",
      "Epoch: [118][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4532 (1.6171)\tPrec@1 50.000 (41.916)\n",
      "Epoch: [118][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4103 (1.6227)\tPrec@1 43.750 (41.754)\n",
      "Epoch: [118][2500/3124]\tTime 0.001 (0.001)\tLoss 2.0274 (1.6286)\tPrec@1 37.500 (41.536)\n",
      "Epoch: [118][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7398 (1.6324)\tPrec@1 50.000 (41.380)\n",
      "EPOCH: 118 train Results: Prec@1 41.380 Loss: 1.6324\n",
      "Test: [0/624]\tTime 0.003 (0.003)\tLoss 1.0639 (1.0639)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5426 (1.5201)\tPrec@1 25.000 (45.330)\n",
      "EPOCH: 118 val Results: Prec@1 45.330 Loss: 1.5201\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/3124]\tTime 0.001 (0.001)\tLoss 1.8210 (1.8210)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [119][625/3124]\tTime 0.010 (0.001)\tLoss 1.5318 (1.5902)\tPrec@1 50.000 (42.722)\n",
      "Epoch: [119][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6270 (1.6160)\tPrec@1 50.000 (42.126)\n",
      "Epoch: [119][1875/3124]\tTime 0.001 (0.001)\tLoss 2.1929 (1.6212)\tPrec@1 31.250 (41.928)\n",
      "Epoch: [119][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3414 (1.6254)\tPrec@1 56.250 (41.668)\n",
      "Epoch: [119][3124/3124]\tTime 0.003 (0.001)\tLoss 1.8941 (1.6289)\tPrec@1 31.250 (41.516)\n",
      "EPOCH: 119 train Results: Prec@1 41.516 Loss: 1.6289\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2701 (1.2701)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4558 (1.5389)\tPrec@1 31.250 (44.590)\n",
      "EPOCH: 119 val Results: Prec@1 44.590 Loss: 1.5389\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/3124]\tTime 0.003 (0.003)\tLoss 1.6541 (1.6541)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [120][625/3124]\tTime 0.001 (0.001)\tLoss 1.5960 (1.6053)\tPrec@1 31.250 (42.452)\n",
      "Epoch: [120][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5716 (1.6181)\tPrec@1 43.750 (41.647)\n",
      "Epoch: [120][1875/3124]\tTime 0.001 (0.001)\tLoss 1.1381 (1.6241)\tPrec@1 56.250 (41.788)\n",
      "Epoch: [120][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5372 (1.6245)\tPrec@1 50.000 (41.808)\n",
      "Epoch: [120][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6411 (1.6266)\tPrec@1 37.500 (41.762)\n",
      "EPOCH: 120 train Results: Prec@1 41.762 Loss: 1.6266\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.8744 (0.8744)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2305 (1.4915)\tPrec@1 50.000 (46.290)\n",
      "EPOCH: 120 val Results: Prec@1 46.290 Loss: 1.4915\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/3124]\tTime 0.004 (0.004)\tLoss 1.6161 (1.6161)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [121][625/3124]\tTime 0.005 (0.001)\tLoss 1.5607 (1.5958)\tPrec@1 43.750 (42.632)\n",
      "Epoch: [121][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3131 (1.6189)\tPrec@1 50.000 (42.001)\n",
      "Epoch: [121][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8046 (1.6204)\tPrec@1 18.750 (41.821)\n",
      "Epoch: [121][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7658 (1.6237)\tPrec@1 25.000 (41.751)\n",
      "Epoch: [121][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3112 (1.6256)\tPrec@1 62.500 (41.704)\n",
      "EPOCH: 121 train Results: Prec@1 41.704 Loss: 1.6256\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1774 (1.1774)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4738 (1.5281)\tPrec@1 37.500 (44.510)\n",
      "EPOCH: 121 val Results: Prec@1 44.510 Loss: 1.5281\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/3124]\tTime 0.003 (0.003)\tLoss 1.3919 (1.3919)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [122][625/3124]\tTime 0.001 (0.001)\tLoss 1.5099 (1.6156)\tPrec@1 50.000 (42.013)\n",
      "Epoch: [122][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8203 (1.6149)\tPrec@1 18.750 (42.336)\n",
      "Epoch: [122][1875/3124]\tTime 0.001 (0.001)\tLoss 1.2482 (1.6198)\tPrec@1 50.000 (41.931)\n",
      "Epoch: [122][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5018 (1.6277)\tPrec@1 43.750 (41.751)\n",
      "Epoch: [122][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7369 (1.6305)\tPrec@1 37.500 (41.516)\n",
      "EPOCH: 122 train Results: Prec@1 41.516 Loss: 1.6305\n",
      "Test: [0/624]\tTime 0.005 (0.005)\tLoss 1.1981 (1.1981)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6469 (1.5052)\tPrec@1 25.000 (46.620)\n",
      "EPOCH: 122 val Results: Prec@1 46.620 Loss: 1.5052\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/3124]\tTime 0.002 (0.002)\tLoss 1.3935 (1.3935)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [123][625/3124]\tTime 0.001 (0.001)\tLoss 2.1295 (1.5999)\tPrec@1 18.750 (43.011)\n",
      "Epoch: [123][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4212 (1.6202)\tPrec@1 56.250 (41.792)\n",
      "Epoch: [123][1875/3124]\tTime 0.001 (0.001)\tLoss 2.0035 (1.6239)\tPrec@1 37.500 (41.781)\n",
      "Epoch: [123][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8331 (1.6278)\tPrec@1 31.250 (41.561)\n",
      "Epoch: [123][3124/3124]\tTime 0.002 (0.001)\tLoss 1.3116 (1.6286)\tPrec@1 50.000 (41.582)\n",
      "EPOCH: 123 train Results: Prec@1 41.582 Loss: 1.6286\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1972 (1.1972)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5641 (1.5227)\tPrec@1 50.000 (44.870)\n",
      "EPOCH: 123 val Results: Prec@1 44.870 Loss: 1.5227\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/3124]\tTime 0.001 (0.001)\tLoss 1.4960 (1.4960)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [124][625/3124]\tTime 0.001 (0.001)\tLoss 1.5430 (1.6258)\tPrec@1 56.250 (42.093)\n",
      "Epoch: [124][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5752 (1.6289)\tPrec@1 37.500 (41.911)\n",
      "Epoch: [124][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4001 (1.6314)\tPrec@1 43.750 (41.591)\n",
      "Epoch: [124][2500/3124]\tTime 0.002 (0.001)\tLoss 1.7514 (1.6291)\tPrec@1 31.250 (41.661)\n",
      "Epoch: [124][3124/3124]\tTime 0.002 (0.001)\tLoss 1.6910 (1.6312)\tPrec@1 31.250 (41.572)\n",
      "EPOCH: 124 train Results: Prec@1 41.572 Loss: 1.6312\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1777 (1.1777)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3193 (1.5375)\tPrec@1 50.000 (44.360)\n",
      "EPOCH: 124 val Results: Prec@1 44.360 Loss: 1.5375\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/3124]\tTime 0.001 (0.001)\tLoss 1.8802 (1.8802)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [125][625/3124]\tTime 0.001 (0.001)\tLoss 1.5836 (1.6051)\tPrec@1 43.750 (42.622)\n",
      "Epoch: [125][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5588 (1.6203)\tPrec@1 62.500 (42.021)\n",
      "Epoch: [125][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5236 (1.6242)\tPrec@1 56.250 (41.741)\n",
      "Epoch: [125][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3393 (1.6267)\tPrec@1 56.250 (41.553)\n",
      "Epoch: [125][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6973 (1.6262)\tPrec@1 37.500 (41.670)\n",
      "EPOCH: 125 train Results: Prec@1 41.670 Loss: 1.6262\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1281 (1.1281)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3014 (1.5427)\tPrec@1 37.500 (44.420)\n",
      "EPOCH: 125 val Results: Prec@1 44.420 Loss: 1.5427\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/3124]\tTime 0.002 (0.002)\tLoss 1.4455 (1.4455)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [126][625/3124]\tTime 0.003 (0.001)\tLoss 1.8440 (1.6058)\tPrec@1 43.750 (42.242)\n",
      "Epoch: [126][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4515 (1.6156)\tPrec@1 37.500 (41.956)\n",
      "Epoch: [126][1875/3124]\tTime 0.001 (0.001)\tLoss 2.0179 (1.6258)\tPrec@1 31.250 (41.774)\n",
      "Epoch: [126][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3589 (1.6305)\tPrec@1 50.000 (41.531)\n",
      "Epoch: [126][3124/3124]\tTime 0.001 (0.001)\tLoss 1.1922 (1.6320)\tPrec@1 68.750 (41.388)\n",
      "EPOCH: 126 train Results: Prec@1 41.388 Loss: 1.6320\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.3654 (1.3654)\tPrec@1 37.500 (37.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2979 (1.5226)\tPrec@1 50.000 (44.940)\n",
      "EPOCH: 126 val Results: Prec@1 44.940 Loss: 1.5226\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/3124]\tTime 0.001 (0.001)\tLoss 1.5359 (1.5359)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [127][625/3124]\tTime 0.001 (0.001)\tLoss 1.8493 (1.6186)\tPrec@1 31.250 (42.472)\n",
      "Epoch: [127][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5303 (1.6216)\tPrec@1 37.500 (42.081)\n",
      "Epoch: [127][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6956 (1.6238)\tPrec@1 43.750 (42.064)\n",
      "Epoch: [127][2500/3124]\tTime 0.001 (0.001)\tLoss 2.0293 (1.6292)\tPrec@1 18.750 (41.763)\n",
      "Epoch: [127][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3678 (1.6299)\tPrec@1 56.250 (41.742)\n",
      "EPOCH: 127 train Results: Prec@1 41.742 Loss: 1.6299\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0928 (1.0928)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5215 (1.5038)\tPrec@1 25.000 (45.690)\n",
      "EPOCH: 127 val Results: Prec@1 45.690 Loss: 1.5038\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/3124]\tTime 0.003 (0.003)\tLoss 1.1893 (1.1893)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [128][625/3124]\tTime 0.001 (0.001)\tLoss 1.3761 (1.6071)\tPrec@1 62.500 (42.752)\n",
      "Epoch: [128][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6528 (1.6182)\tPrec@1 18.750 (42.026)\n",
      "Epoch: [128][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4206 (1.6243)\tPrec@1 50.000 (41.781)\n",
      "Epoch: [128][2500/3124]\tTime 0.002 (0.001)\tLoss 1.4758 (1.6243)\tPrec@1 43.750 (41.711)\n",
      "Epoch: [128][3124/3124]\tTime 0.002 (0.001)\tLoss 1.6196 (1.6264)\tPrec@1 56.250 (41.752)\n",
      "EPOCH: 128 train Results: Prec@1 41.752 Loss: 1.6264\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.3162 (1.3162)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3727 (1.5280)\tPrec@1 50.000 (44.140)\n",
      "EPOCH: 128 val Results: Prec@1 44.140 Loss: 1.5280\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/3124]\tTime 0.003 (0.003)\tLoss 2.0238 (2.0238)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [129][625/3124]\tTime 0.001 (0.001)\tLoss 2.0508 (1.5996)\tPrec@1 31.250 (42.652)\n",
      "Epoch: [129][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3015 (1.6102)\tPrec@1 56.250 (42.161)\n",
      "Epoch: [129][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7238 (1.6211)\tPrec@1 37.500 (41.724)\n",
      "Epoch: [129][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3650 (1.6222)\tPrec@1 50.000 (41.758)\n",
      "Epoch: [129][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6977 (1.6280)\tPrec@1 56.250 (41.550)\n",
      "EPOCH: 129 train Results: Prec@1 41.550 Loss: 1.6280\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0754 (1.0754)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6592 (1.5202)\tPrec@1 37.500 (44.980)\n",
      "EPOCH: 129 val Results: Prec@1 44.980 Loss: 1.5202\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/3124]\tTime 0.001 (0.001)\tLoss 1.6504 (1.6504)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [130][625/3124]\tTime 0.001 (0.001)\tLoss 1.5454 (1.6033)\tPrec@1 37.500 (42.322)\n",
      "Epoch: [130][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4532 (1.6145)\tPrec@1 37.500 (41.996)\n",
      "Epoch: [130][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4970 (1.6248)\tPrec@1 50.000 (41.648)\n",
      "Epoch: [130][2500/3124]\tTime 0.002 (0.001)\tLoss 1.7755 (1.6272)\tPrec@1 37.500 (41.593)\n",
      "Epoch: [130][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7735 (1.6302)\tPrec@1 31.250 (41.486)\n",
      "EPOCH: 130 train Results: Prec@1 41.486 Loss: 1.6302\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1328 (1.1328)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3669 (1.5017)\tPrec@1 43.750 (45.730)\n",
      "EPOCH: 130 val Results: Prec@1 45.730 Loss: 1.5017\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/3124]\tTime 0.003 (0.003)\tLoss 1.5712 (1.5712)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [131][625/3124]\tTime 0.001 (0.001)\tLoss 1.3549 (1.6104)\tPrec@1 43.750 (42.442)\n",
      "Epoch: [131][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5528 (1.6224)\tPrec@1 43.750 (41.752)\n",
      "Epoch: [131][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7507 (1.6241)\tPrec@1 31.250 (41.824)\n",
      "Epoch: [131][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5082 (1.6272)\tPrec@1 50.000 (41.688)\n",
      "Epoch: [131][3124/3124]\tTime 0.003 (0.001)\tLoss 2.2204 (1.6293)\tPrec@1 6.250 (41.464)\n",
      "EPOCH: 131 train Results: Prec@1 41.464 Loss: 1.6293\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0658 (1.0658)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4215 (1.5110)\tPrec@1 31.250 (45.950)\n",
      "EPOCH: 131 val Results: Prec@1 45.950 Loss: 1.5110\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/3124]\tTime 0.001 (0.001)\tLoss 1.7401 (1.7401)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [132][625/3124]\tTime 0.001 (0.001)\tLoss 1.7800 (1.6230)\tPrec@1 18.750 (41.913)\n",
      "Epoch: [132][1250/3124]\tTime 0.001 (0.001)\tLoss 2.0339 (1.6266)\tPrec@1 25.000 (41.717)\n",
      "Epoch: [132][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3427 (1.6315)\tPrec@1 56.250 (41.488)\n",
      "Epoch: [132][2500/3124]\tTime 0.010 (0.001)\tLoss 1.7619 (1.6310)\tPrec@1 43.750 (41.416)\n",
      "Epoch: [132][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8252 (1.6327)\tPrec@1 31.250 (41.318)\n",
      "EPOCH: 132 train Results: Prec@1 41.318 Loss: 1.6327\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9747 (0.9747)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3892 (1.5185)\tPrec@1 31.250 (45.790)\n",
      "EPOCH: 132 val Results: Prec@1 45.790 Loss: 1.5185\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/3124]\tTime 0.001 (0.001)\tLoss 1.4601 (1.4601)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [133][625/3124]\tTime 0.001 (0.001)\tLoss 1.5226 (1.6211)\tPrec@1 56.250 (42.602)\n",
      "Epoch: [133][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4652 (1.6171)\tPrec@1 50.000 (42.016)\n",
      "Epoch: [133][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4485 (1.6210)\tPrec@1 50.000 (41.734)\n",
      "Epoch: [133][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5551 (1.6299)\tPrec@1 50.000 (41.506)\n",
      "Epoch: [133][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5656 (1.6309)\tPrec@1 50.000 (41.534)\n",
      "EPOCH: 133 train Results: Prec@1 41.534 Loss: 1.6309\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0474 (1.0474)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3152 (1.5230)\tPrec@1 62.500 (45.840)\n",
      "EPOCH: 133 val Results: Prec@1 45.840 Loss: 1.5230\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/3124]\tTime 0.003 (0.003)\tLoss 1.8799 (1.8799)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [134][625/3124]\tTime 0.001 (0.001)\tLoss 1.5026 (1.6267)\tPrec@1 56.250 (41.134)\n",
      "Epoch: [134][1250/3124]\tTime 0.001 (0.001)\tLoss 1.1887 (1.6318)\tPrec@1 56.250 (41.297)\n",
      "Epoch: [134][1875/3124]\tTime 0.002 (0.001)\tLoss 1.3142 (1.6295)\tPrec@1 62.500 (41.401)\n",
      "Epoch: [134][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6292 (1.6288)\tPrec@1 37.500 (41.546)\n",
      "Epoch: [134][3124/3124]\tTime 0.001 (0.001)\tLoss 2.1509 (1.6313)\tPrec@1 31.250 (41.552)\n",
      "EPOCH: 134 train Results: Prec@1 41.552 Loss: 1.6313\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1904 (1.1904)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5374 (1.5028)\tPrec@1 37.500 (45.930)\n",
      "EPOCH: 134 val Results: Prec@1 45.930 Loss: 1.5028\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/3124]\tTime 0.024 (0.024)\tLoss 1.5926 (1.5926)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [135][625/3124]\tTime 0.001 (0.001)\tLoss 1.6138 (1.6102)\tPrec@1 50.000 (41.843)\n",
      "Epoch: [135][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4662 (1.6178)\tPrec@1 43.750 (41.817)\n",
      "Epoch: [135][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9775 (1.6198)\tPrec@1 43.750 (41.698)\n",
      "Epoch: [135][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5787 (1.6245)\tPrec@1 25.000 (41.636)\n",
      "Epoch: [135][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5489 (1.6286)\tPrec@1 31.250 (41.464)\n",
      "EPOCH: 135 train Results: Prec@1 41.464 Loss: 1.6286\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2495 (1.2495)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4420 (1.4911)\tPrec@1 31.250 (46.510)\n",
      "EPOCH: 135 val Results: Prec@1 46.510 Loss: 1.4911\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/3124]\tTime 0.001 (0.001)\tLoss 1.3062 (1.3062)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [136][625/3124]\tTime 0.001 (0.001)\tLoss 1.8197 (1.6026)\tPrec@1 37.500 (42.831)\n",
      "Epoch: [136][1250/3124]\tTime 0.002 (0.001)\tLoss 1.3722 (1.6108)\tPrec@1 62.500 (42.651)\n",
      "Epoch: [136][1875/3124]\tTime 0.001 (0.001)\tLoss 2.2744 (1.6173)\tPrec@1 12.500 (42.267)\n",
      "Epoch: [136][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5416 (1.6224)\tPrec@1 43.750 (42.151)\n",
      "Epoch: [136][3124/3124]\tTime 0.001 (0.001)\tLoss 2.0929 (1.6254)\tPrec@1 31.250 (41.970)\n",
      "EPOCH: 136 train Results: Prec@1 41.970 Loss: 1.6254\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2506 (1.2506)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2638 (1.5278)\tPrec@1 37.500 (45.450)\n",
      "EPOCH: 136 val Results: Prec@1 45.450 Loss: 1.5278\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/3124]\tTime 0.001 (0.001)\tLoss 1.7481 (1.7481)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [137][625/3124]\tTime 0.001 (0.001)\tLoss 1.7987 (1.6280)\tPrec@1 37.500 (42.382)\n",
      "Epoch: [137][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9150 (1.6230)\tPrec@1 31.250 (42.246)\n",
      "Epoch: [137][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6940 (1.6208)\tPrec@1 43.750 (42.084)\n",
      "Epoch: [137][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3060 (1.6260)\tPrec@1 68.750 (41.863)\n",
      "Epoch: [137][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7404 (1.6280)\tPrec@1 56.250 (41.812)\n",
      "EPOCH: 137 train Results: Prec@1 41.812 Loss: 1.6280\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0045 (1.0045)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7344 (1.5146)\tPrec@1 18.750 (46.010)\n",
      "EPOCH: 137 val Results: Prec@1 46.010 Loss: 1.5146\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/3124]\tTime 0.002 (0.002)\tLoss 1.6398 (1.6398)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [138][625/3124]\tTime 0.002 (0.001)\tLoss 1.7070 (1.6196)\tPrec@1 31.250 (41.803)\n",
      "Epoch: [138][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6832 (1.6279)\tPrec@1 37.500 (41.532)\n",
      "Epoch: [138][1875/3124]\tTime 0.001 (0.001)\tLoss 2.0926 (1.6251)\tPrec@1 18.750 (41.631)\n",
      "Epoch: [138][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6926 (1.6273)\tPrec@1 50.000 (41.708)\n",
      "Epoch: [138][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3329 (1.6304)\tPrec@1 50.000 (41.470)\n",
      "EPOCH: 138 train Results: Prec@1 41.470 Loss: 1.6304\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0969 (1.0969)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5006 (1.5178)\tPrec@1 31.250 (45.530)\n",
      "EPOCH: 138 val Results: Prec@1 45.530 Loss: 1.5178\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/3124]\tTime 0.003 (0.003)\tLoss 1.7261 (1.7261)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [139][625/3124]\tTime 0.001 (0.001)\tLoss 1.7932 (1.6160)\tPrec@1 31.250 (41.973)\n",
      "Epoch: [139][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3726 (1.6172)\tPrec@1 62.500 (41.926)\n",
      "Epoch: [139][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4983 (1.6195)\tPrec@1 56.250 (42.111)\n",
      "Epoch: [139][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6394 (1.6246)\tPrec@1 43.750 (41.831)\n",
      "Epoch: [139][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7482 (1.6265)\tPrec@1 37.500 (41.742)\n",
      "EPOCH: 139 train Results: Prec@1 41.742 Loss: 1.6265\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.4508 (1.4508)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5538 (1.5200)\tPrec@1 31.250 (45.560)\n",
      "EPOCH: 139 val Results: Prec@1 45.560 Loss: 1.5200\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/3124]\tTime 0.001 (0.001)\tLoss 2.2828 (2.2828)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [140][625/3124]\tTime 0.001 (0.001)\tLoss 1.9079 (1.6046)\tPrec@1 25.000 (43.061)\n",
      "Epoch: [140][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8135 (1.6105)\tPrec@1 43.750 (42.551)\n",
      "Epoch: [140][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4529 (1.6140)\tPrec@1 43.750 (42.231)\n",
      "Epoch: [140][2500/3124]\tTime 0.001 (0.001)\tLoss 2.0183 (1.6209)\tPrec@1 31.250 (42.031)\n",
      "Epoch: [140][3124/3124]\tTime 0.004 (0.001)\tLoss 1.7372 (1.6248)\tPrec@1 43.750 (41.916)\n",
      "EPOCH: 140 train Results: Prec@1 41.916 Loss: 1.6248\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0511 (1.0511)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5930 (1.5249)\tPrec@1 25.000 (45.620)\n",
      "EPOCH: 140 val Results: Prec@1 45.620 Loss: 1.5249\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/3124]\tTime 0.001 (0.001)\tLoss 1.3378 (1.3378)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [141][625/3124]\tTime 0.001 (0.001)\tLoss 1.7322 (1.6097)\tPrec@1 31.250 (42.123)\n",
      "Epoch: [141][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3734 (1.6209)\tPrec@1 43.750 (41.906)\n",
      "Epoch: [141][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9449 (1.6271)\tPrec@1 31.250 (41.545)\n",
      "Epoch: [141][2500/3124]\tTime 0.010 (0.001)\tLoss 1.4077 (1.6305)\tPrec@1 62.500 (41.598)\n",
      "Epoch: [141][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6418 (1.6293)\tPrec@1 50.000 (41.580)\n",
      "EPOCH: 141 train Results: Prec@1 41.580 Loss: 1.6293\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9589 (0.9589)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6609 (1.5111)\tPrec@1 43.750 (46.120)\n",
      "EPOCH: 141 val Results: Prec@1 46.120 Loss: 1.5111\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/3124]\tTime 0.002 (0.002)\tLoss 1.8437 (1.8437)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [142][625/3124]\tTime 0.001 (0.001)\tLoss 1.4841 (1.6024)\tPrec@1 43.750 (43.351)\n",
      "Epoch: [142][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5589 (1.6176)\tPrec@1 62.500 (42.401)\n",
      "Epoch: [142][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3784 (1.6180)\tPrec@1 62.500 (42.191)\n",
      "Epoch: [142][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6870 (1.6216)\tPrec@1 43.750 (42.141)\n",
      "Epoch: [142][3124/3124]\tTime 0.001 (0.001)\tLoss 1.2592 (1.6249)\tPrec@1 68.750 (41.848)\n",
      "EPOCH: 142 train Results: Prec@1 41.848 Loss: 1.6249\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2697 (1.2697)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3169 (1.5123)\tPrec@1 31.250 (45.820)\n",
      "EPOCH: 142 val Results: Prec@1 45.820 Loss: 1.5123\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/3124]\tTime 0.002 (0.002)\tLoss 1.7748 (1.7748)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [143][625/3124]\tTime 0.001 (0.001)\tLoss 1.7555 (1.6109)\tPrec@1 37.500 (42.632)\n",
      "Epoch: [143][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6125 (1.6198)\tPrec@1 25.000 (41.936)\n",
      "Epoch: [143][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6998 (1.6221)\tPrec@1 43.750 (41.954)\n",
      "Epoch: [143][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9003 (1.6244)\tPrec@1 31.250 (41.826)\n",
      "Epoch: [143][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6588 (1.6255)\tPrec@1 50.000 (41.728)\n",
      "EPOCH: 143 train Results: Prec@1 41.728 Loss: 1.6255\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9096 (0.9096)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2469 (1.5159)\tPrec@1 50.000 (45.570)\n",
      "EPOCH: 143 val Results: Prec@1 45.570 Loss: 1.5159\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/3124]\tTime 0.001 (0.001)\tLoss 1.9344 (1.9344)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [144][625/3124]\tTime 0.001 (0.001)\tLoss 1.6931 (1.6067)\tPrec@1 43.750 (42.192)\n",
      "Epoch: [144][1250/3124]\tTime 0.001 (0.001)\tLoss 2.1358 (1.6129)\tPrec@1 25.000 (42.141)\n",
      "Epoch: [144][1875/3124]\tTime 0.005 (0.001)\tLoss 1.7061 (1.6185)\tPrec@1 43.750 (42.014)\n",
      "Epoch: [144][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4825 (1.6219)\tPrec@1 50.000 (41.888)\n",
      "Epoch: [144][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7008 (1.6253)\tPrec@1 43.750 (41.618)\n",
      "EPOCH: 144 train Results: Prec@1 41.618 Loss: 1.6253\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0272 (1.0272)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5816 (1.5106)\tPrec@1 37.500 (45.700)\n",
      "EPOCH: 144 val Results: Prec@1 45.700 Loss: 1.5106\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/3124]\tTime 0.001 (0.001)\tLoss 1.7842 (1.7842)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [145][625/3124]\tTime 0.003 (0.001)\tLoss 1.9953 (1.6149)\tPrec@1 18.750 (42.232)\n",
      "Epoch: [145][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6517 (1.6157)\tPrec@1 31.250 (42.286)\n",
      "Epoch: [145][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6490 (1.6214)\tPrec@1 31.250 (42.128)\n",
      "Epoch: [145][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4711 (1.6233)\tPrec@1 56.250 (41.958)\n",
      "Epoch: [145][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7116 (1.6262)\tPrec@1 37.500 (41.842)\n",
      "EPOCH: 145 train Results: Prec@1 41.842 Loss: 1.6262\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9830 (0.9830)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6604 (1.5144)\tPrec@1 31.250 (45.800)\n",
      "EPOCH: 145 val Results: Prec@1 45.800 Loss: 1.5144\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/3124]\tTime 0.002 (0.002)\tLoss 1.4534 (1.4534)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [146][625/3124]\tTime 0.001 (0.001)\tLoss 0.9625 (1.6118)\tPrec@1 62.500 (42.612)\n",
      "Epoch: [146][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7212 (1.6274)\tPrec@1 37.500 (41.857)\n",
      "Epoch: [146][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5180 (1.6267)\tPrec@1 50.000 (41.834)\n",
      "Epoch: [146][2500/3124]\tTime 0.003 (0.001)\tLoss 1.5729 (1.6272)\tPrec@1 43.750 (41.771)\n",
      "Epoch: [146][3124/3124]\tTime 0.001 (0.001)\tLoss 1.2991 (1.6277)\tPrec@1 56.250 (41.720)\n",
      "EPOCH: 146 train Results: Prec@1 41.720 Loss: 1.6277\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2547 (1.2547)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3918 (1.5336)\tPrec@1 43.750 (44.530)\n",
      "EPOCH: 146 val Results: Prec@1 44.530 Loss: 1.5336\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/3124]\tTime 0.001 (0.001)\tLoss 1.4538 (1.4538)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [147][625/3124]\tTime 0.001 (0.001)\tLoss 1.5989 (1.6139)\tPrec@1 37.500 (42.442)\n",
      "Epoch: [147][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7802 (1.6237)\tPrec@1 37.500 (41.752)\n",
      "Epoch: [147][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4945 (1.6278)\tPrec@1 37.500 (41.778)\n",
      "Epoch: [147][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9244 (1.6305)\tPrec@1 31.250 (41.736)\n",
      "Epoch: [147][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9071 (1.6318)\tPrec@1 31.250 (41.720)\n",
      "EPOCH: 147 train Results: Prec@1 41.720 Loss: 1.6318\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1569 (1.1569)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5360 (1.5183)\tPrec@1 37.500 (45.690)\n",
      "EPOCH: 147 val Results: Prec@1 45.690 Loss: 1.5183\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/3124]\tTime 0.001 (0.001)\tLoss 1.9342 (1.9342)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [148][625/3124]\tTime 0.001 (0.001)\tLoss 1.2739 (1.6049)\tPrec@1 56.250 (42.612)\n",
      "Epoch: [148][1250/3124]\tTime 0.003 (0.001)\tLoss 2.0566 (1.6177)\tPrec@1 25.000 (41.812)\n",
      "Epoch: [148][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4171 (1.6226)\tPrec@1 37.500 (41.541)\n",
      "Epoch: [148][2500/3124]\tTime 0.002 (0.001)\tLoss 2.2231 (1.6264)\tPrec@1 31.250 (41.646)\n",
      "Epoch: [148][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7662 (1.6271)\tPrec@1 25.000 (41.658)\n",
      "EPOCH: 148 train Results: Prec@1 41.658 Loss: 1.6271\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2668 (1.2668)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4786 (1.5166)\tPrec@1 37.500 (45.930)\n",
      "EPOCH: 148 val Results: Prec@1 45.930 Loss: 1.5166\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/3124]\tTime 0.001 (0.001)\tLoss 1.4710 (1.4710)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [149][625/3124]\tTime 0.001 (0.001)\tLoss 1.4711 (1.6062)\tPrec@1 43.750 (42.851)\n",
      "Epoch: [149][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8392 (1.6166)\tPrec@1 37.500 (42.326)\n",
      "Epoch: [149][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4179 (1.6185)\tPrec@1 25.000 (41.994)\n",
      "Epoch: [149][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8509 (1.6249)\tPrec@1 25.000 (41.781)\n",
      "Epoch: [149][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5986 (1.6301)\tPrec@1 50.000 (41.538)\n",
      "EPOCH: 149 train Results: Prec@1 41.538 Loss: 1.6301\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2534 (1.2534)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3959 (1.5214)\tPrec@1 56.250 (44.780)\n",
      "EPOCH: 149 val Results: Prec@1 44.780 Loss: 1.5214\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/3124]\tTime 0.002 (0.002)\tLoss 1.3961 (1.3961)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [150][625/3124]\tTime 0.001 (0.001)\tLoss 2.2803 (1.6029)\tPrec@1 37.500 (42.352)\n",
      "Epoch: [150][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9341 (1.6102)\tPrec@1 25.000 (42.241)\n",
      "Epoch: [150][1875/3124]\tTime 0.001 (0.001)\tLoss 1.2761 (1.6228)\tPrec@1 75.000 (41.914)\n",
      "Epoch: [150][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6695 (1.6266)\tPrec@1 37.500 (41.683)\n",
      "Epoch: [150][3124/3124]\tTime 0.001 (0.001)\tLoss 2.1325 (1.6268)\tPrec@1 37.500 (41.758)\n",
      "EPOCH: 150 train Results: Prec@1 41.758 Loss: 1.6268\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0529 (1.0529)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2862 (1.5170)\tPrec@1 56.250 (45.410)\n",
      "EPOCH: 150 val Results: Prec@1 45.410 Loss: 1.5170\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/3124]\tTime 0.002 (0.002)\tLoss 1.6963 (1.6963)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [151][625/3124]\tTime 0.001 (0.001)\tLoss 1.6982 (1.6132)\tPrec@1 31.250 (42.512)\n",
      "Epoch: [151][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5772 (1.6226)\tPrec@1 43.750 (42.086)\n",
      "Epoch: [151][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5956 (1.6238)\tPrec@1 50.000 (42.108)\n",
      "Epoch: [151][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7393 (1.6251)\tPrec@1 31.250 (42.011)\n",
      "Epoch: [151][3124/3124]\tTime 0.003 (0.001)\tLoss 1.5633 (1.6269)\tPrec@1 43.750 (41.844)\n",
      "EPOCH: 151 train Results: Prec@1 41.844 Loss: 1.6269\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9604 (0.9604)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6405 (1.5088)\tPrec@1 37.500 (45.480)\n",
      "EPOCH: 151 val Results: Prec@1 45.480 Loss: 1.5088\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/3124]\tTime 0.001 (0.001)\tLoss 1.4833 (1.4833)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [152][625/3124]\tTime 0.001 (0.001)\tLoss 1.4891 (1.6087)\tPrec@1 31.250 (42.542)\n",
      "Epoch: [152][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5449 (1.6190)\tPrec@1 37.500 (41.976)\n",
      "Epoch: [152][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8281 (1.6214)\tPrec@1 37.500 (41.888)\n",
      "Epoch: [152][2500/3124]\tTime 0.002 (0.001)\tLoss 2.0865 (1.6197)\tPrec@1 18.750 (41.991)\n",
      "Epoch: [152][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4615 (1.6222)\tPrec@1 37.500 (41.954)\n",
      "EPOCH: 152 train Results: Prec@1 41.954 Loss: 1.6222\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1692 (1.1692)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5563 (1.5027)\tPrec@1 31.250 (46.260)\n",
      "EPOCH: 152 val Results: Prec@1 46.260 Loss: 1.5027\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/3124]\tTime 0.001 (0.001)\tLoss 1.8901 (1.8901)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [153][625/3124]\tTime 0.001 (0.001)\tLoss 1.8438 (1.6062)\tPrec@1 37.500 (42.871)\n",
      "Epoch: [153][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8885 (1.6126)\tPrec@1 18.750 (42.621)\n",
      "Epoch: [153][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4533 (1.6176)\tPrec@1 56.250 (42.377)\n",
      "Epoch: [153][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8011 (1.6200)\tPrec@1 31.250 (42.216)\n",
      "Epoch: [153][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7134 (1.6252)\tPrec@1 37.500 (41.970)\n",
      "EPOCH: 153 train Results: Prec@1 41.970 Loss: 1.6252\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1639 (1.1639)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5713 (1.5162)\tPrec@1 25.000 (45.470)\n",
      "EPOCH: 153 val Results: Prec@1 45.470 Loss: 1.5162\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/3124]\tTime 0.002 (0.002)\tLoss 1.6641 (1.6641)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [154][625/3124]\tTime 0.001 (0.001)\tLoss 1.6010 (1.6096)\tPrec@1 50.000 (42.332)\n",
      "Epoch: [154][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7383 (1.6150)\tPrec@1 37.500 (42.166)\n",
      "Epoch: [154][1875/3124]\tTime 0.009 (0.001)\tLoss 1.4815 (1.6202)\tPrec@1 37.500 (41.914)\n",
      "Epoch: [154][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4934 (1.6272)\tPrec@1 31.250 (41.733)\n",
      "Epoch: [154][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8314 (1.6287)\tPrec@1 25.000 (41.644)\n",
      "EPOCH: 154 train Results: Prec@1 41.644 Loss: 1.6287\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.1580 (1.1580)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5174 (1.5275)\tPrec@1 50.000 (44.890)\n",
      "EPOCH: 154 val Results: Prec@1 44.890 Loss: 1.5275\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/3124]\tTime 0.001 (0.001)\tLoss 1.5329 (1.5329)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [155][625/3124]\tTime 0.001 (0.001)\tLoss 1.4318 (1.6003)\tPrec@1 37.500 (42.792)\n",
      "Epoch: [155][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6957 (1.6123)\tPrec@1 43.750 (41.946)\n",
      "Epoch: [155][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6579 (1.6148)\tPrec@1 50.000 (41.951)\n",
      "Epoch: [155][2500/3124]\tTime 0.001 (0.001)\tLoss 2.0998 (1.6203)\tPrec@1 37.500 (41.701)\n",
      "Epoch: [155][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7824 (1.6249)\tPrec@1 31.250 (41.458)\n",
      "EPOCH: 155 train Results: Prec@1 41.458 Loss: 1.6249\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2129 (1.2129)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6717 (1.5154)\tPrec@1 18.750 (45.380)\n",
      "EPOCH: 155 val Results: Prec@1 45.380 Loss: 1.5154\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/3124]\tTime 0.001 (0.001)\tLoss 1.2139 (1.2139)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [156][625/3124]\tTime 0.001 (0.001)\tLoss 1.8208 (1.6101)\tPrec@1 43.750 (42.083)\n",
      "Epoch: [156][1250/3124]\tTime 0.002 (0.001)\tLoss 1.9613 (1.6167)\tPrec@1 18.750 (41.812)\n",
      "Epoch: [156][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5181 (1.6234)\tPrec@1 25.000 (41.581)\n",
      "Epoch: [156][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8510 (1.6264)\tPrec@1 37.500 (41.563)\n",
      "Epoch: [156][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5899 (1.6272)\tPrec@1 31.250 (41.504)\n",
      "EPOCH: 156 train Results: Prec@1 41.504 Loss: 1.6272\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0119 (1.0119)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2509 (1.5168)\tPrec@1 50.000 (45.180)\n",
      "EPOCH: 156 val Results: Prec@1 45.180 Loss: 1.5168\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/3124]\tTime 0.001 (0.001)\tLoss 1.7992 (1.7992)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [157][625/3124]\tTime 0.002 (0.001)\tLoss 1.9489 (1.6204)\tPrec@1 18.750 (41.803)\n",
      "Epoch: [157][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5830 (1.6210)\tPrec@1 37.500 (42.016)\n",
      "Epoch: [157][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4166 (1.6285)\tPrec@1 56.250 (41.541)\n",
      "Epoch: [157][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5355 (1.6326)\tPrec@1 43.750 (41.436)\n",
      "Epoch: [157][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7613 (1.6316)\tPrec@1 31.250 (41.530)\n",
      "EPOCH: 157 train Results: Prec@1 41.530 Loss: 1.6316\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0393 (1.0393)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5933 (1.5301)\tPrec@1 25.000 (45.170)\n",
      "EPOCH: 157 val Results: Prec@1 45.170 Loss: 1.5301\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/3124]\tTime 0.001 (0.001)\tLoss 1.4835 (1.4835)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [158][625/3124]\tTime 0.001 (0.001)\tLoss 2.0161 (1.6048)\tPrec@1 37.500 (42.782)\n",
      "Epoch: [158][1250/3124]\tTime 0.002 (0.001)\tLoss 1.5021 (1.6179)\tPrec@1 50.000 (41.981)\n",
      "Epoch: [158][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7084 (1.6234)\tPrec@1 43.750 (41.738)\n",
      "Epoch: [158][2500/3124]\tTime 0.001 (0.001)\tLoss 1.1967 (1.6228)\tPrec@1 62.500 (41.738)\n",
      "Epoch: [158][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9785 (1.6248)\tPrec@1 31.250 (41.724)\n",
      "EPOCH: 158 train Results: Prec@1 41.724 Loss: 1.6248\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2327 (1.2327)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6360 (1.5285)\tPrec@1 31.250 (44.280)\n",
      "EPOCH: 158 val Results: Prec@1 44.280 Loss: 1.5285\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/3124]\tTime 0.004 (0.004)\tLoss 1.2659 (1.2659)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [159][625/3124]\tTime 0.001 (0.001)\tLoss 1.7135 (1.6175)\tPrec@1 12.500 (42.212)\n",
      "Epoch: [159][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9301 (1.6238)\tPrec@1 25.000 (41.971)\n",
      "Epoch: [159][1875/3124]\tTime 0.001 (0.001)\tLoss 1.2081 (1.6253)\tPrec@1 56.250 (42.098)\n",
      "Epoch: [159][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8615 (1.6296)\tPrec@1 50.000 (41.866)\n",
      "Epoch: [159][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3736 (1.6293)\tPrec@1 50.000 (41.712)\n",
      "EPOCH: 159 train Results: Prec@1 41.712 Loss: 1.6293\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1962 (1.1962)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5400 (1.5240)\tPrec@1 31.250 (45.940)\n",
      "EPOCH: 159 val Results: Prec@1 45.940 Loss: 1.5240\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/3124]\tTime 0.001 (0.001)\tLoss 1.8864 (1.8864)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [160][625/3124]\tTime 0.001 (0.001)\tLoss 1.6751 (1.6239)\tPrec@1 37.500 (41.703)\n",
      "Epoch: [160][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5153 (1.6259)\tPrec@1 37.500 (41.587)\n",
      "Epoch: [160][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6098 (1.6328)\tPrec@1 37.500 (41.318)\n",
      "Epoch: [160][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8990 (1.6335)\tPrec@1 37.500 (41.496)\n",
      "Epoch: [160][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6575 (1.6301)\tPrec@1 18.750 (41.604)\n",
      "EPOCH: 160 train Results: Prec@1 41.604 Loss: 1.6301\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.4773 (1.4773)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3428 (1.5279)\tPrec@1 43.750 (43.730)\n",
      "EPOCH: 160 val Results: Prec@1 43.730 Loss: 1.5279\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/3124]\tTime 0.002 (0.002)\tLoss 1.5981 (1.5981)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [161][625/3124]\tTime 0.001 (0.001)\tLoss 1.6503 (1.6025)\tPrec@1 43.750 (41.823)\n",
      "Epoch: [161][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7622 (1.6149)\tPrec@1 37.500 (41.667)\n",
      "Epoch: [161][1875/3124]\tTime 0.004 (0.001)\tLoss 1.2376 (1.6227)\tPrec@1 50.000 (41.405)\n",
      "Epoch: [161][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7816 (1.6258)\tPrec@1 37.500 (41.401)\n",
      "Epoch: [161][3124/3124]\tTime 0.001 (0.001)\tLoss 2.2496 (1.6320)\tPrec@1 37.500 (41.242)\n",
      "EPOCH: 161 train Results: Prec@1 41.242 Loss: 1.6320\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0385 (1.0385)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3168 (1.5012)\tPrec@1 43.750 (45.910)\n",
      "EPOCH: 161 val Results: Prec@1 45.910 Loss: 1.5012\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/3124]\tTime 0.002 (0.002)\tLoss 1.6632 (1.6632)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [162][625/3124]\tTime 0.001 (0.001)\tLoss 1.5916 (1.5986)\tPrec@1 43.750 (42.192)\n",
      "Epoch: [162][1250/3124]\tTime 0.001 (0.001)\tLoss 1.4850 (1.6173)\tPrec@1 50.000 (41.757)\n",
      "Epoch: [162][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9380 (1.6222)\tPrec@1 18.750 (41.764)\n",
      "Epoch: [162][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6657 (1.6253)\tPrec@1 25.000 (41.586)\n",
      "Epoch: [162][3124/3124]\tTime 0.002 (0.001)\tLoss 1.6683 (1.6279)\tPrec@1 31.250 (41.408)\n",
      "EPOCH: 162 train Results: Prec@1 41.408 Loss: 1.6279\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9471 (0.9471)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4063 (1.5242)\tPrec@1 37.500 (44.590)\n",
      "EPOCH: 162 val Results: Prec@1 44.590 Loss: 1.5242\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/3124]\tTime 0.002 (0.002)\tLoss 2.1905 (2.1905)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [163][625/3124]\tTime 0.001 (0.001)\tLoss 1.5685 (1.6235)\tPrec@1 37.500 (41.593)\n",
      "Epoch: [163][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3693 (1.6179)\tPrec@1 43.750 (42.036)\n",
      "Epoch: [163][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4735 (1.6204)\tPrec@1 50.000 (41.994)\n",
      "Epoch: [163][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5142 (1.6230)\tPrec@1 43.750 (41.911)\n",
      "Epoch: [163][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5477 (1.6261)\tPrec@1 50.000 (41.778)\n",
      "EPOCH: 163 train Results: Prec@1 41.778 Loss: 1.6261\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.1708 (1.1708)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.1355 (1.5110)\tPrec@1 37.500 (46.260)\n",
      "EPOCH: 163 val Results: Prec@1 46.260 Loss: 1.5110\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/3124]\tTime 0.001 (0.001)\tLoss 1.7002 (1.7002)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [164][625/3124]\tTime 0.001 (0.001)\tLoss 2.0186 (1.5964)\tPrec@1 12.500 (43.151)\n",
      "Epoch: [164][1250/3124]\tTime 0.002 (0.001)\tLoss 1.9701 (1.6155)\tPrec@1 25.000 (42.616)\n",
      "Epoch: [164][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5622 (1.6235)\tPrec@1 37.500 (42.178)\n",
      "Epoch: [164][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6867 (1.6272)\tPrec@1 43.750 (41.838)\n",
      "Epoch: [164][3124/3124]\tTime 0.003 (0.001)\tLoss 1.8802 (1.6268)\tPrec@1 31.250 (41.828)\n",
      "EPOCH: 164 train Results: Prec@1 41.828 Loss: 1.6268\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1885 (1.1885)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4618 (1.5213)\tPrec@1 43.750 (45.800)\n",
      "EPOCH: 164 val Results: Prec@1 45.800 Loss: 1.5213\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/3124]\tTime 0.003 (0.003)\tLoss 2.2204 (2.2204)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [165][625/3124]\tTime 0.001 (0.001)\tLoss 2.0588 (1.5998)\tPrec@1 25.000 (42.562)\n",
      "Epoch: [165][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5827 (1.6194)\tPrec@1 50.000 (42.341)\n",
      "Epoch: [165][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4451 (1.6193)\tPrec@1 62.500 (42.341)\n",
      "Epoch: [165][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7180 (1.6262)\tPrec@1 31.250 (42.053)\n",
      "Epoch: [165][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8691 (1.6292)\tPrec@1 37.500 (41.910)\n",
      "EPOCH: 165 train Results: Prec@1 41.910 Loss: 1.6292\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0158 (1.0158)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4439 (1.5191)\tPrec@1 43.750 (45.200)\n",
      "EPOCH: 165 val Results: Prec@1 45.200 Loss: 1.5191\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/3124]\tTime 0.001 (0.001)\tLoss 2.0408 (2.0408)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [166][625/3124]\tTime 0.003 (0.001)\tLoss 1.1501 (1.6108)\tPrec@1 62.500 (42.292)\n",
      "Epoch: [166][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9314 (1.6194)\tPrec@1 12.500 (41.732)\n",
      "Epoch: [166][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9073 (1.6271)\tPrec@1 31.250 (41.638)\n",
      "Epoch: [166][2500/3124]\tTime 0.001 (0.001)\tLoss 2.2618 (1.6284)\tPrec@1 50.000 (41.621)\n",
      "Epoch: [166][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4596 (1.6303)\tPrec@1 43.750 (41.658)\n",
      "EPOCH: 166 train Results: Prec@1 41.658 Loss: 1.6303\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0952 (1.0952)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6079 (1.5249)\tPrec@1 50.000 (45.300)\n",
      "EPOCH: 166 val Results: Prec@1 45.300 Loss: 1.5249\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/3124]\tTime 0.001 (0.001)\tLoss 2.2637 (2.2637)\tPrec@1 6.250 (6.250)\n",
      "Epoch: [167][625/3124]\tTime 0.001 (0.001)\tLoss 1.2592 (1.6103)\tPrec@1 43.750 (41.863)\n",
      "Epoch: [167][1250/3124]\tTime 0.001 (0.001)\tLoss 1.1634 (1.6096)\tPrec@1 62.500 (42.196)\n",
      "Epoch: [167][1875/3124]\tTime 0.001 (0.001)\tLoss 2.0752 (1.6257)\tPrec@1 25.000 (41.614)\n",
      "Epoch: [167][2500/3124]\tTime 0.007 (0.001)\tLoss 1.4936 (1.6286)\tPrec@1 50.000 (41.433)\n",
      "Epoch: [167][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3813 (1.6305)\tPrec@1 56.250 (41.474)\n",
      "EPOCH: 167 train Results: Prec@1 41.474 Loss: 1.6305\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0267 (1.0267)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4688 (1.5118)\tPrec@1 31.250 (46.190)\n",
      "EPOCH: 167 val Results: Prec@1 46.190 Loss: 1.5118\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/3124]\tTime 0.002 (0.002)\tLoss 1.5354 (1.5354)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [168][625/3124]\tTime 0.001 (0.001)\tLoss 1.1268 (1.5993)\tPrec@1 75.000 (43.091)\n",
      "Epoch: [168][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5700 (1.6140)\tPrec@1 43.750 (42.411)\n",
      "Epoch: [168][1875/3124]\tTime 0.002 (0.001)\tLoss 1.6780 (1.6258)\tPrec@1 18.750 (41.734)\n",
      "Epoch: [168][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6504 (1.6260)\tPrec@1 37.500 (41.723)\n",
      "Epoch: [168][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6165 (1.6280)\tPrec@1 37.500 (41.690)\n",
      "EPOCH: 168 train Results: Prec@1 41.690 Loss: 1.6280\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.4128 (1.4128)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5223 (1.5126)\tPrec@1 37.500 (46.010)\n",
      "EPOCH: 168 val Results: Prec@1 46.010 Loss: 1.5126\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/3124]\tTime 0.001 (0.001)\tLoss 1.7169 (1.7169)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [169][625/3124]\tTime 0.001 (0.001)\tLoss 1.2290 (1.6036)\tPrec@1 50.000 (42.342)\n",
      "Epoch: [169][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6483 (1.6100)\tPrec@1 37.500 (42.221)\n",
      "Epoch: [169][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5902 (1.6240)\tPrec@1 43.750 (41.738)\n",
      "Epoch: [169][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7413 (1.6260)\tPrec@1 37.500 (41.701)\n",
      "Epoch: [169][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4311 (1.6281)\tPrec@1 56.250 (41.632)\n",
      "EPOCH: 169 train Results: Prec@1 41.632 Loss: 1.6281\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.4921 (1.4921)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5479 (1.5134)\tPrec@1 25.000 (45.670)\n",
      "EPOCH: 169 val Results: Prec@1 45.670 Loss: 1.5134\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/3124]\tTime 0.006 (0.006)\tLoss 1.3969 (1.3969)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [170][625/3124]\tTime 0.001 (0.001)\tLoss 1.6401 (1.6155)\tPrec@1 31.250 (42.063)\n",
      "Epoch: [170][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6713 (1.6248)\tPrec@1 31.250 (41.901)\n",
      "Epoch: [170][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7611 (1.6237)\tPrec@1 37.500 (41.801)\n",
      "Epoch: [170][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3600 (1.6288)\tPrec@1 50.000 (41.648)\n",
      "Epoch: [170][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7818 (1.6280)\tPrec@1 25.000 (41.674)\n",
      "EPOCH: 170 train Results: Prec@1 41.674 Loss: 1.6280\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0563 (1.0563)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3749 (1.4949)\tPrec@1 31.250 (46.580)\n",
      "EPOCH: 170 val Results: Prec@1 46.580 Loss: 1.4949\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/3124]\tTime 0.004 (0.004)\tLoss 1.4040 (1.4040)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [171][625/3124]\tTime 0.001 (0.001)\tLoss 1.4625 (1.6104)\tPrec@1 50.000 (42.073)\n",
      "Epoch: [171][1250/3124]\tTime 0.001 (0.001)\tLoss 2.0412 (1.6192)\tPrec@1 37.500 (41.876)\n",
      "Epoch: [171][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3247 (1.6219)\tPrec@1 56.250 (41.871)\n",
      "Epoch: [171][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5258 (1.6207)\tPrec@1 43.750 (42.016)\n",
      "Epoch: [171][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4747 (1.6240)\tPrec@1 62.500 (41.760)\n",
      "EPOCH: 171 train Results: Prec@1 41.760 Loss: 1.6240\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1000 (1.1000)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4020 (1.5101)\tPrec@1 37.500 (45.350)\n",
      "EPOCH: 171 val Results: Prec@1 45.350 Loss: 1.5101\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/3124]\tTime 0.003 (0.003)\tLoss 1.4044 (1.4044)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [172][625/3124]\tTime 0.001 (0.001)\tLoss 1.2210 (1.6123)\tPrec@1 43.750 (42.592)\n",
      "Epoch: [172][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2526 (1.6142)\tPrec@1 50.000 (42.391)\n",
      "Epoch: [172][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6022 (1.6254)\tPrec@1 50.000 (41.764)\n",
      "Epoch: [172][2500/3124]\tTime 0.003 (0.001)\tLoss 1.6598 (1.6283)\tPrec@1 43.750 (41.668)\n",
      "Epoch: [172][3124/3124]\tTime 0.003 (0.001)\tLoss 1.4999 (1.6299)\tPrec@1 56.250 (41.678)\n",
      "EPOCH: 172 train Results: Prec@1 41.678 Loss: 1.6299\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2496 (1.2496)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5662 (1.5354)\tPrec@1 37.500 (44.980)\n",
      "EPOCH: 172 val Results: Prec@1 44.980 Loss: 1.5354\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/3124]\tTime 0.001 (0.001)\tLoss 1.4913 (1.4913)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [173][625/3124]\tTime 0.001 (0.001)\tLoss 1.9884 (1.6044)\tPrec@1 18.750 (42.802)\n",
      "Epoch: [173][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6300 (1.6199)\tPrec@1 31.250 (41.886)\n",
      "Epoch: [173][1875/3124]\tTime 0.001 (0.001)\tLoss 1.3538 (1.6182)\tPrec@1 68.750 (41.978)\n",
      "Epoch: [173][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4204 (1.6207)\tPrec@1 43.750 (41.826)\n",
      "Epoch: [173][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6696 (1.6231)\tPrec@1 31.250 (41.738)\n",
      "EPOCH: 173 train Results: Prec@1 41.738 Loss: 1.6231\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.3280 (1.3280)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6804 (1.5275)\tPrec@1 37.500 (45.600)\n",
      "EPOCH: 173 val Results: Prec@1 45.600 Loss: 1.5275\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/3124]\tTime 0.008 (0.008)\tLoss 1.1450 (1.1450)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [174][625/3124]\tTime 0.001 (0.001)\tLoss 1.3949 (1.6003)\tPrec@1 62.500 (42.782)\n",
      "Epoch: [174][1250/3124]\tTime 0.007 (0.001)\tLoss 1.5467 (1.6163)\tPrec@1 50.000 (42.036)\n",
      "Epoch: [174][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5993 (1.6202)\tPrec@1 37.500 (41.988)\n",
      "Epoch: [174][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5348 (1.6235)\tPrec@1 37.500 (41.816)\n",
      "Epoch: [174][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3649 (1.6271)\tPrec@1 43.750 (41.726)\n",
      "EPOCH: 174 train Results: Prec@1 41.726 Loss: 1.6271\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9462 (0.9462)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5514 (1.5318)\tPrec@1 31.250 (44.750)\n",
      "EPOCH: 174 val Results: Prec@1 44.750 Loss: 1.5318\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/3124]\tTime 0.005 (0.005)\tLoss 1.5429 (1.5429)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [175][625/3124]\tTime 0.001 (0.001)\tLoss 1.9009 (1.6067)\tPrec@1 37.500 (42.282)\n",
      "Epoch: [175][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2970 (1.6113)\tPrec@1 50.000 (42.251)\n",
      "Epoch: [175][1875/3124]\tTime 0.001 (0.001)\tLoss 1.9203 (1.6151)\tPrec@1 25.000 (41.961)\n",
      "Epoch: [175][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7854 (1.6200)\tPrec@1 18.750 (41.751)\n",
      "Epoch: [175][3124/3124]\tTime 0.001 (0.001)\tLoss 2.1066 (1.6234)\tPrec@1 25.000 (41.548)\n",
      "EPOCH: 175 train Results: Prec@1 41.548 Loss: 1.6234\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0312 (1.0312)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5673 (1.5311)\tPrec@1 37.500 (45.030)\n",
      "EPOCH: 175 val Results: Prec@1 45.030 Loss: 1.5311\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/3124]\tTime 0.003 (0.003)\tLoss 1.8578 (1.8578)\tPrec@1 25.000 (25.000)\n",
      "Epoch: [176][625/3124]\tTime 0.001 (0.001)\tLoss 1.5338 (1.6162)\tPrec@1 50.000 (42.013)\n",
      "Epoch: [176][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6434 (1.6260)\tPrec@1 43.750 (41.702)\n",
      "Epoch: [176][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7690 (1.6261)\tPrec@1 37.500 (41.771)\n",
      "Epoch: [176][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3445 (1.6286)\tPrec@1 50.000 (41.691)\n",
      "Epoch: [176][3124/3124]\tTime 0.003 (0.001)\tLoss 1.1473 (1.6283)\tPrec@1 62.500 (41.616)\n",
      "EPOCH: 176 train Results: Prec@1 41.616 Loss: 1.6283\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0320 (1.0320)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7145 (1.5234)\tPrec@1 25.000 (45.540)\n",
      "EPOCH: 176 val Results: Prec@1 45.540 Loss: 1.5234\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/3124]\tTime 0.002 (0.002)\tLoss 1.9335 (1.9335)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [177][625/3124]\tTime 0.001 (0.001)\tLoss 1.7600 (1.6083)\tPrec@1 37.500 (42.602)\n",
      "Epoch: [177][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5895 (1.6238)\tPrec@1 37.500 (41.951)\n",
      "Epoch: [177][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5596 (1.6274)\tPrec@1 50.000 (41.768)\n",
      "Epoch: [177][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4126 (1.6282)\tPrec@1 37.500 (41.546)\n",
      "Epoch: [177][3124/3124]\tTime 0.007 (0.001)\tLoss 1.3246 (1.6285)\tPrec@1 37.500 (41.590)\n",
      "EPOCH: 177 train Results: Prec@1 41.590 Loss: 1.6285\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2408 (1.2408)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.8353 (1.5215)\tPrec@1 31.250 (44.970)\n",
      "EPOCH: 177 val Results: Prec@1 44.970 Loss: 1.5215\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/3124]\tTime 0.001 (0.001)\tLoss 1.2438 (1.2438)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [178][625/3124]\tTime 0.001 (0.001)\tLoss 1.3693 (1.6053)\tPrec@1 50.000 (42.272)\n",
      "Epoch: [178][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3314 (1.6191)\tPrec@1 62.500 (41.787)\n",
      "Epoch: [178][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8335 (1.6296)\tPrec@1 25.000 (41.568)\n",
      "Epoch: [178][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8155 (1.6315)\tPrec@1 37.500 (41.553)\n",
      "Epoch: [178][3124/3124]\tTime 0.001 (0.001)\tLoss 2.1778 (1.6303)\tPrec@1 18.750 (41.456)\n",
      "EPOCH: 178 train Results: Prec@1 41.456 Loss: 1.6303\n",
      "Test: [0/624]\tTime 0.003 (0.003)\tLoss 0.9961 (0.9961)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3930 (1.5108)\tPrec@1 62.500 (45.960)\n",
      "EPOCH: 178 val Results: Prec@1 45.960 Loss: 1.5108\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/3124]\tTime 0.001 (0.001)\tLoss 1.8302 (1.8302)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [179][625/3124]\tTime 0.001 (0.001)\tLoss 1.6537 (1.6064)\tPrec@1 43.750 (42.951)\n",
      "Epoch: [179][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9171 (1.6192)\tPrec@1 43.750 (42.276)\n",
      "Epoch: [179][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4837 (1.6296)\tPrec@1 56.250 (41.861)\n",
      "Epoch: [179][2500/3124]\tTime 0.010 (0.001)\tLoss 1.5013 (1.6313)\tPrec@1 43.750 (41.598)\n",
      "Epoch: [179][3124/3124]\tTime 0.001 (0.001)\tLoss 1.8958 (1.6297)\tPrec@1 18.750 (41.650)\n",
      "EPOCH: 179 train Results: Prec@1 41.650 Loss: 1.6297\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0408 (1.0408)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4585 (1.5181)\tPrec@1 43.750 (45.430)\n",
      "EPOCH: 179 val Results: Prec@1 45.430 Loss: 1.5181\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/3124]\tTime 0.001 (0.001)\tLoss 1.5627 (1.5627)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [180][625/3124]\tTime 0.001 (0.001)\tLoss 1.5177 (1.6172)\tPrec@1 56.250 (42.133)\n",
      "Epoch: [180][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3932 (1.6202)\tPrec@1 62.500 (41.807)\n",
      "Epoch: [180][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7624 (1.6227)\tPrec@1 31.250 (41.951)\n",
      "Epoch: [180][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5466 (1.6272)\tPrec@1 50.000 (41.833)\n",
      "Epoch: [180][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9941 (1.6278)\tPrec@1 18.750 (41.904)\n",
      "EPOCH: 180 train Results: Prec@1 41.904 Loss: 1.6278\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.8860 (0.8860)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5541 (1.5123)\tPrec@1 25.000 (45.350)\n",
      "EPOCH: 180 val Results: Prec@1 45.350 Loss: 1.5123\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/3124]\tTime 0.002 (0.002)\tLoss 1.3076 (1.3076)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [181][625/3124]\tTime 0.001 (0.001)\tLoss 2.1923 (1.6089)\tPrec@1 31.250 (42.113)\n",
      "Epoch: [181][1250/3124]\tTime 0.003 (0.001)\tLoss 1.2841 (1.6229)\tPrec@1 56.250 (41.332)\n",
      "Epoch: [181][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7810 (1.6301)\tPrec@1 31.250 (41.445)\n",
      "Epoch: [181][2500/3124]\tTime 0.001 (0.001)\tLoss 1.9346 (1.6317)\tPrec@1 18.750 (41.481)\n",
      "Epoch: [181][3124/3124]\tTime 0.001 (0.001)\tLoss 2.3989 (1.6322)\tPrec@1 18.750 (41.408)\n",
      "EPOCH: 181 train Results: Prec@1 41.408 Loss: 1.6322\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0887 (1.0887)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2068 (1.5094)\tPrec@1 50.000 (45.490)\n",
      "EPOCH: 181 val Results: Prec@1 45.490 Loss: 1.5094\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/3124]\tTime 0.001 (0.001)\tLoss 1.9426 (1.9426)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [182][625/3124]\tTime 0.001 (0.001)\tLoss 1.9594 (1.6105)\tPrec@1 25.000 (42.682)\n",
      "Epoch: [182][1250/3124]\tTime 0.001 (0.001)\tLoss 2.0697 (1.6263)\tPrec@1 18.750 (41.986)\n",
      "Epoch: [182][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4653 (1.6326)\tPrec@1 31.250 (41.491)\n",
      "Epoch: [182][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7165 (1.6336)\tPrec@1 31.250 (41.488)\n",
      "Epoch: [182][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9245 (1.6313)\tPrec@1 31.250 (41.576)\n",
      "EPOCH: 182 train Results: Prec@1 41.576 Loss: 1.6313\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2034 (1.2034)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2103 (1.5011)\tPrec@1 37.500 (45.340)\n",
      "EPOCH: 182 val Results: Prec@1 45.340 Loss: 1.5011\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/3124]\tTime 0.001 (0.001)\tLoss 1.3911 (1.3911)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [183][625/3124]\tTime 0.010 (0.001)\tLoss 1.4032 (1.6002)\tPrec@1 43.750 (43.001)\n",
      "Epoch: [183][1250/3124]\tTime 0.001 (0.002)\tLoss 1.6016 (1.6133)\tPrec@1 43.750 (42.226)\n",
      "Epoch: [183][1875/3124]\tTime 0.002 (0.002)\tLoss 1.5978 (1.6261)\tPrec@1 31.250 (41.824)\n",
      "Epoch: [183][2500/3124]\tTime 0.001 (0.002)\tLoss 1.3257 (1.6272)\tPrec@1 56.250 (41.546)\n",
      "Epoch: [183][3124/3124]\tTime 0.001 (0.002)\tLoss 1.7020 (1.6318)\tPrec@1 31.250 (41.570)\n",
      "EPOCH: 183 train Results: Prec@1 41.570 Loss: 1.6318\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1670 (1.1670)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4242 (1.5211)\tPrec@1 37.500 (45.370)\n",
      "EPOCH: 183 val Results: Prec@1 45.370 Loss: 1.5211\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/3124]\tTime 0.003 (0.003)\tLoss 1.7218 (1.7218)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [184][625/3124]\tTime 0.001 (0.002)\tLoss 1.3095 (1.6039)\tPrec@1 37.500 (42.362)\n",
      "Epoch: [184][1250/3124]\tTime 0.002 (0.002)\tLoss 1.2629 (1.6131)\tPrec@1 31.250 (42.161)\n",
      "Epoch: [184][1875/3124]\tTime 0.001 (0.002)\tLoss 1.7297 (1.6221)\tPrec@1 43.750 (41.998)\n",
      "Epoch: [184][2500/3124]\tTime 0.001 (0.002)\tLoss 1.5839 (1.6295)\tPrec@1 31.250 (41.606)\n",
      "Epoch: [184][3124/3124]\tTime 0.001 (0.002)\tLoss 1.9147 (1.6314)\tPrec@1 25.000 (41.554)\n",
      "EPOCH: 184 train Results: Prec@1 41.554 Loss: 1.6314\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.9626 (0.9626)\tPrec@1 68.750 (68.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2731 (1.4926)\tPrec@1 50.000 (46.100)\n",
      "EPOCH: 184 val Results: Prec@1 46.100 Loss: 1.4926\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/3124]\tTime 0.003 (0.003)\tLoss 1.5096 (1.5096)\tPrec@1 31.250 (31.250)\n",
      "Epoch: [185][625/3124]\tTime 0.001 (0.001)\tLoss 1.1502 (1.6175)\tPrec@1 50.000 (42.153)\n",
      "Epoch: [185][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6723 (1.6229)\tPrec@1 43.750 (42.096)\n",
      "Epoch: [185][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5215 (1.6223)\tPrec@1 43.750 (42.168)\n",
      "Epoch: [185][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3857 (1.6243)\tPrec@1 56.250 (41.898)\n",
      "Epoch: [185][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6704 (1.6300)\tPrec@1 37.500 (41.636)\n",
      "EPOCH: 185 train Results: Prec@1 41.636 Loss: 1.6300\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.2261 (1.2261)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.1803 (1.5414)\tPrec@1 43.750 (44.360)\n",
      "EPOCH: 185 val Results: Prec@1 44.360 Loss: 1.5414\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/3124]\tTime 0.007 (0.007)\tLoss 1.4122 (1.4122)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [186][625/3124]\tTime 0.001 (0.002)\tLoss 1.6862 (1.6090)\tPrec@1 31.250 (42.642)\n",
      "Epoch: [186][1250/3124]\tTime 0.001 (0.002)\tLoss 1.7914 (1.6185)\tPrec@1 31.250 (42.001)\n",
      "Epoch: [186][1875/3124]\tTime 0.001 (0.002)\tLoss 2.0817 (1.6259)\tPrec@1 18.750 (41.734)\n",
      "Epoch: [186][2500/3124]\tTime 0.001 (0.002)\tLoss 1.2650 (1.6302)\tPrec@1 43.750 (41.478)\n",
      "Epoch: [186][3124/3124]\tTime 0.001 (0.002)\tLoss 1.7075 (1.6301)\tPrec@1 31.250 (41.632)\n",
      "EPOCH: 186 train Results: Prec@1 41.632 Loss: 1.6301\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0437 (1.0437)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6288 (1.5066)\tPrec@1 25.000 (46.390)\n",
      "EPOCH: 186 val Results: Prec@1 46.390 Loss: 1.5066\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/3124]\tTime 0.001 (0.001)\tLoss 1.9236 (1.9236)\tPrec@1 18.750 (18.750)\n",
      "Epoch: [187][625/3124]\tTime 0.001 (0.001)\tLoss 1.5209 (1.6170)\tPrec@1 56.250 (42.362)\n",
      "Epoch: [187][1250/3124]\tTime 0.001 (0.001)\tLoss 1.9633 (1.6250)\tPrec@1 6.250 (41.777)\n",
      "Epoch: [187][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6713 (1.6270)\tPrec@1 31.250 (41.838)\n",
      "Epoch: [187][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8779 (1.6327)\tPrec@1 43.750 (41.568)\n",
      "Epoch: [187][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9639 (1.6328)\tPrec@1 25.000 (41.538)\n",
      "EPOCH: 187 train Results: Prec@1 41.538 Loss: 1.6328\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2525 (1.2525)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2181 (1.5343)\tPrec@1 50.000 (43.730)\n",
      "EPOCH: 187 val Results: Prec@1 43.730 Loss: 1.5343\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/3124]\tTime 0.001 (0.001)\tLoss 1.2328 (1.2328)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [188][625/3124]\tTime 0.001 (0.001)\tLoss 1.4498 (1.5984)\tPrec@1 50.000 (42.712)\n",
      "Epoch: [188][1250/3124]\tTime 0.001 (0.001)\tLoss 1.5161 (1.6117)\tPrec@1 37.500 (42.291)\n",
      "Epoch: [188][1875/3124]\tTime 0.001 (0.001)\tLoss 2.0118 (1.6146)\tPrec@1 31.250 (42.214)\n",
      "Epoch: [188][2500/3124]\tTime 0.001 (0.001)\tLoss 1.8670 (1.6194)\tPrec@1 43.750 (41.958)\n",
      "Epoch: [188][3124/3124]\tTime 0.001 (0.001)\tLoss 1.3481 (1.6235)\tPrec@1 62.500 (41.796)\n",
      "EPOCH: 188 train Results: Prec@1 41.796 Loss: 1.6235\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 0.9968 (0.9968)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.6072 (1.5106)\tPrec@1 37.500 (46.060)\n",
      "EPOCH: 188 val Results: Prec@1 46.060 Loss: 1.5106\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/3124]\tTime 0.003 (0.003)\tLoss 1.6122 (1.6122)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [189][625/3124]\tTime 0.001 (0.001)\tLoss 1.5807 (1.5974)\tPrec@1 25.000 (42.033)\n",
      "Epoch: [189][1250/3124]\tTime 0.003 (0.001)\tLoss 1.5043 (1.6092)\tPrec@1 50.000 (41.762)\n",
      "Epoch: [189][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5837 (1.6178)\tPrec@1 43.750 (41.581)\n",
      "Epoch: [189][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3649 (1.6234)\tPrec@1 50.000 (41.683)\n",
      "Epoch: [189][3124/3124]\tTime 0.001 (0.001)\tLoss 1.9325 (1.6293)\tPrec@1 43.750 (41.618)\n",
      "EPOCH: 189 train Results: Prec@1 41.618 Loss: 1.6293\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0355 (1.0355)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3987 (1.4986)\tPrec@1 37.500 (46.310)\n",
      "EPOCH: 189 val Results: Prec@1 46.310 Loss: 1.4986\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/3124]\tTime 0.003 (0.003)\tLoss 1.5776 (1.5776)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [190][625/3124]\tTime 0.001 (0.001)\tLoss 1.7274 (1.6106)\tPrec@1 43.750 (42.063)\n",
      "Epoch: [190][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8304 (1.6189)\tPrec@1 31.250 (41.837)\n",
      "Epoch: [190][1875/3124]\tTime 0.002 (0.001)\tLoss 1.3500 (1.6262)\tPrec@1 50.000 (41.501)\n",
      "Epoch: [190][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4588 (1.6291)\tPrec@1 62.500 (41.506)\n",
      "Epoch: [190][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7248 (1.6323)\tPrec@1 31.250 (41.376)\n",
      "EPOCH: 190 train Results: Prec@1 41.376 Loss: 1.6323\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2823 (1.2823)\tPrec@1 43.750 (43.750)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4217 (1.5146)\tPrec@1 31.250 (45.500)\n",
      "EPOCH: 190 val Results: Prec@1 45.500 Loss: 1.5146\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/3124]\tTime 0.002 (0.002)\tLoss 1.6381 (1.6381)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [191][625/3124]\tTime 0.001 (0.001)\tLoss 1.2994 (1.6064)\tPrec@1 68.750 (41.933)\n",
      "Epoch: [191][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2989 (1.6127)\tPrec@1 43.750 (41.956)\n",
      "Epoch: [191][1875/3124]\tTime 0.001 (0.001)\tLoss 1.6514 (1.6170)\tPrec@1 37.500 (41.891)\n",
      "Epoch: [191][2500/3124]\tTime 0.001 (0.001)\tLoss 1.3961 (1.6190)\tPrec@1 37.500 (41.833)\n",
      "Epoch: [191][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6163 (1.6228)\tPrec@1 56.250 (41.666)\n",
      "EPOCH: 191 train Results: Prec@1 41.666 Loss: 1.6228\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0773 (1.0773)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.5891 (1.5182)\tPrec@1 25.000 (46.150)\n",
      "EPOCH: 191 val Results: Prec@1 46.150 Loss: 1.5182\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/3124]\tTime 0.001 (0.001)\tLoss 1.3573 (1.3573)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [192][625/3124]\tTime 0.001 (0.001)\tLoss 1.6128 (1.6118)\tPrec@1 31.250 (41.803)\n",
      "Epoch: [192][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7063 (1.6176)\tPrec@1 50.000 (41.612)\n",
      "Epoch: [192][1875/3124]\tTime 0.001 (0.001)\tLoss 1.2800 (1.6257)\tPrec@1 50.000 (41.441)\n",
      "Epoch: [192][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5251 (1.6264)\tPrec@1 25.000 (41.368)\n",
      "Epoch: [192][3124/3124]\tTime 0.001 (0.001)\tLoss 1.2671 (1.6305)\tPrec@1 31.250 (41.268)\n",
      "EPOCH: 192 train Results: Prec@1 41.268 Loss: 1.6305\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1852 (1.1852)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.001 (0.000)\tLoss 1.5836 (1.5244)\tPrec@1 31.250 (44.440)\n",
      "EPOCH: 192 val Results: Prec@1 44.440 Loss: 1.5244\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/3124]\tTime 0.001 (0.001)\tLoss 1.7264 (1.7264)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [193][625/3124]\tTime 0.003 (0.001)\tLoss 2.0660 (1.6137)\tPrec@1 25.000 (41.933)\n",
      "Epoch: [193][1250/3124]\tTime 0.001 (0.001)\tLoss 2.0098 (1.6201)\tPrec@1 12.500 (41.782)\n",
      "Epoch: [193][1875/3124]\tTime 0.001 (0.001)\tLoss 1.8688 (1.6286)\tPrec@1 25.000 (41.371)\n",
      "Epoch: [193][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4710 (1.6315)\tPrec@1 50.000 (41.426)\n",
      "Epoch: [193][3124/3124]\tTime 0.001 (0.001)\tLoss 2.2520 (1.6303)\tPrec@1 31.250 (41.490)\n",
      "EPOCH: 193 train Results: Prec@1 41.490 Loss: 1.6303\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.0757 (1.0757)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7041 (1.5167)\tPrec@1 43.750 (45.010)\n",
      "EPOCH: 193 val Results: Prec@1 45.010 Loss: 1.5167\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/3124]\tTime 0.001 (0.001)\tLoss 1.4409 (1.4409)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [194][625/3124]\tTime 0.002 (0.001)\tLoss 1.4047 (1.6146)\tPrec@1 50.000 (42.412)\n",
      "Epoch: [194][1250/3124]\tTime 0.001 (0.001)\tLoss 1.6437 (1.6221)\tPrec@1 31.250 (41.991)\n",
      "Epoch: [194][1875/3124]\tTime 0.001 (0.001)\tLoss 1.5771 (1.6268)\tPrec@1 56.250 (41.591)\n",
      "Epoch: [194][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6135 (1.6301)\tPrec@1 31.250 (41.418)\n",
      "Epoch: [194][3124/3124]\tTime 0.001 (0.001)\tLoss 1.6174 (1.6315)\tPrec@1 37.500 (41.490)\n",
      "EPOCH: 194 train Results: Prec@1 41.490 Loss: 1.6315\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.1772 (1.1772)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4172 (1.5223)\tPrec@1 50.000 (44.860)\n",
      "EPOCH: 194 val Results: Prec@1 44.860 Loss: 1.5223\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/3124]\tTime 0.001 (0.001)\tLoss 1.1498 (1.1498)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [195][625/3124]\tTime 0.012 (0.002)\tLoss 2.1976 (1.6192)\tPrec@1 25.000 (42.202)\n",
      "Epoch: [195][1250/3124]\tTime 0.001 (0.002)\tLoss 1.8793 (1.6287)\tPrec@1 12.500 (41.742)\n",
      "Epoch: [195][1875/3124]\tTime 0.001 (0.002)\tLoss 1.6564 (1.6313)\tPrec@1 43.750 (41.508)\n",
      "Epoch: [195][2500/3124]\tTime 0.001 (0.002)\tLoss 1.3026 (1.6354)\tPrec@1 56.250 (41.383)\n",
      "Epoch: [195][3124/3124]\tTime 0.001 (0.002)\tLoss 1.6911 (1.6357)\tPrec@1 31.250 (41.422)\n",
      "EPOCH: 195 train Results: Prec@1 41.422 Loss: 1.6357\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.2639 (1.2639)\tPrec@1 50.000 (50.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.8962 (1.5452)\tPrec@1 25.000 (43.750)\n",
      "EPOCH: 195 val Results: Prec@1 43.750 Loss: 1.5452\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/3124]\tTime 0.002 (0.002)\tLoss 1.7377 (1.7377)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [196][625/3124]\tTime 0.001 (0.001)\tLoss 1.5431 (1.6098)\tPrec@1 25.000 (42.562)\n",
      "Epoch: [196][1250/3124]\tTime 0.001 (0.001)\tLoss 1.2753 (1.6164)\tPrec@1 56.250 (42.286)\n",
      "Epoch: [196][1875/3124]\tTime 0.001 (0.001)\tLoss 1.7428 (1.6271)\tPrec@1 43.750 (41.818)\n",
      "Epoch: [196][2500/3124]\tTime 0.001 (0.001)\tLoss 1.7995 (1.6295)\tPrec@1 37.500 (41.691)\n",
      "Epoch: [196][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5229 (1.6308)\tPrec@1 43.750 (41.522)\n",
      "EPOCH: 196 train Results: Prec@1 41.522 Loss: 1.6308\n",
      "Test: [0/624]\tTime 0.002 (0.002)\tLoss 1.0168 (1.0168)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.3829 (1.5162)\tPrec@1 31.250 (45.340)\n",
      "EPOCH: 196 val Results: Prec@1 45.340 Loss: 1.5162\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/3124]\tTime 0.001 (0.001)\tLoss 1.2160 (1.2160)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [197][625/3124]\tTime 0.001 (0.001)\tLoss 1.5124 (1.5895)\tPrec@1 43.750 (42.941)\n",
      "Epoch: [197][1250/3124]\tTime 0.001 (0.001)\tLoss 1.7022 (1.6095)\tPrec@1 37.500 (42.576)\n",
      "Epoch: [197][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4421 (1.6188)\tPrec@1 37.500 (42.181)\n",
      "Epoch: [197][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6340 (1.6274)\tPrec@1 43.750 (41.723)\n",
      "Epoch: [197][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4920 (1.6296)\tPrec@1 56.250 (41.626)\n",
      "EPOCH: 197 train Results: Prec@1 41.626 Loss: 1.6296\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.0812 (1.0812)\tPrec@1 62.500 (62.500)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.7084 (1.5084)\tPrec@1 31.250 (45.940)\n",
      "EPOCH: 197 val Results: Prec@1 45.940 Loss: 1.5084\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/3124]\tTime 0.004 (0.004)\tLoss 1.4665 (1.4665)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [198][625/3124]\tTime 0.001 (0.001)\tLoss 1.8114 (1.6087)\tPrec@1 31.250 (42.113)\n",
      "Epoch: [198][1250/3124]\tTime 0.001 (0.001)\tLoss 1.8537 (1.6278)\tPrec@1 37.500 (41.687)\n",
      "Epoch: [198][1875/3124]\tTime 0.005 (0.001)\tLoss 1.9926 (1.6279)\tPrec@1 37.500 (41.758)\n",
      "Epoch: [198][2500/3124]\tTime 0.001 (0.001)\tLoss 1.4372 (1.6275)\tPrec@1 43.750 (41.761)\n",
      "Epoch: [198][3124/3124]\tTime 0.001 (0.001)\tLoss 1.5979 (1.6295)\tPrec@1 37.500 (41.588)\n",
      "EPOCH: 198 train Results: Prec@1 41.588 Loss: 1.6295\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 1.2071 (1.2071)\tPrec@1 56.250 (56.250)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4553 (1.5411)\tPrec@1 18.750 (43.890)\n",
      "EPOCH: 198 val Results: Prec@1 43.890 Loss: 1.5411\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/3124]\tTime 0.002 (0.002)\tLoss 1.9499 (1.9499)\tPrec@1 37.500 (37.500)\n",
      "Epoch: [199][625/3124]\tTime 0.002 (0.001)\tLoss 1.6393 (1.5985)\tPrec@1 37.500 (43.041)\n",
      "Epoch: [199][1250/3124]\tTime 0.001 (0.001)\tLoss 1.3486 (1.6135)\tPrec@1 50.000 (42.456)\n",
      "Epoch: [199][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4053 (1.6207)\tPrec@1 50.000 (42.171)\n",
      "Epoch: [199][2500/3124]\tTime 0.001 (0.001)\tLoss 1.6613 (1.6270)\tPrec@1 50.000 (41.831)\n",
      "Epoch: [199][3124/3124]\tTime 0.001 (0.001)\tLoss 1.4919 (1.6301)\tPrec@1 37.500 (41.676)\n",
      "EPOCH: 199 train Results: Prec@1 41.676 Loss: 1.6301\n",
      "Test: [0/624]\tTime 0.000 (0.000)\tLoss 0.8410 (0.8410)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.2880 (1.5255)\tPrec@1 50.000 (45.880)\n",
      "EPOCH: 199 val Results: Prec@1 45.880 Loss: 1.5255\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/3124]\tTime 0.001 (0.001)\tLoss 1.6333 (1.6333)\tPrec@1 43.750 (43.750)\n",
      "Epoch: [200][625/3124]\tTime 0.001 (0.001)\tLoss 1.6524 (1.6336)\tPrec@1 50.000 (41.284)\n",
      "Epoch: [200][1250/3124]\tTime 0.002 (0.001)\tLoss 1.1590 (1.6372)\tPrec@1 62.500 (41.377)\n",
      "Epoch: [200][1875/3124]\tTime 0.001 (0.001)\tLoss 1.4195 (1.6362)\tPrec@1 62.500 (41.435)\n",
      "Epoch: [200][2500/3124]\tTime 0.001 (0.001)\tLoss 1.5611 (1.6327)\tPrec@1 37.500 (41.413)\n",
      "Epoch: [200][3124/3124]\tTime 0.001 (0.001)\tLoss 1.7148 (1.6341)\tPrec@1 31.250 (41.360)\n",
      "EPOCH: 200 train Results: Prec@1 41.360 Loss: 1.6341\n",
      "Test: [0/624]\tTime 0.001 (0.001)\tLoss 1.1331 (1.1331)\tPrec@1 75.000 (75.000)\n",
      "Test: [624/624]\tTime 0.000 (0.000)\tLoss 1.4550 (1.5082)\tPrec@1 50.000 (45.810)\n",
      "EPOCH: 200 val Results: Prec@1 45.810 Loss: 1.5082\n",
      "Best Prec@1: 46.950\n",
      "\n",
      "End time:  Thu Apr  4 23:58:10 2024\n",
      "train executed in 810.9016 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 16\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'scheduler': None, \n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer2 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Thu Apr  4 23:58:10 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/781]\tTime 0.028 (0.028)\tLoss 4.8473 (4.8473)\tPrec@1 10.938 (10.938)\n",
      "Epoch: [1][156/781]\tTime 0.003 (0.002)\tLoss 2.2352 (2.8937)\tPrec@1 29.688 (25.995)\n",
      "Epoch: [1][312/781]\tTime 0.002 (0.002)\tLoss 1.8190 (2.4371)\tPrec@1 40.625 (29.907)\n",
      "Epoch: [1][468/781]\tTime 0.003 (0.002)\tLoss 1.8261 (2.2225)\tPrec@1 40.625 (32.513)\n",
      "Epoch: [1][624/781]\tTime 0.003 (0.002)\tLoss 1.7003 (2.0929)\tPrec@1 39.062 (34.163)\n",
      "Epoch: [1][780/781]\tTime 0.001 (0.002)\tLoss 1.4736 (2.0061)\tPrec@1 46.875 (35.509)\n",
      "Epoch: [1][781/781]\tTime 0.003 (0.002)\tLoss 1.8692 (2.0060)\tPrec@1 31.250 (35.508)\n",
      "EPOCH: 1 train Results: Prec@1 35.508 Loss: 2.0060\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.5254 (1.5254)\tPrec@1 40.625 (40.625)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4976 (1.5380)\tPrec@1 31.250 (45.300)\n",
      "EPOCH: 1 val Results: Prec@1 45.300 Loss: 1.5380\n",
      "Best Prec@1: 45.300\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/781]\tTime 0.002 (0.002)\tLoss 1.4847 (1.4847)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [2][156/781]\tTime 0.001 (0.002)\tLoss 1.5823 (1.5279)\tPrec@1 46.875 (45.462)\n",
      "Epoch: [2][312/781]\tTime 0.002 (0.002)\tLoss 1.5654 (1.5163)\tPrec@1 40.625 (45.697)\n",
      "Epoch: [2][468/781]\tTime 0.002 (0.002)\tLoss 1.7418 (1.5151)\tPrec@1 42.188 (45.749)\n",
      "Epoch: [2][624/781]\tTime 0.001 (0.002)\tLoss 1.7300 (1.5106)\tPrec@1 39.062 (45.940)\n",
      "Epoch: [2][780/781]\tTime 0.002 (0.002)\tLoss 1.5350 (1.5023)\tPrec@1 50.000 (46.265)\n",
      "Epoch: [2][781/781]\tTime 0.002 (0.002)\tLoss 1.0902 (1.5022)\tPrec@1 62.500 (46.270)\n",
      "EPOCH: 2 train Results: Prec@1 46.270 Loss: 1.5022\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3968 (1.3968)\tPrec@1 45.312 (45.312)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.6062 (1.4249)\tPrec@1 25.000 (48.880)\n",
      "EPOCH: 2 val Results: Prec@1 48.880 Loss: 1.4249\n",
      "Best Prec@1: 48.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/781]\tTime 0.003 (0.003)\tLoss 1.3400 (1.3400)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [3][156/781]\tTime 0.003 (0.002)\tLoss 1.5486 (1.4159)\tPrec@1 50.000 (50.249)\n",
      "Epoch: [3][312/781]\tTime 0.002 (0.002)\tLoss 1.4106 (1.4132)\tPrec@1 51.562 (50.329)\n",
      "Epoch: [3][468/781]\tTime 0.001 (0.002)\tLoss 1.5600 (1.4175)\tPrec@1 39.062 (49.947)\n",
      "Epoch: [3][624/781]\tTime 0.001 (0.002)\tLoss 1.5439 (1.4205)\tPrec@1 40.625 (49.513)\n",
      "Epoch: [3][780/781]\tTime 0.002 (0.002)\tLoss 1.4316 (1.4202)\tPrec@1 46.875 (49.448)\n",
      "Epoch: [3][781/781]\tTime 0.001 (0.002)\tLoss 1.7800 (1.4203)\tPrec@1 25.000 (49.440)\n",
      "EPOCH: 3 train Results: Prec@1 49.440 Loss: 1.4203\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.2851 (1.2851)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3514 (1.3833)\tPrec@1 31.250 (50.380)\n",
      "EPOCH: 3 val Results: Prec@1 50.380 Loss: 1.3833\n",
      "Best Prec@1: 50.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/781]\tTime 0.002 (0.002)\tLoss 1.3807 (1.3807)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [4][156/781]\tTime 0.001 (0.002)\tLoss 1.2618 (1.3592)\tPrec@1 60.938 (51.632)\n",
      "Epoch: [4][312/781]\tTime 0.002 (0.002)\tLoss 1.4205 (1.3699)\tPrec@1 50.000 (51.123)\n",
      "Epoch: [4][468/781]\tTime 0.003 (0.002)\tLoss 1.4610 (1.3751)\tPrec@1 45.312 (50.890)\n",
      "Epoch: [4][624/781]\tTime 0.001 (0.002)\tLoss 1.4519 (1.3822)\tPrec@1 37.500 (50.655)\n",
      "Epoch: [4][780/781]\tTime 0.001 (0.002)\tLoss 1.4558 (1.3855)\tPrec@1 40.625 (50.626)\n",
      "Epoch: [4][781/781]\tTime 0.002 (0.002)\tLoss 1.3193 (1.3855)\tPrec@1 62.500 (50.630)\n",
      "EPOCH: 4 train Results: Prec@1 50.630 Loss: 1.3855\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3281 (1.3281)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3594 (1.3648)\tPrec@1 43.750 (50.970)\n",
      "EPOCH: 4 val Results: Prec@1 50.970 Loss: 1.3648\n",
      "Best Prec@1: 50.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/781]\tTime 0.002 (0.002)\tLoss 1.3701 (1.3701)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [5][156/781]\tTime 0.001 (0.002)\tLoss 1.6057 (1.3168)\tPrec@1 45.312 (53.115)\n",
      "Epoch: [5][312/781]\tTime 0.002 (0.002)\tLoss 1.5277 (1.3471)\tPrec@1 45.312 (51.772)\n",
      "Epoch: [5][468/781]\tTime 0.003 (0.002)\tLoss 1.4042 (1.3538)\tPrec@1 56.250 (51.416)\n",
      "Epoch: [5][624/781]\tTime 0.001 (0.002)\tLoss 1.2717 (1.3592)\tPrec@1 53.125 (51.120)\n",
      "Epoch: [5][780/781]\tTime 0.001 (0.002)\tLoss 1.2602 (1.3666)\tPrec@1 57.812 (50.956)\n",
      "Epoch: [5][781/781]\tTime 0.002 (0.002)\tLoss 1.6777 (1.3667)\tPrec@1 37.500 (50.952)\n",
      "EPOCH: 5 train Results: Prec@1 50.952 Loss: 1.3667\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2335 (1.2335)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2744 (1.3493)\tPrec@1 43.750 (51.400)\n",
      "EPOCH: 5 val Results: Prec@1 51.400 Loss: 1.3493\n",
      "Best Prec@1: 51.400\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/781]\tTime 0.002 (0.002)\tLoss 1.1742 (1.1742)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [6][156/781]\tTime 0.003 (0.002)\tLoss 1.5467 (1.2986)\tPrec@1 48.438 (53.881)\n",
      "Epoch: [6][312/781]\tTime 0.001 (0.002)\tLoss 1.2089 (1.3161)\tPrec@1 59.375 (53.260)\n",
      "Epoch: [6][468/781]\tTime 0.001 (0.002)\tLoss 1.2177 (1.3271)\tPrec@1 56.250 (52.662)\n",
      "Epoch: [6][624/781]\tTime 0.001 (0.002)\tLoss 1.2618 (1.3381)\tPrec@1 46.875 (52.322)\n",
      "Epoch: [6][780/781]\tTime 0.001 (0.002)\tLoss 1.6497 (1.3456)\tPrec@1 39.062 (52.177)\n",
      "Epoch: [6][781/781]\tTime 0.002 (0.002)\tLoss 1.2663 (1.3456)\tPrec@1 62.500 (52.180)\n",
      "EPOCH: 6 train Results: Prec@1 52.180 Loss: 1.3456\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1769 (1.1769)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.001 (0.000)\tLoss 1.3522 (1.3363)\tPrec@1 37.500 (51.950)\n",
      "EPOCH: 6 val Results: Prec@1 51.950 Loss: 1.3363\n",
      "Best Prec@1: 51.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/781]\tTime 0.002 (0.002)\tLoss 1.3232 (1.3232)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [7][156/781]\tTime 0.002 (0.002)\tLoss 1.3102 (1.3014)\tPrec@1 54.688 (54.250)\n",
      "Epoch: [7][312/781]\tTime 0.001 (0.002)\tLoss 1.4813 (1.3149)\tPrec@1 35.938 (53.425)\n",
      "Epoch: [7][468/781]\tTime 0.002 (0.002)\tLoss 1.4226 (1.3259)\tPrec@1 48.438 (52.882)\n",
      "Epoch: [7][624/781]\tTime 0.001 (0.002)\tLoss 1.2673 (1.3294)\tPrec@1 54.688 (52.712)\n",
      "Epoch: [7][780/781]\tTime 0.002 (0.002)\tLoss 1.4854 (1.3365)\tPrec@1 37.500 (52.419)\n",
      "Epoch: [7][781/781]\tTime 0.002 (0.002)\tLoss 1.3079 (1.3365)\tPrec@1 56.250 (52.420)\n",
      "EPOCH: 7 train Results: Prec@1 52.420 Loss: 1.3365\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2721 (1.2721)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4721 (1.3315)\tPrec@1 25.000 (52.070)\n",
      "EPOCH: 7 val Results: Prec@1 52.070 Loss: 1.3315\n",
      "Best Prec@1: 52.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/781]\tTime 0.003 (0.003)\tLoss 1.2623 (1.2623)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [8][156/781]\tTime 0.002 (0.002)\tLoss 1.3471 (1.2979)\tPrec@1 53.125 (53.543)\n",
      "Epoch: [8][312/781]\tTime 0.001 (0.002)\tLoss 1.4313 (1.3186)\tPrec@1 50.000 (52.536)\n",
      "Epoch: [8][468/781]\tTime 0.001 (0.002)\tLoss 1.2657 (1.3197)\tPrec@1 59.375 (52.615)\n",
      "Epoch: [8][624/781]\tTime 0.001 (0.002)\tLoss 1.2505 (1.3191)\tPrec@1 51.562 (52.695)\n",
      "Epoch: [8][780/781]\tTime 0.001 (0.002)\tLoss 1.3835 (1.3244)\tPrec@1 51.562 (52.531)\n",
      "Epoch: [8][781/781]\tTime 0.002 (0.002)\tLoss 1.4517 (1.3245)\tPrec@1 50.000 (52.530)\n",
      "EPOCH: 8 train Results: Prec@1 52.530 Loss: 1.3245\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2122 (1.2122)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2191 (1.3204)\tPrec@1 43.750 (52.860)\n",
      "EPOCH: 8 val Results: Prec@1 52.860 Loss: 1.3204\n",
      "Best Prec@1: 52.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/781]\tTime 0.002 (0.002)\tLoss 1.3155 (1.3155)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [9][156/781]\tTime 0.002 (0.002)\tLoss 1.4141 (1.2694)\tPrec@1 40.625 (54.717)\n",
      "Epoch: [9][312/781]\tTime 0.002 (0.002)\tLoss 1.3647 (1.2874)\tPrec@1 53.125 (54.019)\n",
      "Epoch: [9][468/781]\tTime 0.002 (0.002)\tLoss 1.5312 (1.3043)\tPrec@1 39.062 (53.385)\n",
      "Epoch: [9][624/781]\tTime 0.001 (0.002)\tLoss 1.2372 (1.3094)\tPrec@1 54.688 (53.178)\n",
      "Epoch: [9][780/781]\tTime 0.001 (0.002)\tLoss 1.5450 (1.3140)\tPrec@1 50.000 (52.979)\n",
      "Epoch: [9][781/781]\tTime 0.001 (0.002)\tLoss 1.2702 (1.3140)\tPrec@1 56.250 (52.980)\n",
      "EPOCH: 9 train Results: Prec@1 52.980 Loss: 1.3140\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1537 (1.1537)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1430 (1.3207)\tPrec@1 56.250 (53.170)\n",
      "EPOCH: 9 val Results: Prec@1 53.170 Loss: 1.3207\n",
      "Best Prec@1: 53.170\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/781]\tTime 0.002 (0.002)\tLoss 1.3442 (1.3442)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [10][156/781]\tTime 0.001 (0.002)\tLoss 1.0548 (1.2550)\tPrec@1 60.938 (55.514)\n",
      "Epoch: [10][312/781]\tTime 0.002 (0.002)\tLoss 1.4325 (1.2765)\tPrec@1 50.000 (54.782)\n",
      "Epoch: [10][468/781]\tTime 0.002 (0.002)\tLoss 1.4071 (1.2893)\tPrec@1 48.438 (54.194)\n",
      "Epoch: [10][624/781]\tTime 0.001 (0.002)\tLoss 1.4577 (1.2935)\tPrec@1 53.125 (53.970)\n",
      "Epoch: [10][780/781]\tTime 0.003 (0.002)\tLoss 1.1860 (1.3000)\tPrec@1 62.500 (53.717)\n",
      "Epoch: [10][781/781]\tTime 0.002 (0.002)\tLoss 0.9810 (1.2999)\tPrec@1 68.750 (53.722)\n",
      "EPOCH: 10 train Results: Prec@1 53.722 Loss: 1.2999\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1873 (1.1873)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5601 (1.3167)\tPrec@1 50.000 (52.990)\n",
      "EPOCH: 10 val Results: Prec@1 52.990 Loss: 1.3167\n",
      "Best Prec@1: 53.170\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/781]\tTime 0.005 (0.005)\tLoss 1.1575 (1.1575)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [11][156/781]\tTime 0.001 (0.002)\tLoss 1.3369 (1.2504)\tPrec@1 43.750 (55.414)\n",
      "Epoch: [11][312/781]\tTime 0.001 (0.002)\tLoss 1.2005 (1.2688)\tPrec@1 56.250 (54.663)\n",
      "Epoch: [11][468/781]\tTime 0.001 (0.002)\tLoss 1.4905 (1.2821)\tPrec@1 43.750 (54.021)\n",
      "Epoch: [11][624/781]\tTime 0.001 (0.002)\tLoss 1.0959 (1.2889)\tPrec@1 64.062 (53.750)\n",
      "Epoch: [11][780/781]\tTime 0.002 (0.002)\tLoss 1.4089 (1.2959)\tPrec@1 45.312 (53.567)\n",
      "Epoch: [11][781/781]\tTime 0.002 (0.002)\tLoss 0.7660 (1.2957)\tPrec@1 87.500 (53.578)\n",
      "EPOCH: 11 train Results: Prec@1 53.578 Loss: 1.2957\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1092 (1.1092)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3283 (1.3039)\tPrec@1 50.000 (53.380)\n",
      "EPOCH: 11 val Results: Prec@1 53.380 Loss: 1.3039\n",
      "Best Prec@1: 53.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/781]\tTime 0.002 (0.002)\tLoss 1.2411 (1.2411)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [12][156/781]\tTime 0.001 (0.002)\tLoss 1.2153 (1.2330)\tPrec@1 64.062 (56.051)\n",
      "Epoch: [12][312/781]\tTime 0.001 (0.002)\tLoss 1.1867 (1.2539)\tPrec@1 62.500 (55.351)\n",
      "Epoch: [12][468/781]\tTime 0.002 (0.002)\tLoss 1.2191 (1.2718)\tPrec@1 62.500 (54.554)\n",
      "Epoch: [12][624/781]\tTime 0.005 (0.002)\tLoss 1.2153 (1.2790)\tPrec@1 56.250 (54.318)\n",
      "Epoch: [12][780/781]\tTime 0.002 (0.002)\tLoss 1.1077 (1.2862)\tPrec@1 53.125 (54.099)\n",
      "Epoch: [12][781/781]\tTime 0.003 (0.002)\tLoss 1.3230 (1.2862)\tPrec@1 37.500 (54.094)\n",
      "EPOCH: 12 train Results: Prec@1 54.094 Loss: 1.2862\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2301 (1.2301)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0460 (1.3033)\tPrec@1 43.750 (53.010)\n",
      "EPOCH: 12 val Results: Prec@1 53.010 Loss: 1.3033\n",
      "Best Prec@1: 53.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/781]\tTime 0.003 (0.003)\tLoss 1.2685 (1.2685)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [13][156/781]\tTime 0.003 (0.003)\tLoss 1.2193 (1.2392)\tPrec@1 54.688 (55.643)\n",
      "Epoch: [13][312/781]\tTime 0.001 (0.002)\tLoss 1.0389 (1.2516)\tPrec@1 62.500 (55.277)\n",
      "Epoch: [13][468/781]\tTime 0.002 (0.002)\tLoss 1.3938 (1.2657)\tPrec@1 45.312 (54.794)\n",
      "Epoch: [13][624/781]\tTime 0.002 (0.002)\tLoss 1.1017 (1.2770)\tPrec@1 57.812 (54.407)\n",
      "Epoch: [13][780/781]\tTime 0.003 (0.002)\tLoss 1.1666 (1.2824)\tPrec@1 54.688 (54.277)\n",
      "Epoch: [13][781/781]\tTime 0.002 (0.002)\tLoss 1.3547 (1.2824)\tPrec@1 50.000 (54.276)\n",
      "EPOCH: 13 train Results: Prec@1 54.276 Loss: 1.2824\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.2301 (1.2301)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2735 (1.2927)\tPrec@1 50.000 (53.210)\n",
      "EPOCH: 13 val Results: Prec@1 53.210 Loss: 1.2927\n",
      "Best Prec@1: 53.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/781]\tTime 0.004 (0.004)\tLoss 1.0854 (1.0854)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [14][156/781]\tTime 0.001 (0.002)\tLoss 1.3829 (1.2446)\tPrec@1 51.562 (55.991)\n",
      "Epoch: [14][312/781]\tTime 0.001 (0.002)\tLoss 1.2962 (1.2565)\tPrec@1 51.562 (55.182)\n",
      "Epoch: [14][468/781]\tTime 0.001 (0.002)\tLoss 1.3862 (1.2702)\tPrec@1 48.438 (54.737)\n",
      "Epoch: [14][624/781]\tTime 0.001 (0.002)\tLoss 1.1725 (1.2758)\tPrec@1 46.875 (54.610)\n",
      "Epoch: [14][780/781]\tTime 0.003 (0.002)\tLoss 1.4343 (1.2803)\tPrec@1 40.625 (54.395)\n",
      "Epoch: [14][781/781]\tTime 0.002 (0.002)\tLoss 1.5606 (1.2803)\tPrec@1 56.250 (54.396)\n",
      "EPOCH: 14 train Results: Prec@1 54.396 Loss: 1.2803\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2383 (1.2383)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4723 (1.2895)\tPrec@1 31.250 (53.820)\n",
      "EPOCH: 14 val Results: Prec@1 53.820 Loss: 1.2895\n",
      "Best Prec@1: 53.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/781]\tTime 0.003 (0.003)\tLoss 1.1130 (1.1130)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [15][156/781]\tTime 0.001 (0.002)\tLoss 1.4841 (1.2204)\tPrec@1 51.562 (56.340)\n",
      "Epoch: [15][312/781]\tTime 0.001 (0.002)\tLoss 1.0806 (1.2470)\tPrec@1 62.500 (55.511)\n",
      "Epoch: [15][468/781]\tTime 0.001 (0.002)\tLoss 1.3256 (1.2582)\tPrec@1 53.125 (54.991)\n",
      "Epoch: [15][624/781]\tTime 0.001 (0.002)\tLoss 1.3981 (1.2668)\tPrec@1 53.125 (54.690)\n",
      "Epoch: [15][780/781]\tTime 0.002 (0.002)\tLoss 1.4250 (1.2710)\tPrec@1 46.875 (54.641)\n",
      "Epoch: [15][781/781]\tTime 0.002 (0.002)\tLoss 1.7684 (1.2711)\tPrec@1 18.750 (54.630)\n",
      "EPOCH: 15 train Results: Prec@1 54.630 Loss: 1.2711\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1784 (1.1784)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4944 (1.3001)\tPrec@1 43.750 (53.080)\n",
      "EPOCH: 15 val Results: Prec@1 53.080 Loss: 1.3001\n",
      "Best Prec@1: 53.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/781]\tTime 0.002 (0.002)\tLoss 1.2110 (1.2110)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [16][156/781]\tTime 0.001 (0.002)\tLoss 1.1409 (1.2000)\tPrec@1 65.625 (57.414)\n",
      "Epoch: [16][312/781]\tTime 0.001 (0.002)\tLoss 1.1184 (1.2310)\tPrec@1 60.938 (56.175)\n",
      "Epoch: [16][468/781]\tTime 0.001 (0.002)\tLoss 1.3234 (1.2517)\tPrec@1 54.688 (55.540)\n",
      "Epoch: [16][624/781]\tTime 0.002 (0.002)\tLoss 1.2864 (1.2635)\tPrec@1 48.438 (55.005)\n",
      "Epoch: [16][780/781]\tTime 0.003 (0.003)\tLoss 1.4534 (1.2722)\tPrec@1 51.562 (54.665)\n",
      "Epoch: [16][781/781]\tTime 0.002 (0.003)\tLoss 1.4864 (1.2723)\tPrec@1 43.750 (54.662)\n",
      "EPOCH: 16 train Results: Prec@1 54.662 Loss: 1.2723\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1093 (1.1093)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3020 (1.2875)\tPrec@1 56.250 (53.790)\n",
      "EPOCH: 16 val Results: Prec@1 53.790 Loss: 1.2875\n",
      "Best Prec@1: 53.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/781]\tTime 0.006 (0.006)\tLoss 1.1778 (1.1778)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [17][156/781]\tTime 0.001 (0.004)\tLoss 1.4903 (1.2143)\tPrec@1 59.375 (56.837)\n",
      "Epoch: [17][312/781]\tTime 0.001 (0.003)\tLoss 1.4205 (1.2315)\tPrec@1 51.562 (56.035)\n",
      "Epoch: [17][468/781]\tTime 0.001 (0.003)\tLoss 1.2648 (1.2488)\tPrec@1 51.562 (55.384)\n",
      "Epoch: [17][624/781]\tTime 0.002 (0.003)\tLoss 1.2693 (1.2566)\tPrec@1 57.812 (55.005)\n",
      "Epoch: [17][780/781]\tTime 0.001 (0.003)\tLoss 1.2251 (1.2644)\tPrec@1 56.250 (54.742)\n",
      "Epoch: [17][781/781]\tTime 0.001 (0.003)\tLoss 1.1006 (1.2643)\tPrec@1 62.500 (54.744)\n",
      "EPOCH: 17 train Results: Prec@1 54.744 Loss: 1.2643\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1919 (1.1919)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3268 (1.2999)\tPrec@1 37.500 (53.040)\n",
      "EPOCH: 17 val Results: Prec@1 53.040 Loss: 1.2999\n",
      "Best Prec@1: 53.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/781]\tTime 0.003 (0.003)\tLoss 1.1577 (1.1577)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [18][156/781]\tTime 0.002 (0.003)\tLoss 1.1577 (1.2194)\tPrec@1 54.688 (56.240)\n",
      "Epoch: [18][312/781]\tTime 0.003 (0.003)\tLoss 1.0267 (1.2351)\tPrec@1 64.062 (55.586)\n",
      "Epoch: [18][468/781]\tTime 0.003 (0.002)\tLoss 1.1053 (1.2553)\tPrec@1 64.062 (55.021)\n",
      "Epoch: [18][624/781]\tTime 0.001 (0.002)\tLoss 1.2084 (1.2596)\tPrec@1 54.688 (54.780)\n",
      "Epoch: [18][780/781]\tTime 0.001 (0.002)\tLoss 1.2285 (1.2637)\tPrec@1 51.562 (54.681)\n",
      "Epoch: [18][781/781]\tTime 0.002 (0.002)\tLoss 1.3822 (1.2638)\tPrec@1 37.500 (54.676)\n",
      "EPOCH: 18 train Results: Prec@1 54.676 Loss: 1.2638\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1719 (1.1719)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1470 (1.2810)\tPrec@1 50.000 (53.820)\n",
      "EPOCH: 18 val Results: Prec@1 53.820 Loss: 1.2810\n",
      "Best Prec@1: 53.820\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/781]\tTime 0.002 (0.002)\tLoss 1.3009 (1.3009)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [19][156/781]\tTime 0.001 (0.002)\tLoss 1.2707 (1.1920)\tPrec@1 56.250 (57.683)\n",
      "Epoch: [19][312/781]\tTime 0.008 (0.002)\tLoss 1.3730 (1.2284)\tPrec@1 57.812 (56.330)\n",
      "Epoch: [19][468/781]\tTime 0.003 (0.002)\tLoss 1.2859 (1.2406)\tPrec@1 56.250 (55.860)\n",
      "Epoch: [19][624/781]\tTime 0.001 (0.002)\tLoss 1.0242 (1.2533)\tPrec@1 60.938 (55.388)\n",
      "Epoch: [19][780/781]\tTime 0.002 (0.002)\tLoss 1.3042 (1.2572)\tPrec@1 51.562 (55.246)\n",
      "Epoch: [19][781/781]\tTime 0.002 (0.002)\tLoss 1.8482 (1.2574)\tPrec@1 43.750 (55.242)\n",
      "EPOCH: 19 train Results: Prec@1 55.242 Loss: 1.2574\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2095 (1.2095)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.6627 (1.2699)\tPrec@1 25.000 (54.190)\n",
      "EPOCH: 19 val Results: Prec@1 54.190 Loss: 1.2699\n",
      "Best Prec@1: 54.190\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/781]\tTime 0.002 (0.002)\tLoss 1.3361 (1.3361)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [20][156/781]\tTime 0.003 (0.002)\tLoss 1.0460 (1.2256)\tPrec@1 67.188 (56.529)\n",
      "Epoch: [20][312/781]\tTime 0.002 (0.002)\tLoss 1.0853 (1.2343)\tPrec@1 56.250 (56.030)\n",
      "Epoch: [20][468/781]\tTime 0.001 (0.002)\tLoss 1.2918 (1.2515)\tPrec@1 54.688 (55.374)\n",
      "Epoch: [20][624/781]\tTime 0.001 (0.002)\tLoss 1.2765 (1.2557)\tPrec@1 59.375 (55.215)\n",
      "Epoch: [20][780/781]\tTime 0.001 (0.002)\tLoss 1.2832 (1.2601)\tPrec@1 54.688 (55.030)\n",
      "Epoch: [20][781/781]\tTime 0.002 (0.002)\tLoss 1.2984 (1.2601)\tPrec@1 50.000 (55.028)\n",
      "EPOCH: 20 train Results: Prec@1 55.028 Loss: 1.2601\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2607 (1.2607)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2093 (1.2671)\tPrec@1 56.250 (54.740)\n",
      "EPOCH: 20 val Results: Prec@1 54.740 Loss: 1.2671\n",
      "Best Prec@1: 54.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/781]\tTime 0.002 (0.002)\tLoss 1.2383 (1.2383)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [21][156/781]\tTime 0.001 (0.002)\tLoss 1.3546 (1.1951)\tPrec@1 53.125 (57.295)\n",
      "Epoch: [21][312/781]\tTime 0.004 (0.002)\tLoss 1.1751 (1.2246)\tPrec@1 50.000 (56.390)\n",
      "Epoch: [21][468/781]\tTime 0.002 (0.002)\tLoss 1.1484 (1.2361)\tPrec@1 56.250 (55.970)\n",
      "Epoch: [21][624/781]\tTime 0.001 (0.002)\tLoss 1.2916 (1.2484)\tPrec@1 54.688 (55.515)\n",
      "Epoch: [21][780/781]\tTime 0.003 (0.003)\tLoss 1.2835 (1.2544)\tPrec@1 53.125 (55.238)\n",
      "Epoch: [21][781/781]\tTime 0.003 (0.003)\tLoss 1.0255 (1.2544)\tPrec@1 68.750 (55.242)\n",
      "EPOCH: 21 train Results: Prec@1 55.242 Loss: 1.2544\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2081 (1.2081)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.005 (0.001)\tLoss 1.0235 (1.2674)\tPrec@1 50.000 (54.420)\n",
      "EPOCH: 21 val Results: Prec@1 54.420 Loss: 1.2674\n",
      "Best Prec@1: 54.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/781]\tTime 0.006 (0.006)\tLoss 1.2483 (1.2483)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [22][156/781]\tTime 0.003 (0.003)\tLoss 1.0413 (1.2095)\tPrec@1 68.750 (56.857)\n",
      "Epoch: [22][312/781]\tTime 0.001 (0.003)\tLoss 1.2335 (1.2297)\tPrec@1 54.688 (56.015)\n",
      "Epoch: [22][468/781]\tTime 0.002 (0.003)\tLoss 1.2941 (1.2393)\tPrec@1 48.438 (55.677)\n",
      "Epoch: [22][624/781]\tTime 0.001 (0.003)\tLoss 1.2306 (1.2486)\tPrec@1 57.812 (55.373)\n",
      "Epoch: [22][780/781]\tTime 0.003 (0.003)\tLoss 1.1722 (1.2580)\tPrec@1 62.500 (54.980)\n",
      "Epoch: [22][781/781]\tTime 0.002 (0.003)\tLoss 1.0892 (1.2580)\tPrec@1 56.250 (54.980)\n",
      "EPOCH: 22 train Results: Prec@1 54.980 Loss: 1.2580\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1965 (1.1965)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1493 (1.2670)\tPrec@1 50.000 (54.580)\n",
      "EPOCH: 22 val Results: Prec@1 54.580 Loss: 1.2670\n",
      "Best Prec@1: 54.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/781]\tTime 0.004 (0.004)\tLoss 1.0257 (1.0257)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [23][156/781]\tTime 0.001 (0.003)\tLoss 1.2786 (1.1939)\tPrec@1 48.438 (57.325)\n",
      "Epoch: [23][312/781]\tTime 0.001 (0.002)\tLoss 1.2207 (1.2268)\tPrec@1 59.375 (56.574)\n",
      "Epoch: [23][468/781]\tTime 0.001 (0.002)\tLoss 1.3418 (1.2360)\tPrec@1 46.875 (56.030)\n",
      "Epoch: [23][624/781]\tTime 0.001 (0.002)\tLoss 1.2079 (1.2482)\tPrec@1 57.812 (55.587)\n",
      "Epoch: [23][780/781]\tTime 0.001 (0.002)\tLoss 1.3606 (1.2559)\tPrec@1 51.562 (55.238)\n",
      "Epoch: [23][781/781]\tTime 0.003 (0.002)\tLoss 1.2836 (1.2559)\tPrec@1 50.000 (55.236)\n",
      "EPOCH: 23 train Results: Prec@1 55.236 Loss: 1.2559\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2623 (1.2623)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2319 (1.2835)\tPrec@1 43.750 (54.010)\n",
      "EPOCH: 23 val Results: Prec@1 54.010 Loss: 1.2835\n",
      "Best Prec@1: 54.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/781]\tTime 0.004 (0.004)\tLoss 1.1537 (1.1537)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [24][156/781]\tTime 0.001 (0.002)\tLoss 1.2913 (1.1840)\tPrec@1 53.125 (57.862)\n",
      "Epoch: [24][312/781]\tTime 0.002 (0.002)\tLoss 1.5328 (1.2225)\tPrec@1 50.000 (56.465)\n",
      "Epoch: [24][468/781]\tTime 0.001 (0.002)\tLoss 1.2885 (1.2382)\tPrec@1 46.875 (55.997)\n",
      "Epoch: [24][624/781]\tTime 0.005 (0.002)\tLoss 1.3678 (1.2447)\tPrec@1 53.125 (55.720)\n",
      "Epoch: [24][780/781]\tTime 0.003 (0.002)\tLoss 1.5003 (1.2510)\tPrec@1 46.875 (55.258)\n",
      "Epoch: [24][781/781]\tTime 0.002 (0.002)\tLoss 1.7552 (1.2512)\tPrec@1 31.250 (55.250)\n",
      "EPOCH: 24 train Results: Prec@1 55.250 Loss: 1.2512\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2661 (1.2661)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3319 (1.2898)\tPrec@1 43.750 (53.870)\n",
      "EPOCH: 24 val Results: Prec@1 53.870 Loss: 1.2898\n",
      "Best Prec@1: 54.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/781]\tTime 0.002 (0.002)\tLoss 1.1740 (1.1740)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [25][156/781]\tTime 0.002 (0.002)\tLoss 1.2245 (1.2030)\tPrec@1 56.250 (57.235)\n",
      "Epoch: [25][312/781]\tTime 0.001 (0.002)\tLoss 1.2287 (1.2198)\tPrec@1 59.375 (56.490)\n",
      "Epoch: [25][468/781]\tTime 0.001 (0.002)\tLoss 1.3473 (1.2328)\tPrec@1 50.000 (56.103)\n",
      "Epoch: [25][624/781]\tTime 0.001 (0.002)\tLoss 1.3491 (1.2432)\tPrec@1 48.438 (55.612)\n",
      "Epoch: [25][780/781]\tTime 0.002 (0.002)\tLoss 1.2560 (1.2465)\tPrec@1 53.125 (55.570)\n",
      "Epoch: [25][781/781]\tTime 0.004 (0.002)\tLoss 2.1114 (1.2468)\tPrec@1 37.500 (55.564)\n",
      "EPOCH: 25 train Results: Prec@1 55.564 Loss: 1.2468\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1313 (1.1313)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2481 (1.2823)\tPrec@1 37.500 (54.190)\n",
      "EPOCH: 25 val Results: Prec@1 54.190 Loss: 1.2823\n",
      "Best Prec@1: 54.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/781]\tTime 0.002 (0.002)\tLoss 1.2345 (1.2345)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [26][156/781]\tTime 0.001 (0.003)\tLoss 1.3192 (1.1908)\tPrec@1 53.125 (57.444)\n",
      "Epoch: [26][312/781]\tTime 0.001 (0.002)\tLoss 1.2091 (1.2144)\tPrec@1 53.125 (56.814)\n",
      "Epoch: [26][468/781]\tTime 0.001 (0.002)\tLoss 1.2176 (1.2278)\tPrec@1 59.375 (56.127)\n",
      "Epoch: [26][624/781]\tTime 0.001 (0.002)\tLoss 1.1354 (1.2415)\tPrec@1 54.688 (55.715)\n",
      "Epoch: [26][780/781]\tTime 0.001 (0.002)\tLoss 1.2249 (1.2483)\tPrec@1 56.250 (55.492)\n",
      "Epoch: [26][781/781]\tTime 0.008 (0.002)\tLoss 2.0221 (1.2485)\tPrec@1 25.000 (55.482)\n",
      "EPOCH: 26 train Results: Prec@1 55.482 Loss: 1.2485\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2658 (1.2658)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2603 (1.2760)\tPrec@1 62.500 (54.170)\n",
      "EPOCH: 26 val Results: Prec@1 54.170 Loss: 1.2760\n",
      "Best Prec@1: 54.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/781]\tTime 0.002 (0.002)\tLoss 1.3121 (1.3121)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [27][156/781]\tTime 0.002 (0.002)\tLoss 1.1692 (1.1864)\tPrec@1 60.938 (57.912)\n",
      "Epoch: [27][312/781]\tTime 0.002 (0.002)\tLoss 1.0680 (1.2128)\tPrec@1 60.938 (56.879)\n",
      "Epoch: [27][468/781]\tTime 0.001 (0.002)\tLoss 1.1695 (1.2252)\tPrec@1 59.375 (56.300)\n",
      "Epoch: [27][624/781]\tTime 0.001 (0.002)\tLoss 1.0792 (1.2378)\tPrec@1 57.812 (55.755)\n",
      "Epoch: [27][780/781]\tTime 0.001 (0.002)\tLoss 1.5843 (1.2480)\tPrec@1 42.188 (55.524)\n",
      "Epoch: [27][781/781]\tTime 0.002 (0.002)\tLoss 1.6961 (1.2481)\tPrec@1 31.250 (55.516)\n",
      "EPOCH: 27 train Results: Prec@1 55.516 Loss: 1.2481\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1372 (1.1372)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1364 (1.2837)\tPrec@1 56.250 (54.180)\n",
      "EPOCH: 27 val Results: Prec@1 54.180 Loss: 1.2837\n",
      "Best Prec@1: 54.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/781]\tTime 0.003 (0.003)\tLoss 1.0682 (1.0682)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [28][156/781]\tTime 0.011 (0.002)\tLoss 1.0411 (1.1931)\tPrec@1 67.188 (57.305)\n",
      "Epoch: [28][312/781]\tTime 0.003 (0.002)\tLoss 1.4832 (1.2137)\tPrec@1 42.188 (56.470)\n",
      "Epoch: [28][468/781]\tTime 0.001 (0.002)\tLoss 1.1797 (1.2291)\tPrec@1 59.375 (56.183)\n",
      "Epoch: [28][624/781]\tTime 0.001 (0.002)\tLoss 1.0879 (1.2375)\tPrec@1 62.500 (55.943)\n",
      "Epoch: [28][780/781]\tTime 0.001 (0.002)\tLoss 1.4677 (1.2462)\tPrec@1 48.438 (55.600)\n",
      "Epoch: [28][781/781]\tTime 0.002 (0.002)\tLoss 1.6158 (1.2463)\tPrec@1 43.750 (55.596)\n",
      "EPOCH: 28 train Results: Prec@1 55.596 Loss: 1.2463\n",
      "Test: [0/156]\tTime 0.011 (0.011)\tLoss 1.2212 (1.2212)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.001 (0.001)\tLoss 1.1857 (1.2718)\tPrec@1 56.250 (54.880)\n",
      "EPOCH: 28 val Results: Prec@1 54.880 Loss: 1.2718\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/781]\tTime 0.002 (0.002)\tLoss 1.1202 (1.1202)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [29][156/781]\tTime 0.002 (0.002)\tLoss 1.1905 (1.1905)\tPrec@1 62.500 (57.404)\n",
      "Epoch: [29][312/781]\tTime 0.002 (0.002)\tLoss 1.0834 (1.2129)\tPrec@1 62.500 (56.699)\n",
      "Epoch: [29][468/781]\tTime 0.002 (0.002)\tLoss 1.0789 (1.2264)\tPrec@1 62.500 (56.210)\n",
      "Epoch: [29][624/781]\tTime 0.001 (0.002)\tLoss 1.2073 (1.2397)\tPrec@1 64.062 (55.840)\n",
      "Epoch: [29][780/781]\tTime 0.002 (0.002)\tLoss 1.3505 (1.2475)\tPrec@1 48.438 (55.552)\n",
      "Epoch: [29][781/781]\tTime 0.002 (0.002)\tLoss 1.4631 (1.2476)\tPrec@1 56.250 (55.552)\n",
      "EPOCH: 29 train Results: Prec@1 55.552 Loss: 1.2476\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.1432 (1.1432)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.0081 (1.2768)\tPrec@1 68.750 (54.000)\n",
      "EPOCH: 29 val Results: Prec@1 54.000 Loss: 1.2768\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/781]\tTime 0.002 (0.002)\tLoss 1.2921 (1.2921)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [30][156/781]\tTime 0.002 (0.002)\tLoss 1.2213 (1.1939)\tPrec@1 56.250 (57.395)\n",
      "Epoch: [30][312/781]\tTime 0.002 (0.002)\tLoss 1.2110 (1.2151)\tPrec@1 51.562 (56.729)\n",
      "Epoch: [30][468/781]\tTime 0.003 (0.002)\tLoss 1.2591 (1.2246)\tPrec@1 53.125 (56.537)\n",
      "Epoch: [30][624/781]\tTime 0.002 (0.002)\tLoss 1.2983 (1.2355)\tPrec@1 48.438 (56.005)\n",
      "Epoch: [30][780/781]\tTime 0.001 (0.002)\tLoss 1.4015 (1.2442)\tPrec@1 48.438 (55.826)\n",
      "Epoch: [30][781/781]\tTime 0.002 (0.002)\tLoss 1.3401 (1.2442)\tPrec@1 37.500 (55.820)\n",
      "EPOCH: 30 train Results: Prec@1 55.820 Loss: 1.2442\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1735 (1.1735)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1829 (1.2849)\tPrec@1 56.250 (54.080)\n",
      "EPOCH: 30 val Results: Prec@1 54.080 Loss: 1.2849\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/781]\tTime 0.002 (0.002)\tLoss 1.0261 (1.0261)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [31][156/781]\tTime 0.002 (0.002)\tLoss 1.3333 (1.1930)\tPrec@1 48.438 (57.524)\n",
      "Epoch: [31][312/781]\tTime 0.007 (0.002)\tLoss 1.1644 (1.2120)\tPrec@1 60.938 (56.724)\n",
      "Epoch: [31][468/781]\tTime 0.003 (0.002)\tLoss 1.1115 (1.2226)\tPrec@1 65.625 (56.463)\n",
      "Epoch: [31][624/781]\tTime 0.002 (0.002)\tLoss 1.0528 (1.2325)\tPrec@1 62.500 (56.123)\n",
      "Epoch: [31][780/781]\tTime 0.001 (0.002)\tLoss 1.4637 (1.2442)\tPrec@1 48.438 (55.674)\n",
      "Epoch: [31][781/781]\tTime 0.002 (0.002)\tLoss 0.8487 (1.2441)\tPrec@1 75.000 (55.680)\n",
      "EPOCH: 31 train Results: Prec@1 55.680 Loss: 1.2441\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2398 (1.2398)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3362 (1.2963)\tPrec@1 25.000 (53.300)\n",
      "EPOCH: 31 val Results: Prec@1 53.300 Loss: 1.2963\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/781]\tTime 0.003 (0.003)\tLoss 1.2279 (1.2279)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [32][156/781]\tTime 0.001 (0.002)\tLoss 1.1934 (1.1925)\tPrec@1 60.938 (57.414)\n",
      "Epoch: [32][312/781]\tTime 0.002 (0.002)\tLoss 1.0259 (1.2083)\tPrec@1 57.812 (56.769)\n",
      "Epoch: [32][468/781]\tTime 0.006 (0.002)\tLoss 1.5328 (1.2223)\tPrec@1 42.188 (56.150)\n",
      "Epoch: [32][624/781]\tTime 0.002 (0.002)\tLoss 1.2262 (1.2299)\tPrec@1 56.250 (56.028)\n",
      "Epoch: [32][780/781]\tTime 0.001 (0.002)\tLoss 1.1243 (1.2401)\tPrec@1 65.625 (55.742)\n",
      "Epoch: [32][781/781]\tTime 0.002 (0.002)\tLoss 1.1223 (1.2400)\tPrec@1 56.250 (55.742)\n",
      "EPOCH: 32 train Results: Prec@1 55.742 Loss: 1.2400\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2569 (1.2569)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3561 (1.2841)\tPrec@1 31.250 (53.740)\n",
      "EPOCH: 32 val Results: Prec@1 53.740 Loss: 1.2841\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/781]\tTime 0.013 (0.013)\tLoss 1.4055 (1.4055)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [33][156/781]\tTime 0.001 (0.002)\tLoss 1.2700 (1.2040)\tPrec@1 56.250 (56.688)\n",
      "Epoch: [33][312/781]\tTime 0.003 (0.002)\tLoss 1.3814 (1.2164)\tPrec@1 50.000 (56.360)\n",
      "Epoch: [33][468/781]\tTime 0.001 (0.002)\tLoss 1.2652 (1.2238)\tPrec@1 56.250 (56.060)\n",
      "Epoch: [33][624/781]\tTime 0.002 (0.002)\tLoss 1.1651 (1.2365)\tPrec@1 56.250 (55.722)\n",
      "Epoch: [33][780/781]\tTime 0.002 (0.002)\tLoss 1.2711 (1.2438)\tPrec@1 54.688 (55.508)\n",
      "Epoch: [33][781/781]\tTime 0.002 (0.002)\tLoss 1.2184 (1.2438)\tPrec@1 56.250 (55.508)\n",
      "EPOCH: 33 train Results: Prec@1 55.508 Loss: 1.2438\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.1334 (1.1334)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3049 (1.2771)\tPrec@1 43.750 (54.550)\n",
      "EPOCH: 33 val Results: Prec@1 54.550 Loss: 1.2771\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/781]\tTime 0.005 (0.005)\tLoss 1.1214 (1.1214)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [34][156/781]\tTime 0.001 (0.002)\tLoss 1.2802 (1.1886)\tPrec@1 53.125 (58.330)\n",
      "Epoch: [34][312/781]\tTime 0.001 (0.002)\tLoss 1.1971 (1.2108)\tPrec@1 60.938 (57.278)\n",
      "Epoch: [34][468/781]\tTime 0.002 (0.002)\tLoss 1.3312 (1.2185)\tPrec@1 53.125 (56.926)\n",
      "Epoch: [34][624/781]\tTime 0.002 (0.002)\tLoss 1.3522 (1.2322)\tPrec@1 57.812 (56.233)\n",
      "Epoch: [34][780/781]\tTime 0.001 (0.002)\tLoss 1.0476 (1.2400)\tPrec@1 62.500 (55.942)\n",
      "Epoch: [34][781/781]\tTime 0.003 (0.002)\tLoss 1.7907 (1.2401)\tPrec@1 37.500 (55.936)\n",
      "EPOCH: 34 train Results: Prec@1 55.936 Loss: 1.2401\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1798 (1.1798)\tPrec@1 65.625 (65.625)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2663 (1.2799)\tPrec@1 43.750 (54.120)\n",
      "EPOCH: 34 val Results: Prec@1 54.120 Loss: 1.2799\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/781]\tTime 0.002 (0.002)\tLoss 1.2007 (1.2007)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [35][156/781]\tTime 0.002 (0.002)\tLoss 1.1710 (1.1914)\tPrec@1 56.250 (58.021)\n",
      "Epoch: [35][312/781]\tTime 0.005 (0.002)\tLoss 1.5493 (1.2056)\tPrec@1 51.562 (56.909)\n",
      "Epoch: [35][468/781]\tTime 0.001 (0.002)\tLoss 1.3150 (1.2258)\tPrec@1 54.688 (56.320)\n",
      "Epoch: [35][624/781]\tTime 0.001 (0.002)\tLoss 1.3633 (1.2359)\tPrec@1 46.875 (55.910)\n",
      "Epoch: [35][780/781]\tTime 0.001 (0.002)\tLoss 1.1315 (1.2430)\tPrec@1 60.938 (55.594)\n",
      "Epoch: [35][781/781]\tTime 0.001 (0.002)\tLoss 1.4978 (1.2431)\tPrec@1 50.000 (55.592)\n",
      "EPOCH: 35 train Results: Prec@1 55.592 Loss: 1.2431\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1990 (1.1990)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1811 (1.2902)\tPrec@1 43.750 (54.390)\n",
      "EPOCH: 35 val Results: Prec@1 54.390 Loss: 1.2902\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/781]\tTime 0.003 (0.003)\tLoss 1.3074 (1.3074)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [36][156/781]\tTime 0.002 (0.002)\tLoss 1.3983 (1.1838)\tPrec@1 46.875 (57.594)\n",
      "Epoch: [36][312/781]\tTime 0.002 (0.002)\tLoss 1.3706 (1.2060)\tPrec@1 48.438 (56.889)\n",
      "Epoch: [36][468/781]\tTime 0.001 (0.002)\tLoss 1.2764 (1.2179)\tPrec@1 46.875 (56.453)\n",
      "Epoch: [36][624/781]\tTime 0.003 (0.002)\tLoss 1.2406 (1.2294)\tPrec@1 53.125 (56.068)\n",
      "Epoch: [36][780/781]\tTime 0.013 (0.002)\tLoss 1.4694 (1.2390)\tPrec@1 42.188 (55.760)\n",
      "Epoch: [36][781/781]\tTime 0.002 (0.002)\tLoss 1.0345 (1.2389)\tPrec@1 62.500 (55.762)\n",
      "EPOCH: 36 train Results: Prec@1 55.762 Loss: 1.2389\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2224 (1.2760)\tPrec@1 50.000 (54.700)\n",
      "EPOCH: 36 val Results: Prec@1 54.700 Loss: 1.2760\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/781]\tTime 0.005 (0.005)\tLoss 1.0045 (1.0045)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [37][156/781]\tTime 0.001 (0.002)\tLoss 1.0739 (1.1710)\tPrec@1 62.500 (58.738)\n",
      "Epoch: [37][312/781]\tTime 0.001 (0.002)\tLoss 1.3978 (1.1981)\tPrec@1 48.438 (57.608)\n",
      "Epoch: [37][468/781]\tTime 0.001 (0.002)\tLoss 1.4830 (1.2181)\tPrec@1 48.438 (56.700)\n",
      "Epoch: [37][624/781]\tTime 0.002 (0.002)\tLoss 1.2107 (1.2307)\tPrec@1 59.375 (56.125)\n",
      "Epoch: [37][780/781]\tTime 0.001 (0.002)\tLoss 1.2412 (1.2349)\tPrec@1 59.375 (56.034)\n",
      "Epoch: [37][781/781]\tTime 0.002 (0.002)\tLoss 1.2442 (1.2349)\tPrec@1 43.750 (56.030)\n",
      "EPOCH: 37 train Results: Prec@1 56.030 Loss: 1.2349\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2818 (1.2818)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3332 (1.2839)\tPrec@1 50.000 (54.010)\n",
      "EPOCH: 37 val Results: Prec@1 54.010 Loss: 1.2839\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/781]\tTime 0.002 (0.002)\tLoss 1.2761 (1.2761)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [38][156/781]\tTime 0.001 (0.002)\tLoss 1.1452 (1.1944)\tPrec@1 54.688 (57.842)\n",
      "Epoch: [38][312/781]\tTime 0.001 (0.002)\tLoss 1.0714 (1.2122)\tPrec@1 70.312 (56.979)\n",
      "Epoch: [38][468/781]\tTime 0.001 (0.002)\tLoss 1.2361 (1.2239)\tPrec@1 54.688 (56.680)\n",
      "Epoch: [38][624/781]\tTime 0.002 (0.002)\tLoss 1.3909 (1.2299)\tPrec@1 53.125 (56.245)\n",
      "Epoch: [38][780/781]\tTime 0.001 (0.002)\tLoss 1.2581 (1.2398)\tPrec@1 50.000 (55.786)\n",
      "Epoch: [38][781/781]\tTime 0.003 (0.002)\tLoss 1.2846 (1.2398)\tPrec@1 31.250 (55.778)\n",
      "EPOCH: 38 train Results: Prec@1 55.778 Loss: 1.2398\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2792 (1.2792)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1122 (1.2779)\tPrec@1 56.250 (53.990)\n",
      "EPOCH: 38 val Results: Prec@1 53.990 Loss: 1.2779\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/781]\tTime 0.005 (0.005)\tLoss 1.0428 (1.0428)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [39][156/781]\tTime 0.001 (0.002)\tLoss 1.3542 (1.1682)\tPrec@1 50.000 (58.698)\n",
      "Epoch: [39][312/781]\tTime 0.003 (0.002)\tLoss 1.2609 (1.2032)\tPrec@1 57.812 (57.258)\n",
      "Epoch: [39][468/781]\tTime 0.001 (0.002)\tLoss 1.3674 (1.2221)\tPrec@1 54.688 (56.690)\n",
      "Epoch: [39][624/781]\tTime 0.002 (0.002)\tLoss 1.8486 (1.2272)\tPrec@1 37.500 (56.475)\n",
      "Epoch: [39][780/781]\tTime 0.001 (0.002)\tLoss 1.4037 (1.2394)\tPrec@1 42.188 (56.002)\n",
      "Epoch: [39][781/781]\tTime 0.007 (0.002)\tLoss 1.4648 (1.2394)\tPrec@1 31.250 (55.994)\n",
      "EPOCH: 39 train Results: Prec@1 55.994 Loss: 1.2394\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3115 (1.3115)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3441 (1.2848)\tPrec@1 50.000 (53.730)\n",
      "EPOCH: 39 val Results: Prec@1 53.730 Loss: 1.2848\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/781]\tTime 0.002 (0.002)\tLoss 1.2927 (1.2927)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [40][156/781]\tTime 0.002 (0.002)\tLoss 1.2995 (1.1832)\tPrec@1 54.688 (57.663)\n",
      "Epoch: [40][312/781]\tTime 0.003 (0.002)\tLoss 1.2704 (1.1967)\tPrec@1 54.688 (57.208)\n",
      "Epoch: [40][468/781]\tTime 0.001 (0.002)\tLoss 1.1816 (1.2132)\tPrec@1 53.125 (56.693)\n",
      "Epoch: [40][624/781]\tTime 0.004 (0.002)\tLoss 1.4812 (1.2274)\tPrec@1 48.438 (56.095)\n",
      "Epoch: [40][780/781]\tTime 0.001 (0.002)\tLoss 1.1614 (1.2385)\tPrec@1 59.375 (55.828)\n",
      "Epoch: [40][781/781]\tTime 0.002 (0.002)\tLoss 1.7076 (1.2387)\tPrec@1 43.750 (55.824)\n",
      "EPOCH: 40 train Results: Prec@1 55.824 Loss: 1.2387\n",
      "Test: [0/156]\tTime 0.006 (0.006)\tLoss 1.3598 (1.3598)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2831 (1.2863)\tPrec@1 43.750 (53.570)\n",
      "EPOCH: 40 val Results: Prec@1 53.570 Loss: 1.2863\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/781]\tTime 0.004 (0.004)\tLoss 1.3477 (1.3477)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [41][156/781]\tTime 0.041 (0.017)\tLoss 1.2211 (1.1732)\tPrec@1 64.062 (58.430)\n",
      "Epoch: [41][312/781]\tTime 0.016 (0.018)\tLoss 1.3528 (1.2038)\tPrec@1 53.125 (57.343)\n",
      "Epoch: [41][468/781]\tTime 0.027 (0.019)\tLoss 1.2460 (1.2177)\tPrec@1 60.938 (56.920)\n",
      "Epoch: [41][624/781]\tTime 0.004 (0.015)\tLoss 1.3037 (1.2303)\tPrec@1 64.062 (56.395)\n",
      "Epoch: [41][780/781]\tTime 0.001 (0.013)\tLoss 1.2159 (1.2378)\tPrec@1 62.500 (56.058)\n",
      "Epoch: [41][781/781]\tTime 0.002 (0.013)\tLoss 1.7066 (1.2379)\tPrec@1 43.750 (56.054)\n",
      "EPOCH: 41 train Results: Prec@1 56.054 Loss: 1.2379\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1185 (1.1185)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2542 (1.2755)\tPrec@1 50.000 (54.000)\n",
      "EPOCH: 41 val Results: Prec@1 54.000 Loss: 1.2755\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/781]\tTime 0.002 (0.002)\tLoss 1.5384 (1.5384)\tPrec@1 42.188 (42.188)\n",
      "Epoch: [42][156/781]\tTime 0.002 (0.002)\tLoss 1.1837 (1.1935)\tPrec@1 60.938 (57.962)\n",
      "Epoch: [42][312/781]\tTime 0.001 (0.002)\tLoss 1.1859 (1.2053)\tPrec@1 60.938 (57.203)\n",
      "Epoch: [42][468/781]\tTime 0.001 (0.002)\tLoss 1.3039 (1.2239)\tPrec@1 54.688 (56.420)\n",
      "Epoch: [42][624/781]\tTime 0.002 (0.002)\tLoss 1.3910 (1.2339)\tPrec@1 43.750 (56.120)\n",
      "Epoch: [42][780/781]\tTime 0.001 (0.002)\tLoss 1.2118 (1.2406)\tPrec@1 57.812 (55.814)\n",
      "Epoch: [42][781/781]\tTime 0.002 (0.002)\tLoss 0.9945 (1.2405)\tPrec@1 68.750 (55.818)\n",
      "EPOCH: 42 train Results: Prec@1 55.818 Loss: 1.2405\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.0994 (1.0994)\tPrec@1 65.625 (65.625)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3665 (1.2874)\tPrec@1 37.500 (53.870)\n",
      "EPOCH: 42 val Results: Prec@1 53.870 Loss: 1.2874\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/781]\tTime 0.002 (0.002)\tLoss 1.1306 (1.1306)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [43][156/781]\tTime 0.001 (0.002)\tLoss 1.2753 (1.1825)\tPrec@1 57.812 (57.822)\n",
      "Epoch: [43][312/781]\tTime 0.002 (0.002)\tLoss 1.2382 (1.2051)\tPrec@1 51.562 (57.034)\n",
      "Epoch: [43][468/781]\tTime 0.003 (0.002)\tLoss 1.2482 (1.2157)\tPrec@1 56.250 (56.586)\n",
      "Epoch: [43][624/781]\tTime 0.001 (0.002)\tLoss 1.3722 (1.2345)\tPrec@1 56.250 (55.990)\n",
      "Epoch: [43][780/781]\tTime 0.002 (0.002)\tLoss 1.1361 (1.2389)\tPrec@1 62.500 (55.786)\n",
      "Epoch: [43][781/781]\tTime 0.002 (0.002)\tLoss 1.1189 (1.2388)\tPrec@1 68.750 (55.790)\n",
      "EPOCH: 43 train Results: Prec@1 55.790 Loss: 1.2388\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3415 (1.3415)\tPrec@1 50.000 (50.000)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1966 (1.2818)\tPrec@1 43.750 (53.770)\n",
      "EPOCH: 43 val Results: Prec@1 53.770 Loss: 1.2818\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/781]\tTime 0.002 (0.002)\tLoss 1.1547 (1.1547)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [44][156/781]\tTime 0.002 (0.002)\tLoss 1.0633 (1.1873)\tPrec@1 67.188 (58.191)\n",
      "Epoch: [44][312/781]\tTime 0.001 (0.002)\tLoss 1.2375 (1.2006)\tPrec@1 60.938 (57.358)\n",
      "Epoch: [44][468/781]\tTime 0.002 (0.002)\tLoss 1.2403 (1.2183)\tPrec@1 48.438 (56.600)\n",
      "Epoch: [44][624/781]\tTime 0.001 (0.002)\tLoss 1.2010 (1.2284)\tPrec@1 57.812 (56.167)\n",
      "Epoch: [44][780/781]\tTime 0.001 (0.002)\tLoss 1.1827 (1.2368)\tPrec@1 51.562 (55.830)\n",
      "Epoch: [44][781/781]\tTime 0.003 (0.002)\tLoss 1.5370 (1.2369)\tPrec@1 50.000 (55.828)\n",
      "EPOCH: 44 train Results: Prec@1 55.828 Loss: 1.2369\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3099 (1.3099)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4333 (1.2776)\tPrec@1 37.500 (53.970)\n",
      "EPOCH: 44 val Results: Prec@1 53.970 Loss: 1.2776\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/781]\tTime 0.002 (0.002)\tLoss 1.4896 (1.4896)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [45][156/781]\tTime 0.001 (0.002)\tLoss 1.3238 (1.1867)\tPrec@1 53.125 (57.852)\n",
      "Epoch: [45][312/781]\tTime 0.002 (0.002)\tLoss 1.4663 (1.2020)\tPrec@1 54.688 (57.343)\n",
      "Epoch: [45][468/781]\tTime 0.001 (0.002)\tLoss 1.2912 (1.2171)\tPrec@1 50.000 (56.523)\n",
      "Epoch: [45][624/781]\tTime 0.002 (0.002)\tLoss 1.4492 (1.2291)\tPrec@1 46.875 (56.102)\n",
      "Epoch: [45][780/781]\tTime 0.001 (0.002)\tLoss 1.4387 (1.2377)\tPrec@1 57.812 (55.806)\n",
      "Epoch: [45][781/781]\tTime 0.003 (0.002)\tLoss 1.3410 (1.2378)\tPrec@1 50.000 (55.804)\n",
      "EPOCH: 45 train Results: Prec@1 55.804 Loss: 1.2378\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3062 (1.3062)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.4375 (1.2807)\tPrec@1 50.000 (53.870)\n",
      "EPOCH: 45 val Results: Prec@1 53.870 Loss: 1.2807\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/781]\tTime 0.002 (0.002)\tLoss 1.0743 (1.0743)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [46][156/781]\tTime 0.001 (0.002)\tLoss 1.3452 (1.1809)\tPrec@1 43.750 (58.300)\n",
      "Epoch: [46][312/781]\tTime 0.001 (0.002)\tLoss 1.2425 (1.1937)\tPrec@1 56.250 (57.298)\n",
      "Epoch: [46][468/781]\tTime 0.005 (0.002)\tLoss 1.3491 (1.2135)\tPrec@1 53.125 (56.566)\n",
      "Epoch: [46][624/781]\tTime 0.001 (0.002)\tLoss 1.4364 (1.2261)\tPrec@1 56.250 (56.145)\n",
      "Epoch: [46][780/781]\tTime 0.003 (0.002)\tLoss 1.2258 (1.2348)\tPrec@1 46.875 (55.884)\n",
      "Epoch: [46][781/781]\tTime 0.002 (0.002)\tLoss 1.3938 (1.2349)\tPrec@1 50.000 (55.882)\n",
      "EPOCH: 46 train Results: Prec@1 55.882 Loss: 1.2349\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2591 (1.2591)\tPrec@1 48.438 (48.438)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4052 (1.2946)\tPrec@1 50.000 (53.120)\n",
      "EPOCH: 46 val Results: Prec@1 53.120 Loss: 1.2946\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/781]\tTime 0.002 (0.002)\tLoss 1.4846 (1.4846)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [47][156/781]\tTime 0.001 (0.002)\tLoss 1.4207 (1.1861)\tPrec@1 51.562 (58.002)\n",
      "Epoch: [47][312/781]\tTime 0.003 (0.002)\tLoss 1.1316 (1.2052)\tPrec@1 60.938 (56.944)\n",
      "Epoch: [47][468/781]\tTime 0.001 (0.002)\tLoss 1.0177 (1.2213)\tPrec@1 59.375 (56.387)\n",
      "Epoch: [47][624/781]\tTime 0.002 (0.002)\tLoss 1.2430 (1.2296)\tPrec@1 56.250 (56.090)\n",
      "Epoch: [47][780/781]\tTime 0.003 (0.002)\tLoss 1.2749 (1.2387)\tPrec@1 53.125 (55.702)\n",
      "Epoch: [47][781/781]\tTime 0.002 (0.002)\tLoss 1.6021 (1.2388)\tPrec@1 50.000 (55.700)\n",
      "EPOCH: 47 train Results: Prec@1 55.700 Loss: 1.2388\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1001 (1.1001)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1535 (1.2783)\tPrec@1 68.750 (54.200)\n",
      "EPOCH: 47 val Results: Prec@1 54.200 Loss: 1.2783\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/781]\tTime 0.003 (0.003)\tLoss 1.0945 (1.0945)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [48][156/781]\tTime 0.001 (0.002)\tLoss 1.1796 (1.1891)\tPrec@1 54.688 (58.071)\n",
      "Epoch: [48][312/781]\tTime 0.001 (0.002)\tLoss 1.2723 (1.2113)\tPrec@1 57.812 (57.104)\n",
      "Epoch: [48][468/781]\tTime 0.002 (0.002)\tLoss 1.2405 (1.2181)\tPrec@1 59.375 (56.873)\n",
      "Epoch: [48][624/781]\tTime 0.002 (0.002)\tLoss 1.3091 (1.2260)\tPrec@1 59.375 (56.490)\n",
      "Epoch: [48][780/781]\tTime 0.006 (0.002)\tLoss 1.3089 (1.2360)\tPrec@1 56.250 (56.080)\n",
      "Epoch: [48][781/781]\tTime 0.003 (0.002)\tLoss 1.6864 (1.2362)\tPrec@1 50.000 (56.078)\n",
      "EPOCH: 48 train Results: Prec@1 56.078 Loss: 1.2362\n",
      "Test: [0/156]\tTime 0.008 (0.008)\tLoss 1.3701 (1.3701)\tPrec@1 48.438 (48.438)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5739 (1.3090)\tPrec@1 37.500 (52.900)\n",
      "EPOCH: 48 val Results: Prec@1 52.900 Loss: 1.3090\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/781]\tTime 0.005 (0.005)\tLoss 1.3989 (1.3989)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [49][156/781]\tTime 0.009 (0.002)\tLoss 1.3481 (1.1719)\tPrec@1 53.125 (58.698)\n",
      "Epoch: [49][312/781]\tTime 0.002 (0.002)\tLoss 1.2254 (1.1960)\tPrec@1 62.500 (57.483)\n",
      "Epoch: [49][468/781]\tTime 0.002 (0.002)\tLoss 1.2445 (1.2133)\tPrec@1 53.125 (56.816)\n",
      "Epoch: [49][624/781]\tTime 0.001 (0.002)\tLoss 1.1275 (1.2251)\tPrec@1 60.938 (56.312)\n",
      "Epoch: [49][780/781]\tTime 0.001 (0.002)\tLoss 1.5425 (1.2353)\tPrec@1 43.750 (55.870)\n",
      "Epoch: [49][781/781]\tTime 0.008 (0.002)\tLoss 1.1540 (1.2353)\tPrec@1 56.250 (55.870)\n",
      "EPOCH: 49 train Results: Prec@1 55.870 Loss: 1.2353\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3293 (1.3293)\tPrec@1 50.000 (50.000)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0542 (1.2854)\tPrec@1 50.000 (53.700)\n",
      "EPOCH: 49 val Results: Prec@1 53.700 Loss: 1.2854\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/781]\tTime 0.002 (0.002)\tLoss 1.0376 (1.0376)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [50][156/781]\tTime 0.002 (0.002)\tLoss 1.1833 (1.1749)\tPrec@1 54.688 (58.439)\n",
      "Epoch: [50][312/781]\tTime 0.001 (0.002)\tLoss 1.1248 (1.1968)\tPrec@1 64.062 (57.508)\n",
      "Epoch: [50][468/781]\tTime 0.002 (0.002)\tLoss 1.3351 (1.2121)\tPrec@1 46.875 (57.023)\n",
      "Epoch: [50][624/781]\tTime 0.002 (0.002)\tLoss 1.1038 (1.2201)\tPrec@1 56.250 (56.568)\n",
      "Epoch: [50][780/781]\tTime 0.001 (0.002)\tLoss 1.2109 (1.2317)\tPrec@1 59.375 (56.166)\n",
      "Epoch: [50][781/781]\tTime 0.002 (0.002)\tLoss 1.2830 (1.2317)\tPrec@1 50.000 (56.164)\n",
      "EPOCH: 50 train Results: Prec@1 56.164 Loss: 1.2317\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1632 (1.1632)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.6143 (1.2854)\tPrec@1 37.500 (53.840)\n",
      "EPOCH: 50 val Results: Prec@1 53.840 Loss: 1.2854\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/781]\tTime 0.002 (0.002)\tLoss 1.3301 (1.3301)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [51][156/781]\tTime 0.002 (0.002)\tLoss 1.4326 (1.1815)\tPrec@1 54.688 (57.852)\n",
      "Epoch: [51][312/781]\tTime 0.002 (0.002)\tLoss 1.2457 (1.2051)\tPrec@1 57.812 (56.974)\n",
      "Epoch: [51][468/781]\tTime 0.001 (0.002)\tLoss 1.5486 (1.2169)\tPrec@1 40.625 (56.570)\n",
      "Epoch: [51][624/781]\tTime 0.001 (0.002)\tLoss 1.2627 (1.2250)\tPrec@1 64.062 (56.365)\n",
      "Epoch: [51][780/781]\tTime 0.001 (0.002)\tLoss 1.1321 (1.2357)\tPrec@1 60.938 (56.104)\n",
      "Epoch: [51][781/781]\tTime 0.002 (0.002)\tLoss 1.3463 (1.2358)\tPrec@1 50.000 (56.102)\n",
      "EPOCH: 51 train Results: Prec@1 56.102 Loss: 1.2358\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2177 (1.2177)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2269 (1.2914)\tPrec@1 43.750 (53.880)\n",
      "EPOCH: 51 val Results: Prec@1 53.880 Loss: 1.2914\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/781]\tTime 0.005 (0.005)\tLoss 1.0527 (1.0527)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [52][156/781]\tTime 0.001 (0.002)\tLoss 1.3242 (1.1748)\tPrec@1 56.250 (58.599)\n",
      "Epoch: [52][312/781]\tTime 0.002 (0.002)\tLoss 1.2455 (1.1919)\tPrec@1 50.000 (57.842)\n",
      "Epoch: [52][468/781]\tTime 0.002 (0.002)\tLoss 1.1237 (1.2111)\tPrec@1 59.375 (56.960)\n",
      "Epoch: [52][624/781]\tTime 0.001 (0.002)\tLoss 1.4750 (1.2257)\tPrec@1 43.750 (56.343)\n",
      "Epoch: [52][780/781]\tTime 0.003 (0.002)\tLoss 1.3827 (1.2360)\tPrec@1 56.250 (56.018)\n",
      "Epoch: [52][781/781]\tTime 0.002 (0.002)\tLoss 1.2546 (1.2360)\tPrec@1 43.750 (56.014)\n",
      "EPOCH: 52 train Results: Prec@1 56.014 Loss: 1.2360\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2056 (1.2056)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2308 (1.2918)\tPrec@1 50.000 (53.380)\n",
      "EPOCH: 52 val Results: Prec@1 53.380 Loss: 1.2918\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/781]\tTime 0.002 (0.002)\tLoss 1.1150 (1.1150)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [53][156/781]\tTime 0.002 (0.002)\tLoss 1.2391 (1.1754)\tPrec@1 60.938 (58.091)\n",
      "Epoch: [53][312/781]\tTime 0.002 (0.002)\tLoss 1.3821 (1.1958)\tPrec@1 53.125 (57.348)\n",
      "Epoch: [53][468/781]\tTime 0.001 (0.002)\tLoss 1.2808 (1.2159)\tPrec@1 53.125 (56.626)\n",
      "Epoch: [53][624/781]\tTime 0.001 (0.002)\tLoss 1.4427 (1.2271)\tPrec@1 46.875 (56.265)\n",
      "Epoch: [53][780/781]\tTime 0.001 (0.002)\tLoss 1.2048 (1.2372)\tPrec@1 59.375 (55.884)\n",
      "Epoch: [53][781/781]\tTime 0.002 (0.002)\tLoss 1.3932 (1.2373)\tPrec@1 56.250 (55.884)\n",
      "EPOCH: 53 train Results: Prec@1 55.884 Loss: 1.2373\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3394 (1.3394)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.001 (0.001)\tLoss 1.4196 (1.2851)\tPrec@1 56.250 (53.520)\n",
      "EPOCH: 53 val Results: Prec@1 53.520 Loss: 1.2851\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/781]\tTime 0.002 (0.002)\tLoss 1.2897 (1.2897)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [54][156/781]\tTime 0.002 (0.002)\tLoss 1.2628 (1.1719)\tPrec@1 60.938 (58.430)\n",
      "Epoch: [54][312/781]\tTime 0.001 (0.002)\tLoss 1.1215 (1.1969)\tPrec@1 59.375 (57.443)\n",
      "Epoch: [54][468/781]\tTime 0.001 (0.002)\tLoss 1.2755 (1.2127)\tPrec@1 50.000 (56.773)\n",
      "Epoch: [54][624/781]\tTime 0.001 (0.002)\tLoss 1.5484 (1.2254)\tPrec@1 48.438 (56.307)\n",
      "Epoch: [54][780/781]\tTime 0.007 (0.002)\tLoss 1.3631 (1.2370)\tPrec@1 53.125 (55.970)\n",
      "Epoch: [54][781/781]\tTime 0.002 (0.002)\tLoss 1.1781 (1.2369)\tPrec@1 75.000 (55.976)\n",
      "EPOCH: 54 train Results: Prec@1 55.976 Loss: 1.2369\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1534 (1.1534)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2403 (1.2896)\tPrec@1 50.000 (53.990)\n",
      "EPOCH: 54 val Results: Prec@1 53.990 Loss: 1.2896\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/781]\tTime 0.002 (0.002)\tLoss 1.1454 (1.1454)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [55][156/781]\tTime 0.001 (0.002)\tLoss 1.4205 (1.1740)\tPrec@1 59.375 (58.330)\n",
      "Epoch: [55][312/781]\tTime 0.001 (0.002)\tLoss 1.2818 (1.2010)\tPrec@1 50.000 (57.069)\n",
      "Epoch: [55][468/781]\tTime 0.002 (0.002)\tLoss 1.3204 (1.2125)\tPrec@1 62.500 (56.953)\n",
      "Epoch: [55][624/781]\tTime 0.001 (0.002)\tLoss 1.0517 (1.2202)\tPrec@1 60.938 (56.648)\n",
      "Epoch: [55][780/781]\tTime 0.001 (0.002)\tLoss 1.3048 (1.2296)\tPrec@1 51.562 (56.312)\n",
      "Epoch: [55][781/781]\tTime 0.002 (0.002)\tLoss 1.1605 (1.2295)\tPrec@1 56.250 (56.312)\n",
      "EPOCH: 55 train Results: Prec@1 56.312 Loss: 1.2295\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2302 (1.2302)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.002 (0.000)\tLoss 1.5036 (1.2810)\tPrec@1 37.500 (53.690)\n",
      "EPOCH: 55 val Results: Prec@1 53.690 Loss: 1.2810\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/781]\tTime 0.003 (0.003)\tLoss 0.9775 (0.9775)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [56][156/781]\tTime 0.001 (0.002)\tLoss 1.0532 (1.1912)\tPrec@1 56.250 (57.623)\n",
      "Epoch: [56][312/781]\tTime 0.008 (0.002)\tLoss 1.1231 (1.2040)\tPrec@1 60.938 (57.233)\n",
      "Epoch: [56][468/781]\tTime 0.001 (0.002)\tLoss 1.0242 (1.2100)\tPrec@1 62.500 (57.066)\n",
      "Epoch: [56][624/781]\tTime 0.002 (0.002)\tLoss 1.0631 (1.2205)\tPrec@1 71.875 (56.665)\n",
      "Epoch: [56][780/781]\tTime 0.002 (0.002)\tLoss 1.4391 (1.2331)\tPrec@1 53.125 (56.246)\n",
      "Epoch: [56][781/781]\tTime 0.002 (0.002)\tLoss 1.3122 (1.2331)\tPrec@1 62.500 (56.248)\n",
      "EPOCH: 56 train Results: Prec@1 56.248 Loss: 1.2331\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1913 (1.1913)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4445 (1.2789)\tPrec@1 37.500 (53.750)\n",
      "EPOCH: 56 val Results: Prec@1 53.750 Loss: 1.2789\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/781]\tTime 0.003 (0.003)\tLoss 1.2397 (1.2397)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [57][156/781]\tTime 0.001 (0.002)\tLoss 1.3559 (1.1746)\tPrec@1 51.562 (57.743)\n",
      "Epoch: [57][312/781]\tTime 0.008 (0.002)\tLoss 1.0997 (1.1996)\tPrec@1 59.375 (56.989)\n",
      "Epoch: [57][468/781]\tTime 0.001 (0.002)\tLoss 1.4090 (1.2111)\tPrec@1 51.562 (56.650)\n",
      "Epoch: [57][624/781]\tTime 0.001 (0.002)\tLoss 1.2595 (1.2216)\tPrec@1 50.000 (56.195)\n",
      "Epoch: [57][780/781]\tTime 0.001 (0.002)\tLoss 1.2588 (1.2345)\tPrec@1 50.000 (55.782)\n",
      "Epoch: [57][781/781]\tTime 0.001 (0.002)\tLoss 1.3548 (1.2346)\tPrec@1 56.250 (55.782)\n",
      "EPOCH: 57 train Results: Prec@1 55.782 Loss: 1.2346\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1009 (1.1009)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2801 (1.2704)\tPrec@1 37.500 (54.630)\n",
      "EPOCH: 57 val Results: Prec@1 54.630 Loss: 1.2704\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/781]\tTime 0.002 (0.002)\tLoss 1.1665 (1.1665)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [58][156/781]\tTime 0.001 (0.002)\tLoss 1.2083 (1.1804)\tPrec@1 56.250 (58.171)\n",
      "Epoch: [58][312/781]\tTime 0.002 (0.002)\tLoss 1.2647 (1.2057)\tPrec@1 56.250 (57.228)\n",
      "Epoch: [58][468/781]\tTime 0.001 (0.002)\tLoss 1.2175 (1.2177)\tPrec@1 59.375 (57.003)\n",
      "Epoch: [58][624/781]\tTime 0.001 (0.002)\tLoss 1.1018 (1.2262)\tPrec@1 59.375 (56.602)\n",
      "Epoch: [58][780/781]\tTime 0.002 (0.002)\tLoss 1.4043 (1.2382)\tPrec@1 50.000 (56.062)\n",
      "Epoch: [58][781/781]\tTime 0.002 (0.002)\tLoss 1.2765 (1.2382)\tPrec@1 43.750 (56.058)\n",
      "EPOCH: 58 train Results: Prec@1 56.058 Loss: 1.2382\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1971 (1.1971)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3127 (1.2747)\tPrec@1 31.250 (54.190)\n",
      "EPOCH: 58 val Results: Prec@1 54.190 Loss: 1.2747\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/781]\tTime 0.002 (0.002)\tLoss 1.2581 (1.2581)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [59][156/781]\tTime 0.001 (0.002)\tLoss 1.1849 (1.1700)\tPrec@1 54.688 (58.798)\n",
      "Epoch: [59][312/781]\tTime 0.001 (0.002)\tLoss 1.1831 (1.1886)\tPrec@1 59.375 (57.778)\n",
      "Epoch: [59][468/781]\tTime 0.001 (0.002)\tLoss 1.1831 (1.2096)\tPrec@1 60.938 (57.033)\n",
      "Epoch: [59][624/781]\tTime 0.001 (0.002)\tLoss 1.1487 (1.2204)\tPrec@1 62.500 (56.578)\n",
      "Epoch: [59][780/781]\tTime 0.001 (0.002)\tLoss 1.1031 (1.2306)\tPrec@1 60.938 (56.260)\n",
      "Epoch: [59][781/781]\tTime 0.008 (0.002)\tLoss 1.1614 (1.2305)\tPrec@1 62.500 (56.262)\n",
      "EPOCH: 59 train Results: Prec@1 56.262 Loss: 1.2305\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2743 (1.2743)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3823 (1.2938)\tPrec@1 37.500 (53.410)\n",
      "EPOCH: 59 val Results: Prec@1 53.410 Loss: 1.2938\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/781]\tTime 0.002 (0.002)\tLoss 1.0877 (1.0877)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [60][156/781]\tTime 0.002 (0.002)\tLoss 1.2008 (1.1908)\tPrec@1 50.000 (58.221)\n",
      "Epoch: [60][312/781]\tTime 0.003 (0.002)\tLoss 1.2383 (1.2050)\tPrec@1 54.688 (57.184)\n",
      "Epoch: [60][468/781]\tTime 0.006 (0.002)\tLoss 1.3470 (1.2185)\tPrec@1 50.000 (56.656)\n",
      "Epoch: [60][624/781]\tTime 0.003 (0.002)\tLoss 1.2320 (1.2284)\tPrec@1 51.562 (56.220)\n",
      "Epoch: [60][780/781]\tTime 0.002 (0.002)\tLoss 1.3796 (1.2330)\tPrec@1 45.312 (56.036)\n",
      "Epoch: [60][781/781]\tTime 0.002 (0.002)\tLoss 1.2528 (1.2330)\tPrec@1 62.500 (56.038)\n",
      "EPOCH: 60 train Results: Prec@1 56.038 Loss: 1.2330\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2158 (1.2158)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1347 (1.2899)\tPrec@1 43.750 (54.020)\n",
      "EPOCH: 60 val Results: Prec@1 54.020 Loss: 1.2899\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/781]\tTime 0.004 (0.004)\tLoss 1.0057 (1.0057)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [61][156/781]\tTime 0.009 (0.006)\tLoss 1.2692 (1.1771)\tPrec@1 48.438 (57.972)\n",
      "Epoch: [61][312/781]\tTime 0.006 (0.015)\tLoss 1.1653 (1.2013)\tPrec@1 59.375 (57.054)\n",
      "Epoch: [61][468/781]\tTime 0.031 (0.018)\tLoss 1.1862 (1.2114)\tPrec@1 57.812 (56.726)\n",
      "Epoch: [61][624/781]\tTime 0.001 (0.018)\tLoss 1.3730 (1.2257)\tPrec@1 48.438 (56.172)\n",
      "Epoch: [61][780/781]\tTime 0.001 (0.015)\tLoss 1.5295 (1.2358)\tPrec@1 46.875 (55.944)\n",
      "Epoch: [61][781/781]\tTime 0.002 (0.015)\tLoss 1.3461 (1.2359)\tPrec@1 56.250 (55.944)\n",
      "EPOCH: 61 train Results: Prec@1 55.944 Loss: 1.2359\n",
      "Test: [0/156]\tTime 0.003 (0.003)\tLoss 1.3031 (1.3031)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.009 (0.001)\tLoss 1.1830 (1.2776)\tPrec@1 37.500 (53.980)\n",
      "EPOCH: 61 val Results: Prec@1 53.980 Loss: 1.2776\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/781]\tTime 0.005 (0.005)\tLoss 1.1739 (1.1739)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [62][156/781]\tTime 0.002 (0.003)\tLoss 1.3777 (1.1730)\tPrec@1 50.000 (58.479)\n",
      "Epoch: [62][312/781]\tTime 0.001 (0.003)\tLoss 0.9085 (1.1921)\tPrec@1 71.875 (57.673)\n",
      "Epoch: [62][468/781]\tTime 0.002 (0.003)\tLoss 1.1963 (1.2114)\tPrec@1 57.812 (56.860)\n",
      "Epoch: [62][624/781]\tTime 0.001 (0.003)\tLoss 1.2166 (1.2194)\tPrec@1 53.125 (56.610)\n",
      "Epoch: [62][780/781]\tTime 0.001 (0.003)\tLoss 1.0173 (1.2295)\tPrec@1 67.188 (56.268)\n",
      "Epoch: [62][781/781]\tTime 0.002 (0.003)\tLoss 0.9563 (1.2294)\tPrec@1 81.250 (56.276)\n",
      "EPOCH: 62 train Results: Prec@1 56.276 Loss: 1.2294\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2895 (1.2895)\tPrec@1 50.000 (50.000)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1157 (1.2824)\tPrec@1 56.250 (54.310)\n",
      "EPOCH: 62 val Results: Prec@1 54.310 Loss: 1.2824\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/781]\tTime 0.002 (0.002)\tLoss 1.2471 (1.2471)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [63][156/781]\tTime 0.005 (0.002)\tLoss 1.1194 (1.1656)\tPrec@1 70.312 (59.136)\n",
      "Epoch: [63][312/781]\tTime 0.016 (0.002)\tLoss 1.3479 (1.1983)\tPrec@1 45.312 (57.433)\n",
      "Epoch: [63][468/781]\tTime 0.001 (0.002)\tLoss 1.2244 (1.2124)\tPrec@1 53.125 (56.960)\n",
      "Epoch: [63][624/781]\tTime 0.002 (0.002)\tLoss 1.3561 (1.2258)\tPrec@1 51.562 (56.485)\n",
      "Epoch: [63][780/781]\tTime 0.001 (0.002)\tLoss 1.2255 (1.2310)\tPrec@1 56.250 (56.302)\n",
      "Epoch: [63][781/781]\tTime 0.004 (0.002)\tLoss 1.4748 (1.2311)\tPrec@1 37.500 (56.296)\n",
      "EPOCH: 63 train Results: Prec@1 56.296 Loss: 1.2311\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2191 (1.2191)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.0365 (1.2811)\tPrec@1 43.750 (54.320)\n",
      "EPOCH: 63 val Results: Prec@1 54.320 Loss: 1.2811\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/781]\tTime 0.002 (0.002)\tLoss 1.2784 (1.2784)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [64][156/781]\tTime 0.001 (0.003)\tLoss 1.3132 (1.1670)\tPrec@1 48.438 (57.822)\n",
      "Epoch: [64][312/781]\tTime 0.002 (0.003)\tLoss 0.9736 (1.1962)\tPrec@1 62.500 (57.099)\n",
      "Epoch: [64][468/781]\tTime 0.001 (0.003)\tLoss 1.3960 (1.2133)\tPrec@1 50.000 (56.563)\n",
      "Epoch: [64][624/781]\tTime 0.002 (0.003)\tLoss 1.3226 (1.2258)\tPrec@1 46.875 (56.142)\n",
      "Epoch: [64][780/781]\tTime 0.002 (0.002)\tLoss 1.3758 (1.2338)\tPrec@1 51.562 (55.904)\n",
      "Epoch: [64][781/781]\tTime 0.002 (0.002)\tLoss 1.5648 (1.2339)\tPrec@1 31.250 (55.896)\n",
      "EPOCH: 64 train Results: Prec@1 55.896 Loss: 1.2339\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1783 (1.1783)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0371 (1.2881)\tPrec@1 62.500 (53.540)\n",
      "EPOCH: 64 val Results: Prec@1 53.540 Loss: 1.2881\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/781]\tTime 0.003 (0.003)\tLoss 1.1557 (1.1557)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [65][156/781]\tTime 0.002 (0.002)\tLoss 1.2904 (1.1679)\tPrec@1 51.562 (58.171)\n",
      "Epoch: [65][312/781]\tTime 0.002 (0.002)\tLoss 1.2026 (1.1895)\tPrec@1 57.812 (57.842)\n",
      "Epoch: [65][468/781]\tTime 0.009 (0.002)\tLoss 1.2472 (1.2118)\tPrec@1 50.000 (56.873)\n",
      "Epoch: [65][624/781]\tTime 0.001 (0.002)\tLoss 1.3731 (1.2227)\tPrec@1 46.875 (56.403)\n",
      "Epoch: [65][780/781]\tTime 0.002 (0.002)\tLoss 1.2419 (1.2299)\tPrec@1 59.375 (56.068)\n",
      "Epoch: [65][781/781]\tTime 0.002 (0.002)\tLoss 1.3950 (1.2300)\tPrec@1 43.750 (56.064)\n",
      "EPOCH: 65 train Results: Prec@1 56.064 Loss: 1.2300\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2436 (1.2436)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1290 (1.2747)\tPrec@1 56.250 (54.800)\n",
      "EPOCH: 65 val Results: Prec@1 54.800 Loss: 1.2747\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/781]\tTime 0.002 (0.002)\tLoss 1.1278 (1.1278)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [66][156/781]\tTime 0.001 (0.002)\tLoss 1.4753 (1.1718)\tPrec@1 42.188 (58.250)\n",
      "Epoch: [66][312/781]\tTime 0.003 (0.002)\tLoss 1.2646 (1.1953)\tPrec@1 51.562 (57.094)\n",
      "Epoch: [66][468/781]\tTime 0.003 (0.002)\tLoss 1.3759 (1.2098)\tPrec@1 51.562 (56.733)\n",
      "Epoch: [66][624/781]\tTime 0.001 (0.002)\tLoss 1.3532 (1.2227)\tPrec@1 54.688 (56.370)\n",
      "Epoch: [66][780/781]\tTime 0.001 (0.002)\tLoss 1.4121 (1.2321)\tPrec@1 48.438 (56.124)\n",
      "Epoch: [66][781/781]\tTime 0.002 (0.002)\tLoss 1.3314 (1.2321)\tPrec@1 56.250 (56.124)\n",
      "EPOCH: 66 train Results: Prec@1 56.124 Loss: 1.2321\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1832 (1.1832)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2607 (1.2858)\tPrec@1 43.750 (53.970)\n",
      "EPOCH: 66 val Results: Prec@1 53.970 Loss: 1.2858\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/781]\tTime 0.002 (0.002)\tLoss 1.1171 (1.1171)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [67][156/781]\tTime 0.001 (0.002)\tLoss 1.4131 (1.1674)\tPrec@1 46.875 (58.519)\n",
      "Epoch: [67][312/781]\tTime 0.001 (0.002)\tLoss 1.2341 (1.1941)\tPrec@1 57.812 (57.328)\n",
      "Epoch: [67][468/781]\tTime 0.001 (0.002)\tLoss 1.2747 (1.2161)\tPrec@1 60.938 (56.557)\n",
      "Epoch: [67][624/781]\tTime 0.002 (0.002)\tLoss 1.2370 (1.2225)\tPrec@1 53.125 (56.373)\n",
      "Epoch: [67][780/781]\tTime 0.001 (0.002)\tLoss 1.1866 (1.2326)\tPrec@1 54.688 (55.984)\n",
      "Epoch: [67][781/781]\tTime 0.005 (0.002)\tLoss 1.4454 (1.2326)\tPrec@1 37.500 (55.978)\n",
      "EPOCH: 67 train Results: Prec@1 55.978 Loss: 1.2326\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1633 (1.1633)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 0.9709 (1.2832)\tPrec@1 68.750 (54.040)\n",
      "EPOCH: 67 val Results: Prec@1 54.040 Loss: 1.2832\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/781]\tTime 0.002 (0.002)\tLoss 0.9995 (0.9995)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [68][156/781]\tTime 0.013 (0.003)\tLoss 1.0378 (1.1689)\tPrec@1 67.188 (58.668)\n",
      "Epoch: [68][312/781]\tTime 0.001 (0.003)\tLoss 1.5616 (1.2062)\tPrec@1 48.438 (57.293)\n",
      "Epoch: [68][468/781]\tTime 0.002 (0.003)\tLoss 1.3229 (1.2194)\tPrec@1 50.000 (56.670)\n",
      "Epoch: [68][624/781]\tTime 0.001 (0.003)\tLoss 1.5055 (1.2268)\tPrec@1 51.562 (56.655)\n",
      "Epoch: [68][780/781]\tTime 0.003 (0.003)\tLoss 1.4587 (1.2362)\tPrec@1 51.562 (56.346)\n",
      "Epoch: [68][781/781]\tTime 0.002 (0.003)\tLoss 0.8806 (1.2361)\tPrec@1 68.750 (56.350)\n",
      "EPOCH: 68 train Results: Prec@1 56.350 Loss: 1.2361\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1818 (1.1818)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 0.9872 (1.2879)\tPrec@1 68.750 (53.730)\n",
      "EPOCH: 68 val Results: Prec@1 53.730 Loss: 1.2879\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/781]\tTime 0.002 (0.002)\tLoss 1.2268 (1.2268)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [69][156/781]\tTime 0.002 (0.002)\tLoss 1.1438 (1.1582)\tPrec@1 57.812 (58.867)\n",
      "Epoch: [69][312/781]\tTime 0.005 (0.002)\tLoss 1.2544 (1.1892)\tPrec@1 57.812 (57.917)\n",
      "Epoch: [69][468/781]\tTime 0.006 (0.002)\tLoss 1.2050 (1.2064)\tPrec@1 56.250 (57.183)\n",
      "Epoch: [69][624/781]\tTime 0.002 (0.002)\tLoss 1.3372 (1.2219)\tPrec@1 60.938 (56.585)\n",
      "Epoch: [69][780/781]\tTime 0.001 (0.002)\tLoss 1.2560 (1.2315)\tPrec@1 53.125 (56.198)\n",
      "Epoch: [69][781/781]\tTime 0.002 (0.002)\tLoss 1.2985 (1.2315)\tPrec@1 62.500 (56.200)\n",
      "EPOCH: 69 train Results: Prec@1 56.200 Loss: 1.2315\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2252 (1.2252)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2136 (1.2808)\tPrec@1 43.750 (53.960)\n",
      "EPOCH: 69 val Results: Prec@1 53.960 Loss: 1.2808\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/781]\tTime 0.002 (0.002)\tLoss 0.9788 (0.9788)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [70][156/781]\tTime 0.001 (0.002)\tLoss 1.3375 (1.1631)\tPrec@1 45.312 (59.315)\n",
      "Epoch: [70][312/781]\tTime 0.001 (0.002)\tLoss 1.2441 (1.1959)\tPrec@1 54.688 (58.182)\n",
      "Epoch: [70][468/781]\tTime 0.002 (0.002)\tLoss 1.1712 (1.2098)\tPrec@1 54.688 (57.399)\n",
      "Epoch: [70][624/781]\tTime 0.003 (0.002)\tLoss 1.3250 (1.2280)\tPrec@1 53.125 (56.462)\n",
      "Epoch: [70][780/781]\tTime 0.001 (0.002)\tLoss 1.3828 (1.2341)\tPrec@1 48.438 (56.194)\n",
      "Epoch: [70][781/781]\tTime 0.003 (0.002)\tLoss 1.3739 (1.2342)\tPrec@1 62.500 (56.196)\n",
      "EPOCH: 70 train Results: Prec@1 56.196 Loss: 1.2342\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3262 (1.3262)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.003 (0.000)\tLoss 1.2844 (1.2862)\tPrec@1 43.750 (53.910)\n",
      "EPOCH: 70 val Results: Prec@1 53.910 Loss: 1.2862\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/781]\tTime 0.003 (0.003)\tLoss 1.1498 (1.1498)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [71][156/781]\tTime 0.001 (0.003)\tLoss 1.1009 (1.1699)\tPrec@1 57.812 (58.529)\n",
      "Epoch: [71][312/781]\tTime 0.003 (0.003)\tLoss 1.0797 (1.1948)\tPrec@1 65.625 (57.788)\n",
      "Epoch: [71][468/781]\tTime 0.002 (0.003)\tLoss 0.9924 (1.2154)\tPrec@1 65.625 (56.796)\n",
      "Epoch: [71][624/781]\tTime 0.001 (0.003)\tLoss 1.0516 (1.2219)\tPrec@1 60.938 (56.595)\n",
      "Epoch: [71][780/781]\tTime 0.004 (0.003)\tLoss 0.9963 (1.2323)\tPrec@1 57.812 (56.104)\n",
      "Epoch: [71][781/781]\tTime 0.003 (0.003)\tLoss 1.1481 (1.2322)\tPrec@1 56.250 (56.104)\n",
      "EPOCH: 71 train Results: Prec@1 56.104 Loss: 1.2322\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2612 (1.2612)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0033 (1.2872)\tPrec@1 50.000 (53.540)\n",
      "EPOCH: 71 val Results: Prec@1 53.540 Loss: 1.2872\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/781]\tTime 0.002 (0.002)\tLoss 1.3420 (1.3420)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [72][156/781]\tTime 0.002 (0.002)\tLoss 1.2354 (1.1808)\tPrec@1 57.812 (58.489)\n",
      "Epoch: [72][312/781]\tTime 0.001 (0.002)\tLoss 1.2191 (1.1977)\tPrec@1 60.938 (57.618)\n",
      "Epoch: [72][468/781]\tTime 0.002 (0.002)\tLoss 1.5108 (1.2149)\tPrec@1 42.188 (56.690)\n",
      "Epoch: [72][624/781]\tTime 0.002 (0.002)\tLoss 1.2400 (1.2248)\tPrec@1 56.250 (56.380)\n",
      "Epoch: [72][780/781]\tTime 0.003 (0.002)\tLoss 0.9081 (1.2334)\tPrec@1 70.312 (56.102)\n",
      "Epoch: [72][781/781]\tTime 0.001 (0.002)\tLoss 1.5275 (1.2335)\tPrec@1 43.750 (56.098)\n",
      "EPOCH: 72 train Results: Prec@1 56.098 Loss: 1.2335\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1908 (1.1908)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3338 (1.2788)\tPrec@1 50.000 (54.020)\n",
      "EPOCH: 72 val Results: Prec@1 54.020 Loss: 1.2788\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/781]\tTime 0.002 (0.002)\tLoss 1.0350 (1.0350)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [73][156/781]\tTime 0.001 (0.002)\tLoss 1.1096 (1.1857)\tPrec@1 54.688 (57.464)\n",
      "Epoch: [73][312/781]\tTime 0.007 (0.002)\tLoss 1.3679 (1.1992)\tPrec@1 54.688 (57.059)\n",
      "Epoch: [73][468/781]\tTime 0.001 (0.002)\tLoss 1.2273 (1.2137)\tPrec@1 45.312 (56.433)\n",
      "Epoch: [73][624/781]\tTime 0.002 (0.002)\tLoss 1.0853 (1.2263)\tPrec@1 64.062 (56.075)\n",
      "Epoch: [73][780/781]\tTime 0.001 (0.002)\tLoss 1.1436 (1.2340)\tPrec@1 59.375 (55.996)\n",
      "Epoch: [73][781/781]\tTime 0.003 (0.002)\tLoss 1.7030 (1.2341)\tPrec@1 43.750 (55.992)\n",
      "EPOCH: 73 train Results: Prec@1 55.992 Loss: 1.2341\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1402 (1.1402)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1598 (1.2830)\tPrec@1 50.000 (54.410)\n",
      "EPOCH: 73 val Results: Prec@1 54.410 Loss: 1.2830\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/781]\tTime 0.002 (0.002)\tLoss 1.1385 (1.1385)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [74][156/781]\tTime 0.001 (0.002)\tLoss 1.3354 (1.1756)\tPrec@1 57.812 (58.678)\n",
      "Epoch: [74][312/781]\tTime 0.003 (0.002)\tLoss 1.1552 (1.2051)\tPrec@1 56.250 (57.338)\n",
      "Epoch: [74][468/781]\tTime 0.001 (0.002)\tLoss 1.1255 (1.2150)\tPrec@1 60.938 (56.963)\n",
      "Epoch: [74][624/781]\tTime 0.001 (0.002)\tLoss 1.1535 (1.2210)\tPrec@1 62.500 (56.763)\n",
      "Epoch: [74][780/781]\tTime 0.001 (0.002)\tLoss 1.4240 (1.2299)\tPrec@1 48.438 (56.430)\n",
      "Epoch: [74][781/781]\tTime 0.002 (0.002)\tLoss 0.9049 (1.2298)\tPrec@1 68.750 (56.434)\n",
      "EPOCH: 74 train Results: Prec@1 56.434 Loss: 1.2298\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1465 (1.1465)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2871 (1.2854)\tPrec@1 50.000 (53.880)\n",
      "EPOCH: 74 val Results: Prec@1 53.880 Loss: 1.2854\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/781]\tTime 0.003 (0.003)\tLoss 1.0329 (1.0329)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [75][156/781]\tTime 0.002 (0.002)\tLoss 1.2560 (1.1524)\tPrec@1 57.812 (59.126)\n",
      "Epoch: [75][312/781]\tTime 0.001 (0.002)\tLoss 0.9226 (1.1922)\tPrec@1 67.188 (57.673)\n",
      "Epoch: [75][468/781]\tTime 0.001 (0.002)\tLoss 1.4757 (1.2099)\tPrec@1 46.875 (56.966)\n",
      "Epoch: [75][624/781]\tTime 0.001 (0.002)\tLoss 1.4690 (1.2196)\tPrec@1 51.562 (56.650)\n",
      "Epoch: [75][780/781]\tTime 0.005 (0.002)\tLoss 1.2804 (1.2300)\tPrec@1 54.688 (56.320)\n",
      "Epoch: [75][781/781]\tTime 0.002 (0.002)\tLoss 1.1765 (1.2300)\tPrec@1 50.000 (56.318)\n",
      "EPOCH: 75 train Results: Prec@1 56.318 Loss: 1.2300\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2806 (1.2806)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2847 (1.2858)\tPrec@1 31.250 (53.810)\n",
      "EPOCH: 75 val Results: Prec@1 53.810 Loss: 1.2858\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/781]\tTime 0.004 (0.004)\tLoss 0.9698 (0.9698)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [76][156/781]\tTime 0.001 (0.002)\tLoss 1.1152 (1.1757)\tPrec@1 57.812 (58.430)\n",
      "Epoch: [76][312/781]\tTime 0.002 (0.002)\tLoss 1.1115 (1.1946)\tPrec@1 62.500 (57.493)\n",
      "Epoch: [76][468/781]\tTime 0.002 (0.002)\tLoss 1.1447 (1.2150)\tPrec@1 60.938 (56.836)\n",
      "Epoch: [76][624/781]\tTime 0.003 (0.002)\tLoss 1.3379 (1.2234)\tPrec@1 48.438 (56.420)\n",
      "Epoch: [76][780/781]\tTime 0.001 (0.002)\tLoss 1.0611 (1.2282)\tPrec@1 64.062 (56.282)\n",
      "Epoch: [76][781/781]\tTime 0.003 (0.002)\tLoss 1.1679 (1.2282)\tPrec@1 37.500 (56.276)\n",
      "EPOCH: 76 train Results: Prec@1 56.276 Loss: 1.2282\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2332 (1.2332)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2458 (1.2891)\tPrec@1 56.250 (54.560)\n",
      "EPOCH: 76 val Results: Prec@1 54.560 Loss: 1.2891\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/781]\tTime 0.002 (0.002)\tLoss 1.1144 (1.1144)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [77][156/781]\tTime 0.002 (0.002)\tLoss 1.0703 (1.1767)\tPrec@1 57.812 (58.380)\n",
      "Epoch: [77][312/781]\tTime 0.001 (0.002)\tLoss 1.1692 (1.2024)\tPrec@1 53.125 (57.423)\n",
      "Epoch: [77][468/781]\tTime 0.001 (0.002)\tLoss 1.1936 (1.2171)\tPrec@1 59.375 (56.830)\n",
      "Epoch: [77][624/781]\tTime 0.002 (0.002)\tLoss 1.2240 (1.2241)\tPrec@1 53.125 (56.465)\n",
      "Epoch: [77][780/781]\tTime 0.001 (0.002)\tLoss 1.2480 (1.2325)\tPrec@1 56.250 (56.164)\n",
      "Epoch: [77][781/781]\tTime 0.003 (0.002)\tLoss 1.4102 (1.2326)\tPrec@1 56.250 (56.164)\n",
      "EPOCH: 77 train Results: Prec@1 56.164 Loss: 1.2326\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2132 (1.2132)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1804 (1.2904)\tPrec@1 50.000 (53.540)\n",
      "EPOCH: 77 val Results: Prec@1 53.540 Loss: 1.2904\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/781]\tTime 0.002 (0.002)\tLoss 1.1904 (1.1904)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [78][156/781]\tTime 0.002 (0.002)\tLoss 1.1572 (1.1527)\tPrec@1 51.562 (58.818)\n",
      "Epoch: [78][312/781]\tTime 0.002 (0.002)\tLoss 1.2667 (1.1870)\tPrec@1 54.688 (57.458)\n",
      "Epoch: [78][468/781]\tTime 0.001 (0.002)\tLoss 1.2534 (1.2093)\tPrec@1 56.250 (56.746)\n",
      "Epoch: [78][624/781]\tTime 0.001 (0.002)\tLoss 1.1916 (1.2249)\tPrec@1 54.688 (56.115)\n",
      "Epoch: [78][780/781]\tTime 0.004 (0.002)\tLoss 1.1772 (1.2330)\tPrec@1 56.250 (55.800)\n",
      "Epoch: [78][781/781]\tTime 0.002 (0.002)\tLoss 1.6765 (1.2332)\tPrec@1 31.250 (55.792)\n",
      "EPOCH: 78 train Results: Prec@1 55.792 Loss: 1.2332\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1561 (1.1561)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 0.9768 (1.2644)\tPrec@1 56.250 (54.050)\n",
      "EPOCH: 78 val Results: Prec@1 54.050 Loss: 1.2644\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/781]\tTime 0.002 (0.002)\tLoss 1.0396 (1.0396)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [79][156/781]\tTime 0.002 (0.002)\tLoss 1.2274 (1.1544)\tPrec@1 62.500 (58.887)\n",
      "Epoch: [79][312/781]\tTime 0.001 (0.002)\tLoss 1.1815 (1.1824)\tPrec@1 62.500 (57.912)\n",
      "Epoch: [79][468/781]\tTime 0.001 (0.002)\tLoss 1.1422 (1.2036)\tPrec@1 54.688 (57.070)\n",
      "Epoch: [79][624/781]\tTime 0.001 (0.002)\tLoss 1.2690 (1.2198)\tPrec@1 50.000 (56.520)\n",
      "Epoch: [79][780/781]\tTime 0.003 (0.002)\tLoss 1.0966 (1.2308)\tPrec@1 54.688 (55.982)\n",
      "Epoch: [79][781/781]\tTime 0.002 (0.002)\tLoss 1.4922 (1.2309)\tPrec@1 50.000 (55.980)\n",
      "EPOCH: 79 train Results: Prec@1 55.980 Loss: 1.2309\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.2378 (1.2378)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2382 (1.2705)\tPrec@1 50.000 (54.530)\n",
      "EPOCH: 79 val Results: Prec@1 54.530 Loss: 1.2705\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/781]\tTime 0.003 (0.003)\tLoss 1.0159 (1.0159)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [80][156/781]\tTime 0.003 (0.002)\tLoss 1.1470 (1.1668)\tPrec@1 56.250 (58.529)\n",
      "Epoch: [80][312/781]\tTime 0.001 (0.002)\tLoss 1.3594 (1.1965)\tPrec@1 50.000 (57.338)\n",
      "Epoch: [80][468/781]\tTime 0.001 (0.002)\tLoss 1.1174 (1.2127)\tPrec@1 60.938 (56.820)\n",
      "Epoch: [80][624/781]\tTime 0.001 (0.003)\tLoss 1.3056 (1.2252)\tPrec@1 50.000 (56.480)\n",
      "Epoch: [80][780/781]\tTime 0.001 (0.002)\tLoss 1.0675 (1.2331)\tPrec@1 62.500 (56.028)\n",
      "Epoch: [80][781/781]\tTime 0.004 (0.002)\tLoss 1.7166 (1.2333)\tPrec@1 37.500 (56.022)\n",
      "EPOCH: 80 train Results: Prec@1 56.022 Loss: 1.2333\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.2129 (1.2129)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5020 (1.2850)\tPrec@1 37.500 (53.720)\n",
      "EPOCH: 80 val Results: Prec@1 53.720 Loss: 1.2850\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/781]\tTime 0.002 (0.002)\tLoss 1.1146 (1.1146)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [81][156/781]\tTime 0.001 (0.002)\tLoss 1.1502 (1.1836)\tPrec@1 57.812 (57.723)\n",
      "Epoch: [81][312/781]\tTime 0.001 (0.002)\tLoss 1.1407 (1.2040)\tPrec@1 60.938 (57.014)\n",
      "Epoch: [81][468/781]\tTime 0.001 (0.002)\tLoss 1.2477 (1.2157)\tPrec@1 51.562 (56.533)\n",
      "Epoch: [81][624/781]\tTime 0.001 (0.002)\tLoss 1.3709 (1.2262)\tPrec@1 51.562 (56.300)\n",
      "Epoch: [81][780/781]\tTime 0.001 (0.002)\tLoss 1.0988 (1.2366)\tPrec@1 57.812 (55.910)\n",
      "Epoch: [81][781/781]\tTime 0.002 (0.002)\tLoss 1.4806 (1.2366)\tPrec@1 50.000 (55.908)\n",
      "EPOCH: 81 train Results: Prec@1 55.908 Loss: 1.2366\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2444 (1.2444)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.4542 (1.2914)\tPrec@1 50.000 (53.690)\n",
      "EPOCH: 81 val Results: Prec@1 53.690 Loss: 1.2914\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/781]\tTime 0.004 (0.004)\tLoss 1.1475 (1.1475)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [82][156/781]\tTime 0.003 (0.002)\tLoss 1.1671 (1.1689)\tPrec@1 48.438 (58.459)\n",
      "Epoch: [82][312/781]\tTime 0.002 (0.002)\tLoss 1.2842 (1.1942)\tPrec@1 51.562 (57.388)\n",
      "Epoch: [82][468/781]\tTime 0.002 (0.002)\tLoss 1.4738 (1.2082)\tPrec@1 40.625 (56.976)\n",
      "Epoch: [82][624/781]\tTime 0.001 (0.002)\tLoss 1.1343 (1.2133)\tPrec@1 57.812 (56.775)\n",
      "Epoch: [82][780/781]\tTime 0.001 (0.002)\tLoss 1.3551 (1.2285)\tPrec@1 43.750 (56.182)\n",
      "Epoch: [82][781/781]\tTime 0.002 (0.002)\tLoss 1.0042 (1.2285)\tPrec@1 56.250 (56.182)\n",
      "EPOCH: 82 train Results: Prec@1 56.182 Loss: 1.2285\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.2539 (1.2539)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2790 (1.2770)\tPrec@1 56.250 (53.920)\n",
      "EPOCH: 82 val Results: Prec@1 53.920 Loss: 1.2770\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/781]\tTime 0.002 (0.002)\tLoss 1.2271 (1.2271)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [83][156/781]\tTime 0.001 (0.002)\tLoss 1.3563 (1.1715)\tPrec@1 53.125 (58.230)\n",
      "Epoch: [83][312/781]\tTime 0.001 (0.002)\tLoss 1.4095 (1.1988)\tPrec@1 48.438 (57.523)\n",
      "Epoch: [83][468/781]\tTime 0.003 (0.002)\tLoss 1.2601 (1.2122)\tPrec@1 53.125 (57.143)\n",
      "Epoch: [83][624/781]\tTime 0.007 (0.002)\tLoss 1.3525 (1.2258)\tPrec@1 42.188 (56.583)\n",
      "Epoch: [83][780/781]\tTime 0.001 (0.002)\tLoss 1.3533 (1.2312)\tPrec@1 56.250 (56.276)\n",
      "Epoch: [83][781/781]\tTime 0.002 (0.002)\tLoss 1.9779 (1.2314)\tPrec@1 25.000 (56.266)\n",
      "EPOCH: 83 train Results: Prec@1 56.266 Loss: 1.2314\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1113 (1.1113)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3744 (1.2892)\tPrec@1 37.500 (53.350)\n",
      "EPOCH: 83 val Results: Prec@1 53.350 Loss: 1.2892\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/781]\tTime 0.002 (0.002)\tLoss 1.0752 (1.0752)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [84][156/781]\tTime 0.001 (0.002)\tLoss 1.2347 (1.1818)\tPrec@1 62.500 (57.454)\n",
      "Epoch: [84][312/781]\tTime 0.001 (0.002)\tLoss 1.3363 (1.1982)\tPrec@1 53.125 (57.258)\n",
      "Epoch: [84][468/781]\tTime 0.005 (0.002)\tLoss 1.0955 (1.2172)\tPrec@1 62.500 (56.430)\n",
      "Epoch: [84][624/781]\tTime 0.001 (0.002)\tLoss 1.3812 (1.2249)\tPrec@1 53.125 (56.360)\n",
      "Epoch: [84][780/781]\tTime 0.001 (0.002)\tLoss 1.4545 (1.2277)\tPrec@1 43.750 (56.330)\n",
      "Epoch: [84][781/781]\tTime 0.003 (0.002)\tLoss 0.9971 (1.2277)\tPrec@1 68.750 (56.334)\n",
      "EPOCH: 84 train Results: Prec@1 56.334 Loss: 1.2277\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2739 (1.2739)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1873 (1.2984)\tPrec@1 62.500 (53.220)\n",
      "EPOCH: 84 val Results: Prec@1 53.220 Loss: 1.2984\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/781]\tTime 0.003 (0.003)\tLoss 1.2058 (1.2058)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [85][156/781]\tTime 0.004 (0.002)\tLoss 1.0092 (1.1718)\tPrec@1 70.312 (58.051)\n",
      "Epoch: [85][312/781]\tTime 0.001 (0.002)\tLoss 1.1674 (1.1972)\tPrec@1 57.812 (57.498)\n",
      "Epoch: [85][468/781]\tTime 0.001 (0.002)\tLoss 1.1994 (1.2147)\tPrec@1 54.688 (56.720)\n",
      "Epoch: [85][624/781]\tTime 0.001 (0.002)\tLoss 1.1288 (1.2268)\tPrec@1 54.688 (56.373)\n",
      "Epoch: [85][780/781]\tTime 0.003 (0.002)\tLoss 1.2785 (1.2352)\tPrec@1 53.125 (55.992)\n",
      "Epoch: [85][781/781]\tTime 0.002 (0.002)\tLoss 0.9259 (1.2351)\tPrec@1 68.750 (55.996)\n",
      "EPOCH: 85 train Results: Prec@1 55.996 Loss: 1.2351\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2276 (1.2276)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4870 (1.2777)\tPrec@1 50.000 (53.930)\n",
      "EPOCH: 85 val Results: Prec@1 53.930 Loss: 1.2777\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/781]\tTime 0.003 (0.003)\tLoss 1.1042 (1.1042)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [86][156/781]\tTime 0.002 (0.002)\tLoss 1.3165 (1.1935)\tPrec@1 56.250 (57.544)\n",
      "Epoch: [86][312/781]\tTime 0.001 (0.002)\tLoss 1.1500 (1.1990)\tPrec@1 57.812 (57.169)\n",
      "Epoch: [86][468/781]\tTime 0.005 (0.002)\tLoss 1.1047 (1.2124)\tPrec@1 62.500 (56.756)\n",
      "Epoch: [86][624/781]\tTime 0.001 (0.002)\tLoss 1.5147 (1.2242)\tPrec@1 43.750 (56.235)\n",
      "Epoch: [86][780/781]\tTime 0.001 (0.002)\tLoss 1.3073 (1.2319)\tPrec@1 51.562 (55.794)\n",
      "Epoch: [86][781/781]\tTime 0.002 (0.002)\tLoss 1.7894 (1.2321)\tPrec@1 31.250 (55.786)\n",
      "EPOCH: 86 train Results: Prec@1 55.786 Loss: 1.2321\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2738 (1.2738)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4586 (1.2742)\tPrec@1 50.000 (54.240)\n",
      "EPOCH: 86 val Results: Prec@1 54.240 Loss: 1.2742\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/781]\tTime 0.006 (0.006)\tLoss 1.3884 (1.3884)\tPrec@1 42.188 (42.188)\n",
      "Epoch: [87][156/781]\tTime 0.002 (0.002)\tLoss 1.1302 (1.1754)\tPrec@1 65.625 (57.892)\n",
      "Epoch: [87][312/781]\tTime 0.001 (0.002)\tLoss 1.1745 (1.1954)\tPrec@1 62.500 (57.333)\n",
      "Epoch: [87][468/781]\tTime 0.001 (0.002)\tLoss 1.2550 (1.2123)\tPrec@1 56.250 (56.513)\n",
      "Epoch: [87][624/781]\tTime 0.002 (0.002)\tLoss 1.5655 (1.2267)\tPrec@1 48.438 (56.180)\n",
      "Epoch: [87][780/781]\tTime 0.007 (0.002)\tLoss 1.4432 (1.2332)\tPrec@1 46.875 (56.004)\n",
      "Epoch: [87][781/781]\tTime 0.002 (0.002)\tLoss 1.6480 (1.2333)\tPrec@1 43.750 (56.000)\n",
      "EPOCH: 87 train Results: Prec@1 56.000 Loss: 1.2333\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1287 (1.1287)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2566 (1.2757)\tPrec@1 43.750 (54.330)\n",
      "EPOCH: 87 val Results: Prec@1 54.330 Loss: 1.2757\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/781]\tTime 0.004 (0.004)\tLoss 1.1578 (1.1578)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [88][156/781]\tTime 0.001 (0.002)\tLoss 0.9893 (1.1634)\tPrec@1 71.875 (58.848)\n",
      "Epoch: [88][312/781]\tTime 0.002 (0.002)\tLoss 1.4010 (1.1936)\tPrec@1 42.188 (57.708)\n",
      "Epoch: [88][468/781]\tTime 0.002 (0.002)\tLoss 1.2483 (1.2119)\tPrec@1 51.562 (57.030)\n",
      "Epoch: [88][624/781]\tTime 0.002 (0.002)\tLoss 0.9760 (1.2261)\tPrec@1 70.312 (56.337)\n",
      "Epoch: [88][780/781]\tTime 0.001 (0.002)\tLoss 1.6421 (1.2343)\tPrec@1 40.625 (56.202)\n",
      "Epoch: [88][781/781]\tTime 0.006 (0.002)\tLoss 1.3097 (1.2343)\tPrec@1 56.250 (56.202)\n",
      "EPOCH: 88 train Results: Prec@1 56.202 Loss: 1.2343\n",
      "Test: [0/156]\tTime 0.003 (0.003)\tLoss 1.2071 (1.2071)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2298 (1.2726)\tPrec@1 50.000 (54.710)\n",
      "EPOCH: 88 val Results: Prec@1 54.710 Loss: 1.2726\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/781]\tTime 0.003 (0.003)\tLoss 1.1226 (1.1226)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [89][156/781]\tTime 0.001 (0.002)\tLoss 0.9128 (1.1652)\tPrec@1 64.062 (58.390)\n",
      "Epoch: [89][312/781]\tTime 0.001 (0.002)\tLoss 1.3051 (1.1866)\tPrec@1 53.125 (57.438)\n",
      "Epoch: [89][468/781]\tTime 0.001 (0.002)\tLoss 1.1812 (1.2035)\tPrec@1 60.938 (56.820)\n",
      "Epoch: [89][624/781]\tTime 0.001 (0.002)\tLoss 1.4029 (1.2184)\tPrec@1 51.562 (56.377)\n",
      "Epoch: [89][780/781]\tTime 0.002 (0.002)\tLoss 1.4201 (1.2305)\tPrec@1 57.812 (55.984)\n",
      "Epoch: [89][781/781]\tTime 0.003 (0.002)\tLoss 1.6676 (1.2306)\tPrec@1 43.750 (55.980)\n",
      "EPOCH: 89 train Results: Prec@1 55.980 Loss: 1.2306\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.4288 (1.4288)\tPrec@1 48.438 (48.438)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.4953 (1.2869)\tPrec@1 43.750 (54.140)\n",
      "EPOCH: 89 val Results: Prec@1 54.140 Loss: 1.2869\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/781]\tTime 0.002 (0.002)\tLoss 1.0833 (1.0833)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [90][156/781]\tTime 0.001 (0.002)\tLoss 1.2883 (1.1513)\tPrec@1 59.375 (58.569)\n",
      "Epoch: [90][312/781]\tTime 0.001 (0.002)\tLoss 1.1259 (1.1891)\tPrec@1 57.812 (57.408)\n",
      "Epoch: [90][468/781]\tTime 0.001 (0.002)\tLoss 1.4516 (1.2109)\tPrec@1 53.125 (56.903)\n",
      "Epoch: [90][624/781]\tTime 0.001 (0.002)\tLoss 1.5874 (1.2178)\tPrec@1 45.312 (56.605)\n",
      "Epoch: [90][780/781]\tTime 0.001 (0.002)\tLoss 1.1340 (1.2296)\tPrec@1 56.250 (56.194)\n",
      "Epoch: [90][781/781]\tTime 0.002 (0.002)\tLoss 0.7606 (1.2294)\tPrec@1 81.250 (56.202)\n",
      "EPOCH: 90 train Results: Prec@1 56.202 Loss: 1.2294\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2866 (1.2866)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.5957 (1.2953)\tPrec@1 37.500 (53.290)\n",
      "EPOCH: 90 val Results: Prec@1 53.290 Loss: 1.2953\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/781]\tTime 0.002 (0.002)\tLoss 1.0673 (1.0673)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [91][156/781]\tTime 0.001 (0.003)\tLoss 1.2097 (1.1689)\tPrec@1 57.812 (58.609)\n",
      "Epoch: [91][312/781]\tTime 0.002 (0.002)\tLoss 1.3338 (1.1949)\tPrec@1 57.812 (57.453)\n",
      "Epoch: [91][468/781]\tTime 0.002 (0.002)\tLoss 1.0229 (1.2050)\tPrec@1 67.188 (57.160)\n",
      "Epoch: [91][624/781]\tTime 0.002 (0.002)\tLoss 1.1604 (1.2205)\tPrec@1 64.062 (56.597)\n",
      "Epoch: [91][780/781]\tTime 0.001 (0.002)\tLoss 1.2577 (1.2326)\tPrec@1 53.125 (56.216)\n",
      "Epoch: [91][781/781]\tTime 0.001 (0.002)\tLoss 1.5876 (1.2327)\tPrec@1 37.500 (56.210)\n",
      "EPOCH: 91 train Results: Prec@1 56.210 Loss: 1.2327\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1599 (1.1599)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0659 (1.2818)\tPrec@1 56.250 (53.910)\n",
      "EPOCH: 91 val Results: Prec@1 53.910 Loss: 1.2818\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/781]\tTime 0.004 (0.004)\tLoss 1.3753 (1.3753)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [92][156/781]\tTime 0.002 (0.002)\tLoss 1.2486 (1.1870)\tPrec@1 56.250 (57.723)\n",
      "Epoch: [92][312/781]\tTime 0.001 (0.002)\tLoss 0.9243 (1.2033)\tPrec@1 65.625 (57.268)\n",
      "Epoch: [92][468/781]\tTime 0.002 (0.002)\tLoss 1.1840 (1.2132)\tPrec@1 59.375 (56.783)\n",
      "Epoch: [92][624/781]\tTime 0.001 (0.002)\tLoss 1.4861 (1.2244)\tPrec@1 46.875 (56.303)\n",
      "Epoch: [92][780/781]\tTime 0.001 (0.002)\tLoss 1.1564 (1.2343)\tPrec@1 51.562 (55.962)\n",
      "Epoch: [92][781/781]\tTime 0.002 (0.002)\tLoss 1.2234 (1.2343)\tPrec@1 56.250 (55.962)\n",
      "EPOCH: 92 train Results: Prec@1 55.962 Loss: 1.2343\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.2757 (1.2757)\tPrec@1 50.000 (50.000)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2474 (1.2865)\tPrec@1 50.000 (53.730)\n",
      "EPOCH: 92 val Results: Prec@1 53.730 Loss: 1.2865\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/781]\tTime 0.002 (0.002)\tLoss 1.2981 (1.2981)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [93][156/781]\tTime 0.002 (0.002)\tLoss 1.0974 (1.1465)\tPrec@1 57.812 (59.654)\n",
      "Epoch: [93][312/781]\tTime 0.001 (0.002)\tLoss 1.2904 (1.1823)\tPrec@1 57.812 (58.287)\n",
      "Epoch: [93][468/781]\tTime 0.001 (0.002)\tLoss 1.1366 (1.2011)\tPrec@1 57.812 (57.516)\n",
      "Epoch: [93][624/781]\tTime 0.001 (0.002)\tLoss 1.2724 (1.2155)\tPrec@1 57.812 (56.873)\n",
      "Epoch: [93][780/781]\tTime 0.008 (0.002)\tLoss 1.4340 (1.2285)\tPrec@1 50.000 (56.388)\n",
      "Epoch: [93][781/781]\tTime 0.002 (0.002)\tLoss 0.8702 (1.2284)\tPrec@1 62.500 (56.390)\n",
      "EPOCH: 93 train Results: Prec@1 56.390 Loss: 1.2284\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2025 (1.2025)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1920 (1.2869)\tPrec@1 62.500 (54.330)\n",
      "EPOCH: 93 val Results: Prec@1 54.330 Loss: 1.2869\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/781]\tTime 0.002 (0.002)\tLoss 1.0828 (1.0828)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [94][156/781]\tTime 0.001 (0.002)\tLoss 1.1744 (1.1496)\tPrec@1 62.500 (58.758)\n",
      "Epoch: [94][312/781]\tTime 0.001 (0.002)\tLoss 1.2703 (1.1874)\tPrec@1 60.938 (57.678)\n",
      "Epoch: [94][468/781]\tTime 0.002 (0.002)\tLoss 1.1828 (1.2108)\tPrec@1 56.250 (56.763)\n",
      "Epoch: [94][624/781]\tTime 0.004 (0.002)\tLoss 1.3364 (1.2215)\tPrec@1 54.688 (56.462)\n",
      "Epoch: [94][780/781]\tTime 0.001 (0.002)\tLoss 1.4369 (1.2325)\tPrec@1 56.250 (56.092)\n",
      "Epoch: [94][781/781]\tTime 0.002 (0.002)\tLoss 1.6272 (1.2326)\tPrec@1 43.750 (56.088)\n",
      "EPOCH: 94 train Results: Prec@1 56.088 Loss: 1.2326\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2137 (1.2137)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4739 (1.2879)\tPrec@1 56.250 (54.030)\n",
      "EPOCH: 94 val Results: Prec@1 54.030 Loss: 1.2879\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/781]\tTime 0.003 (0.003)\tLoss 1.1217 (1.1217)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [95][156/781]\tTime 0.003 (0.002)\tLoss 1.2100 (1.1554)\tPrec@1 59.375 (59.186)\n",
      "Epoch: [95][312/781]\tTime 0.001 (0.002)\tLoss 1.2696 (1.1906)\tPrec@1 54.688 (57.648)\n",
      "Epoch: [95][468/781]\tTime 0.002 (0.002)\tLoss 1.1395 (1.2076)\tPrec@1 59.375 (57.006)\n",
      "Epoch: [95][624/781]\tTime 0.001 (0.002)\tLoss 1.1220 (1.2214)\tPrec@1 64.062 (56.480)\n",
      "Epoch: [95][780/781]\tTime 0.001 (0.002)\tLoss 1.2725 (1.2301)\tPrec@1 56.250 (56.238)\n",
      "Epoch: [95][781/781]\tTime 0.001 (0.002)\tLoss 1.4602 (1.2302)\tPrec@1 56.250 (56.238)\n",
      "EPOCH: 95 train Results: Prec@1 56.238 Loss: 1.2302\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1648 (1.1648)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5293 (1.2836)\tPrec@1 31.250 (54.250)\n",
      "EPOCH: 95 val Results: Prec@1 54.250 Loss: 1.2836\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/781]\tTime 0.002 (0.002)\tLoss 1.2434 (1.2434)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [96][156/781]\tTime 0.001 (0.002)\tLoss 1.0030 (1.1736)\tPrec@1 68.750 (58.648)\n",
      "Epoch: [96][312/781]\tTime 0.001 (0.002)\tLoss 1.0864 (1.2039)\tPrec@1 62.500 (57.398)\n",
      "Epoch: [96][468/781]\tTime 0.001 (0.002)\tLoss 1.3289 (1.2229)\tPrec@1 51.562 (56.733)\n",
      "Epoch: [96][624/781]\tTime 0.002 (0.002)\tLoss 1.1299 (1.2255)\tPrec@1 57.812 (56.612)\n",
      "Epoch: [96][780/781]\tTime 0.002 (0.002)\tLoss 1.3822 (1.2349)\tPrec@1 57.812 (56.154)\n",
      "Epoch: [96][781/781]\tTime 0.002 (0.002)\tLoss 1.2905 (1.2349)\tPrec@1 68.750 (56.158)\n",
      "EPOCH: 96 train Results: Prec@1 56.158 Loss: 1.2349\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2621 (1.2621)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4373 (1.2907)\tPrec@1 43.750 (53.890)\n",
      "EPOCH: 96 val Results: Prec@1 53.890 Loss: 1.2907\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/781]\tTime 0.002 (0.002)\tLoss 1.2942 (1.2942)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [97][156/781]\tTime 0.001 (0.002)\tLoss 1.1366 (1.1585)\tPrec@1 64.062 (58.967)\n",
      "Epoch: [97][312/781]\tTime 0.001 (0.002)\tLoss 1.3886 (1.1873)\tPrec@1 46.875 (57.912)\n",
      "Epoch: [97][468/781]\tTime 0.002 (0.002)\tLoss 1.3073 (1.2081)\tPrec@1 46.875 (57.156)\n",
      "Epoch: [97][624/781]\tTime 0.001 (0.002)\tLoss 1.3142 (1.2195)\tPrec@1 51.562 (56.805)\n",
      "Epoch: [97][780/781]\tTime 0.001 (0.002)\tLoss 1.0825 (1.2284)\tPrec@1 57.812 (56.466)\n",
      "Epoch: [97][781/781]\tTime 0.002 (0.002)\tLoss 1.1390 (1.2283)\tPrec@1 50.000 (56.464)\n",
      "EPOCH: 97 train Results: Prec@1 56.464 Loss: 1.2283\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3717 (1.3717)\tPrec@1 45.312 (45.312)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.4506 (1.2825)\tPrec@1 37.500 (54.380)\n",
      "EPOCH: 97 val Results: Prec@1 54.380 Loss: 1.2825\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/781]\tTime 0.002 (0.002)\tLoss 1.2199 (1.2199)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [98][156/781]\tTime 0.001 (0.002)\tLoss 1.0538 (1.1666)\tPrec@1 62.500 (58.420)\n",
      "Epoch: [98][312/781]\tTime 0.002 (0.002)\tLoss 1.2582 (1.1935)\tPrec@1 50.000 (57.678)\n",
      "Epoch: [98][468/781]\tTime 0.001 (0.002)\tLoss 1.1385 (1.2111)\tPrec@1 64.062 (56.893)\n",
      "Epoch: [98][624/781]\tTime 0.001 (0.002)\tLoss 1.5558 (1.2233)\tPrec@1 39.062 (56.470)\n",
      "Epoch: [98][780/781]\tTime 0.003 (0.002)\tLoss 1.3071 (1.2323)\tPrec@1 57.812 (56.150)\n",
      "Epoch: [98][781/781]\tTime 0.002 (0.002)\tLoss 0.9589 (1.2323)\tPrec@1 81.250 (56.158)\n",
      "EPOCH: 98 train Results: Prec@1 56.158 Loss: 1.2323\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1581 (1.1581)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3502 (1.2829)\tPrec@1 37.500 (54.130)\n",
      "EPOCH: 98 val Results: Prec@1 54.130 Loss: 1.2829\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/781]\tTime 0.002 (0.002)\tLoss 1.1611 (1.1611)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [99][156/781]\tTime 0.002 (0.002)\tLoss 1.3379 (1.1653)\tPrec@1 54.688 (58.658)\n",
      "Epoch: [99][312/781]\tTime 0.001 (0.002)\tLoss 1.2878 (1.2024)\tPrec@1 51.562 (57.188)\n",
      "Epoch: [99][468/781]\tTime 0.001 (0.002)\tLoss 1.5911 (1.2123)\tPrec@1 42.188 (56.856)\n",
      "Epoch: [99][624/781]\tTime 0.001 (0.002)\tLoss 1.2335 (1.2248)\tPrec@1 59.375 (56.453)\n",
      "Epoch: [99][780/781]\tTime 0.002 (0.002)\tLoss 1.2342 (1.2314)\tPrec@1 57.812 (56.124)\n",
      "Epoch: [99][781/781]\tTime 0.002 (0.002)\tLoss 0.9248 (1.2313)\tPrec@1 68.750 (56.128)\n",
      "EPOCH: 99 train Results: Prec@1 56.128 Loss: 1.2313\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2169 (1.2169)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5771 (1.2854)\tPrec@1 37.500 (54.040)\n",
      "EPOCH: 99 val Results: Prec@1 54.040 Loss: 1.2854\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/781]\tTime 0.002 (0.002)\tLoss 1.0871 (1.0871)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [100][156/781]\tTime 0.002 (0.003)\tLoss 1.1562 (1.1718)\tPrec@1 62.500 (58.499)\n",
      "Epoch: [100][312/781]\tTime 0.001 (0.003)\tLoss 1.1951 (1.2021)\tPrec@1 51.562 (57.443)\n",
      "Epoch: [100][468/781]\tTime 0.002 (0.002)\tLoss 1.3023 (1.2154)\tPrec@1 54.688 (56.740)\n",
      "Epoch: [100][624/781]\tTime 0.001 (0.002)\tLoss 1.2340 (1.2305)\tPrec@1 59.375 (56.347)\n",
      "Epoch: [100][780/781]\tTime 0.003 (0.002)\tLoss 1.4236 (1.2352)\tPrec@1 50.000 (56.162)\n",
      "Epoch: [100][781/781]\tTime 0.002 (0.002)\tLoss 1.3364 (1.2353)\tPrec@1 68.750 (56.166)\n",
      "EPOCH: 100 train Results: Prec@1 56.166 Loss: 1.2353\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2755 (1.2755)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3595 (1.2859)\tPrec@1 31.250 (53.850)\n",
      "EPOCH: 100 val Results: Prec@1 53.850 Loss: 1.2859\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/781]\tTime 0.002 (0.002)\tLoss 1.1265 (1.1265)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [101][156/781]\tTime 0.002 (0.002)\tLoss 1.2402 (1.1817)\tPrec@1 60.938 (58.420)\n",
      "Epoch: [101][312/781]\tTime 0.002 (0.002)\tLoss 1.4460 (1.2039)\tPrec@1 46.875 (57.578)\n",
      "Epoch: [101][468/781]\tTime 0.002 (0.002)\tLoss 1.2315 (1.2209)\tPrec@1 60.938 (56.823)\n",
      "Epoch: [101][624/781]\tTime 0.001 (0.002)\tLoss 1.1088 (1.2245)\tPrec@1 59.375 (56.572)\n",
      "Epoch: [101][780/781]\tTime 0.001 (0.002)\tLoss 1.1096 (1.2354)\tPrec@1 60.938 (56.240)\n",
      "Epoch: [101][781/781]\tTime 0.002 (0.002)\tLoss 1.5113 (1.2354)\tPrec@1 50.000 (56.238)\n",
      "EPOCH: 101 train Results: Prec@1 56.238 Loss: 1.2354\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1854 (1.1854)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2937 (1.2819)\tPrec@1 37.500 (54.050)\n",
      "EPOCH: 101 val Results: Prec@1 54.050 Loss: 1.2819\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/781]\tTime 0.002 (0.002)\tLoss 1.3090 (1.3090)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [102][156/781]\tTime 0.002 (0.002)\tLoss 1.1570 (1.1564)\tPrec@1 54.688 (58.788)\n",
      "Epoch: [102][312/781]\tTime 0.001 (0.002)\tLoss 1.3112 (1.1977)\tPrec@1 57.812 (57.129)\n",
      "Epoch: [102][468/781]\tTime 0.001 (0.002)\tLoss 1.1245 (1.2134)\tPrec@1 62.500 (56.666)\n",
      "Epoch: [102][624/781]\tTime 0.001 (0.002)\tLoss 1.0737 (1.2223)\tPrec@1 60.938 (56.425)\n",
      "Epoch: [102][780/781]\tTime 0.001 (0.002)\tLoss 1.1554 (1.2307)\tPrec@1 54.688 (56.058)\n",
      "Epoch: [102][781/781]\tTime 0.001 (0.002)\tLoss 1.3419 (1.2307)\tPrec@1 50.000 (56.056)\n",
      "EPOCH: 102 train Results: Prec@1 56.056 Loss: 1.2307\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.2505 (1.2505)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1835 (1.3007)\tPrec@1 50.000 (53.480)\n",
      "EPOCH: 102 val Results: Prec@1 53.480 Loss: 1.3007\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/781]\tTime 0.002 (0.002)\tLoss 1.1259 (1.1259)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [103][156/781]\tTime 0.009 (0.002)\tLoss 1.1192 (1.1826)\tPrec@1 64.062 (57.942)\n",
      "Epoch: [103][312/781]\tTime 0.001 (0.002)\tLoss 1.3827 (1.2011)\tPrec@1 48.438 (57.124)\n",
      "Epoch: [103][468/781]\tTime 0.002 (0.002)\tLoss 1.2419 (1.2152)\tPrec@1 56.250 (56.656)\n",
      "Epoch: [103][624/781]\tTime 0.003 (0.002)\tLoss 1.3753 (1.2237)\tPrec@1 45.312 (56.460)\n",
      "Epoch: [103][780/781]\tTime 0.001 (0.002)\tLoss 1.4076 (1.2326)\tPrec@1 56.250 (56.240)\n",
      "Epoch: [103][781/781]\tTime 0.002 (0.002)\tLoss 1.4529 (1.2326)\tPrec@1 50.000 (56.238)\n",
      "EPOCH: 103 train Results: Prec@1 56.238 Loss: 1.2326\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2100 (1.2100)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.002 (0.001)\tLoss 1.2417 (1.3002)\tPrec@1 50.000 (53.430)\n",
      "EPOCH: 103 val Results: Prec@1 53.430 Loss: 1.3002\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/781]\tTime 0.003 (0.003)\tLoss 1.1655 (1.1655)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [104][156/781]\tTime 0.003 (0.003)\tLoss 0.9600 (1.1703)\tPrec@1 64.062 (58.141)\n",
      "Epoch: [104][312/781]\tTime 0.031 (0.003)\tLoss 1.2593 (1.1964)\tPrec@1 56.250 (57.498)\n",
      "Epoch: [104][468/781]\tTime 0.001 (0.004)\tLoss 1.1267 (1.2063)\tPrec@1 54.688 (56.883)\n",
      "Epoch: [104][624/781]\tTime 0.012 (0.004)\tLoss 1.0472 (1.2198)\tPrec@1 65.625 (56.450)\n",
      "Epoch: [104][780/781]\tTime 0.002 (0.004)\tLoss 1.2005 (1.2299)\tPrec@1 60.938 (56.082)\n",
      "Epoch: [104][781/781]\tTime 0.002 (0.004)\tLoss 1.3394 (1.2300)\tPrec@1 37.500 (56.076)\n",
      "EPOCH: 104 train Results: Prec@1 56.076 Loss: 1.2300\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2415 (1.2415)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.003 (0.001)\tLoss 1.3219 (1.2789)\tPrec@1 37.500 (54.240)\n",
      "EPOCH: 104 val Results: Prec@1 54.240 Loss: 1.2789\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/781]\tTime 0.003 (0.003)\tLoss 1.1175 (1.1175)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [105][156/781]\tTime 0.004 (0.003)\tLoss 1.3495 (1.1718)\tPrec@1 50.000 (58.380)\n",
      "Epoch: [105][312/781]\tTime 0.001 (0.004)\tLoss 1.3692 (1.1958)\tPrec@1 43.750 (57.413)\n",
      "Epoch: [105][468/781]\tTime 0.002 (0.004)\tLoss 1.5941 (1.2066)\tPrec@1 50.000 (56.993)\n",
      "Epoch: [105][624/781]\tTime 0.002 (0.004)\tLoss 1.0352 (1.2155)\tPrec@1 60.938 (56.700)\n",
      "Epoch: [105][780/781]\tTime 0.003 (0.004)\tLoss 1.3392 (1.2294)\tPrec@1 56.250 (56.276)\n",
      "Epoch: [105][781/781]\tTime 0.003 (0.004)\tLoss 1.8538 (1.2296)\tPrec@1 37.500 (56.270)\n",
      "EPOCH: 105 train Results: Prec@1 56.270 Loss: 1.2296\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1449 (1.1449)\tPrec@1 67.188 (67.188)\n",
      "Test: [156/156]\tTime 0.001 (0.001)\tLoss 1.2859 (1.2951)\tPrec@1 37.500 (53.730)\n",
      "EPOCH: 105 val Results: Prec@1 53.730 Loss: 1.2951\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/781]\tTime 0.004 (0.004)\tLoss 1.0995 (1.0995)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [106][156/781]\tTime 0.001 (0.004)\tLoss 1.1347 (1.1715)\tPrec@1 57.812 (58.907)\n",
      "Epoch: [106][312/781]\tTime 0.001 (0.003)\tLoss 1.2769 (1.1982)\tPrec@1 53.125 (57.528)\n",
      "Epoch: [106][468/781]\tTime 0.010 (0.003)\tLoss 1.0265 (1.2166)\tPrec@1 62.500 (56.850)\n",
      "Epoch: [106][624/781]\tTime 0.001 (0.003)\tLoss 1.0209 (1.2288)\tPrec@1 65.625 (56.515)\n",
      "Epoch: [106][780/781]\tTime 0.001 (0.003)\tLoss 1.3080 (1.2342)\tPrec@1 56.250 (56.356)\n",
      "Epoch: [106][781/781]\tTime 0.002 (0.003)\tLoss 1.2787 (1.2342)\tPrec@1 37.500 (56.350)\n",
      "EPOCH: 106 train Results: Prec@1 56.350 Loss: 1.2342\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1552 (1.1552)\tPrec@1 67.188 (67.188)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.4040 (1.2846)\tPrec@1 31.250 (53.660)\n",
      "EPOCH: 106 val Results: Prec@1 53.660 Loss: 1.2846\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/781]\tTime 0.002 (0.002)\tLoss 0.9777 (0.9777)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [107][156/781]\tTime 0.001 (0.003)\tLoss 1.2856 (1.1726)\tPrec@1 53.125 (58.310)\n",
      "Epoch: [107][312/781]\tTime 0.002 (0.002)\tLoss 1.4444 (1.1968)\tPrec@1 54.688 (57.398)\n",
      "Epoch: [107][468/781]\tTime 0.003 (0.002)\tLoss 1.3244 (1.2088)\tPrec@1 51.562 (56.913)\n",
      "Epoch: [107][624/781]\tTime 0.001 (0.003)\tLoss 1.2016 (1.2265)\tPrec@1 62.500 (56.242)\n",
      "Epoch: [107][780/781]\tTime 0.001 (0.003)\tLoss 1.1632 (1.2370)\tPrec@1 59.375 (55.824)\n",
      "Epoch: [107][781/781]\tTime 0.002 (0.003)\tLoss 1.1001 (1.2369)\tPrec@1 68.750 (55.828)\n",
      "EPOCH: 107 train Results: Prec@1 55.828 Loss: 1.2369\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2004 (1.2004)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3635 (1.2813)\tPrec@1 37.500 (54.200)\n",
      "EPOCH: 107 val Results: Prec@1 54.200 Loss: 1.2813\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/781]\tTime 0.003 (0.003)\tLoss 1.0702 (1.0702)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [108][156/781]\tTime 0.001 (0.002)\tLoss 1.3679 (1.1669)\tPrec@1 57.812 (58.559)\n",
      "Epoch: [108][312/781]\tTime 0.001 (0.002)\tLoss 1.1305 (1.1932)\tPrec@1 65.625 (57.433)\n",
      "Epoch: [108][468/781]\tTime 0.002 (0.002)\tLoss 1.2160 (1.2122)\tPrec@1 56.250 (56.753)\n",
      "Epoch: [108][624/781]\tTime 0.001 (0.002)\tLoss 1.1924 (1.2220)\tPrec@1 62.500 (56.428)\n",
      "Epoch: [108][780/781]\tTime 0.002 (0.002)\tLoss 1.3136 (1.2309)\tPrec@1 53.125 (56.140)\n",
      "Epoch: [108][781/781]\tTime 0.002 (0.002)\tLoss 1.4008 (1.2310)\tPrec@1 43.750 (56.136)\n",
      "EPOCH: 108 train Results: Prec@1 56.136 Loss: 1.2310\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2206 (1.2206)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4004 (1.3084)\tPrec@1 56.250 (53.060)\n",
      "EPOCH: 108 val Results: Prec@1 53.060 Loss: 1.3084\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/781]\tTime 0.004 (0.004)\tLoss 0.9982 (0.9982)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [109][156/781]\tTime 0.002 (0.002)\tLoss 1.2054 (1.1744)\tPrec@1 54.688 (58.529)\n",
      "Epoch: [109][312/781]\tTime 0.001 (0.002)\tLoss 1.2005 (1.2006)\tPrec@1 51.562 (57.388)\n",
      "Epoch: [109][468/781]\tTime 0.001 (0.002)\tLoss 1.0811 (1.2125)\tPrec@1 57.812 (56.810)\n",
      "Epoch: [109][624/781]\tTime 0.002 (0.002)\tLoss 1.2987 (1.2198)\tPrec@1 46.875 (56.490)\n",
      "Epoch: [109][780/781]\tTime 0.001 (0.002)\tLoss 1.3674 (1.2285)\tPrec@1 57.812 (56.216)\n",
      "Epoch: [109][781/781]\tTime 0.002 (0.002)\tLoss 1.4010 (1.2285)\tPrec@1 50.000 (56.214)\n",
      "EPOCH: 109 train Results: Prec@1 56.214 Loss: 1.2285\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1445 (1.1445)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3829 (1.2683)\tPrec@1 43.750 (54.450)\n",
      "EPOCH: 109 val Results: Prec@1 54.450 Loss: 1.2683\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/781]\tTime 0.002 (0.002)\tLoss 1.2111 (1.2111)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [110][156/781]\tTime 0.002 (0.002)\tLoss 1.1153 (1.1615)\tPrec@1 57.812 (58.439)\n",
      "Epoch: [110][312/781]\tTime 0.001 (0.002)\tLoss 1.1021 (1.1952)\tPrec@1 67.188 (57.413)\n",
      "Epoch: [110][468/781]\tTime 0.001 (0.002)\tLoss 1.2741 (1.2094)\tPrec@1 60.938 (57.053)\n",
      "Epoch: [110][624/781]\tTime 0.004 (0.002)\tLoss 1.4002 (1.2246)\tPrec@1 48.438 (56.487)\n",
      "Epoch: [110][780/781]\tTime 0.004 (0.002)\tLoss 1.3764 (1.2316)\tPrec@1 39.062 (56.170)\n",
      "Epoch: [110][781/781]\tTime 0.002 (0.002)\tLoss 1.5671 (1.2317)\tPrec@1 50.000 (56.168)\n",
      "EPOCH: 110 train Results: Prec@1 56.168 Loss: 1.2317\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1757 (1.1757)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.5016 (1.2740)\tPrec@1 37.500 (54.250)\n",
      "EPOCH: 110 val Results: Prec@1 54.250 Loss: 1.2740\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/781]\tTime 0.003 (0.003)\tLoss 1.0697 (1.0697)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [111][156/781]\tTime 0.002 (0.002)\tLoss 1.2187 (1.1714)\tPrec@1 60.938 (57.852)\n",
      "Epoch: [111][312/781]\tTime 0.001 (0.002)\tLoss 1.2619 (1.1991)\tPrec@1 56.250 (56.984)\n",
      "Epoch: [111][468/781]\tTime 0.001 (0.002)\tLoss 1.3910 (1.2173)\tPrec@1 45.312 (56.433)\n",
      "Epoch: [111][624/781]\tTime 0.002 (0.002)\tLoss 1.2766 (1.2270)\tPrec@1 59.375 (56.225)\n",
      "Epoch: [111][780/781]\tTime 0.001 (0.002)\tLoss 1.2860 (1.2306)\tPrec@1 60.938 (56.236)\n",
      "Epoch: [111][781/781]\tTime 0.002 (0.002)\tLoss 1.4066 (1.2307)\tPrec@1 37.500 (56.230)\n",
      "EPOCH: 111 train Results: Prec@1 56.230 Loss: 1.2307\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1837 (1.1837)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4539 (1.3010)\tPrec@1 50.000 (53.040)\n",
      "EPOCH: 111 val Results: Prec@1 53.040 Loss: 1.3010\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/781]\tTime 0.002 (0.002)\tLoss 1.1065 (1.1065)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [112][156/781]\tTime 0.001 (0.002)\tLoss 1.2105 (1.1659)\tPrec@1 57.812 (58.350)\n",
      "Epoch: [112][312/781]\tTime 0.002 (0.002)\tLoss 1.1628 (1.1948)\tPrec@1 54.688 (57.353)\n",
      "Epoch: [112][468/781]\tTime 0.001 (0.002)\tLoss 1.3792 (1.2097)\tPrec@1 54.688 (56.840)\n",
      "Epoch: [112][624/781]\tTime 0.007 (0.002)\tLoss 1.1165 (1.2296)\tPrec@1 56.250 (56.155)\n",
      "Epoch: [112][780/781]\tTime 0.001 (0.002)\tLoss 1.3084 (1.2327)\tPrec@1 57.812 (56.104)\n",
      "Epoch: [112][781/781]\tTime 0.007 (0.002)\tLoss 1.0969 (1.2326)\tPrec@1 68.750 (56.108)\n",
      "EPOCH: 112 train Results: Prec@1 56.108 Loss: 1.2326\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2490 (1.2490)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1860 (1.2845)\tPrec@1 37.500 (54.010)\n",
      "EPOCH: 112 val Results: Prec@1 54.010 Loss: 1.2845\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/781]\tTime 0.002 (0.002)\tLoss 1.0756 (1.0756)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [113][156/781]\tTime 0.002 (0.002)\tLoss 1.1279 (1.1565)\tPrec@1 64.062 (59.226)\n",
      "Epoch: [113][312/781]\tTime 0.001 (0.002)\tLoss 1.2105 (1.1880)\tPrec@1 56.250 (57.862)\n",
      "Epoch: [113][468/781]\tTime 0.001 (0.002)\tLoss 1.0442 (1.2012)\tPrec@1 62.500 (57.289)\n",
      "Epoch: [113][624/781]\tTime 0.001 (0.002)\tLoss 1.0953 (1.2145)\tPrec@1 59.375 (56.627)\n",
      "Epoch: [113][780/781]\tTime 0.001 (0.002)\tLoss 1.3134 (1.2280)\tPrec@1 57.812 (56.248)\n",
      "Epoch: [113][781/781]\tTime 0.009 (0.002)\tLoss 1.4785 (1.2281)\tPrec@1 56.250 (56.248)\n",
      "EPOCH: 113 train Results: Prec@1 56.248 Loss: 1.2281\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2499 (1.2499)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2556 (1.2677)\tPrec@1 56.250 (54.410)\n",
      "EPOCH: 113 val Results: Prec@1 54.410 Loss: 1.2677\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/781]\tTime 0.003 (0.003)\tLoss 1.1353 (1.1353)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [114][156/781]\tTime 0.002 (0.002)\tLoss 1.2242 (1.1808)\tPrec@1 54.688 (57.842)\n",
      "Epoch: [114][312/781]\tTime 0.001 (0.002)\tLoss 1.2675 (1.2012)\tPrec@1 59.375 (57.009)\n",
      "Epoch: [114][468/781]\tTime 0.002 (0.002)\tLoss 1.3147 (1.2174)\tPrec@1 42.188 (56.460)\n",
      "Epoch: [114][624/781]\tTime 0.001 (0.002)\tLoss 1.0105 (1.2212)\tPrec@1 60.938 (56.367)\n",
      "Epoch: [114][780/781]\tTime 0.001 (0.002)\tLoss 1.3043 (1.2312)\tPrec@1 51.562 (56.048)\n",
      "Epoch: [114][781/781]\tTime 0.002 (0.002)\tLoss 1.4438 (1.2313)\tPrec@1 43.750 (56.044)\n",
      "EPOCH: 114 train Results: Prec@1 56.044 Loss: 1.2313\n",
      "Test: [0/156]\tTime 0.003 (0.003)\tLoss 1.2237 (1.2237)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3395 (1.2816)\tPrec@1 43.750 (54.160)\n",
      "EPOCH: 114 val Results: Prec@1 54.160 Loss: 1.2816\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/781]\tTime 0.003 (0.003)\tLoss 1.0036 (1.0036)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [115][156/781]\tTime 0.001 (0.002)\tLoss 1.0281 (1.1586)\tPrec@1 64.062 (58.639)\n",
      "Epoch: [115][312/781]\tTime 0.001 (0.002)\tLoss 1.1504 (1.1888)\tPrec@1 60.938 (57.773)\n",
      "Epoch: [115][468/781]\tTime 0.002 (0.002)\tLoss 1.4610 (1.2119)\tPrec@1 43.750 (56.930)\n",
      "Epoch: [115][624/781]\tTime 0.004 (0.002)\tLoss 1.3626 (1.2254)\tPrec@1 43.750 (56.477)\n",
      "Epoch: [115][780/781]\tTime 0.001 (0.002)\tLoss 1.3581 (1.2324)\tPrec@1 62.500 (56.244)\n",
      "Epoch: [115][781/781]\tTime 0.002 (0.002)\tLoss 1.2845 (1.2324)\tPrec@1 50.000 (56.242)\n",
      "EPOCH: 115 train Results: Prec@1 56.242 Loss: 1.2324\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1752 (1.1752)\tPrec@1 65.625 (65.625)\n",
      "Test: [156/156]\tTime 0.001 (0.000)\tLoss 1.2269 (1.2808)\tPrec@1 50.000 (53.730)\n",
      "EPOCH: 115 val Results: Prec@1 53.730 Loss: 1.2808\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/781]\tTime 0.002 (0.002)\tLoss 1.0829 (1.0829)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [116][156/781]\tTime 0.002 (0.002)\tLoss 1.3483 (1.1669)\tPrec@1 56.250 (58.639)\n",
      "Epoch: [116][312/781]\tTime 0.001 (0.002)\tLoss 1.3359 (1.1894)\tPrec@1 53.125 (57.478)\n",
      "Epoch: [116][468/781]\tTime 0.002 (0.002)\tLoss 1.1200 (1.2063)\tPrec@1 65.625 (56.956)\n",
      "Epoch: [116][624/781]\tTime 0.001 (0.002)\tLoss 1.2242 (1.2187)\tPrec@1 57.812 (56.508)\n",
      "Epoch: [116][780/781]\tTime 0.004 (0.002)\tLoss 1.2245 (1.2269)\tPrec@1 57.812 (56.170)\n",
      "Epoch: [116][781/781]\tTime 0.001 (0.002)\tLoss 1.8126 (1.2271)\tPrec@1 18.750 (56.158)\n",
      "EPOCH: 116 train Results: Prec@1 56.158 Loss: 1.2271\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2411 (1.2411)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.002 (0.001)\tLoss 1.3870 (1.2871)\tPrec@1 62.500 (54.190)\n",
      "EPOCH: 116 val Results: Prec@1 54.190 Loss: 1.2871\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/781]\tTime 0.003 (0.003)\tLoss 1.2228 (1.2228)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [117][156/781]\tTime 0.002 (0.002)\tLoss 1.3776 (1.1762)\tPrec@1 48.438 (58.479)\n",
      "Epoch: [117][312/781]\tTime 0.002 (0.002)\tLoss 1.1770 (1.1958)\tPrec@1 56.250 (57.528)\n",
      "Epoch: [117][468/781]\tTime 0.001 (0.002)\tLoss 1.1681 (1.2161)\tPrec@1 64.062 (56.926)\n",
      "Epoch: [117][624/781]\tTime 0.004 (0.002)\tLoss 1.3037 (1.2274)\tPrec@1 50.000 (56.278)\n",
      "Epoch: [117][780/781]\tTime 0.009 (0.002)\tLoss 1.1227 (1.2320)\tPrec@1 54.688 (56.102)\n",
      "Epoch: [117][781/781]\tTime 0.002 (0.002)\tLoss 1.6087 (1.2322)\tPrec@1 43.750 (56.098)\n",
      "EPOCH: 117 train Results: Prec@1 56.098 Loss: 1.2322\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1519 (1.1519)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3670 (1.2802)\tPrec@1 50.000 (54.120)\n",
      "EPOCH: 117 val Results: Prec@1 54.120 Loss: 1.2802\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/781]\tTime 0.002 (0.002)\tLoss 1.0988 (1.0988)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [118][156/781]\tTime 0.003 (0.003)\tLoss 1.1778 (1.1782)\tPrec@1 56.250 (57.773)\n",
      "Epoch: [118][312/781]\tTime 0.001 (0.002)\tLoss 1.0791 (1.1981)\tPrec@1 64.062 (57.303)\n",
      "Epoch: [118][468/781]\tTime 0.002 (0.002)\tLoss 1.2240 (1.2137)\tPrec@1 56.250 (56.746)\n",
      "Epoch: [118][624/781]\tTime 0.002 (0.002)\tLoss 1.2949 (1.2238)\tPrec@1 46.875 (56.415)\n",
      "Epoch: [118][780/781]\tTime 0.001 (0.002)\tLoss 1.0636 (1.2345)\tPrec@1 60.938 (56.010)\n",
      "Epoch: [118][781/781]\tTime 0.003 (0.002)\tLoss 2.0051 (1.2348)\tPrec@1 56.250 (56.010)\n",
      "EPOCH: 118 train Results: Prec@1 56.010 Loss: 1.2348\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2136 (1.2136)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1441 (1.2933)\tPrec@1 37.500 (53.900)\n",
      "EPOCH: 118 val Results: Prec@1 53.900 Loss: 1.2933\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/781]\tTime 0.002 (0.002)\tLoss 1.1866 (1.1866)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [119][156/781]\tTime 0.001 (0.002)\tLoss 1.3793 (1.1454)\tPrec@1 46.875 (59.494)\n",
      "Epoch: [119][312/781]\tTime 0.001 (0.002)\tLoss 1.3620 (1.1823)\tPrec@1 43.750 (57.892)\n",
      "Epoch: [119][468/781]\tTime 0.004 (0.002)\tLoss 1.3022 (1.2019)\tPrec@1 59.375 (57.313)\n",
      "Epoch: [119][624/781]\tTime 0.001 (0.002)\tLoss 1.2396 (1.2156)\tPrec@1 56.250 (56.830)\n",
      "Epoch: [119][780/781]\tTime 0.001 (0.002)\tLoss 1.2435 (1.2274)\tPrec@1 53.125 (56.300)\n",
      "Epoch: [119][781/781]\tTime 0.001 (0.002)\tLoss 1.5102 (1.2275)\tPrec@1 50.000 (56.298)\n",
      "EPOCH: 119 train Results: Prec@1 56.298 Loss: 1.2275\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2386 (1.2386)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2759 (1.2878)\tPrec@1 43.750 (54.190)\n",
      "EPOCH: 119 val Results: Prec@1 54.190 Loss: 1.2878\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/781]\tTime 0.002 (0.002)\tLoss 0.9721 (0.9721)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [120][156/781]\tTime 0.001 (0.002)\tLoss 1.1813 (1.1736)\tPrec@1 57.812 (57.832)\n",
      "Epoch: [120][312/781]\tTime 0.002 (0.002)\tLoss 1.0876 (1.2045)\tPrec@1 64.062 (56.929)\n",
      "Epoch: [120][468/781]\tTime 0.003 (0.002)\tLoss 1.2139 (1.2154)\tPrec@1 60.938 (56.833)\n",
      "Epoch: [120][624/781]\tTime 0.002 (0.002)\tLoss 1.2594 (1.2213)\tPrec@1 51.562 (56.477)\n",
      "Epoch: [120][780/781]\tTime 0.001 (0.002)\tLoss 1.0155 (1.2309)\tPrec@1 65.625 (56.140)\n",
      "Epoch: [120][781/781]\tTime 0.006 (0.002)\tLoss 1.5307 (1.2310)\tPrec@1 31.250 (56.132)\n",
      "EPOCH: 120 train Results: Prec@1 56.132 Loss: 1.2310\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1336 (1.1336)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1781 (1.2898)\tPrec@1 68.750 (54.130)\n",
      "EPOCH: 120 val Results: Prec@1 54.130 Loss: 1.2898\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/781]\tTime 0.004 (0.004)\tLoss 1.1027 (1.1027)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [121][156/781]\tTime 0.011 (0.002)\tLoss 1.1192 (1.1523)\tPrec@1 59.375 (58.987)\n",
      "Epoch: [121][312/781]\tTime 0.001 (0.002)\tLoss 1.2111 (1.1935)\tPrec@1 59.375 (57.493)\n",
      "Epoch: [121][468/781]\tTime 0.006 (0.002)\tLoss 1.2397 (1.2083)\tPrec@1 51.562 (57.073)\n",
      "Epoch: [121][624/781]\tTime 0.001 (0.002)\tLoss 1.2910 (1.2205)\tPrec@1 50.000 (56.523)\n",
      "Epoch: [121][780/781]\tTime 0.002 (0.002)\tLoss 0.9512 (1.2301)\tPrec@1 62.500 (56.196)\n",
      "Epoch: [121][781/781]\tTime 0.003 (0.002)\tLoss 1.2881 (1.2301)\tPrec@1 56.250 (56.196)\n",
      "EPOCH: 121 train Results: Prec@1 56.196 Loss: 1.2301\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.2437 (1.2437)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1354 (1.3064)\tPrec@1 43.750 (52.940)\n",
      "EPOCH: 121 val Results: Prec@1 52.940 Loss: 1.3064\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/781]\tTime 0.004 (0.004)\tLoss 1.3258 (1.3258)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [122][156/781]\tTime 0.003 (0.002)\tLoss 1.2077 (1.1670)\tPrec@1 50.000 (58.708)\n",
      "Epoch: [122][312/781]\tTime 0.002 (0.002)\tLoss 1.0589 (1.1860)\tPrec@1 62.500 (58.062)\n",
      "Epoch: [122][468/781]\tTime 0.001 (0.002)\tLoss 1.2795 (1.2072)\tPrec@1 59.375 (57.313)\n",
      "Epoch: [122][624/781]\tTime 0.001 (0.002)\tLoss 1.4528 (1.2231)\tPrec@1 50.000 (56.615)\n",
      "Epoch: [122][780/781]\tTime 0.004 (0.002)\tLoss 1.4332 (1.2323)\tPrec@1 53.125 (56.222)\n",
      "Epoch: [122][781/781]\tTime 0.003 (0.002)\tLoss 1.2988 (1.2323)\tPrec@1 50.000 (56.220)\n",
      "EPOCH: 122 train Results: Prec@1 56.220 Loss: 1.2323\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3232 (1.3232)\tPrec@1 50.000 (50.000)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2356 (1.2797)\tPrec@1 43.750 (54.500)\n",
      "EPOCH: 122 val Results: Prec@1 54.500 Loss: 1.2797\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/781]\tTime 0.003 (0.003)\tLoss 1.0415 (1.0415)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [123][156/781]\tTime 0.001 (0.003)\tLoss 1.1866 (1.1678)\tPrec@1 48.438 (58.957)\n",
      "Epoch: [123][312/781]\tTime 0.002 (0.002)\tLoss 1.3163 (1.1955)\tPrec@1 54.688 (57.633)\n",
      "Epoch: [123][468/781]\tTime 0.004 (0.002)\tLoss 1.5382 (1.2110)\tPrec@1 48.438 (57.003)\n",
      "Epoch: [123][624/781]\tTime 0.004 (0.002)\tLoss 1.2360 (1.2239)\tPrec@1 54.688 (56.608)\n",
      "Epoch: [123][780/781]\tTime 0.001 (0.002)\tLoss 1.3025 (1.2313)\tPrec@1 51.562 (56.260)\n",
      "Epoch: [123][781/781]\tTime 0.003 (0.002)\tLoss 0.8241 (1.2311)\tPrec@1 87.500 (56.270)\n",
      "EPOCH: 123 train Results: Prec@1 56.270 Loss: 1.2311\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2902 (1.2902)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5384 (1.2998)\tPrec@1 37.500 (53.170)\n",
      "EPOCH: 123 val Results: Prec@1 53.170 Loss: 1.2998\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/781]\tTime 0.002 (0.002)\tLoss 1.0137 (1.0137)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [124][156/781]\tTime 0.005 (0.002)\tLoss 1.3262 (1.1852)\tPrec@1 53.125 (58.420)\n",
      "Epoch: [124][312/781]\tTime 0.001 (0.002)\tLoss 1.2251 (1.2029)\tPrec@1 59.375 (57.298)\n",
      "Epoch: [124][468/781]\tTime 0.001 (0.002)\tLoss 1.1433 (1.2171)\tPrec@1 53.125 (56.760)\n",
      "Epoch: [124][624/781]\tTime 0.001 (0.002)\tLoss 1.4754 (1.2199)\tPrec@1 48.438 (56.553)\n",
      "Epoch: [124][780/781]\tTime 0.001 (0.002)\tLoss 1.2339 (1.2292)\tPrec@1 57.812 (56.224)\n",
      "Epoch: [124][781/781]\tTime 0.002 (0.002)\tLoss 1.4115 (1.2293)\tPrec@1 50.000 (56.222)\n",
      "EPOCH: 124 train Results: Prec@1 56.222 Loss: 1.2293\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3009 (1.3009)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3934 (1.2943)\tPrec@1 37.500 (53.380)\n",
      "EPOCH: 124 val Results: Prec@1 53.380 Loss: 1.2943\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/781]\tTime 0.002 (0.002)\tLoss 1.3229 (1.3229)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [125][156/781]\tTime 0.001 (0.002)\tLoss 1.1526 (1.1769)\tPrec@1 56.250 (58.230)\n",
      "Epoch: [125][312/781]\tTime 0.002 (0.002)\tLoss 1.4373 (1.2066)\tPrec@1 53.125 (57.099)\n",
      "Epoch: [125][468/781]\tTime 0.003 (0.002)\tLoss 1.2724 (1.2177)\tPrec@1 57.812 (56.746)\n",
      "Epoch: [125][624/781]\tTime 0.001 (0.002)\tLoss 1.3217 (1.2261)\tPrec@1 57.812 (56.455)\n",
      "Epoch: [125][780/781]\tTime 0.001 (0.002)\tLoss 1.1218 (1.2328)\tPrec@1 59.375 (56.206)\n",
      "Epoch: [125][781/781]\tTime 0.002 (0.002)\tLoss 1.4290 (1.2329)\tPrec@1 37.500 (56.200)\n",
      "EPOCH: 125 train Results: Prec@1 56.200 Loss: 1.2329\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1673 (1.1673)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2395 (1.2840)\tPrec@1 50.000 (54.190)\n",
      "EPOCH: 125 val Results: Prec@1 54.190 Loss: 1.2840\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/781]\tTime 0.003 (0.003)\tLoss 1.0181 (1.0181)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [126][156/781]\tTime 0.002 (0.002)\tLoss 1.1371 (1.1523)\tPrec@1 60.938 (59.176)\n",
      "Epoch: [126][312/781]\tTime 0.001 (0.002)\tLoss 1.1819 (1.1901)\tPrec@1 53.125 (57.917)\n",
      "Epoch: [126][468/781]\tTime 0.002 (0.002)\tLoss 1.4041 (1.2092)\tPrec@1 48.438 (57.243)\n",
      "Epoch: [126][624/781]\tTime 0.001 (0.002)\tLoss 1.2915 (1.2218)\tPrec@1 51.562 (56.638)\n",
      "Epoch: [126][780/781]\tTime 0.001 (0.002)\tLoss 1.2304 (1.2315)\tPrec@1 56.250 (56.386)\n",
      "Epoch: [126][781/781]\tTime 0.002 (0.002)\tLoss 0.7627 (1.2313)\tPrec@1 75.000 (56.392)\n",
      "EPOCH: 126 train Results: Prec@1 56.392 Loss: 1.2313\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2036 (1.2036)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4586 (1.2928)\tPrec@1 43.750 (53.330)\n",
      "EPOCH: 126 val Results: Prec@1 53.330 Loss: 1.2928\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/781]\tTime 0.002 (0.002)\tLoss 1.1666 (1.1666)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [127][156/781]\tTime 0.001 (0.002)\tLoss 0.9233 (1.1606)\tPrec@1 70.312 (58.838)\n",
      "Epoch: [127][312/781]\tTime 0.001 (0.002)\tLoss 1.2006 (1.1883)\tPrec@1 50.000 (57.588)\n",
      "Epoch: [127][468/781]\tTime 0.001 (0.002)\tLoss 1.4211 (1.2071)\tPrec@1 45.312 (56.863)\n",
      "Epoch: [127][624/781]\tTime 0.002 (0.002)\tLoss 1.2540 (1.2228)\tPrec@1 54.688 (56.390)\n",
      "Epoch: [127][780/781]\tTime 0.001 (0.002)\tLoss 1.2250 (1.2309)\tPrec@1 60.938 (56.132)\n",
      "Epoch: [127][781/781]\tTime 0.002 (0.002)\tLoss 1.0090 (1.2308)\tPrec@1 62.500 (56.134)\n",
      "EPOCH: 127 train Results: Prec@1 56.134 Loss: 1.2308\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2012 (1.2012)\tPrec@1 67.188 (67.188)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3885 (1.2810)\tPrec@1 37.500 (54.220)\n",
      "EPOCH: 127 val Results: Prec@1 54.220 Loss: 1.2810\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/781]\tTime 0.002 (0.002)\tLoss 1.0769 (1.0769)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [128][156/781]\tTime 0.001 (0.002)\tLoss 1.3963 (1.1645)\tPrec@1 45.312 (58.230)\n",
      "Epoch: [128][312/781]\tTime 0.001 (0.002)\tLoss 1.1286 (1.1957)\tPrec@1 59.375 (57.039)\n",
      "Epoch: [128][468/781]\tTime 0.001 (0.002)\tLoss 1.2434 (1.2085)\tPrec@1 56.250 (56.753)\n",
      "Epoch: [128][624/781]\tTime 0.001 (0.002)\tLoss 1.2550 (1.2215)\tPrec@1 59.375 (56.483)\n",
      "Epoch: [128][780/781]\tTime 0.001 (0.002)\tLoss 1.2692 (1.2295)\tPrec@1 57.812 (56.196)\n",
      "Epoch: [128][781/781]\tTime 0.002 (0.002)\tLoss 1.2880 (1.2295)\tPrec@1 50.000 (56.194)\n",
      "EPOCH: 128 train Results: Prec@1 56.194 Loss: 1.2295\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.0857 (1.0857)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 0.9602 (1.2835)\tPrec@1 68.750 (53.470)\n",
      "EPOCH: 128 val Results: Prec@1 53.470 Loss: 1.2835\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/781]\tTime 0.002 (0.002)\tLoss 1.1357 (1.1357)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [129][156/781]\tTime 0.001 (0.002)\tLoss 1.2388 (1.1601)\tPrec@1 64.062 (58.798)\n",
      "Epoch: [129][312/781]\tTime 0.002 (0.002)\tLoss 1.0927 (1.1808)\tPrec@1 60.938 (58.112)\n",
      "Epoch: [129][468/781]\tTime 0.001 (0.002)\tLoss 1.4265 (1.1999)\tPrec@1 46.875 (57.363)\n",
      "Epoch: [129][624/781]\tTime 0.003 (0.002)\tLoss 1.0997 (1.2140)\tPrec@1 64.062 (56.737)\n",
      "Epoch: [129][780/781]\tTime 0.002 (0.002)\tLoss 1.1165 (1.2256)\tPrec@1 56.250 (56.302)\n",
      "Epoch: [129][781/781]\tTime 0.003 (0.002)\tLoss 1.4947 (1.2257)\tPrec@1 56.250 (56.302)\n",
      "EPOCH: 129 train Results: Prec@1 56.302 Loss: 1.2257\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2405 (1.2405)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2164 (1.2860)\tPrec@1 50.000 (53.910)\n",
      "EPOCH: 129 val Results: Prec@1 53.910 Loss: 1.2860\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/781]\tTime 0.003 (0.003)\tLoss 1.1417 (1.1417)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [130][156/781]\tTime 0.002 (0.002)\tLoss 1.0483 (1.1649)\tPrec@1 54.688 (58.987)\n",
      "Epoch: [130][312/781]\tTime 0.002 (0.002)\tLoss 1.0832 (1.1852)\tPrec@1 59.375 (57.987)\n",
      "Epoch: [130][468/781]\tTime 0.003 (0.002)\tLoss 1.2307 (1.2126)\tPrec@1 51.562 (56.890)\n",
      "Epoch: [130][624/781]\tTime 0.003 (0.002)\tLoss 1.4039 (1.2246)\tPrec@1 54.688 (56.487)\n",
      "Epoch: [130][780/781]\tTime 0.001 (0.002)\tLoss 1.4150 (1.2311)\tPrec@1 51.562 (56.224)\n",
      "Epoch: [130][781/781]\tTime 0.003 (0.002)\tLoss 1.6224 (1.2312)\tPrec@1 37.500 (56.218)\n",
      "EPOCH: 130 train Results: Prec@1 56.218 Loss: 1.2312\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.2224 (1.2224)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2408 (1.2886)\tPrec@1 56.250 (54.130)\n",
      "EPOCH: 130 val Results: Prec@1 54.130 Loss: 1.2886\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/781]\tTime 0.003 (0.003)\tLoss 1.0219 (1.0219)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [131][156/781]\tTime 0.004 (0.002)\tLoss 1.1565 (1.1692)\tPrec@1 56.250 (58.250)\n",
      "Epoch: [131][312/781]\tTime 0.001 (0.002)\tLoss 1.2368 (1.1955)\tPrec@1 57.812 (57.188)\n",
      "Epoch: [131][468/781]\tTime 0.001 (0.002)\tLoss 1.2798 (1.2133)\tPrec@1 54.688 (56.610)\n",
      "Epoch: [131][624/781]\tTime 0.001 (0.002)\tLoss 1.4813 (1.2244)\tPrec@1 48.438 (56.237)\n",
      "Epoch: [131][780/781]\tTime 0.006 (0.002)\tLoss 0.9917 (1.2337)\tPrec@1 60.938 (55.880)\n",
      "Epoch: [131][781/781]\tTime 0.003 (0.002)\tLoss 2.0182 (1.2340)\tPrec@1 43.750 (55.876)\n",
      "EPOCH: 131 train Results: Prec@1 55.876 Loss: 1.2340\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.2431 (1.2431)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2590 (1.2708)\tPrec@1 50.000 (54.840)\n",
      "EPOCH: 131 val Results: Prec@1 54.840 Loss: 1.2708\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/781]\tTime 0.004 (0.004)\tLoss 1.1358 (1.1358)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [132][156/781]\tTime 0.002 (0.002)\tLoss 1.2705 (1.1697)\tPrec@1 51.562 (59.156)\n",
      "Epoch: [132][312/781]\tTime 0.002 (0.002)\tLoss 1.3257 (1.2067)\tPrec@1 48.438 (57.323)\n",
      "Epoch: [132][468/781]\tTime 0.002 (0.002)\tLoss 1.2112 (1.2209)\tPrec@1 60.938 (56.810)\n",
      "Epoch: [132][624/781]\tTime 0.001 (0.002)\tLoss 1.3717 (1.2256)\tPrec@1 39.062 (56.458)\n",
      "Epoch: [132][780/781]\tTime 0.002 (0.002)\tLoss 1.2474 (1.2313)\tPrec@1 54.688 (56.228)\n",
      "Epoch: [132][781/781]\tTime 0.002 (0.002)\tLoss 1.6153 (1.2315)\tPrec@1 50.000 (56.226)\n",
      "EPOCH: 132 train Results: Prec@1 56.226 Loss: 1.2315\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.0883 (1.0883)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2407 (1.2832)\tPrec@1 37.500 (54.640)\n",
      "EPOCH: 132 val Results: Prec@1 54.640 Loss: 1.2832\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/781]\tTime 0.003 (0.003)\tLoss 0.9573 (0.9573)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [133][156/781]\tTime 0.001 (0.002)\tLoss 1.2082 (1.1725)\tPrec@1 51.562 (58.250)\n",
      "Epoch: [133][312/781]\tTime 0.002 (0.002)\tLoss 1.1243 (1.1854)\tPrec@1 56.250 (58.122)\n",
      "Epoch: [133][468/781]\tTime 0.002 (0.002)\tLoss 1.3807 (1.2058)\tPrec@1 51.562 (57.170)\n",
      "Epoch: [133][624/781]\tTime 0.001 (0.002)\tLoss 1.1920 (1.2208)\tPrec@1 59.375 (56.565)\n",
      "Epoch: [133][780/781]\tTime 0.002 (0.002)\tLoss 1.2301 (1.2291)\tPrec@1 54.688 (56.204)\n",
      "Epoch: [133][781/781]\tTime 0.003 (0.002)\tLoss 1.1314 (1.2291)\tPrec@1 62.500 (56.206)\n",
      "EPOCH: 133 train Results: Prec@1 56.206 Loss: 1.2291\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2638 (1.2638)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2668 (1.2812)\tPrec@1 56.250 (53.650)\n",
      "EPOCH: 133 val Results: Prec@1 53.650 Loss: 1.2812\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/781]\tTime 0.002 (0.002)\tLoss 1.2209 (1.2209)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [134][156/781]\tTime 0.005 (0.002)\tLoss 1.1363 (1.1775)\tPrec@1 68.750 (57.852)\n",
      "Epoch: [134][312/781]\tTime 0.001 (0.002)\tLoss 1.2232 (1.2022)\tPrec@1 54.688 (57.203)\n",
      "Epoch: [134][468/781]\tTime 0.001 (0.002)\tLoss 1.2368 (1.2121)\tPrec@1 56.250 (56.880)\n",
      "Epoch: [134][624/781]\tTime 0.001 (0.002)\tLoss 1.1356 (1.2199)\tPrec@1 64.062 (56.535)\n",
      "Epoch: [134][780/781]\tTime 0.001 (0.002)\tLoss 1.0471 (1.2275)\tPrec@1 57.812 (56.204)\n",
      "Epoch: [134][781/781]\tTime 0.008 (0.002)\tLoss 1.5048 (1.2276)\tPrec@1 37.500 (56.198)\n",
      "EPOCH: 134 train Results: Prec@1 56.198 Loss: 1.2276\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1468 (1.1468)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2064 (1.2811)\tPrec@1 37.500 (54.210)\n",
      "EPOCH: 134 val Results: Prec@1 54.210 Loss: 1.2811\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/781]\tTime 0.005 (0.005)\tLoss 1.0991 (1.0991)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [135][156/781]\tTime 0.005 (0.002)\tLoss 1.3380 (1.1575)\tPrec@1 54.688 (58.828)\n",
      "Epoch: [135][312/781]\tTime 0.002 (0.002)\tLoss 1.3070 (1.1906)\tPrec@1 56.250 (57.812)\n",
      "Epoch: [135][468/781]\tTime 0.002 (0.002)\tLoss 1.3621 (1.2064)\tPrec@1 51.562 (56.973)\n",
      "Epoch: [135][624/781]\tTime 0.001 (0.002)\tLoss 1.3059 (1.2197)\tPrec@1 54.688 (56.490)\n",
      "Epoch: [135][780/781]\tTime 0.001 (0.002)\tLoss 1.2762 (1.2325)\tPrec@1 54.688 (56.030)\n",
      "Epoch: [135][781/781]\tTime 0.001 (0.002)\tLoss 1.2926 (1.2325)\tPrec@1 50.000 (56.028)\n",
      "EPOCH: 135 train Results: Prec@1 56.028 Loss: 1.2325\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2654 (1.2654)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1676 (1.2811)\tPrec@1 50.000 (53.960)\n",
      "EPOCH: 135 val Results: Prec@1 53.960 Loss: 1.2811\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/781]\tTime 0.003 (0.003)\tLoss 1.0353 (1.0353)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [136][156/781]\tTime 0.001 (0.002)\tLoss 1.1678 (1.1690)\tPrec@1 54.688 (58.459)\n",
      "Epoch: [136][312/781]\tTime 0.001 (0.002)\tLoss 1.1370 (1.1902)\tPrec@1 62.500 (57.523)\n",
      "Epoch: [136][468/781]\tTime 0.001 (0.002)\tLoss 1.1593 (1.2058)\tPrec@1 65.625 (56.916)\n",
      "Epoch: [136][624/781]\tTime 0.001 (0.002)\tLoss 1.4269 (1.2190)\tPrec@1 59.375 (56.568)\n",
      "Epoch: [136][780/781]\tTime 0.002 (0.002)\tLoss 1.3813 (1.2280)\tPrec@1 48.438 (56.266)\n",
      "Epoch: [136][781/781]\tTime 0.002 (0.002)\tLoss 1.0509 (1.2280)\tPrec@1 68.750 (56.270)\n",
      "EPOCH: 136 train Results: Prec@1 56.270 Loss: 1.2280\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.1436 (1.1436)\tPrec@1 65.625 (65.625)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.0945 (1.2801)\tPrec@1 56.250 (54.340)\n",
      "EPOCH: 136 val Results: Prec@1 54.340 Loss: 1.2801\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/781]\tTime 0.004 (0.004)\tLoss 1.2324 (1.2324)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [137][156/781]\tTime 0.001 (0.002)\tLoss 1.3474 (1.1878)\tPrec@1 46.875 (57.335)\n",
      "Epoch: [137][312/781]\tTime 0.001 (0.002)\tLoss 1.1978 (1.1961)\tPrec@1 57.812 (57.333)\n",
      "Epoch: [137][468/781]\tTime 0.002 (0.002)\tLoss 1.0323 (1.2065)\tPrec@1 67.188 (56.763)\n",
      "Epoch: [137][624/781]\tTime 0.003 (0.002)\tLoss 1.3704 (1.2180)\tPrec@1 56.250 (56.360)\n",
      "Epoch: [137][780/781]\tTime 0.001 (0.002)\tLoss 1.1882 (1.2319)\tPrec@1 56.250 (55.988)\n",
      "Epoch: [137][781/781]\tTime 0.002 (0.002)\tLoss 1.7020 (1.2320)\tPrec@1 43.750 (55.984)\n",
      "EPOCH: 137 train Results: Prec@1 55.984 Loss: 1.2320\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.1491 (1.1491)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2430 (1.2867)\tPrec@1 62.500 (54.020)\n",
      "EPOCH: 137 val Results: Prec@1 54.020 Loss: 1.2867\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/781]\tTime 0.003 (0.003)\tLoss 1.2879 (1.2879)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [138][156/781]\tTime 0.008 (0.002)\tLoss 1.2851 (1.1736)\tPrec@1 48.438 (58.250)\n",
      "Epoch: [138][312/781]\tTime 0.002 (0.003)\tLoss 1.2819 (1.1982)\tPrec@1 53.125 (57.368)\n",
      "Epoch: [138][468/781]\tTime 0.002 (0.002)\tLoss 1.3425 (1.2170)\tPrec@1 48.438 (56.636)\n",
      "Epoch: [138][624/781]\tTime 0.002 (0.002)\tLoss 1.3133 (1.2268)\tPrec@1 53.125 (56.233)\n",
      "Epoch: [138][780/781]\tTime 0.001 (0.002)\tLoss 1.4640 (1.2368)\tPrec@1 42.188 (55.804)\n",
      "Epoch: [138][781/781]\tTime 0.002 (0.002)\tLoss 0.9967 (1.2367)\tPrec@1 62.500 (55.806)\n",
      "EPOCH: 138 train Results: Prec@1 55.806 Loss: 1.2367\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.2296 (1.2296)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3338 (1.2814)\tPrec@1 56.250 (54.090)\n",
      "EPOCH: 138 val Results: Prec@1 54.090 Loss: 1.2814\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/781]\tTime 0.002 (0.002)\tLoss 1.3132 (1.3132)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [139][156/781]\tTime 0.001 (0.002)\tLoss 1.2104 (1.1764)\tPrec@1 50.000 (57.633)\n",
      "Epoch: [139][312/781]\tTime 0.001 (0.002)\tLoss 1.0089 (1.1970)\tPrec@1 67.188 (57.423)\n",
      "Epoch: [139][468/781]\tTime 0.003 (0.002)\tLoss 1.2748 (1.2115)\tPrec@1 56.250 (56.883)\n",
      "Epoch: [139][624/781]\tTime 0.002 (0.002)\tLoss 1.2046 (1.2255)\tPrec@1 53.125 (56.562)\n",
      "Epoch: [139][780/781]\tTime 0.001 (0.002)\tLoss 1.2366 (1.2323)\tPrec@1 59.375 (56.242)\n",
      "Epoch: [139][781/781]\tTime 0.002 (0.002)\tLoss 1.5039 (1.2324)\tPrec@1 37.500 (56.236)\n",
      "EPOCH: 139 train Results: Prec@1 56.236 Loss: 1.2324\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3186 (1.3186)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0876 (1.2987)\tPrec@1 56.250 (53.420)\n",
      "EPOCH: 139 val Results: Prec@1 53.420 Loss: 1.2987\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/781]\tTime 0.003 (0.003)\tLoss 1.3642 (1.3642)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [140][156/781]\tTime 0.001 (0.002)\tLoss 1.1501 (1.1583)\tPrec@1 59.375 (58.957)\n",
      "Epoch: [140][312/781]\tTime 0.003 (0.002)\tLoss 1.3178 (1.1920)\tPrec@1 50.000 (57.523)\n",
      "Epoch: [140][468/781]\tTime 0.002 (0.002)\tLoss 1.0134 (1.2048)\tPrec@1 60.938 (57.110)\n",
      "Epoch: [140][624/781]\tTime 0.001 (0.002)\tLoss 1.1563 (1.2173)\tPrec@1 56.250 (56.553)\n",
      "Epoch: [140][780/781]\tTime 0.001 (0.002)\tLoss 1.4630 (1.2293)\tPrec@1 48.438 (56.142)\n",
      "Epoch: [140][781/781]\tTime 0.002 (0.002)\tLoss 1.6766 (1.2294)\tPrec@1 43.750 (56.138)\n",
      "EPOCH: 140 train Results: Prec@1 56.138 Loss: 1.2294\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2437 (1.2437)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1454 (1.2954)\tPrec@1 43.750 (53.190)\n",
      "EPOCH: 140 val Results: Prec@1 53.190 Loss: 1.2954\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/781]\tTime 0.002 (0.002)\tLoss 1.0183 (1.0183)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [141][156/781]\tTime 0.001 (0.002)\tLoss 1.2953 (1.1658)\tPrec@1 57.812 (58.798)\n",
      "Epoch: [141][312/781]\tTime 0.001 (0.002)\tLoss 1.1949 (1.1990)\tPrec@1 53.125 (57.623)\n",
      "Epoch: [141][468/781]\tTime 0.001 (0.002)\tLoss 1.3257 (1.2182)\tPrec@1 54.688 (56.633)\n",
      "Epoch: [141][624/781]\tTime 0.001 (0.002)\tLoss 1.3912 (1.2266)\tPrec@1 50.000 (56.182)\n",
      "Epoch: [141][780/781]\tTime 0.001 (0.002)\tLoss 1.5294 (1.2332)\tPrec@1 45.312 (56.082)\n",
      "Epoch: [141][781/781]\tTime 0.001 (0.002)\tLoss 1.1727 (1.2332)\tPrec@1 50.000 (56.080)\n",
      "EPOCH: 141 train Results: Prec@1 56.080 Loss: 1.2332\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1883 (1.1883)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1866 (1.2786)\tPrec@1 50.000 (54.160)\n",
      "EPOCH: 141 val Results: Prec@1 54.160 Loss: 1.2786\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/781]\tTime 0.003 (0.003)\tLoss 1.0895 (1.0895)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [142][156/781]\tTime 0.002 (0.002)\tLoss 1.1345 (1.1561)\tPrec@1 54.688 (58.838)\n",
      "Epoch: [142][312/781]\tTime 0.002 (0.002)\tLoss 1.3040 (1.1932)\tPrec@1 60.938 (57.638)\n",
      "Epoch: [142][468/781]\tTime 0.002 (0.002)\tLoss 1.1599 (1.2081)\tPrec@1 62.500 (57.070)\n",
      "Epoch: [142][624/781]\tTime 0.001 (0.002)\tLoss 1.4391 (1.2222)\tPrec@1 56.250 (56.492)\n",
      "Epoch: [142][780/781]\tTime 0.002 (0.002)\tLoss 1.3960 (1.2347)\tPrec@1 54.688 (56.102)\n",
      "Epoch: [142][781/781]\tTime 0.002 (0.002)\tLoss 1.0894 (1.2346)\tPrec@1 56.250 (56.102)\n",
      "EPOCH: 142 train Results: Prec@1 56.102 Loss: 1.2346\n",
      "Test: [0/156]\tTime 0.006 (0.006)\tLoss 1.2456 (1.2456)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1233 (1.2904)\tPrec@1 43.750 (53.130)\n",
      "EPOCH: 142 val Results: Prec@1 53.130 Loss: 1.2904\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/781]\tTime 0.002 (0.002)\tLoss 1.1741 (1.1741)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [143][156/781]\tTime 0.001 (0.002)\tLoss 1.1233 (1.1725)\tPrec@1 62.500 (58.549)\n",
      "Epoch: [143][312/781]\tTime 0.001 (0.002)\tLoss 1.3592 (1.1981)\tPrec@1 45.312 (57.548)\n",
      "Epoch: [143][468/781]\tTime 0.002 (0.002)\tLoss 1.4109 (1.2147)\tPrec@1 50.000 (56.823)\n",
      "Epoch: [143][624/781]\tTime 0.001 (0.002)\tLoss 1.3489 (1.2241)\tPrec@1 45.312 (56.557)\n",
      "Epoch: [143][780/781]\tTime 0.002 (0.002)\tLoss 1.3213 (1.2304)\tPrec@1 50.000 (56.432)\n",
      "Epoch: [143][781/781]\tTime 0.002 (0.002)\tLoss 1.2923 (1.2305)\tPrec@1 50.000 (56.430)\n",
      "EPOCH: 143 train Results: Prec@1 56.430 Loss: 1.2305\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1493 (1.1493)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1909 (1.2809)\tPrec@1 50.000 (54.160)\n",
      "EPOCH: 143 val Results: Prec@1 54.160 Loss: 1.2809\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/781]\tTime 0.002 (0.002)\tLoss 1.2133 (1.2133)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [144][156/781]\tTime 0.003 (0.002)\tLoss 1.0883 (1.1797)\tPrec@1 68.750 (58.191)\n",
      "Epoch: [144][312/781]\tTime 0.001 (0.002)\tLoss 1.5336 (1.1961)\tPrec@1 51.562 (57.598)\n",
      "Epoch: [144][468/781]\tTime 0.002 (0.002)\tLoss 1.3010 (1.2108)\tPrec@1 57.812 (57.219)\n",
      "Epoch: [144][624/781]\tTime 0.002 (0.002)\tLoss 1.3117 (1.2199)\tPrec@1 48.438 (56.852)\n",
      "Epoch: [144][780/781]\tTime 0.001 (0.002)\tLoss 1.0976 (1.2273)\tPrec@1 56.250 (56.398)\n",
      "Epoch: [144][781/781]\tTime 0.002 (0.002)\tLoss 1.6562 (1.2274)\tPrec@1 25.000 (56.388)\n",
      "EPOCH: 144 train Results: Prec@1 56.388 Loss: 1.2274\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2093 (1.2093)\tPrec@1 50.000 (50.000)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.5253 (1.2936)\tPrec@1 37.500 (53.700)\n",
      "EPOCH: 144 val Results: Prec@1 53.700 Loss: 1.2936\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/781]\tTime 0.002 (0.002)\tLoss 1.1884 (1.1884)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [145][156/781]\tTime 0.004 (0.003)\tLoss 1.0988 (1.1806)\tPrec@1 56.250 (58.260)\n",
      "Epoch: [145][312/781]\tTime 0.001 (0.002)\tLoss 1.3631 (1.1863)\tPrec@1 50.000 (58.167)\n",
      "Epoch: [145][468/781]\tTime 0.003 (0.002)\tLoss 1.2450 (1.2058)\tPrec@1 48.438 (57.429)\n",
      "Epoch: [145][624/781]\tTime 0.001 (0.002)\tLoss 1.2216 (1.2183)\tPrec@1 57.812 (57.013)\n",
      "Epoch: [145][780/781]\tTime 0.001 (0.002)\tLoss 1.1846 (1.2280)\tPrec@1 59.375 (56.706)\n",
      "Epoch: [145][781/781]\tTime 0.002 (0.002)\tLoss 1.3486 (1.2281)\tPrec@1 56.250 (56.706)\n",
      "EPOCH: 145 train Results: Prec@1 56.706 Loss: 1.2281\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2237 (1.2237)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.4651 (1.2977)\tPrec@1 43.750 (53.760)\n",
      "EPOCH: 145 val Results: Prec@1 53.760 Loss: 1.2977\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/781]\tTime 0.002 (0.002)\tLoss 1.3761 (1.3761)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [146][156/781]\tTime 0.003 (0.002)\tLoss 1.0089 (1.1592)\tPrec@1 67.188 (58.937)\n",
      "Epoch: [146][312/781]\tTime 0.003 (0.002)\tLoss 1.1663 (1.1933)\tPrec@1 59.375 (57.533)\n",
      "Epoch: [146][468/781]\tTime 0.005 (0.002)\tLoss 1.0870 (1.2077)\tPrec@1 65.625 (56.993)\n",
      "Epoch: [146][624/781]\tTime 0.002 (0.002)\tLoss 1.1458 (1.2188)\tPrec@1 62.500 (56.640)\n",
      "Epoch: [146][780/781]\tTime 0.001 (0.002)\tLoss 1.1489 (1.2282)\tPrec@1 60.938 (56.312)\n",
      "Epoch: [146][781/781]\tTime 0.002 (0.002)\tLoss 0.9974 (1.2281)\tPrec@1 56.250 (56.312)\n",
      "EPOCH: 146 train Results: Prec@1 56.312 Loss: 1.2281\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2017 (1.2017)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2191 (1.2844)\tPrec@1 62.500 (54.120)\n",
      "EPOCH: 146 val Results: Prec@1 54.120 Loss: 1.2844\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/781]\tTime 0.004 (0.004)\tLoss 0.9936 (0.9936)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [147][156/781]\tTime 0.003 (0.002)\tLoss 1.3145 (1.1682)\tPrec@1 51.562 (58.380)\n",
      "Epoch: [147][312/781]\tTime 0.001 (0.002)\tLoss 1.3771 (1.2027)\tPrec@1 48.438 (56.999)\n",
      "Epoch: [147][468/781]\tTime 0.002 (0.002)\tLoss 1.0663 (1.2124)\tPrec@1 67.188 (56.743)\n",
      "Epoch: [147][624/781]\tTime 0.001 (0.002)\tLoss 1.2517 (1.2243)\tPrec@1 54.688 (56.362)\n",
      "Epoch: [147][780/781]\tTime 0.001 (0.002)\tLoss 1.1771 (1.2312)\tPrec@1 59.375 (56.086)\n",
      "Epoch: [147][781/781]\tTime 0.002 (0.002)\tLoss 1.7270 (1.2314)\tPrec@1 25.000 (56.076)\n",
      "EPOCH: 147 train Results: Prec@1 56.076 Loss: 1.2314\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1831 (1.1831)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.5369 (1.2889)\tPrec@1 37.500 (53.580)\n",
      "EPOCH: 147 val Results: Prec@1 53.580 Loss: 1.2889\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/781]\tTime 0.003 (0.003)\tLoss 1.1514 (1.1514)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [148][156/781]\tTime 0.003 (0.002)\tLoss 1.1329 (1.1720)\tPrec@1 57.812 (58.439)\n",
      "Epoch: [148][312/781]\tTime 0.001 (0.002)\tLoss 1.3710 (1.2004)\tPrec@1 60.938 (57.583)\n",
      "Epoch: [148][468/781]\tTime 0.002 (0.002)\tLoss 1.2771 (1.2144)\tPrec@1 46.875 (56.746)\n",
      "Epoch: [148][624/781]\tTime 0.002 (0.002)\tLoss 1.2461 (1.2248)\tPrec@1 57.812 (56.532)\n",
      "Epoch: [148][780/781]\tTime 0.001 (0.002)\tLoss 1.2048 (1.2326)\tPrec@1 54.688 (56.264)\n",
      "Epoch: [148][781/781]\tTime 0.003 (0.002)\tLoss 1.2582 (1.2326)\tPrec@1 62.500 (56.266)\n",
      "EPOCH: 148 train Results: Prec@1 56.266 Loss: 1.2326\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2374 (1.2374)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4881 (1.2875)\tPrec@1 31.250 (54.120)\n",
      "EPOCH: 148 val Results: Prec@1 54.120 Loss: 1.2875\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/781]\tTime 0.004 (0.004)\tLoss 0.9735 (0.9735)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [149][156/781]\tTime 0.004 (0.002)\tLoss 1.2469 (1.1572)\tPrec@1 53.125 (59.256)\n",
      "Epoch: [149][312/781]\tTime 0.005 (0.002)\tLoss 1.4019 (1.1877)\tPrec@1 48.438 (57.917)\n",
      "Epoch: [149][468/781]\tTime 0.002 (0.002)\tLoss 1.2164 (1.2072)\tPrec@1 53.125 (57.070)\n",
      "Epoch: [149][624/781]\tTime 0.004 (0.002)\tLoss 1.1614 (1.2201)\tPrec@1 60.938 (56.693)\n",
      "Epoch: [149][780/781]\tTime 0.001 (0.002)\tLoss 1.2961 (1.2311)\tPrec@1 50.000 (56.298)\n",
      "Epoch: [149][781/781]\tTime 0.006 (0.002)\tLoss 1.3268 (1.2311)\tPrec@1 50.000 (56.296)\n",
      "EPOCH: 149 train Results: Prec@1 56.296 Loss: 1.2311\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1805 (1.1805)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3739 (1.2889)\tPrec@1 50.000 (54.450)\n",
      "EPOCH: 149 val Results: Prec@1 54.450 Loss: 1.2889\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/781]\tTime 0.008 (0.008)\tLoss 1.3105 (1.3105)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [150][156/781]\tTime 0.003 (0.002)\tLoss 1.3293 (1.1569)\tPrec@1 56.250 (58.688)\n",
      "Epoch: [150][312/781]\tTime 0.001 (0.002)\tLoss 1.3170 (1.1797)\tPrec@1 56.250 (57.678)\n",
      "Epoch: [150][468/781]\tTime 0.001 (0.002)\tLoss 1.2670 (1.2069)\tPrec@1 56.250 (56.813)\n",
      "Epoch: [150][624/781]\tTime 0.003 (0.002)\tLoss 1.2382 (1.2186)\tPrec@1 57.812 (56.528)\n",
      "Epoch: [150][780/781]\tTime 0.002 (0.002)\tLoss 1.0624 (1.2279)\tPrec@1 62.500 (56.132)\n",
      "Epoch: [150][781/781]\tTime 0.002 (0.002)\tLoss 2.0141 (1.2282)\tPrec@1 43.750 (56.128)\n",
      "EPOCH: 150 train Results: Prec@1 56.128 Loss: 1.2282\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2474 (1.2474)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1976 (1.2940)\tPrec@1 50.000 (53.880)\n",
      "EPOCH: 150 val Results: Prec@1 53.880 Loss: 1.2940\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/781]\tTime 0.002 (0.002)\tLoss 1.2040 (1.2040)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [151][156/781]\tTime 0.003 (0.002)\tLoss 1.0919 (1.1599)\tPrec@1 59.375 (58.310)\n",
      "Epoch: [151][312/781]\tTime 0.001 (0.002)\tLoss 1.2433 (1.1890)\tPrec@1 57.812 (57.433)\n",
      "Epoch: [151][468/781]\tTime 0.003 (0.002)\tLoss 1.2437 (1.2034)\tPrec@1 51.562 (56.920)\n",
      "Epoch: [151][624/781]\tTime 0.010 (0.002)\tLoss 1.1162 (1.2163)\tPrec@1 50.000 (56.410)\n",
      "Epoch: [151][780/781]\tTime 0.001 (0.002)\tLoss 1.2819 (1.2285)\tPrec@1 60.938 (55.920)\n",
      "Epoch: [151][781/781]\tTime 0.002 (0.002)\tLoss 1.6263 (1.2286)\tPrec@1 37.500 (55.914)\n",
      "EPOCH: 151 train Results: Prec@1 55.914 Loss: 1.2286\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1510 (1.1510)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3475 (1.2846)\tPrec@1 56.250 (53.770)\n",
      "EPOCH: 151 val Results: Prec@1 53.770 Loss: 1.2846\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/781]\tTime 0.002 (0.002)\tLoss 1.1465 (1.1465)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [152][156/781]\tTime 0.002 (0.002)\tLoss 1.1112 (1.1754)\tPrec@1 54.688 (57.613)\n",
      "Epoch: [152][312/781]\tTime 0.001 (0.002)\tLoss 1.0329 (1.1970)\tPrec@1 62.500 (56.854)\n",
      "Epoch: [152][468/781]\tTime 0.001 (0.002)\tLoss 1.2404 (1.2118)\tPrec@1 64.062 (56.566)\n",
      "Epoch: [152][624/781]\tTime 0.001 (0.002)\tLoss 1.2903 (1.2210)\tPrec@1 50.000 (56.430)\n",
      "Epoch: [152][780/781]\tTime 0.001 (0.002)\tLoss 1.0801 (1.2271)\tPrec@1 67.188 (56.282)\n",
      "Epoch: [152][781/781]\tTime 0.004 (0.002)\tLoss 1.1146 (1.2271)\tPrec@1 62.500 (56.284)\n",
      "EPOCH: 152 train Results: Prec@1 56.284 Loss: 1.2271\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1654 (1.1654)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2737 (1.2817)\tPrec@1 37.500 (54.280)\n",
      "EPOCH: 152 val Results: Prec@1 54.280 Loss: 1.2817\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/781]\tTime 0.002 (0.002)\tLoss 0.9501 (0.9501)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [153][156/781]\tTime 0.001 (0.002)\tLoss 1.2952 (1.1769)\tPrec@1 50.000 (58.250)\n",
      "Epoch: [153][312/781]\tTime 0.002 (0.002)\tLoss 1.3544 (1.1960)\tPrec@1 46.875 (57.498)\n",
      "Epoch: [153][468/781]\tTime 0.001 (0.002)\tLoss 1.2886 (1.2092)\tPrec@1 59.375 (57.080)\n",
      "Epoch: [153][624/781]\tTime 0.002 (0.002)\tLoss 1.2538 (1.2224)\tPrec@1 59.375 (56.538)\n",
      "Epoch: [153][780/781]\tTime 0.001 (0.002)\tLoss 1.3579 (1.2313)\tPrec@1 43.750 (56.088)\n",
      "Epoch: [153][781/781]\tTime 0.002 (0.002)\tLoss 1.8355 (1.2315)\tPrec@1 43.750 (56.084)\n",
      "EPOCH: 153 train Results: Prec@1 56.084 Loss: 1.2315\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2962 (1.2962)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5475 (1.2887)\tPrec@1 43.750 (53.570)\n",
      "EPOCH: 153 val Results: Prec@1 53.570 Loss: 1.2887\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/781]\tTime 0.002 (0.002)\tLoss 0.9878 (0.9878)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [154][156/781]\tTime 0.001 (0.002)\tLoss 1.2444 (1.1656)\tPrec@1 53.125 (58.509)\n",
      "Epoch: [154][312/781]\tTime 0.001 (0.002)\tLoss 1.2211 (1.1898)\tPrec@1 53.125 (57.673)\n",
      "Epoch: [154][468/781]\tTime 0.002 (0.002)\tLoss 1.3866 (1.2026)\tPrec@1 51.562 (57.026)\n",
      "Epoch: [154][624/781]\tTime 0.001 (0.002)\tLoss 1.2993 (1.2228)\tPrec@1 50.000 (56.523)\n",
      "Epoch: [154][780/781]\tTime 0.002 (0.002)\tLoss 1.3692 (1.2311)\tPrec@1 43.750 (56.256)\n",
      "Epoch: [154][781/781]\tTime 0.002 (0.002)\tLoss 1.7940 (1.2312)\tPrec@1 43.750 (56.252)\n",
      "EPOCH: 154 train Results: Prec@1 56.252 Loss: 1.2312\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.1453 (1.1453)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.4486 (1.2923)\tPrec@1 37.500 (53.580)\n",
      "EPOCH: 154 val Results: Prec@1 53.580 Loss: 1.2923\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/781]\tTime 0.002 (0.002)\tLoss 1.2564 (1.2564)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [155][156/781]\tTime 0.002 (0.002)\tLoss 1.3301 (1.1572)\tPrec@1 56.250 (58.708)\n",
      "Epoch: [155][312/781]\tTime 0.001 (0.002)\tLoss 0.9484 (1.1798)\tPrec@1 68.750 (57.877)\n",
      "Epoch: [155][468/781]\tTime 0.004 (0.002)\tLoss 1.3159 (1.1999)\tPrec@1 57.812 (57.180)\n",
      "Epoch: [155][624/781]\tTime 0.001 (0.002)\tLoss 1.3323 (1.2166)\tPrec@1 43.750 (56.600)\n",
      "Epoch: [155][780/781]\tTime 0.001 (0.002)\tLoss 1.1215 (1.2294)\tPrec@1 64.062 (56.082)\n",
      "Epoch: [155][781/781]\tTime 0.002 (0.002)\tLoss 1.4943 (1.2295)\tPrec@1 50.000 (56.080)\n",
      "EPOCH: 155 train Results: Prec@1 56.080 Loss: 1.2295\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2552 (1.2552)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5509 (1.3105)\tPrec@1 43.750 (53.030)\n",
      "EPOCH: 155 val Results: Prec@1 53.030 Loss: 1.3105\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/781]\tTime 0.003 (0.003)\tLoss 1.2148 (1.2148)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [156][156/781]\tTime 0.001 (0.002)\tLoss 1.1610 (1.1601)\tPrec@1 60.938 (58.838)\n",
      "Epoch: [156][312/781]\tTime 0.003 (0.002)\tLoss 1.2919 (1.1905)\tPrec@1 59.375 (57.543)\n",
      "Epoch: [156][468/781]\tTime 0.002 (0.002)\tLoss 1.2465 (1.2106)\tPrec@1 60.938 (56.773)\n",
      "Epoch: [156][624/781]\tTime 0.003 (0.002)\tLoss 1.3834 (1.2220)\tPrec@1 54.688 (56.468)\n",
      "Epoch: [156][780/781]\tTime 0.001 (0.002)\tLoss 1.3539 (1.2314)\tPrec@1 42.188 (56.018)\n",
      "Epoch: [156][781/781]\tTime 0.006 (0.002)\tLoss 1.3908 (1.2315)\tPrec@1 50.000 (56.016)\n",
      "EPOCH: 156 train Results: Prec@1 56.016 Loss: 1.2315\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2597 (1.2597)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3726 (1.2846)\tPrec@1 50.000 (54.100)\n",
      "EPOCH: 156 val Results: Prec@1 54.100 Loss: 1.2846\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/781]\tTime 0.004 (0.004)\tLoss 1.3707 (1.3707)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [157][156/781]\tTime 0.001 (0.002)\tLoss 1.2448 (1.1616)\tPrec@1 56.250 (58.360)\n",
      "Epoch: [157][312/781]\tTime 0.001 (0.002)\tLoss 1.2158 (1.1920)\tPrec@1 53.125 (57.653)\n",
      "Epoch: [157][468/781]\tTime 0.009 (0.002)\tLoss 1.3754 (1.2132)\tPrec@1 51.562 (56.653)\n",
      "Epoch: [157][624/781]\tTime 0.001 (0.002)\tLoss 1.0941 (1.2215)\tPrec@1 64.062 (56.538)\n",
      "Epoch: [157][780/781]\tTime 0.002 (0.002)\tLoss 1.2996 (1.2279)\tPrec@1 50.000 (56.446)\n",
      "Epoch: [157][781/781]\tTime 0.002 (0.002)\tLoss 1.4821 (1.2280)\tPrec@1 43.750 (56.442)\n",
      "EPOCH: 157 train Results: Prec@1 56.442 Loss: 1.2280\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1549 (1.1549)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.0381 (1.2958)\tPrec@1 56.250 (54.170)\n",
      "EPOCH: 157 val Results: Prec@1 54.170 Loss: 1.2958\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/781]\tTime 0.003 (0.003)\tLoss 1.1111 (1.1111)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [158][156/781]\tTime 0.004 (0.003)\tLoss 1.2288 (1.1707)\tPrec@1 53.125 (58.400)\n",
      "Epoch: [158][312/781]\tTime 0.001 (0.003)\tLoss 1.2632 (1.1912)\tPrec@1 59.375 (57.578)\n",
      "Epoch: [158][468/781]\tTime 0.001 (0.003)\tLoss 1.3050 (1.2125)\tPrec@1 53.125 (56.730)\n",
      "Epoch: [158][624/781]\tTime 0.002 (0.003)\tLoss 1.3497 (1.2211)\tPrec@1 54.688 (56.492)\n",
      "Epoch: [158][780/781]\tTime 0.002 (0.003)\tLoss 1.2016 (1.2305)\tPrec@1 60.938 (56.178)\n",
      "Epoch: [158][781/781]\tTime 0.002 (0.003)\tLoss 1.4063 (1.2306)\tPrec@1 43.750 (56.174)\n",
      "EPOCH: 158 train Results: Prec@1 56.174 Loss: 1.2306\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.0800 (1.0800)\tPrec@1 64.062 (64.062)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2985 (1.2938)\tPrec@1 68.750 (53.700)\n",
      "EPOCH: 158 val Results: Prec@1 53.700 Loss: 1.2938\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/781]\tTime 0.003 (0.003)\tLoss 1.0112 (1.0112)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [159][156/781]\tTime 0.003 (0.002)\tLoss 1.1314 (1.1751)\tPrec@1 56.250 (58.539)\n",
      "Epoch: [159][312/781]\tTime 0.003 (0.002)\tLoss 1.2017 (1.1960)\tPrec@1 53.125 (57.837)\n",
      "Epoch: [159][468/781]\tTime 0.001 (0.002)\tLoss 1.0866 (1.2089)\tPrec@1 59.375 (57.279)\n",
      "Epoch: [159][624/781]\tTime 0.001 (0.002)\tLoss 1.1990 (1.2184)\tPrec@1 54.688 (56.840)\n",
      "Epoch: [159][780/781]\tTime 0.002 (0.002)\tLoss 1.2698 (1.2290)\tPrec@1 48.438 (56.342)\n",
      "Epoch: [159][781/781]\tTime 0.003 (0.002)\tLoss 1.0139 (1.2289)\tPrec@1 43.750 (56.338)\n",
      "EPOCH: 159 train Results: Prec@1 56.338 Loss: 1.2289\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2814 (1.2814)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1714 (1.2845)\tPrec@1 56.250 (54.060)\n",
      "EPOCH: 159 val Results: Prec@1 54.060 Loss: 1.2845\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/781]\tTime 0.002 (0.002)\tLoss 1.2928 (1.2928)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [160][156/781]\tTime 0.001 (0.002)\tLoss 0.9942 (1.1884)\tPrec@1 62.500 (58.012)\n",
      "Epoch: [160][312/781]\tTime 0.001 (0.002)\tLoss 1.3729 (1.2065)\tPrec@1 56.250 (57.208)\n",
      "Epoch: [160][468/781]\tTime 0.001 (0.002)\tLoss 1.1525 (1.2231)\tPrec@1 57.812 (56.570)\n",
      "Epoch: [160][624/781]\tTime 0.003 (0.002)\tLoss 1.2564 (1.2311)\tPrec@1 54.688 (56.270)\n",
      "Epoch: [160][780/781]\tTime 0.001 (0.002)\tLoss 1.1355 (1.2287)\tPrec@1 62.500 (56.344)\n",
      "Epoch: [160][781/781]\tTime 0.002 (0.002)\tLoss 1.4279 (1.2288)\tPrec@1 43.750 (56.340)\n",
      "EPOCH: 160 train Results: Prec@1 56.340 Loss: 1.2288\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1409 (1.1409)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 0.9265 (1.2897)\tPrec@1 68.750 (53.820)\n",
      "EPOCH: 160 val Results: Prec@1 53.820 Loss: 1.2897\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/781]\tTime 0.003 (0.003)\tLoss 1.1069 (1.1069)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [161][156/781]\tTime 0.001 (0.002)\tLoss 1.0160 (1.1626)\tPrec@1 67.188 (58.648)\n",
      "Epoch: [161][312/781]\tTime 0.002 (0.002)\tLoss 1.2021 (1.1882)\tPrec@1 53.125 (57.508)\n",
      "Epoch: [161][468/781]\tTime 0.001 (0.002)\tLoss 1.0735 (1.2083)\tPrec@1 57.812 (56.916)\n",
      "Epoch: [161][624/781]\tTime 0.001 (0.002)\tLoss 1.2825 (1.2195)\tPrec@1 60.938 (56.508)\n",
      "Epoch: [161][780/781]\tTime 0.001 (0.002)\tLoss 1.2439 (1.2300)\tPrec@1 45.312 (56.258)\n",
      "Epoch: [161][781/781]\tTime 0.002 (0.002)\tLoss 1.9105 (1.2302)\tPrec@1 43.750 (56.254)\n",
      "EPOCH: 161 train Results: Prec@1 56.254 Loss: 1.2302\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1502 (1.1502)\tPrec@1 67.188 (67.188)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0629 (1.2837)\tPrec@1 50.000 (53.870)\n",
      "EPOCH: 161 val Results: Prec@1 53.870 Loss: 1.2837\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/781]\tTime 0.003 (0.003)\tLoss 0.9649 (0.9649)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [162][156/781]\tTime 0.001 (0.002)\tLoss 1.3287 (1.1657)\tPrec@1 62.500 (58.280)\n",
      "Epoch: [162][312/781]\tTime 0.002 (0.002)\tLoss 1.3361 (1.1959)\tPrec@1 53.125 (57.363)\n",
      "Epoch: [162][468/781]\tTime 0.001 (0.002)\tLoss 1.1026 (1.2070)\tPrec@1 59.375 (56.993)\n",
      "Epoch: [162][624/781]\tTime 0.002 (0.002)\tLoss 1.3469 (1.2163)\tPrec@1 51.562 (56.553)\n",
      "Epoch: [162][780/781]\tTime 0.005 (0.002)\tLoss 1.0467 (1.2243)\tPrec@1 60.938 (56.320)\n",
      "Epoch: [162][781/781]\tTime 0.010 (0.002)\tLoss 1.2391 (1.2243)\tPrec@1 50.000 (56.318)\n",
      "EPOCH: 162 train Results: Prec@1 56.318 Loss: 1.2243\n",
      "Test: [0/156]\tTime 0.005 (0.005)\tLoss 1.1761 (1.1761)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.001 (0.000)\tLoss 1.4703 (1.2963)\tPrec@1 37.500 (53.510)\n",
      "EPOCH: 162 val Results: Prec@1 53.510 Loss: 1.2963\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/781]\tTime 0.002 (0.002)\tLoss 1.3198 (1.3198)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [163][156/781]\tTime 0.001 (0.002)\tLoss 1.0968 (1.1755)\tPrec@1 54.688 (58.081)\n",
      "Epoch: [163][312/781]\tTime 0.001 (0.002)\tLoss 1.0434 (1.1884)\tPrec@1 62.500 (58.017)\n",
      "Epoch: [163][468/781]\tTime 0.001 (0.002)\tLoss 1.1837 (1.2072)\tPrec@1 56.250 (57.199)\n",
      "Epoch: [163][624/781]\tTime 0.002 (0.002)\tLoss 1.3874 (1.2205)\tPrec@1 54.688 (56.672)\n",
      "Epoch: [163][780/781]\tTime 0.001 (0.002)\tLoss 1.2863 (1.2305)\tPrec@1 54.688 (56.270)\n",
      "Epoch: [163][781/781]\tTime 0.002 (0.002)\tLoss 1.1823 (1.2304)\tPrec@1 56.250 (56.270)\n",
      "EPOCH: 163 train Results: Prec@1 56.270 Loss: 1.2304\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1221 (1.1221)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0129 (1.2717)\tPrec@1 62.500 (54.550)\n",
      "EPOCH: 163 val Results: Prec@1 54.550 Loss: 1.2717\n",
      "Best Prec@1: 54.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/781]\tTime 0.002 (0.002)\tLoss 1.0605 (1.0605)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [164][156/781]\tTime 0.001 (0.002)\tLoss 1.2793 (1.1626)\tPrec@1 53.125 (58.559)\n",
      "Epoch: [164][312/781]\tTime 0.002 (0.002)\tLoss 1.3363 (1.1883)\tPrec@1 54.688 (57.763)\n",
      "Epoch: [164][468/781]\tTime 0.002 (0.002)\tLoss 1.2416 (1.2055)\tPrec@1 56.250 (57.003)\n",
      "Epoch: [164][624/781]\tTime 0.001 (0.002)\tLoss 1.2558 (1.2213)\tPrec@1 59.375 (56.453)\n",
      "Epoch: [164][780/781]\tTime 0.002 (0.002)\tLoss 1.0656 (1.2294)\tPrec@1 65.625 (56.144)\n",
      "Epoch: [164][781/781]\tTime 0.002 (0.002)\tLoss 1.6813 (1.2295)\tPrec@1 50.000 (56.142)\n",
      "EPOCH: 164 train Results: Prec@1 56.142 Loss: 1.2295\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3160 (1.3160)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3933 (1.2777)\tPrec@1 31.250 (54.970)\n",
      "EPOCH: 164 val Results: Prec@1 54.970 Loss: 1.2777\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/781]\tTime 0.003 (0.003)\tLoss 1.1810 (1.1810)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [165][156/781]\tTime 0.002 (0.002)\tLoss 1.3507 (1.1456)\tPrec@1 50.000 (59.435)\n",
      "Epoch: [165][312/781]\tTime 0.001 (0.002)\tLoss 1.3476 (1.1883)\tPrec@1 51.562 (57.708)\n",
      "Epoch: [165][468/781]\tTime 0.004 (0.002)\tLoss 1.3228 (1.2067)\tPrec@1 56.250 (57.043)\n",
      "Epoch: [165][624/781]\tTime 0.001 (0.002)\tLoss 1.2807 (1.2188)\tPrec@1 54.688 (56.562)\n",
      "Epoch: [165][780/781]\tTime 0.003 (0.002)\tLoss 1.4655 (1.2290)\tPrec@1 54.688 (56.110)\n",
      "Epoch: [165][781/781]\tTime 0.002 (0.002)\tLoss 1.8400 (1.2292)\tPrec@1 31.250 (56.102)\n",
      "EPOCH: 165 train Results: Prec@1 56.102 Loss: 1.2292\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1658 (1.1658)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1491 (1.3063)\tPrec@1 50.000 (52.780)\n",
      "EPOCH: 165 val Results: Prec@1 52.780 Loss: 1.3063\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/781]\tTime 0.003 (0.003)\tLoss 1.1761 (1.1761)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [166][156/781]\tTime 0.001 (0.002)\tLoss 1.1251 (1.1633)\tPrec@1 54.688 (58.260)\n",
      "Epoch: [166][312/781]\tTime 0.002 (0.002)\tLoss 1.1715 (1.1968)\tPrec@1 56.250 (57.109)\n",
      "Epoch: [166][468/781]\tTime 0.002 (0.002)\tLoss 1.4258 (1.2163)\tPrec@1 57.812 (56.620)\n",
      "Epoch: [166][624/781]\tTime 0.002 (0.002)\tLoss 1.3550 (1.2243)\tPrec@1 60.938 (56.350)\n",
      "Epoch: [166][780/781]\tTime 0.001 (0.002)\tLoss 1.0954 (1.2330)\tPrec@1 59.375 (55.990)\n",
      "Epoch: [166][781/781]\tTime 0.002 (0.002)\tLoss 1.4504 (1.2330)\tPrec@1 50.000 (55.988)\n",
      "EPOCH: 166 train Results: Prec@1 55.988 Loss: 1.2330\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2065 (1.2065)\tPrec@1 65.625 (65.625)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1668 (1.2875)\tPrec@1 43.750 (54.050)\n",
      "EPOCH: 166 val Results: Prec@1 54.050 Loss: 1.2875\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/781]\tTime 0.003 (0.003)\tLoss 1.5848 (1.5848)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [167][156/781]\tTime 0.002 (0.003)\tLoss 1.2147 (1.1758)\tPrec@1 54.688 (58.390)\n",
      "Epoch: [167][312/781]\tTime 0.002 (0.003)\tLoss 1.1713 (1.1906)\tPrec@1 62.500 (57.583)\n",
      "Epoch: [167][468/781]\tTime 0.002 (0.003)\tLoss 1.3583 (1.2098)\tPrec@1 46.875 (57.000)\n",
      "Epoch: [167][624/781]\tTime 0.001 (0.002)\tLoss 1.5838 (1.2219)\tPrec@1 45.312 (56.492)\n",
      "Epoch: [167][780/781]\tTime 0.002 (0.002)\tLoss 1.2984 (1.2316)\tPrec@1 56.250 (56.086)\n",
      "Epoch: [167][781/781]\tTime 0.002 (0.002)\tLoss 1.3532 (1.2316)\tPrec@1 43.750 (56.082)\n",
      "EPOCH: 167 train Results: Prec@1 56.082 Loss: 1.2316\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1115 (1.1115)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 0.8879 (1.2875)\tPrec@1 62.500 (54.110)\n",
      "EPOCH: 167 val Results: Prec@1 54.110 Loss: 1.2875\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/781]\tTime 0.003 (0.003)\tLoss 1.1903 (1.1903)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [168][156/781]\tTime 0.002 (0.003)\tLoss 1.0010 (1.1626)\tPrec@1 71.875 (58.738)\n",
      "Epoch: [168][312/781]\tTime 0.001 (0.003)\tLoss 1.2031 (1.1925)\tPrec@1 54.688 (57.738)\n",
      "Epoch: [168][468/781]\tTime 0.001 (0.003)\tLoss 1.0642 (1.2072)\tPrec@1 53.125 (57.296)\n",
      "Epoch: [168][624/781]\tTime 0.001 (0.002)\tLoss 1.4517 (1.2241)\tPrec@1 50.000 (56.593)\n",
      "Epoch: [168][780/781]\tTime 0.001 (0.002)\tLoss 1.2467 (1.2335)\tPrec@1 48.438 (56.208)\n",
      "Epoch: [168][781/781]\tTime 0.002 (0.002)\tLoss 1.6305 (1.2336)\tPrec@1 43.750 (56.204)\n",
      "EPOCH: 168 train Results: Prec@1 56.204 Loss: 1.2336\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1083 (1.1083)\tPrec@1 65.625 (65.625)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1294 (1.2767)\tPrec@1 56.250 (54.140)\n",
      "EPOCH: 168 val Results: Prec@1 54.140 Loss: 1.2767\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/781]\tTime 0.005 (0.005)\tLoss 0.8676 (0.8676)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [169][156/781]\tTime 0.002 (0.002)\tLoss 1.1366 (1.1784)\tPrec@1 56.250 (58.728)\n",
      "Epoch: [169][312/781]\tTime 0.001 (0.002)\tLoss 1.1542 (1.1941)\tPrec@1 60.938 (58.002)\n",
      "Epoch: [169][468/781]\tTime 0.005 (0.002)\tLoss 1.1988 (1.2132)\tPrec@1 60.938 (57.080)\n",
      "Epoch: [169][624/781]\tTime 0.003 (0.002)\tLoss 1.2322 (1.2249)\tPrec@1 57.812 (56.665)\n",
      "Epoch: [169][780/781]\tTime 0.002 (0.002)\tLoss 1.1171 (1.2316)\tPrec@1 59.375 (56.522)\n",
      "Epoch: [169][781/781]\tTime 0.001 (0.002)\tLoss 1.3357 (1.2316)\tPrec@1 43.750 (56.518)\n",
      "EPOCH: 169 train Results: Prec@1 56.518 Loss: 1.2316\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1443 (1.1443)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2090 (1.3024)\tPrec@1 56.250 (53.320)\n",
      "EPOCH: 169 val Results: Prec@1 53.320 Loss: 1.3024\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/781]\tTime 0.009 (0.009)\tLoss 1.1952 (1.1952)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [170][156/781]\tTime 0.002 (0.003)\tLoss 1.2729 (1.1668)\tPrec@1 54.688 (58.559)\n",
      "Epoch: [170][312/781]\tTime 0.001 (0.003)\tLoss 1.3093 (1.1991)\tPrec@1 56.250 (57.169)\n",
      "Epoch: [170][468/781]\tTime 0.001 (0.002)\tLoss 1.2743 (1.2125)\tPrec@1 64.062 (56.673)\n",
      "Epoch: [170][624/781]\tTime 0.002 (0.002)\tLoss 1.3142 (1.2268)\tPrec@1 53.125 (56.080)\n",
      "Epoch: [170][780/781]\tTime 0.001 (0.002)\tLoss 1.0769 (1.2310)\tPrec@1 62.500 (55.988)\n",
      "Epoch: [170][781/781]\tTime 0.002 (0.002)\tLoss 1.7992 (1.2312)\tPrec@1 31.250 (55.980)\n",
      "EPOCH: 170 train Results: Prec@1 55.980 Loss: 1.2312\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1720 (1.1720)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.0855 (1.2686)\tPrec@1 50.000 (54.900)\n",
      "EPOCH: 170 val Results: Prec@1 54.900 Loss: 1.2686\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/781]\tTime 0.003 (0.003)\tLoss 1.3041 (1.3041)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [171][156/781]\tTime 0.003 (0.003)\tLoss 1.1752 (1.1778)\tPrec@1 56.250 (58.240)\n",
      "Epoch: [171][312/781]\tTime 0.003 (0.003)\tLoss 1.4436 (1.2039)\tPrec@1 50.000 (57.198)\n",
      "Epoch: [171][468/781]\tTime 0.003 (0.002)\tLoss 1.0567 (1.2121)\tPrec@1 60.938 (56.846)\n",
      "Epoch: [171][624/781]\tTime 0.001 (0.003)\tLoss 1.1406 (1.2194)\tPrec@1 60.938 (56.648)\n",
      "Epoch: [171][780/781]\tTime 0.001 (0.003)\tLoss 1.3964 (1.2286)\tPrec@1 50.000 (56.236)\n",
      "Epoch: [171][781/781]\tTime 0.002 (0.003)\tLoss 1.3180 (1.2286)\tPrec@1 50.000 (56.234)\n",
      "EPOCH: 171 train Results: Prec@1 56.234 Loss: 1.2286\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2037 (1.2037)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2346 (1.2882)\tPrec@1 37.500 (53.480)\n",
      "EPOCH: 171 val Results: Prec@1 53.480 Loss: 1.2882\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/781]\tTime 0.003 (0.003)\tLoss 1.1309 (1.1309)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [172][156/781]\tTime 0.002 (0.002)\tLoss 1.1470 (1.1692)\tPrec@1 48.438 (58.917)\n",
      "Epoch: [172][312/781]\tTime 0.001 (0.002)\tLoss 1.1267 (1.1914)\tPrec@1 57.812 (57.892)\n",
      "Epoch: [172][468/781]\tTime 0.002 (0.002)\tLoss 1.2188 (1.2079)\tPrec@1 57.812 (57.226)\n",
      "Epoch: [172][624/781]\tTime 0.002 (0.002)\tLoss 1.3721 (1.2191)\tPrec@1 53.125 (56.697)\n",
      "Epoch: [172][780/781]\tTime 0.001 (0.002)\tLoss 1.1917 (1.2282)\tPrec@1 59.375 (56.324)\n",
      "Epoch: [172][781/781]\tTime 0.002 (0.002)\tLoss 1.1588 (1.2282)\tPrec@1 56.250 (56.324)\n",
      "EPOCH: 172 train Results: Prec@1 56.324 Loss: 1.2282\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1722 (1.1722)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2176 (1.2842)\tPrec@1 56.250 (53.950)\n",
      "EPOCH: 172 val Results: Prec@1 53.950 Loss: 1.2842\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/781]\tTime 0.002 (0.002)\tLoss 1.3435 (1.3435)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [173][156/781]\tTime 0.001 (0.002)\tLoss 1.3613 (1.1792)\tPrec@1 56.250 (58.539)\n",
      "Epoch: [173][312/781]\tTime 0.003 (0.002)\tLoss 1.3651 (1.1989)\tPrec@1 54.688 (57.508)\n",
      "Epoch: [173][468/781]\tTime 0.003 (0.002)\tLoss 1.1426 (1.2057)\tPrec@1 60.938 (57.196)\n",
      "Epoch: [173][624/781]\tTime 0.001 (0.002)\tLoss 1.2490 (1.2176)\tPrec@1 56.250 (56.587)\n",
      "Epoch: [173][780/781]\tTime 0.004 (0.002)\tLoss 1.2415 (1.2294)\tPrec@1 56.250 (56.172)\n",
      "Epoch: [173][781/781]\tTime 0.002 (0.002)\tLoss 1.4552 (1.2295)\tPrec@1 43.750 (56.168)\n",
      "EPOCH: 173 train Results: Prec@1 56.168 Loss: 1.2295\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3013 (1.3013)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.001 (0.000)\tLoss 1.5494 (1.2886)\tPrec@1 43.750 (53.890)\n",
      "EPOCH: 173 val Results: Prec@1 53.890 Loss: 1.2886\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/781]\tTime 0.006 (0.006)\tLoss 1.1518 (1.1518)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [174][156/781]\tTime 0.001 (0.002)\tLoss 1.2468 (1.1668)\tPrec@1 54.688 (58.648)\n",
      "Epoch: [174][312/781]\tTime 0.003 (0.002)\tLoss 1.0399 (1.1953)\tPrec@1 62.500 (57.428)\n",
      "Epoch: [174][468/781]\tTime 0.002 (0.002)\tLoss 1.2227 (1.2083)\tPrec@1 59.375 (56.750)\n",
      "Epoch: [174][624/781]\tTime 0.003 (0.002)\tLoss 1.2818 (1.2205)\tPrec@1 65.625 (56.438)\n",
      "Epoch: [174][780/781]\tTime 0.002 (0.002)\tLoss 1.2631 (1.2311)\tPrec@1 56.250 (56.094)\n",
      "Epoch: [174][781/781]\tTime 0.003 (0.002)\tLoss 1.1740 (1.2311)\tPrec@1 56.250 (56.094)\n",
      "EPOCH: 174 train Results: Prec@1 56.094 Loss: 1.2311\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1691 (1.1691)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.3713 (1.2891)\tPrec@1 43.750 (53.400)\n",
      "EPOCH: 174 val Results: Prec@1 53.400 Loss: 1.2891\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/781]\tTime 0.003 (0.003)\tLoss 1.0340 (1.0340)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [175][156/781]\tTime 0.006 (0.002)\tLoss 1.3806 (1.1656)\tPrec@1 50.000 (58.678)\n",
      "Epoch: [175][312/781]\tTime 0.001 (0.002)\tLoss 1.2452 (1.1909)\tPrec@1 53.125 (57.678)\n",
      "Epoch: [175][468/781]\tTime 0.002 (0.002)\tLoss 1.3646 (1.2092)\tPrec@1 56.250 (57.036)\n",
      "Epoch: [175][624/781]\tTime 0.001 (0.002)\tLoss 1.2607 (1.2221)\tPrec@1 59.375 (56.580)\n",
      "Epoch: [175][780/781]\tTime 0.001 (0.002)\tLoss 1.2326 (1.2336)\tPrec@1 59.375 (56.068)\n",
      "Epoch: [175][781/781]\tTime 0.002 (0.002)\tLoss 1.8048 (1.2338)\tPrec@1 43.750 (56.064)\n",
      "EPOCH: 175 train Results: Prec@1 56.064 Loss: 1.2338\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2620 (1.2620)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2946 (1.2871)\tPrec@1 56.250 (53.880)\n",
      "EPOCH: 175 val Results: Prec@1 53.880 Loss: 1.2871\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/781]\tTime 0.002 (0.002)\tLoss 1.1167 (1.1167)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [176][156/781]\tTime 0.001 (0.002)\tLoss 1.2790 (1.1824)\tPrec@1 56.250 (57.982)\n",
      "Epoch: [176][312/781]\tTime 0.001 (0.002)\tLoss 1.2939 (1.2059)\tPrec@1 51.562 (57.184)\n",
      "Epoch: [176][468/781]\tTime 0.002 (0.002)\tLoss 1.1554 (1.2139)\tPrec@1 54.688 (56.760)\n",
      "Epoch: [176][624/781]\tTime 0.001 (0.002)\tLoss 1.1692 (1.2269)\tPrec@1 56.250 (56.193)\n",
      "Epoch: [176][780/781]\tTime 0.003 (0.002)\tLoss 1.1534 (1.2321)\tPrec@1 53.125 (56.026)\n",
      "Epoch: [176][781/781]\tTime 0.002 (0.002)\tLoss 1.2616 (1.2321)\tPrec@1 56.250 (56.026)\n",
      "EPOCH: 176 train Results: Prec@1 56.026 Loss: 1.2321\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.0901 (1.0901)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.2861 (1.2727)\tPrec@1 37.500 (54.640)\n",
      "EPOCH: 176 val Results: Prec@1 54.640 Loss: 1.2727\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/781]\tTime 0.002 (0.002)\tLoss 1.3961 (1.3961)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [177][156/781]\tTime 0.001 (0.002)\tLoss 1.0492 (1.1759)\tPrec@1 62.500 (58.857)\n",
      "Epoch: [177][312/781]\tTime 0.001 (0.002)\tLoss 1.3489 (1.2071)\tPrec@1 51.562 (57.488)\n",
      "Epoch: [177][468/781]\tTime 0.001 (0.002)\tLoss 1.1746 (1.2174)\tPrec@1 59.375 (56.996)\n",
      "Epoch: [177][624/781]\tTime 0.004 (0.002)\tLoss 1.1624 (1.2289)\tPrec@1 62.500 (56.455)\n",
      "Epoch: [177][780/781]\tTime 0.008 (0.002)\tLoss 1.2865 (1.2339)\tPrec@1 59.375 (56.072)\n",
      "Epoch: [177][781/781]\tTime 0.001 (0.002)\tLoss 1.2978 (1.2339)\tPrec@1 43.750 (56.068)\n",
      "EPOCH: 177 train Results: Prec@1 56.068 Loss: 1.2339\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3010 (1.3010)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.7289 (1.2973)\tPrec@1 37.500 (53.480)\n",
      "EPOCH: 177 val Results: Prec@1 53.480 Loss: 1.2973\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/781]\tTime 0.002 (0.002)\tLoss 1.1951 (1.1951)\tPrec@1 50.000 (50.000)\n",
      "Epoch: [178][156/781]\tTime 0.001 (0.002)\tLoss 1.2253 (1.1684)\tPrec@1 57.812 (58.469)\n",
      "Epoch: [178][312/781]\tTime 0.002 (0.002)\tLoss 1.1351 (1.1957)\tPrec@1 57.812 (57.418)\n",
      "Epoch: [178][468/781]\tTime 0.002 (0.002)\tLoss 1.2835 (1.2175)\tPrec@1 54.688 (56.463)\n",
      "Epoch: [178][624/781]\tTime 0.004 (0.002)\tLoss 1.1586 (1.2255)\tPrec@1 60.938 (56.367)\n",
      "Epoch: [178][780/781]\tTime 0.006 (0.002)\tLoss 1.2145 (1.2325)\tPrec@1 54.688 (56.196)\n",
      "Epoch: [178][781/781]\tTime 0.002 (0.002)\tLoss 1.8410 (1.2327)\tPrec@1 43.750 (56.192)\n",
      "EPOCH: 178 train Results: Prec@1 56.192 Loss: 1.2327\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2652 (1.2652)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4190 (1.2840)\tPrec@1 43.750 (53.950)\n",
      "EPOCH: 178 val Results: Prec@1 53.950 Loss: 1.2840\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/781]\tTime 0.004 (0.004)\tLoss 1.3527 (1.3527)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [179][156/781]\tTime 0.009 (0.002)\tLoss 1.2293 (1.1555)\tPrec@1 56.250 (59.335)\n",
      "Epoch: [179][312/781]\tTime 0.001 (0.002)\tLoss 1.4824 (1.1883)\tPrec@1 50.000 (58.007)\n",
      "Epoch: [179][468/781]\tTime 0.001 (0.002)\tLoss 1.3811 (1.2139)\tPrec@1 45.312 (56.970)\n",
      "Epoch: [179][624/781]\tTime 0.002 (0.002)\tLoss 1.0644 (1.2236)\tPrec@1 59.375 (56.542)\n",
      "Epoch: [179][780/781]\tTime 0.002 (0.002)\tLoss 1.4876 (1.2321)\tPrec@1 46.875 (56.226)\n",
      "Epoch: [179][781/781]\tTime 0.002 (0.002)\tLoss 1.3244 (1.2321)\tPrec@1 56.250 (56.226)\n",
      "EPOCH: 179 train Results: Prec@1 56.226 Loss: 1.2321\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2852 (1.2852)\tPrec@1 54.688 (54.688)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2016 (1.2823)\tPrec@1 56.250 (54.360)\n",
      "EPOCH: 179 val Results: Prec@1 54.360 Loss: 1.2823\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/781]\tTime 0.005 (0.005)\tLoss 1.1168 (1.1168)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [180][156/781]\tTime 0.002 (0.002)\tLoss 1.1590 (1.1716)\tPrec@1 64.062 (58.310)\n",
      "Epoch: [180][312/781]\tTime 0.002 (0.002)\tLoss 1.2437 (1.1972)\tPrec@1 54.688 (57.094)\n",
      "Epoch: [180][468/781]\tTime 0.002 (0.002)\tLoss 1.1982 (1.2128)\tPrec@1 54.688 (56.666)\n",
      "Epoch: [180][624/781]\tTime 0.001 (0.002)\tLoss 1.0194 (1.2248)\tPrec@1 64.062 (56.250)\n",
      "Epoch: [180][780/781]\tTime 0.003 (0.002)\tLoss 1.3800 (1.2321)\tPrec@1 43.750 (56.046)\n",
      "Epoch: [180][781/781]\tTime 0.003 (0.002)\tLoss 1.8838 (1.2323)\tPrec@1 37.500 (56.040)\n",
      "EPOCH: 180 train Results: Prec@1 56.040 Loss: 1.2323\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2626 (1.2626)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.002 (0.001)\tLoss 1.2414 (1.2960)\tPrec@1 56.250 (53.610)\n",
      "EPOCH: 180 val Results: Prec@1 53.610 Loss: 1.2960\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/781]\tTime 0.002 (0.002)\tLoss 1.1756 (1.1756)\tPrec@1 56.250 (56.250)\n",
      "Epoch: [181][156/781]\tTime 0.012 (0.003)\tLoss 1.2234 (1.1645)\tPrec@1 59.375 (58.519)\n",
      "Epoch: [181][312/781]\tTime 0.002 (0.003)\tLoss 1.0479 (1.1967)\tPrec@1 62.500 (57.363)\n",
      "Epoch: [181][468/781]\tTime 0.001 (0.003)\tLoss 1.3096 (1.2193)\tPrec@1 53.125 (56.497)\n",
      "Epoch: [181][624/781]\tTime 0.001 (0.003)\tLoss 1.1866 (1.2289)\tPrec@1 60.938 (56.108)\n",
      "Epoch: [181][780/781]\tTime 0.001 (0.003)\tLoss 1.2091 (1.2351)\tPrec@1 59.375 (55.936)\n",
      "Epoch: [181][781/781]\tTime 0.002 (0.003)\tLoss 2.2889 (1.2354)\tPrec@1 25.000 (55.926)\n",
      "EPOCH: 181 train Results: Prec@1 55.926 Loss: 1.2354\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1760 (1.1760)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2689 (1.2806)\tPrec@1 37.500 (54.350)\n",
      "EPOCH: 181 val Results: Prec@1 54.350 Loss: 1.2806\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/781]\tTime 0.002 (0.002)\tLoss 1.0973 (1.0973)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [182][156/781]\tTime 0.001 (0.002)\tLoss 1.2194 (1.1743)\tPrec@1 48.438 (58.270)\n",
      "Epoch: [182][312/781]\tTime 0.001 (0.002)\tLoss 1.1626 (1.1952)\tPrec@1 64.062 (57.548)\n",
      "Epoch: [182][468/781]\tTime 0.002 (0.002)\tLoss 1.3534 (1.2166)\tPrec@1 48.438 (56.736)\n",
      "Epoch: [182][624/781]\tTime 0.001 (0.002)\tLoss 1.2002 (1.2263)\tPrec@1 57.812 (56.315)\n",
      "Epoch: [182][780/781]\tTime 0.001 (0.002)\tLoss 1.3803 (1.2314)\tPrec@1 53.125 (56.056)\n",
      "Epoch: [182][781/781]\tTime 0.002 (0.002)\tLoss 1.5500 (1.2315)\tPrec@1 37.500 (56.050)\n",
      "EPOCH: 182 train Results: Prec@1 56.050 Loss: 1.2315\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2466 (1.2466)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4220 (1.2838)\tPrec@1 31.250 (54.230)\n",
      "EPOCH: 182 val Results: Prec@1 54.230 Loss: 1.2838\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/781]\tTime 0.002 (0.002)\tLoss 1.1229 (1.1229)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [183][156/781]\tTime 0.001 (0.002)\tLoss 1.1723 (1.1674)\tPrec@1 51.562 (58.171)\n",
      "Epoch: [183][312/781]\tTime 0.002 (0.002)\tLoss 1.2178 (1.1941)\tPrec@1 51.562 (57.588)\n",
      "Epoch: [183][468/781]\tTime 0.001 (0.002)\tLoss 1.3167 (1.2158)\tPrec@1 46.875 (56.766)\n",
      "Epoch: [183][624/781]\tTime 0.001 (0.002)\tLoss 1.3116 (1.2243)\tPrec@1 51.562 (56.472)\n",
      "Epoch: [183][780/781]\tTime 0.001 (0.002)\tLoss 1.4001 (1.2343)\tPrec@1 56.250 (56.086)\n",
      "Epoch: [183][781/781]\tTime 0.001 (0.002)\tLoss 1.3463 (1.2344)\tPrec@1 37.500 (56.080)\n",
      "EPOCH: 183 train Results: Prec@1 56.080 Loss: 1.2344\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1354 (1.1354)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5804 (1.2869)\tPrec@1 43.750 (53.840)\n",
      "EPOCH: 183 val Results: Prec@1 53.840 Loss: 1.2869\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/781]\tTime 0.002 (0.002)\tLoss 0.8956 (0.8956)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [184][156/781]\tTime 0.002 (0.002)\tLoss 1.4765 (1.1667)\tPrec@1 42.188 (58.469)\n",
      "Epoch: [184][312/781]\tTime 0.001 (0.002)\tLoss 1.2524 (1.1882)\tPrec@1 54.688 (57.628)\n",
      "Epoch: [184][468/781]\tTime 0.002 (0.002)\tLoss 1.3494 (1.2096)\tPrec@1 45.312 (57.086)\n",
      "Epoch: [184][624/781]\tTime 0.001 (0.002)\tLoss 1.2696 (1.2226)\tPrec@1 50.000 (56.428)\n",
      "Epoch: [184][780/781]\tTime 0.002 (0.002)\tLoss 1.2490 (1.2315)\tPrec@1 60.938 (56.044)\n",
      "Epoch: [184][781/781]\tTime 0.002 (0.002)\tLoss 1.9869 (1.2318)\tPrec@1 37.500 (56.038)\n",
      "EPOCH: 184 train Results: Prec@1 56.038 Loss: 1.2318\n",
      "Test: [0/156]\tTime 0.000 (0.000)\tLoss 1.1575 (1.1575)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.0904 (1.2816)\tPrec@1 56.250 (54.300)\n",
      "EPOCH: 184 val Results: Prec@1 54.300 Loss: 1.2816\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/781]\tTime 0.002 (0.002)\tLoss 1.1743 (1.1743)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [185][156/781]\tTime 0.007 (0.002)\tLoss 1.2990 (1.1747)\tPrec@1 53.125 (57.992)\n",
      "Epoch: [185][312/781]\tTime 0.007 (0.002)\tLoss 1.2249 (1.1942)\tPrec@1 62.500 (57.283)\n",
      "Epoch: [185][468/781]\tTime 0.001 (0.002)\tLoss 1.2668 (1.2043)\tPrec@1 50.000 (57.083)\n",
      "Epoch: [185][624/781]\tTime 0.001 (0.002)\tLoss 1.0328 (1.2137)\tPrec@1 70.312 (56.818)\n",
      "Epoch: [185][780/781]\tTime 0.001 (0.002)\tLoss 1.2292 (1.2295)\tPrec@1 53.125 (56.138)\n",
      "Epoch: [185][781/781]\tTime 0.004 (0.002)\tLoss 1.1381 (1.2295)\tPrec@1 56.250 (56.138)\n",
      "EPOCH: 185 train Results: Prec@1 56.138 Loss: 1.2295\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3485 (1.3485)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1074 (1.2821)\tPrec@1 37.500 (53.870)\n",
      "EPOCH: 185 val Results: Prec@1 53.870 Loss: 1.2821\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/781]\tTime 0.003 (0.003)\tLoss 1.0261 (1.0261)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [186][156/781]\tTime 0.001 (0.003)\tLoss 1.3954 (1.1691)\tPrec@1 50.000 (58.818)\n",
      "Epoch: [186][312/781]\tTime 0.003 (0.002)\tLoss 1.2556 (1.1932)\tPrec@1 53.125 (57.703)\n",
      "Epoch: [186][468/781]\tTime 0.003 (0.002)\tLoss 1.4426 (1.2111)\tPrec@1 46.875 (56.953)\n",
      "Epoch: [186][624/781]\tTime 0.001 (0.002)\tLoss 1.3215 (1.2264)\tPrec@1 59.375 (56.417)\n",
      "Epoch: [186][780/781]\tTime 0.001 (0.002)\tLoss 0.9318 (1.2309)\tPrec@1 68.750 (56.318)\n",
      "Epoch: [186][781/781]\tTime 0.002 (0.002)\tLoss 1.4884 (1.2310)\tPrec@1 50.000 (56.316)\n",
      "EPOCH: 186 train Results: Prec@1 56.316 Loss: 1.2310\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3068 (1.3068)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5639 (1.2840)\tPrec@1 50.000 (54.120)\n",
      "EPOCH: 186 val Results: Prec@1 54.120 Loss: 1.2840\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/781]\tTime 0.002 (0.002)\tLoss 1.2561 (1.2561)\tPrec@1 46.875 (46.875)\n",
      "Epoch: [187][156/781]\tTime 0.002 (0.002)\tLoss 1.0777 (1.1699)\tPrec@1 64.062 (58.718)\n",
      "Epoch: [187][312/781]\tTime 0.002 (0.002)\tLoss 1.3354 (1.1978)\tPrec@1 53.125 (57.298)\n",
      "Epoch: [187][468/781]\tTime 0.003 (0.002)\tLoss 1.1040 (1.2104)\tPrec@1 59.375 (56.833)\n",
      "Epoch: [187][624/781]\tTime 0.001 (0.002)\tLoss 1.2108 (1.2269)\tPrec@1 62.500 (56.380)\n",
      "Epoch: [187][780/781]\tTime 0.001 (0.002)\tLoss 1.1269 (1.2366)\tPrec@1 59.375 (56.116)\n",
      "Epoch: [187][781/781]\tTime 0.002 (0.002)\tLoss 1.7163 (1.2367)\tPrec@1 37.500 (56.110)\n",
      "EPOCH: 187 train Results: Prec@1 56.110 Loss: 1.2367\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2065 (1.2065)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.2523 (1.2862)\tPrec@1 43.750 (53.910)\n",
      "EPOCH: 187 val Results: Prec@1 53.910 Loss: 1.2862\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/781]\tTime 0.002 (0.002)\tLoss 0.8726 (0.8726)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [188][156/781]\tTime 0.002 (0.002)\tLoss 1.4053 (1.1698)\tPrec@1 46.875 (58.768)\n",
      "Epoch: [188][312/781]\tTime 0.001 (0.002)\tLoss 1.2780 (1.1912)\tPrec@1 54.688 (57.653)\n",
      "Epoch: [188][468/781]\tTime 0.006 (0.002)\tLoss 1.1886 (1.2011)\tPrec@1 51.562 (57.246)\n",
      "Epoch: [188][624/781]\tTime 0.001 (0.002)\tLoss 1.6853 (1.2164)\tPrec@1 43.750 (56.667)\n",
      "Epoch: [188][780/781]\tTime 0.001 (0.002)\tLoss 1.4243 (1.2308)\tPrec@1 50.000 (56.048)\n",
      "Epoch: [188][781/781]\tTime 0.007 (0.002)\tLoss 0.9773 (1.2307)\tPrec@1 68.750 (56.052)\n",
      "EPOCH: 188 train Results: Prec@1 56.052 Loss: 1.2307\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2020 (1.2020)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4145 (1.2850)\tPrec@1 31.250 (54.390)\n",
      "EPOCH: 188 val Results: Prec@1 54.390 Loss: 1.2850\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/781]\tTime 0.002 (0.002)\tLoss 1.1507 (1.1507)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [189][156/781]\tTime 0.001 (0.002)\tLoss 1.1381 (1.1584)\tPrec@1 54.688 (58.509)\n",
      "Epoch: [189][312/781]\tTime 0.002 (0.002)\tLoss 1.3550 (1.1936)\tPrec@1 50.000 (57.348)\n",
      "Epoch: [189][468/781]\tTime 0.002 (0.002)\tLoss 1.3109 (1.2060)\tPrec@1 46.875 (56.973)\n",
      "Epoch: [189][624/781]\tTime 0.001 (0.002)\tLoss 1.2348 (1.2205)\tPrec@1 57.812 (56.383)\n",
      "Epoch: [189][780/781]\tTime 0.002 (0.002)\tLoss 1.5016 (1.2333)\tPrec@1 39.062 (55.940)\n",
      "Epoch: [189][781/781]\tTime 0.007 (0.002)\tLoss 1.8420 (1.2335)\tPrec@1 37.500 (55.934)\n",
      "EPOCH: 189 train Results: Prec@1 55.934 Loss: 1.2335\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2218 (1.2218)\tPrec@1 51.562 (51.562)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.6376 (1.2772)\tPrec@1 50.000 (54.230)\n",
      "EPOCH: 189 val Results: Prec@1 54.230 Loss: 1.2772\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/781]\tTime 0.007 (0.007)\tLoss 1.2703 (1.2703)\tPrec@1 53.125 (53.125)\n",
      "Epoch: [190][156/781]\tTime 0.002 (0.002)\tLoss 1.2037 (1.1728)\tPrec@1 56.250 (58.330)\n",
      "Epoch: [190][312/781]\tTime 0.003 (0.002)\tLoss 1.3130 (1.1906)\tPrec@1 51.562 (57.573)\n",
      "Epoch: [190][468/781]\tTime 0.002 (0.002)\tLoss 1.0687 (1.2081)\tPrec@1 57.812 (56.943)\n",
      "Epoch: [190][624/781]\tTime 0.002 (0.002)\tLoss 1.2652 (1.2235)\tPrec@1 54.688 (56.398)\n",
      "Epoch: [190][780/781]\tTime 0.003 (0.002)\tLoss 1.2005 (1.2339)\tPrec@1 53.125 (55.960)\n",
      "Epoch: [190][781/781]\tTime 0.002 (0.002)\tLoss 1.6326 (1.2340)\tPrec@1 43.750 (55.956)\n",
      "EPOCH: 190 train Results: Prec@1 55.956 Loss: 1.2340\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1957 (1.1957)\tPrec@1 62.500 (62.500)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.5774 (1.2739)\tPrec@1 31.250 (54.160)\n",
      "EPOCH: 190 val Results: Prec@1 54.160 Loss: 1.2739\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/781]\tTime 0.002 (0.002)\tLoss 1.1528 (1.1528)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [191][156/781]\tTime 0.002 (0.002)\tLoss 1.1453 (1.1682)\tPrec@1 56.250 (58.529)\n",
      "Epoch: [191][312/781]\tTime 0.003 (0.002)\tLoss 1.3421 (1.1988)\tPrec@1 46.875 (57.578)\n",
      "Epoch: [191][468/781]\tTime 0.002 (0.002)\tLoss 1.1985 (1.2130)\tPrec@1 57.812 (57.140)\n",
      "Epoch: [191][624/781]\tTime 0.001 (0.002)\tLoss 1.0642 (1.2219)\tPrec@1 59.375 (56.685)\n",
      "Epoch: [191][780/781]\tTime 0.002 (0.002)\tLoss 1.3577 (1.2316)\tPrec@1 53.125 (56.348)\n",
      "Epoch: [191][781/781]\tTime 0.002 (0.002)\tLoss 1.0902 (1.2315)\tPrec@1 68.750 (56.352)\n",
      "EPOCH: 191 train Results: Prec@1 56.352 Loss: 1.2315\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2523 (1.2523)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3633 (1.2816)\tPrec@1 43.750 (53.850)\n",
      "EPOCH: 191 val Results: Prec@1 53.850 Loss: 1.2816\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/781]\tTime 0.007 (0.007)\tLoss 0.9688 (0.9688)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [192][156/781]\tTime 0.001 (0.002)\tLoss 1.1774 (1.1641)\tPrec@1 62.500 (58.658)\n",
      "Epoch: [192][312/781]\tTime 0.001 (0.002)\tLoss 1.1785 (1.1899)\tPrec@1 51.562 (57.748)\n",
      "Epoch: [192][468/781]\tTime 0.002 (0.002)\tLoss 1.4171 (1.2064)\tPrec@1 50.000 (57.343)\n",
      "Epoch: [192][624/781]\tTime 0.001 (0.002)\tLoss 1.1978 (1.2145)\tPrec@1 53.125 (57.020)\n",
      "Epoch: [192][780/781]\tTime 0.007 (0.002)\tLoss 1.2846 (1.2277)\tPrec@1 54.688 (56.446)\n",
      "Epoch: [192][781/781]\tTime 0.002 (0.002)\tLoss 1.3496 (1.2277)\tPrec@1 43.750 (56.442)\n",
      "EPOCH: 192 train Results: Prec@1 56.442 Loss: 1.2277\n",
      "Test: [0/156]\tTime 0.007 (0.007)\tLoss 1.1897 (1.1897)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1892 (1.2783)\tPrec@1 68.750 (54.010)\n",
      "EPOCH: 192 val Results: Prec@1 54.010 Loss: 1.2783\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/781]\tTime 0.002 (0.002)\tLoss 0.9835 (0.9835)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [193][156/781]\tTime 0.008 (0.002)\tLoss 1.3445 (1.1657)\tPrec@1 59.375 (58.668)\n",
      "Epoch: [193][312/781]\tTime 0.002 (0.002)\tLoss 1.2793 (1.1947)\tPrec@1 56.250 (57.458)\n",
      "Epoch: [193][468/781]\tTime 0.003 (0.002)\tLoss 1.1011 (1.2108)\tPrec@1 60.938 (56.940)\n",
      "Epoch: [193][624/781]\tTime 0.001 (0.002)\tLoss 1.4501 (1.2232)\tPrec@1 54.688 (56.597)\n",
      "Epoch: [193][780/781]\tTime 0.004 (0.002)\tLoss 1.2813 (1.2297)\tPrec@1 56.250 (56.364)\n",
      "Epoch: [193][781/781]\tTime 0.002 (0.002)\tLoss 1.5570 (1.2298)\tPrec@1 37.500 (56.358)\n",
      "EPOCH: 193 train Results: Prec@1 56.358 Loss: 1.2298\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.1159 (1.1159)\tPrec@1 65.625 (65.625)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3313 (1.2791)\tPrec@1 37.500 (54.310)\n",
      "EPOCH: 193 val Results: Prec@1 54.310 Loss: 1.2791\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/781]\tTime 0.004 (0.004)\tLoss 0.9998 (0.9998)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [194][156/781]\tTime 0.001 (0.002)\tLoss 1.0490 (1.1788)\tPrec@1 64.062 (58.002)\n",
      "Epoch: [194][312/781]\tTime 0.002 (0.002)\tLoss 1.0400 (1.1931)\tPrec@1 57.812 (57.403)\n",
      "Epoch: [194][468/781]\tTime 0.002 (0.002)\tLoss 1.0530 (1.2102)\tPrec@1 64.062 (56.716)\n",
      "Epoch: [194][624/781]\tTime 0.002 (0.002)\tLoss 1.0614 (1.2207)\tPrec@1 60.938 (56.453)\n",
      "Epoch: [194][780/781]\tTime 0.003 (0.002)\tLoss 1.2229 (1.2273)\tPrec@1 50.000 (56.228)\n",
      "Epoch: [194][781/781]\tTime 0.002 (0.002)\tLoss 1.3810 (1.2273)\tPrec@1 43.750 (56.224)\n",
      "EPOCH: 194 train Results: Prec@1 56.224 Loss: 1.2273\n",
      "Test: [0/156]\tTime 0.002 (0.002)\tLoss 1.2277 (1.2277)\tPrec@1 50.000 (50.000)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.3734 (1.2750)\tPrec@1 43.750 (54.370)\n",
      "EPOCH: 194 val Results: Prec@1 54.370 Loss: 1.2750\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/781]\tTime 0.002 (0.002)\tLoss 1.1853 (1.1853)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [195][156/781]\tTime 0.002 (0.002)\tLoss 1.2135 (1.1695)\tPrec@1 56.250 (57.842)\n",
      "Epoch: [195][312/781]\tTime 0.001 (0.002)\tLoss 1.2807 (1.1973)\tPrec@1 54.688 (57.104)\n",
      "Epoch: [195][468/781]\tTime 0.002 (0.002)\tLoss 1.2239 (1.2082)\tPrec@1 57.812 (56.876)\n",
      "Epoch: [195][624/781]\tTime 0.001 (0.002)\tLoss 1.4601 (1.2218)\tPrec@1 50.000 (56.347)\n",
      "Epoch: [195][780/781]\tTime 0.002 (0.002)\tLoss 1.1119 (1.2269)\tPrec@1 54.688 (56.282)\n",
      "Epoch: [195][781/781]\tTime 0.002 (0.002)\tLoss 1.7275 (1.2271)\tPrec@1 31.250 (56.274)\n",
      "EPOCH: 195 train Results: Prec@1 56.274 Loss: 1.2271\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2018 (1.2018)\tPrec@1 57.812 (57.812)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.4809 (1.2914)\tPrec@1 31.250 (53.180)\n",
      "EPOCH: 195 val Results: Prec@1 53.180 Loss: 1.2914\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/781]\tTime 0.003 (0.003)\tLoss 1.1241 (1.1241)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [196][156/781]\tTime 0.002 (0.002)\tLoss 1.1116 (1.1663)\tPrec@1 64.062 (58.021)\n",
      "Epoch: [196][312/781]\tTime 0.001 (0.002)\tLoss 1.3723 (1.1915)\tPrec@1 48.438 (57.593)\n",
      "Epoch: [196][468/781]\tTime 0.001 (0.002)\tLoss 1.0790 (1.2179)\tPrec@1 65.625 (56.666)\n",
      "Epoch: [196][624/781]\tTime 0.001 (0.002)\tLoss 1.2510 (1.2248)\tPrec@1 57.812 (56.385)\n",
      "Epoch: [196][780/781]\tTime 0.001 (0.002)\tLoss 1.0572 (1.2315)\tPrec@1 62.500 (56.106)\n",
      "Epoch: [196][781/781]\tTime 0.002 (0.002)\tLoss 1.3765 (1.2315)\tPrec@1 37.500 (56.100)\n",
      "EPOCH: 196 train Results: Prec@1 56.100 Loss: 1.2315\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1500 (1.1500)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1966 (1.2756)\tPrec@1 43.750 (54.320)\n",
      "EPOCH: 196 val Results: Prec@1 54.320 Loss: 1.2756\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/781]\tTime 0.002 (0.002)\tLoss 1.0675 (1.0675)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [197][156/781]\tTime 0.001 (0.002)\tLoss 1.1240 (1.1446)\tPrec@1 62.500 (59.544)\n",
      "Epoch: [197][312/781]\tTime 0.001 (0.002)\tLoss 1.3072 (1.1796)\tPrec@1 53.125 (58.022)\n",
      "Epoch: [197][468/781]\tTime 0.002 (0.002)\tLoss 1.1237 (1.2038)\tPrec@1 62.500 (57.269)\n",
      "Epoch: [197][624/781]\tTime 0.003 (0.002)\tLoss 1.3189 (1.2229)\tPrec@1 53.125 (56.540)\n",
      "Epoch: [197][780/781]\tTime 0.002 (0.002)\tLoss 1.1226 (1.2297)\tPrec@1 60.938 (56.220)\n",
      "Epoch: [197][781/781]\tTime 0.002 (0.002)\tLoss 0.9559 (1.2296)\tPrec@1 56.250 (56.220)\n",
      "EPOCH: 197 train Results: Prec@1 56.220 Loss: 1.2296\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2305 (1.2305)\tPrec@1 60.938 (60.938)\n",
      "Test: [156/156]\tTime 0.000 (0.001)\tLoss 1.1930 (1.2736)\tPrec@1 50.000 (53.900)\n",
      "EPOCH: 197 val Results: Prec@1 53.900 Loss: 1.2736\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/781]\tTime 0.003 (0.003)\tLoss 1.1490 (1.1490)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [198][156/781]\tTime 0.001 (0.002)\tLoss 1.3468 (1.1644)\tPrec@1 53.125 (57.962)\n",
      "Epoch: [198][312/781]\tTime 0.002 (0.002)\tLoss 1.1222 (1.1977)\tPrec@1 50.000 (57.119)\n",
      "Epoch: [198][468/781]\tTime 0.001 (0.002)\tLoss 1.6870 (1.2138)\tPrec@1 43.750 (56.593)\n",
      "Epoch: [198][624/781]\tTime 0.001 (0.002)\tLoss 1.0356 (1.2222)\tPrec@1 60.938 (56.250)\n",
      "Epoch: [198][780/781]\tTime 0.002 (0.002)\tLoss 1.2452 (1.2302)\tPrec@1 59.375 (56.068)\n",
      "Epoch: [198][781/781]\tTime 0.002 (0.002)\tLoss 1.3074 (1.2302)\tPrec@1 50.000 (56.066)\n",
      "EPOCH: 198 train Results: Prec@1 56.066 Loss: 1.2302\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.3052 (1.3052)\tPrec@1 59.375 (59.375)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.4706 (1.2888)\tPrec@1 31.250 (53.440)\n",
      "EPOCH: 198 val Results: Prec@1 53.440 Loss: 1.2888\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/781]\tTime 0.002 (0.002)\tLoss 1.4381 (1.4381)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [199][156/781]\tTime 0.004 (0.002)\tLoss 1.0450 (1.1682)\tPrec@1 64.062 (58.479)\n",
      "Epoch: [199][312/781]\tTime 0.002 (0.002)\tLoss 1.1450 (1.1845)\tPrec@1 56.250 (57.803)\n",
      "Epoch: [199][468/781]\tTime 0.002 (0.002)\tLoss 1.1449 (1.2030)\tPrec@1 54.688 (57.150)\n",
      "Epoch: [199][624/781]\tTime 0.002 (0.002)\tLoss 1.3447 (1.2189)\tPrec@1 51.562 (56.435)\n",
      "Epoch: [199][780/781]\tTime 0.004 (0.002)\tLoss 1.2979 (1.2273)\tPrec@1 53.125 (56.064)\n",
      "Epoch: [199][781/781]\tTime 0.001 (0.002)\tLoss 1.2718 (1.2273)\tPrec@1 56.250 (56.064)\n",
      "EPOCH: 199 train Results: Prec@1 56.064 Loss: 1.2273\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.2359 (1.2359)\tPrec@1 53.125 (53.125)\n",
      "Test: [156/156]\tTime 0.000 (0.000)\tLoss 1.1849 (1.2748)\tPrec@1 43.750 (54.100)\n",
      "EPOCH: 199 val Results: Prec@1 54.100 Loss: 1.2748\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/781]\tTime 0.003 (0.003)\tLoss 0.9719 (0.9719)\tPrec@1 60.938 (60.938)\n",
      "Epoch: [200][156/781]\tTime 0.002 (0.002)\tLoss 1.1746 (1.1742)\tPrec@1 59.375 (58.061)\n",
      "Epoch: [200][312/781]\tTime 0.002 (0.002)\tLoss 1.0469 (1.2022)\tPrec@1 64.062 (57.104)\n",
      "Epoch: [200][468/781]\tTime 0.003 (0.002)\tLoss 1.0940 (1.2175)\tPrec@1 60.938 (56.620)\n",
      "Epoch: [200][624/781]\tTime 0.002 (0.002)\tLoss 1.4860 (1.2227)\tPrec@1 42.188 (56.495)\n",
      "Epoch: [200][780/781]\tTime 0.002 (0.002)\tLoss 1.4277 (1.2329)\tPrec@1 51.562 (56.048)\n",
      "Epoch: [200][781/781]\tTime 0.002 (0.002)\tLoss 1.4746 (1.2329)\tPrec@1 37.500 (56.042)\n",
      "EPOCH: 200 train Results: Prec@1 56.042 Loss: 1.2329\n",
      "Test: [0/156]\tTime 0.001 (0.001)\tLoss 1.1742 (1.1742)\tPrec@1 56.250 (56.250)\n",
      "Test: [156/156]\tTime 0.001 (0.000)\tLoss 1.3109 (1.2810)\tPrec@1 37.500 (54.650)\n",
      "EPOCH: 200 val Results: Prec@1 54.650 Loss: 1.2810\n",
      "Best Prec@1: 54.970\n",
      "\n",
      "End time:  Fri Apr  5 00:04:21 2024\n",
      "train executed in 370.4974 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 64\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'scheduler': None, \n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer3 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Fri Apr  5 00:04:21 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/195]\tTime 0.023 (0.023)\tLoss 5.4415 (5.4415)\tPrec@1 9.375 (9.375)\n",
      "Epoch: [1][39/195]\tTime 0.003 (0.007)\tLoss 2.7997 (3.9533)\tPrec@1 23.047 (17.832)\n",
      "Epoch: [1][78/195]\tTime 0.006 (0.006)\tLoss 2.4477 (3.3110)\tPrec@1 29.297 (22.607)\n",
      "Epoch: [1][117/195]\tTime 0.005 (0.006)\tLoss 2.1101 (2.9831)\tPrec@1 32.031 (25.556)\n",
      "Epoch: [1][156/195]\tTime 0.004 (0.006)\tLoss 2.0648 (2.7768)\tPrec@1 32.031 (27.142)\n",
      "Epoch: [1][195/195]\tTime 0.002 (0.006)\tLoss 1.8976 (2.6320)\tPrec@1 37.500 (28.440)\n",
      "EPOCH: 1 train Results: Prec@1 28.440 Loss: 2.6320\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.8507 (1.8507)\tPrec@1 38.672 (38.672)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5348 (1.7647)\tPrec@1 31.250 (39.100)\n",
      "EPOCH: 1 val Results: Prec@1 39.100 Loss: 1.7647\n",
      "Best Prec@1: 39.100\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/195]\tTime 0.004 (0.004)\tLoss 1.8571 (1.8571)\tPrec@1 36.719 (36.719)\n",
      "Epoch: [2][39/195]\tTime 0.004 (0.006)\tLoss 1.7678 (1.8140)\tPrec@1 37.500 (38.506)\n",
      "Epoch: [2][78/195]\tTime 0.008 (0.006)\tLoss 1.6053 (1.7935)\tPrec@1 44.531 (38.509)\n",
      "Epoch: [2][117/195]\tTime 0.008 (0.006)\tLoss 1.8446 (1.7767)\tPrec@1 39.062 (38.966)\n",
      "Epoch: [2][156/195]\tTime 0.008 (0.006)\tLoss 1.7908 (1.7602)\tPrec@1 40.234 (39.237)\n",
      "Epoch: [2][195/195]\tTime 0.004 (0.007)\tLoss 1.5616 (1.7449)\tPrec@1 43.750 (39.568)\n",
      "EPOCH: 2 train Results: Prec@1 39.568 Loss: 1.7449\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.6172 (1.6172)\tPrec@1 42.969 (42.969)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4513 (1.6080)\tPrec@1 37.500 (42.960)\n",
      "EPOCH: 2 val Results: Prec@1 42.960 Loss: 1.6080\n",
      "Best Prec@1: 42.960\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/195]\tTime 0.005 (0.005)\tLoss 1.5816 (1.5816)\tPrec@1 46.094 (46.094)\n",
      "Epoch: [3][39/195]\tTime 0.006 (0.006)\tLoss 1.6290 (1.5995)\tPrec@1 42.578 (44.033)\n",
      "Epoch: [3][78/195]\tTime 0.008 (0.006)\tLoss 1.5507 (1.5858)\tPrec@1 48.047 (44.225)\n",
      "Epoch: [3][117/195]\tTime 0.008 (0.006)\tLoss 1.5399 (1.5817)\tPrec@1 43.750 (44.253)\n",
      "Epoch: [3][156/195]\tTime 0.008 (0.006)\tLoss 1.6282 (1.5787)\tPrec@1 42.969 (44.414)\n",
      "Epoch: [3][195/195]\tTime 0.003 (0.007)\tLoss 1.5169 (1.5732)\tPrec@1 45.000 (44.542)\n",
      "EPOCH: 3 train Results: Prec@1 44.542 Loss: 1.5732\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.5387 (1.5387)\tPrec@1 46.094 (46.094)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3874 (1.5331)\tPrec@1 37.500 (45.560)\n",
      "EPOCH: 3 val Results: Prec@1 45.560 Loss: 1.5331\n",
      "Best Prec@1: 45.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/195]\tTime 0.007 (0.007)\tLoss 1.5585 (1.5585)\tPrec@1 41.406 (41.406)\n",
      "Epoch: [4][39/195]\tTime 0.003 (0.006)\tLoss 1.4489 (1.4971)\tPrec@1 51.562 (46.943)\n",
      "Epoch: [4][78/195]\tTime 0.020 (0.006)\tLoss 1.5831 (1.4967)\tPrec@1 46.094 (47.251)\n",
      "Epoch: [4][117/195]\tTime 0.010 (0.006)\tLoss 1.5180 (1.4926)\tPrec@1 47.656 (47.312)\n",
      "Epoch: [4][156/195]\tTime 0.010 (0.006)\tLoss 1.4408 (1.4914)\tPrec@1 49.609 (47.213)\n",
      "Epoch: [4][195/195]\tTime 0.007 (0.006)\tLoss 1.3661 (1.4890)\tPrec@1 51.250 (47.344)\n",
      "EPOCH: 4 train Results: Prec@1 47.344 Loss: 1.4890\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.4883 (1.4883)\tPrec@1 49.609 (49.609)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4238 (1.4865)\tPrec@1 31.250 (47.350)\n",
      "EPOCH: 4 val Results: Prec@1 47.350 Loss: 1.4865\n",
      "Best Prec@1: 47.350\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/195]\tTime 0.006 (0.006)\tLoss 1.4475 (1.4475)\tPrec@1 48.438 (48.438)\n",
      "Epoch: [5][39/195]\tTime 0.006 (0.007)\tLoss 1.4760 (1.4217)\tPrec@1 46.094 (49.795)\n",
      "Epoch: [5][78/195]\tTime 0.004 (0.007)\tLoss 1.4730 (1.4340)\tPrec@1 49.219 (49.357)\n",
      "Epoch: [5][117/195]\tTime 0.004 (0.006)\tLoss 1.3882 (1.4316)\tPrec@1 51.172 (49.427)\n",
      "Epoch: [5][156/195]\tTime 0.009 (0.006)\tLoss 1.3908 (1.4305)\tPrec@1 53.516 (49.423)\n",
      "Epoch: [5][195/195]\tTime 0.002 (0.006)\tLoss 1.4138 (1.4312)\tPrec@1 51.250 (49.562)\n",
      "EPOCH: 5 train Results: Prec@1 49.562 Loss: 1.4312\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.4525 (1.4525)\tPrec@1 45.703 (45.703)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3314 (1.4447)\tPrec@1 43.750 (48.640)\n",
      "EPOCH: 5 val Results: Prec@1 48.640 Loss: 1.4447\n",
      "Best Prec@1: 48.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/195]\tTime 0.007 (0.007)\tLoss 1.4049 (1.4049)\tPrec@1 55.078 (55.078)\n",
      "Epoch: [6][39/195]\tTime 0.005 (0.005)\tLoss 1.4101 (1.3651)\tPrec@1 46.094 (51.943)\n",
      "Epoch: [6][78/195]\tTime 0.006 (0.005)\tLoss 1.3190 (1.3706)\tPrec@1 51.562 (51.607)\n",
      "Epoch: [6][117/195]\tTime 0.003 (0.006)\tLoss 1.4443 (1.3734)\tPrec@1 50.000 (51.400)\n",
      "Epoch: [6][156/195]\tTime 0.008 (0.006)\tLoss 1.4145 (1.3759)\tPrec@1 45.312 (51.256)\n",
      "Epoch: [6][195/195]\tTime 0.003 (0.006)\tLoss 1.6186 (1.3768)\tPrec@1 40.000 (51.240)\n",
      "EPOCH: 6 train Results: Prec@1 51.240 Loss: 1.3768\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.4052 (1.4052)\tPrec@1 48.047 (48.047)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4430 (1.4128)\tPrec@1 31.250 (49.610)\n",
      "EPOCH: 6 val Results: Prec@1 49.610 Loss: 1.4128\n",
      "Best Prec@1: 49.610\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/195]\tTime 0.005 (0.005)\tLoss 1.3251 (1.3251)\tPrec@1 55.859 (55.859)\n",
      "Epoch: [7][39/195]\tTime 0.004 (0.007)\tLoss 1.3315 (1.3311)\tPrec@1 51.562 (53.643)\n",
      "Epoch: [7][78/195]\tTime 0.009 (0.006)\tLoss 1.3445 (1.3331)\tPrec@1 53.125 (53.397)\n",
      "Epoch: [7][117/195]\tTime 0.011 (0.006)\tLoss 1.3385 (1.3344)\tPrec@1 52.344 (52.973)\n",
      "Epoch: [7][156/195]\tTime 0.011 (0.007)\tLoss 1.4062 (1.3349)\tPrec@1 51.172 (52.891)\n",
      "Epoch: [7][195/195]\tTime 0.003 (0.007)\tLoss 1.4054 (1.3368)\tPrec@1 56.250 (52.820)\n",
      "EPOCH: 7 train Results: Prec@1 52.820 Loss: 1.3368\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.3945 (1.3945)\tPrec@1 48.047 (48.047)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4578 (1.3820)\tPrec@1 31.250 (50.550)\n",
      "EPOCH: 7 val Results: Prec@1 50.550 Loss: 1.3820\n",
      "Best Prec@1: 50.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/195]\tTime 0.018 (0.018)\tLoss 1.3248 (1.3248)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [8][39/195]\tTime 0.005 (0.006)\tLoss 1.3002 (1.2998)\tPrec@1 51.562 (54.365)\n",
      "Epoch: [8][78/195]\tTime 0.005 (0.006)\tLoss 1.2864 (1.3065)\tPrec@1 54.688 (54.099)\n",
      "Epoch: [8][117/195]\tTime 0.006 (0.006)\tLoss 1.3427 (1.3021)\tPrec@1 55.469 (54.360)\n",
      "Epoch: [8][156/195]\tTime 0.003 (0.006)\tLoss 1.2594 (1.3014)\tPrec@1 55.078 (54.354)\n",
      "Epoch: [8][195/195]\tTime 0.002 (0.006)\tLoss 1.4679 (1.3030)\tPrec@1 43.750 (54.184)\n",
      "EPOCH: 8 train Results: Prec@1 54.184 Loss: 1.3030\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.3561 (1.3561)\tPrec@1 50.781 (50.781)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4500 (1.3540)\tPrec@1 37.500 (51.750)\n",
      "EPOCH: 8 val Results: Prec@1 51.750 Loss: 1.3540\n",
      "Best Prec@1: 51.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/195]\tTime 0.010 (0.010)\tLoss 1.3181 (1.3181)\tPrec@1 55.078 (55.078)\n",
      "Epoch: [9][39/195]\tTime 0.004 (0.006)\tLoss 1.1497 (1.2461)\tPrec@1 59.766 (56.318)\n",
      "Epoch: [9][78/195]\tTime 0.006 (0.005)\tLoss 1.3959 (1.2523)\tPrec@1 52.734 (56.047)\n",
      "Epoch: [9][117/195]\tTime 0.005 (0.006)\tLoss 1.2738 (1.2603)\tPrec@1 51.953 (55.677)\n",
      "Epoch: [9][156/195]\tTime 0.007 (0.006)\tLoss 1.2638 (1.2634)\tPrec@1 56.641 (55.372)\n",
      "Epoch: [9][195/195]\tTime 0.007 (0.006)\tLoss 1.5069 (1.2683)\tPrec@1 45.000 (55.156)\n",
      "EPOCH: 9 train Results: Prec@1 55.156 Loss: 1.2683\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.3222 (1.3222)\tPrec@1 54.688 (54.688)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4670 (1.3376)\tPrec@1 31.250 (51.840)\n",
      "EPOCH: 9 val Results: Prec@1 51.840 Loss: 1.3376\n",
      "Best Prec@1: 51.840\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/195]\tTime 0.004 (0.004)\tLoss 1.2929 (1.2929)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [10][39/195]\tTime 0.003 (0.005)\tLoss 1.1916 (1.2129)\tPrec@1 53.125 (56.914)\n",
      "Epoch: [10][78/195]\tTime 0.006 (0.006)\tLoss 1.3399 (1.2334)\tPrec@1 51.953 (56.304)\n",
      "Epoch: [10][117/195]\tTime 0.009 (0.006)\tLoss 1.2746 (1.2395)\tPrec@1 51.172 (56.005)\n",
      "Epoch: [10][156/195]\tTime 0.004 (0.006)\tLoss 1.3107 (1.2420)\tPrec@1 54.688 (55.862)\n",
      "Epoch: [10][195/195]\tTime 0.002 (0.006)\tLoss 1.2000 (1.2461)\tPrec@1 55.000 (55.778)\n",
      "EPOCH: 10 train Results: Prec@1 55.778 Loss: 1.2461\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.3394 (1.3394)\tPrec@1 51.953 (51.953)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5397 (1.3168)\tPrec@1 25.000 (52.440)\n",
      "EPOCH: 10 val Results: Prec@1 52.440 Loss: 1.3168\n",
      "Best Prec@1: 52.440\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/195]\tTime 0.006 (0.006)\tLoss 1.1628 (1.1628)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [11][39/195]\tTime 0.004 (0.006)\tLoss 1.2426 (1.1940)\tPrec@1 53.906 (58.184)\n",
      "Epoch: [11][78/195]\tTime 0.004 (0.006)\tLoss 1.2956 (1.2048)\tPrec@1 53.516 (57.679)\n",
      "Epoch: [11][117/195]\tTime 0.005 (0.006)\tLoss 1.2670 (1.2127)\tPrec@1 57.422 (57.332)\n",
      "Epoch: [11][156/195]\tTime 0.006 (0.006)\tLoss 1.1704 (1.2168)\tPrec@1 59.766 (57.156)\n",
      "Epoch: [11][195/195]\tTime 0.002 (0.006)\tLoss 1.3773 (1.2220)\tPrec@1 48.750 (56.936)\n",
      "EPOCH: 11 train Results: Prec@1 56.936 Loss: 1.2220\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2756 (1.2756)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.001 (0.002)\tLoss 1.5524 (1.3087)\tPrec@1 31.250 (53.190)\n",
      "EPOCH: 11 val Results: Prec@1 53.190 Loss: 1.3087\n",
      "Best Prec@1: 53.190\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/195]\tTime 0.004 (0.004)\tLoss 1.1768 (1.1768)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [12][39/195]\tTime 0.006 (0.007)\tLoss 1.1731 (1.1771)\tPrec@1 59.375 (58.496)\n",
      "Epoch: [12][78/195]\tTime 0.006 (0.006)\tLoss 1.2398 (1.1839)\tPrec@1 56.641 (58.292)\n",
      "Epoch: [12][117/195]\tTime 0.006 (0.006)\tLoss 1.1085 (1.1894)\tPrec@1 62.109 (58.028)\n",
      "Epoch: [12][156/195]\tTime 0.007 (0.006)\tLoss 1.2460 (1.1975)\tPrec@1 58.594 (57.865)\n",
      "Epoch: [12][195/195]\tTime 0.003 (0.006)\tLoss 1.0684 (1.2039)\tPrec@1 61.250 (57.560)\n",
      "EPOCH: 12 train Results: Prec@1 57.560 Loss: 1.2039\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2618 (1.2618)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3238 (1.2929)\tPrec@1 31.250 (53.660)\n",
      "EPOCH: 12 val Results: Prec@1 53.660 Loss: 1.2929\n",
      "Best Prec@1: 53.660\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/195]\tTime 0.009 (0.009)\tLoss 1.1183 (1.1183)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [13][39/195]\tTime 0.005 (0.006)\tLoss 1.0809 (1.1526)\tPrec@1 61.719 (59.287)\n",
      "Epoch: [13][78/195]\tTime 0.005 (0.006)\tLoss 1.0810 (1.1621)\tPrec@1 63.672 (59.217)\n",
      "Epoch: [13][117/195]\tTime 0.010 (0.006)\tLoss 1.2692 (1.1723)\tPrec@1 52.344 (58.961)\n",
      "Epoch: [13][156/195]\tTime 0.003 (0.006)\tLoss 1.2186 (1.1850)\tPrec@1 57.031 (58.211)\n",
      "Epoch: [13][195/195]\tTime 0.001 (0.006)\tLoss 1.2372 (1.1906)\tPrec@1 52.500 (58.006)\n",
      "EPOCH: 13 train Results: Prec@1 58.006 Loss: 1.1906\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2776 (1.2776)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.003)\tLoss 1.4908 (1.2931)\tPrec@1 37.500 (53.190)\n",
      "EPOCH: 13 val Results: Prec@1 53.190 Loss: 1.2931\n",
      "Best Prec@1: 53.660\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/195]\tTime 0.008 (0.008)\tLoss 1.0523 (1.0523)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [14][39/195]\tTime 0.005 (0.006)\tLoss 1.0850 (1.1481)\tPrec@1 63.281 (60.078)\n",
      "Epoch: [14][78/195]\tTime 0.010 (0.006)\tLoss 1.2299 (1.1550)\tPrec@1 57.812 (59.222)\n",
      "Epoch: [14][117/195]\tTime 0.011 (0.006)\tLoss 1.1124 (1.1622)\tPrec@1 57.031 (58.812)\n",
      "Epoch: [14][156/195]\tTime 0.003 (0.006)\tLoss 1.1579 (1.1676)\tPrec@1 57.031 (58.736)\n",
      "Epoch: [14][195/195]\tTime 0.010 (0.006)\tLoss 1.3353 (1.1746)\tPrec@1 55.000 (58.392)\n",
      "EPOCH: 14 train Results: Prec@1 58.392 Loss: 1.1746\n",
      "Test: [0/39]\tTime 0.010 (0.010)\tLoss 1.2525 (1.2525)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5236 (1.2752)\tPrec@1 31.250 (53.850)\n",
      "EPOCH: 14 val Results: Prec@1 53.850 Loss: 1.2752\n",
      "Best Prec@1: 53.850\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/195]\tTime 0.008 (0.008)\tLoss 1.0671 (1.0671)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [15][39/195]\tTime 0.008 (0.006)\tLoss 1.1776 (1.1233)\tPrec@1 57.812 (60.479)\n",
      "Epoch: [15][78/195]\tTime 0.006 (0.006)\tLoss 1.0725 (1.1361)\tPrec@1 61.328 (60.151)\n",
      "Epoch: [15][117/195]\tTime 0.006 (0.006)\tLoss 1.1979 (1.1485)\tPrec@1 55.078 (59.620)\n",
      "Epoch: [15][156/195]\tTime 0.012 (0.006)\tLoss 1.2156 (1.1563)\tPrec@1 55.469 (59.328)\n",
      "Epoch: [15][195/195]\tTime 0.001 (0.006)\tLoss 1.1556 (1.1619)\tPrec@1 56.250 (59.048)\n",
      "EPOCH: 15 train Results: Prec@1 59.048 Loss: 1.1619\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2647 (1.2647)\tPrec@1 54.297 (54.297)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4472 (1.2720)\tPrec@1 43.750 (53.880)\n",
      "EPOCH: 15 val Results: Prec@1 53.880 Loss: 1.2720\n",
      "Best Prec@1: 53.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/195]\tTime 0.010 (0.010)\tLoss 1.0699 (1.0699)\tPrec@1 65.234 (65.234)\n",
      "Epoch: [16][39/195]\tTime 0.007 (0.007)\tLoss 1.0609 (1.0922)\tPrec@1 63.672 (62.178)\n",
      "Epoch: [16][78/195]\tTime 0.006 (0.006)\tLoss 1.0703 (1.1125)\tPrec@1 60.156 (61.120)\n",
      "Epoch: [16][117/195]\tTime 0.004 (0.007)\tLoss 1.1890 (1.1297)\tPrec@1 55.859 (60.295)\n",
      "Epoch: [16][156/195]\tTime 0.004 (0.007)\tLoss 1.1413 (1.1419)\tPrec@1 60.547 (59.791)\n",
      "Epoch: [16][195/195]\tTime 0.003 (0.006)\tLoss 1.3015 (1.1506)\tPrec@1 56.250 (59.418)\n",
      "EPOCH: 16 train Results: Prec@1 59.418 Loss: 1.1506\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2201 (1.2201)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4679 (1.2669)\tPrec@1 25.000 (54.540)\n",
      "EPOCH: 16 val Results: Prec@1 54.540 Loss: 1.2669\n",
      "Best Prec@1: 54.540\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/195]\tTime 0.005 (0.005)\tLoss 1.0673 (1.0673)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [17][39/195]\tTime 0.005 (0.005)\tLoss 1.0565 (1.1025)\tPrec@1 63.281 (61.660)\n",
      "Epoch: [17][78/195]\tTime 0.005 (0.005)\tLoss 1.1881 (1.1119)\tPrec@1 58.594 (61.101)\n",
      "Epoch: [17][117/195]\tTime 0.006 (0.006)\tLoss 1.1614 (1.1241)\tPrec@1 58.594 (60.626)\n",
      "Epoch: [17][156/195]\tTime 0.004 (0.006)\tLoss 1.1744 (1.1329)\tPrec@1 57.031 (60.226)\n",
      "Epoch: [17][195/195]\tTime 0.002 (0.006)\tLoss 1.0868 (1.1429)\tPrec@1 58.750 (59.810)\n",
      "EPOCH: 17 train Results: Prec@1 59.810 Loss: 1.1429\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2426 (1.2426)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4360 (1.2633)\tPrec@1 43.750 (54.450)\n",
      "EPOCH: 17 val Results: Prec@1 54.450 Loss: 1.2633\n",
      "Best Prec@1: 54.540\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/195]\tTime 0.005 (0.005)\tLoss 1.0826 (1.0826)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [18][39/195]\tTime 0.008 (0.006)\tLoss 1.1203 (1.1058)\tPrec@1 56.641 (60.889)\n",
      "Epoch: [18][78/195]\tTime 0.006 (0.006)\tLoss 1.0236 (1.1099)\tPrec@1 63.672 (60.824)\n",
      "Epoch: [18][117/195]\tTime 0.004 (0.006)\tLoss 0.9956 (1.1249)\tPrec@1 66.016 (60.097)\n",
      "Epoch: [18][156/195]\tTime 0.008 (0.006)\tLoss 1.1835 (1.1302)\tPrec@1 64.062 (59.895)\n",
      "Epoch: [18][195/195]\tTime 0.002 (0.006)\tLoss 1.0863 (1.1354)\tPrec@1 62.500 (59.678)\n",
      "EPOCH: 18 train Results: Prec@1 59.678 Loss: 1.1354\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2406 (1.2406)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3573 (1.2538)\tPrec@1 37.500 (55.120)\n",
      "EPOCH: 18 val Results: Prec@1 55.120 Loss: 1.2538\n",
      "Best Prec@1: 55.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/195]\tTime 0.009 (0.009)\tLoss 0.9855 (0.9855)\tPrec@1 64.844 (64.844)\n",
      "Epoch: [19][39/195]\tTime 0.003 (0.005)\tLoss 1.1917 (1.0751)\tPrec@1 56.250 (62.441)\n",
      "Epoch: [19][78/195]\tTime 0.004 (0.006)\tLoss 1.1198 (1.0944)\tPrec@1 62.109 (61.654)\n",
      "Epoch: [19][117/195]\tTime 0.004 (0.006)\tLoss 1.0639 (1.1031)\tPrec@1 63.672 (61.341)\n",
      "Epoch: [19][156/195]\tTime 0.004 (0.006)\tLoss 1.0956 (1.1175)\tPrec@1 60.938 (60.689)\n",
      "Epoch: [19][195/195]\tTime 0.001 (0.006)\tLoss 1.2516 (1.1252)\tPrec@1 58.750 (60.446)\n",
      "EPOCH: 19 train Results: Prec@1 60.446 Loss: 1.1252\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2692 (1.2692)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.7188 (1.2567)\tPrec@1 31.250 (54.470)\n",
      "EPOCH: 19 val Results: Prec@1 54.470 Loss: 1.2567\n",
      "Best Prec@1: 55.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/195]\tTime 0.005 (0.005)\tLoss 1.1101 (1.1101)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [20][39/195]\tTime 0.007 (0.006)\tLoss 0.9409 (1.0928)\tPrec@1 71.484 (61.387)\n",
      "Epoch: [20][78/195]\tTime 0.004 (0.007)\tLoss 1.0568 (1.0938)\tPrec@1 62.109 (61.481)\n",
      "Epoch: [20][117/195]\tTime 0.005 (0.006)\tLoss 1.1572 (1.1066)\tPrec@1 57.812 (61.086)\n",
      "Epoch: [20][156/195]\tTime 0.011 (0.006)\tLoss 1.2079 (1.1112)\tPrec@1 55.469 (60.878)\n",
      "Epoch: [20][195/195]\tTime 0.004 (0.006)\tLoss 1.1785 (1.1168)\tPrec@1 51.250 (60.562)\n",
      "EPOCH: 20 train Results: Prec@1 60.562 Loss: 1.1168\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2301 (1.2301)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4105 (1.2502)\tPrec@1 31.250 (55.190)\n",
      "EPOCH: 20 val Results: Prec@1 55.190 Loss: 1.2502\n",
      "Best Prec@1: 55.190\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/195]\tTime 0.009 (0.009)\tLoss 1.0069 (1.0069)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [21][39/195]\tTime 0.005 (0.007)\tLoss 1.1844 (1.0494)\tPrec@1 57.812 (62.939)\n",
      "Epoch: [21][78/195]\tTime 0.005 (0.007)\tLoss 1.2350 (1.0725)\tPrec@1 56.641 (61.961)\n",
      "Epoch: [21][117/195]\tTime 0.007 (0.007)\tLoss 1.1242 (1.0839)\tPrec@1 60.938 (61.619)\n",
      "Epoch: [21][156/195]\tTime 0.004 (0.006)\tLoss 1.1414 (1.1002)\tPrec@1 57.422 (61.027)\n",
      "Epoch: [21][195/195]\tTime 0.001 (0.006)\tLoss 1.0744 (1.1084)\tPrec@1 61.250 (60.618)\n",
      "EPOCH: 21 train Results: Prec@1 60.618 Loss: 1.1084\n",
      "Test: [0/39]\tTime 0.007 (0.007)\tLoss 1.2513 (1.2513)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3684 (1.2479)\tPrec@1 37.500 (54.790)\n",
      "EPOCH: 21 val Results: Prec@1 54.790 Loss: 1.2479\n",
      "Best Prec@1: 55.190\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/195]\tTime 0.006 (0.006)\tLoss 1.0291 (1.0291)\tPrec@1 65.234 (65.234)\n",
      "Epoch: [22][39/195]\tTime 0.007 (0.007)\tLoss 1.1008 (1.0718)\tPrec@1 61.719 (62.725)\n",
      "Epoch: [22][78/195]\tTime 0.009 (0.006)\tLoss 1.0034 (1.0744)\tPrec@1 64.062 (62.292)\n",
      "Epoch: [22][117/195]\tTime 0.015 (0.006)\tLoss 1.1014 (1.0833)\tPrec@1 62.109 (61.811)\n",
      "Epoch: [22][156/195]\tTime 0.012 (0.006)\tLoss 1.0903 (1.0936)\tPrec@1 63.281 (61.236)\n",
      "Epoch: [22][195/195]\tTime 0.007 (0.006)\tLoss 1.0886 (1.1030)\tPrec@1 63.750 (60.862)\n",
      "EPOCH: 22 train Results: Prec@1 60.862 Loss: 1.1030\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2335 (1.2335)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.001 (0.003)\tLoss 1.2843 (1.2534)\tPrec@1 37.500 (55.160)\n",
      "EPOCH: 22 val Results: Prec@1 55.160 Loss: 1.2534\n",
      "Best Prec@1: 55.190\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/195]\tTime 0.006 (0.006)\tLoss 0.9948 (0.9948)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [23][39/195]\tTime 0.008 (0.006)\tLoss 1.0577 (1.0543)\tPrec@1 65.234 (62.754)\n",
      "Epoch: [23][78/195]\tTime 0.010 (0.006)\tLoss 1.1018 (1.0686)\tPrec@1 62.109 (62.208)\n",
      "Epoch: [23][117/195]\tTime 0.004 (0.006)\tLoss 1.1000 (1.0760)\tPrec@1 64.453 (61.964)\n",
      "Epoch: [23][156/195]\tTime 0.007 (0.007)\tLoss 1.0969 (1.0879)\tPrec@1 55.859 (61.497)\n",
      "Epoch: [23][195/195]\tTime 0.006 (0.008)\tLoss 1.2745 (1.0979)\tPrec@1 55.000 (61.154)\n",
      "EPOCH: 23 train Results: Prec@1 61.154 Loss: 1.0979\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2573 (1.2573)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4189 (1.2463)\tPrec@1 31.250 (55.250)\n",
      "EPOCH: 23 val Results: Prec@1 55.250 Loss: 1.2463\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/195]\tTime 0.010 (0.010)\tLoss 1.0785 (1.0785)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [24][39/195]\tTime 0.003 (0.010)\tLoss 1.0298 (1.0316)\tPrec@1 64.844 (64.141)\n",
      "Epoch: [24][78/195]\tTime 0.012 (0.012)\tLoss 1.1411 (1.0559)\tPrec@1 60.938 (62.950)\n",
      "Epoch: [24][117/195]\tTime 0.013 (0.011)\tLoss 1.1147 (1.0728)\tPrec@1 60.547 (62.090)\n",
      "Epoch: [24][156/195]\tTime 0.005 (0.010)\tLoss 1.1156 (1.0825)\tPrec@1 62.891 (61.654)\n",
      "Epoch: [24][195/195]\tTime 0.002 (0.010)\tLoss 1.3199 (1.0893)\tPrec@1 52.500 (61.380)\n",
      "EPOCH: 24 train Results: Prec@1 61.380 Loss: 1.0893\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2537 (1.2537)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5932 (1.2560)\tPrec@1 31.250 (55.090)\n",
      "EPOCH: 24 val Results: Prec@1 55.090 Loss: 1.2560\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/195]\tTime 0.005 (0.005)\tLoss 0.9601 (0.9601)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [25][39/195]\tTime 0.004 (0.006)\tLoss 0.9093 (1.0346)\tPrec@1 69.141 (63.770)\n",
      "Epoch: [25][78/195]\tTime 0.005 (0.006)\tLoss 1.1115 (1.0496)\tPrec@1 58.203 (63.222)\n",
      "Epoch: [25][117/195]\tTime 0.015 (0.008)\tLoss 1.1281 (1.0613)\tPrec@1 54.688 (62.579)\n",
      "Epoch: [25][156/195]\tTime 0.009 (0.008)\tLoss 1.1681 (1.0738)\tPrec@1 60.156 (62.000)\n",
      "Epoch: [25][195/195]\tTime 0.002 (0.007)\tLoss 1.2153 (1.0777)\tPrec@1 53.750 (61.772)\n",
      "EPOCH: 25 train Results: Prec@1 61.772 Loss: 1.0777\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2309 (1.2309)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5192 (1.2422)\tPrec@1 31.250 (55.360)\n",
      "EPOCH: 25 val Results: Prec@1 55.360 Loss: 1.2422\n",
      "Best Prec@1: 55.360\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/195]\tTime 0.005 (0.005)\tLoss 1.0869 (1.0869)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [26][39/195]\tTime 0.005 (0.006)\tLoss 0.9887 (1.0331)\tPrec@1 66.016 (63.262)\n",
      "Epoch: [26][78/195]\tTime 0.003 (0.006)\tLoss 1.0584 (1.0468)\tPrec@1 63.281 (63.123)\n",
      "Epoch: [26][117/195]\tTime 0.006 (0.006)\tLoss 1.1059 (1.0548)\tPrec@1 62.500 (62.599)\n",
      "Epoch: [26][156/195]\tTime 0.013 (0.007)\tLoss 1.0642 (1.0674)\tPrec@1 63.281 (62.177)\n",
      "Epoch: [26][195/195]\tTime 0.002 (0.007)\tLoss 1.2224 (1.0762)\tPrec@1 58.750 (61.726)\n",
      "EPOCH: 26 train Results: Prec@1 61.726 Loss: 1.0762\n",
      "Test: [0/39]\tTime 0.007 (0.007)\tLoss 1.2354 (1.2354)\tPrec@1 54.688 (54.688)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6281 (1.2438)\tPrec@1 31.250 (55.270)\n",
      "EPOCH: 26 val Results: Prec@1 55.270 Loss: 1.2438\n",
      "Best Prec@1: 55.360\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/195]\tTime 0.009 (0.009)\tLoss 1.1358 (1.1358)\tPrec@1 57.812 (57.812)\n",
      "Epoch: [27][39/195]\tTime 0.004 (0.006)\tLoss 0.9521 (1.0156)\tPrec@1 66.406 (64.258)\n",
      "Epoch: [27][78/195]\tTime 0.017 (0.006)\tLoss 0.9823 (1.0304)\tPrec@1 65.625 (63.504)\n",
      "Epoch: [27][117/195]\tTime 0.006 (0.006)\tLoss 0.9285 (1.0415)\tPrec@1 66.406 (63.212)\n",
      "Epoch: [27][156/195]\tTime 0.010 (0.006)\tLoss 1.0662 (1.0530)\tPrec@1 58.594 (62.684)\n",
      "Epoch: [27][195/195]\tTime 0.002 (0.006)\tLoss 1.3720 (1.0639)\tPrec@1 51.250 (62.382)\n",
      "EPOCH: 27 train Results: Prec@1 62.382 Loss: 1.0639\n",
      "Test: [0/39]\tTime 0.005 (0.005)\tLoss 1.2093 (1.2093)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6779 (1.2518)\tPrec@1 31.250 (55.020)\n",
      "EPOCH: 27 val Results: Prec@1 55.020 Loss: 1.2518\n",
      "Best Prec@1: 55.360\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/195]\tTime 0.015 (0.015)\tLoss 0.9662 (0.9662)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [28][39/195]\tTime 0.005 (0.007)\tLoss 1.0723 (1.0224)\tPrec@1 64.062 (63.516)\n",
      "Epoch: [28][78/195]\tTime 0.004 (0.006)\tLoss 1.1095 (1.0316)\tPrec@1 58.594 (63.439)\n",
      "Epoch: [28][117/195]\tTime 0.005 (0.006)\tLoss 1.0115 (1.0429)\tPrec@1 62.109 (63.116)\n",
      "Epoch: [28][156/195]\tTime 0.020 (0.006)\tLoss 1.0570 (1.0541)\tPrec@1 61.719 (62.744)\n",
      "Epoch: [28][195/195]\tTime 0.009 (0.006)\tLoss 1.2653 (1.0634)\tPrec@1 51.250 (62.416)\n",
      "EPOCH: 28 train Results: Prec@1 62.416 Loss: 1.0634\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2141 (1.2141)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4850 (1.2348)\tPrec@1 37.500 (55.840)\n",
      "EPOCH: 28 val Results: Prec@1 55.840 Loss: 1.2348\n",
      "Best Prec@1: 55.840\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/195]\tTime 0.004 (0.004)\tLoss 0.9755 (0.9755)\tPrec@1 66.797 (66.797)\n",
      "Epoch: [29][39/195]\tTime 0.005 (0.005)\tLoss 1.0271 (1.0074)\tPrec@1 65.625 (64.717)\n",
      "Epoch: [29][78/195]\tTime 0.012 (0.006)\tLoss 0.9868 (1.0219)\tPrec@1 66.016 (64.062)\n",
      "Epoch: [29][117/195]\tTime 0.004 (0.006)\tLoss 0.9720 (1.0349)\tPrec@1 67.578 (63.516)\n",
      "Epoch: [29][156/195]\tTime 0.005 (0.006)\tLoss 1.0946 (1.0499)\tPrec@1 62.891 (62.888)\n",
      "Epoch: [29][195/195]\tTime 0.003 (0.007)\tLoss 1.1767 (1.0572)\tPrec@1 51.250 (62.460)\n",
      "EPOCH: 29 train Results: Prec@1 62.460 Loss: 1.0572\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2062 (1.2062)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5372 (1.2408)\tPrec@1 18.750 (55.130)\n",
      "EPOCH: 29 val Results: Prec@1 55.130 Loss: 1.2408\n",
      "Best Prec@1: 55.840\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/195]\tTime 0.004 (0.004)\tLoss 1.1157 (1.1157)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [30][39/195]\tTime 0.003 (0.006)\tLoss 1.0793 (1.0158)\tPrec@1 60.938 (64.199)\n",
      "Epoch: [30][78/195]\tTime 0.006 (0.007)\tLoss 1.0307 (1.0211)\tPrec@1 63.281 (64.221)\n",
      "Epoch: [30][117/195]\tTime 0.005 (0.008)\tLoss 0.9884 (1.0322)\tPrec@1 65.234 (63.626)\n",
      "Epoch: [30][156/195]\tTime 0.006 (0.009)\tLoss 1.0501 (1.0415)\tPrec@1 65.234 (63.222)\n",
      "Epoch: [30][195/195]\tTime 0.002 (0.010)\tLoss 0.9210 (1.0513)\tPrec@1 68.750 (62.824)\n",
      "EPOCH: 30 train Results: Prec@1 62.824 Loss: 1.0513\n",
      "Test: [0/39]\tTime 0.014 (0.014)\tLoss 1.2620 (1.2620)\tPrec@1 54.688 (54.688)\n",
      "Test: [39/39]\tTime 0.000 (0.003)\tLoss 1.5119 (1.2483)\tPrec@1 43.750 (55.400)\n",
      "EPOCH: 30 val Results: Prec@1 55.400 Loss: 1.2483\n",
      "Best Prec@1: 55.840\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/195]\tTime 0.016 (0.016)\tLoss 0.9598 (0.9598)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [31][39/195]\tTime 0.004 (0.009)\tLoss 1.0425 (0.9936)\tPrec@1 57.812 (65.039)\n",
      "Epoch: [31][78/195]\tTime 0.005 (0.009)\tLoss 0.9404 (1.0102)\tPrec@1 64.844 (64.147)\n",
      "Epoch: [31][117/195]\tTime 0.016 (0.008)\tLoss 1.0430 (1.0230)\tPrec@1 64.062 (63.798)\n",
      "Epoch: [31][156/195]\tTime 0.004 (0.009)\tLoss 0.9928 (1.0362)\tPrec@1 67.578 (63.306)\n",
      "Epoch: [31][195/195]\tTime 0.002 (0.008)\tLoss 1.0685 (1.0465)\tPrec@1 61.250 (62.984)\n",
      "EPOCH: 31 train Results: Prec@1 62.984 Loss: 1.0465\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2566 (1.2566)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5174 (1.2404)\tPrec@1 25.000 (55.940)\n",
      "EPOCH: 31 val Results: Prec@1 55.940 Loss: 1.2404\n",
      "Best Prec@1: 55.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/195]\tTime 0.007 (0.007)\tLoss 0.9464 (0.9464)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [32][39/195]\tTime 0.007 (0.005)\tLoss 0.9938 (1.0004)\tPrec@1 66.797 (64.980)\n",
      "Epoch: [32][78/195]\tTime 0.009 (0.005)\tLoss 0.9106 (1.0113)\tPrec@1 63.281 (64.374)\n",
      "Epoch: [32][117/195]\tTime 0.010 (0.005)\tLoss 1.1105 (1.0289)\tPrec@1 58.984 (63.731)\n",
      "Epoch: [32][156/195]\tTime 0.016 (0.006)\tLoss 1.1101 (1.0376)\tPrec@1 62.109 (63.363)\n",
      "Epoch: [32][195/195]\tTime 0.002 (0.007)\tLoss 0.9188 (1.0489)\tPrec@1 75.000 (62.974)\n",
      "EPOCH: 32 train Results: Prec@1 62.974 Loss: 1.0489\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2319 (1.2319)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.004)\tLoss 1.4925 (1.2297)\tPrec@1 31.250 (56.160)\n",
      "EPOCH: 32 val Results: Prec@1 56.160 Loss: 1.2297\n",
      "Best Prec@1: 56.160\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/195]\tTime 0.014 (0.014)\tLoss 1.0050 (1.0050)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [33][39/195]\tTime 0.015 (0.010)\tLoss 1.0878 (1.0090)\tPrec@1 64.453 (64.268)\n",
      "Epoch: [33][78/195]\tTime 0.017 (0.012)\tLoss 1.1155 (1.0094)\tPrec@1 61.719 (64.463)\n",
      "Epoch: [33][117/195]\tTime 0.014 (0.012)\tLoss 1.1705 (1.0154)\tPrec@1 63.281 (64.109)\n",
      "Epoch: [33][156/195]\tTime 0.016 (0.011)\tLoss 1.1214 (1.0283)\tPrec@1 59.766 (63.642)\n",
      "Epoch: [33][195/195]\tTime 0.006 (0.011)\tLoss 1.2076 (1.0381)\tPrec@1 58.750 (63.348)\n",
      "EPOCH: 33 train Results: Prec@1 63.348 Loss: 1.0381\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2540 (1.2540)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.7639 (1.2375)\tPrec@1 25.000 (55.620)\n",
      "EPOCH: 33 val Results: Prec@1 55.620 Loss: 1.2375\n",
      "Best Prec@1: 56.160\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/195]\tTime 0.006 (0.006)\tLoss 0.9068 (0.9068)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [34][39/195]\tTime 0.004 (0.006)\tLoss 1.0829 (0.9873)\tPrec@1 60.156 (65.303)\n",
      "Epoch: [34][78/195]\tTime 0.005 (0.007)\tLoss 0.9178 (0.9980)\tPrec@1 70.312 (65.111)\n",
      "Epoch: [34][117/195]\tTime 0.013 (0.007)\tLoss 1.0712 (1.0083)\tPrec@1 63.672 (64.397)\n",
      "Epoch: [34][156/195]\tTime 0.013 (0.007)\tLoss 1.0987 (1.0224)\tPrec@1 60.938 (63.913)\n",
      "Epoch: [34][195/195]\tTime 0.002 (0.007)\tLoss 0.8488 (1.0338)\tPrec@1 72.500 (63.402)\n",
      "EPOCH: 34 train Results: Prec@1 63.402 Loss: 1.0338\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2264 (1.2264)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6393 (1.2266)\tPrec@1 25.000 (56.180)\n",
      "EPOCH: 34 val Results: Prec@1 56.180 Loss: 1.2266\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/195]\tTime 0.015 (0.015)\tLoss 0.9833 (0.9833)\tPrec@1 66.797 (66.797)\n",
      "Epoch: [35][39/195]\tTime 0.007 (0.007)\tLoss 1.0582 (0.9828)\tPrec@1 63.281 (65.293)\n",
      "Epoch: [35][78/195]\tTime 0.006 (0.006)\tLoss 1.2078 (0.9894)\tPrec@1 57.031 (64.666)\n",
      "Epoch: [35][117/195]\tTime 0.008 (0.006)\tLoss 0.9975 (1.0099)\tPrec@1 69.141 (63.970)\n",
      "Epoch: [35][156/195]\tTime 0.004 (0.006)\tLoss 1.1041 (1.0214)\tPrec@1 58.984 (63.595)\n",
      "Epoch: [35][195/195]\tTime 0.002 (0.006)\tLoss 0.9360 (1.0297)\tPrec@1 68.750 (63.394)\n",
      "EPOCH: 35 train Results: Prec@1 63.394 Loss: 1.0297\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2288 (1.2288)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5123 (1.2322)\tPrec@1 31.250 (56.090)\n",
      "EPOCH: 35 val Results: Prec@1 56.090 Loss: 1.2322\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/195]\tTime 0.006 (0.006)\tLoss 1.0166 (1.0166)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [36][39/195]\tTime 0.004 (0.006)\tLoss 1.0149 (0.9736)\tPrec@1 65.234 (65.605)\n",
      "Epoch: [36][78/195]\tTime 0.004 (0.005)\tLoss 0.9887 (0.9908)\tPrec@1 65.625 (64.987)\n",
      "Epoch: [36][117/195]\tTime 0.006 (0.005)\tLoss 0.9821 (1.0008)\tPrec@1 62.891 (64.433)\n",
      "Epoch: [36][156/195]\tTime 0.004 (0.006)\tLoss 1.1193 (1.0136)\tPrec@1 65.234 (63.921)\n",
      "Epoch: [36][195/195]\tTime 0.007 (0.006)\tLoss 1.0833 (1.0259)\tPrec@1 60.000 (63.504)\n",
      "EPOCH: 36 train Results: Prec@1 63.504 Loss: 1.0259\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2207 (1.2207)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4481 (1.2335)\tPrec@1 25.000 (55.850)\n",
      "EPOCH: 36 val Results: Prec@1 55.850 Loss: 1.2335\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/195]\tTime 0.007 (0.007)\tLoss 0.9067 (0.9067)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [37][39/195]\tTime 0.007 (0.006)\tLoss 1.0711 (0.9647)\tPrec@1 63.672 (65.742)\n",
      "Epoch: [37][78/195]\tTime 0.006 (0.006)\tLoss 0.9485 (0.9811)\tPrec@1 63.672 (65.393)\n",
      "Epoch: [37][117/195]\tTime 0.007 (0.006)\tLoss 1.1513 (1.0008)\tPrec@1 58.203 (64.668)\n",
      "Epoch: [37][156/195]\tTime 0.009 (0.006)\tLoss 0.9935 (1.0154)\tPrec@1 65.625 (64.077)\n",
      "Epoch: [37][195/195]\tTime 0.002 (0.006)\tLoss 1.0492 (1.0250)\tPrec@1 66.250 (63.712)\n",
      "EPOCH: 37 train Results: Prec@1 63.712 Loss: 1.0250\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2287 (1.2287)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.001 (0.001)\tLoss 1.5146 (1.2340)\tPrec@1 37.500 (55.730)\n",
      "EPOCH: 37 val Results: Prec@1 55.730 Loss: 1.2340\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/195]\tTime 0.005 (0.005)\tLoss 0.9282 (0.9282)\tPrec@1 65.234 (65.234)\n",
      "Epoch: [38][39/195]\tTime 0.003 (0.005)\tLoss 0.9769 (0.9791)\tPrec@1 62.500 (65.420)\n",
      "Epoch: [38][78/195]\tTime 0.008 (0.005)\tLoss 1.0688 (0.9861)\tPrec@1 61.719 (64.923)\n",
      "Epoch: [38][117/195]\tTime 0.004 (0.005)\tLoss 1.0445 (0.9981)\tPrec@1 62.500 (64.572)\n",
      "Epoch: [38][156/195]\tTime 0.012 (0.006)\tLoss 1.0038 (1.0029)\tPrec@1 67.188 (64.369)\n",
      "Epoch: [38][195/195]\tTime 0.004 (0.006)\tLoss 1.0619 (1.0150)\tPrec@1 60.000 (63.956)\n",
      "EPOCH: 38 train Results: Prec@1 63.956 Loss: 1.0150\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.1849 (1.1849)\tPrec@1 57.812 (57.812)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4587 (1.2314)\tPrec@1 43.750 (56.030)\n",
      "EPOCH: 38 val Results: Prec@1 56.030 Loss: 1.2314\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/195]\tTime 0.004 (0.004)\tLoss 0.9368 (0.9368)\tPrec@1 63.672 (63.672)\n",
      "Epoch: [39][39/195]\tTime 0.005 (0.006)\tLoss 1.1350 (0.9589)\tPrec@1 58.984 (66.182)\n",
      "Epoch: [39][78/195]\tTime 0.014 (0.006)\tLoss 0.9787 (0.9794)\tPrec@1 64.844 (65.239)\n",
      "Epoch: [39][117/195]\tTime 0.011 (0.006)\tLoss 0.9876 (0.9937)\tPrec@1 64.453 (64.711)\n",
      "Epoch: [39][156/195]\tTime 0.007 (0.006)\tLoss 1.1822 (1.0021)\tPrec@1 57.422 (64.463)\n",
      "Epoch: [39][195/195]\tTime 0.001 (0.006)\tLoss 1.1123 (1.0143)\tPrec@1 53.750 (64.096)\n",
      "EPOCH: 39 train Results: Prec@1 64.096 Loss: 1.0143\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1726 (1.1726)\tPrec@1 59.766 (59.766)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.6296 (1.2305)\tPrec@1 31.250 (56.060)\n",
      "EPOCH: 39 val Results: Prec@1 56.060 Loss: 1.2305\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/195]\tTime 0.010 (0.010)\tLoss 0.8871 (0.8871)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [40][39/195]\tTime 0.007 (0.006)\tLoss 0.9853 (0.9611)\tPrec@1 65.234 (66.133)\n",
      "Epoch: [40][78/195]\tTime 0.009 (0.006)\tLoss 0.9990 (0.9664)\tPrec@1 67.188 (65.976)\n",
      "Epoch: [40][117/195]\tTime 0.009 (0.007)\tLoss 0.9894 (0.9803)\tPrec@1 64.453 (65.446)\n",
      "Epoch: [40][156/195]\tTime 0.003 (0.007)\tLoss 1.1088 (0.9974)\tPrec@1 57.031 (64.801)\n",
      "Epoch: [40][195/195]\tTime 0.009 (0.007)\tLoss 1.1842 (1.0137)\tPrec@1 56.250 (64.166)\n",
      "EPOCH: 40 train Results: Prec@1 64.166 Loss: 1.0137\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2707 (1.2707)\tPrec@1 54.688 (54.688)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.6504 (1.2326)\tPrec@1 31.250 (55.920)\n",
      "EPOCH: 40 val Results: Prec@1 55.920 Loss: 1.2326\n",
      "Best Prec@1: 56.180\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/195]\tTime 0.006 (0.006)\tLoss 1.0440 (1.0440)\tPrec@1 64.453 (64.453)\n",
      "Epoch: [41][39/195]\tTime 0.004 (0.006)\tLoss 0.9797 (0.9514)\tPrec@1 66.797 (66.602)\n",
      "Epoch: [41][78/195]\tTime 0.015 (0.006)\tLoss 1.1031 (0.9719)\tPrec@1 57.031 (65.699)\n",
      "Epoch: [41][117/195]\tTime 0.004 (0.006)\tLoss 1.0548 (0.9860)\tPrec@1 61.328 (65.168)\n",
      "Epoch: [41][156/195]\tTime 0.004 (0.006)\tLoss 1.0083 (0.9992)\tPrec@1 66.797 (64.610)\n",
      "Epoch: [41][195/195]\tTime 0.002 (0.006)\tLoss 0.9258 (1.0092)\tPrec@1 71.250 (64.110)\n",
      "EPOCH: 41 train Results: Prec@1 64.110 Loss: 1.0092\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1916 (1.1916)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5560 (1.2239)\tPrec@1 50.000 (56.400)\n",
      "EPOCH: 41 val Results: Prec@1 56.400 Loss: 1.2239\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/195]\tTime 0.005 (0.005)\tLoss 0.9715 (0.9715)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [42][39/195]\tTime 0.005 (0.006)\tLoss 0.9334 (0.9582)\tPrec@1 63.281 (66.279)\n",
      "Epoch: [42][78/195]\tTime 0.006 (0.006)\tLoss 0.9505 (0.9669)\tPrec@1 69.141 (65.872)\n",
      "Epoch: [42][117/195]\tTime 0.007 (0.006)\tLoss 1.0901 (0.9812)\tPrec@1 60.547 (65.079)\n",
      "Epoch: [42][156/195]\tTime 0.008 (0.006)\tLoss 1.0615 (0.9930)\tPrec@1 62.109 (64.575)\n",
      "Epoch: [42][195/195]\tTime 0.002 (0.006)\tLoss 1.1892 (1.0056)\tPrec@1 61.250 (64.126)\n",
      "EPOCH: 42 train Results: Prec@1 64.126 Loss: 1.0056\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2343 (1.2343)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.2757 (1.2349)\tPrec@1 56.250 (56.040)\n",
      "EPOCH: 42 val Results: Prec@1 56.040 Loss: 1.2349\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/195]\tTime 0.005 (0.005)\tLoss 0.9427 (0.9427)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [43][39/195]\tTime 0.004 (0.006)\tLoss 0.9289 (0.9567)\tPrec@1 67.578 (66.006)\n",
      "Epoch: [43][78/195]\tTime 0.013 (0.007)\tLoss 0.9980 (0.9714)\tPrec@1 62.891 (65.551)\n",
      "Epoch: [43][117/195]\tTime 0.011 (0.006)\tLoss 0.9583 (0.9794)\tPrec@1 65.625 (65.258)\n",
      "Epoch: [43][156/195]\tTime 0.012 (0.007)\tLoss 1.0220 (0.9973)\tPrec@1 62.891 (64.518)\n",
      "Epoch: [43][195/195]\tTime 0.005 (0.007)\tLoss 0.9649 (1.0078)\tPrec@1 63.750 (64.108)\n",
      "EPOCH: 43 train Results: Prec@1 64.108 Loss: 1.0078\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2306 (1.2306)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4227 (1.2327)\tPrec@1 37.500 (56.280)\n",
      "EPOCH: 43 val Results: Prec@1 56.280 Loss: 1.2327\n",
      "Best Prec@1: 56.400\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/195]\tTime 0.007 (0.007)\tLoss 0.9285 (0.9285)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [44][39/195]\tTime 0.006 (0.007)\tLoss 0.9499 (0.9561)\tPrec@1 66.016 (66.553)\n",
      "Epoch: [44][78/195]\tTime 0.005 (0.006)\tLoss 0.9311 (0.9665)\tPrec@1 67.578 (65.788)\n",
      "Epoch: [44][117/195]\tTime 0.004 (0.007)\tLoss 0.9893 (0.9840)\tPrec@1 62.500 (65.119)\n",
      "Epoch: [44][156/195]\tTime 0.008 (0.007)\tLoss 1.0356 (0.9952)\tPrec@1 60.938 (64.503)\n",
      "Epoch: [44][195/195]\tTime 0.002 (0.007)\tLoss 1.1082 (1.0033)\tPrec@1 62.500 (64.152)\n",
      "EPOCH: 44 train Results: Prec@1 64.152 Loss: 1.0033\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2340 (1.2340)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.001 (0.002)\tLoss 1.6621 (1.2266)\tPrec@1 31.250 (56.520)\n",
      "EPOCH: 44 val Results: Prec@1 56.520 Loss: 1.2266\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/195]\tTime 0.005 (0.005)\tLoss 0.9693 (0.9693)\tPrec@1 66.797 (66.797)\n",
      "Epoch: [45][39/195]\tTime 0.004 (0.005)\tLoss 1.0716 (0.9453)\tPrec@1 62.500 (66.514)\n",
      "Epoch: [45][78/195]\tTime 0.003 (0.005)\tLoss 1.0481 (0.9528)\tPrec@1 58.984 (66.134)\n",
      "Epoch: [45][117/195]\tTime 0.004 (0.005)\tLoss 1.0616 (0.9706)\tPrec@1 62.891 (65.400)\n",
      "Epoch: [45][156/195]\tTime 0.003 (0.006)\tLoss 1.0113 (0.9856)\tPrec@1 59.766 (64.839)\n",
      "Epoch: [45][195/195]\tTime 0.006 (0.006)\tLoss 1.1646 (0.9944)\tPrec@1 63.750 (64.618)\n",
      "EPOCH: 45 train Results: Prec@1 64.618 Loss: 0.9944\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2654 (1.2654)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.7498 (1.2388)\tPrec@1 37.500 (55.460)\n",
      "EPOCH: 45 val Results: Prec@1 55.460 Loss: 1.2388\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/195]\tTime 0.008 (0.008)\tLoss 0.9352 (0.9352)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [46][39/195]\tTime 0.007 (0.006)\tLoss 0.9669 (0.9470)\tPrec@1 66.797 (66.680)\n",
      "Epoch: [46][78/195]\tTime 0.010 (0.006)\tLoss 0.9407 (0.9496)\tPrec@1 64.844 (66.421)\n",
      "Epoch: [46][117/195]\tTime 0.004 (0.005)\tLoss 1.0658 (0.9670)\tPrec@1 62.109 (65.741)\n",
      "Epoch: [46][156/195]\tTime 0.004 (0.005)\tLoss 0.9806 (0.9768)\tPrec@1 64.844 (65.416)\n",
      "Epoch: [46][195/195]\tTime 0.003 (0.005)\tLoss 0.9118 (0.9899)\tPrec@1 66.250 (64.882)\n",
      "EPOCH: 46 train Results: Prec@1 64.882 Loss: 0.9899\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2505 (1.2505)\tPrec@1 57.812 (57.812)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5956 (1.2365)\tPrec@1 25.000 (56.170)\n",
      "EPOCH: 46 val Results: Prec@1 56.170 Loss: 1.2365\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/195]\tTime 0.009 (0.009)\tLoss 1.0820 (1.0820)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [47][39/195]\tTime 0.007 (0.008)\tLoss 1.0022 (0.9364)\tPrec@1 65.234 (67.051)\n",
      "Epoch: [47][78/195]\tTime 0.004 (0.007)\tLoss 1.0763 (0.9512)\tPrec@1 62.500 (66.555)\n",
      "Epoch: [47][117/195]\tTime 0.004 (0.007)\tLoss 0.9757 (0.9702)\tPrec@1 65.625 (65.817)\n",
      "Epoch: [47][156/195]\tTime 0.004 (0.006)\tLoss 0.9665 (0.9776)\tPrec@1 69.141 (65.488)\n",
      "Epoch: [47][195/195]\tTime 0.002 (0.006)\tLoss 1.0828 (0.9890)\tPrec@1 58.750 (65.088)\n",
      "EPOCH: 47 train Results: Prec@1 65.088 Loss: 0.9890\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1893 (1.1893)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5657 (1.2315)\tPrec@1 31.250 (56.110)\n",
      "EPOCH: 47 val Results: Prec@1 56.110 Loss: 1.2315\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/195]\tTime 0.005 (0.005)\tLoss 0.9042 (0.9042)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [48][39/195]\tTime 0.006 (0.006)\tLoss 1.0340 (0.9451)\tPrec@1 64.062 (66.914)\n",
      "Epoch: [48][78/195]\tTime 0.009 (0.006)\tLoss 0.9345 (0.9540)\tPrec@1 67.578 (66.451)\n",
      "Epoch: [48][117/195]\tTime 0.007 (0.006)\tLoss 1.0290 (0.9670)\tPrec@1 65.234 (65.903)\n",
      "Epoch: [48][156/195]\tTime 0.004 (0.006)\tLoss 1.0449 (0.9801)\tPrec@1 65.234 (65.379)\n",
      "Epoch: [48][195/195]\tTime 0.003 (0.006)\tLoss 1.2950 (0.9900)\tPrec@1 58.750 (65.060)\n",
      "EPOCH: 48 train Results: Prec@1 65.060 Loss: 0.9900\n",
      "Test: [0/39]\tTime 0.006 (0.006)\tLoss 1.2281 (1.2281)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6548 (1.2264)\tPrec@1 31.250 (56.170)\n",
      "EPOCH: 48 val Results: Prec@1 56.170 Loss: 1.2264\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/195]\tTime 0.005 (0.005)\tLoss 0.9662 (0.9662)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [49][39/195]\tTime 0.004 (0.006)\tLoss 0.9097 (0.9224)\tPrec@1 66.797 (67.461)\n",
      "Epoch: [49][78/195]\tTime 0.004 (0.006)\tLoss 1.0262 (0.9463)\tPrec@1 64.844 (66.634)\n",
      "Epoch: [49][117/195]\tTime 0.010 (0.006)\tLoss 0.9549 (0.9619)\tPrec@1 63.672 (65.966)\n",
      "Epoch: [49][156/195]\tTime 0.010 (0.006)\tLoss 0.9358 (0.9741)\tPrec@1 66.797 (65.391)\n",
      "Epoch: [49][195/195]\tTime 0.002 (0.006)\tLoss 1.2806 (0.9856)\tPrec@1 55.000 (64.934)\n",
      "EPOCH: 49 train Results: Prec@1 64.934 Loss: 0.9856\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2571 (1.2571)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3651 (1.2324)\tPrec@1 37.500 (55.960)\n",
      "EPOCH: 49 val Results: Prec@1 55.960 Loss: 1.2324\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/195]\tTime 0.008 (0.008)\tLoss 0.9385 (0.9385)\tPrec@1 66.016 (66.016)\n",
      "Epoch: [50][39/195]\tTime 0.008 (0.007)\tLoss 0.9777 (0.9329)\tPrec@1 66.016 (67.559)\n",
      "Epoch: [50][78/195]\tTime 0.005 (0.006)\tLoss 0.9793 (0.9471)\tPrec@1 62.891 (66.960)\n",
      "Epoch: [50][117/195]\tTime 0.006 (0.006)\tLoss 1.1518 (0.9610)\tPrec@1 58.594 (66.466)\n",
      "Epoch: [50][156/195]\tTime 0.007 (0.006)\tLoss 0.9910 (0.9726)\tPrec@1 67.188 (65.948)\n",
      "Epoch: [50][195/195]\tTime 0.003 (0.006)\tLoss 0.9394 (0.9868)\tPrec@1 70.000 (65.422)\n",
      "EPOCH: 50 train Results: Prec@1 65.422 Loss: 0.9868\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2285 (1.2285)\tPrec@1 51.562 (51.562)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5650 (1.2256)\tPrec@1 37.500 (56.090)\n",
      "EPOCH: 50 val Results: Prec@1 56.090 Loss: 1.2256\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/195]\tTime 0.013 (0.013)\tLoss 0.9459 (0.9459)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [51][39/195]\tTime 0.008 (0.006)\tLoss 0.9927 (0.9331)\tPrec@1 66.797 (66.963)\n",
      "Epoch: [51][78/195]\tTime 0.006 (0.007)\tLoss 0.9537 (0.9550)\tPrec@1 65.625 (66.035)\n",
      "Epoch: [51][117/195]\tTime 0.004 (0.006)\tLoss 1.0648 (0.9664)\tPrec@1 63.281 (65.529)\n",
      "Epoch: [51][156/195]\tTime 0.010 (0.006)\tLoss 1.0188 (0.9746)\tPrec@1 67.188 (65.346)\n",
      "Epoch: [51][195/195]\tTime 0.003 (0.006)\tLoss 0.9524 (0.9854)\tPrec@1 67.500 (65.034)\n",
      "EPOCH: 51 train Results: Prec@1 65.034 Loss: 0.9854\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2726 (1.2726)\tPrec@1 54.688 (54.688)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5287 (1.2338)\tPrec@1 37.500 (56.340)\n",
      "EPOCH: 51 val Results: Prec@1 56.340 Loss: 1.2338\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/195]\tTime 0.007 (0.007)\tLoss 0.8394 (0.8394)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [52][39/195]\tTime 0.006 (0.008)\tLoss 0.9630 (0.9217)\tPrec@1 69.141 (68.096)\n",
      "Epoch: [52][78/195]\tTime 0.004 (0.007)\tLoss 1.0601 (0.9351)\tPrec@1 61.328 (67.435)\n",
      "Epoch: [52][117/195]\tTime 0.012 (0.007)\tLoss 0.9592 (0.9500)\tPrec@1 67.969 (66.790)\n",
      "Epoch: [52][156/195]\tTime 0.005 (0.007)\tLoss 1.1510 (0.9665)\tPrec@1 59.766 (66.190)\n",
      "Epoch: [52][195/195]\tTime 0.002 (0.007)\tLoss 1.1077 (0.9801)\tPrec@1 62.500 (65.542)\n",
      "EPOCH: 52 train Results: Prec@1 65.542 Loss: 0.9801\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2415 (1.2415)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4677 (1.2358)\tPrec@1 31.250 (56.150)\n",
      "EPOCH: 52 val Results: Prec@1 56.150 Loss: 1.2358\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/195]\tTime 0.008 (0.008)\tLoss 0.9275 (0.9275)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [53][39/195]\tTime 0.006 (0.006)\tLoss 0.9120 (0.9154)\tPrec@1 70.703 (67.900)\n",
      "Epoch: [53][78/195]\tTime 0.013 (0.006)\tLoss 1.0279 (0.9347)\tPrec@1 68.359 (67.074)\n",
      "Epoch: [53][117/195]\tTime 0.003 (0.006)\tLoss 0.9998 (0.9523)\tPrec@1 66.016 (66.307)\n",
      "Epoch: [53][156/195]\tTime 0.014 (0.006)\tLoss 1.0352 (0.9632)\tPrec@1 63.672 (65.988)\n",
      "Epoch: [53][195/195]\tTime 0.002 (0.006)\tLoss 1.0049 (0.9758)\tPrec@1 68.750 (65.500)\n",
      "EPOCH: 53 train Results: Prec@1 65.500 Loss: 0.9758\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2812 (1.2812)\tPrec@1 53.906 (53.906)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3800 (1.2407)\tPrec@1 25.000 (56.040)\n",
      "EPOCH: 53 val Results: Prec@1 56.040 Loss: 1.2407\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/195]\tTime 0.004 (0.004)\tLoss 0.8830 (0.8830)\tPrec@1 69.922 (69.922)\n",
      "Epoch: [54][39/195]\tTime 0.004 (0.006)\tLoss 0.9375 (0.9238)\tPrec@1 62.891 (67.012)\n",
      "Epoch: [54][78/195]\tTime 0.003 (0.006)\tLoss 0.9640 (0.9382)\tPrec@1 67.578 (66.742)\n",
      "Epoch: [54][117/195]\tTime 0.010 (0.006)\tLoss 0.9664 (0.9537)\tPrec@1 66.406 (66.029)\n",
      "Epoch: [54][156/195]\tTime 0.006 (0.006)\tLoss 1.0091 (0.9679)\tPrec@1 65.234 (65.501)\n",
      "Epoch: [54][195/195]\tTime 0.003 (0.006)\tLoss 1.0894 (0.9807)\tPrec@1 60.000 (65.018)\n",
      "EPOCH: 54 train Results: Prec@1 65.018 Loss: 0.9807\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2429 (1.2429)\tPrec@1 54.297 (54.297)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3469 (1.2344)\tPrec@1 37.500 (55.990)\n",
      "EPOCH: 54 val Results: Prec@1 55.990 Loss: 1.2344\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/195]\tTime 0.007 (0.007)\tLoss 0.9507 (0.9507)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [55][39/195]\tTime 0.005 (0.006)\tLoss 0.9400 (0.9156)\tPrec@1 70.312 (68.027)\n",
      "Epoch: [55][78/195]\tTime 0.009 (0.006)\tLoss 0.9415 (0.9362)\tPrec@1 67.578 (66.866)\n",
      "Epoch: [55][117/195]\tTime 0.019 (0.006)\tLoss 1.0076 (0.9469)\tPrec@1 62.109 (66.429)\n",
      "Epoch: [55][156/195]\tTime 0.007 (0.007)\tLoss 0.9465 (0.9567)\tPrec@1 64.453 (66.065)\n",
      "Epoch: [55][195/195]\tTime 0.002 (0.006)\tLoss 1.0529 (0.9689)\tPrec@1 62.500 (65.684)\n",
      "EPOCH: 55 train Results: Prec@1 65.684 Loss: 0.9689\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.3320 (1.3320)\tPrec@1 50.391 (50.391)\n",
      "Test: [39/39]\tTime 0.001 (0.001)\tLoss 1.6619 (1.2452)\tPrec@1 31.250 (55.750)\n",
      "EPOCH: 55 val Results: Prec@1 55.750 Loss: 1.2452\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/195]\tTime 0.009 (0.009)\tLoss 0.9332 (0.9332)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [56][39/195]\tTime 0.004 (0.006)\tLoss 0.9042 (0.9303)\tPrec@1 63.281 (66.846)\n",
      "Epoch: [56][78/195]\tTime 0.004 (0.006)\tLoss 0.8478 (0.9348)\tPrec@1 70.312 (66.767)\n",
      "Epoch: [56][117/195]\tTime 0.005 (0.006)\tLoss 0.9804 (0.9442)\tPrec@1 68.750 (66.588)\n",
      "Epoch: [56][156/195]\tTime 0.008 (0.006)\tLoss 0.9822 (0.9574)\tPrec@1 63.672 (66.093)\n",
      "Epoch: [56][195/195]\tTime 0.002 (0.006)\tLoss 1.0338 (0.9713)\tPrec@1 66.250 (65.632)\n",
      "EPOCH: 56 train Results: Prec@1 65.632 Loss: 0.9713\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1733 (1.1733)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4030 (1.2387)\tPrec@1 37.500 (55.500)\n",
      "EPOCH: 56 val Results: Prec@1 55.500 Loss: 1.2387\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/195]\tTime 0.004 (0.004)\tLoss 0.9505 (0.9505)\tPrec@1 66.016 (66.016)\n",
      "Epoch: [57][39/195]\tTime 0.003 (0.006)\tLoss 0.8604 (0.9117)\tPrec@1 74.219 (68.281)\n",
      "Epoch: [57][78/195]\tTime 0.005 (0.006)\tLoss 0.8988 (0.9367)\tPrec@1 71.484 (67.267)\n",
      "Epoch: [57][117/195]\tTime 0.006 (0.006)\tLoss 1.0520 (0.9431)\tPrec@1 63.281 (66.873)\n",
      "Epoch: [57][156/195]\tTime 0.005 (0.006)\tLoss 1.0534 (0.9571)\tPrec@1 63.672 (66.337)\n",
      "Epoch: [57][195/195]\tTime 0.001 (0.006)\tLoss 0.9727 (0.9719)\tPrec@1 62.500 (65.744)\n",
      "EPOCH: 57 train Results: Prec@1 65.744 Loss: 0.9719\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.3043 (1.3043)\tPrec@1 54.688 (54.688)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4924 (1.2406)\tPrec@1 31.250 (56.090)\n",
      "EPOCH: 57 val Results: Prec@1 56.090 Loss: 1.2406\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/195]\tTime 0.009 (0.009)\tLoss 0.9038 (0.9038)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [58][39/195]\tTime 0.004 (0.005)\tLoss 0.9352 (0.9153)\tPrec@1 67.188 (68.086)\n",
      "Epoch: [58][78/195]\tTime 0.004 (0.005)\tLoss 1.0092 (0.9325)\tPrec@1 63.281 (67.212)\n",
      "Epoch: [58][117/195]\tTime 0.004 (0.006)\tLoss 0.9429 (0.9438)\tPrec@1 64.453 (66.578)\n",
      "Epoch: [58][156/195]\tTime 0.013 (0.006)\tLoss 0.9219 (0.9544)\tPrec@1 66.406 (66.304)\n",
      "Epoch: [58][195/195]\tTime 0.003 (0.006)\tLoss 1.0712 (0.9680)\tPrec@1 63.750 (65.714)\n",
      "EPOCH: 58 train Results: Prec@1 65.714 Loss: 0.9680\n",
      "Test: [0/39]\tTime 0.012 (0.012)\tLoss 1.2043 (1.2043)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5267 (1.2356)\tPrec@1 31.250 (55.540)\n",
      "EPOCH: 58 val Results: Prec@1 55.540 Loss: 1.2356\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/195]\tTime 0.004 (0.004)\tLoss 0.8838 (0.8838)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [59][39/195]\tTime 0.006 (0.006)\tLoss 0.9143 (0.9172)\tPrec@1 69.531 (67.686)\n",
      "Epoch: [59][78/195]\tTime 0.008 (0.006)\tLoss 1.0742 (0.9240)\tPrec@1 59.375 (67.207)\n",
      "Epoch: [59][117/195]\tTime 0.004 (0.006)\tLoss 0.9135 (0.9409)\tPrec@1 67.188 (66.648)\n",
      "Epoch: [59][156/195]\tTime 0.005 (0.006)\tLoss 1.0693 (0.9550)\tPrec@1 62.891 (66.103)\n",
      "Epoch: [59][195/195]\tTime 0.003 (0.006)\tLoss 0.9517 (0.9661)\tPrec@1 68.750 (65.784)\n",
      "EPOCH: 59 train Results: Prec@1 65.784 Loss: 0.9661\n",
      "Test: [0/39]\tTime 0.012 (0.012)\tLoss 1.2577 (1.2577)\tPrec@1 53.516 (53.516)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3796 (1.2507)\tPrec@1 56.250 (55.610)\n",
      "EPOCH: 59 val Results: Prec@1 55.610 Loss: 1.2507\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/195]\tTime 0.008 (0.008)\tLoss 0.8623 (0.8623)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [60][39/195]\tTime 0.005 (0.005)\tLoss 0.8720 (0.9233)\tPrec@1 69.531 (68.008)\n",
      "Epoch: [60][78/195]\tTime 0.003 (0.006)\tLoss 0.8280 (0.9276)\tPrec@1 73.047 (67.573)\n",
      "Epoch: [60][117/195]\tTime 0.013 (0.006)\tLoss 0.9405 (0.9412)\tPrec@1 64.844 (66.860)\n",
      "Epoch: [60][156/195]\tTime 0.009 (0.006)\tLoss 1.0319 (0.9544)\tPrec@1 63.672 (66.356)\n",
      "Epoch: [60][195/195]\tTime 0.071 (0.007)\tLoss 1.1129 (0.9625)\tPrec@1 61.250 (65.984)\n",
      "EPOCH: 60 train Results: Prec@1 65.984 Loss: 0.9625\n",
      "Test: [0/39]\tTime 0.008 (0.008)\tLoss 1.2489 (1.2489)\tPrec@1 59.766 (59.766)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2313 (1.2444)\tPrec@1 43.750 (55.560)\n",
      "EPOCH: 60 val Results: Prec@1 55.560 Loss: 1.2444\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/195]\tTime 0.006 (0.006)\tLoss 0.8690 (0.8690)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [61][39/195]\tTime 0.006 (0.005)\tLoss 0.9666 (0.9046)\tPrec@1 66.797 (68.564)\n",
      "Epoch: [61][78/195]\tTime 0.008 (0.005)\tLoss 1.0197 (0.9230)\tPrec@1 65.625 (67.544)\n",
      "Epoch: [61][117/195]\tTime 0.004 (0.006)\tLoss 0.9969 (0.9426)\tPrec@1 65.625 (66.787)\n",
      "Epoch: [61][156/195]\tTime 0.004 (0.006)\tLoss 0.9834 (0.9562)\tPrec@1 64.844 (66.167)\n",
      "Epoch: [61][195/195]\tTime 0.002 (0.006)\tLoss 1.0442 (0.9670)\tPrec@1 63.750 (65.738)\n",
      "EPOCH: 61 train Results: Prec@1 65.738 Loss: 0.9670\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2518 (1.2518)\tPrec@1 54.688 (54.688)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3628 (1.2401)\tPrec@1 18.750 (55.880)\n",
      "EPOCH: 61 val Results: Prec@1 55.880 Loss: 1.2401\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/195]\tTime 0.009 (0.009)\tLoss 0.8758 (0.8758)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [62][39/195]\tTime 0.004 (0.005)\tLoss 0.8812 (0.8964)\tPrec@1 67.969 (68.691)\n",
      "Epoch: [62][78/195]\tTime 0.004 (0.006)\tLoss 0.8487 (0.9097)\tPrec@1 67.969 (67.776)\n",
      "Epoch: [62][117/195]\tTime 0.004 (0.005)\tLoss 1.0979 (0.9304)\tPrec@1 61.328 (67.002)\n",
      "Epoch: [62][156/195]\tTime 0.004 (0.005)\tLoss 1.0978 (0.9454)\tPrec@1 62.500 (66.458)\n",
      "Epoch: [62][195/195]\tTime 0.003 (0.005)\tLoss 0.7858 (0.9589)\tPrec@1 73.750 (66.018)\n",
      "EPOCH: 62 train Results: Prec@1 66.018 Loss: 0.9589\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2585 (1.2585)\tPrec@1 53.906 (53.906)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3265 (1.2471)\tPrec@1 43.750 (55.750)\n",
      "EPOCH: 62 val Results: Prec@1 55.750 Loss: 1.2471\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/195]\tTime 0.004 (0.004)\tLoss 0.8243 (0.8243)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [63][39/195]\tTime 0.003 (0.005)\tLoss 0.9236 (0.9076)\tPrec@1 65.625 (68.057)\n",
      "Epoch: [63][78/195]\tTime 0.011 (0.005)\tLoss 0.9209 (0.9274)\tPrec@1 68.750 (67.128)\n",
      "Epoch: [63][117/195]\tTime 0.005 (0.006)\tLoss 1.1623 (0.9452)\tPrec@1 60.547 (66.396)\n",
      "Epoch: [63][156/195]\tTime 0.005 (0.006)\tLoss 1.1118 (0.9569)\tPrec@1 59.375 (66.068)\n",
      "Epoch: [63][195/195]\tTime 0.002 (0.006)\tLoss 0.9937 (0.9664)\tPrec@1 67.500 (65.722)\n",
      "EPOCH: 63 train Results: Prec@1 65.722 Loss: 0.9664\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2381 (1.2381)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3521 (1.2314)\tPrec@1 25.000 (55.660)\n",
      "EPOCH: 63 val Results: Prec@1 55.660 Loss: 1.2314\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/195]\tTime 0.009 (0.009)\tLoss 0.8399 (0.8399)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [64][39/195]\tTime 0.005 (0.006)\tLoss 0.8595 (0.8950)\tPrec@1 69.531 (68.369)\n",
      "Epoch: [64][78/195]\tTime 0.053 (0.006)\tLoss 0.9151 (0.9188)\tPrec@1 65.234 (67.459)\n",
      "Epoch: [64][117/195]\tTime 0.007 (0.006)\tLoss 0.9437 (0.9349)\tPrec@1 65.234 (66.906)\n",
      "Epoch: [64][156/195]\tTime 0.008 (0.007)\tLoss 1.0411 (0.9489)\tPrec@1 61.328 (66.396)\n",
      "Epoch: [64][195/195]\tTime 0.002 (0.006)\tLoss 1.1240 (0.9601)\tPrec@1 63.750 (66.136)\n",
      "EPOCH: 64 train Results: Prec@1 66.136 Loss: 0.9601\n",
      "Test: [0/39]\tTime 0.006 (0.006)\tLoss 1.2667 (1.2667)\tPrec@1 51.172 (51.172)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4949 (1.2442)\tPrec@1 31.250 (56.050)\n",
      "EPOCH: 64 val Results: Prec@1 56.050 Loss: 1.2442\n",
      "Best Prec@1: 56.520\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/195]\tTime 0.009 (0.009)\tLoss 0.9259 (0.9259)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [65][39/195]\tTime 0.008 (0.006)\tLoss 0.8818 (0.9011)\tPrec@1 68.359 (68.203)\n",
      "Epoch: [65][78/195]\tTime 0.004 (0.006)\tLoss 0.8900 (0.9092)\tPrec@1 70.312 (67.820)\n",
      "Epoch: [65][117/195]\tTime 0.007 (0.006)\tLoss 0.8585 (0.9292)\tPrec@1 69.531 (67.095)\n",
      "Epoch: [65][156/195]\tTime 0.005 (0.006)\tLoss 0.9539 (0.9459)\tPrec@1 63.281 (66.516)\n",
      "Epoch: [65][195/195]\tTime 0.007 (0.006)\tLoss 1.0961 (0.9586)\tPrec@1 56.250 (66.012)\n",
      "EPOCH: 65 train Results: Prec@1 66.012 Loss: 0.9586\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2525 (1.2525)\tPrec@1 53.906 (53.906)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3652 (1.2299)\tPrec@1 43.750 (56.560)\n",
      "EPOCH: 65 val Results: Prec@1 56.560 Loss: 1.2299\n",
      "Best Prec@1: 56.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/195]\tTime 0.006 (0.006)\tLoss 0.9339 (0.9339)\tPrec@1 66.016 (66.016)\n",
      "Epoch: [66][39/195]\tTime 0.007 (0.006)\tLoss 0.9262 (0.8913)\tPrec@1 69.531 (68.525)\n",
      "Epoch: [66][78/195]\tTime 0.004 (0.006)\tLoss 0.8927 (0.9074)\tPrec@1 70.703 (67.845)\n",
      "Epoch: [66][117/195]\tTime 0.003 (0.006)\tLoss 1.0790 (0.9248)\tPrec@1 60.156 (66.992)\n",
      "Epoch: [66][156/195]\tTime 0.010 (0.006)\tLoss 1.0166 (0.9410)\tPrec@1 65.625 (66.486)\n",
      "Epoch: [66][195/195]\tTime 0.002 (0.006)\tLoss 0.9828 (0.9514)\tPrec@1 62.500 (66.190)\n",
      "EPOCH: 66 train Results: Prec@1 66.190 Loss: 0.9514\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2799 (1.2799)\tPrec@1 54.297 (54.297)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5024 (1.2508)\tPrec@1 37.500 (55.880)\n",
      "EPOCH: 66 val Results: Prec@1 55.880 Loss: 1.2508\n",
      "Best Prec@1: 56.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/195]\tTime 0.006 (0.006)\tLoss 0.8936 (0.8936)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [67][39/195]\tTime 0.009 (0.006)\tLoss 0.9445 (0.8968)\tPrec@1 65.234 (68.213)\n",
      "Epoch: [67][78/195]\tTime 0.009 (0.006)\tLoss 0.9139 (0.9102)\tPrec@1 66.406 (67.717)\n",
      "Epoch: [67][117/195]\tTime 0.004 (0.006)\tLoss 1.0595 (0.9304)\tPrec@1 63.672 (67.224)\n",
      "Epoch: [67][156/195]\tTime 0.004 (0.006)\tLoss 0.9596 (0.9399)\tPrec@1 65.234 (66.680)\n",
      "Epoch: [67][195/195]\tTime 0.003 (0.006)\tLoss 0.9656 (0.9525)\tPrec@1 65.000 (66.176)\n",
      "EPOCH: 67 train Results: Prec@1 66.176 Loss: 0.9525\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2307 (1.2307)\tPrec@1 53.516 (53.516)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3549 (1.2445)\tPrec@1 37.500 (55.500)\n",
      "EPOCH: 67 val Results: Prec@1 55.500 Loss: 1.2445\n",
      "Best Prec@1: 56.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/195]\tTime 0.005 (0.005)\tLoss 0.8990 (0.8990)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [68][39/195]\tTime 0.006 (0.006)\tLoss 0.9160 (0.8844)\tPrec@1 67.188 (69.072)\n",
      "Epoch: [68][78/195]\tTime 0.003 (0.006)\tLoss 1.0110 (0.9122)\tPrec@1 64.062 (67.588)\n",
      "Epoch: [68][117/195]\tTime 0.004 (0.006)\tLoss 0.9762 (0.9282)\tPrec@1 67.969 (66.926)\n",
      "Epoch: [68][156/195]\tTime 0.003 (0.006)\tLoss 1.0081 (0.9426)\tPrec@1 64.453 (66.541)\n",
      "Epoch: [68][195/195]\tTime 0.012 (0.006)\tLoss 1.2233 (0.9542)\tPrec@1 56.250 (66.094)\n",
      "EPOCH: 68 train Results: Prec@1 66.094 Loss: 0.9542\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2513 (1.2513)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3873 (1.2438)\tPrec@1 37.500 (56.110)\n",
      "EPOCH: 68 val Results: Prec@1 56.110 Loss: 1.2438\n",
      "Best Prec@1: 56.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/195]\tTime 0.004 (0.004)\tLoss 0.8364 (0.8364)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [69][39/195]\tTime 0.004 (0.005)\tLoss 0.8608 (0.8705)\tPrec@1 66.797 (69.443)\n",
      "Epoch: [69][78/195]\tTime 0.006 (0.005)\tLoss 0.9716 (0.8982)\tPrec@1 68.750 (68.295)\n",
      "Epoch: [69][117/195]\tTime 0.007 (0.005)\tLoss 0.9136 (0.9178)\tPrec@1 70.703 (67.512)\n",
      "Epoch: [69][156/195]\tTime 0.003 (0.005)\tLoss 0.9673 (0.9347)\tPrec@1 67.578 (66.936)\n",
      "Epoch: [69][195/195]\tTime 0.002 (0.005)\tLoss 1.1650 (0.9481)\tPrec@1 56.250 (66.398)\n",
      "EPOCH: 69 train Results: Prec@1 66.398 Loss: 0.9481\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2296 (1.2296)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.6638 (1.2411)\tPrec@1 25.000 (56.230)\n",
      "EPOCH: 69 val Results: Prec@1 56.230 Loss: 1.2411\n",
      "Best Prec@1: 56.560\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/195]\tTime 0.008 (0.008)\tLoss 0.8020 (0.8020)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [70][39/195]\tTime 0.007 (0.006)\tLoss 1.0309 (0.8834)\tPrec@1 58.594 (69.258)\n",
      "Epoch: [70][78/195]\tTime 0.004 (0.006)\tLoss 0.8666 (0.8985)\tPrec@1 69.141 (68.710)\n",
      "Epoch: [70][117/195]\tTime 0.003 (0.006)\tLoss 0.9514 (0.9192)\tPrec@1 66.406 (67.810)\n",
      "Epoch: [70][156/195]\tTime 0.006 (0.006)\tLoss 1.0429 (0.9416)\tPrec@1 61.719 (66.822)\n",
      "Epoch: [70][195/195]\tTime 0.003 (0.006)\tLoss 1.0298 (0.9519)\tPrec@1 58.750 (66.354)\n",
      "EPOCH: 70 train Results: Prec@1 66.354 Loss: 0.9519\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.1933 (1.1933)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6582 (1.2374)\tPrec@1 18.750 (56.590)\n",
      "EPOCH: 70 val Results: Prec@1 56.590 Loss: 1.2374\n",
      "Best Prec@1: 56.590\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/195]\tTime 0.008 (0.008)\tLoss 0.8725 (0.8725)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [71][39/195]\tTime 0.006 (0.007)\tLoss 0.8575 (0.8788)\tPrec@1 67.969 (69.473)\n",
      "Epoch: [71][78/195]\tTime 0.066 (0.008)\tLoss 1.0397 (0.9030)\tPrec@1 64.453 (68.478)\n",
      "Epoch: [71][117/195]\tTime 0.014 (0.007)\tLoss 0.9472 (0.9263)\tPrec@1 65.234 (67.459)\n",
      "Epoch: [71][156/195]\tTime 0.012 (0.007)\tLoss 0.9415 (0.9362)\tPrec@1 69.141 (67.123)\n",
      "Epoch: [71][195/195]\tTime 0.002 (0.006)\tLoss 0.9067 (0.9502)\tPrec@1 67.500 (66.520)\n",
      "EPOCH: 71 train Results: Prec@1 66.520 Loss: 0.9502\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2252 (1.2252)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3308 (1.2411)\tPrec@1 31.250 (56.270)\n",
      "EPOCH: 71 val Results: Prec@1 56.270 Loss: 1.2411\n",
      "Best Prec@1: 56.590\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/195]\tTime 0.008 (0.008)\tLoss 0.8052 (0.8052)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [72][39/195]\tTime 0.006 (0.006)\tLoss 1.0644 (0.8886)\tPrec@1 60.547 (68.789)\n",
      "Epoch: [72][78/195]\tTime 0.006 (0.006)\tLoss 0.9901 (0.9039)\tPrec@1 63.672 (68.142)\n",
      "Epoch: [72][117/195]\tTime 0.018 (0.006)\tLoss 1.0069 (0.9204)\tPrec@1 62.500 (67.413)\n",
      "Epoch: [72][156/195]\tTime 0.003 (0.006)\tLoss 0.9936 (0.9367)\tPrec@1 65.234 (66.732)\n",
      "Epoch: [72][195/195]\tTime 0.002 (0.006)\tLoss 0.9354 (0.9473)\tPrec@1 63.750 (66.386)\n",
      "EPOCH: 72 train Results: Prec@1 66.386 Loss: 0.9473\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2364 (1.2364)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.003)\tLoss 1.4171 (1.2321)\tPrec@1 37.500 (56.630)\n",
      "EPOCH: 72 val Results: Prec@1 56.630 Loss: 1.2321\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/195]\tTime 0.007 (0.007)\tLoss 0.8286 (0.8286)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [73][39/195]\tTime 0.009 (0.006)\tLoss 0.8889 (0.8868)\tPrec@1 69.531 (68.311)\n",
      "Epoch: [73][78/195]\tTime 0.006 (0.006)\tLoss 0.9725 (0.8953)\tPrec@1 66.406 (67.939)\n",
      "Epoch: [73][117/195]\tTime 0.015 (0.006)\tLoss 0.8884 (0.9111)\tPrec@1 67.969 (67.350)\n",
      "Epoch: [73][156/195]\tTime 0.006 (0.006)\tLoss 0.8901 (0.9274)\tPrec@1 66.797 (66.929)\n",
      "Epoch: [73][195/195]\tTime 0.002 (0.006)\tLoss 0.9821 (0.9411)\tPrec@1 61.250 (66.538)\n",
      "EPOCH: 73 train Results: Prec@1 66.538 Loss: 0.9411\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1999 (1.1999)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5822 (1.2431)\tPrec@1 31.250 (56.420)\n",
      "EPOCH: 73 val Results: Prec@1 56.420 Loss: 1.2431\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/195]\tTime 0.004 (0.004)\tLoss 0.8786 (0.8786)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [74][39/195]\tTime 0.006 (0.006)\tLoss 0.9583 (0.8939)\tPrec@1 67.188 (68.281)\n",
      "Epoch: [74][78/195]\tTime 0.011 (0.006)\tLoss 0.9683 (0.9100)\tPrec@1 66.406 (67.850)\n",
      "Epoch: [74][117/195]\tTime 0.004 (0.006)\tLoss 1.0329 (0.9243)\tPrec@1 62.891 (67.489)\n",
      "Epoch: [74][156/195]\tTime 0.004 (0.006)\tLoss 0.9471 (0.9356)\tPrec@1 67.969 (66.988)\n",
      "Epoch: [74][195/195]\tTime 0.001 (0.006)\tLoss 1.0629 (0.9493)\tPrec@1 65.000 (66.432)\n",
      "EPOCH: 74 train Results: Prec@1 66.432 Loss: 0.9493\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2225 (1.2225)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.8122 (1.2444)\tPrec@1 25.000 (55.840)\n",
      "EPOCH: 74 val Results: Prec@1 55.840 Loss: 1.2444\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/195]\tTime 0.005 (0.005)\tLoss 0.8515 (0.8515)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [75][39/195]\tTime 0.004 (0.006)\tLoss 1.0438 (0.8713)\tPrec@1 65.625 (69.531)\n",
      "Epoch: [75][78/195]\tTime 0.004 (0.006)\tLoss 0.8089 (0.9024)\tPrec@1 71.875 (68.132)\n",
      "Epoch: [75][117/195]\tTime 0.004 (0.006)\tLoss 0.9715 (0.9179)\tPrec@1 65.625 (67.588)\n",
      "Epoch: [75][156/195]\tTime 0.005 (0.006)\tLoss 1.0276 (0.9334)\tPrec@1 67.578 (66.998)\n",
      "Epoch: [75][195/195]\tTime 0.002 (0.006)\tLoss 0.8811 (0.9471)\tPrec@1 68.750 (66.506)\n",
      "EPOCH: 75 train Results: Prec@1 66.506 Loss: 0.9471\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2066 (1.2066)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 2.0482 (1.2499)\tPrec@1 31.250 (55.810)\n",
      "EPOCH: 75 val Results: Prec@1 55.810 Loss: 1.2499\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/195]\tTime 0.008 (0.008)\tLoss 0.9027 (0.9027)\tPrec@1 69.922 (69.922)\n",
      "Epoch: [76][39/195]\tTime 0.010 (0.007)\tLoss 0.9698 (0.8940)\tPrec@1 62.109 (68.633)\n",
      "Epoch: [76][78/195]\tTime 0.004 (0.008)\tLoss 0.9061 (0.9043)\tPrec@1 67.969 (68.127)\n",
      "Epoch: [76][117/195]\tTime 0.008 (0.007)\tLoss 0.9517 (0.9190)\tPrec@1 67.578 (67.469)\n",
      "Epoch: [76][156/195]\tTime 0.012 (0.007)\tLoss 0.9015 (0.9319)\tPrec@1 66.406 (66.909)\n",
      "Epoch: [76][195/195]\tTime 0.007 (0.007)\tLoss 0.8265 (0.9437)\tPrec@1 76.250 (66.580)\n",
      "EPOCH: 76 train Results: Prec@1 66.580 Loss: 0.9437\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2372 (1.2372)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4771 (1.2308)\tPrec@1 37.500 (56.530)\n",
      "EPOCH: 76 val Results: Prec@1 56.530 Loss: 1.2308\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/195]\tTime 0.006 (0.006)\tLoss 0.9017 (0.9017)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [77][39/195]\tTime 0.005 (0.006)\tLoss 0.8960 (0.8746)\tPrec@1 69.531 (68.955)\n",
      "Epoch: [77][78/195]\tTime 0.008 (0.006)\tLoss 0.8978 (0.8953)\tPrec@1 67.969 (68.008)\n",
      "Epoch: [77][117/195]\tTime 0.004 (0.006)\tLoss 0.8820 (0.9089)\tPrec@1 68.750 (67.684)\n",
      "Epoch: [77][156/195]\tTime 0.010 (0.006)\tLoss 0.9992 (0.9240)\tPrec@1 66.797 (67.163)\n",
      "Epoch: [77][195/195]\tTime 0.002 (0.006)\tLoss 1.1792 (0.9376)\tPrec@1 61.250 (66.704)\n",
      "EPOCH: 77 train Results: Prec@1 66.704 Loss: 0.9376\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2449 (1.2449)\tPrec@1 57.812 (57.812)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6137 (1.2459)\tPrec@1 31.250 (56.450)\n",
      "EPOCH: 77 val Results: Prec@1 56.450 Loss: 1.2459\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/195]\tTime 0.005 (0.005)\tLoss 0.8952 (0.8952)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [78][39/195]\tTime 0.006 (0.006)\tLoss 0.9232 (0.8785)\tPrec@1 66.016 (69.443)\n",
      "Epoch: [78][78/195]\tTime 0.008 (0.006)\tLoss 0.9709 (0.8991)\tPrec@1 66.797 (68.518)\n",
      "Epoch: [78][117/195]\tTime 0.004 (0.005)\tLoss 0.9141 (0.9197)\tPrec@1 70.312 (67.591)\n",
      "Epoch: [78][156/195]\tTime 0.005 (0.006)\tLoss 0.9656 (0.9340)\tPrec@1 64.453 (66.991)\n",
      "Epoch: [78][195/195]\tTime 0.003 (0.006)\tLoss 1.0078 (0.9440)\tPrec@1 60.000 (66.552)\n",
      "EPOCH: 78 train Results: Prec@1 66.552 Loss: 0.9440\n",
      "Test: [0/39]\tTime 0.005 (0.005)\tLoss 1.2064 (1.2064)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.1966 (1.2300)\tPrec@1 43.750 (56.690)\n",
      "EPOCH: 78 val Results: Prec@1 56.690 Loss: 1.2300\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/195]\tTime 0.004 (0.004)\tLoss 0.6869 (0.6869)\tPrec@1 76.953 (76.953)\n",
      "Epoch: [79][39/195]\tTime 0.005 (0.006)\tLoss 0.9349 (0.8635)\tPrec@1 65.234 (70.010)\n",
      "Epoch: [79][78/195]\tTime 0.018 (0.006)\tLoss 0.8920 (0.8874)\tPrec@1 67.578 (68.681)\n",
      "Epoch: [79][117/195]\tTime 0.006 (0.007)\tLoss 1.0393 (0.9108)\tPrec@1 64.844 (67.671)\n",
      "Epoch: [79][156/195]\tTime 0.006 (0.007)\tLoss 1.0185 (0.9278)\tPrec@1 63.281 (67.150)\n",
      "Epoch: [79][195/195]\tTime 0.002 (0.007)\tLoss 1.0028 (0.9438)\tPrec@1 66.250 (66.472)\n",
      "EPOCH: 79 train Results: Prec@1 66.472 Loss: 0.9438\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2028 (1.2028)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6607 (1.2239)\tPrec@1 18.750 (56.380)\n",
      "EPOCH: 79 val Results: Prec@1 56.380 Loss: 1.2239\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/195]\tTime 0.006 (0.006)\tLoss 0.9055 (0.9055)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [80][39/195]\tTime 0.007 (0.007)\tLoss 0.9639 (0.8791)\tPrec@1 67.188 (69.209)\n",
      "Epoch: [80][78/195]\tTime 0.004 (0.007)\tLoss 0.9390 (0.8990)\tPrec@1 67.578 (68.384)\n",
      "Epoch: [80][117/195]\tTime 0.005 (0.007)\tLoss 0.8674 (0.9146)\tPrec@1 70.703 (67.720)\n",
      "Epoch: [80][156/195]\tTime 0.004 (0.006)\tLoss 1.0509 (0.9303)\tPrec@1 63.672 (67.071)\n",
      "Epoch: [80][195/195]\tTime 0.002 (0.007)\tLoss 0.9663 (0.9381)\tPrec@1 60.000 (66.834)\n",
      "EPOCH: 80 train Results: Prec@1 66.834 Loss: 0.9381\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2221 (1.2221)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6002 (1.2481)\tPrec@1 31.250 (55.950)\n",
      "EPOCH: 80 val Results: Prec@1 55.950 Loss: 1.2481\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/195]\tTime 0.010 (0.010)\tLoss 0.8937 (0.8937)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [81][39/195]\tTime 0.011 (0.007)\tLoss 0.9382 (0.8723)\tPrec@1 66.016 (68.916)\n",
      "Epoch: [81][78/195]\tTime 0.013 (0.007)\tLoss 0.8455 (0.8921)\tPrec@1 72.266 (68.265)\n",
      "Epoch: [81][117/195]\tTime 0.009 (0.007)\tLoss 0.9211 (0.9047)\tPrec@1 67.188 (67.730)\n",
      "Epoch: [81][156/195]\tTime 0.005 (0.007)\tLoss 1.0954 (0.9230)\tPrec@1 63.672 (67.202)\n",
      "Epoch: [81][195/195]\tTime 0.012 (0.006)\tLoss 0.8273 (0.9398)\tPrec@1 66.250 (66.560)\n",
      "EPOCH: 81 train Results: Prec@1 66.560 Loss: 0.9398\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2199 (1.2199)\tPrec@1 58.984 (58.984)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5085 (1.2458)\tPrec@1 25.000 (56.160)\n",
      "EPOCH: 81 val Results: Prec@1 56.160 Loss: 1.2458\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/195]\tTime 0.006 (0.006)\tLoss 0.9440 (0.9440)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [82][39/195]\tTime 0.004 (0.005)\tLoss 0.8898 (0.8758)\tPrec@1 66.797 (68.799)\n",
      "Epoch: [82][78/195]\tTime 0.007 (0.005)\tLoss 0.8835 (0.8914)\tPrec@1 70.703 (68.315)\n",
      "Epoch: [82][117/195]\tTime 0.004 (0.005)\tLoss 0.9396 (0.9110)\tPrec@1 67.578 (67.674)\n",
      "Epoch: [82][156/195]\tTime 0.007 (0.005)\tLoss 0.9727 (0.9214)\tPrec@1 67.188 (67.304)\n",
      "Epoch: [82][195/195]\tTime 0.002 (0.005)\tLoss 0.8474 (0.9389)\tPrec@1 70.000 (66.616)\n",
      "EPOCH: 82 train Results: Prec@1 66.616 Loss: 0.9389\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2203 (1.2203)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2932 (1.2363)\tPrec@1 37.500 (56.380)\n",
      "EPOCH: 82 val Results: Prec@1 56.380 Loss: 1.2363\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/195]\tTime 0.009 (0.009)\tLoss 0.8548 (0.8548)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [83][39/195]\tTime 0.009 (0.006)\tLoss 0.9038 (0.8699)\tPrec@1 69.141 (69.482)\n",
      "Epoch: [83][78/195]\tTime 0.005 (0.006)\tLoss 1.0220 (0.8946)\tPrec@1 63.281 (68.537)\n",
      "Epoch: [83][117/195]\tTime 0.004 (0.006)\tLoss 0.9797 (0.9105)\tPrec@1 64.062 (67.757)\n",
      "Epoch: [83][156/195]\tTime 0.017 (0.006)\tLoss 0.9496 (0.9268)\tPrec@1 65.234 (67.267)\n",
      "Epoch: [83][195/195]\tTime 0.002 (0.006)\tLoss 1.0099 (0.9338)\tPrec@1 66.250 (66.936)\n",
      "EPOCH: 83 train Results: Prec@1 66.936 Loss: 0.9338\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2181 (1.2181)\tPrec@1 58.984 (58.984)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3016 (1.2392)\tPrec@1 37.500 (56.640)\n",
      "EPOCH: 83 val Results: Prec@1 56.640 Loss: 1.2392\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/195]\tTime 0.012 (0.012)\tLoss 0.7770 (0.7770)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [84][39/195]\tTime 0.005 (0.005)\tLoss 0.7767 (0.8733)\tPrec@1 74.219 (69.219)\n",
      "Epoch: [84][78/195]\tTime 0.011 (0.006)\tLoss 1.0255 (0.8857)\tPrec@1 65.625 (68.612)\n",
      "Epoch: [84][117/195]\tTime 0.008 (0.006)\tLoss 0.8380 (0.9073)\tPrec@1 67.188 (67.658)\n",
      "Epoch: [84][156/195]\tTime 0.003 (0.006)\tLoss 0.9348 (0.9203)\tPrec@1 66.406 (67.252)\n",
      "Epoch: [84][195/195]\tTime 0.002 (0.007)\tLoss 1.1994 (0.9263)\tPrec@1 51.250 (67.022)\n",
      "EPOCH: 84 train Results: Prec@1 67.022 Loss: 0.9263\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2669 (1.2669)\tPrec@1 52.344 (52.344)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4688 (1.2571)\tPrec@1 43.750 (55.950)\n",
      "EPOCH: 84 val Results: Prec@1 55.950 Loss: 1.2571\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/195]\tTime 0.005 (0.005)\tLoss 0.8178 (0.8178)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [85][39/195]\tTime 0.005 (0.006)\tLoss 0.8076 (0.8642)\tPrec@1 71.875 (69.580)\n",
      "Epoch: [85][78/195]\tTime 0.004 (0.006)\tLoss 0.8769 (0.8847)\tPrec@1 69.531 (68.696)\n",
      "Epoch: [85][117/195]\tTime 0.006 (0.006)\tLoss 0.9847 (0.9024)\tPrec@1 60.547 (67.830)\n",
      "Epoch: [85][156/195]\tTime 0.008 (0.006)\tLoss 0.9402 (0.9177)\tPrec@1 67.188 (67.292)\n",
      "Epoch: [85][195/195]\tTime 0.003 (0.006)\tLoss 0.9681 (0.9316)\tPrec@1 62.500 (66.780)\n",
      "EPOCH: 85 train Results: Prec@1 66.780 Loss: 0.9316\n",
      "Test: [0/39]\tTime 0.009 (0.009)\tLoss 1.1977 (1.1977)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4942 (1.2437)\tPrec@1 31.250 (56.330)\n",
      "EPOCH: 85 val Results: Prec@1 56.330 Loss: 1.2437\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/195]\tTime 0.005 (0.005)\tLoss 0.8770 (0.8770)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [86][39/195]\tTime 0.007 (0.006)\tLoss 0.8634 (0.8798)\tPrec@1 68.750 (68.926)\n",
      "Epoch: [86][78/195]\tTime 0.004 (0.006)\tLoss 0.8527 (0.8904)\tPrec@1 69.141 (68.097)\n",
      "Epoch: [86][117/195]\tTime 0.005 (0.005)\tLoss 0.9797 (0.9054)\tPrec@1 64.844 (67.505)\n",
      "Epoch: [86][156/195]\tTime 0.006 (0.005)\tLoss 1.0660 (0.9174)\tPrec@1 62.500 (67.227)\n",
      "Epoch: [86][195/195]\tTime 0.002 (0.005)\tLoss 1.0756 (0.9337)\tPrec@1 58.750 (66.586)\n",
      "EPOCH: 86 train Results: Prec@1 66.586 Loss: 0.9337\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2344 (1.2344)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3021 (1.2274)\tPrec@1 25.000 (56.680)\n",
      "EPOCH: 86 val Results: Prec@1 56.680 Loss: 1.2274\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/195]\tTime 0.008 (0.008)\tLoss 0.8771 (0.8771)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [87][39/195]\tTime 0.008 (0.005)\tLoss 0.8140 (0.8645)\tPrec@1 70.703 (69.541)\n",
      "Epoch: [87][78/195]\tTime 0.007 (0.005)\tLoss 0.9432 (0.8941)\tPrec@1 66.406 (68.518)\n",
      "Epoch: [87][117/195]\tTime 0.006 (0.006)\tLoss 1.0056 (0.9065)\tPrec@1 63.672 (67.856)\n",
      "Epoch: [87][156/195]\tTime 0.013 (0.006)\tLoss 0.9502 (0.9201)\tPrec@1 69.141 (67.357)\n",
      "Epoch: [87][195/195]\tTime 0.003 (0.006)\tLoss 1.0086 (0.9308)\tPrec@1 70.000 (66.918)\n",
      "EPOCH: 87 train Results: Prec@1 66.918 Loss: 0.9308\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2162 (1.2162)\tPrec@1 58.984 (58.984)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2504 (1.2472)\tPrec@1 37.500 (55.830)\n",
      "EPOCH: 87 val Results: Prec@1 55.830 Loss: 1.2472\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/195]\tTime 0.014 (0.014)\tLoss 0.7816 (0.7816)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [88][39/195]\tTime 0.004 (0.008)\tLoss 0.8086 (0.8508)\tPrec@1 73.047 (70.312)\n",
      "Epoch: [88][78/195]\tTime 0.004 (0.008)\tLoss 0.9822 (0.8830)\tPrec@1 61.328 (68.740)\n",
      "Epoch: [88][117/195]\tTime 0.005 (0.007)\tLoss 1.0178 (0.9023)\tPrec@1 63.281 (67.866)\n",
      "Epoch: [88][156/195]\tTime 0.013 (0.007)\tLoss 1.0107 (0.9215)\tPrec@1 64.844 (67.118)\n",
      "Epoch: [88][195/195]\tTime 0.002 (0.007)\tLoss 1.3322 (0.9317)\tPrec@1 57.500 (66.764)\n",
      "EPOCH: 88 train Results: Prec@1 66.764 Loss: 0.9317\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1699 (1.1699)\tPrec@1 60.938 (60.938)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2209 (1.2463)\tPrec@1 50.000 (56.310)\n",
      "EPOCH: 88 val Results: Prec@1 56.310 Loss: 1.2463\n",
      "Best Prec@1: 56.690\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/195]\tTime 0.005 (0.005)\tLoss 0.8576 (0.8576)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [89][39/195]\tTime 0.004 (0.006)\tLoss 0.8496 (0.8767)\tPrec@1 70.703 (69.053)\n",
      "Epoch: [89][78/195]\tTime 0.008 (0.006)\tLoss 0.9371 (0.8867)\tPrec@1 65.234 (68.552)\n",
      "Epoch: [89][117/195]\tTime 0.005 (0.006)\tLoss 0.9884 (0.9056)\tPrec@1 65.625 (68.032)\n",
      "Epoch: [89][156/195]\tTime 0.099 (0.006)\tLoss 0.9677 (0.9213)\tPrec@1 67.578 (67.459)\n",
      "Epoch: [89][195/195]\tTime 0.002 (0.007)\tLoss 1.1108 (0.9335)\tPrec@1 63.750 (66.930)\n",
      "EPOCH: 89 train Results: Prec@1 66.930 Loss: 0.9335\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2218 (1.2218)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.003)\tLoss 1.5808 (1.2463)\tPrec@1 50.000 (56.740)\n",
      "EPOCH: 89 val Results: Prec@1 56.740 Loss: 1.2463\n",
      "Best Prec@1: 56.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/195]\tTime 0.015 (0.015)\tLoss 0.9640 (0.9640)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [90][39/195]\tTime 0.004 (0.011)\tLoss 0.9220 (0.8651)\tPrec@1 67.578 (69.404)\n",
      "Epoch: [90][78/195]\tTime 0.009 (0.014)\tLoss 0.8830 (0.8850)\tPrec@1 69.531 (68.710)\n",
      "Epoch: [90][117/195]\tTime 0.008 (0.013)\tLoss 0.9200 (0.9014)\tPrec@1 66.406 (68.128)\n",
      "Epoch: [90][156/195]\tTime 0.010 (0.012)\tLoss 1.0301 (0.9105)\tPrec@1 62.891 (67.810)\n",
      "Epoch: [90][195/195]\tTime 0.017 (0.012)\tLoss 0.9021 (0.9261)\tPrec@1 65.000 (67.202)\n",
      "EPOCH: 90 train Results: Prec@1 67.202 Loss: 0.9261\n",
      "Test: [0/39]\tTime 0.007 (0.007)\tLoss 1.2090 (1.2090)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.004)\tLoss 1.8996 (1.2526)\tPrec@1 31.250 (56.130)\n",
      "EPOCH: 90 val Results: Prec@1 56.130 Loss: 1.2526\n",
      "Best Prec@1: 56.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/195]\tTime 0.006 (0.006)\tLoss 0.8757 (0.8757)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [91][39/195]\tTime 0.010 (0.012)\tLoss 0.9164 (0.8694)\tPrec@1 66.016 (69.121)\n",
      "Epoch: [91][78/195]\tTime 0.004 (0.011)\tLoss 1.0052 (0.8827)\tPrec@1 64.453 (68.819)\n",
      "Epoch: [91][117/195]\tTime 0.008 (0.010)\tLoss 1.0415 (0.9000)\tPrec@1 63.672 (68.230)\n",
      "Epoch: [91][156/195]\tTime 0.004 (0.010)\tLoss 0.9930 (0.9177)\tPrec@1 66.406 (67.561)\n",
      "Epoch: [91][195/195]\tTime 0.002 (0.010)\tLoss 0.8869 (0.9296)\tPrec@1 68.750 (67.012)\n",
      "EPOCH: 91 train Results: Prec@1 67.012 Loss: 0.9296\n",
      "Test: [0/39]\tTime 0.006 (0.006)\tLoss 1.1885 (1.1885)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2886 (1.2471)\tPrec@1 43.750 (56.530)\n",
      "EPOCH: 91 val Results: Prec@1 56.530 Loss: 1.2471\n",
      "Best Prec@1: 56.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/195]\tTime 0.007 (0.007)\tLoss 0.8857 (0.8857)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [92][39/195]\tTime 0.006 (0.012)\tLoss 0.9806 (0.8738)\tPrec@1 67.969 (69.541)\n",
      "Epoch: [92][78/195]\tTime 0.006 (0.013)\tLoss 0.9685 (0.8906)\tPrec@1 65.234 (68.893)\n",
      "Epoch: [92][117/195]\tTime 0.011 (0.014)\tLoss 0.8779 (0.8997)\tPrec@1 67.578 (68.323)\n",
      "Epoch: [92][156/195]\tTime 0.008 (0.013)\tLoss 0.9487 (0.9137)\tPrec@1 62.891 (67.787)\n",
      "Epoch: [92][195/195]\tTime 0.003 (0.013)\tLoss 0.9104 (0.9265)\tPrec@1 63.750 (67.224)\n",
      "EPOCH: 92 train Results: Prec@1 67.224 Loss: 0.9265\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2465 (1.2465)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3945 (1.2572)\tPrec@1 37.500 (56.310)\n",
      "EPOCH: 92 val Results: Prec@1 56.310 Loss: 1.2572\n",
      "Best Prec@1: 56.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/195]\tTime 0.010 (0.010)\tLoss 0.8313 (0.8313)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [93][39/195]\tTime 0.007 (0.013)\tLoss 0.8875 (0.8496)\tPrec@1 66.016 (70.020)\n",
      "Epoch: [93][78/195]\tTime 0.007 (0.012)\tLoss 0.9208 (0.8754)\tPrec@1 64.844 (69.047)\n",
      "Epoch: [93][117/195]\tTime 0.010 (0.013)\tLoss 0.8630 (0.8918)\tPrec@1 67.969 (68.422)\n",
      "Epoch: [93][156/195]\tTime 0.004 (0.012)\tLoss 1.0527 (0.9103)\tPrec@1 61.719 (67.630)\n",
      "Epoch: [93][195/195]\tTime 0.013 (0.012)\tLoss 0.9658 (0.9250)\tPrec@1 61.250 (67.090)\n",
      "EPOCH: 93 train Results: Prec@1 67.090 Loss: 0.9250\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2283 (1.2283)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.007)\tLoss 1.2175 (1.2512)\tPrec@1 37.500 (56.350)\n",
      "EPOCH: 93 val Results: Prec@1 56.350 Loss: 1.2512\n",
      "Best Prec@1: 56.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/195]\tTime 0.008 (0.008)\tLoss 0.8343 (0.8343)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [94][39/195]\tTime 0.014 (0.007)\tLoss 0.9173 (0.8524)\tPrec@1 64.844 (69.707)\n",
      "Epoch: [94][78/195]\tTime 0.005 (0.009)\tLoss 0.9549 (0.8682)\tPrec@1 65.234 (69.235)\n",
      "Epoch: [94][117/195]\tTime 0.013 (0.009)\tLoss 0.8534 (0.8931)\tPrec@1 68.750 (68.300)\n",
      "Epoch: [94][156/195]\tTime 0.010 (0.009)\tLoss 0.9852 (0.9068)\tPrec@1 66.406 (67.976)\n",
      "Epoch: [94][195/195]\tTime 0.002 (0.009)\tLoss 1.1776 (0.9208)\tPrec@1 63.750 (67.508)\n",
      "EPOCH: 94 train Results: Prec@1 67.508 Loss: 0.9208\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2956 (1.2956)\tPrec@1 53.516 (53.516)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4126 (1.2596)\tPrec@1 37.500 (55.770)\n",
      "EPOCH: 94 val Results: Prec@1 55.770 Loss: 1.2596\n",
      "Best Prec@1: 56.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/195]\tTime 0.005 (0.005)\tLoss 0.8642 (0.8642)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [95][39/195]\tTime 0.013 (0.008)\tLoss 0.9323 (0.8621)\tPrec@1 62.891 (69.346)\n",
      "Epoch: [95][78/195]\tTime 0.007 (0.009)\tLoss 0.9745 (0.8812)\tPrec@1 68.750 (68.987)\n",
      "Epoch: [95][117/195]\tTime 0.004 (0.008)\tLoss 0.9642 (0.8997)\tPrec@1 64.453 (68.402)\n",
      "Epoch: [95][156/195]\tTime 0.004 (0.009)\tLoss 0.9643 (0.9167)\tPrec@1 66.797 (67.961)\n",
      "Epoch: [95][195/195]\tTime 0.003 (0.009)\tLoss 0.9879 (0.9285)\tPrec@1 70.000 (67.490)\n",
      "EPOCH: 95 train Results: Prec@1 67.490 Loss: 0.9285\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1957 (1.1957)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5938 (1.2534)\tPrec@1 37.500 (55.910)\n",
      "EPOCH: 95 val Results: Prec@1 55.910 Loss: 1.2534\n",
      "Best Prec@1: 56.740\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/195]\tTime 0.007 (0.007)\tLoss 0.9854 (0.9854)\tPrec@1 64.062 (64.062)\n",
      "Epoch: [96][39/195]\tTime 0.004 (0.006)\tLoss 0.9671 (0.8526)\tPrec@1 64.062 (70.176)\n",
      "Epoch: [96][78/195]\tTime 0.006 (0.006)\tLoss 0.9182 (0.8761)\tPrec@1 69.141 (69.210)\n",
      "Epoch: [96][117/195]\tTime 0.007 (0.006)\tLoss 0.9370 (0.8941)\tPrec@1 66.797 (68.535)\n",
      "Epoch: [96][156/195]\tTime 0.009 (0.006)\tLoss 0.9284 (0.9040)\tPrec@1 64.062 (68.053)\n",
      "Epoch: [96][195/195]\tTime 0.002 (0.006)\tLoss 0.9959 (0.9196)\tPrec@1 65.000 (67.416)\n",
      "EPOCH: 96 train Results: Prec@1 67.416 Loss: 0.9196\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2625 (1.2625)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3701 (1.2423)\tPrec@1 31.250 (56.810)\n",
      "EPOCH: 96 val Results: Prec@1 56.810 Loss: 1.2423\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/195]\tTime 0.004 (0.004)\tLoss 0.9668 (0.9668)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [97][39/195]\tTime 0.004 (0.006)\tLoss 0.8629 (0.8598)\tPrec@1 69.141 (69.336)\n",
      "Epoch: [97][78/195]\tTime 0.011 (0.006)\tLoss 0.9569 (0.8804)\tPrec@1 67.578 (68.785)\n",
      "Epoch: [97][117/195]\tTime 0.008 (0.007)\tLoss 1.0578 (0.8928)\tPrec@1 62.500 (68.336)\n",
      "Epoch: [97][156/195]\tTime 0.005 (0.006)\tLoss 0.9580 (0.9093)\tPrec@1 66.016 (67.725)\n",
      "Epoch: [97][195/195]\tTime 0.002 (0.006)\tLoss 0.7754 (0.9225)\tPrec@1 72.500 (67.210)\n",
      "EPOCH: 97 train Results: Prec@1 67.210 Loss: 0.9225\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2888 (1.2888)\tPrec@1 54.297 (54.297)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4817 (1.2523)\tPrec@1 37.500 (56.280)\n",
      "EPOCH: 97 val Results: Prec@1 56.280 Loss: 1.2523\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/195]\tTime 0.012 (0.012)\tLoss 0.9082 (0.9082)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [98][39/195]\tTime 0.007 (0.005)\tLoss 0.8416 (0.8563)\tPrec@1 72.656 (70.117)\n",
      "Epoch: [98][78/195]\tTime 0.004 (0.007)\tLoss 0.8779 (0.8733)\tPrec@1 69.141 (69.482)\n",
      "Epoch: [98][117/195]\tTime 0.007 (0.007)\tLoss 0.9314 (0.8914)\tPrec@1 66.797 (68.776)\n",
      "Epoch: [98][156/195]\tTime 0.012 (0.007)\tLoss 0.9973 (0.9101)\tPrec@1 64.062 (68.019)\n",
      "Epoch: [98][195/195]\tTime 0.002 (0.006)\tLoss 0.9196 (0.9217)\tPrec@1 65.000 (67.484)\n",
      "EPOCH: 98 train Results: Prec@1 67.484 Loss: 0.9217\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2857 (1.2857)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3515 (1.2579)\tPrec@1 37.500 (56.370)\n",
      "EPOCH: 98 val Results: Prec@1 56.370 Loss: 1.2579\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/195]\tTime 0.006 (0.006)\tLoss 0.9147 (0.9147)\tPrec@1 66.016 (66.016)\n",
      "Epoch: [99][39/195]\tTime 0.003 (0.005)\tLoss 0.8898 (0.8516)\tPrec@1 70.703 (69.932)\n",
      "Epoch: [99][78/195]\tTime 0.005 (0.006)\tLoss 0.8971 (0.8735)\tPrec@1 67.578 (68.973)\n",
      "Epoch: [99][117/195]\tTime 0.004 (0.006)\tLoss 0.9991 (0.8908)\tPrec@1 62.500 (68.174)\n",
      "Epoch: [99][156/195]\tTime 0.005 (0.006)\tLoss 1.0107 (0.9077)\tPrec@1 63.281 (67.705)\n",
      "Epoch: [99][195/195]\tTime 0.002 (0.006)\tLoss 0.8769 (0.9200)\tPrec@1 67.500 (67.162)\n",
      "EPOCH: 99 train Results: Prec@1 67.162 Loss: 0.9200\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2563 (1.2563)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5423 (1.2544)\tPrec@1 31.250 (55.980)\n",
      "EPOCH: 99 val Results: Prec@1 55.980 Loss: 1.2544\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/195]\tTime 0.023 (0.023)\tLoss 0.8200 (0.8200)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [100][39/195]\tTime 0.009 (0.022)\tLoss 0.9100 (0.8549)\tPrec@1 66.406 (69.980)\n",
      "Epoch: [100][78/195]\tTime 0.007 (0.014)\tLoss 0.8486 (0.8706)\tPrec@1 69.531 (69.190)\n",
      "Epoch: [100][117/195]\tTime 0.007 (0.012)\tLoss 1.0382 (0.8933)\tPrec@1 66.406 (68.326)\n",
      "Epoch: [100][156/195]\tTime 0.004 (0.011)\tLoss 0.9717 (0.9113)\tPrec@1 63.672 (67.735)\n",
      "Epoch: [100][195/195]\tTime 0.008 (0.010)\tLoss 1.0179 (0.9197)\tPrec@1 67.500 (67.382)\n",
      "EPOCH: 100 train Results: Prec@1 67.382 Loss: 0.9197\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2605 (1.2605)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4146 (1.2460)\tPrec@1 25.000 (56.270)\n",
      "EPOCH: 100 val Results: Prec@1 56.270 Loss: 1.2460\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/195]\tTime 0.005 (0.005)\tLoss 0.8205 (0.8205)\tPrec@1 69.922 (69.922)\n",
      "Epoch: [101][39/195]\tTime 0.013 (0.010)\tLoss 0.9020 (0.8619)\tPrec@1 68.359 (69.746)\n",
      "Epoch: [101][78/195]\tTime 0.005 (0.009)\tLoss 0.9078 (0.8701)\tPrec@1 67.578 (69.195)\n",
      "Epoch: [101][117/195]\tTime 0.020 (0.009)\tLoss 0.9557 (0.8905)\tPrec@1 67.969 (68.436)\n",
      "Epoch: [101][156/195]\tTime 0.004 (0.012)\tLoss 0.9884 (0.9024)\tPrec@1 65.234 (67.857)\n",
      "Epoch: [101][195/195]\tTime 0.002 (0.011)\tLoss 1.0739 (0.9168)\tPrec@1 58.750 (67.248)\n",
      "EPOCH: 101 train Results: Prec@1 67.248 Loss: 0.9168\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2440 (1.2440)\tPrec@1 55.859 (55.859)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2688 (1.2566)\tPrec@1 43.750 (56.280)\n",
      "EPOCH: 101 val Results: Prec@1 56.280 Loss: 1.2566\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/195]\tTime 0.005 (0.005)\tLoss 0.7878 (0.7878)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [102][39/195]\tTime 0.019 (0.021)\tLoss 0.9831 (0.8444)\tPrec@1 66.016 (70.654)\n",
      "Epoch: [102][78/195]\tTime 0.005 (0.016)\tLoss 0.9039 (0.8697)\tPrec@1 65.625 (69.531)\n",
      "Epoch: [102][117/195]\tTime 0.005 (0.014)\tLoss 0.9940 (0.8868)\tPrec@1 65.234 (68.657)\n",
      "Epoch: [102][156/195]\tTime 0.009 (0.013)\tLoss 0.9790 (0.9042)\tPrec@1 64.844 (67.869)\n",
      "Epoch: [102][195/195]\tTime 0.002 (0.012)\tLoss 0.9910 (0.9155)\tPrec@1 66.250 (67.516)\n",
      "EPOCH: 102 train Results: Prec@1 67.516 Loss: 0.9155\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2923 (1.2923)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.002 (0.002)\tLoss 1.2954 (1.2687)\tPrec@1 56.250 (55.580)\n",
      "EPOCH: 102 val Results: Prec@1 55.580 Loss: 1.2687\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/195]\tTime 0.006 (0.006)\tLoss 0.7900 (0.7900)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [103][39/195]\tTime 0.009 (0.010)\tLoss 0.8206 (0.8594)\tPrec@1 73.438 (69.502)\n",
      "Epoch: [103][78/195]\tTime 0.007 (0.008)\tLoss 0.9297 (0.8722)\tPrec@1 67.188 (69.052)\n",
      "Epoch: [103][117/195]\tTime 0.004 (0.008)\tLoss 1.0404 (0.8850)\tPrec@1 64.453 (68.604)\n",
      "Epoch: [103][156/195]\tTime 0.009 (0.008)\tLoss 1.0844 (0.9004)\tPrec@1 57.422 (67.994)\n",
      "Epoch: [103][195/195]\tTime 0.003 (0.007)\tLoss 1.0054 (0.9126)\tPrec@1 71.250 (67.514)\n",
      "EPOCH: 103 train Results: Prec@1 67.514 Loss: 0.9126\n",
      "Test: [0/39]\tTime 0.006 (0.006)\tLoss 1.2767 (1.2767)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4488 (1.2630)\tPrec@1 37.500 (56.390)\n",
      "EPOCH: 103 val Results: Prec@1 56.390 Loss: 1.2630\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/195]\tTime 0.006 (0.006)\tLoss 0.9050 (0.9050)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [104][39/195]\tTime 0.011 (0.007)\tLoss 0.8545 (0.8506)\tPrec@1 68.750 (70.029)\n",
      "Epoch: [104][78/195]\tTime 0.008 (0.014)\tLoss 0.9155 (0.8755)\tPrec@1 67.188 (69.062)\n",
      "Epoch: [104][117/195]\tTime 0.004 (0.012)\tLoss 0.8892 (0.8893)\tPrec@1 70.312 (68.601)\n",
      "Epoch: [104][156/195]\tTime 0.004 (0.012)\tLoss 0.9001 (0.9083)\tPrec@1 65.625 (67.894)\n",
      "Epoch: [104][195/195]\tTime 0.007 (0.011)\tLoss 0.8571 (0.9188)\tPrec@1 66.250 (67.534)\n",
      "EPOCH: 104 train Results: Prec@1 67.534 Loss: 0.9188\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2437 (1.2437)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2533 (1.2511)\tPrec@1 43.750 (56.420)\n",
      "EPOCH: 104 val Results: Prec@1 56.420 Loss: 1.2511\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/195]\tTime 0.006 (0.006)\tLoss 0.8598 (0.8598)\tPrec@1 69.922 (69.922)\n",
      "Epoch: [105][39/195]\tTime 0.011 (0.010)\tLoss 0.9076 (0.8569)\tPrec@1 67.969 (69.883)\n",
      "Epoch: [105][78/195]\tTime 0.106 (0.016)\tLoss 0.9801 (0.8727)\tPrec@1 66.016 (69.081)\n",
      "Epoch: [105][117/195]\tTime 0.004 (0.013)\tLoss 1.0744 (0.8886)\tPrec@1 63.672 (68.459)\n",
      "Epoch: [105][156/195]\tTime 0.007 (0.011)\tLoss 0.9985 (0.9002)\tPrec@1 65.234 (68.011)\n",
      "Epoch: [105][195/195]\tTime 0.012 (0.010)\tLoss 0.9487 (0.9156)\tPrec@1 62.500 (67.480)\n",
      "EPOCH: 105 train Results: Prec@1 67.480 Loss: 0.9156\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2888 (1.2888)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.1781 (1.2625)\tPrec@1 50.000 (55.640)\n",
      "EPOCH: 105 val Results: Prec@1 55.640 Loss: 1.2625\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/195]\tTime 0.008 (0.008)\tLoss 0.8589 (0.8589)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [106][39/195]\tTime 0.006 (0.019)\tLoss 0.8414 (0.8439)\tPrec@1 67.578 (70.420)\n",
      "Epoch: [106][78/195]\tTime 0.004 (0.014)\tLoss 0.8928 (0.8647)\tPrec@1 65.625 (69.497)\n",
      "Epoch: [106][117/195]\tTime 0.025 (0.013)\tLoss 0.9356 (0.8877)\tPrec@1 66.797 (68.730)\n",
      "Epoch: [106][156/195]\tTime 0.004 (0.013)\tLoss 0.9318 (0.9068)\tPrec@1 64.453 (67.956)\n",
      "Epoch: [106][195/195]\tTime 0.003 (0.013)\tLoss 0.9863 (0.9193)\tPrec@1 67.500 (67.428)\n",
      "EPOCH: 106 train Results: Prec@1 67.428 Loss: 0.9193\n",
      "Test: [0/39]\tTime 0.011 (0.011)\tLoss 1.2600 (1.2600)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.010 (0.003)\tLoss 1.2745 (1.2536)\tPrec@1 31.250 (56.280)\n",
      "EPOCH: 106 val Results: Prec@1 56.280 Loss: 1.2536\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/195]\tTime 0.020 (0.020)\tLoss 0.7062 (0.7062)\tPrec@1 78.516 (78.516)\n",
      "Epoch: [107][39/195]\tTime 0.020 (0.011)\tLoss 0.8906 (0.8481)\tPrec@1 67.969 (69.951)\n",
      "Epoch: [107][78/195]\tTime 0.010 (0.010)\tLoss 0.9459 (0.8676)\tPrec@1 67.188 (69.299)\n",
      "Epoch: [107][117/195]\tTime 0.012 (0.011)\tLoss 0.9387 (0.8793)\tPrec@1 64.453 (68.813)\n",
      "Epoch: [107][156/195]\tTime 0.035 (0.013)\tLoss 1.0068 (0.9004)\tPrec@1 63.281 (67.986)\n",
      "Epoch: [107][195/195]\tTime 0.002 (0.013)\tLoss 0.9529 (0.9140)\tPrec@1 68.750 (67.552)\n",
      "EPOCH: 107 train Results: Prec@1 67.552 Loss: 0.9140\n",
      "Test: [0/39]\tTime 0.011 (0.011)\tLoss 1.2986 (1.2986)\tPrec@1 55.859 (55.859)\n",
      "Test: [39/39]\tTime 0.007 (0.004)\tLoss 1.3055 (1.2497)\tPrec@1 50.000 (56.400)\n",
      "EPOCH: 107 val Results: Prec@1 56.400 Loss: 1.2497\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/195]\tTime 0.017 (0.017)\tLoss 0.8401 (0.8401)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [108][39/195]\tTime 0.003 (0.011)\tLoss 0.9180 (0.8369)\tPrec@1 69.922 (71.240)\n",
      "Epoch: [108][78/195]\tTime 0.014 (0.010)\tLoss 0.9177 (0.8623)\tPrec@1 69.922 (70.105)\n",
      "Epoch: [108][117/195]\tTime 0.017 (0.011)\tLoss 1.0266 (0.8808)\tPrec@1 64.062 (69.237)\n",
      "Epoch: [108][156/195]\tTime 0.016 (0.011)\tLoss 0.9066 (0.8979)\tPrec@1 69.531 (68.349)\n",
      "Epoch: [108][195/195]\tTime 0.002 (0.012)\tLoss 0.8715 (0.9144)\tPrec@1 67.500 (67.754)\n",
      "EPOCH: 108 train Results: Prec@1 67.754 Loss: 0.9144\n",
      "Test: [0/39]\tTime 0.007 (0.007)\tLoss 1.3056 (1.3056)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3782 (1.2742)\tPrec@1 50.000 (54.980)\n",
      "EPOCH: 108 val Results: Prec@1 54.980 Loss: 1.2742\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/195]\tTime 0.016 (0.016)\tLoss 0.8380 (0.8380)\tPrec@1 66.797 (66.797)\n",
      "Epoch: [109][39/195]\tTime 0.006 (0.014)\tLoss 0.9322 (0.8650)\tPrec@1 66.797 (69.502)\n",
      "Epoch: [109][78/195]\tTime 0.004 (0.012)\tLoss 0.8278 (0.8814)\tPrec@1 72.656 (68.963)\n",
      "Epoch: [109][117/195]\tTime 0.007 (0.011)\tLoss 0.8469 (0.8936)\tPrec@1 70.312 (68.419)\n",
      "Epoch: [109][156/195]\tTime 0.005 (0.010)\tLoss 0.9024 (0.9055)\tPrec@1 70.312 (67.919)\n",
      "Epoch: [109][195/195]\tTime 0.019 (0.010)\tLoss 1.0366 (0.9163)\tPrec@1 58.750 (67.416)\n",
      "EPOCH: 109 train Results: Prec@1 67.416 Loss: 0.9163\n",
      "Test: [0/39]\tTime 0.009 (0.009)\tLoss 1.3072 (1.3072)\tPrec@1 53.125 (53.125)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2034 (1.2532)\tPrec@1 56.250 (56.240)\n",
      "EPOCH: 109 val Results: Prec@1 56.240 Loss: 1.2532\n",
      "Best Prec@1: 56.810\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/195]\tTime 0.009 (0.009)\tLoss 0.8228 (0.8228)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [110][39/195]\tTime 0.013 (0.011)\tLoss 0.7863 (0.8467)\tPrec@1 71.484 (70.195)\n",
      "Epoch: [110][78/195]\tTime 0.008 (0.009)\tLoss 0.8710 (0.8713)\tPrec@1 69.531 (69.200)\n",
      "Epoch: [110][117/195]\tTime 0.013 (0.009)\tLoss 0.9703 (0.8837)\tPrec@1 63.281 (68.565)\n",
      "Epoch: [110][156/195]\tTime 0.007 (0.010)\tLoss 0.9415 (0.8993)\tPrec@1 64.453 (67.931)\n",
      "Epoch: [110][195/195]\tTime 0.001 (0.010)\tLoss 1.1823 (0.9109)\tPrec@1 60.000 (67.546)\n",
      "EPOCH: 110 train Results: Prec@1 67.546 Loss: 0.9109\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2674 (1.2674)\tPrec@1 53.516 (53.516)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.1877 (1.2613)\tPrec@1 50.000 (56.900)\n",
      "EPOCH: 110 val Results: Prec@1 56.900 Loss: 1.2613\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/195]\tTime 0.004 (0.004)\tLoss 0.9337 (0.9337)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [111][39/195]\tTime 0.004 (0.006)\tLoss 0.8974 (0.8500)\tPrec@1 65.234 (70.049)\n",
      "Epoch: [111][78/195]\tTime 0.008 (0.007)\tLoss 1.0916 (0.8731)\tPrec@1 60.547 (69.413)\n",
      "Epoch: [111][117/195]\tTime 0.003 (0.007)\tLoss 1.0078 (0.8973)\tPrec@1 65.625 (68.535)\n",
      "Epoch: [111][156/195]\tTime 0.016 (0.007)\tLoss 0.9733 (0.9112)\tPrec@1 64.453 (67.777)\n",
      "Epoch: [111][195/195]\tTime 0.002 (0.007)\tLoss 0.9064 (0.9197)\tPrec@1 65.000 (67.426)\n",
      "EPOCH: 111 train Results: Prec@1 67.426 Loss: 0.9197\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2406 (1.2406)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3240 (1.2745)\tPrec@1 50.000 (55.740)\n",
      "EPOCH: 111 val Results: Prec@1 55.740 Loss: 1.2745\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/195]\tTime 0.005 (0.005)\tLoss 0.7819 (0.7819)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [112][39/195]\tTime 0.005 (0.006)\tLoss 0.8750 (0.8449)\tPrec@1 71.484 (70.225)\n",
      "Epoch: [112][78/195]\tTime 0.004 (0.006)\tLoss 0.9306 (0.8706)\tPrec@1 67.188 (69.447)\n",
      "Epoch: [112][117/195]\tTime 0.004 (0.006)\tLoss 0.9446 (0.8842)\tPrec@1 67.578 (69.061)\n",
      "Epoch: [112][156/195]\tTime 0.003 (0.006)\tLoss 0.8268 (0.9024)\tPrec@1 71.875 (68.175)\n",
      "Epoch: [112][195/195]\tTime 0.004 (0.006)\tLoss 1.0542 (0.9118)\tPrec@1 60.000 (67.798)\n",
      "EPOCH: 112 train Results: Prec@1 67.798 Loss: 0.9118\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2515 (1.2515)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.001 (0.001)\tLoss 1.1633 (1.2539)\tPrec@1 37.500 (56.610)\n",
      "EPOCH: 112 val Results: Prec@1 56.610 Loss: 1.2539\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/195]\tTime 0.007 (0.007)\tLoss 0.8081 (0.8081)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [113][39/195]\tTime 0.005 (0.006)\tLoss 0.8701 (0.8531)\tPrec@1 66.016 (70.117)\n",
      "Epoch: [113][78/195]\tTime 0.005 (0.006)\tLoss 0.8569 (0.8683)\tPrec@1 71.094 (69.358)\n",
      "Epoch: [113][117/195]\tTime 0.004 (0.007)\tLoss 0.9026 (0.8828)\tPrec@1 69.922 (68.677)\n",
      "Epoch: [113][156/195]\tTime 0.007 (0.006)\tLoss 1.0187 (0.8970)\tPrec@1 61.328 (68.242)\n",
      "Epoch: [113][195/195]\tTime 0.002 (0.006)\tLoss 0.9537 (0.9119)\tPrec@1 65.000 (67.714)\n",
      "EPOCH: 113 train Results: Prec@1 67.714 Loss: 0.9119\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2287 (1.2287)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.2171 (1.2581)\tPrec@1 37.500 (56.190)\n",
      "EPOCH: 113 val Results: Prec@1 56.190 Loss: 1.2581\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/195]\tTime 0.005 (0.005)\tLoss 0.9180 (0.9180)\tPrec@1 69.531 (69.531)\n",
      "Epoch: [114][39/195]\tTime 0.004 (0.008)\tLoss 0.8116 (0.8454)\tPrec@1 71.875 (70.352)\n",
      "Epoch: [114][78/195]\tTime 0.007 (0.010)\tLoss 0.8625 (0.8655)\tPrec@1 69.141 (69.497)\n",
      "Epoch: [114][117/195]\tTime 0.006 (0.011)\tLoss 0.9475 (0.8867)\tPrec@1 69.141 (68.671)\n",
      "Epoch: [114][156/195]\tTime 0.016 (0.010)\tLoss 0.8816 (0.8971)\tPrec@1 68.750 (68.359)\n",
      "Epoch: [114][195/195]\tTime 0.003 (0.010)\tLoss 1.1537 (0.9129)\tPrec@1 60.000 (67.686)\n",
      "EPOCH: 114 train Results: Prec@1 67.686 Loss: 0.9129\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2705 (1.2705)\tPrec@1 53.516 (53.516)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4409 (1.2633)\tPrec@1 50.000 (55.420)\n",
      "EPOCH: 114 val Results: Prec@1 55.420 Loss: 1.2633\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/195]\tTime 0.011 (0.011)\tLoss 0.7965 (0.7965)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [115][39/195]\tTime 0.005 (0.009)\tLoss 0.7905 (0.8306)\tPrec@1 71.875 (71.504)\n",
      "Epoch: [115][78/195]\tTime 0.006 (0.010)\tLoss 0.8817 (0.8596)\tPrec@1 68.750 (70.050)\n",
      "Epoch: [115][117/195]\tTime 0.006 (0.012)\tLoss 1.0090 (0.8776)\tPrec@1 65.625 (69.263)\n",
      "Epoch: [115][156/195]\tTime 0.005 (0.012)\tLoss 1.0305 (0.8959)\tPrec@1 63.672 (68.553)\n",
      "Epoch: [115][195/195]\tTime 0.004 (0.011)\tLoss 1.1771 (0.9093)\tPrec@1 58.750 (68.026)\n",
      "EPOCH: 115 train Results: Prec@1 68.026 Loss: 0.9093\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2560 (1.2560)\tPrec@1 53.125 (53.125)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2967 (1.2553)\tPrec@1 50.000 (56.300)\n",
      "EPOCH: 115 val Results: Prec@1 56.300 Loss: 1.2553\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/195]\tTime 0.018 (0.018)\tLoss 0.8678 (0.8678)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [116][39/195]\tTime 0.004 (0.010)\tLoss 0.9363 (0.8635)\tPrec@1 62.109 (69.580)\n",
      "Epoch: [116][78/195]\tTime 0.015 (0.009)\tLoss 0.9794 (0.8788)\tPrec@1 65.234 (69.047)\n",
      "Epoch: [116][117/195]\tTime 0.006 (0.009)\tLoss 0.9728 (0.8912)\tPrec@1 68.359 (68.343)\n",
      "Epoch: [116][156/195]\tTime 0.010 (0.008)\tLoss 0.9365 (0.9049)\tPrec@1 69.141 (67.869)\n",
      "Epoch: [116][195/195]\tTime 0.002 (0.009)\tLoss 1.1253 (0.9135)\tPrec@1 60.000 (67.468)\n",
      "EPOCH: 116 train Results: Prec@1 67.468 Loss: 0.9135\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.3229 (1.3229)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.2649 (1.2493)\tPrec@1 37.500 (55.730)\n",
      "EPOCH: 116 val Results: Prec@1 55.730 Loss: 1.2493\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/195]\tTime 0.011 (0.011)\tLoss 0.7721 (0.7721)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [117][39/195]\tTime 0.004 (0.006)\tLoss 0.9494 (0.8507)\tPrec@1 64.453 (69.619)\n",
      "Epoch: [117][78/195]\tTime 0.007 (0.007)\tLoss 0.9559 (0.8667)\tPrec@1 65.234 (69.165)\n",
      "Epoch: [117][117/195]\tTime 0.007 (0.007)\tLoss 0.9250 (0.8829)\tPrec@1 67.188 (68.561)\n",
      "Epoch: [117][156/195]\tTime 0.003 (0.007)\tLoss 1.0367 (0.8983)\tPrec@1 67.578 (67.929)\n",
      "Epoch: [117][195/195]\tTime 0.001 (0.007)\tLoss 1.0278 (0.9111)\tPrec@1 60.000 (67.458)\n",
      "EPOCH: 117 train Results: Prec@1 67.458 Loss: 0.9111\n",
      "Test: [0/39]\tTime 0.010 (0.010)\tLoss 1.2293 (1.2293)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.8051 (1.2535)\tPrec@1 37.500 (55.830)\n",
      "EPOCH: 117 val Results: Prec@1 55.830 Loss: 1.2535\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/195]\tTime 0.005 (0.005)\tLoss 0.8912 (0.8912)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [118][39/195]\tTime 0.009 (0.009)\tLoss 0.8630 (0.8658)\tPrec@1 69.531 (69.170)\n",
      "Epoch: [118][78/195]\tTime 0.008 (0.008)\tLoss 0.8697 (0.8682)\tPrec@1 67.969 (68.982)\n",
      "Epoch: [118][117/195]\tTime 0.007 (0.007)\tLoss 0.9386 (0.8775)\tPrec@1 65.625 (68.594)\n",
      "Epoch: [118][156/195]\tTime 0.015 (0.007)\tLoss 0.9815 (0.8952)\tPrec@1 66.797 (68.011)\n",
      "Epoch: [118][195/195]\tTime 0.004 (0.007)\tLoss 0.9928 (0.9085)\tPrec@1 63.750 (67.558)\n",
      "EPOCH: 118 train Results: Prec@1 67.558 Loss: 0.9085\n",
      "Test: [0/39]\tTime 0.087 (0.087)\tLoss 1.2761 (1.2761)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.005)\tLoss 1.1734 (1.2630)\tPrec@1 50.000 (56.210)\n",
      "EPOCH: 118 val Results: Prec@1 56.210 Loss: 1.2630\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/195]\tTime 0.012 (0.012)\tLoss 0.8937 (0.8937)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [119][39/195]\tTime 0.004 (0.006)\tLoss 0.9394 (0.8406)\tPrec@1 66.016 (70.488)\n",
      "Epoch: [119][78/195]\tTime 0.004 (0.006)\tLoss 0.9219 (0.8654)\tPrec@1 67.969 (69.086)\n",
      "Epoch: [119][117/195]\tTime 0.004 (0.006)\tLoss 0.9211 (0.8806)\tPrec@1 66.016 (68.767)\n",
      "Epoch: [119][156/195]\tTime 0.007 (0.006)\tLoss 0.8574 (0.8927)\tPrec@1 67.969 (68.220)\n",
      "Epoch: [119][195/195]\tTime 0.008 (0.006)\tLoss 1.1496 (0.9067)\tPrec@1 63.750 (67.748)\n",
      "EPOCH: 119 train Results: Prec@1 67.748 Loss: 0.9067\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2514 (1.2514)\tPrec@1 60.156 (60.156)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.2332 (1.2664)\tPrec@1 43.750 (55.800)\n",
      "EPOCH: 119 val Results: Prec@1 55.800 Loss: 1.2664\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/195]\tTime 0.009 (0.009)\tLoss 0.7223 (0.7223)\tPrec@1 76.953 (76.953)\n",
      "Epoch: [120][39/195]\tTime 0.003 (0.006)\tLoss 0.9146 (0.8511)\tPrec@1 69.141 (70.117)\n",
      "Epoch: [120][78/195]\tTime 0.004 (0.006)\tLoss 0.7941 (0.8698)\tPrec@1 70.312 (69.076)\n",
      "Epoch: [120][117/195]\tTime 0.007 (0.006)\tLoss 0.9741 (0.8847)\tPrec@1 67.188 (68.614)\n",
      "Epoch: [120][156/195]\tTime 0.006 (0.007)\tLoss 1.0258 (0.8949)\tPrec@1 63.672 (68.093)\n",
      "Epoch: [120][195/195]\tTime 0.003 (0.006)\tLoss 1.0103 (0.9076)\tPrec@1 66.250 (67.706)\n",
      "EPOCH: 120 train Results: Prec@1 67.706 Loss: 0.9076\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2664 (1.2664)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.1595 (1.2547)\tPrec@1 50.000 (56.560)\n",
      "EPOCH: 120 val Results: Prec@1 56.560 Loss: 1.2547\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/195]\tTime 0.007 (0.007)\tLoss 0.7738 (0.7738)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [121][39/195]\tTime 0.020 (0.006)\tLoss 0.8693 (0.8406)\tPrec@1 67.969 (69.590)\n",
      "Epoch: [121][78/195]\tTime 0.006 (0.005)\tLoss 0.9192 (0.8664)\tPrec@1 69.531 (68.963)\n",
      "Epoch: [121][117/195]\tTime 0.005 (0.006)\tLoss 0.8576 (0.8795)\tPrec@1 69.141 (68.568)\n",
      "Epoch: [121][156/195]\tTime 0.006 (0.006)\tLoss 0.8474 (0.8930)\tPrec@1 68.359 (67.991)\n",
      "Epoch: [121][195/195]\tTime 0.003 (0.006)\tLoss 0.7702 (0.9053)\tPrec@1 70.000 (67.560)\n",
      "EPOCH: 121 train Results: Prec@1 67.560 Loss: 0.9053\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2534 (1.2534)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3127 (1.2647)\tPrec@1 31.250 (55.540)\n",
      "EPOCH: 121 val Results: Prec@1 55.540 Loss: 1.2647\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/195]\tTime 0.008 (0.008)\tLoss 0.8155 (0.8155)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [122][39/195]\tTime 0.008 (0.006)\tLoss 0.8606 (0.8368)\tPrec@1 64.844 (70.420)\n",
      "Epoch: [122][78/195]\tTime 0.006 (0.006)\tLoss 0.8971 (0.8593)\tPrec@1 69.531 (69.576)\n",
      "Epoch: [122][117/195]\tTime 0.007 (0.006)\tLoss 0.9347 (0.8822)\tPrec@1 72.656 (68.608)\n",
      "Epoch: [122][156/195]\tTime 0.008 (0.006)\tLoss 0.9040 (0.8965)\tPrec@1 69.141 (67.984)\n",
      "Epoch: [122][195/195]\tTime 0.011 (0.006)\tLoss 1.0915 (0.9081)\tPrec@1 63.750 (67.610)\n",
      "EPOCH: 122 train Results: Prec@1 67.610 Loss: 0.9081\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2776 (1.2776)\tPrec@1 54.688 (54.688)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2552 (1.2698)\tPrec@1 37.500 (56.040)\n",
      "EPOCH: 122 val Results: Prec@1 56.040 Loss: 1.2698\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/195]\tTime 0.004 (0.004)\tLoss 0.8266 (0.8266)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [123][39/195]\tTime 0.009 (0.007)\tLoss 0.7961 (0.8365)\tPrec@1 68.750 (70.684)\n",
      "Epoch: [123][78/195]\tTime 0.008 (0.007)\tLoss 0.9005 (0.8592)\tPrec@1 67.969 (69.744)\n",
      "Epoch: [123][117/195]\tTime 0.004 (0.008)\tLoss 1.0523 (0.8754)\tPrec@1 59.375 (69.260)\n",
      "Epoch: [123][156/195]\tTime 0.003 (0.008)\tLoss 0.9326 (0.8899)\tPrec@1 66.016 (68.646)\n",
      "Epoch: [123][195/195]\tTime 0.004 (0.007)\tLoss 1.0721 (0.9062)\tPrec@1 62.500 (67.958)\n",
      "EPOCH: 123 train Results: Prec@1 67.958 Loss: 0.9062\n",
      "Test: [0/39]\tTime 0.006 (0.006)\tLoss 1.3238 (1.3238)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3440 (1.2659)\tPrec@1 50.000 (55.920)\n",
      "EPOCH: 123 val Results: Prec@1 55.920 Loss: 1.2659\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/195]\tTime 0.006 (0.006)\tLoss 0.7708 (0.7708)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [124][39/195]\tTime 0.004 (0.007)\tLoss 0.8708 (0.8528)\tPrec@1 69.531 (69.990)\n",
      "Epoch: [124][78/195]\tTime 0.003 (0.006)\tLoss 0.8737 (0.8761)\tPrec@1 67.188 (68.908)\n",
      "Epoch: [124][117/195]\tTime 0.037 (0.006)\tLoss 0.8899 (0.8858)\tPrec@1 67.578 (68.488)\n",
      "Epoch: [124][156/195]\tTime 0.004 (0.006)\tLoss 0.9089 (0.8951)\tPrec@1 67.969 (68.228)\n",
      "Epoch: [124][195/195]\tTime 0.004 (0.006)\tLoss 1.1119 (0.9101)\tPrec@1 60.000 (67.736)\n",
      "EPOCH: 124 train Results: Prec@1 67.736 Loss: 0.9101\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.3099 (1.3099)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5263 (1.2671)\tPrec@1 37.500 (56.490)\n",
      "EPOCH: 124 val Results: Prec@1 56.490 Loss: 1.2671\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/195]\tTime 0.005 (0.005)\tLoss 0.7767 (0.7767)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [125][39/195]\tTime 0.003 (0.006)\tLoss 0.7652 (0.8432)\tPrec@1 71.875 (70.410)\n",
      "Epoch: [125][78/195]\tTime 0.005 (0.006)\tLoss 0.8516 (0.8671)\tPrec@1 69.531 (69.180)\n",
      "Epoch: [125][117/195]\tTime 0.011 (0.008)\tLoss 0.8850 (0.8790)\tPrec@1 67.578 (68.790)\n",
      "Epoch: [125][156/195]\tTime 0.009 (0.008)\tLoss 0.8522 (0.8920)\tPrec@1 70.703 (68.320)\n",
      "Epoch: [125][195/195]\tTime 0.021 (0.008)\tLoss 0.8122 (0.9025)\tPrec@1 71.250 (67.900)\n",
      "EPOCH: 125 train Results: Prec@1 67.900 Loss: 0.9025\n",
      "Test: [0/39]\tTime 0.005 (0.005)\tLoss 1.2366 (1.2366)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.001 (0.003)\tLoss 1.1333 (1.2682)\tPrec@1 50.000 (55.730)\n",
      "EPOCH: 125 val Results: Prec@1 55.730 Loss: 1.2682\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/195]\tTime 0.007 (0.007)\tLoss 0.8218 (0.8218)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [126][39/195]\tTime 0.004 (0.007)\tLoss 0.8897 (0.8297)\tPrec@1 71.094 (71.016)\n",
      "Epoch: [126][78/195]\tTime 0.004 (0.006)\tLoss 0.8731 (0.8543)\tPrec@1 66.406 (69.744)\n",
      "Epoch: [126][117/195]\tTime 0.006 (0.007)\tLoss 0.8683 (0.8710)\tPrec@1 74.609 (69.055)\n",
      "Epoch: [126][156/195]\tTime 0.007 (0.008)\tLoss 0.9484 (0.8905)\tPrec@1 61.328 (68.339)\n",
      "Epoch: [126][195/195]\tTime 0.002 (0.007)\tLoss 0.9353 (0.9067)\tPrec@1 71.250 (67.784)\n",
      "EPOCH: 126 train Results: Prec@1 67.784 Loss: 0.9067\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2912 (1.2912)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.001 (0.002)\tLoss 1.4369 (1.2617)\tPrec@1 37.500 (56.390)\n",
      "EPOCH: 126 val Results: Prec@1 56.390 Loss: 1.2617\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/195]\tTime 0.008 (0.008)\tLoss 0.8986 (0.8986)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [127][39/195]\tTime 0.014 (0.006)\tLoss 0.7624 (0.8386)\tPrec@1 74.219 (70.615)\n",
      "Epoch: [127][78/195]\tTime 0.004 (0.006)\tLoss 0.8872 (0.8575)\tPrec@1 67.188 (69.675)\n",
      "Epoch: [127][117/195]\tTime 0.005 (0.006)\tLoss 1.1520 (0.8761)\tPrec@1 59.375 (68.995)\n",
      "Epoch: [127][156/195]\tTime 0.008 (0.006)\tLoss 0.9179 (0.8951)\tPrec@1 65.234 (68.245)\n",
      "Epoch: [127][195/195]\tTime 0.002 (0.006)\tLoss 0.9283 (0.9070)\tPrec@1 68.750 (67.930)\n",
      "EPOCH: 127 train Results: Prec@1 67.930 Loss: 0.9070\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2777 (1.2777)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4082 (1.2642)\tPrec@1 43.750 (56.560)\n",
      "EPOCH: 127 val Results: Prec@1 56.560 Loss: 1.2642\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/195]\tTime 0.010 (0.010)\tLoss 0.8796 (0.8796)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [128][39/195]\tTime 0.005 (0.006)\tLoss 0.8614 (0.8464)\tPrec@1 67.969 (69.844)\n",
      "Epoch: [128][78/195]\tTime 0.004 (0.006)\tLoss 0.8498 (0.8595)\tPrec@1 69.531 (69.462)\n",
      "Epoch: [128][117/195]\tTime 0.004 (0.006)\tLoss 0.8610 (0.8723)\tPrec@1 69.141 (68.747)\n",
      "Epoch: [128][156/195]\tTime 0.003 (0.006)\tLoss 1.0104 (0.8876)\tPrec@1 63.672 (68.175)\n",
      "Epoch: [128][195/195]\tTime 0.014 (0.006)\tLoss 1.0094 (0.9018)\tPrec@1 63.750 (67.778)\n",
      "EPOCH: 128 train Results: Prec@1 67.778 Loss: 0.9018\n",
      "Test: [0/39]\tTime 0.007 (0.007)\tLoss 1.2885 (1.2885)\tPrec@1 53.516 (53.516)\n",
      "Test: [39/39]\tTime 0.001 (0.002)\tLoss 1.2714 (1.2793)\tPrec@1 50.000 (55.500)\n",
      "EPOCH: 128 val Results: Prec@1 55.500 Loss: 1.2793\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/195]\tTime 0.010 (0.010)\tLoss 0.9159 (0.9159)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [129][39/195]\tTime 0.003 (0.006)\tLoss 0.8601 (0.8493)\tPrec@1 67.188 (69.893)\n",
      "Epoch: [129][78/195]\tTime 0.004 (0.006)\tLoss 0.9956 (0.8662)\tPrec@1 64.062 (69.269)\n",
      "Epoch: [129][117/195]\tTime 0.004 (0.006)\tLoss 0.9077 (0.8796)\tPrec@1 67.578 (68.796)\n",
      "Epoch: [129][156/195]\tTime 0.008 (0.006)\tLoss 0.7901 (0.8935)\tPrec@1 71.484 (68.148)\n",
      "Epoch: [129][195/195]\tTime 0.003 (0.007)\tLoss 0.9491 (0.9063)\tPrec@1 65.000 (67.786)\n",
      "EPOCH: 129 train Results: Prec@1 67.786 Loss: 0.9063\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2969 (1.2969)\tPrec@1 55.859 (55.859)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3883 (1.2747)\tPrec@1 37.500 (55.690)\n",
      "EPOCH: 129 val Results: Prec@1 55.690 Loss: 1.2747\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/195]\tTime 0.005 (0.005)\tLoss 0.8140 (0.8140)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [130][39/195]\tTime 0.003 (0.006)\tLoss 0.7816 (0.8385)\tPrec@1 73.828 (70.371)\n",
      "Epoch: [130][78/195]\tTime 0.006 (0.006)\tLoss 0.9532 (0.8511)\tPrec@1 66.797 (69.912)\n",
      "Epoch: [130][117/195]\tTime 0.009 (0.006)\tLoss 0.9757 (0.8742)\tPrec@1 61.719 (68.972)\n",
      "Epoch: [130][156/195]\tTime 0.007 (0.006)\tLoss 0.9653 (0.8892)\tPrec@1 66.797 (68.432)\n",
      "Epoch: [130][195/195]\tTime 0.002 (0.006)\tLoss 1.0586 (0.9009)\tPrec@1 53.750 (67.996)\n",
      "EPOCH: 130 train Results: Prec@1 67.996 Loss: 0.9009\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2875 (1.2875)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3077 (1.2680)\tPrec@1 31.250 (55.730)\n",
      "EPOCH: 130 val Results: Prec@1 55.730 Loss: 1.2680\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/195]\tTime 0.021 (0.021)\tLoss 0.7949 (0.7949)\tPrec@1 73.047 (73.047)\n",
      "Epoch: [131][39/195]\tTime 0.010 (0.008)\tLoss 0.8043 (0.8383)\tPrec@1 73.438 (70.127)\n",
      "Epoch: [131][78/195]\tTime 0.021 (0.007)\tLoss 0.8832 (0.8590)\tPrec@1 66.016 (69.393)\n",
      "Epoch: [131][117/195]\tTime 0.015 (0.007)\tLoss 1.0213 (0.8775)\tPrec@1 63.672 (68.671)\n",
      "Epoch: [131][156/195]\tTime 0.007 (0.007)\tLoss 1.1299 (0.8935)\tPrec@1 59.766 (68.138)\n",
      "Epoch: [131][195/195]\tTime 0.002 (0.007)\tLoss 0.9054 (0.9064)\tPrec@1 63.750 (67.776)\n",
      "EPOCH: 131 train Results: Prec@1 67.776 Loss: 0.9064\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2540 (1.2540)\tPrec@1 55.859 (55.859)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.7092 (1.2687)\tPrec@1 18.750 (55.940)\n",
      "EPOCH: 131 val Results: Prec@1 55.940 Loss: 1.2687\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/195]\tTime 0.006 (0.006)\tLoss 0.9391 (0.9391)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [132][39/195]\tTime 0.013 (0.007)\tLoss 0.9257 (0.8325)\tPrec@1 68.750 (70.605)\n",
      "Epoch: [132][78/195]\tTime 0.008 (0.006)\tLoss 1.0146 (0.8615)\tPrec@1 63.672 (69.492)\n",
      "Epoch: [132][117/195]\tTime 0.006 (0.006)\tLoss 0.9028 (0.8784)\tPrec@1 66.406 (68.823)\n",
      "Epoch: [132][156/195]\tTime 0.015 (0.006)\tLoss 1.0364 (0.8932)\tPrec@1 58.984 (68.195)\n",
      "Epoch: [132][195/195]\tTime 0.003 (0.006)\tLoss 0.9618 (0.9023)\tPrec@1 66.250 (67.790)\n",
      "EPOCH: 132 train Results: Prec@1 67.790 Loss: 0.9023\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2123 (1.2123)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5616 (1.2595)\tPrec@1 25.000 (56.000)\n",
      "EPOCH: 132 val Results: Prec@1 56.000 Loss: 1.2595\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/195]\tTime 0.012 (0.012)\tLoss 0.7804 (0.7804)\tPrec@1 73.047 (73.047)\n",
      "Epoch: [133][39/195]\tTime 0.004 (0.007)\tLoss 0.9022 (0.8404)\tPrec@1 64.844 (70.117)\n",
      "Epoch: [133][78/195]\tTime 0.017 (0.007)\tLoss 0.8867 (0.8544)\tPrec@1 69.531 (69.769)\n",
      "Epoch: [133][117/195]\tTime 0.003 (0.006)\tLoss 0.9165 (0.8726)\tPrec@1 69.531 (69.094)\n",
      "Epoch: [133][156/195]\tTime 0.008 (0.006)\tLoss 0.8906 (0.8915)\tPrec@1 69.922 (68.402)\n",
      "Epoch: [133][195/195]\tTime 0.002 (0.006)\tLoss 0.9250 (0.9060)\tPrec@1 68.750 (67.742)\n",
      "EPOCH: 133 train Results: Prec@1 67.742 Loss: 0.9060\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2762 (1.2762)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.1980 (1.2629)\tPrec@1 56.250 (55.800)\n",
      "EPOCH: 133 val Results: Prec@1 55.800 Loss: 1.2629\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/195]\tTime 0.007 (0.007)\tLoss 0.7715 (0.7715)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [134][39/195]\tTime 0.007 (0.005)\tLoss 0.8687 (0.8410)\tPrec@1 67.188 (70.088)\n",
      "Epoch: [134][78/195]\tTime 0.003 (0.005)\tLoss 0.8182 (0.8587)\tPrec@1 72.266 (69.704)\n",
      "Epoch: [134][117/195]\tTime 0.011 (0.005)\tLoss 0.9129 (0.8712)\tPrec@1 68.750 (69.250)\n",
      "Epoch: [134][156/195]\tTime 0.005 (0.005)\tLoss 0.8900 (0.8873)\tPrec@1 66.406 (68.476)\n",
      "Epoch: [134][195/195]\tTime 0.004 (0.006)\tLoss 0.8709 (0.9015)\tPrec@1 66.250 (68.002)\n",
      "EPOCH: 134 train Results: Prec@1 68.002 Loss: 0.9015\n",
      "Test: [0/39]\tTime 0.007 (0.007)\tLoss 1.2489 (1.2489)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.1928 (1.2681)\tPrec@1 50.000 (56.130)\n",
      "EPOCH: 134 val Results: Prec@1 56.130 Loss: 1.2681\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/195]\tTime 0.004 (0.004)\tLoss 0.7993 (0.7993)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [135][39/195]\tTime 0.005 (0.007)\tLoss 0.7750 (0.8237)\tPrec@1 72.656 (70.811)\n",
      "Epoch: [135][78/195]\tTime 0.013 (0.008)\tLoss 0.9329 (0.8484)\tPrec@1 67.578 (69.932)\n",
      "Epoch: [135][117/195]\tTime 0.005 (0.007)\tLoss 1.0244 (0.8686)\tPrec@1 64.453 (69.329)\n",
      "Epoch: [135][156/195]\tTime 0.007 (0.007)\tLoss 0.8506 (0.8864)\tPrec@1 67.578 (68.541)\n",
      "Epoch: [135][195/195]\tTime 0.001 (0.006)\tLoss 1.0438 (0.9004)\tPrec@1 60.000 (68.026)\n",
      "EPOCH: 135 train Results: Prec@1 68.026 Loss: 0.9004\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2521 (1.2521)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2511 (1.2636)\tPrec@1 31.250 (55.620)\n",
      "EPOCH: 135 val Results: Prec@1 55.620 Loss: 1.2636\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/195]\tTime 0.004 (0.004)\tLoss 0.6852 (0.6852)\tPrec@1 75.391 (75.391)\n",
      "Epoch: [136][39/195]\tTime 0.005 (0.005)\tLoss 0.8854 (0.8447)\tPrec@1 72.266 (70.840)\n",
      "Epoch: [136][78/195]\tTime 0.004 (0.006)\tLoss 0.8356 (0.8595)\tPrec@1 70.703 (69.942)\n",
      "Epoch: [136][117/195]\tTime 0.005 (0.007)\tLoss 0.9245 (0.8791)\tPrec@1 67.969 (69.028)\n",
      "Epoch: [136][156/195]\tTime 0.003 (0.007)\tLoss 1.0295 (0.8913)\tPrec@1 62.891 (68.432)\n",
      "Epoch: [136][195/195]\tTime 0.003 (0.006)\tLoss 0.8166 (0.9041)\tPrec@1 68.750 (67.888)\n",
      "EPOCH: 136 train Results: Prec@1 67.888 Loss: 0.9041\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2522 (1.2522)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4023 (1.2583)\tPrec@1 31.250 (56.240)\n",
      "EPOCH: 136 val Results: Prec@1 56.240 Loss: 1.2583\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/195]\tTime 0.006 (0.006)\tLoss 0.8087 (0.8087)\tPrec@1 75.391 (75.391)\n",
      "Epoch: [137][39/195]\tTime 0.006 (0.006)\tLoss 0.8474 (0.8569)\tPrec@1 70.703 (70.166)\n",
      "Epoch: [137][78/195]\tTime 0.016 (0.006)\tLoss 0.8407 (0.8649)\tPrec@1 68.750 (69.556)\n",
      "Epoch: [137][117/195]\tTime 0.007 (0.006)\tLoss 0.8863 (0.8702)\tPrec@1 68.750 (69.167)\n",
      "Epoch: [137][156/195]\tTime 0.004 (0.005)\tLoss 0.9188 (0.8849)\tPrec@1 66.406 (68.603)\n",
      "Epoch: [137][195/195]\tTime 0.004 (0.006)\tLoss 1.0583 (0.9004)\tPrec@1 62.500 (68.012)\n",
      "EPOCH: 137 train Results: Prec@1 68.012 Loss: 0.9004\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2317 (1.2317)\tPrec@1 55.859 (55.859)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.6361 (1.2613)\tPrec@1 31.250 (55.960)\n",
      "EPOCH: 137 val Results: Prec@1 55.960 Loss: 1.2613\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/195]\tTime 0.008 (0.008)\tLoss 0.8413 (0.8413)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [138][39/195]\tTime 0.003 (0.006)\tLoss 0.8492 (0.8519)\tPrec@1 73.047 (69.766)\n",
      "Epoch: [138][78/195]\tTime 0.003 (0.006)\tLoss 0.8436 (0.8602)\tPrec@1 71.875 (69.190)\n",
      "Epoch: [138][117/195]\tTime 0.006 (0.006)\tLoss 0.9224 (0.8778)\tPrec@1 66.406 (68.628)\n",
      "Epoch: [138][156/195]\tTime 0.005 (0.006)\tLoss 1.0602 (0.8897)\tPrec@1 61.719 (68.225)\n",
      "Epoch: [138][195/195]\tTime 0.003 (0.006)\tLoss 1.0401 (0.9039)\tPrec@1 60.000 (67.748)\n",
      "EPOCH: 138 train Results: Prec@1 67.748 Loss: 0.9039\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1734 (1.1734)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2861 (1.2641)\tPrec@1 37.500 (56.430)\n",
      "EPOCH: 138 val Results: Prec@1 56.430 Loss: 1.2641\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/195]\tTime 0.004 (0.004)\tLoss 0.8014 (0.8014)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [139][39/195]\tTime 0.014 (0.007)\tLoss 0.8469 (0.8307)\tPrec@1 67.969 (70.742)\n",
      "Epoch: [139][78/195]\tTime 0.014 (0.007)\tLoss 1.0392 (0.8534)\tPrec@1 64.062 (69.838)\n",
      "Epoch: [139][117/195]\tTime 0.006 (0.006)\tLoss 0.9739 (0.8670)\tPrec@1 63.281 (69.243)\n",
      "Epoch: [139][156/195]\tTime 0.003 (0.006)\tLoss 0.9319 (0.8826)\tPrec@1 64.844 (68.613)\n",
      "Epoch: [139][195/195]\tTime 0.004 (0.006)\tLoss 1.1225 (0.8945)\tPrec@1 62.500 (68.256)\n",
      "EPOCH: 139 train Results: Prec@1 68.256 Loss: 0.8945\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2951 (1.2951)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4516 (1.2639)\tPrec@1 31.250 (55.980)\n",
      "EPOCH: 139 val Results: Prec@1 55.980 Loss: 1.2639\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/195]\tTime 0.012 (0.012)\tLoss 0.9158 (0.9158)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [140][39/195]\tTime 0.003 (0.007)\tLoss 0.9086 (0.8430)\tPrec@1 67.578 (70.420)\n",
      "Epoch: [140][78/195]\tTime 0.005 (0.006)\tLoss 0.9160 (0.8604)\tPrec@1 68.750 (69.408)\n",
      "Epoch: [140][117/195]\tTime 0.012 (0.006)\tLoss 0.8487 (0.8709)\tPrec@1 67.969 (68.916)\n",
      "Epoch: [140][156/195]\tTime 0.004 (0.006)\tLoss 0.9059 (0.8846)\tPrec@1 69.531 (68.456)\n",
      "Epoch: [140][195/195]\tTime 0.003 (0.006)\tLoss 0.9956 (0.8980)\tPrec@1 57.500 (67.908)\n",
      "EPOCH: 140 train Results: Prec@1 67.908 Loss: 0.8980\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2672 (1.2672)\tPrec@1 55.859 (55.859)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6662 (1.2574)\tPrec@1 37.500 (56.210)\n",
      "EPOCH: 140 val Results: Prec@1 56.210 Loss: 1.2574\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/195]\tTime 0.008 (0.008)\tLoss 0.7358 (0.7358)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [141][39/195]\tTime 0.007 (0.006)\tLoss 0.8560 (0.8314)\tPrec@1 68.359 (70.430)\n",
      "Epoch: [141][78/195]\tTime 0.015 (0.006)\tLoss 0.9347 (0.8559)\tPrec@1 65.625 (69.462)\n",
      "Epoch: [141][117/195]\tTime 0.009 (0.006)\tLoss 0.9744 (0.8737)\tPrec@1 67.969 (68.919)\n",
      "Epoch: [141][156/195]\tTime 0.013 (0.006)\tLoss 0.9735 (0.8860)\tPrec@1 64.062 (68.429)\n",
      "Epoch: [141][195/195]\tTime 0.002 (0.006)\tLoss 1.1113 (0.8978)\tPrec@1 62.500 (68.102)\n",
      "EPOCH: 141 train Results: Prec@1 68.102 Loss: 0.8978\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2384 (1.2384)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4966 (1.2503)\tPrec@1 37.500 (56.390)\n",
      "EPOCH: 141 val Results: Prec@1 56.390 Loss: 1.2503\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/195]\tTime 0.008 (0.008)\tLoss 0.8072 (0.8072)\tPrec@1 73.047 (73.047)\n",
      "Epoch: [142][39/195]\tTime 0.009 (0.007)\tLoss 0.7977 (0.8304)\tPrec@1 70.312 (70.791)\n",
      "Epoch: [142][78/195]\tTime 0.005 (0.006)\tLoss 0.7639 (0.8508)\tPrec@1 73.047 (69.783)\n",
      "Epoch: [142][117/195]\tTime 0.008 (0.006)\tLoss 0.9401 (0.8663)\tPrec@1 66.016 (69.160)\n",
      "Epoch: [142][156/195]\tTime 0.007 (0.006)\tLoss 0.8809 (0.8828)\tPrec@1 68.359 (68.539)\n",
      "Epoch: [142][195/195]\tTime 0.002 (0.006)\tLoss 0.8713 (0.8967)\tPrec@1 75.000 (68.062)\n",
      "EPOCH: 142 train Results: Prec@1 68.062 Loss: 0.8967\n",
      "Test: [0/39]\tTime 0.007 (0.007)\tLoss 1.2079 (1.2079)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3202 (1.2606)\tPrec@1 43.750 (56.110)\n",
      "EPOCH: 142 val Results: Prec@1 56.110 Loss: 1.2606\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/195]\tTime 0.010 (0.010)\tLoss 0.7021 (0.7021)\tPrec@1 79.297 (79.297)\n",
      "Epoch: [143][39/195]\tTime 0.005 (0.007)\tLoss 0.8011 (0.8209)\tPrec@1 71.875 (71.543)\n",
      "Epoch: [143][78/195]\tTime 0.007 (0.007)\tLoss 0.7815 (0.8433)\tPrec@1 71.875 (70.555)\n",
      "Epoch: [143][117/195]\tTime 0.004 (0.007)\tLoss 0.8840 (0.8648)\tPrec@1 68.359 (69.634)\n",
      "Epoch: [143][156/195]\tTime 0.004 (0.006)\tLoss 1.0106 (0.8836)\tPrec@1 63.281 (68.892)\n",
      "Epoch: [143][195/195]\tTime 0.009 (0.006)\tLoss 0.9568 (0.8990)\tPrec@1 66.250 (68.252)\n",
      "EPOCH: 143 train Results: Prec@1 68.252 Loss: 0.8990\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1508 (1.1508)\tPrec@1 59.766 (59.766)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.7740 (1.2669)\tPrec@1 25.000 (55.450)\n",
      "EPOCH: 143 val Results: Prec@1 55.450 Loss: 1.2669\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/195]\tTime 0.006 (0.006)\tLoss 0.7320 (0.7320)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [144][39/195]\tTime 0.005 (0.005)\tLoss 0.8386 (0.8351)\tPrec@1 71.484 (70.752)\n",
      "Epoch: [144][78/195]\tTime 0.010 (0.006)\tLoss 1.0253 (0.8546)\tPrec@1 64.844 (69.848)\n",
      "Epoch: [144][117/195]\tTime 0.011 (0.006)\tLoss 0.8519 (0.8680)\tPrec@1 68.750 (69.309)\n",
      "Epoch: [144][156/195]\tTime 0.006 (0.006)\tLoss 0.8452 (0.8823)\tPrec@1 71.875 (68.752)\n",
      "Epoch: [144][195/195]\tTime 0.001 (0.006)\tLoss 0.9684 (0.8942)\tPrec@1 67.500 (68.280)\n",
      "EPOCH: 144 train Results: Prec@1 68.280 Loss: 0.8942\n",
      "Test: [0/39]\tTime 0.008 (0.008)\tLoss 1.2228 (1.2228)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6775 (1.2657)\tPrec@1 18.750 (56.180)\n",
      "EPOCH: 144 val Results: Prec@1 56.180 Loss: 1.2657\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/195]\tTime 0.006 (0.006)\tLoss 0.8961 (0.8961)\tPrec@1 69.922 (69.922)\n",
      "Epoch: [145][39/195]\tTime 0.008 (0.005)\tLoss 0.9111 (0.8413)\tPrec@1 69.141 (70.244)\n",
      "Epoch: [145][78/195]\tTime 0.007 (0.006)\tLoss 1.0009 (0.8498)\tPrec@1 63.672 (69.981)\n",
      "Epoch: [145][117/195]\tTime 0.004 (0.006)\tLoss 0.9973 (0.8688)\tPrec@1 64.062 (69.114)\n",
      "Epoch: [145][156/195]\tTime 0.004 (0.006)\tLoss 0.9745 (0.8885)\tPrec@1 65.234 (68.320)\n",
      "Epoch: [145][195/195]\tTime 0.008 (0.006)\tLoss 1.1056 (0.9000)\tPrec@1 65.000 (67.916)\n",
      "EPOCH: 145 train Results: Prec@1 67.916 Loss: 0.9000\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.3053 (1.3053)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.6338 (1.2675)\tPrec@1 37.500 (55.740)\n",
      "EPOCH: 145 val Results: Prec@1 55.740 Loss: 1.2675\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/195]\tTime 0.005 (0.005)\tLoss 0.8392 (0.8392)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [146][39/195]\tTime 0.006 (0.007)\tLoss 0.9095 (0.8401)\tPrec@1 69.531 (70.703)\n",
      "Epoch: [146][78/195]\tTime 0.004 (0.006)\tLoss 0.9304 (0.8618)\tPrec@1 66.797 (69.546)\n",
      "Epoch: [146][117/195]\tTime 0.011 (0.007)\tLoss 0.7627 (0.8771)\tPrec@1 73.047 (69.038)\n",
      "Epoch: [146][156/195]\tTime 0.004 (0.006)\tLoss 0.9520 (0.8909)\tPrec@1 67.188 (68.591)\n",
      "Epoch: [146][195/195]\tTime 0.002 (0.007)\tLoss 0.6984 (0.9010)\tPrec@1 77.500 (68.164)\n",
      "EPOCH: 146 train Results: Prec@1 68.164 Loss: 0.9010\n",
      "Test: [0/39]\tTime 0.005 (0.005)\tLoss 1.2432 (1.2432)\tPrec@1 60.547 (60.547)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4806 (1.2769)\tPrec@1 43.750 (55.490)\n",
      "EPOCH: 146 val Results: Prec@1 55.490 Loss: 1.2769\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/195]\tTime 0.004 (0.004)\tLoss 0.8038 (0.8038)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [147][39/195]\tTime 0.004 (0.007)\tLoss 0.9093 (0.8323)\tPrec@1 69.141 (70.410)\n",
      "Epoch: [147][78/195]\tTime 0.004 (0.007)\tLoss 1.0161 (0.8545)\tPrec@1 63.281 (69.437)\n",
      "Epoch: [147][117/195]\tTime 0.006 (0.007)\tLoss 0.8967 (0.8680)\tPrec@1 64.844 (69.055)\n",
      "Epoch: [147][156/195]\tTime 0.006 (0.007)\tLoss 0.8986 (0.8809)\tPrec@1 67.969 (68.541)\n",
      "Epoch: [147][195/195]\tTime 0.009 (0.007)\tLoss 1.1955 (0.8958)\tPrec@1 58.750 (68.118)\n",
      "EPOCH: 147 train Results: Prec@1 68.118 Loss: 0.8958\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2500 (1.2500)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.9026 (1.2738)\tPrec@1 31.250 (55.930)\n",
      "EPOCH: 147 val Results: Prec@1 55.930 Loss: 1.2738\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/195]\tTime 0.007 (0.007)\tLoss 0.7626 (0.7626)\tPrec@1 73.047 (73.047)\n",
      "Epoch: [148][39/195]\tTime 0.005 (0.007)\tLoss 0.9173 (0.8356)\tPrec@1 67.578 (70.547)\n",
      "Epoch: [148][78/195]\tTime 0.016 (0.009)\tLoss 0.9586 (0.8500)\tPrec@1 67.969 (69.778)\n",
      "Epoch: [148][117/195]\tTime 0.005 (0.009)\tLoss 0.9743 (0.8662)\tPrec@1 62.891 (69.160)\n",
      "Epoch: [148][156/195]\tTime 0.004 (0.008)\tLoss 0.8704 (0.8826)\tPrec@1 69.922 (68.476)\n",
      "Epoch: [148][195/195]\tTime 0.002 (0.009)\tLoss 0.9408 (0.8972)\tPrec@1 65.000 (67.950)\n",
      "EPOCH: 148 train Results: Prec@1 67.950 Loss: 0.8972\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2418 (1.2418)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5353 (1.2792)\tPrec@1 43.750 (55.640)\n",
      "EPOCH: 148 val Results: Prec@1 55.640 Loss: 1.2792\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/195]\tTime 0.005 (0.005)\tLoss 0.7596 (0.7596)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [149][39/195]\tTime 0.016 (0.013)\tLoss 0.8063 (0.8405)\tPrec@1 67.969 (70.713)\n",
      "Epoch: [149][78/195]\tTime 0.004 (0.025)\tLoss 0.8200 (0.8590)\tPrec@1 72.266 (69.744)\n",
      "Epoch: [149][117/195]\tTime 0.020 (0.032)\tLoss 0.8343 (0.8776)\tPrec@1 70.312 (69.008)\n",
      "Epoch: [149][156/195]\tTime 0.086 (0.035)\tLoss 0.8300 (0.8915)\tPrec@1 71.094 (68.471)\n",
      "Epoch: [149][195/195]\tTime 0.007 (0.036)\tLoss 0.8949 (0.9057)\tPrec@1 66.250 (67.896)\n",
      "EPOCH: 149 train Results: Prec@1 67.896 Loss: 0.9057\n",
      "Test: [0/39]\tTime 0.033 (0.033)\tLoss 1.2490 (1.2490)\tPrec@1 57.812 (57.812)\n",
      "Test: [39/39]\tTime 0.000 (0.014)\tLoss 1.5907 (1.2649)\tPrec@1 43.750 (55.730)\n",
      "EPOCH: 149 val Results: Prec@1 55.730 Loss: 1.2649\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/195]\tTime 0.078 (0.078)\tLoss 0.8075 (0.8075)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [150][39/195]\tTime 0.026 (0.046)\tLoss 0.9426 (0.8133)\tPrec@1 69.531 (71.582)\n",
      "Epoch: [150][78/195]\tTime 0.006 (0.036)\tLoss 0.9718 (0.8355)\tPrec@1 65.625 (70.693)\n",
      "Epoch: [150][117/195]\tTime 0.004 (0.026)\tLoss 1.0261 (0.8672)\tPrec@1 64.062 (69.372)\n",
      "Epoch: [150][156/195]\tTime 0.004 (0.022)\tLoss 0.9507 (0.8835)\tPrec@1 63.672 (68.613)\n",
      "Epoch: [150][195/195]\tTime 0.004 (0.019)\tLoss 1.0527 (0.8942)\tPrec@1 67.500 (68.160)\n",
      "EPOCH: 150 train Results: Prec@1 68.160 Loss: 0.8942\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2930 (1.2930)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3477 (1.2763)\tPrec@1 37.500 (55.650)\n",
      "EPOCH: 150 val Results: Prec@1 55.650 Loss: 1.2763\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/195]\tTime 0.008 (0.008)\tLoss 0.8125 (0.8125)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [151][39/195]\tTime 0.003 (0.006)\tLoss 0.9008 (0.8323)\tPrec@1 68.750 (70.576)\n",
      "Epoch: [151][78/195]\tTime 0.006 (0.006)\tLoss 0.8134 (0.8426)\tPrec@1 66.406 (69.991)\n",
      "Epoch: [151][117/195]\tTime 0.004 (0.006)\tLoss 0.9728 (0.8606)\tPrec@1 64.062 (69.240)\n",
      "Epoch: [151][156/195]\tTime 0.007 (0.007)\tLoss 1.0040 (0.8763)\tPrec@1 65.234 (68.752)\n",
      "Epoch: [151][195/195]\tTime 0.004 (0.006)\tLoss 1.2034 (0.8917)\tPrec@1 62.500 (68.106)\n",
      "EPOCH: 151 train Results: Prec@1 68.106 Loss: 0.8917\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2527 (1.2527)\tPrec@1 55.859 (55.859)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2696 (1.2794)\tPrec@1 56.250 (55.420)\n",
      "EPOCH: 151 val Results: Prec@1 55.420 Loss: 1.2794\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/195]\tTime 0.006 (0.006)\tLoss 0.7066 (0.7066)\tPrec@1 76.172 (76.172)\n",
      "Epoch: [152][39/195]\tTime 0.006 (0.008)\tLoss 0.8695 (0.8256)\tPrec@1 66.016 (71.562)\n",
      "Epoch: [152][78/195]\tTime 0.010 (0.006)\tLoss 0.8807 (0.8481)\tPrec@1 72.656 (70.441)\n",
      "Epoch: [152][117/195]\tTime 0.008 (0.007)\tLoss 0.9360 (0.8675)\tPrec@1 65.234 (69.535)\n",
      "Epoch: [152][156/195]\tTime 0.011 (0.007)\tLoss 0.9137 (0.8820)\tPrec@1 69.922 (68.927)\n",
      "Epoch: [152][195/195]\tTime 0.003 (0.007)\tLoss 0.8571 (0.8973)\tPrec@1 68.750 (68.360)\n",
      "EPOCH: 152 train Results: Prec@1 68.360 Loss: 0.8973\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2464 (1.2464)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2134 (1.2709)\tPrec@1 37.500 (55.600)\n",
      "EPOCH: 152 val Results: Prec@1 55.600 Loss: 1.2709\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/195]\tTime 0.004 (0.004)\tLoss 0.7574 (0.7574)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [153][39/195]\tTime 0.004 (0.006)\tLoss 0.8344 (0.8276)\tPrec@1 67.578 (70.850)\n",
      "Epoch: [153][78/195]\tTime 0.016 (0.006)\tLoss 0.9079 (0.8485)\tPrec@1 69.531 (70.045)\n",
      "Epoch: [153][117/195]\tTime 0.005 (0.006)\tLoss 0.8354 (0.8638)\tPrec@1 71.484 (69.409)\n",
      "Epoch: [153][156/195]\tTime 0.008 (0.006)\tLoss 0.9226 (0.8814)\tPrec@1 67.188 (68.787)\n",
      "Epoch: [153][195/195]\tTime 0.001 (0.006)\tLoss 1.0756 (0.8931)\tPrec@1 63.750 (68.290)\n",
      "EPOCH: 153 train Results: Prec@1 68.290 Loss: 0.8931\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.3198 (1.3198)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4057 (1.2752)\tPrec@1 50.000 (55.480)\n",
      "EPOCH: 153 val Results: Prec@1 55.480 Loss: 1.2752\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/195]\tTime 0.009 (0.009)\tLoss 0.7280 (0.7280)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [154][39/195]\tTime 0.003 (0.005)\tLoss 0.8506 (0.8263)\tPrec@1 72.266 (70.771)\n",
      "Epoch: [154][78/195]\tTime 0.005 (0.005)\tLoss 0.9225 (0.8426)\tPrec@1 69.141 (70.273)\n",
      "Epoch: [154][117/195]\tTime 0.007 (0.005)\tLoss 0.9871 (0.8626)\tPrec@1 64.844 (69.498)\n",
      "Epoch: [154][156/195]\tTime 0.005 (0.005)\tLoss 0.9342 (0.8812)\tPrec@1 64.453 (68.842)\n",
      "Epoch: [154][195/195]\tTime 0.002 (0.005)\tLoss 1.0110 (0.8972)\tPrec@1 61.250 (68.214)\n",
      "EPOCH: 154 train Results: Prec@1 68.214 Loss: 0.8972\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2544 (1.2544)\tPrec@1 57.812 (57.812)\n",
      "Test: [39/39]\tTime 0.001 (0.002)\tLoss 1.3468 (1.2667)\tPrec@1 50.000 (56.400)\n",
      "EPOCH: 154 val Results: Prec@1 56.400 Loss: 1.2667\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/195]\tTime 0.009 (0.009)\tLoss 0.8290 (0.8290)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [155][39/195]\tTime 0.010 (0.006)\tLoss 0.8653 (0.8187)\tPrec@1 70.703 (70.840)\n",
      "Epoch: [155][78/195]\tTime 0.006 (0.006)\tLoss 0.8331 (0.8344)\tPrec@1 73.047 (70.184)\n",
      "Epoch: [155][117/195]\tTime 0.013 (0.006)\tLoss 1.0368 (0.8590)\tPrec@1 62.891 (69.339)\n",
      "Epoch: [155][156/195]\tTime 0.009 (0.006)\tLoss 0.9721 (0.8790)\tPrec@1 62.500 (68.675)\n",
      "Epoch: [155][195/195]\tTime 0.004 (0.007)\tLoss 0.7776 (0.8924)\tPrec@1 71.250 (68.156)\n",
      "EPOCH: 155 train Results: Prec@1 68.156 Loss: 0.8924\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2112 (1.2112)\tPrec@1 59.766 (59.766)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5131 (1.2765)\tPrec@1 37.500 (55.750)\n",
      "EPOCH: 155 val Results: Prec@1 55.750 Loss: 1.2765\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/195]\tTime 0.007 (0.007)\tLoss 0.8110 (0.8110)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [156][39/195]\tTime 0.003 (0.006)\tLoss 0.7638 (0.8152)\tPrec@1 73.438 (70.586)\n",
      "Epoch: [156][78/195]\tTime 0.004 (0.006)\tLoss 0.9556 (0.8403)\tPrec@1 65.625 (70.164)\n",
      "Epoch: [156][117/195]\tTime 0.005 (0.006)\tLoss 0.8049 (0.8634)\tPrec@1 69.531 (69.210)\n",
      "Epoch: [156][156/195]\tTime 0.004 (0.006)\tLoss 1.0248 (0.8820)\tPrec@1 64.062 (68.558)\n",
      "Epoch: [156][195/195]\tTime 0.002 (0.006)\tLoss 1.0997 (0.8928)\tPrec@1 58.750 (68.202)\n",
      "EPOCH: 156 train Results: Prec@1 68.202 Loss: 0.8928\n",
      "Test: [0/39]\tTime 0.005 (0.005)\tLoss 1.2666 (1.2666)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.6580 (1.2694)\tPrec@1 37.500 (56.010)\n",
      "EPOCH: 156 val Results: Prec@1 56.010 Loss: 1.2694\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/195]\tTime 0.007 (0.007)\tLoss 0.8861 (0.8861)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [157][39/195]\tTime 0.004 (0.006)\tLoss 0.9347 (0.8190)\tPrec@1 64.062 (71.133)\n",
      "Epoch: [157][78/195]\tTime 0.004 (0.005)\tLoss 0.8450 (0.8434)\tPrec@1 69.922 (70.238)\n",
      "Epoch: [157][117/195]\tTime 0.006 (0.006)\tLoss 0.8457 (0.8668)\tPrec@1 71.094 (69.260)\n",
      "Epoch: [157][156/195]\tTime 0.006 (0.006)\tLoss 0.9187 (0.8818)\tPrec@1 66.016 (68.636)\n",
      "Epoch: [157][195/195]\tTime 0.006 (0.006)\tLoss 0.8193 (0.8911)\tPrec@1 67.500 (68.310)\n",
      "EPOCH: 157 train Results: Prec@1 68.310 Loss: 0.8911\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2646 (1.2646)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3345 (1.2750)\tPrec@1 37.500 (55.790)\n",
      "EPOCH: 157 val Results: Prec@1 55.790 Loss: 1.2750\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/195]\tTime 0.004 (0.004)\tLoss 0.7977 (0.7977)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [158][39/195]\tTime 0.006 (0.006)\tLoss 0.9203 (0.8328)\tPrec@1 68.359 (70.459)\n",
      "Epoch: [158][78/195]\tTime 0.004 (0.006)\tLoss 0.9070 (0.8463)\tPrec@1 66.016 (69.838)\n",
      "Epoch: [158][117/195]\tTime 0.005 (0.006)\tLoss 1.0099 (0.8664)\tPrec@1 58.203 (68.902)\n",
      "Epoch: [158][156/195]\tTime 0.010 (0.006)\tLoss 0.9945 (0.8798)\tPrec@1 66.406 (68.429)\n",
      "Epoch: [158][195/195]\tTime 0.002 (0.006)\tLoss 1.0011 (0.8897)\tPrec@1 66.250 (68.182)\n",
      "EPOCH: 158 train Results: Prec@1 68.182 Loss: 0.8897\n",
      "Test: [0/39]\tTime 0.008 (0.008)\tLoss 1.1908 (1.1908)\tPrec@1 58.594 (58.594)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3589 (1.2746)\tPrec@1 56.250 (56.180)\n",
      "EPOCH: 158 val Results: Prec@1 56.180 Loss: 1.2746\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/195]\tTime 0.004 (0.004)\tLoss 0.7536 (0.7536)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [159][39/195]\tTime 0.004 (0.007)\tLoss 0.8086 (0.8200)\tPrec@1 67.969 (71.035)\n",
      "Epoch: [159][78/195]\tTime 0.003 (0.007)\tLoss 0.8268 (0.8451)\tPrec@1 68.359 (70.446)\n",
      "Epoch: [159][117/195]\tTime 0.004 (0.007)\tLoss 0.9564 (0.8609)\tPrec@1 66.016 (69.760)\n",
      "Epoch: [159][156/195]\tTime 0.003 (0.007)\tLoss 0.8837 (0.8765)\tPrec@1 68.359 (69.054)\n",
      "Epoch: [159][195/195]\tTime 0.004 (0.007)\tLoss 0.9787 (0.8902)\tPrec@1 63.750 (68.458)\n",
      "EPOCH: 159 train Results: Prec@1 68.458 Loss: 0.8902\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2325 (1.2325)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.7770 (1.2800)\tPrec@1 31.250 (55.900)\n",
      "EPOCH: 159 val Results: Prec@1 55.900 Loss: 1.2800\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/195]\tTime 0.004 (0.004)\tLoss 0.7832 (0.7832)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [160][39/195]\tTime 0.005 (0.006)\tLoss 0.8560 (0.8456)\tPrec@1 67.969 (69.990)\n",
      "Epoch: [160][78/195]\tTime 0.005 (0.006)\tLoss 0.8246 (0.8505)\tPrec@1 72.656 (69.719)\n",
      "Epoch: [160][117/195]\tTime 0.006 (0.006)\tLoss 0.8580 (0.8701)\tPrec@1 71.094 (68.968)\n",
      "Epoch: [160][156/195]\tTime 0.006 (0.006)\tLoss 0.8641 (0.8816)\tPrec@1 70.312 (68.578)\n",
      "Epoch: [160][195/195]\tTime 0.006 (0.006)\tLoss 0.9394 (0.8893)\tPrec@1 70.000 (68.262)\n",
      "EPOCH: 160 train Results: Prec@1 68.262 Loss: 0.8893\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1982 (1.1982)\tPrec@1 58.984 (58.984)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3345 (1.2749)\tPrec@1 43.750 (56.130)\n",
      "EPOCH: 160 val Results: Prec@1 56.130 Loss: 1.2749\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/195]\tTime 0.006 (0.006)\tLoss 0.8335 (0.8335)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [161][39/195]\tTime 0.004 (0.007)\tLoss 0.7546 (0.8195)\tPrec@1 74.609 (71.133)\n",
      "Epoch: [161][78/195]\tTime 0.010 (0.006)\tLoss 0.8812 (0.8486)\tPrec@1 67.969 (69.897)\n",
      "Epoch: [161][117/195]\tTime 0.009 (0.006)\tLoss 0.9290 (0.8679)\tPrec@1 66.797 (69.339)\n",
      "Epoch: [161][156/195]\tTime 0.004 (0.007)\tLoss 0.9124 (0.8798)\tPrec@1 67.969 (68.862)\n",
      "Epoch: [161][195/195]\tTime 0.003 (0.006)\tLoss 1.2808 (0.8943)\tPrec@1 61.250 (68.286)\n",
      "EPOCH: 161 train Results: Prec@1 68.286 Loss: 0.8943\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2090 (1.2090)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.1968 (1.2702)\tPrec@1 50.000 (56.240)\n",
      "EPOCH: 161 val Results: Prec@1 56.240 Loss: 1.2702\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/195]\tTime 0.009 (0.009)\tLoss 0.7256 (0.7256)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [162][39/195]\tTime 0.005 (0.006)\tLoss 0.8639 (0.8142)\tPrec@1 73.828 (71.641)\n",
      "Epoch: [162][78/195]\tTime 0.009 (0.006)\tLoss 0.8086 (0.8435)\tPrec@1 73.047 (70.105)\n",
      "Epoch: [162][117/195]\tTime 0.010 (0.007)\tLoss 0.9361 (0.8655)\tPrec@1 67.969 (69.392)\n",
      "Epoch: [162][156/195]\tTime 0.009 (0.007)\tLoss 0.8844 (0.8778)\tPrec@1 71.484 (68.802)\n",
      "Epoch: [162][195/195]\tTime 0.001 (0.006)\tLoss 1.1530 (0.8917)\tPrec@1 63.750 (68.188)\n",
      "EPOCH: 162 train Results: Prec@1 68.188 Loss: 0.8917\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2040 (1.2040)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.7115 (1.2766)\tPrec@1 31.250 (56.110)\n",
      "EPOCH: 162 val Results: Prec@1 56.110 Loss: 1.2766\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/195]\tTime 0.004 (0.004)\tLoss 0.8749 (0.8749)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [163][39/195]\tTime 0.003 (0.006)\tLoss 0.8286 (0.8236)\tPrec@1 72.656 (71.465)\n",
      "Epoch: [163][78/195]\tTime 0.007 (0.006)\tLoss 0.8679 (0.8358)\tPrec@1 68.359 (70.713)\n",
      "Epoch: [163][117/195]\tTime 0.004 (0.006)\tLoss 0.9827 (0.8606)\tPrec@1 63.281 (69.700)\n",
      "Epoch: [163][156/195]\tTime 0.007 (0.006)\tLoss 0.8719 (0.8765)\tPrec@1 70.312 (69.096)\n",
      "Epoch: [163][195/195]\tTime 0.002 (0.006)\tLoss 0.9434 (0.8887)\tPrec@1 63.750 (68.532)\n",
      "EPOCH: 163 train Results: Prec@1 68.532 Loss: 0.8887\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2128 (1.2128)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2395 (1.2661)\tPrec@1 43.750 (56.290)\n",
      "EPOCH: 163 val Results: Prec@1 56.290 Loss: 1.2661\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/195]\tTime 0.008 (0.008)\tLoss 0.8268 (0.8268)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [164][39/195]\tTime 0.004 (0.006)\tLoss 0.7925 (0.8106)\tPrec@1 71.875 (71.494)\n",
      "Epoch: [164][78/195]\tTime 0.004 (0.006)\tLoss 0.9149 (0.8362)\tPrec@1 67.969 (70.560)\n",
      "Epoch: [164][117/195]\tTime 0.005 (0.006)\tLoss 0.8796 (0.8610)\tPrec@1 67.969 (69.429)\n",
      "Epoch: [164][156/195]\tTime 0.004 (0.006)\tLoss 1.0169 (0.8782)\tPrec@1 64.062 (68.713)\n",
      "Epoch: [164][195/195]\tTime 0.003 (0.005)\tLoss 0.8926 (0.8903)\tPrec@1 65.000 (68.214)\n",
      "EPOCH: 164 train Results: Prec@1 68.214 Loss: 0.8903\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2862 (1.2862)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3662 (1.2883)\tPrec@1 50.000 (55.440)\n",
      "EPOCH: 164 val Results: Prec@1 55.440 Loss: 1.2883\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/195]\tTime 0.012 (0.012)\tLoss 0.9098 (0.9098)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [165][39/195]\tTime 0.007 (0.006)\tLoss 0.8312 (0.8060)\tPrec@1 69.141 (71.758)\n",
      "Epoch: [165][78/195]\tTime 0.006 (0.007)\tLoss 0.8229 (0.8363)\tPrec@1 68.750 (70.456)\n",
      "Epoch: [165][117/195]\tTime 0.005 (0.007)\tLoss 1.0037 (0.8544)\tPrec@1 65.234 (69.776)\n",
      "Epoch: [165][156/195]\tTime 0.007 (0.007)\tLoss 0.8988 (0.8709)\tPrec@1 70.703 (69.170)\n",
      "Epoch: [165][195/195]\tTime 0.002 (0.007)\tLoss 1.0849 (0.8863)\tPrec@1 56.250 (68.628)\n",
      "EPOCH: 165 train Results: Prec@1 68.628 Loss: 0.8863\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2141 (1.2141)\tPrec@1 58.984 (58.984)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3801 (1.2760)\tPrec@1 43.750 (55.820)\n",
      "EPOCH: 165 val Results: Prec@1 55.820 Loss: 1.2760\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/195]\tTime 0.008 (0.008)\tLoss 0.8040 (0.8040)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [166][39/195]\tTime 0.004 (0.006)\tLoss 0.9097 (0.8254)\tPrec@1 68.750 (71.074)\n",
      "Epoch: [166][78/195]\tTime 0.007 (0.007)\tLoss 0.8658 (0.8413)\tPrec@1 71.484 (70.342)\n",
      "Epoch: [166][117/195]\tTime 0.006 (0.007)\tLoss 0.9620 (0.8599)\tPrec@1 67.188 (69.687)\n",
      "Epoch: [166][156/195]\tTime 0.007 (0.007)\tLoss 0.9278 (0.8743)\tPrec@1 66.797 (69.056)\n",
      "Epoch: [166][195/195]\tTime 0.003 (0.006)\tLoss 0.8784 (0.8901)\tPrec@1 72.500 (68.488)\n",
      "EPOCH: 166 train Results: Prec@1 68.488 Loss: 0.8901\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2311 (1.2311)\tPrec@1 61.719 (61.719)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3155 (1.2811)\tPrec@1 37.500 (55.590)\n",
      "EPOCH: 166 val Results: Prec@1 55.590 Loss: 1.2811\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/195]\tTime 0.010 (0.010)\tLoss 0.8096 (0.8096)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [167][39/195]\tTime 0.007 (0.006)\tLoss 0.8341 (0.8179)\tPrec@1 68.359 (70.752)\n",
      "Epoch: [167][78/195]\tTime 0.007 (0.006)\tLoss 0.8446 (0.8342)\tPrec@1 71.484 (70.268)\n",
      "Epoch: [167][117/195]\tTime 0.004 (0.006)\tLoss 0.8709 (0.8569)\tPrec@1 66.016 (69.396)\n",
      "Epoch: [167][156/195]\tTime 0.010 (0.006)\tLoss 0.8483 (0.8725)\tPrec@1 68.750 (68.760)\n",
      "Epoch: [167][195/195]\tTime 0.003 (0.006)\tLoss 0.9569 (0.8890)\tPrec@1 65.000 (68.254)\n",
      "EPOCH: 167 train Results: Prec@1 68.254 Loss: 0.8890\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2552 (1.2552)\tPrec@1 57.812 (57.812)\n",
      "Test: [39/39]\tTime 0.000 (0.004)\tLoss 1.2075 (1.2717)\tPrec@1 31.250 (55.860)\n",
      "EPOCH: 167 val Results: Prec@1 55.860 Loss: 1.2717\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/195]\tTime 0.006 (0.006)\tLoss 0.7732 (0.7732)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [168][39/195]\tTime 0.004 (0.005)\tLoss 0.8034 (0.8296)\tPrec@1 71.094 (70.391)\n",
      "Epoch: [168][78/195]\tTime 0.005 (0.005)\tLoss 0.9019 (0.8411)\tPrec@1 67.969 (70.080)\n",
      "Epoch: [168][117/195]\tTime 0.004 (0.006)\tLoss 0.9467 (0.8575)\tPrec@1 68.750 (69.485)\n",
      "Epoch: [168][156/195]\tTime 0.005 (0.006)\tLoss 0.9279 (0.8771)\tPrec@1 67.969 (68.668)\n",
      "Epoch: [168][195/195]\tTime 0.002 (0.006)\tLoss 0.8962 (0.8894)\tPrec@1 67.500 (68.212)\n",
      "EPOCH: 168 train Results: Prec@1 68.212 Loss: 0.8894\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2223 (1.2223)\tPrec@1 59.766 (59.766)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.6613 (1.2700)\tPrec@1 25.000 (55.970)\n",
      "EPOCH: 168 val Results: Prec@1 55.970 Loss: 1.2700\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/195]\tTime 0.005 (0.005)\tLoss 0.8387 (0.8387)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [169][39/195]\tTime 0.007 (0.006)\tLoss 0.8542 (0.8283)\tPrec@1 68.750 (70.752)\n",
      "Epoch: [169][78/195]\tTime 0.010 (0.007)\tLoss 0.8746 (0.8469)\tPrec@1 68.359 (69.932)\n",
      "Epoch: [169][117/195]\tTime 0.007 (0.009)\tLoss 0.9272 (0.8622)\tPrec@1 65.234 (69.448)\n",
      "Epoch: [169][156/195]\tTime 0.004 (0.008)\tLoss 0.8536 (0.8756)\tPrec@1 69.922 (68.904)\n",
      "Epoch: [169][195/195]\tTime 0.002 (0.008)\tLoss 0.9116 (0.8914)\tPrec@1 65.000 (68.246)\n",
      "EPOCH: 169 train Results: Prec@1 68.246 Loss: 0.8914\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2024 (1.2024)\tPrec@1 59.766 (59.766)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.1538 (1.2879)\tPrec@1 56.250 (55.540)\n",
      "EPOCH: 169 val Results: Prec@1 55.540 Loss: 1.2879\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/195]\tTime 0.006 (0.006)\tLoss 0.7753 (0.7753)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [170][39/195]\tTime 0.004 (0.006)\tLoss 0.8781 (0.8193)\tPrec@1 68.750 (70.820)\n",
      "Epoch: [170][78/195]\tTime 0.004 (0.005)\tLoss 0.8673 (0.8407)\tPrec@1 71.094 (70.268)\n",
      "Epoch: [170][117/195]\tTime 0.007 (0.005)\tLoss 0.9247 (0.8598)\tPrec@1 71.094 (69.551)\n",
      "Epoch: [170][156/195]\tTime 0.004 (0.007)\tLoss 0.9270 (0.8812)\tPrec@1 63.672 (68.693)\n",
      "Epoch: [170][195/195]\tTime 0.004 (0.007)\tLoss 0.9570 (0.8932)\tPrec@1 63.750 (68.340)\n",
      "EPOCH: 170 train Results: Prec@1 68.340 Loss: 0.8932\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.3315 (1.3315)\tPrec@1 54.297 (54.297)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.0290 (1.2725)\tPrec@1 50.000 (55.570)\n",
      "EPOCH: 170 val Results: Prec@1 55.570 Loss: 1.2725\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/195]\tTime 0.006 (0.006)\tLoss 0.7628 (0.7628)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [171][39/195]\tTime 0.006 (0.006)\tLoss 0.8148 (0.8055)\tPrec@1 69.531 (71.943)\n",
      "Epoch: [171][78/195]\tTime 0.011 (0.006)\tLoss 0.8725 (0.8383)\tPrec@1 64.844 (70.337)\n",
      "Epoch: [171][117/195]\tTime 0.006 (0.006)\tLoss 0.8760 (0.8534)\tPrec@1 67.578 (69.736)\n",
      "Epoch: [171][156/195]\tTime 0.004 (0.006)\tLoss 0.9047 (0.8684)\tPrec@1 67.969 (69.215)\n",
      "Epoch: [171][195/195]\tTime 0.003 (0.006)\tLoss 1.1367 (0.8838)\tPrec@1 60.000 (68.472)\n",
      "EPOCH: 171 train Results: Prec@1 68.472 Loss: 0.8838\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2689 (1.2689)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4846 (1.2874)\tPrec@1 50.000 (55.650)\n",
      "EPOCH: 171 val Results: Prec@1 55.650 Loss: 1.2874\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/195]\tTime 0.011 (0.011)\tLoss 0.7276 (0.7276)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [172][39/195]\tTime 0.005 (0.006)\tLoss 0.7936 (0.8211)\tPrec@1 70.312 (70.645)\n",
      "Epoch: [172][78/195]\tTime 0.008 (0.006)\tLoss 0.8636 (0.8392)\tPrec@1 68.359 (69.981)\n",
      "Epoch: [172][117/195]\tTime 0.005 (0.006)\tLoss 0.8897 (0.8602)\tPrec@1 70.312 (69.290)\n",
      "Epoch: [172][156/195]\tTime 0.017 (0.006)\tLoss 0.8916 (0.8779)\tPrec@1 66.406 (68.628)\n",
      "Epoch: [172][195/195]\tTime 0.002 (0.006)\tLoss 1.0231 (0.8911)\tPrec@1 66.250 (68.138)\n",
      "EPOCH: 172 train Results: Prec@1 68.138 Loss: 0.8911\n",
      "Test: [0/39]\tTime 0.005 (0.005)\tLoss 1.2351 (1.2351)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4852 (1.2758)\tPrec@1 56.250 (55.280)\n",
      "EPOCH: 172 val Results: Prec@1 55.280 Loss: 1.2758\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/195]\tTime 0.006 (0.006)\tLoss 0.9284 (0.9284)\tPrec@1 64.453 (64.453)\n",
      "Epoch: [173][39/195]\tTime 0.004 (0.006)\tLoss 0.8661 (0.8344)\tPrec@1 69.922 (70.293)\n",
      "Epoch: [173][78/195]\tTime 0.086 (0.007)\tLoss 0.9447 (0.8524)\tPrec@1 66.797 (69.853)\n",
      "Epoch: [173][117/195]\tTime 0.004 (0.006)\tLoss 0.9313 (0.8585)\tPrec@1 67.578 (69.581)\n",
      "Epoch: [173][156/195]\tTime 0.004 (0.006)\tLoss 1.0099 (0.8747)\tPrec@1 63.281 (68.991)\n",
      "Epoch: [173][195/195]\tTime 0.003 (0.006)\tLoss 0.9827 (0.8888)\tPrec@1 70.000 (68.450)\n",
      "EPOCH: 173 train Results: Prec@1 68.450 Loss: 0.8888\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2268 (1.2268)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.2248 (1.2853)\tPrec@1 50.000 (56.160)\n",
      "EPOCH: 173 val Results: Prec@1 56.160 Loss: 1.2853\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/195]\tTime 0.005 (0.005)\tLoss 0.7772 (0.7772)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [174][39/195]\tTime 0.005 (0.006)\tLoss 0.8008 (0.8144)\tPrec@1 70.312 (71.025)\n",
      "Epoch: [174][78/195]\tTime 0.005 (0.005)\tLoss 0.8110 (0.8387)\tPrec@1 72.656 (70.243)\n",
      "Epoch: [174][117/195]\tTime 0.005 (0.005)\tLoss 0.9357 (0.8541)\tPrec@1 64.844 (69.720)\n",
      "Epoch: [174][156/195]\tTime 0.003 (0.005)\tLoss 0.9144 (0.8693)\tPrec@1 69.922 (69.141)\n",
      "Epoch: [174][195/195]\tTime 0.003 (0.006)\tLoss 0.9622 (0.8870)\tPrec@1 66.250 (68.468)\n",
      "EPOCH: 174 train Results: Prec@1 68.468 Loss: 0.8870\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2105 (1.2105)\tPrec@1 54.297 (54.297)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.2795 (1.2787)\tPrec@1 43.750 (55.660)\n",
      "EPOCH: 174 val Results: Prec@1 55.660 Loss: 1.2787\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/195]\tTime 0.012 (0.012)\tLoss 0.7637 (0.7637)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [175][39/195]\tTime 0.007 (0.006)\tLoss 0.8255 (0.8259)\tPrec@1 72.266 (70.596)\n",
      "Epoch: [175][78/195]\tTime 0.004 (0.006)\tLoss 0.8065 (0.8346)\tPrec@1 71.094 (70.209)\n",
      "Epoch: [175][117/195]\tTime 0.010 (0.006)\tLoss 0.9520 (0.8562)\tPrec@1 67.188 (69.541)\n",
      "Epoch: [175][156/195]\tTime 0.004 (0.006)\tLoss 0.9374 (0.8709)\tPrec@1 63.672 (69.073)\n",
      "Epoch: [175][195/195]\tTime 0.002 (0.006)\tLoss 0.9283 (0.8891)\tPrec@1 72.500 (68.426)\n",
      "EPOCH: 175 train Results: Prec@1 68.426 Loss: 0.8891\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2418 (1.2418)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.3153 (1.2799)\tPrec@1 43.750 (55.310)\n",
      "EPOCH: 175 val Results: Prec@1 55.310 Loss: 1.2799\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/195]\tTime 0.007 (0.007)\tLoss 0.8787 (0.8787)\tPrec@1 69.922 (69.922)\n",
      "Epoch: [176][39/195]\tTime 0.005 (0.006)\tLoss 0.8886 (0.8334)\tPrec@1 67.578 (70.010)\n",
      "Epoch: [176][78/195]\tTime 0.007 (0.005)\tLoss 0.8116 (0.8461)\tPrec@1 72.266 (69.778)\n",
      "Epoch: [176][117/195]\tTime 0.009 (0.006)\tLoss 0.8663 (0.8597)\tPrec@1 70.703 (69.369)\n",
      "Epoch: [176][156/195]\tTime 0.005 (0.006)\tLoss 0.9377 (0.8752)\tPrec@1 65.625 (68.757)\n",
      "Epoch: [176][195/195]\tTime 0.003 (0.006)\tLoss 0.9250 (0.8866)\tPrec@1 61.250 (68.456)\n",
      "EPOCH: 176 train Results: Prec@1 68.456 Loss: 0.8866\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2256 (1.2256)\tPrec@1 60.547 (60.547)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5229 (1.2818)\tPrec@1 31.250 (55.560)\n",
      "EPOCH: 176 val Results: Prec@1 55.560 Loss: 1.2818\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/195]\tTime 0.004 (0.004)\tLoss 0.8356 (0.8356)\tPrec@1 69.922 (69.922)\n",
      "Epoch: [177][39/195]\tTime 0.004 (0.006)\tLoss 0.7010 (0.8024)\tPrec@1 76.953 (72.197)\n",
      "Epoch: [177][78/195]\tTime 0.006 (0.006)\tLoss 0.9773 (0.8358)\tPrec@1 62.891 (70.575)\n",
      "Epoch: [177][117/195]\tTime 0.006 (0.006)\tLoss 0.9620 (0.8522)\tPrec@1 65.234 (69.902)\n",
      "Epoch: [177][156/195]\tTime 0.006 (0.006)\tLoss 0.9000 (0.8703)\tPrec@1 66.406 (69.098)\n",
      "Epoch: [177][195/195]\tTime 0.007 (0.006)\tLoss 0.8722 (0.8817)\tPrec@1 71.250 (68.650)\n",
      "EPOCH: 177 train Results: Prec@1 68.650 Loss: 0.8817\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2813 (1.2813)\tPrec@1 57.812 (57.812)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5619 (1.2833)\tPrec@1 43.750 (55.560)\n",
      "EPOCH: 177 val Results: Prec@1 55.560 Loss: 1.2833\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/195]\tTime 0.006 (0.006)\tLoss 0.8933 (0.8933)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [178][39/195]\tTime 0.004 (0.006)\tLoss 0.8245 (0.8166)\tPrec@1 67.969 (70.938)\n",
      "Epoch: [178][78/195]\tTime 0.005 (0.005)\tLoss 0.9133 (0.8483)\tPrec@1 65.234 (69.803)\n",
      "Epoch: [178][117/195]\tTime 0.007 (0.005)\tLoss 0.9870 (0.8675)\tPrec@1 63.672 (69.108)\n",
      "Epoch: [178][156/195]\tTime 0.012 (0.006)\tLoss 0.9021 (0.8793)\tPrec@1 65.625 (68.613)\n",
      "Epoch: [178][195/195]\tTime 0.003 (0.006)\tLoss 1.0288 (0.8903)\tPrec@1 62.500 (68.208)\n",
      "EPOCH: 178 train Results: Prec@1 68.208 Loss: 0.8903\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2791 (1.2791)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.003)\tLoss 1.2615 (1.2857)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 178 val Results: Prec@1 55.300 Loss: 1.2857\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/195]\tTime 0.004 (0.004)\tLoss 0.8503 (0.8503)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [179][39/195]\tTime 0.008 (0.006)\tLoss 0.8170 (0.8160)\tPrec@1 71.094 (71.680)\n",
      "Epoch: [179][78/195]\tTime 0.004 (0.006)\tLoss 0.8892 (0.8375)\tPrec@1 71.875 (70.753)\n",
      "Epoch: [179][117/195]\tTime 0.003 (0.006)\tLoss 0.9868 (0.8651)\tPrec@1 65.625 (69.455)\n",
      "Epoch: [179][156/195]\tTime 0.004 (0.006)\tLoss 0.8683 (0.8791)\tPrec@1 68.750 (68.840)\n",
      "Epoch: [179][195/195]\tTime 0.003 (0.006)\tLoss 1.1616 (0.8902)\tPrec@1 63.750 (68.386)\n",
      "EPOCH: 179 train Results: Prec@1 68.386 Loss: 0.8902\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2859 (1.2859)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.001 (0.001)\tLoss 1.3257 (1.2883)\tPrec@1 43.750 (55.300)\n",
      "EPOCH: 179 val Results: Prec@1 55.300 Loss: 1.2883\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/195]\tTime 0.008 (0.008)\tLoss 0.8460 (0.8460)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [180][39/195]\tTime 0.005 (0.007)\tLoss 0.8150 (0.8399)\tPrec@1 73.047 (70.400)\n",
      "Epoch: [180][78/195]\tTime 0.007 (0.006)\tLoss 0.8217 (0.8458)\tPrec@1 71.875 (69.947)\n",
      "Epoch: [180][117/195]\tTime 0.003 (0.006)\tLoss 0.8831 (0.8620)\tPrec@1 67.969 (69.204)\n",
      "Epoch: [180][156/195]\tTime 0.009 (0.006)\tLoss 0.8492 (0.8758)\tPrec@1 70.312 (68.683)\n",
      "Epoch: [180][195/195]\tTime 0.002 (0.006)\tLoss 1.1684 (0.8882)\tPrec@1 57.500 (68.224)\n",
      "EPOCH: 180 train Results: Prec@1 68.224 Loss: 0.8882\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1784 (1.1784)\tPrec@1 57.031 (57.031)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.4789 (1.2807)\tPrec@1 43.750 (55.270)\n",
      "EPOCH: 180 val Results: Prec@1 55.270 Loss: 1.2807\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/195]\tTime 0.004 (0.004)\tLoss 0.7822 (0.7822)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [181][39/195]\tTime 0.004 (0.006)\tLoss 0.8542 (0.8140)\tPrec@1 69.922 (71.699)\n",
      "Epoch: [181][78/195]\tTime 0.004 (0.005)\tLoss 0.9528 (0.8386)\tPrec@1 66.797 (70.461)\n",
      "Epoch: [181][117/195]\tTime 0.007 (0.005)\tLoss 0.8371 (0.8659)\tPrec@1 71.875 (69.488)\n",
      "Epoch: [181][156/195]\tTime 0.004 (0.005)\tLoss 0.9111 (0.8779)\tPrec@1 66.406 (69.111)\n",
      "Epoch: [181][195/195]\tTime 0.002 (0.005)\tLoss 1.2321 (0.8896)\tPrec@1 58.750 (68.570)\n",
      "EPOCH: 181 train Results: Prec@1 68.570 Loss: 0.8896\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1795 (1.1795)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5730 (1.2871)\tPrec@1 31.250 (55.790)\n",
      "EPOCH: 181 val Results: Prec@1 55.790 Loss: 1.2871\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/195]\tTime 0.008 (0.008)\tLoss 0.7519 (0.7519)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [182][39/195]\tTime 0.008 (0.007)\tLoss 0.9080 (0.8216)\tPrec@1 67.969 (70.703)\n",
      "Epoch: [182][78/195]\tTime 0.007 (0.007)\tLoss 0.9137 (0.8387)\tPrec@1 64.844 (69.719)\n",
      "Epoch: [182][117/195]\tTime 0.004 (0.007)\tLoss 0.9057 (0.8554)\tPrec@1 68.750 (69.425)\n",
      "Epoch: [182][156/195]\tTime 0.005 (0.006)\tLoss 0.9505 (0.8700)\tPrec@1 64.062 (68.917)\n",
      "Epoch: [182][195/195]\tTime 0.005 (0.006)\tLoss 1.1700 (0.8805)\tPrec@1 53.750 (68.480)\n",
      "EPOCH: 182 train Results: Prec@1 68.480 Loss: 0.8805\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2305 (1.2305)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5190 (1.2960)\tPrec@1 31.250 (55.820)\n",
      "EPOCH: 182 val Results: Prec@1 55.820 Loss: 1.2960\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/195]\tTime 0.007 (0.007)\tLoss 0.8465 (0.8465)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [183][39/195]\tTime 0.005 (0.006)\tLoss 0.7945 (0.8152)\tPrec@1 73.047 (71.084)\n",
      "Epoch: [183][78/195]\tTime 0.010 (0.008)\tLoss 0.8239 (0.8317)\tPrec@1 73.438 (70.589)\n",
      "Epoch: [183][117/195]\tTime 0.003 (0.008)\tLoss 0.9082 (0.8537)\tPrec@1 67.578 (69.627)\n",
      "Epoch: [183][156/195]\tTime 0.009 (0.007)\tLoss 0.9762 (0.8707)\tPrec@1 64.062 (69.054)\n",
      "Epoch: [183][195/195]\tTime 0.004 (0.007)\tLoss 1.0867 (0.8850)\tPrec@1 61.250 (68.438)\n",
      "EPOCH: 183 train Results: Prec@1 68.438 Loss: 0.8850\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.1874 (1.1874)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.9515 (1.2882)\tPrec@1 25.000 (55.350)\n",
      "EPOCH: 183 val Results: Prec@1 55.350 Loss: 1.2882\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/195]\tTime 0.005 (0.005)\tLoss 0.7425 (0.7425)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [184][39/195]\tTime 0.007 (0.006)\tLoss 1.0068 (0.8236)\tPrec@1 62.109 (71.055)\n",
      "Epoch: [184][78/195]\tTime 0.004 (0.006)\tLoss 0.9163 (0.8485)\tPrec@1 67.188 (69.813)\n",
      "Epoch: [184][117/195]\tTime 0.006 (0.006)\tLoss 1.0125 (0.8640)\tPrec@1 63.672 (69.465)\n",
      "Epoch: [184][156/195]\tTime 0.004 (0.005)\tLoss 0.8781 (0.8800)\tPrec@1 71.484 (68.725)\n",
      "Epoch: [184][195/195]\tTime 0.003 (0.005)\tLoss 1.2810 (0.8894)\tPrec@1 56.250 (68.374)\n",
      "EPOCH: 184 train Results: Prec@1 68.374 Loss: 0.8894\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2365 (1.2365)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5220 (1.2899)\tPrec@1 31.250 (55.540)\n",
      "EPOCH: 184 val Results: Prec@1 55.540 Loss: 1.2899\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/195]\tTime 0.007 (0.007)\tLoss 0.8410 (0.8410)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [185][39/195]\tTime 0.004 (0.006)\tLoss 0.9970 (0.8386)\tPrec@1 66.406 (70.303)\n",
      "Epoch: [185][78/195]\tTime 0.007 (0.006)\tLoss 0.7890 (0.8497)\tPrec@1 73.047 (69.783)\n",
      "Epoch: [185][117/195]\tTime 0.006 (0.006)\tLoss 0.9345 (0.8596)\tPrec@1 65.625 (69.554)\n",
      "Epoch: [185][156/195]\tTime 0.014 (0.006)\tLoss 0.8601 (0.8689)\tPrec@1 66.797 (69.044)\n",
      "Epoch: [185][195/195]\tTime 0.003 (0.006)\tLoss 0.9471 (0.8892)\tPrec@1 68.750 (68.278)\n",
      "EPOCH: 185 train Results: Prec@1 68.278 Loss: 0.8892\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2436 (1.2436)\tPrec@1 60.156 (60.156)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3574 (1.2932)\tPrec@1 37.500 (55.060)\n",
      "EPOCH: 185 val Results: Prec@1 55.060 Loss: 1.2932\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/195]\tTime 0.007 (0.007)\tLoss 0.8931 (0.8931)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [186][39/195]\tTime 0.003 (0.007)\tLoss 0.7813 (0.8181)\tPrec@1 71.875 (71.055)\n",
      "Epoch: [186][78/195]\tTime 0.003 (0.007)\tLoss 0.9181 (0.8384)\tPrec@1 68.750 (70.219)\n",
      "Epoch: [186][117/195]\tTime 0.004 (0.007)\tLoss 0.9326 (0.8591)\tPrec@1 66.406 (69.396)\n",
      "Epoch: [186][156/195]\tTime 0.005 (0.006)\tLoss 0.9068 (0.8760)\tPrec@1 73.047 (68.675)\n",
      "Epoch: [186][195/195]\tTime 0.002 (0.006)\tLoss 0.8320 (0.8854)\tPrec@1 67.500 (68.400)\n",
      "EPOCH: 186 train Results: Prec@1 68.400 Loss: 0.8854\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2273 (1.2273)\tPrec@1 62.500 (62.500)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.7880 (1.2820)\tPrec@1 25.000 (56.080)\n",
      "EPOCH: 186 val Results: Prec@1 56.080 Loss: 1.2820\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/195]\tTime 0.013 (0.013)\tLoss 0.8257 (0.8257)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [187][39/195]\tTime 0.004 (0.006)\tLoss 0.7799 (0.8214)\tPrec@1 71.875 (70.957)\n",
      "Epoch: [187][78/195]\tTime 0.006 (0.006)\tLoss 0.9571 (0.8318)\tPrec@1 64.844 (70.476)\n",
      "Epoch: [187][117/195]\tTime 0.014 (0.006)\tLoss 0.9718 (0.8494)\tPrec@1 66.797 (69.889)\n",
      "Epoch: [187][156/195]\tTime 0.003 (0.006)\tLoss 0.9360 (0.8676)\tPrec@1 68.750 (69.258)\n",
      "Epoch: [187][195/195]\tTime 0.002 (0.006)\tLoss 1.1201 (0.8853)\tPrec@1 62.500 (68.606)\n",
      "EPOCH: 187 train Results: Prec@1 68.606 Loss: 0.8853\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2316 (1.2316)\tPrec@1 54.297 (54.297)\n",
      "Test: [39/39]\tTime 0.001 (0.002)\tLoss 1.5899 (1.2788)\tPrec@1 37.500 (56.070)\n",
      "EPOCH: 187 val Results: Prec@1 56.070 Loss: 1.2788\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/195]\tTime 0.007 (0.007)\tLoss 0.8487 (0.8487)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [188][39/195]\tTime 0.008 (0.006)\tLoss 0.9595 (0.8122)\tPrec@1 64.844 (71.309)\n",
      "Epoch: [188][78/195]\tTime 0.023 (0.006)\tLoss 0.9005 (0.8347)\tPrec@1 68.750 (70.392)\n",
      "Epoch: [188][117/195]\tTime 0.009 (0.006)\tLoss 0.9126 (0.8541)\tPrec@1 66.797 (69.687)\n",
      "Epoch: [188][156/195]\tTime 0.005 (0.006)\tLoss 1.0429 (0.8686)\tPrec@1 64.453 (69.263)\n",
      "Epoch: [188][195/195]\tTime 0.002 (0.006)\tLoss 1.0495 (0.8841)\tPrec@1 58.750 (68.612)\n",
      "EPOCH: 188 train Results: Prec@1 68.612 Loss: 0.8841\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2267 (1.2267)\tPrec@1 56.250 (56.250)\n",
      "Test: [39/39]\tTime 0.000 (0.003)\tLoss 1.5884 (1.2823)\tPrec@1 37.500 (56.220)\n",
      "EPOCH: 188 val Results: Prec@1 56.220 Loss: 1.2823\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/195]\tTime 0.015 (0.015)\tLoss 0.8371 (0.8371)\tPrec@1 67.188 (67.188)\n",
      "Epoch: [189][39/195]\tTime 0.009 (0.013)\tLoss 0.8079 (0.8115)\tPrec@1 73.438 (71.738)\n",
      "Epoch: [189][78/195]\tTime 0.004 (0.010)\tLoss 0.9409 (0.8414)\tPrec@1 67.188 (70.451)\n",
      "Epoch: [189][117/195]\tTime 0.005 (0.009)\tLoss 1.0116 (0.8584)\tPrec@1 65.625 (69.684)\n",
      "Epoch: [189][156/195]\tTime 0.005 (0.009)\tLoss 0.9444 (0.8727)\tPrec@1 67.969 (69.041)\n",
      "Epoch: [189][195/195]\tTime 0.012 (0.011)\tLoss 1.1575 (0.8900)\tPrec@1 61.250 (68.430)\n",
      "EPOCH: 189 train Results: Prec@1 68.430 Loss: 0.8900\n",
      "Test: [0/39]\tTime 0.005 (0.005)\tLoss 1.1894 (1.1894)\tPrec@1 58.203 (58.203)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.6247 (1.2748)\tPrec@1 25.000 (55.680)\n",
      "EPOCH: 189 val Results: Prec@1 55.680 Loss: 1.2748\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/195]\tTime 0.030 (0.030)\tLoss 0.7165 (0.7165)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [190][39/195]\tTime 0.012 (0.008)\tLoss 0.8816 (0.8158)\tPrec@1 71.484 (71.289)\n",
      "Epoch: [190][78/195]\tTime 0.011 (0.011)\tLoss 0.9068 (0.8295)\tPrec@1 66.797 (70.327)\n",
      "Epoch: [190][117/195]\tTime 0.004 (0.010)\tLoss 0.8618 (0.8494)\tPrec@1 68.750 (69.468)\n",
      "Epoch: [190][156/195]\tTime 0.004 (0.010)\tLoss 0.8646 (0.8673)\tPrec@1 67.578 (68.830)\n",
      "Epoch: [190][195/195]\tTime 0.003 (0.009)\tLoss 1.1644 (0.8837)\tPrec@1 55.000 (68.262)\n",
      "EPOCH: 190 train Results: Prec@1 68.262 Loss: 0.8837\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2545 (1.2545)\tPrec@1 58.984 (58.984)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5905 (1.2755)\tPrec@1 37.500 (56.350)\n",
      "EPOCH: 190 val Results: Prec@1 56.350 Loss: 1.2755\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/195]\tTime 0.008 (0.008)\tLoss 0.8201 (0.8201)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [191][39/195]\tTime 0.003 (0.005)\tLoss 0.7009 (0.8126)\tPrec@1 75.000 (71.553)\n",
      "Epoch: [191][78/195]\tTime 0.010 (0.005)\tLoss 0.8115 (0.8328)\tPrec@1 73.047 (70.787)\n",
      "Epoch: [191][117/195]\tTime 0.005 (0.005)\tLoss 0.8055 (0.8537)\tPrec@1 70.312 (69.885)\n",
      "Epoch: [191][156/195]\tTime 0.012 (0.006)\tLoss 0.8603 (0.8682)\tPrec@1 67.578 (69.285)\n",
      "Epoch: [191][195/195]\tTime 0.002 (0.005)\tLoss 0.9610 (0.8850)\tPrec@1 68.750 (68.648)\n",
      "EPOCH: 191 train Results: Prec@1 68.648 Loss: 0.8850\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.1962 (1.1962)\tPrec@1 61.719 (61.719)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3515 (1.2728)\tPrec@1 37.500 (56.110)\n",
      "EPOCH: 191 val Results: Prec@1 56.110 Loss: 1.2728\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/195]\tTime 0.009 (0.009)\tLoss 0.6791 (0.6791)\tPrec@1 76.562 (76.562)\n",
      "Epoch: [192][39/195]\tTime 0.008 (0.005)\tLoss 0.8484 (0.8196)\tPrec@1 69.922 (70.596)\n",
      "Epoch: [192][78/195]\tTime 0.006 (0.006)\tLoss 0.8415 (0.8325)\tPrec@1 68.750 (70.421)\n",
      "Epoch: [192][117/195]\tTime 0.005 (0.006)\tLoss 0.8678 (0.8508)\tPrec@1 67.188 (69.660)\n",
      "Epoch: [192][156/195]\tTime 0.008 (0.006)\tLoss 0.9810 (0.8651)\tPrec@1 66.406 (69.098)\n",
      "Epoch: [192][195/195]\tTime 0.002 (0.005)\tLoss 0.8905 (0.8806)\tPrec@1 67.500 (68.542)\n",
      "EPOCH: 192 train Results: Prec@1 68.542 Loss: 0.8806\n",
      "Test: [0/39]\tTime 0.001 (0.001)\tLoss 1.2090 (1.2090)\tPrec@1 59.375 (59.375)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3102 (1.2812)\tPrec@1 56.250 (56.270)\n",
      "EPOCH: 192 val Results: Prec@1 56.270 Loss: 1.2812\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/195]\tTime 0.007 (0.007)\tLoss 0.7049 (0.7049)\tPrec@1 76.562 (76.562)\n",
      "Epoch: [193][39/195]\tTime 0.007 (0.005)\tLoss 0.8353 (0.8198)\tPrec@1 67.969 (70.645)\n",
      "Epoch: [193][78/195]\tTime 0.003 (0.006)\tLoss 0.9266 (0.8454)\tPrec@1 67.969 (69.793)\n",
      "Epoch: [193][117/195]\tTime 0.003 (0.006)\tLoss 0.7686 (0.8621)\tPrec@1 72.656 (69.200)\n",
      "Epoch: [193][156/195]\tTime 0.008 (0.006)\tLoss 1.0464 (0.8773)\tPrec@1 62.500 (68.668)\n",
      "Epoch: [193][195/195]\tTime 0.002 (0.007)\tLoss 1.0389 (0.8900)\tPrec@1 61.250 (68.354)\n",
      "EPOCH: 193 train Results: Prec@1 68.354 Loss: 0.8900\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2385 (1.2385)\tPrec@1 57.422 (57.422)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.5368 (1.2838)\tPrec@1 43.750 (55.420)\n",
      "EPOCH: 193 val Results: Prec@1 55.420 Loss: 1.2838\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/195]\tTime 0.005 (0.005)\tLoss 0.7800 (0.7800)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [194][39/195]\tTime 0.005 (0.006)\tLoss 0.7336 (0.8166)\tPrec@1 74.219 (71.475)\n",
      "Epoch: [194][78/195]\tTime 0.005 (0.006)\tLoss 0.8196 (0.8351)\tPrec@1 70.703 (70.352)\n",
      "Epoch: [194][117/195]\tTime 0.009 (0.006)\tLoss 0.9013 (0.8564)\tPrec@1 68.750 (69.366)\n",
      "Epoch: [194][156/195]\tTime 0.014 (0.006)\tLoss 0.8254 (0.8710)\tPrec@1 70.312 (68.718)\n",
      "Epoch: [194][195/195]\tTime 0.002 (0.007)\tLoss 1.0293 (0.8830)\tPrec@1 60.000 (68.264)\n",
      "EPOCH: 194 train Results: Prec@1 68.264 Loss: 0.8830\n",
      "Test: [0/39]\tTime 0.004 (0.004)\tLoss 1.2347 (1.2347)\tPrec@1 55.469 (55.469)\n",
      "Test: [39/39]\tTime 0.000 (0.003)\tLoss 1.2875 (1.2849)\tPrec@1 50.000 (55.860)\n",
      "EPOCH: 194 val Results: Prec@1 55.860 Loss: 1.2849\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/195]\tTime 0.008 (0.008)\tLoss 0.7083 (0.7083)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [195][39/195]\tTime 0.004 (0.008)\tLoss 0.8425 (0.8131)\tPrec@1 69.922 (71.299)\n",
      "Epoch: [195][78/195]\tTime 0.007 (0.007)\tLoss 0.8239 (0.8374)\tPrec@1 73.047 (70.115)\n",
      "Epoch: [195][117/195]\tTime 0.005 (0.007)\tLoss 0.9132 (0.8559)\tPrec@1 68.359 (69.617)\n",
      "Epoch: [195][156/195]\tTime 0.007 (0.008)\tLoss 1.0100 (0.8741)\tPrec@1 61.719 (68.872)\n",
      "Epoch: [195][195/195]\tTime 0.006 (0.008)\tLoss 0.9270 (0.8829)\tPrec@1 61.250 (68.590)\n",
      "EPOCH: 195 train Results: Prec@1 68.590 Loss: 0.8829\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.1829 (1.1829)\tPrec@1 58.984 (58.984)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.5302 (1.2888)\tPrec@1 50.000 (55.630)\n",
      "EPOCH: 195 val Results: Prec@1 55.630 Loss: 1.2888\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/195]\tTime 0.035 (0.035)\tLoss 0.7242 (0.7242)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [196][39/195]\tTime 0.008 (0.009)\tLoss 0.7665 (0.8273)\tPrec@1 72.656 (71.074)\n",
      "Epoch: [196][78/195]\tTime 0.003 (0.008)\tLoss 1.0327 (0.8457)\tPrec@1 61.328 (70.233)\n",
      "Epoch: [196][117/195]\tTime 0.004 (0.010)\tLoss 0.9716 (0.8683)\tPrec@1 62.500 (69.227)\n",
      "Epoch: [196][156/195]\tTime 0.004 (0.011)\tLoss 0.9780 (0.8790)\tPrec@1 66.406 (68.780)\n",
      "Epoch: [196][195/195]\tTime 0.004 (0.010)\tLoss 0.9110 (0.8889)\tPrec@1 62.500 (68.346)\n",
      "EPOCH: 196 train Results: Prec@1 68.346 Loss: 0.8889\n",
      "Test: [0/39]\tTime 0.007 (0.007)\tLoss 1.1076 (1.1076)\tPrec@1 61.328 (61.328)\n",
      "Test: [39/39]\tTime 0.000 (0.003)\tLoss 1.4476 (1.2740)\tPrec@1 37.500 (55.880)\n",
      "EPOCH: 196 val Results: Prec@1 55.880 Loss: 1.2740\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/195]\tTime 0.014 (0.014)\tLoss 0.7850 (0.7850)\tPrec@1 76.953 (76.953)\n",
      "Epoch: [197][39/195]\tTime 0.007 (0.007)\tLoss 0.9210 (0.8033)\tPrec@1 67.969 (71.592)\n",
      "Epoch: [197][78/195]\tTime 0.003 (0.006)\tLoss 0.8487 (0.8294)\tPrec@1 66.797 (70.673)\n",
      "Epoch: [197][117/195]\tTime 0.004 (0.006)\tLoss 0.7548 (0.8497)\tPrec@1 72.266 (69.998)\n",
      "Epoch: [197][156/195]\tTime 0.004 (0.010)\tLoss 0.9071 (0.8723)\tPrec@1 67.188 (69.034)\n",
      "Epoch: [197][195/195]\tTime 0.005 (0.009)\tLoss 0.9086 (0.8857)\tPrec@1 71.250 (68.502)\n",
      "EPOCH: 197 train Results: Prec@1 68.502 Loss: 0.8857\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2670 (1.2670)\tPrec@1 55.078 (55.078)\n",
      "Test: [39/39]\tTime 0.000 (0.002)\tLoss 1.1886 (1.2668)\tPrec@1 50.000 (56.120)\n",
      "EPOCH: 197 val Results: Prec@1 56.120 Loss: 1.2668\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/195]\tTime 0.015 (0.015)\tLoss 0.7682 (0.7682)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [198][39/195]\tTime 0.009 (0.007)\tLoss 0.7940 (0.8179)\tPrec@1 73.438 (70.879)\n",
      "Epoch: [198][78/195]\tTime 0.007 (0.007)\tLoss 0.9637 (0.8387)\tPrec@1 66.016 (70.149)\n",
      "Epoch: [198][117/195]\tTime 0.004 (0.007)\tLoss 0.8977 (0.8569)\tPrec@1 65.625 (69.402)\n",
      "Epoch: [198][156/195]\tTime 0.004 (0.006)\tLoss 0.8093 (0.8717)\tPrec@1 70.312 (68.842)\n",
      "Epoch: [198][195/195]\tTime 0.002 (0.006)\tLoss 0.8873 (0.8803)\tPrec@1 61.250 (68.584)\n",
      "EPOCH: 198 train Results: Prec@1 68.584 Loss: 0.8803\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2172 (1.2172)\tPrec@1 55.859 (55.859)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.4832 (1.2811)\tPrec@1 18.750 (56.150)\n",
      "EPOCH: 198 val Results: Prec@1 56.150 Loss: 1.2811\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/195]\tTime 0.004 (0.004)\tLoss 0.8795 (0.8795)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [199][39/195]\tTime 0.007 (0.006)\tLoss 0.8331 (0.8074)\tPrec@1 65.234 (71.289)\n",
      "Epoch: [199][78/195]\tTime 0.005 (0.006)\tLoss 0.8661 (0.8253)\tPrec@1 70.703 (70.723)\n",
      "Epoch: [199][117/195]\tTime 0.009 (0.006)\tLoss 0.9287 (0.8462)\tPrec@1 66.406 (69.713)\n",
      "Epoch: [199][156/195]\tTime 0.006 (0.006)\tLoss 0.9071 (0.8695)\tPrec@1 63.672 (68.882)\n",
      "Epoch: [199][195/195]\tTime 0.003 (0.006)\tLoss 0.8792 (0.8835)\tPrec@1 71.250 (68.340)\n",
      "EPOCH: 199 train Results: Prec@1 68.340 Loss: 0.8835\n",
      "Test: [0/39]\tTime 0.002 (0.002)\tLoss 1.2473 (1.2473)\tPrec@1 56.641 (56.641)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.7229 (1.2863)\tPrec@1 31.250 (55.740)\n",
      "EPOCH: 199 val Results: Prec@1 55.740 Loss: 1.2863\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/195]\tTime 0.012 (0.012)\tLoss 0.7944 (0.7944)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [200][39/195]\tTime 0.006 (0.011)\tLoss 0.8882 (0.8259)\tPrec@1 69.141 (71.055)\n",
      "Epoch: [200][78/195]\tTime 0.003 (0.013)\tLoss 0.8195 (0.8400)\tPrec@1 74.219 (70.402)\n",
      "Epoch: [200][117/195]\tTime 0.004 (0.011)\tLoss 0.9202 (0.8579)\tPrec@1 66.406 (69.700)\n",
      "Epoch: [200][156/195]\tTime 0.003 (0.010)\tLoss 0.9685 (0.8658)\tPrec@1 68.359 (69.357)\n",
      "Epoch: [200][195/195]\tTime 0.004 (0.009)\tLoss 1.0057 (0.8825)\tPrec@1 65.000 (68.664)\n",
      "EPOCH: 200 train Results: Prec@1 68.664 Loss: 0.8825\n",
      "Test: [0/39]\tTime 0.003 (0.003)\tLoss 1.2449 (1.2449)\tPrec@1 53.125 (53.125)\n",
      "Test: [39/39]\tTime 0.000 (0.001)\tLoss 1.3711 (1.2794)\tPrec@1 31.250 (55.390)\n",
      "EPOCH: 200 val Results: Prec@1 55.390 Loss: 1.2794\n",
      "Best Prec@1: 56.900\n",
      "\n",
      "End time:  Fri Apr  5 00:09:11 2024\n",
      "train executed in 290.3485 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 256\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'scheduler': None, \n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer4 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Fri Apr  5 00:09:11 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/97]\tTime 0.014 (0.014)\tLoss 5.5429 (5.5429)\tPrec@1 11.719 (11.719)\n",
      "Epoch: [1][19/97]\tTime 0.023 (0.013)\tLoss 3.7801 (4.6654)\tPrec@1 17.383 (13.252)\n",
      "Epoch: [1][38/97]\tTime 0.014 (0.017)\tLoss 3.0425 (3.9779)\tPrec@1 23.047 (17.553)\n",
      "Epoch: [1][57/97]\tTime 0.008 (0.015)\tLoss 2.6593 (3.5762)\tPrec@1 26.367 (20.619)\n",
      "Epoch: [1][76/97]\tTime 0.008 (0.014)\tLoss 2.5960 (3.3192)\tPrec@1 26.758 (22.542)\n",
      "Epoch: [1][95/97]\tTime 0.007 (0.014)\tLoss 2.2637 (3.1360)\tPrec@1 32.422 (24.066)\n",
      "Epoch: [1][97/97]\tTime 0.005 (0.014)\tLoss 2.3173 (3.1235)\tPrec@1 26.488 (24.156)\n",
      "EPOCH: 1 train Results: Prec@1 24.156 Loss: 3.1235\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 2.1026 (2.1026)\tPrec@1 31.836 (31.836)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 2.0898 (1.9851)\tPrec@1 32.721 (34.910)\n",
      "EPOCH: 1 val Results: Prec@1 34.910 Loss: 1.9851\n",
      "Best Prec@1: 34.910\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/97]\tTime 0.007 (0.007)\tLoss 2.1610 (2.1610)\tPrec@1 33.008 (33.008)\n",
      "Epoch: [2][19/97]\tTime 0.012 (0.009)\tLoss 2.0735 (2.1536)\tPrec@1 35.742 (33.770)\n",
      "Epoch: [2][38/97]\tTime 0.017 (0.012)\tLoss 2.1059 (2.1234)\tPrec@1 33.984 (33.979)\n",
      "Epoch: [2][57/97]\tTime 0.012 (0.012)\tLoss 1.8354 (2.0868)\tPrec@1 41.992 (34.230)\n",
      "Epoch: [2][76/97]\tTime 0.030 (0.012)\tLoss 1.9802 (2.0590)\tPrec@1 36.328 (34.707)\n",
      "Epoch: [2][95/97]\tTime 0.012 (0.012)\tLoss 1.7935 (2.0304)\tPrec@1 37.305 (35.150)\n",
      "Epoch: [2][97/97]\tTime 0.005 (0.012)\tLoss 1.8844 (2.0278)\tPrec@1 36.607 (35.170)\n",
      "EPOCH: 2 train Results: Prec@1 35.170 Loss: 2.0278\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.8318 (1.8318)\tPrec@1 37.305 (37.305)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.8534 (1.7566)\tPrec@1 37.500 (39.140)\n",
      "EPOCH: 2 val Results: Prec@1 39.140 Loss: 1.7566\n",
      "Best Prec@1: 39.140\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/97]\tTime 0.009 (0.009)\tLoss 1.8345 (1.8345)\tPrec@1 37.305 (37.305)\n",
      "Epoch: [3][19/97]\tTime 0.010 (0.011)\tLoss 1.9657 (1.8261)\tPrec@1 37.109 (39.014)\n",
      "Epoch: [3][38/97]\tTime 0.009 (0.013)\tLoss 1.8670 (1.8140)\tPrec@1 37.891 (38.922)\n",
      "Epoch: [3][57/97]\tTime 0.012 (0.014)\tLoss 1.7596 (1.8061)\tPrec@1 39.844 (38.935)\n",
      "Epoch: [3][76/97]\tTime 0.018 (0.013)\tLoss 1.7774 (1.7954)\tPrec@1 39.844 (39.123)\n",
      "Epoch: [3][95/97]\tTime 0.009 (0.013)\tLoss 1.7352 (1.7836)\tPrec@1 39.258 (39.217)\n",
      "Epoch: [3][97/97]\tTime 0.006 (0.013)\tLoss 1.6416 (1.7820)\tPrec@1 44.048 (39.270)\n",
      "EPOCH: 3 train Results: Prec@1 39.270 Loss: 1.7820\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.7115 (1.7115)\tPrec@1 40.820 (40.820)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.7548 (1.6558)\tPrec@1 40.441 (41.790)\n",
      "EPOCH: 3 val Results: Prec@1 41.790 Loss: 1.6558\n",
      "Best Prec@1: 41.790\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/97]\tTime 0.013 (0.013)\tLoss 1.6750 (1.6750)\tPrec@1 40.430 (40.430)\n",
      "Epoch: [4][19/97]\tTime 0.013 (0.014)\tLoss 1.6046 (1.6682)\tPrec@1 44.336 (41.523)\n",
      "Epoch: [4][38/97]\tTime 0.016 (0.012)\tLoss 1.6762 (1.6649)\tPrec@1 40.234 (41.722)\n",
      "Epoch: [4][57/97]\tTime 0.010 (0.012)\tLoss 1.6923 (1.6610)\tPrec@1 41.211 (41.857)\n",
      "Epoch: [4][76/97]\tTime 0.014 (0.013)\tLoss 1.6645 (1.6584)\tPrec@1 42.188 (41.941)\n",
      "Epoch: [4][95/97]\tTime 0.012 (0.012)\tLoss 1.6150 (1.6554)\tPrec@1 42.578 (41.986)\n",
      "Epoch: [4][97/97]\tTime 0.012 (0.012)\tLoss 1.6369 (1.6547)\tPrec@1 41.964 (42.012)\n",
      "EPOCH: 4 train Results: Prec@1 42.012 Loss: 1.6547\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.6329 (1.6329)\tPrec@1 42.969 (42.969)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.6959 (1.5970)\tPrec@1 40.074 (43.080)\n",
      "EPOCH: 4 val Results: Prec@1 43.080 Loss: 1.5970\n",
      "Best Prec@1: 43.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/97]\tTime 0.014 (0.014)\tLoss 1.6484 (1.6484)\tPrec@1 40.430 (40.430)\n",
      "Epoch: [5][19/97]\tTime 0.008 (0.012)\tLoss 1.5646 (1.5760)\tPrec@1 44.531 (43.838)\n",
      "Epoch: [5][38/97]\tTime 0.013 (0.011)\tLoss 1.6468 (1.5888)\tPrec@1 42.969 (43.610)\n",
      "Epoch: [5][57/97]\tTime 0.015 (0.012)\tLoss 1.5900 (1.5831)\tPrec@1 46.875 (43.801)\n",
      "Epoch: [5][76/97]\tTime 0.015 (0.013)\tLoss 1.6197 (1.5831)\tPrec@1 45.117 (43.978)\n",
      "Epoch: [5][95/97]\tTime 0.008 (0.017)\tLoss 1.6663 (1.5784)\tPrec@1 39.258 (44.196)\n",
      "Epoch: [5][97/97]\tTime 0.005 (0.017)\tLoss 1.5440 (1.5786)\tPrec@1 46.131 (44.194)\n",
      "EPOCH: 5 train Results: Prec@1 44.194 Loss: 1.5786\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.5825 (1.5825)\tPrec@1 43.945 (43.945)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.6449 (1.5566)\tPrec@1 40.441 (44.290)\n",
      "EPOCH: 5 val Results: Prec@1 44.290 Loss: 1.5566\n",
      "Best Prec@1: 44.290\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/97]\tTime 0.014 (0.014)\tLoss 1.5326 (1.5326)\tPrec@1 47.266 (47.266)\n",
      "Epoch: [6][19/97]\tTime 0.010 (0.027)\tLoss 1.5669 (1.5114)\tPrec@1 42.773 (46.963)\n",
      "Epoch: [6][38/97]\tTime 0.008 (0.026)\tLoss 1.5214 (1.5163)\tPrec@1 48.438 (46.720)\n",
      "Epoch: [6][57/97]\tTime 0.017 (0.021)\tLoss 1.5166 (1.5184)\tPrec@1 47.461 (46.602)\n",
      "Epoch: [6][76/97]\tTime 0.016 (0.020)\tLoss 1.5007 (1.5171)\tPrec@1 45.703 (46.728)\n",
      "Epoch: [6][95/97]\tTime 0.021 (0.019)\tLoss 1.5515 (1.5171)\tPrec@1 43.555 (46.598)\n",
      "Epoch: [6][97/97]\tTime 0.008 (0.019)\tLoss 1.5593 (1.5178)\tPrec@1 43.750 (46.568)\n",
      "EPOCH: 6 train Results: Prec@1 46.568 Loss: 1.5178\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.5446 (1.5446)\tPrec@1 45.117 (45.117)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.6151 (1.5256)\tPrec@1 40.809 (45.550)\n",
      "EPOCH: 6 val Results: Prec@1 45.550 Loss: 1.5256\n",
      "Best Prec@1: 45.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/97]\tTime 0.018 (0.018)\tLoss 1.5306 (1.5306)\tPrec@1 47.852 (47.852)\n",
      "Epoch: [7][19/97]\tTime 0.010 (0.012)\tLoss 1.4521 (1.4878)\tPrec@1 48.242 (47.695)\n",
      "Epoch: [7][38/97]\tTime 0.007 (0.011)\tLoss 1.4363 (1.4831)\tPrec@1 48.633 (48.052)\n",
      "Epoch: [7][57/97]\tTime 0.013 (0.012)\tLoss 1.5110 (1.4793)\tPrec@1 47.070 (47.922)\n",
      "Epoch: [7][76/97]\tTime 0.013 (0.013)\tLoss 1.4181 (1.4752)\tPrec@1 49.414 (47.727)\n",
      "Epoch: [7][95/97]\tTime 0.010 (0.012)\tLoss 1.4595 (1.4766)\tPrec@1 48.047 (47.788)\n",
      "Epoch: [7][97/97]\tTime 0.005 (0.012)\tLoss 1.4543 (1.4768)\tPrec@1 49.107 (47.798)\n",
      "EPOCH: 7 train Results: Prec@1 47.798 Loss: 1.4768\n",
      "Test: [0/19]\tTime 0.009 (0.009)\tLoss 1.5154 (1.5154)\tPrec@1 45.117 (45.117)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.5966 (1.4980)\tPrec@1 40.441 (46.490)\n",
      "EPOCH: 7 val Results: Prec@1 46.490 Loss: 1.4980\n",
      "Best Prec@1: 46.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/97]\tTime 0.009 (0.009)\tLoss 1.4269 (1.4269)\tPrec@1 48.633 (48.633)\n",
      "Epoch: [8][19/97]\tTime 0.011 (0.019)\tLoss 1.3796 (1.4610)\tPrec@1 52.734 (48.486)\n",
      "Epoch: [8][38/97]\tTime 0.014 (0.019)\tLoss 1.4429 (1.4601)\tPrec@1 49.414 (48.443)\n",
      "Epoch: [8][57/97]\tTime 0.142 (0.018)\tLoss 1.4178 (1.4506)\tPrec@1 49.219 (48.943)\n",
      "Epoch: [8][76/97]\tTime 0.014 (0.019)\tLoss 1.4929 (1.4457)\tPrec@1 44.922 (48.927)\n",
      "Epoch: [8][95/97]\tTime 0.009 (0.017)\tLoss 1.4116 (1.4472)\tPrec@1 51.758 (48.924)\n",
      "Epoch: [8][97/97]\tTime 0.006 (0.017)\tLoss 1.4764 (1.4468)\tPrec@1 51.190 (48.932)\n",
      "EPOCH: 8 train Results: Prec@1 48.932 Loss: 1.4468\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.4917 (1.4917)\tPrec@1 46.094 (46.094)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.5740 (1.4758)\tPrec@1 41.176 (47.280)\n",
      "EPOCH: 8 val Results: Prec@1 47.280 Loss: 1.4758\n",
      "Best Prec@1: 47.280\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/97]\tTime 0.013 (0.013)\tLoss 1.4069 (1.4069)\tPrec@1 49.219 (49.219)\n",
      "Epoch: [9][19/97]\tTime 0.011 (0.010)\tLoss 1.3919 (1.3983)\tPrec@1 50.195 (50.400)\n",
      "Epoch: [9][38/97]\tTime 0.009 (0.010)\tLoss 1.4450 (1.3992)\tPrec@1 50.781 (50.431)\n",
      "Epoch: [9][57/97]\tTime 0.012 (0.010)\tLoss 1.4129 (1.4088)\tPrec@1 48.438 (50.125)\n",
      "Epoch: [9][76/97]\tTime 0.013 (0.010)\tLoss 1.3290 (1.4075)\tPrec@1 52.344 (50.246)\n",
      "Epoch: [9][95/97]\tTime 0.015 (0.010)\tLoss 1.3839 (1.4073)\tPrec@1 52.148 (50.311)\n",
      "Epoch: [9][97/97]\tTime 0.010 (0.010)\tLoss 1.4756 (1.4077)\tPrec@1 49.107 (50.314)\n",
      "EPOCH: 9 train Results: Prec@1 50.314 Loss: 1.4077\n",
      "Test: [0/19]\tTime 0.007 (0.007)\tLoss 1.4664 (1.4664)\tPrec@1 47.656 (47.656)\n",
      "Test: [19/19]\tTime 0.003 (0.003)\tLoss 1.5554 (1.4541)\tPrec@1 40.809 (48.110)\n",
      "EPOCH: 9 val Results: Prec@1 48.110 Loss: 1.4541\n",
      "Best Prec@1: 48.110\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/97]\tTime 0.010 (0.010)\tLoss 1.3635 (1.3635)\tPrec@1 50.586 (50.586)\n",
      "Epoch: [10][19/97]\tTime 0.011 (0.010)\tLoss 1.4011 (1.3662)\tPrec@1 50.391 (51.729)\n",
      "Epoch: [10][38/97]\tTime 0.012 (0.012)\tLoss 1.3817 (1.3779)\tPrec@1 51.758 (51.457)\n",
      "Epoch: [10][57/97]\tTime 0.007 (0.011)\tLoss 1.4218 (1.3806)\tPrec@1 46.875 (51.445)\n",
      "Epoch: [10][76/97]\tTime 0.007 (0.013)\tLoss 1.4225 (1.3807)\tPrec@1 47.852 (51.299)\n",
      "Epoch: [10][95/97]\tTime 0.019 (0.015)\tLoss 1.3867 (1.3814)\tPrec@1 50.586 (51.390)\n",
      "Epoch: [10][97/97]\tTime 0.013 (0.015)\tLoss 1.3182 (1.3808)\tPrec@1 54.762 (51.408)\n",
      "EPOCH: 10 train Results: Prec@1 51.408 Loss: 1.3808\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.4511 (1.4511)\tPrec@1 47.070 (47.070)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.5421 (1.4351)\tPrec@1 41.176 (48.720)\n",
      "EPOCH: 10 val Results: Prec@1 48.720 Loss: 1.4351\n",
      "Best Prec@1: 48.720\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/97]\tTime 0.018 (0.018)\tLoss 1.3113 (1.3113)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [11][19/97]\tTime 0.017 (0.026)\tLoss 1.3509 (1.3453)\tPrec@1 52.344 (52.900)\n",
      "Epoch: [11][38/97]\tTime 0.014 (0.025)\tLoss 1.3453 (1.3503)\tPrec@1 52.930 (52.634)\n",
      "Epoch: [11][57/97]\tTime 0.013 (0.021)\tLoss 1.3203 (1.3523)\tPrec@1 51.367 (52.542)\n",
      "Epoch: [11][76/97]\tTime 0.009 (0.021)\tLoss 1.3374 (1.3527)\tPrec@1 53.906 (52.471)\n",
      "Epoch: [11][95/97]\tTime 0.013 (0.019)\tLoss 1.4177 (1.3545)\tPrec@1 46.484 (52.380)\n",
      "Epoch: [11][97/97]\tTime 0.008 (0.019)\tLoss 1.3960 (1.3542)\tPrec@1 50.893 (52.350)\n",
      "EPOCH: 11 train Results: Prec@1 52.350 Loss: 1.3542\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.4327 (1.4327)\tPrec@1 46.680 (46.680)\n",
      "Test: [19/19]\tTime 0.004 (0.003)\tLoss 1.5195 (1.4177)\tPrec@1 41.912 (49.340)\n",
      "EPOCH: 11 val Results: Prec@1 49.340 Loss: 1.4177\n",
      "Best Prec@1: 49.340\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/97]\tTime 0.013 (0.013)\tLoss 1.3348 (1.3348)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [12][19/97]\tTime 0.014 (0.012)\tLoss 1.3006 (1.3179)\tPrec@1 53.125 (53.506)\n",
      "Epoch: [12][38/97]\tTime 0.019 (0.014)\tLoss 1.3572 (1.3238)\tPrec@1 49.805 (53.190)\n",
      "Epoch: [12][57/97]\tTime 0.030 (0.017)\tLoss 1.3001 (1.3252)\tPrec@1 52.344 (53.162)\n",
      "Epoch: [12][76/97]\tTime 0.010 (0.016)\tLoss 1.3089 (1.3277)\tPrec@1 54.297 (53.209)\n",
      "Epoch: [12][95/97]\tTime 0.009 (0.016)\tLoss 1.3321 (1.3309)\tPrec@1 54.492 (53.082)\n",
      "Epoch: [12][97/97]\tTime 0.017 (0.016)\tLoss 1.2799 (1.3302)\tPrec@1 54.167 (53.100)\n",
      "EPOCH: 12 train Results: Prec@1 53.100 Loss: 1.3302\n",
      "Test: [0/19]\tTime 0.011 (0.011)\tLoss 1.4135 (1.4135)\tPrec@1 47.070 (47.070)\n",
      "Test: [19/19]\tTime 0.001 (0.007)\tLoss 1.5005 (1.3997)\tPrec@1 41.544 (49.770)\n",
      "EPOCH: 12 val Results: Prec@1 49.770 Loss: 1.3997\n",
      "Best Prec@1: 49.770\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/97]\tTime 0.027 (0.027)\tLoss 1.2877 (1.2877)\tPrec@1 51.953 (51.953)\n",
      "Epoch: [13][19/97]\tTime 0.019 (0.022)\tLoss 1.2481 (1.2986)\tPrec@1 55.859 (54.287)\n",
      "Epoch: [13][38/97]\tTime 0.013 (0.019)\tLoss 1.3029 (1.2974)\tPrec@1 53.516 (54.552)\n",
      "Epoch: [13][57/97]\tTime 0.032 (0.019)\tLoss 1.3247 (1.3024)\tPrec@1 53.906 (54.529)\n",
      "Epoch: [13][76/97]\tTime 0.009 (0.019)\tLoss 1.3164 (1.3056)\tPrec@1 50.586 (54.206)\n",
      "Epoch: [13][95/97]\tTime 0.017 (0.020)\tLoss 1.3024 (1.3084)\tPrec@1 53.711 (54.002)\n",
      "Epoch: [13][97/97]\tTime 0.012 (0.020)\tLoss 1.2812 (1.3081)\tPrec@1 59.226 (54.038)\n",
      "EPOCH: 13 train Results: Prec@1 54.038 Loss: 1.3081\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3918 (1.3918)\tPrec@1 48.828 (48.828)\n",
      "Test: [19/19]\tTime 0.002 (0.004)\tLoss 1.4875 (1.3839)\tPrec@1 41.544 (50.300)\n",
      "EPOCH: 13 val Results: Prec@1 50.300 Loss: 1.3839\n",
      "Best Prec@1: 50.300\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/97]\tTime 0.015 (0.015)\tLoss 1.2194 (1.2194)\tPrec@1 56.641 (56.641)\n",
      "Epoch: [14][19/97]\tTime 0.024 (0.023)\tLoss 1.2569 (1.2832)\tPrec@1 55.469 (55.273)\n",
      "Epoch: [14][38/97]\tTime 0.023 (0.028)\tLoss 1.2823 (1.2842)\tPrec@1 56.445 (54.898)\n",
      "Epoch: [14][57/97]\tTime 0.009 (0.029)\tLoss 1.2508 (1.2868)\tPrec@1 56.250 (54.815)\n",
      "Epoch: [14][76/97]\tTime 0.016 (0.026)\tLoss 1.3158 (1.2881)\tPrec@1 53.320 (54.827)\n",
      "Epoch: [14][95/97]\tTime 0.020 (0.024)\tLoss 1.2848 (1.2867)\tPrec@1 54.102 (54.755)\n",
      "Epoch: [14][97/97]\tTime 0.025 (0.025)\tLoss 1.2891 (1.2867)\tPrec@1 52.381 (54.722)\n",
      "EPOCH: 14 train Results: Prec@1 54.722 Loss: 1.2867\n",
      "Test: [0/19]\tTime 0.009 (0.009)\tLoss 1.3853 (1.3853)\tPrec@1 49.609 (49.609)\n",
      "Test: [19/19]\tTime 0.005 (0.007)\tLoss 1.4724 (1.3687)\tPrec@1 42.647 (50.880)\n",
      "EPOCH: 14 val Results: Prec@1 50.880 Loss: 1.3687\n",
      "Best Prec@1: 50.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/97]\tTime 0.015 (0.015)\tLoss 1.2246 (1.2246)\tPrec@1 59.766 (59.766)\n",
      "Epoch: [15][19/97]\tTime 0.018 (0.020)\tLoss 1.3460 (1.2608)\tPrec@1 52.734 (56.104)\n",
      "Epoch: [15][38/97]\tTime 0.040 (0.017)\tLoss 1.2226 (1.2666)\tPrec@1 56.641 (55.824)\n",
      "Epoch: [15][57/97]\tTime 0.031 (0.019)\tLoss 1.2435 (1.2647)\tPrec@1 57.617 (55.886)\n",
      "Epoch: [15][76/97]\tTime 0.017 (0.017)\tLoss 1.2723 (1.2681)\tPrec@1 55.469 (55.578)\n",
      "Epoch: [15][95/97]\tTime 0.010 (0.017)\tLoss 1.3181 (1.2675)\tPrec@1 52.539 (55.566)\n",
      "Epoch: [15][97/97]\tTime 0.010 (0.017)\tLoss 1.3075 (1.2681)\tPrec@1 54.762 (55.544)\n",
      "EPOCH: 15 train Results: Prec@1 55.544 Loss: 1.2681\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3673 (1.3673)\tPrec@1 48.828 (48.828)\n",
      "Test: [19/19]\tTime 0.003 (0.002)\tLoss 1.4605 (1.3531)\tPrec@1 42.279 (51.260)\n",
      "EPOCH: 15 val Results: Prec@1 51.260 Loss: 1.3531\n",
      "Best Prec@1: 51.260\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/97]\tTime 0.008 (0.008)\tLoss 1.2842 (1.2842)\tPrec@1 56.641 (56.641)\n",
      "Epoch: [16][19/97]\tTime 0.008 (0.011)\tLoss 1.2070 (1.2148)\tPrec@1 57.422 (57.383)\n",
      "Epoch: [16][38/97]\tTime 0.011 (0.011)\tLoss 1.1988 (1.2235)\tPrec@1 56.250 (56.851)\n",
      "Epoch: [16][57/97]\tTime 0.010 (0.011)\tLoss 1.2863 (1.2325)\tPrec@1 55.664 (56.499)\n",
      "Epoch: [16][76/97]\tTime 0.014 (0.011)\tLoss 1.2603 (1.2382)\tPrec@1 54.297 (56.428)\n",
      "Epoch: [16][95/97]\tTime 0.066 (0.012)\tLoss 1.2605 (1.2415)\tPrec@1 55.078 (56.295)\n",
      "Epoch: [16][97/97]\tTime 0.012 (0.012)\tLoss 1.2900 (1.2415)\tPrec@1 55.357 (56.298)\n",
      "EPOCH: 16 train Results: Prec@1 56.298 Loss: 1.2415\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3632 (1.3632)\tPrec@1 49.219 (49.219)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4689 (1.3435)\tPrec@1 44.485 (51.680)\n",
      "EPOCH: 16 val Results: Prec@1 51.680 Loss: 1.3435\n",
      "Best Prec@1: 51.680\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/97]\tTime 0.010 (0.010)\tLoss 1.1960 (1.1960)\tPrec@1 57.617 (57.617)\n",
      "Epoch: [17][19/97]\tTime 0.016 (0.013)\tLoss 1.1889 (1.2135)\tPrec@1 59.570 (58.047)\n",
      "Epoch: [17][38/97]\tTime 0.010 (0.011)\tLoss 1.2882 (1.2205)\tPrec@1 56.055 (57.527)\n",
      "Epoch: [17][57/97]\tTime 0.010 (0.011)\tLoss 1.2293 (1.2232)\tPrec@1 55.469 (57.109)\n",
      "Epoch: [17][76/97]\tTime 0.007 (0.011)\tLoss 1.3145 (1.2242)\tPrec@1 49.805 (56.950)\n",
      "Epoch: [17][95/97]\tTime 0.010 (0.011)\tLoss 1.2384 (1.2266)\tPrec@1 58.789 (56.860)\n",
      "Epoch: [17][97/97]\tTime 0.005 (0.010)\tLoss 1.1741 (1.2270)\tPrec@1 60.417 (56.860)\n",
      "EPOCH: 17 train Results: Prec@1 56.860 Loss: 1.2270\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3482 (1.3482)\tPrec@1 49.609 (49.609)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4474 (1.3313)\tPrec@1 42.647 (52.200)\n",
      "EPOCH: 17 val Results: Prec@1 52.200 Loss: 1.3313\n",
      "Best Prec@1: 52.200\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/97]\tTime 0.009 (0.009)\tLoss 1.1819 (1.1819)\tPrec@1 57.617 (57.617)\n",
      "Epoch: [18][19/97]\tTime 0.009 (0.016)\tLoss 1.2065 (1.2066)\tPrec@1 57.812 (57.627)\n",
      "Epoch: [18][38/97]\tTime 0.021 (0.015)\tLoss 1.1353 (1.2048)\tPrec@1 60.742 (57.727)\n",
      "Epoch: [18][57/97]\tTime 0.007 (0.017)\tLoss 1.2302 (1.2105)\tPrec@1 55.859 (57.264)\n",
      "Epoch: [18][76/97]\tTime 0.006 (0.016)\tLoss 1.2393 (1.2094)\tPrec@1 57.031 (57.308)\n",
      "Epoch: [18][95/97]\tTime 0.056 (0.020)\tLoss 1.2627 (1.2094)\tPrec@1 54.297 (57.316)\n",
      "Epoch: [18][97/97]\tTime 0.012 (0.020)\tLoss 1.1640 (1.2096)\tPrec@1 60.119 (57.318)\n",
      "EPOCH: 18 train Results: Prec@1 57.318 Loss: 1.2096\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3452 (1.3452)\tPrec@1 50.195 (50.195)\n",
      "Test: [19/19]\tTime 0.004 (0.004)\tLoss 1.4400 (1.3179)\tPrec@1 42.279 (52.640)\n",
      "EPOCH: 18 val Results: Prec@1 52.640 Loss: 1.3179\n",
      "Best Prec@1: 52.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/97]\tTime 0.020 (0.020)\tLoss 1.1391 (1.1391)\tPrec@1 60.352 (60.352)\n",
      "Epoch: [19][19/97]\tTime 0.008 (0.018)\tLoss 1.2119 (1.1665)\tPrec@1 58.789 (59.092)\n",
      "Epoch: [19][38/97]\tTime 0.013 (0.016)\tLoss 1.2191 (1.1771)\tPrec@1 58.203 (58.719)\n",
      "Epoch: [19][57/97]\tTime 0.011 (0.014)\tLoss 1.2247 (1.1810)\tPrec@1 54.883 (58.503)\n",
      "Epoch: [19][76/97]\tTime 0.028 (0.015)\tLoss 1.2305 (1.1891)\tPrec@1 56.055 (58.127)\n",
      "Epoch: [19][95/97]\tTime 0.014 (0.015)\tLoss 1.2355 (1.1903)\tPrec@1 55.273 (58.142)\n",
      "Epoch: [19][97/97]\tTime 0.007 (0.015)\tLoss 1.2063 (1.1911)\tPrec@1 55.952 (58.090)\n",
      "EPOCH: 19 train Results: Prec@1 58.090 Loss: 1.1911\n",
      "Test: [0/19]\tTime 0.007 (0.007)\tLoss 1.3351 (1.3351)\tPrec@1 50.586 (50.586)\n",
      "Test: [19/19]\tTime 0.001 (0.006)\tLoss 1.4237 (1.3079)\tPrec@1 45.588 (53.550)\n",
      "EPOCH: 19 val Results: Prec@1 53.550 Loss: 1.3079\n",
      "Best Prec@1: 53.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/97]\tTime 0.009 (0.009)\tLoss 1.1898 (1.1898)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [20][19/97]\tTime 0.009 (0.019)\tLoss 1.0908 (1.1781)\tPrec@1 63.477 (58.711)\n",
      "Epoch: [20][38/97]\tTime 0.014 (0.015)\tLoss 1.1738 (1.1758)\tPrec@1 60.938 (58.739)\n",
      "Epoch: [20][57/97]\tTime 0.010 (0.015)\tLoss 1.1479 (1.1774)\tPrec@1 59.375 (58.691)\n",
      "Epoch: [20][76/97]\tTime 0.010 (0.015)\tLoss 1.1776 (1.1779)\tPrec@1 57.422 (58.566)\n",
      "Epoch: [20][95/97]\tTime 0.034 (0.015)\tLoss 1.2033 (1.1810)\tPrec@1 56.445 (58.403)\n",
      "Epoch: [20][97/97]\tTime 0.039 (0.017)\tLoss 1.2256 (1.1818)\tPrec@1 57.143 (58.368)\n",
      "EPOCH: 20 train Results: Prec@1 58.368 Loss: 1.1818\n",
      "Test: [0/19]\tTime 0.007 (0.007)\tLoss 1.3256 (1.3256)\tPrec@1 50.000 (50.000)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4348 (1.3019)\tPrec@1 44.118 (53.020)\n",
      "EPOCH: 20 val Results: Prec@1 53.020 Loss: 1.3019\n",
      "Best Prec@1: 53.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/97]\tTime 0.016 (0.016)\tLoss 1.1640 (1.1640)\tPrec@1 59.375 (59.375)\n",
      "Epoch: [21][19/97]\tTime 0.012 (0.011)\tLoss 1.2118 (1.1390)\tPrec@1 58.008 (60.107)\n",
      "Epoch: [21][38/97]\tTime 0.006 (0.015)\tLoss 1.1474 (1.1554)\tPrec@1 59.180 (59.390)\n",
      "Epoch: [21][57/97]\tTime 0.007 (0.013)\tLoss 1.1107 (1.1574)\tPrec@1 60.547 (59.399)\n",
      "Epoch: [21][76/97]\tTime 0.009 (0.013)\tLoss 1.2074 (1.1656)\tPrec@1 55.469 (59.134)\n",
      "Epoch: [21][95/97]\tTime 0.007 (0.013)\tLoss 1.2109 (1.1683)\tPrec@1 58.594 (59.037)\n",
      "Epoch: [21][97/97]\tTime 0.007 (0.013)\tLoss 1.1843 (1.1689)\tPrec@1 61.012 (59.026)\n",
      "EPOCH: 21 train Results: Prec@1 59.026 Loss: 1.1689\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3104 (1.3104)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.002 (0.006)\tLoss 1.4376 (1.2928)\tPrec@1 44.485 (53.710)\n",
      "EPOCH: 21 val Results: Prec@1 53.710 Loss: 1.2928\n",
      "Best Prec@1: 53.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/97]\tTime 0.009 (0.009)\tLoss 1.1304 (1.1304)\tPrec@1 60.547 (60.547)\n",
      "Epoch: [22][19/97]\tTime 0.008 (0.010)\tLoss 1.1746 (1.1391)\tPrec@1 58.008 (60.312)\n",
      "Epoch: [22][38/97]\tTime 0.019 (0.017)\tLoss 1.1082 (1.1405)\tPrec@1 61.523 (60.046)\n",
      "Epoch: [22][57/97]\tTime 0.013 (0.015)\tLoss 1.1500 (1.1462)\tPrec@1 61.133 (59.772)\n",
      "Epoch: [22][76/97]\tTime 0.013 (0.014)\tLoss 1.1242 (1.1460)\tPrec@1 60.547 (59.740)\n",
      "Epoch: [22][95/97]\tTime 0.009 (0.013)\tLoss 1.0826 (1.1528)\tPrec@1 63.672 (59.328)\n",
      "Epoch: [22][97/97]\tTime 0.007 (0.013)\tLoss 1.2153 (1.1534)\tPrec@1 55.060 (59.282)\n",
      "EPOCH: 22 train Results: Prec@1 59.282 Loss: 1.1534\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2902 (1.2902)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4334 (1.2873)\tPrec@1 45.956 (53.950)\n",
      "EPOCH: 22 val Results: Prec@1 53.950 Loss: 1.2873\n",
      "Best Prec@1: 53.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/97]\tTime 0.014 (0.014)\tLoss 1.1058 (1.1058)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [23][19/97]\tTime 0.017 (0.015)\tLoss 1.1140 (1.1172)\tPrec@1 61.719 (60.840)\n",
      "Epoch: [23][38/97]\tTime 0.013 (0.014)\tLoss 1.0887 (1.1328)\tPrec@1 60.547 (60.367)\n",
      "Epoch: [23][57/97]\tTime 0.011 (0.014)\tLoss 1.2210 (1.1325)\tPrec@1 54.492 (60.190)\n",
      "Epoch: [23][76/97]\tTime 0.012 (0.014)\tLoss 1.1498 (1.1371)\tPrec@1 59.180 (59.976)\n",
      "Epoch: [23][95/97]\tTime 0.014 (0.017)\tLoss 1.1608 (1.1431)\tPrec@1 57.617 (59.674)\n",
      "Epoch: [23][97/97]\tTime 0.006 (0.017)\tLoss 1.1723 (1.1443)\tPrec@1 58.929 (59.638)\n",
      "EPOCH: 23 train Results: Prec@1 59.638 Loss: 1.1443\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3064 (1.3064)\tPrec@1 52.539 (52.539)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4252 (1.2777)\tPrec@1 45.588 (54.170)\n",
      "EPOCH: 23 val Results: Prec@1 54.170 Loss: 1.2777\n",
      "Best Prec@1: 54.170\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/97]\tTime 0.013 (0.013)\tLoss 1.1242 (1.1242)\tPrec@1 59.961 (59.961)\n",
      "Epoch: [24][19/97]\tTime 0.018 (0.025)\tLoss 1.1181 (1.0879)\tPrec@1 61.328 (62.061)\n",
      "Epoch: [24][38/97]\tTime 0.024 (0.021)\tLoss 1.2075 (1.1042)\tPrec@1 57.031 (61.198)\n",
      "Epoch: [24][57/97]\tTime 0.008 (0.020)\tLoss 1.1426 (1.1161)\tPrec@1 60.938 (60.715)\n",
      "Epoch: [24][76/97]\tTime 0.007 (0.018)\tLoss 1.1643 (1.1209)\tPrec@1 59.766 (60.440)\n",
      "Epoch: [24][95/97]\tTime 0.011 (0.019)\tLoss 1.1758 (1.1269)\tPrec@1 56.836 (60.191)\n",
      "Epoch: [24][97/97]\tTime 0.005 (0.019)\tLoss 1.2257 (1.1275)\tPrec@1 57.440 (60.176)\n",
      "EPOCH: 24 train Results: Prec@1 60.176 Loss: 1.1275\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2881 (1.2881)\tPrec@1 51.953 (51.953)\n",
      "Test: [19/19]\tTime 0.002 (0.007)\tLoss 1.4381 (1.2770)\tPrec@1 47.059 (54.220)\n",
      "EPOCH: 24 val Results: Prec@1 54.220 Loss: 1.2770\n",
      "Best Prec@1: 54.220\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/97]\tTime 0.047 (0.047)\tLoss 1.0654 (1.0654)\tPrec@1 61.719 (61.719)\n",
      "Epoch: [25][19/97]\tTime 0.008 (0.012)\tLoss 1.0324 (1.0898)\tPrec@1 61.914 (62.051)\n",
      "Epoch: [25][38/97]\tTime 0.060 (0.026)\tLoss 1.0672 (1.1025)\tPrec@1 62.891 (61.493)\n",
      "Epoch: [25][57/97]\tTime 0.012 (0.025)\tLoss 1.0934 (1.1033)\tPrec@1 60.547 (61.453)\n",
      "Epoch: [25][76/97]\tTime 0.015 (0.029)\tLoss 1.1222 (1.1106)\tPrec@1 60.547 (61.123)\n",
      "Epoch: [25][95/97]\tTime 0.013 (0.026)\tLoss 1.1192 (1.1151)\tPrec@1 58.398 (60.832)\n",
      "Epoch: [25][97/97]\tTime 0.011 (0.025)\tLoss 1.2142 (1.1155)\tPrec@1 54.464 (60.774)\n",
      "EPOCH: 25 train Results: Prec@1 60.774 Loss: 1.1155\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2928 (1.2928)\tPrec@1 54.297 (54.297)\n",
      "Test: [19/19]\tTime 0.005 (0.004)\tLoss 1.4256 (1.2682)\tPrec@1 46.691 (54.630)\n",
      "EPOCH: 25 val Results: Prec@1 54.630 Loss: 1.2682\n",
      "Best Prec@1: 54.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/97]\tTime 0.014 (0.014)\tLoss 1.0947 (1.0947)\tPrec@1 61.523 (61.523)\n",
      "Epoch: [26][19/97]\tTime 0.013 (0.016)\tLoss 1.0598 (1.0781)\tPrec@1 63.086 (61.895)\n",
      "Epoch: [26][38/97]\tTime 0.014 (0.014)\tLoss 1.1235 (1.0882)\tPrec@1 61.328 (61.724)\n",
      "Epoch: [26][57/97]\tTime 0.007 (0.016)\tLoss 1.1276 (1.0912)\tPrec@1 58.984 (61.769)\n",
      "Epoch: [26][76/97]\tTime 0.007 (0.017)\tLoss 1.1504 (1.0979)\tPrec@1 59.961 (61.409)\n",
      "Epoch: [26][95/97]\tTime 0.011 (0.016)\tLoss 1.1435 (1.1027)\tPrec@1 56.836 (61.137)\n",
      "Epoch: [26][97/97]\tTime 0.009 (0.016)\tLoss 1.1737 (1.1034)\tPrec@1 59.821 (61.118)\n",
      "EPOCH: 26 train Results: Prec@1 61.118 Loss: 1.1034\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2809 (1.2809)\tPrec@1 53.320 (53.320)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.4098 (1.2655)\tPrec@1 45.956 (54.200)\n",
      "EPOCH: 26 val Results: Prec@1 54.200 Loss: 1.2655\n",
      "Best Prec@1: 54.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/97]\tTime 0.008 (0.008)\tLoss 1.1166 (1.1166)\tPrec@1 61.523 (61.523)\n",
      "Epoch: [27][19/97]\tTime 0.023 (0.018)\tLoss 1.0500 (1.0623)\tPrec@1 62.305 (62.900)\n",
      "Epoch: [27][38/97]\tTime 0.013 (0.019)\tLoss 1.0981 (1.0772)\tPrec@1 62.500 (62.164)\n",
      "Epoch: [27][57/97]\tTime 0.010 (0.018)\tLoss 1.0595 (1.0814)\tPrec@1 59.180 (61.857)\n",
      "Epoch: [27][76/97]\tTime 0.015 (0.022)\tLoss 1.0578 (1.0881)\tPrec@1 60.352 (61.513)\n",
      "Epoch: [27][95/97]\tTime 0.028 (0.022)\tLoss 1.0832 (1.0954)\tPrec@1 64.258 (61.332)\n",
      "Epoch: [27][97/97]\tTime 0.033 (0.022)\tLoss 1.1628 (1.0959)\tPrec@1 58.036 (61.304)\n",
      "EPOCH: 27 train Results: Prec@1 61.304 Loss: 1.0959\n",
      "Test: [0/19]\tTime 0.014 (0.014)\tLoss 1.2617 (1.2617)\tPrec@1 55.273 (55.273)\n",
      "Test: [19/19]\tTime 0.008 (0.008)\tLoss 1.4285 (1.2644)\tPrec@1 45.956 (54.760)\n",
      "EPOCH: 27 val Results: Prec@1 54.760 Loss: 1.2644\n",
      "Best Prec@1: 54.760\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/97]\tTime 0.029 (0.029)\tLoss 1.0672 (1.0672)\tPrec@1 61.133 (61.133)\n",
      "Epoch: [28][19/97]\tTime 0.011 (0.020)\tLoss 1.0649 (1.0631)\tPrec@1 61.328 (62.578)\n",
      "Epoch: [28][38/97]\tTime 0.010 (0.017)\tLoss 1.0714 (1.0716)\tPrec@1 61.523 (62.335)\n",
      "Epoch: [28][57/97]\tTime 0.013 (0.016)\tLoss 1.1217 (1.0806)\tPrec@1 59.570 (61.843)\n",
      "Epoch: [28][76/97]\tTime 0.008 (0.015)\tLoss 1.1893 (1.0883)\tPrec@1 58.594 (61.648)\n",
      "Epoch: [28][95/97]\tTime 0.007 (0.015)\tLoss 1.1340 (1.0909)\tPrec@1 60.352 (61.483)\n",
      "Epoch: [28][97/97]\tTime 0.008 (0.015)\tLoss 1.2216 (1.0913)\tPrec@1 58.036 (61.484)\n",
      "EPOCH: 28 train Results: Prec@1 61.484 Loss: 1.0913\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2789 (1.2789)\tPrec@1 53.320 (53.320)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4091 (1.2612)\tPrec@1 50.000 (54.890)\n",
      "EPOCH: 28 val Results: Prec@1 54.890 Loss: 1.2612\n",
      "Best Prec@1: 54.890\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/97]\tTime 0.013 (0.013)\tLoss 1.0521 (1.0521)\tPrec@1 62.305 (62.305)\n",
      "Epoch: [29][19/97]\tTime 0.009 (0.010)\tLoss 0.9736 (1.0382)\tPrec@1 67.773 (63.799)\n",
      "Epoch: [29][38/97]\tTime 0.007 (0.010)\tLoss 1.0340 (1.0516)\tPrec@1 65.234 (63.156)\n",
      "Epoch: [29][57/97]\tTime 0.017 (0.010)\tLoss 1.2443 (1.0621)\tPrec@1 53.906 (62.540)\n",
      "Epoch: [29][76/97]\tTime 0.018 (0.014)\tLoss 1.0871 (1.0749)\tPrec@1 61.133 (62.064)\n",
      "Epoch: [29][95/97]\tTime 0.014 (0.015)\tLoss 1.0389 (1.0803)\tPrec@1 62.891 (61.747)\n",
      "Epoch: [29][97/97]\tTime 0.008 (0.015)\tLoss 1.2353 (1.0811)\tPrec@1 55.952 (61.734)\n",
      "EPOCH: 29 train Results: Prec@1 61.734 Loss: 1.0811\n",
      "Test: [0/19]\tTime 0.010 (0.010)\tLoss 1.2568 (1.2568)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.002 (0.005)\tLoss 1.3937 (1.2499)\tPrec@1 48.897 (55.250)\n",
      "EPOCH: 29 val Results: Prec@1 55.250 Loss: 1.2499\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/97]\tTime 0.034 (0.034)\tLoss 1.0686 (1.0686)\tPrec@1 62.500 (62.500)\n",
      "Epoch: [30][19/97]\tTime 0.016 (0.016)\tLoss 1.1002 (1.0575)\tPrec@1 59.375 (62.275)\n",
      "Epoch: [30][38/97]\tTime 0.021 (0.017)\tLoss 1.0975 (1.0553)\tPrec@1 60.352 (62.575)\n",
      "Epoch: [30][57/97]\tTime 0.017 (0.017)\tLoss 1.1055 (1.0580)\tPrec@1 61.523 (62.604)\n",
      "Epoch: [30][76/97]\tTime 0.014 (0.017)\tLoss 1.1146 (1.0642)\tPrec@1 59.961 (62.191)\n",
      "Epoch: [30][95/97]\tTime 0.021 (0.019)\tLoss 1.0945 (1.0720)\tPrec@1 59.961 (61.963)\n",
      "Epoch: [30][97/97]\tTime 0.006 (0.018)\tLoss 1.0873 (1.0725)\tPrec@1 61.607 (61.942)\n",
      "EPOCH: 30 train Results: Prec@1 61.942 Loss: 1.0725\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2604 (1.2604)\tPrec@1 55.078 (55.078)\n",
      "Test: [19/19]\tTime 0.001 (0.005)\tLoss 1.4063 (1.2520)\tPrec@1 46.691 (54.960)\n",
      "EPOCH: 30 val Results: Prec@1 54.960 Loss: 1.2520\n",
      "Best Prec@1: 55.250\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/97]\tTime 0.027 (0.027)\tLoss 0.9908 (0.9908)\tPrec@1 66.211 (66.211)\n",
      "Epoch: [31][19/97]\tTime 0.019 (0.016)\tLoss 1.0509 (1.0354)\tPrec@1 61.914 (63.164)\n",
      "Epoch: [31][38/97]\tTime 0.022 (0.017)\tLoss 1.1453 (1.0421)\tPrec@1 61.133 (63.126)\n",
      "Epoch: [31][57/97]\tTime 0.017 (0.023)\tLoss 1.0575 (1.0481)\tPrec@1 63.281 (62.921)\n",
      "Epoch: [31][76/97]\tTime 0.020 (0.020)\tLoss 1.1322 (1.0553)\tPrec@1 58.203 (62.688)\n",
      "Epoch: [31][95/97]\tTime 0.007 (0.018)\tLoss 1.0174 (1.0634)\tPrec@1 64.453 (62.354)\n",
      "Epoch: [31][97/97]\tTime 0.007 (0.018)\tLoss 1.0989 (1.0652)\tPrec@1 61.310 (62.288)\n",
      "EPOCH: 31 train Results: Prec@1 62.288 Loss: 1.0652\n",
      "Test: [0/19]\tTime 0.007 (0.007)\tLoss 1.2296 (1.2296)\tPrec@1 57.227 (57.227)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4226 (1.2504)\tPrec@1 45.221 (55.260)\n",
      "EPOCH: 31 val Results: Prec@1 55.260 Loss: 1.2504\n",
      "Best Prec@1: 55.260\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/97]\tTime 0.014 (0.014)\tLoss 1.0553 (1.0553)\tPrec@1 65.234 (65.234)\n",
      "Epoch: [32][19/97]\tTime 0.014 (0.011)\tLoss 1.0015 (1.0333)\tPrec@1 66.602 (63.428)\n",
      "Epoch: [32][38/97]\tTime 0.009 (0.011)\tLoss 1.0340 (1.0386)\tPrec@1 62.500 (63.226)\n",
      "Epoch: [32][57/97]\tTime 0.008 (0.011)\tLoss 1.0426 (1.0456)\tPrec@1 64.648 (62.992)\n",
      "Epoch: [32][76/97]\tTime 0.011 (0.011)\tLoss 1.0260 (1.0514)\tPrec@1 60.742 (62.746)\n",
      "Epoch: [32][95/97]\tTime 0.008 (0.011)\tLoss 1.1060 (1.0597)\tPrec@1 59.375 (62.512)\n",
      "Epoch: [32][97/97]\tTime 0.011 (0.011)\tLoss 1.0565 (1.0604)\tPrec@1 63.690 (62.496)\n",
      "EPOCH: 32 train Results: Prec@1 62.496 Loss: 1.0604\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2608 (1.2608)\tPrec@1 56.055 (56.055)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4254 (1.2506)\tPrec@1 44.485 (55.340)\n",
      "EPOCH: 32 val Results: Prec@1 55.340 Loss: 1.2506\n",
      "Best Prec@1: 55.340\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/97]\tTime 0.010 (0.010)\tLoss 1.0769 (1.0769)\tPrec@1 59.570 (59.570)\n",
      "Epoch: [33][19/97]\tTime 0.007 (0.009)\tLoss 1.0853 (1.0160)\tPrec@1 60.938 (64.102)\n",
      "Epoch: [33][38/97]\tTime 0.016 (0.010)\tLoss 0.9994 (1.0222)\tPrec@1 66.211 (63.912)\n",
      "Epoch: [33][57/97]\tTime 0.014 (0.010)\tLoss 1.1144 (1.0284)\tPrec@1 57.617 (63.504)\n",
      "Epoch: [33][76/97]\tTime 0.017 (0.013)\tLoss 1.1041 (1.0419)\tPrec@1 61.719 (63.144)\n",
      "Epoch: [33][95/97]\tTime 0.033 (0.013)\tLoss 1.0042 (1.0489)\tPrec@1 63.672 (62.846)\n",
      "Epoch: [33][97/97]\tTime 0.012 (0.014)\tLoss 1.1332 (1.0504)\tPrec@1 59.226 (62.772)\n",
      "EPOCH: 33 train Results: Prec@1 62.772 Loss: 1.0504\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.2788 (1.2788)\tPrec@1 54.297 (54.297)\n",
      "Test: [19/19]\tTime 0.002 (0.005)\tLoss 1.3827 (1.2438)\tPrec@1 50.000 (55.150)\n",
      "EPOCH: 33 val Results: Prec@1 55.150 Loss: 1.2438\n",
      "Best Prec@1: 55.340\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/97]\tTime 0.032 (0.032)\tLoss 0.9658 (0.9658)\tPrec@1 66.602 (66.602)\n",
      "Epoch: [34][19/97]\tTime 0.027 (0.019)\tLoss 1.0419 (1.0067)\tPrec@1 64.453 (64.395)\n",
      "Epoch: [34][38/97]\tTime 0.031 (0.018)\tLoss 1.1027 (1.0162)\tPrec@1 60.547 (64.108)\n",
      "Epoch: [34][57/97]\tTime 0.013 (0.018)\tLoss 1.0465 (1.0246)\tPrec@1 62.695 (63.766)\n",
      "Epoch: [34][76/97]\tTime 0.009 (0.017)\tLoss 1.0926 (1.0352)\tPrec@1 61.914 (63.520)\n",
      "Epoch: [34][95/97]\tTime 0.014 (0.017)\tLoss 1.0479 (1.0458)\tPrec@1 63.086 (63.161)\n",
      "Epoch: [34][97/97]\tTime 0.010 (0.017)\tLoss 1.1069 (1.0465)\tPrec@1 58.631 (63.128)\n",
      "EPOCH: 34 train Results: Prec@1 63.128 Loss: 1.0465\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.2812 (1.2812)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3741 (1.2403)\tPrec@1 48.162 (55.340)\n",
      "EPOCH: 34 val Results: Prec@1 55.340 Loss: 1.2403\n",
      "Best Prec@1: 55.340\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/97]\tTime 0.014 (0.014)\tLoss 0.9267 (0.9267)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [35][19/97]\tTime 0.017 (0.014)\tLoss 1.0487 (1.0026)\tPrec@1 64.453 (65.068)\n",
      "Epoch: [35][38/97]\tTime 0.007 (0.014)\tLoss 0.9441 (1.0044)\tPrec@1 66.797 (64.593)\n",
      "Epoch: [35][57/97]\tTime 0.007 (0.013)\tLoss 1.0371 (1.0176)\tPrec@1 63.477 (64.197)\n",
      "Epoch: [35][76/97]\tTime 0.007 (0.017)\tLoss 1.0173 (1.0254)\tPrec@1 65.039 (63.847)\n",
      "Epoch: [35][95/97]\tTime 0.011 (0.017)\tLoss 1.0964 (1.0353)\tPrec@1 60.352 (63.509)\n",
      "Epoch: [35][97/97]\tTime 0.011 (0.017)\tLoss 1.0094 (1.0350)\tPrec@1 66.964 (63.546)\n",
      "EPOCH: 35 train Results: Prec@1 63.546 Loss: 1.0350\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2601 (1.2601)\tPrec@1 56.055 (56.055)\n",
      "Test: [19/19]\tTime 0.001 (0.008)\tLoss 1.4101 (1.2486)\tPrec@1 48.529 (55.380)\n",
      "EPOCH: 35 val Results: Prec@1 55.380 Loss: 1.2486\n",
      "Best Prec@1: 55.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/97]\tTime 0.015 (0.015)\tLoss 1.0900 (1.0900)\tPrec@1 60.742 (60.742)\n",
      "Epoch: [36][19/97]\tTime 0.079 (0.021)\tLoss 1.0172 (0.9924)\tPrec@1 65.234 (65.303)\n",
      "Epoch: [36][38/97]\tTime 0.055 (0.023)\tLoss 1.1605 (1.0090)\tPrec@1 60.156 (64.403)\n",
      "Epoch: [36][57/97]\tTime 0.019 (0.022)\tLoss 1.0018 (1.0126)\tPrec@1 64.844 (64.214)\n",
      "Epoch: [36][76/97]\tTime 0.033 (0.022)\tLoss 1.0521 (1.0237)\tPrec@1 63.477 (63.756)\n",
      "Epoch: [36][95/97]\tTime 0.011 (0.021)\tLoss 1.0920 (1.0344)\tPrec@1 64.258 (63.438)\n",
      "Epoch: [36][97/97]\tTime 0.005 (0.021)\tLoss 1.1765 (1.0361)\tPrec@1 58.929 (63.362)\n",
      "EPOCH: 36 train Results: Prec@1 63.362 Loss: 1.0361\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2605 (1.2605)\tPrec@1 56.250 (56.250)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4008 (1.2469)\tPrec@1 48.529 (54.970)\n",
      "EPOCH: 36 val Results: Prec@1 54.970 Loss: 1.2469\n",
      "Best Prec@1: 55.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/97]\tTime 0.009 (0.009)\tLoss 0.9387 (0.9387)\tPrec@1 67.578 (67.578)\n",
      "Epoch: [37][19/97]\tTime 0.016 (0.011)\tLoss 0.9962 (0.9772)\tPrec@1 64.062 (65.645)\n",
      "Epoch: [37][38/97]\tTime 0.019 (0.013)\tLoss 1.0203 (0.9972)\tPrec@1 64.844 (65.234)\n",
      "Epoch: [37][57/97]\tTime 0.018 (0.012)\tLoss 1.1411 (1.0083)\tPrec@1 59.766 (64.861)\n",
      "Epoch: [37][76/97]\tTime 0.014 (0.013)\tLoss 1.0125 (1.0189)\tPrec@1 63.477 (64.278)\n",
      "Epoch: [37][95/97]\tTime 0.029 (0.014)\tLoss 1.0540 (1.0253)\tPrec@1 62.305 (63.949)\n",
      "Epoch: [37][97/97]\tTime 0.015 (0.014)\tLoss 1.0686 (1.0258)\tPrec@1 61.607 (63.926)\n",
      "EPOCH: 37 train Results: Prec@1 63.926 Loss: 1.0258\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2512 (1.2512)\tPrec@1 56.641 (56.641)\n",
      "Test: [19/19]\tTime 0.024 (0.005)\tLoss 1.3794 (1.2441)\tPrec@1 50.000 (55.220)\n",
      "EPOCH: 37 val Results: Prec@1 55.220 Loss: 1.2441\n",
      "Best Prec@1: 55.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/97]\tTime 0.019 (0.019)\tLoss 0.9182 (0.9182)\tPrec@1 66.797 (66.797)\n",
      "Epoch: [38][19/97]\tTime 0.018 (0.018)\tLoss 0.9840 (0.9856)\tPrec@1 65.039 (65.420)\n",
      "Epoch: [38][38/97]\tTime 0.013 (0.017)\tLoss 0.9357 (0.9899)\tPrec@1 68.555 (65.169)\n",
      "Epoch: [38][57/97]\tTime 0.012 (0.016)\tLoss 0.9883 (1.0023)\tPrec@1 61.719 (64.729)\n",
      "Epoch: [38][76/97]\tTime 0.011 (0.015)\tLoss 1.1218 (1.0088)\tPrec@1 61.719 (64.448)\n",
      "Epoch: [38][95/97]\tTime 0.009 (0.014)\tLoss 1.0278 (1.0174)\tPrec@1 64.453 (64.109)\n",
      "Epoch: [38][97/97]\tTime 0.012 (0.014)\tLoss 1.1061 (1.0175)\tPrec@1 58.929 (64.088)\n",
      "EPOCH: 38 train Results: Prec@1 64.088 Loss: 1.0175\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2273 (1.2273)\tPrec@1 56.445 (56.445)\n",
      "Test: [19/19]\tTime 0.005 (0.004)\tLoss 1.3683 (1.2398)\tPrec@1 49.632 (55.470)\n",
      "EPOCH: 38 val Results: Prec@1 55.470 Loss: 1.2398\n",
      "Best Prec@1: 55.470\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/97]\tTime 0.016 (0.016)\tLoss 0.9584 (0.9584)\tPrec@1 65.820 (65.820)\n",
      "Epoch: [39][19/97]\tTime 0.016 (0.018)\tLoss 1.0547 (0.9613)\tPrec@1 63.672 (66.680)\n",
      "Epoch: [39][38/97]\tTime 0.035 (0.017)\tLoss 0.9941 (0.9814)\tPrec@1 66.016 (65.585)\n",
      "Epoch: [39][57/97]\tTime 0.081 (0.021)\tLoss 0.9990 (0.9961)\tPrec@1 63.086 (64.861)\n",
      "Epoch: [39][76/97]\tTime 0.012 (0.022)\tLoss 1.0927 (1.0027)\tPrec@1 62.695 (64.712)\n",
      "Epoch: [39][95/97]\tTime 0.018 (0.020)\tLoss 1.1286 (1.0142)\tPrec@1 62.305 (64.231)\n",
      "Epoch: [39][97/97]\tTime 0.009 (0.020)\tLoss 1.0637 (1.0151)\tPrec@1 62.798 (64.204)\n",
      "EPOCH: 39 train Results: Prec@1 64.204 Loss: 1.0151\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2568 (1.2568)\tPrec@1 58.398 (58.398)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4267 (1.2476)\tPrec@1 45.956 (55.590)\n",
      "EPOCH: 39 val Results: Prec@1 55.590 Loss: 1.2476\n",
      "Best Prec@1: 55.590\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/97]\tTime 0.007 (0.007)\tLoss 0.9914 (0.9914)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [40][19/97]\tTime 0.007 (0.012)\tLoss 1.0539 (0.9693)\tPrec@1 62.891 (66.426)\n",
      "Epoch: [40][38/97]\tTime 0.009 (0.016)\tLoss 0.9506 (0.9754)\tPrec@1 67.578 (66.216)\n",
      "Epoch: [40][57/97]\tTime 0.007 (0.016)\tLoss 1.0576 (0.9829)\tPrec@1 63.672 (65.652)\n",
      "Epoch: [40][76/97]\tTime 0.008 (0.015)\tLoss 1.0934 (0.9955)\tPrec@1 60.938 (65.102)\n",
      "Epoch: [40][95/97]\tTime 0.022 (0.015)\tLoss 1.0334 (1.0087)\tPrec@1 62.500 (64.473)\n",
      "Epoch: [40][97/97]\tTime 0.014 (0.015)\tLoss 1.0952 (1.0102)\tPrec@1 62.500 (64.436)\n",
      "EPOCH: 40 train Results: Prec@1 64.436 Loss: 1.0102\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2653 (1.2653)\tPrec@1 57.422 (57.422)\n",
      "Test: [19/19]\tTime 0.001 (0.005)\tLoss 1.4313 (1.2407)\tPrec@1 48.897 (55.710)\n",
      "EPOCH: 40 val Results: Prec@1 55.710 Loss: 1.2407\n",
      "Best Prec@1: 55.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/97]\tTime 0.023 (0.023)\tLoss 0.9900 (0.9900)\tPrec@1 65.234 (65.234)\n",
      "Epoch: [41][19/97]\tTime 0.012 (0.022)\tLoss 0.9917 (0.9596)\tPrec@1 68.945 (66.523)\n",
      "Epoch: [41][38/97]\tTime 0.010 (0.019)\tLoss 1.0601 (0.9752)\tPrec@1 62.695 (65.905)\n",
      "Epoch: [41][57/97]\tTime 0.014 (0.016)\tLoss 0.9586 (0.9845)\tPrec@1 69.727 (65.622)\n",
      "Epoch: [41][76/97]\tTime 0.010 (0.016)\tLoss 1.0199 (0.9965)\tPrec@1 62.109 (65.021)\n",
      "Epoch: [41][95/97]\tTime 0.017 (0.017)\tLoss 1.0795 (1.0043)\tPrec@1 61.523 (64.671)\n",
      "Epoch: [41][97/97]\tTime 0.010 (0.017)\tLoss 1.0328 (1.0055)\tPrec@1 61.310 (64.614)\n",
      "EPOCH: 41 train Results: Prec@1 64.614 Loss: 1.0055\n",
      "Test: [0/19]\tTime 0.007 (0.007)\tLoss 1.2057 (1.2057)\tPrec@1 59.766 (59.766)\n",
      "Test: [19/19]\tTime 0.007 (0.006)\tLoss 1.3925 (1.2288)\tPrec@1 47.059 (56.450)\n",
      "EPOCH: 41 val Results: Prec@1 56.450 Loss: 1.2288\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/97]\tTime 0.020 (0.020)\tLoss 0.9271 (0.9271)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [42][19/97]\tTime 0.012 (0.034)\tLoss 0.9594 (0.9576)\tPrec@1 64.648 (66.191)\n",
      "Epoch: [42][38/97]\tTime 0.034 (0.028)\tLoss 0.9946 (0.9641)\tPrec@1 63.672 (65.815)\n",
      "Epoch: [42][57/97]\tTime 0.014 (0.025)\tLoss 1.0250 (0.9785)\tPrec@1 64.258 (65.436)\n",
      "Epoch: [42][76/97]\tTime 0.013 (0.025)\tLoss 1.0277 (0.9859)\tPrec@1 61.523 (65.120)\n",
      "Epoch: [42][95/97]\tTime 0.014 (0.022)\tLoss 1.0446 (0.9967)\tPrec@1 64.062 (64.793)\n",
      "Epoch: [42][97/97]\tTime 0.022 (0.022)\tLoss 1.0027 (0.9980)\tPrec@1 64.583 (64.730)\n",
      "EPOCH: 42 train Results: Prec@1 64.730 Loss: 0.9980\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2186 (1.2186)\tPrec@1 59.375 (59.375)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3947 (1.2327)\tPrec@1 49.265 (55.750)\n",
      "EPOCH: 42 val Results: Prec@1 55.750 Loss: 1.2327\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/97]\tTime 0.014 (0.014)\tLoss 0.9432 (0.9432)\tPrec@1 68.164 (68.164)\n",
      "Epoch: [43][19/97]\tTime 0.009 (0.010)\tLoss 0.9457 (0.9596)\tPrec@1 66.406 (66.201)\n",
      "Epoch: [43][38/97]\tTime 0.012 (0.010)\tLoss 0.9917 (0.9731)\tPrec@1 64.844 (65.805)\n",
      "Epoch: [43][57/97]\tTime 0.008 (0.010)\tLoss 0.9946 (0.9795)\tPrec@1 65.039 (65.477)\n",
      "Epoch: [43][76/97]\tTime 0.011 (0.010)\tLoss 1.1168 (0.9945)\tPrec@1 61.328 (65.087)\n",
      "Epoch: [43][95/97]\tTime 0.012 (0.010)\tLoss 1.0966 (1.0019)\tPrec@1 61.328 (64.762)\n",
      "Epoch: [43][97/97]\tTime 0.010 (0.010)\tLoss 0.9616 (1.0010)\tPrec@1 64.286 (64.774)\n",
      "EPOCH: 43 train Results: Prec@1 64.774 Loss: 1.0010\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2406 (1.2406)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4076 (1.2308)\tPrec@1 48.897 (55.970)\n",
      "EPOCH: 43 val Results: Prec@1 55.970 Loss: 1.2308\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/97]\tTime 0.009 (0.009)\tLoss 0.9472 (0.9472)\tPrec@1 67.773 (67.773)\n",
      "Epoch: [44][19/97]\tTime 0.008 (0.011)\tLoss 0.8845 (0.9516)\tPrec@1 67.383 (66.992)\n",
      "Epoch: [44][38/97]\tTime 0.009 (0.010)\tLoss 0.9714 (0.9602)\tPrec@1 66.602 (66.486)\n",
      "Epoch: [44][57/97]\tTime 0.025 (0.013)\tLoss 1.0315 (0.9793)\tPrec@1 65.234 (65.595)\n",
      "Epoch: [44][76/97]\tTime 0.017 (0.015)\tLoss 1.0269 (0.9864)\tPrec@1 66.992 (65.290)\n",
      "Epoch: [44][95/97]\tTime 0.021 (0.014)\tLoss 1.0302 (0.9929)\tPrec@1 65.234 (65.021)\n",
      "Epoch: [44][97/97]\tTime 0.005 (0.014)\tLoss 0.9793 (0.9930)\tPrec@1 68.750 (65.026)\n",
      "EPOCH: 44 train Results: Prec@1 65.026 Loss: 0.9930\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2057 (1.2057)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.001 (0.006)\tLoss 1.3625 (1.2237)\tPrec@1 49.265 (56.260)\n",
      "EPOCH: 44 val Results: Prec@1 56.260 Loss: 1.2237\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/97]\tTime 0.019 (0.019)\tLoss 0.9616 (0.9616)\tPrec@1 65.234 (65.234)\n",
      "Epoch: [45][19/97]\tTime 0.007 (0.015)\tLoss 0.9923 (0.9417)\tPrec@1 66.797 (66.641)\n",
      "Epoch: [45][38/97]\tTime 0.018 (0.013)\tLoss 1.0169 (0.9481)\tPrec@1 66.797 (66.501)\n",
      "Epoch: [45][57/97]\tTime 0.013 (0.012)\tLoss 1.0383 (0.9632)\tPrec@1 62.109 (65.780)\n",
      "Epoch: [45][76/97]\tTime 0.012 (0.012)\tLoss 0.9877 (0.9758)\tPrec@1 65.430 (65.376)\n",
      "Epoch: [45][95/97]\tTime 0.007 (0.012)\tLoss 0.9655 (0.9834)\tPrec@1 66.602 (65.108)\n",
      "Epoch: [45][97/97]\tTime 0.004 (0.012)\tLoss 1.0220 (0.9838)\tPrec@1 63.393 (65.104)\n",
      "EPOCH: 45 train Results: Prec@1 65.104 Loss: 0.9838\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2237 (1.2237)\tPrec@1 56.836 (56.836)\n",
      "Test: [19/19]\tTime 0.003 (0.003)\tLoss 1.3639 (1.2444)\tPrec@1 48.897 (55.430)\n",
      "EPOCH: 45 val Results: Prec@1 55.430 Loss: 1.2444\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/97]\tTime 0.013 (0.013)\tLoss 0.9082 (0.9082)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [46][19/97]\tTime 0.011 (0.013)\tLoss 0.9542 (0.9418)\tPrec@1 65.430 (66.904)\n",
      "Epoch: [46][38/97]\tTime 0.007 (0.011)\tLoss 0.9296 (0.9461)\tPrec@1 68.750 (66.632)\n",
      "Epoch: [46][57/97]\tTime 0.007 (0.011)\tLoss 1.0252 (0.9621)\tPrec@1 63.672 (65.928)\n",
      "Epoch: [46][76/97]\tTime 0.014 (0.011)\tLoss 1.0037 (0.9729)\tPrec@1 64.648 (65.554)\n",
      "Epoch: [46][95/97]\tTime 0.008 (0.011)\tLoss 1.0379 (0.9826)\tPrec@1 62.891 (65.249)\n",
      "Epoch: [46][97/97]\tTime 0.005 (0.011)\tLoss 1.0146 (0.9838)\tPrec@1 62.500 (65.180)\n",
      "EPOCH: 46 train Results: Prec@1 65.180 Loss: 0.9838\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2566 (1.2566)\tPrec@1 53.906 (53.906)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.3427 (1.2373)\tPrec@1 48.529 (56.080)\n",
      "EPOCH: 46 val Results: Prec@1 56.080 Loss: 1.2373\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/97]\tTime 0.011 (0.011)\tLoss 1.0179 (1.0179)\tPrec@1 65.430 (65.430)\n",
      "Epoch: [47][19/97]\tTime 0.014 (0.011)\tLoss 0.9719 (0.9307)\tPrec@1 64.648 (67.510)\n",
      "Epoch: [47][38/97]\tTime 0.008 (0.010)\tLoss 1.0370 (0.9481)\tPrec@1 63.086 (66.877)\n",
      "Epoch: [47][57/97]\tTime 0.010 (0.010)\tLoss 0.9099 (0.9594)\tPrec@1 67.188 (66.453)\n",
      "Epoch: [47][76/97]\tTime 0.011 (0.010)\tLoss 1.0028 (0.9684)\tPrec@1 61.914 (65.985)\n",
      "Epoch: [47][95/97]\tTime 0.013 (0.010)\tLoss 1.0218 (0.9767)\tPrec@1 65.039 (65.672)\n",
      "Epoch: [47][97/97]\tTime 0.012 (0.010)\tLoss 1.1162 (0.9779)\tPrec@1 57.143 (65.606)\n",
      "EPOCH: 47 train Results: Prec@1 65.606 Loss: 0.9779\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2242 (1.2242)\tPrec@1 56.250 (56.250)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.3735 (1.2320)\tPrec@1 50.368 (56.130)\n",
      "EPOCH: 47 val Results: Prec@1 56.130 Loss: 1.2320\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/97]\tTime 0.012 (0.012)\tLoss 0.9534 (0.9534)\tPrec@1 68.945 (68.945)\n",
      "Epoch: [48][19/97]\tTime 0.009 (0.010)\tLoss 1.0094 (0.9474)\tPrec@1 63.867 (66.758)\n",
      "Epoch: [48][38/97]\tTime 0.010 (0.010)\tLoss 1.0009 (0.9528)\tPrec@1 63.477 (66.456)\n",
      "Epoch: [48][57/97]\tTime 0.010 (0.011)\tLoss 0.9775 (0.9607)\tPrec@1 64.844 (66.171)\n",
      "Epoch: [48][76/97]\tTime 0.014 (0.012)\tLoss 0.9642 (0.9694)\tPrec@1 65.039 (65.711)\n",
      "Epoch: [48][95/97]\tTime 0.008 (0.012)\tLoss 0.9635 (0.9768)\tPrec@1 65.625 (65.422)\n",
      "Epoch: [48][97/97]\tTime 0.006 (0.012)\tLoss 1.0602 (0.9787)\tPrec@1 59.524 (65.358)\n",
      "EPOCH: 48 train Results: Prec@1 65.358 Loss: 0.9787\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2380 (1.2380)\tPrec@1 55.664 (55.664)\n",
      "Test: [19/19]\tTime 0.003 (0.003)\tLoss 1.3654 (1.2385)\tPrec@1 46.691 (55.800)\n",
      "EPOCH: 48 val Results: Prec@1 55.800 Loss: 1.2385\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/97]\tTime 0.015 (0.015)\tLoss 0.9881 (0.9881)\tPrec@1 63.672 (63.672)\n",
      "Epoch: [49][19/97]\tTime 0.010 (0.017)\tLoss 0.9255 (0.9202)\tPrec@1 66.797 (67.725)\n",
      "Epoch: [49][38/97]\tTime 0.017 (0.023)\tLoss 0.9669 (0.9402)\tPrec@1 67.578 (66.907)\n",
      "Epoch: [49][57/97]\tTime 0.017 (0.023)\tLoss 0.9956 (0.9538)\tPrec@1 65.625 (66.362)\n",
      "Epoch: [49][76/97]\tTime 0.016 (0.025)\tLoss 1.0067 (0.9625)\tPrec@1 65.430 (66.005)\n",
      "Epoch: [49][95/97]\tTime 0.016 (0.024)\tLoss 1.0380 (0.9711)\tPrec@1 61.719 (65.686)\n",
      "Epoch: [49][97/97]\tTime 0.019 (0.024)\tLoss 1.0712 (0.9722)\tPrec@1 61.607 (65.650)\n",
      "EPOCH: 49 train Results: Prec@1 65.650 Loss: 0.9722\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2097 (1.2097)\tPrec@1 56.250 (56.250)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.3761 (1.2318)\tPrec@1 47.794 (56.010)\n",
      "EPOCH: 49 val Results: Prec@1 56.010 Loss: 1.2318\n",
      "Best Prec@1: 56.450\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/97]\tTime 0.011 (0.011)\tLoss 0.9107 (0.9107)\tPrec@1 68.359 (68.359)\n",
      "Epoch: [50][19/97]\tTime 0.011 (0.014)\tLoss 0.9808 (0.9205)\tPrec@1 62.891 (67.881)\n",
      "Epoch: [50][38/97]\tTime 0.010 (0.016)\tLoss 0.9244 (0.9316)\tPrec@1 67.969 (67.458)\n",
      "Epoch: [50][57/97]\tTime 0.009 (0.015)\tLoss 0.9943 (0.9435)\tPrec@1 64.453 (66.891)\n",
      "Epoch: [50][76/97]\tTime 0.008 (0.016)\tLoss 1.0235 (0.9577)\tPrec@1 64.258 (66.307)\n",
      "Epoch: [50][95/97]\tTime 0.014 (0.015)\tLoss 1.0213 (0.9674)\tPrec@1 62.305 (65.873)\n",
      "Epoch: [50][97/97]\tTime 0.008 (0.015)\tLoss 1.0162 (0.9684)\tPrec@1 64.286 (65.860)\n",
      "EPOCH: 50 train Results: Prec@1 65.860 Loss: 0.9684\n",
      "Test: [0/19]\tTime 0.014 (0.014)\tLoss 1.2058 (1.2058)\tPrec@1 58.398 (58.398)\n",
      "Test: [19/19]\tTime 0.001 (0.005)\tLoss 1.3941 (1.2352)\tPrec@1 50.735 (56.550)\n",
      "EPOCH: 50 val Results: Prec@1 56.550 Loss: 1.2352\n",
      "Best Prec@1: 56.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/97]\tTime 0.032 (0.032)\tLoss 0.9618 (0.9618)\tPrec@1 65.430 (65.430)\n",
      "Epoch: [51][19/97]\tTime 0.010 (0.015)\tLoss 0.9666 (0.9349)\tPrec@1 66.406 (67.119)\n",
      "Epoch: [51][38/97]\tTime 0.007 (0.016)\tLoss 1.0052 (0.9460)\tPrec@1 63.086 (66.702)\n",
      "Epoch: [51][57/97]\tTime 0.011 (0.015)\tLoss 0.9561 (0.9480)\tPrec@1 66.992 (66.484)\n",
      "Epoch: [51][76/97]\tTime 0.013 (0.016)\tLoss 1.0443 (0.9563)\tPrec@1 62.500 (66.216)\n",
      "Epoch: [51][95/97]\tTime 0.014 (0.017)\tLoss 0.9507 (0.9669)\tPrec@1 65.625 (65.843)\n",
      "Epoch: [51][97/97]\tTime 0.011 (0.017)\tLoss 1.0326 (0.9682)\tPrec@1 61.905 (65.794)\n",
      "EPOCH: 51 train Results: Prec@1 65.794 Loss: 0.9682\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.2156 (1.2156)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.001 (0.005)\tLoss 1.4099 (1.2359)\tPrec@1 48.162 (56.540)\n",
      "EPOCH: 51 val Results: Prec@1 56.540 Loss: 1.2359\n",
      "Best Prec@1: 56.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/97]\tTime 0.018 (0.018)\tLoss 0.9602 (0.9602)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [52][19/97]\tTime 0.010 (0.015)\tLoss 0.9425 (0.9129)\tPrec@1 64.258 (68.115)\n",
      "Epoch: [52][38/97]\tTime 0.017 (0.014)\tLoss 1.0223 (0.9263)\tPrec@1 64.453 (67.493)\n",
      "Epoch: [52][57/97]\tTime 0.016 (0.014)\tLoss 0.9006 (0.9362)\tPrec@1 69.922 (67.110)\n",
      "Epoch: [52][76/97]\tTime 0.007 (0.014)\tLoss 1.0808 (0.9463)\tPrec@1 61.523 (66.627)\n",
      "Epoch: [52][95/97]\tTime 0.009 (0.014)\tLoss 0.9945 (0.9577)\tPrec@1 63.867 (66.152)\n",
      "Epoch: [52][97/97]\tTime 0.007 (0.014)\tLoss 1.1498 (0.9598)\tPrec@1 60.119 (66.096)\n",
      "EPOCH: 52 train Results: Prec@1 66.096 Loss: 0.9598\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2218 (1.2218)\tPrec@1 58.203 (58.203)\n",
      "Test: [19/19]\tTime 0.002 (0.004)\tLoss 1.3967 (1.2325)\tPrec@1 51.103 (56.170)\n",
      "EPOCH: 52 val Results: Prec@1 56.170 Loss: 1.2325\n",
      "Best Prec@1: 56.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/97]\tTime 0.024 (0.024)\tLoss 0.8851 (0.8851)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [53][19/97]\tTime 0.009 (0.015)\tLoss 0.8580 (0.8998)\tPrec@1 69.727 (68.477)\n",
      "Epoch: [53][38/97]\tTime 0.012 (0.016)\tLoss 0.9428 (0.9197)\tPrec@1 65.820 (67.553)\n",
      "Epoch: [53][57/97]\tTime 0.022 (0.014)\tLoss 0.9908 (0.9361)\tPrec@1 62.500 (66.824)\n",
      "Epoch: [53][76/97]\tTime 0.014 (0.015)\tLoss 1.0197 (0.9455)\tPrec@1 63.672 (66.454)\n",
      "Epoch: [53][95/97]\tTime 0.031 (0.016)\tLoss 1.0512 (0.9567)\tPrec@1 62.109 (66.089)\n",
      "Epoch: [53][97/97]\tTime 0.010 (0.016)\tLoss 1.1297 (0.9579)\tPrec@1 58.631 (66.054)\n",
      "EPOCH: 53 train Results: Prec@1 66.054 Loss: 0.9579\n",
      "Test: [0/19]\tTime 0.008 (0.008)\tLoss 1.2237 (1.2237)\tPrec@1 55.859 (55.859)\n",
      "Test: [19/19]\tTime 0.002 (0.011)\tLoss 1.3850 (1.2327)\tPrec@1 48.529 (56.360)\n",
      "EPOCH: 53 val Results: Prec@1 56.360 Loss: 1.2327\n",
      "Best Prec@1: 56.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/97]\tTime 0.017 (0.017)\tLoss 0.8453 (0.8453)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [54][19/97]\tTime 0.011 (0.015)\tLoss 0.9520 (0.9112)\tPrec@1 65.039 (68.125)\n",
      "Epoch: [54][38/97]\tTime 0.010 (0.017)\tLoss 0.9421 (0.9221)\tPrec@1 66.016 (67.418)\n",
      "Epoch: [54][57/97]\tTime 0.010 (0.015)\tLoss 0.9943 (0.9364)\tPrec@1 65.625 (66.925)\n",
      "Epoch: [54][76/97]\tTime 0.008 (0.014)\tLoss 1.0329 (0.9472)\tPrec@1 61.719 (66.558)\n",
      "Epoch: [54][95/97]\tTime 0.015 (0.015)\tLoss 0.9327 (0.9580)\tPrec@1 66.211 (66.172)\n",
      "Epoch: [54][97/97]\tTime 0.005 (0.015)\tLoss 1.0512 (0.9594)\tPrec@1 58.929 (66.070)\n",
      "EPOCH: 54 train Results: Prec@1 66.070 Loss: 0.9594\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.1980 (1.1980)\tPrec@1 59.180 (59.180)\n",
      "Test: [19/19]\tTime 0.008 (0.005)\tLoss 1.3856 (1.2284)\tPrec@1 48.529 (56.490)\n",
      "EPOCH: 54 val Results: Prec@1 56.490 Loss: 1.2284\n",
      "Best Prec@1: 56.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/97]\tTime 0.017 (0.017)\tLoss 0.8884 (0.8884)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [55][19/97]\tTime 0.026 (0.012)\tLoss 0.9266 (0.9066)\tPrec@1 68.555 (67.900)\n",
      "Epoch: [55][38/97]\tTime 0.013 (0.022)\tLoss 0.9809 (0.9277)\tPrec@1 63.086 (67.223)\n",
      "Epoch: [55][57/97]\tTime 0.020 (0.021)\tLoss 0.9773 (0.9370)\tPrec@1 66.406 (67.006)\n",
      "Epoch: [55][76/97]\tTime 0.016 (0.019)\tLoss 0.9887 (0.9444)\tPrec@1 62.109 (66.596)\n",
      "Epoch: [55][95/97]\tTime 0.011 (0.017)\tLoss 1.0099 (0.9537)\tPrec@1 66.016 (66.260)\n",
      "Epoch: [55][97/97]\tTime 0.007 (0.017)\tLoss 1.0240 (0.9546)\tPrec@1 66.667 (66.250)\n",
      "EPOCH: 55 train Results: Prec@1 66.250 Loss: 0.9546\n",
      "Test: [0/19]\tTime 0.009 (0.009)\tLoss 1.2517 (1.2517)\tPrec@1 57.031 (57.031)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3818 (1.2332)\tPrec@1 50.368 (56.480)\n",
      "EPOCH: 55 val Results: Prec@1 56.480 Loss: 1.2332\n",
      "Best Prec@1: 56.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/97]\tTime 0.009 (0.009)\tLoss 0.8799 (0.8799)\tPrec@1 68.945 (68.945)\n",
      "Epoch: [56][19/97]\tTime 0.014 (0.011)\tLoss 0.9393 (0.9131)\tPrec@1 66.797 (67.715)\n",
      "Epoch: [56][38/97]\tTime 0.018 (0.014)\tLoss 0.8839 (0.9126)\tPrec@1 68.359 (67.874)\n",
      "Epoch: [56][57/97]\tTime 0.011 (0.014)\tLoss 0.9506 (0.9208)\tPrec@1 65.430 (67.494)\n",
      "Epoch: [56][76/97]\tTime 0.007 (0.013)\tLoss 0.9246 (0.9328)\tPrec@1 67.969 (67.053)\n",
      "Epoch: [56][95/97]\tTime 0.011 (0.013)\tLoss 0.9788 (0.9456)\tPrec@1 65.430 (66.618)\n",
      "Epoch: [56][97/97]\tTime 0.005 (0.012)\tLoss 0.9744 (0.9460)\tPrec@1 64.881 (66.588)\n",
      "EPOCH: 56 train Results: Prec@1 66.588 Loss: 0.9460\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.1786 (1.1786)\tPrec@1 57.617 (57.617)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4312 (1.2310)\tPrec@1 47.059 (56.010)\n",
      "EPOCH: 56 val Results: Prec@1 56.010 Loss: 1.2310\n",
      "Best Prec@1: 56.550\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/97]\tTime 0.009 (0.009)\tLoss 0.9742 (0.9742)\tPrec@1 66.602 (66.602)\n",
      "Epoch: [57][19/97]\tTime 0.007 (0.010)\tLoss 0.8366 (0.8938)\tPrec@1 72.266 (68.740)\n",
      "Epoch: [57][38/97]\tTime 0.010 (0.010)\tLoss 0.9281 (0.9114)\tPrec@1 69.922 (68.084)\n",
      "Epoch: [57][57/97]\tTime 0.021 (0.010)\tLoss 0.9558 (0.9198)\tPrec@1 63.867 (67.457)\n",
      "Epoch: [57][76/97]\tTime 0.018 (0.013)\tLoss 0.9768 (0.9312)\tPrec@1 66.406 (67.089)\n",
      "Epoch: [57][95/97]\tTime 0.009 (0.012)\tLoss 1.0383 (0.9471)\tPrec@1 64.844 (66.634)\n",
      "Epoch: [57][97/97]\tTime 0.008 (0.012)\tLoss 0.8829 (0.9472)\tPrec@1 66.964 (66.632)\n",
      "EPOCH: 57 train Results: Prec@1 66.632 Loss: 0.9472\n",
      "Test: [0/19]\tTime 0.008 (0.008)\tLoss 1.2110 (1.2110)\tPrec@1 57.617 (57.617)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4114 (1.2252)\tPrec@1 48.897 (56.670)\n",
      "EPOCH: 57 val Results: Prec@1 56.670 Loss: 1.2252\n",
      "Best Prec@1: 56.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/97]\tTime 0.007 (0.007)\tLoss 0.9199 (0.9199)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [58][19/97]\tTime 0.007 (0.009)\tLoss 0.9364 (0.9089)\tPrec@1 68.359 (68.262)\n",
      "Epoch: [58][38/97]\tTime 0.011 (0.010)\tLoss 0.9587 (0.9191)\tPrec@1 65.820 (67.593)\n",
      "Epoch: [58][57/97]\tTime 0.037 (0.014)\tLoss 0.9340 (0.9278)\tPrec@1 69.336 (67.275)\n",
      "Epoch: [58][76/97]\tTime 0.018 (0.015)\tLoss 0.9196 (0.9365)\tPrec@1 67.773 (67.099)\n",
      "Epoch: [58][95/97]\tTime 0.019 (0.015)\tLoss 0.9877 (0.9470)\tPrec@1 65.625 (66.703)\n",
      "Epoch: [58][97/97]\tTime 0.018 (0.015)\tLoss 0.9282 (0.9476)\tPrec@1 66.964 (66.672)\n",
      "EPOCH: 58 train Results: Prec@1 66.672 Loss: 0.9476\n",
      "Test: [0/19]\tTime 0.008 (0.008)\tLoss 1.2310 (1.2310)\tPrec@1 55.859 (55.859)\n",
      "Test: [19/19]\tTime 0.011 (0.007)\tLoss 1.3888 (1.2286)\tPrec@1 48.897 (55.980)\n",
      "EPOCH: 58 val Results: Prec@1 55.980 Loss: 1.2286\n",
      "Best Prec@1: 56.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/97]\tTime 0.031 (0.031)\tLoss 0.9315 (0.9315)\tPrec@1 67.773 (67.773)\n",
      "Epoch: [59][19/97]\tTime 0.009 (0.019)\tLoss 0.9393 (0.8969)\tPrec@1 66.602 (68.838)\n",
      "Epoch: [59][38/97]\tTime 0.013 (0.019)\tLoss 0.8462 (0.9028)\tPrec@1 68.750 (68.500)\n",
      "Epoch: [59][57/97]\tTime 0.011 (0.018)\tLoss 0.9601 (0.9183)\tPrec@1 66.211 (67.787)\n",
      "Epoch: [59][76/97]\tTime 0.032 (0.019)\tLoss 0.9086 (0.9288)\tPrec@1 67.773 (67.390)\n",
      "Epoch: [59][95/97]\tTime 0.015 (0.019)\tLoss 1.0118 (0.9396)\tPrec@1 63.672 (66.933)\n",
      "Epoch: [59][97/97]\tTime 0.008 (0.018)\tLoss 1.0057 (0.9411)\tPrec@1 67.262 (66.888)\n",
      "EPOCH: 59 train Results: Prec@1 66.888 Loss: 0.9411\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2333 (1.2333)\tPrec@1 56.445 (56.445)\n",
      "Test: [19/19]\tTime 0.006 (0.004)\tLoss 1.3528 (1.2377)\tPrec@1 49.265 (55.960)\n",
      "EPOCH: 59 val Results: Prec@1 55.960 Loss: 1.2377\n",
      "Best Prec@1: 56.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/97]\tTime 0.012 (0.012)\tLoss 0.8765 (0.8765)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [60][19/97]\tTime 0.008 (0.012)\tLoss 0.8901 (0.8962)\tPrec@1 66.406 (68.623)\n",
      "Epoch: [60][38/97]\tTime 0.014 (0.011)\tLoss 0.8978 (0.9060)\tPrec@1 69.141 (68.535)\n",
      "Epoch: [60][57/97]\tTime 0.011 (0.011)\tLoss 0.9617 (0.9143)\tPrec@1 65.820 (68.016)\n",
      "Epoch: [60][76/97]\tTime 0.008 (0.011)\tLoss 0.9606 (0.9260)\tPrec@1 66.797 (67.492)\n",
      "Epoch: [60][95/97]\tTime 0.014 (0.011)\tLoss 0.9670 (0.9362)\tPrec@1 64.453 (66.947)\n",
      "Epoch: [60][97/97]\tTime 0.007 (0.011)\tLoss 1.0044 (0.9374)\tPrec@1 65.179 (66.910)\n",
      "EPOCH: 60 train Results: Prec@1 66.910 Loss: 0.9374\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2242 (1.2242)\tPrec@1 56.836 (56.836)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4016 (1.2431)\tPrec@1 47.794 (56.090)\n",
      "EPOCH: 60 val Results: Prec@1 56.090 Loss: 1.2431\n",
      "Best Prec@1: 56.670\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/97]\tTime 0.009 (0.009)\tLoss 0.8018 (0.8018)\tPrec@1 71.094 (71.094)\n",
      "Epoch: [61][19/97]\tTime 0.010 (0.011)\tLoss 0.8791 (0.8915)\tPrec@1 67.188 (68.975)\n",
      "Epoch: [61][38/97]\tTime 0.013 (0.012)\tLoss 0.8692 (0.8980)\tPrec@1 70.508 (68.555)\n",
      "Epoch: [61][57/97]\tTime 0.023 (0.012)\tLoss 0.9623 (0.9133)\tPrec@1 65.820 (67.962)\n",
      "Epoch: [61][76/97]\tTime 0.023 (0.012)\tLoss 1.0500 (0.9257)\tPrec@1 61.914 (67.411)\n",
      "Epoch: [61][95/97]\tTime 0.010 (0.012)\tLoss 0.9382 (0.9353)\tPrec@1 66.992 (67.047)\n",
      "Epoch: [61][97/97]\tTime 0.016 (0.012)\tLoss 0.9471 (0.9363)\tPrec@1 65.476 (66.990)\n",
      "EPOCH: 61 train Results: Prec@1 66.990 Loss: 0.9363\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.2347 (1.2347)\tPrec@1 56.641 (56.641)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.3637 (1.2293)\tPrec@1 50.000 (56.710)\n",
      "EPOCH: 61 val Results: Prec@1 56.710 Loss: 1.2293\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/97]\tTime 0.010 (0.010)\tLoss 0.9009 (0.9009)\tPrec@1 69.727 (69.727)\n",
      "Epoch: [62][19/97]\tTime 0.014 (0.011)\tLoss 0.9349 (0.8856)\tPrec@1 69.531 (69.512)\n",
      "Epoch: [62][38/97]\tTime 0.016 (0.011)\tLoss 0.8925 (0.8979)\tPrec@1 68.750 (68.545)\n",
      "Epoch: [62][57/97]\tTime 0.007 (0.013)\tLoss 0.9615 (0.9107)\tPrec@1 65.430 (68.060)\n",
      "Epoch: [62][76/97]\tTime 0.011 (0.012)\tLoss 0.9944 (0.9212)\tPrec@1 65.234 (67.545)\n",
      "Epoch: [62][95/97]\tTime 0.014 (0.012)\tLoss 0.9551 (0.9313)\tPrec@1 66.211 (67.120)\n",
      "Epoch: [62][97/97]\tTime 0.007 (0.012)\tLoss 0.9931 (0.9326)\tPrec@1 65.179 (67.080)\n",
      "EPOCH: 62 train Results: Prec@1 67.080 Loss: 0.9326\n",
      "Test: [0/19]\tTime 0.007 (0.007)\tLoss 1.2375 (1.2375)\tPrec@1 57.031 (57.031)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3795 (1.2395)\tPrec@1 48.529 (56.050)\n",
      "EPOCH: 62 val Results: Prec@1 56.050 Loss: 1.2395\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/97]\tTime 0.008 (0.008)\tLoss 0.8276 (0.8276)\tPrec@1 72.070 (72.070)\n",
      "Epoch: [63][19/97]\tTime 0.019 (0.010)\tLoss 0.9091 (0.8760)\tPrec@1 68.945 (69.043)\n",
      "Epoch: [63][38/97]\tTime 0.007 (0.010)\tLoss 0.9655 (0.8982)\tPrec@1 65.234 (68.269)\n",
      "Epoch: [63][57/97]\tTime 0.014 (0.010)\tLoss 1.0117 (0.9102)\tPrec@1 64.062 (67.908)\n",
      "Epoch: [63][76/97]\tTime 0.007 (0.010)\tLoss 0.9845 (0.9232)\tPrec@1 63.672 (67.297)\n",
      "Epoch: [63][95/97]\tTime 0.007 (0.010)\tLoss 0.9952 (0.9322)\tPrec@1 62.695 (66.890)\n",
      "Epoch: [63][97/97]\tTime 0.005 (0.010)\tLoss 0.9731 (0.9330)\tPrec@1 65.774 (66.878)\n",
      "EPOCH: 63 train Results: Prec@1 66.878 Loss: 0.9330\n",
      "Test: [0/19]\tTime 0.012 (0.012)\tLoss 1.2610 (1.2610)\tPrec@1 56.641 (56.641)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3490 (1.2278)\tPrec@1 48.897 (56.690)\n",
      "EPOCH: 63 val Results: Prec@1 56.690 Loss: 1.2278\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/97]\tTime 0.007 (0.007)\tLoss 0.8344 (0.8344)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [64][19/97]\tTime 0.007 (0.010)\tLoss 0.8649 (0.8760)\tPrec@1 68.945 (68.799)\n",
      "Epoch: [64][38/97]\tTime 0.008 (0.010)\tLoss 0.9175 (0.8898)\tPrec@1 67.383 (68.485)\n",
      "Epoch: [64][57/97]\tTime 0.012 (0.011)\tLoss 0.9540 (0.9022)\tPrec@1 64.844 (67.952)\n",
      "Epoch: [64][76/97]\tTime 0.023 (0.011)\tLoss 0.9355 (0.9153)\tPrec@1 67.969 (67.477)\n",
      "Epoch: [64][95/97]\tTime 0.010 (0.011)\tLoss 0.9497 (0.9265)\tPrec@1 66.797 (67.069)\n",
      "Epoch: [64][97/97]\tTime 0.011 (0.011)\tLoss 0.9708 (0.9276)\tPrec@1 65.774 (67.050)\n",
      "EPOCH: 64 train Results: Prec@1 67.050 Loss: 0.9276\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2398 (1.2398)\tPrec@1 57.617 (57.617)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3641 (1.2370)\tPrec@1 51.103 (56.560)\n",
      "EPOCH: 64 val Results: Prec@1 56.560 Loss: 1.2370\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/97]\tTime 0.014 (0.014)\tLoss 0.8603 (0.8603)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [65][19/97]\tTime 0.013 (0.012)\tLoss 0.9303 (0.8755)\tPrec@1 65.625 (69.141)\n",
      "Epoch: [65][38/97]\tTime 0.013 (0.011)\tLoss 0.8821 (0.8889)\tPrec@1 71.289 (68.555)\n",
      "Epoch: [65][57/97]\tTime 0.007 (0.012)\tLoss 0.8567 (0.9030)\tPrec@1 69.922 (68.009)\n",
      "Epoch: [65][76/97]\tTime 0.023 (0.012)\tLoss 1.0130 (0.9209)\tPrec@1 64.258 (67.454)\n",
      "Epoch: [65][95/97]\tTime 0.014 (0.012)\tLoss 0.9173 (0.9266)\tPrec@1 68.164 (67.181)\n",
      "Epoch: [65][97/97]\tTime 0.005 (0.012)\tLoss 0.8501 (0.9272)\tPrec@1 68.750 (67.152)\n",
      "EPOCH: 65 train Results: Prec@1 67.152 Loss: 0.9272\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2231 (1.2231)\tPrec@1 55.273 (55.273)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.3766 (1.2328)\tPrec@1 48.897 (56.420)\n",
      "EPOCH: 65 val Results: Prec@1 56.420 Loss: 1.2328\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/97]\tTime 0.018 (0.018)\tLoss 0.9039 (0.9039)\tPrec@1 65.625 (65.625)\n",
      "Epoch: [66][19/97]\tTime 0.008 (0.011)\tLoss 0.8976 (0.8774)\tPrec@1 68.359 (69.287)\n",
      "Epoch: [66][38/97]\tTime 0.011 (0.011)\tLoss 0.9844 (0.8847)\tPrec@1 66.211 (69.045)\n",
      "Epoch: [66][57/97]\tTime 0.011 (0.011)\tLoss 0.9787 (0.8943)\tPrec@1 65.039 (68.642)\n",
      "Epoch: [66][76/97]\tTime 0.022 (0.011)\tLoss 0.9332 (0.9110)\tPrec@1 66.211 (68.035)\n",
      "Epoch: [66][95/97]\tTime 0.018 (0.011)\tLoss 1.0362 (0.9205)\tPrec@1 61.523 (67.611)\n",
      "Epoch: [66][97/97]\tTime 0.005 (0.011)\tLoss 0.9673 (0.9216)\tPrec@1 65.476 (67.574)\n",
      "EPOCH: 66 train Results: Prec@1 67.574 Loss: 0.9216\n",
      "Test: [0/19]\tTime 0.009 (0.009)\tLoss 1.2344 (1.2344)\tPrec@1 57.422 (57.422)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4142 (1.2359)\tPrec@1 50.000 (56.540)\n",
      "EPOCH: 66 val Results: Prec@1 56.540 Loss: 1.2359\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/97]\tTime 0.009 (0.009)\tLoss 0.8416 (0.8416)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [67][19/97]\tTime 0.010 (0.011)\tLoss 0.8960 (0.8796)\tPrec@1 68.945 (69.619)\n",
      "Epoch: [67][38/97]\tTime 0.009 (0.011)\tLoss 0.8629 (0.8872)\tPrec@1 68.359 (69.131)\n",
      "Epoch: [67][57/97]\tTime 0.007 (0.011)\tLoss 0.9496 (0.9037)\tPrec@1 68.164 (68.386)\n",
      "Epoch: [67][76/97]\tTime 0.012 (0.010)\tLoss 0.9502 (0.9115)\tPrec@1 64.844 (67.763)\n",
      "Epoch: [67][95/97]\tTime 0.015 (0.011)\tLoss 1.0490 (0.9220)\tPrec@1 65.234 (67.279)\n",
      "Epoch: [67][97/97]\tTime 0.007 (0.011)\tLoss 0.9538 (0.9224)\tPrec@1 65.179 (67.236)\n",
      "EPOCH: 67 train Results: Prec@1 67.236 Loss: 0.9224\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2084 (1.2084)\tPrec@1 57.812 (57.812)\n",
      "Test: [19/19]\tTime 0.003 (0.002)\tLoss 1.3772 (1.2318)\tPrec@1 51.471 (56.530)\n",
      "EPOCH: 67 val Results: Prec@1 56.530 Loss: 1.2318\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/97]\tTime 0.007 (0.007)\tLoss 0.8462 (0.8462)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [68][19/97]\tTime 0.010 (0.010)\tLoss 0.8428 (0.8547)\tPrec@1 67.969 (69.736)\n",
      "Epoch: [68][38/97]\tTime 0.014 (0.010)\tLoss 0.9689 (0.8883)\tPrec@1 64.453 (68.510)\n",
      "Epoch: [68][57/97]\tTime 0.014 (0.010)\tLoss 0.9047 (0.8997)\tPrec@1 67.188 (68.198)\n",
      "Epoch: [68][76/97]\tTime 0.008 (0.010)\tLoss 1.0057 (0.9092)\tPrec@1 66.016 (67.954)\n",
      "Epoch: [68][95/97]\tTime 0.008 (0.010)\tLoss 0.9859 (0.9178)\tPrec@1 65.625 (67.625)\n",
      "Epoch: [68][97/97]\tTime 0.009 (0.010)\tLoss 1.0059 (0.9192)\tPrec@1 66.667 (67.588)\n",
      "EPOCH: 68 train Results: Prec@1 67.588 Loss: 0.9192\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.2007 (1.2007)\tPrec@1 56.836 (56.836)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.3823 (1.2420)\tPrec@1 49.632 (56.490)\n",
      "EPOCH: 68 val Results: Prec@1 56.490 Loss: 1.2420\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/97]\tTime 0.016 (0.016)\tLoss 0.8291 (0.8291)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [69][19/97]\tTime 0.028 (0.013)\tLoss 0.8631 (0.8512)\tPrec@1 69.336 (70.850)\n",
      "Epoch: [69][38/97]\tTime 0.008 (0.011)\tLoss 0.9333 (0.8693)\tPrec@1 66.797 (69.902)\n",
      "Epoch: [69][57/97]\tTime 0.007 (0.012)\tLoss 0.9817 (0.8856)\tPrec@1 65.234 (69.235)\n",
      "Epoch: [69][76/97]\tTime 0.007 (0.012)\tLoss 0.9711 (0.8996)\tPrec@1 63.086 (68.463)\n",
      "Epoch: [69][95/97]\tTime 0.008 (0.012)\tLoss 0.9047 (0.9100)\tPrec@1 69.141 (67.950)\n",
      "Epoch: [69][97/97]\tTime 0.005 (0.012)\tLoss 0.9748 (0.9108)\tPrec@1 64.286 (67.928)\n",
      "EPOCH: 69 train Results: Prec@1 67.928 Loss: 0.9108\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2511 (1.2511)\tPrec@1 56.055 (56.055)\n",
      "Test: [19/19]\tTime 0.005 (0.002)\tLoss 1.3805 (1.2441)\tPrec@1 51.471 (56.140)\n",
      "EPOCH: 69 val Results: Prec@1 56.140 Loss: 1.2441\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/97]\tTime 0.015 (0.015)\tLoss 0.8065 (0.8065)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [70][19/97]\tTime 0.019 (0.011)\tLoss 0.9400 (0.8615)\tPrec@1 66.602 (70.215)\n",
      "Epoch: [70][38/97]\tTime 0.007 (0.011)\tLoss 0.8799 (0.8775)\tPrec@1 65.820 (69.491)\n",
      "Epoch: [70][57/97]\tTime 0.010 (0.011)\tLoss 0.8873 (0.8911)\tPrec@1 69.727 (68.898)\n",
      "Epoch: [70][76/97]\tTime 0.019 (0.011)\tLoss 0.9240 (0.9029)\tPrec@1 66.016 (68.372)\n",
      "Epoch: [70][95/97]\tTime 0.011 (0.012)\tLoss 0.9609 (0.9136)\tPrec@1 66.797 (67.804)\n",
      "Epoch: [70][97/97]\tTime 0.017 (0.013)\tLoss 0.9548 (0.9141)\tPrec@1 67.560 (67.810)\n",
      "EPOCH: 70 train Results: Prec@1 67.810 Loss: 0.9141\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2268 (1.2268)\tPrec@1 55.664 (55.664)\n",
      "Test: [19/19]\tTime 0.008 (0.005)\tLoss 1.4232 (1.2435)\tPrec@1 47.426 (56.430)\n",
      "EPOCH: 70 val Results: Prec@1 56.430 Loss: 1.2435\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/97]\tTime 0.026 (0.026)\tLoss 0.8486 (0.8486)\tPrec@1 69.727 (69.727)\n",
      "Epoch: [71][19/97]\tTime 0.021 (0.017)\tLoss 0.8273 (0.8590)\tPrec@1 70.117 (70.029)\n",
      "Epoch: [71][38/97]\tTime 0.016 (0.018)\tLoss 0.8541 (0.8736)\tPrec@1 69.727 (69.431)\n",
      "Epoch: [71][57/97]\tTime 0.010 (0.017)\tLoss 0.9583 (0.8913)\tPrec@1 65.625 (68.693)\n",
      "Epoch: [71][76/97]\tTime 0.009 (0.015)\tLoss 0.8973 (0.9000)\tPrec@1 68.750 (68.298)\n",
      "Epoch: [71][95/97]\tTime 0.006 (0.015)\tLoss 0.8889 (0.9102)\tPrec@1 71.094 (67.796)\n",
      "Epoch: [71][97/97]\tTime 0.005 (0.015)\tLoss 1.0334 (0.9124)\tPrec@1 62.798 (67.710)\n",
      "EPOCH: 71 train Results: Prec@1 67.710 Loss: 0.9124\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2529 (1.2529)\tPrec@1 56.250 (56.250)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.3419 (1.2494)\tPrec@1 51.838 (56.020)\n",
      "EPOCH: 71 val Results: Prec@1 56.020 Loss: 1.2494\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/97]\tTime 0.011 (0.011)\tLoss 0.7878 (0.7878)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [72][19/97]\tTime 0.008 (0.015)\tLoss 0.9119 (0.8650)\tPrec@1 64.062 (69.775)\n",
      "Epoch: [72][38/97]\tTime 0.011 (0.013)\tLoss 0.9849 (0.8768)\tPrec@1 66.602 (69.191)\n",
      "Epoch: [72][57/97]\tTime 0.007 (0.014)\tLoss 0.9341 (0.8860)\tPrec@1 66.992 (68.827)\n",
      "Epoch: [72][76/97]\tTime 0.018 (0.014)\tLoss 0.9766 (0.8972)\tPrec@1 65.430 (68.499)\n",
      "Epoch: [72][95/97]\tTime 0.018 (0.016)\tLoss 0.9728 (0.9065)\tPrec@1 66.406 (68.178)\n",
      "Epoch: [72][97/97]\tTime 0.006 (0.016)\tLoss 0.8926 (0.9068)\tPrec@1 67.560 (68.164)\n",
      "EPOCH: 72 train Results: Prec@1 68.164 Loss: 0.9068\n",
      "Test: [0/19]\tTime 0.011 (0.011)\tLoss 1.2492 (1.2492)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.001 (0.006)\tLoss 1.4057 (1.2414)\tPrec@1 48.897 (56.160)\n",
      "EPOCH: 72 val Results: Prec@1 56.160 Loss: 1.2414\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/97]\tTime 0.029 (0.029)\tLoss 0.8639 (0.8639)\tPrec@1 67.969 (67.969)\n",
      "Epoch: [73][19/97]\tTime 0.018 (0.013)\tLoss 0.9299 (0.8586)\tPrec@1 68.555 (69.580)\n",
      "Epoch: [73][38/97]\tTime 0.009 (0.011)\tLoss 0.9028 (0.8644)\tPrec@1 66.602 (69.316)\n",
      "Epoch: [73][57/97]\tTime 0.031 (0.012)\tLoss 0.9025 (0.8789)\tPrec@1 68.359 (68.939)\n",
      "Epoch: [73][76/97]\tTime 0.027 (0.014)\tLoss 1.0156 (0.8932)\tPrec@1 66.602 (68.362)\n",
      "Epoch: [73][95/97]\tTime 0.008 (0.014)\tLoss 0.9447 (0.9042)\tPrec@1 68.164 (68.026)\n",
      "Epoch: [73][97/97]\tTime 0.007 (0.014)\tLoss 1.0555 (0.9055)\tPrec@1 59.821 (67.968)\n",
      "EPOCH: 73 train Results: Prec@1 67.968 Loss: 0.9055\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2328 (1.2328)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4167 (1.2382)\tPrec@1 45.588 (56.490)\n",
      "EPOCH: 73 val Results: Prec@1 56.490 Loss: 1.2382\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/97]\tTime 0.009 (0.009)\tLoss 0.8574 (0.8574)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [74][19/97]\tTime 0.009 (0.011)\tLoss 0.8903 (0.8756)\tPrec@1 68.750 (69.434)\n",
      "Epoch: [74][38/97]\tTime 0.013 (0.010)\tLoss 0.8925 (0.8806)\tPrec@1 69.727 (69.301)\n",
      "Epoch: [74][57/97]\tTime 0.015 (0.011)\tLoss 0.8805 (0.8908)\tPrec@1 67.188 (68.774)\n",
      "Epoch: [74][76/97]\tTime 0.014 (0.011)\tLoss 0.9734 (0.8975)\tPrec@1 65.820 (68.471)\n",
      "Epoch: [74][95/97]\tTime 0.009 (0.011)\tLoss 0.9673 (0.9055)\tPrec@1 64.844 (68.186)\n",
      "Epoch: [74][97/97]\tTime 0.008 (0.011)\tLoss 1.0164 (0.9068)\tPrec@1 62.202 (68.116)\n",
      "EPOCH: 74 train Results: Prec@1 68.116 Loss: 0.9068\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2480 (1.2480)\tPrec@1 56.445 (56.445)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.3863 (1.2372)\tPrec@1 50.735 (56.410)\n",
      "EPOCH: 74 val Results: Prec@1 56.410 Loss: 1.2372\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/97]\tTime 0.015 (0.015)\tLoss 0.7902 (0.7902)\tPrec@1 74.805 (74.805)\n",
      "Epoch: [75][19/97]\tTime 0.009 (0.011)\tLoss 0.9547 (0.8433)\tPrec@1 67.188 (70.371)\n",
      "Epoch: [75][38/97]\tTime 0.007 (0.010)\tLoss 0.8635 (0.8660)\tPrec@1 70.508 (69.631)\n",
      "Epoch: [75][57/97]\tTime 0.009 (0.010)\tLoss 0.8545 (0.8776)\tPrec@1 70.508 (69.137)\n",
      "Epoch: [75][76/97]\tTime 0.012 (0.010)\tLoss 0.8647 (0.8860)\tPrec@1 67.383 (68.714)\n",
      "Epoch: [75][95/97]\tTime 0.011 (0.010)\tLoss 0.9919 (0.8997)\tPrec@1 67.969 (68.203)\n",
      "Epoch: [75][97/97]\tTime 0.005 (0.010)\tLoss 0.8514 (0.8995)\tPrec@1 71.726 (68.230)\n",
      "EPOCH: 75 train Results: Prec@1 68.230 Loss: 0.8995\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2252 (1.2252)\tPrec@1 56.445 (56.445)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4033 (1.2429)\tPrec@1 48.897 (56.490)\n",
      "EPOCH: 75 val Results: Prec@1 56.490 Loss: 1.2429\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/97]\tTime 0.009 (0.009)\tLoss 0.8853 (0.8853)\tPrec@1 68.555 (68.555)\n",
      "Epoch: [76][19/97]\tTime 0.015 (0.011)\tLoss 0.9015 (0.8578)\tPrec@1 69.336 (69.990)\n",
      "Epoch: [76][38/97]\tTime 0.023 (0.016)\tLoss 0.8913 (0.8686)\tPrec@1 68.750 (69.656)\n",
      "Epoch: [76][57/97]\tTime 0.018 (0.018)\tLoss 0.9091 (0.8808)\tPrec@1 66.992 (69.087)\n",
      "Epoch: [76][76/97]\tTime 0.013 (0.017)\tLoss 0.8966 (0.8899)\tPrec@1 66.797 (68.732)\n",
      "Epoch: [76][95/97]\tTime 0.014 (0.018)\tLoss 0.9626 (0.9013)\tPrec@1 64.062 (68.195)\n",
      "Epoch: [76][97/97]\tTime 0.011 (0.018)\tLoss 0.9944 (0.9021)\tPrec@1 65.476 (68.178)\n",
      "EPOCH: 76 train Results: Prec@1 68.178 Loss: 0.9021\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.2527 (1.2527)\tPrec@1 55.078 (55.078)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4301 (1.2458)\tPrec@1 48.162 (56.220)\n",
      "EPOCH: 76 val Results: Prec@1 56.220 Loss: 1.2458\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/97]\tTime 0.013 (0.013)\tLoss 0.8606 (0.8606)\tPrec@1 70.898 (70.898)\n",
      "Epoch: [77][19/97]\tTime 0.009 (0.013)\tLoss 0.8651 (0.8471)\tPrec@1 68.750 (70.342)\n",
      "Epoch: [77][38/97]\tTime 0.007 (0.015)\tLoss 0.8624 (0.8590)\tPrec@1 69.336 (69.717)\n",
      "Epoch: [77][57/97]\tTime 0.012 (0.015)\tLoss 0.9203 (0.8738)\tPrec@1 66.602 (69.023)\n",
      "Epoch: [77][76/97]\tTime 0.015 (0.014)\tLoss 0.9698 (0.8832)\tPrec@1 66.016 (68.760)\n",
      "Epoch: [77][95/97]\tTime 0.008 (0.013)\tLoss 0.9236 (0.8946)\tPrec@1 66.992 (68.270)\n",
      "Epoch: [77][97/97]\tTime 0.009 (0.013)\tLoss 0.9537 (0.8952)\tPrec@1 68.750 (68.282)\n",
      "EPOCH: 77 train Results: Prec@1 68.282 Loss: 0.8952\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2789 (1.2789)\tPrec@1 55.078 (55.078)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4200 (1.2454)\tPrec@1 48.897 (56.480)\n",
      "EPOCH: 77 val Results: Prec@1 56.480 Loss: 1.2454\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/97]\tTime 0.010 (0.010)\tLoss 0.8456 (0.8456)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [78][19/97]\tTime 0.010 (0.013)\tLoss 0.8723 (0.8394)\tPrec@1 66.211 (70.488)\n",
      "Epoch: [78][38/97]\tTime 0.020 (0.012)\tLoss 0.9316 (0.8536)\tPrec@1 67.578 (70.047)\n",
      "Epoch: [78][57/97]\tTime 0.007 (0.012)\tLoss 0.8946 (0.8699)\tPrec@1 67.969 (69.562)\n",
      "Epoch: [78][76/97]\tTime 0.011 (0.012)\tLoss 0.8938 (0.8846)\tPrec@1 69.727 (68.884)\n",
      "Epoch: [78][95/97]\tTime 0.006 (0.012)\tLoss 0.8630 (0.8932)\tPrec@1 69.727 (68.481)\n",
      "Epoch: [78][97/97]\tTime 0.009 (0.012)\tLoss 0.9640 (0.8941)\tPrec@1 64.881 (68.442)\n",
      "EPOCH: 78 train Results: Prec@1 68.442 Loss: 0.8941\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.2813 (1.2813)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3938 (1.2447)\tPrec@1 47.059 (56.220)\n",
      "EPOCH: 78 val Results: Prec@1 56.220 Loss: 1.2447\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/97]\tTime 0.009 (0.009)\tLoss 0.7331 (0.7331)\tPrec@1 75.195 (75.195)\n",
      "Epoch: [79][19/97]\tTime 0.013 (0.011)\tLoss 0.8278 (0.8304)\tPrec@1 69.727 (71.143)\n",
      "Epoch: [79][38/97]\tTime 0.013 (0.011)\tLoss 0.8691 (0.8522)\tPrec@1 69.531 (70.328)\n",
      "Epoch: [79][57/97]\tTime 0.014 (0.012)\tLoss 0.9531 (0.8689)\tPrec@1 63.477 (69.575)\n",
      "Epoch: [79][76/97]\tTime 0.046 (0.013)\tLoss 0.9781 (0.8867)\tPrec@1 64.648 (68.882)\n",
      "Epoch: [79][95/97]\tTime 0.020 (0.013)\tLoss 1.0293 (0.9019)\tPrec@1 62.695 (68.125)\n",
      "Epoch: [79][97/97]\tTime 0.014 (0.013)\tLoss 0.9587 (0.9034)\tPrec@1 67.560 (68.086)\n",
      "EPOCH: 79 train Results: Prec@1 68.086 Loss: 0.9034\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.2471 (1.2471)\tPrec@1 57.227 (57.227)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3770 (1.2437)\tPrec@1 48.529 (56.600)\n",
      "EPOCH: 79 val Results: Prec@1 56.600 Loss: 1.2437\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/97]\tTime 0.015 (0.015)\tLoss 0.8351 (0.8351)\tPrec@1 69.336 (69.336)\n",
      "Epoch: [80][19/97]\tTime 0.020 (0.011)\tLoss 0.8597 (0.8363)\tPrec@1 69.336 (70.674)\n",
      "Epoch: [80][38/97]\tTime 0.008 (0.010)\tLoss 0.8585 (0.8501)\tPrec@1 70.312 (69.997)\n",
      "Epoch: [80][57/97]\tTime 0.014 (0.011)\tLoss 0.8573 (0.8693)\tPrec@1 69.336 (69.168)\n",
      "Epoch: [80][76/97]\tTime 0.014 (0.011)\tLoss 0.8945 (0.8835)\tPrec@1 69.531 (68.623)\n",
      "Epoch: [80][95/97]\tTime 0.008 (0.011)\tLoss 0.9458 (0.8899)\tPrec@1 66.797 (68.414)\n",
      "Epoch: [80][97/97]\tTime 0.009 (0.011)\tLoss 0.9693 (0.8913)\tPrec@1 67.857 (68.376)\n",
      "EPOCH: 80 train Results: Prec@1 68.376 Loss: 0.8913\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2629 (1.2629)\tPrec@1 55.664 (55.664)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4012 (1.2559)\tPrec@1 48.162 (56.450)\n",
      "EPOCH: 80 val Results: Prec@1 56.450 Loss: 1.2559\n",
      "Best Prec@1: 56.710\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/97]\tTime 0.011 (0.011)\tLoss 0.7927 (0.7927)\tPrec@1 74.414 (74.414)\n",
      "Epoch: [81][19/97]\tTime 0.009 (0.011)\tLoss 0.8552 (0.8436)\tPrec@1 69.531 (70.283)\n",
      "Epoch: [81][38/97]\tTime 0.007 (0.011)\tLoss 0.8735 (0.8576)\tPrec@1 66.016 (69.526)\n",
      "Epoch: [81][57/97]\tTime 0.016 (0.011)\tLoss 0.9224 (0.8670)\tPrec@1 67.773 (69.407)\n",
      "Epoch: [81][76/97]\tTime 0.011 (0.010)\tLoss 0.9744 (0.8829)\tPrec@1 65.820 (68.912)\n",
      "Epoch: [81][95/97]\tTime 0.012 (0.011)\tLoss 0.9699 (0.8946)\tPrec@1 65.039 (68.463)\n",
      "Epoch: [81][97/97]\tTime 0.006 (0.011)\tLoss 0.9320 (0.8955)\tPrec@1 65.774 (68.436)\n",
      "EPOCH: 81 train Results: Prec@1 68.436 Loss: 0.8955\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2459 (1.2459)\tPrec@1 56.836 (56.836)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.3510 (1.2390)\tPrec@1 50.000 (56.860)\n",
      "EPOCH: 81 val Results: Prec@1 56.860 Loss: 1.2390\n",
      "Best Prec@1: 56.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/97]\tTime 0.010 (0.010)\tLoss 0.8043 (0.8043)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [82][19/97]\tTime 0.009 (0.011)\tLoss 0.8223 (0.8345)\tPrec@1 72.461 (70.742)\n",
      "Epoch: [82][38/97]\tTime 0.013 (0.011)\tLoss 0.8679 (0.8459)\tPrec@1 69.922 (70.453)\n",
      "Epoch: [82][57/97]\tTime 0.010 (0.011)\tLoss 0.9889 (0.8612)\tPrec@1 66.211 (69.969)\n",
      "Epoch: [82][76/97]\tTime 0.007 (0.011)\tLoss 0.9140 (0.8679)\tPrec@1 67.188 (69.569)\n",
      "Epoch: [82][95/97]\tTime 0.018 (0.011)\tLoss 1.0513 (0.8833)\tPrec@1 62.305 (68.958)\n",
      "Epoch: [82][97/97]\tTime 0.011 (0.011)\tLoss 0.9026 (0.8848)\tPrec@1 66.667 (68.878)\n",
      "EPOCH: 82 train Results: Prec@1 68.878 Loss: 0.8848\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2683 (1.2683)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.3663 (1.2449)\tPrec@1 51.103 (56.540)\n",
      "EPOCH: 82 val Results: Prec@1 56.540 Loss: 1.2449\n",
      "Best Prec@1: 56.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/97]\tTime 0.011 (0.011)\tLoss 0.8273 (0.8273)\tPrec@1 71.289 (71.289)\n",
      "Epoch: [83][19/97]\tTime 0.010 (0.011)\tLoss 0.8111 (0.8451)\tPrec@1 72.852 (70.264)\n",
      "Epoch: [83][38/97]\tTime 0.011 (0.011)\tLoss 0.8864 (0.8580)\tPrec@1 69.922 (69.702)\n",
      "Epoch: [83][57/97]\tTime 0.009 (0.011)\tLoss 0.9463 (0.8702)\tPrec@1 64.844 (69.292)\n",
      "Epoch: [83][76/97]\tTime 0.013 (0.011)\tLoss 0.8729 (0.8798)\tPrec@1 69.727 (69.009)\n",
      "Epoch: [83][95/97]\tTime 0.009 (0.011)\tLoss 1.0095 (0.8905)\tPrec@1 63.086 (68.579)\n",
      "Epoch: [83][97/97]\tTime 0.006 (0.011)\tLoss 0.9684 (0.8917)\tPrec@1 64.286 (68.506)\n",
      "EPOCH: 83 train Results: Prec@1 68.506 Loss: 0.8917\n",
      "Test: [0/19]\tTime 0.011 (0.011)\tLoss 1.2356 (1.2356)\tPrec@1 56.055 (56.055)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.3720 (1.2456)\tPrec@1 48.897 (56.610)\n",
      "EPOCH: 83 val Results: Prec@1 56.610 Loss: 1.2456\n",
      "Best Prec@1: 56.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/97]\tTime 0.012 (0.012)\tLoss 0.7820 (0.7820)\tPrec@1 76.172 (76.172)\n",
      "Epoch: [84][19/97]\tTime 0.016 (0.011)\tLoss 0.8058 (0.8365)\tPrec@1 74.219 (71.113)\n",
      "Epoch: [84][38/97]\tTime 0.013 (0.011)\tLoss 0.8969 (0.8500)\tPrec@1 67.969 (70.393)\n",
      "Epoch: [84][57/97]\tTime 0.012 (0.011)\tLoss 0.8588 (0.8663)\tPrec@1 69.141 (69.750)\n",
      "Epoch: [84][76/97]\tTime 0.018 (0.011)\tLoss 0.9068 (0.8766)\tPrec@1 64.648 (69.267)\n",
      "Epoch: [84][95/97]\tTime 0.009 (0.011)\tLoss 0.9687 (0.8820)\tPrec@1 66.211 (69.139)\n",
      "Epoch: [84][97/97]\tTime 0.011 (0.011)\tLoss 0.9913 (0.8837)\tPrec@1 63.988 (69.058)\n",
      "EPOCH: 84 train Results: Prec@1 69.058 Loss: 0.8837\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2582 (1.2582)\tPrec@1 55.273 (55.273)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3983 (1.2405)\tPrec@1 51.838 (57.000)\n",
      "EPOCH: 84 val Results: Prec@1 57.000 Loss: 1.2405\n",
      "Best Prec@1: 57.000\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/97]\tTime 0.014 (0.014)\tLoss 0.7595 (0.7595)\tPrec@1 73.242 (73.242)\n",
      "Epoch: [85][19/97]\tTime 0.007 (0.012)\tLoss 0.8498 (0.8268)\tPrec@1 71.094 (71.641)\n",
      "Epoch: [85][38/97]\tTime 0.026 (0.012)\tLoss 0.9208 (0.8464)\tPrec@1 66.406 (70.413)\n",
      "Epoch: [85][57/97]\tTime 0.016 (0.013)\tLoss 0.9047 (0.8600)\tPrec@1 69.922 (69.777)\n",
      "Epoch: [85][76/97]\tTime 0.014 (0.013)\tLoss 0.8733 (0.8708)\tPrec@1 68.164 (69.295)\n",
      "Epoch: [85][95/97]\tTime 0.009 (0.013)\tLoss 0.9649 (0.8818)\tPrec@1 65.625 (68.837)\n",
      "Epoch: [85][97/97]\tTime 0.008 (0.013)\tLoss 0.9536 (0.8829)\tPrec@1 65.774 (68.790)\n",
      "EPOCH: 85 train Results: Prec@1 68.790 Loss: 0.8829\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2425 (1.2425)\tPrec@1 57.031 (57.031)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4165 (1.2321)\tPrec@1 51.103 (56.790)\n",
      "EPOCH: 85 val Results: Prec@1 56.790 Loss: 1.2321\n",
      "Best Prec@1: 57.000\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/97]\tTime 0.008 (0.008)\tLoss 0.8123 (0.8123)\tPrec@1 70.508 (70.508)\n",
      "Epoch: [86][19/97]\tTime 0.011 (0.009)\tLoss 0.7678 (0.8422)\tPrec@1 72.656 (70.684)\n",
      "Epoch: [86][38/97]\tTime 0.008 (0.010)\tLoss 0.8557 (0.8478)\tPrec@1 67.383 (70.072)\n",
      "Epoch: [86][57/97]\tTime 0.018 (0.010)\tLoss 0.9132 (0.8581)\tPrec@1 68.750 (69.787)\n",
      "Epoch: [86][76/97]\tTime 0.013 (0.010)\tLoss 0.9051 (0.8691)\tPrec@1 66.211 (69.209)\n",
      "Epoch: [86][95/97]\tTime 0.020 (0.011)\tLoss 0.9366 (0.8822)\tPrec@1 66.602 (68.689)\n",
      "Epoch: [86][97/97]\tTime 0.005 (0.011)\tLoss 0.9634 (0.8830)\tPrec@1 65.476 (68.660)\n",
      "EPOCH: 86 train Results: Prec@1 68.660 Loss: 0.8830\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2311 (1.2311)\tPrec@1 56.250 (56.250)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4031 (1.2388)\tPrec@1 49.265 (56.410)\n",
      "EPOCH: 86 val Results: Prec@1 56.410 Loss: 1.2388\n",
      "Best Prec@1: 57.000\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/97]\tTime 0.009 (0.009)\tLoss 0.8743 (0.8743)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [87][19/97]\tTime 0.017 (0.012)\tLoss 0.8140 (0.8309)\tPrec@1 74.609 (71.084)\n",
      "Epoch: [87][38/97]\tTime 0.018 (0.012)\tLoss 0.8962 (0.8504)\tPrec@1 67.773 (70.132)\n",
      "Epoch: [87][57/97]\tTime 0.009 (0.011)\tLoss 0.9624 (0.8632)\tPrec@1 66.406 (69.642)\n",
      "Epoch: [87][76/97]\tTime 0.011 (0.011)\tLoss 0.8702 (0.8752)\tPrec@1 67.188 (69.141)\n",
      "Epoch: [87][95/97]\tTime 0.007 (0.011)\tLoss 0.8741 (0.8841)\tPrec@1 68.945 (68.770)\n",
      "Epoch: [87][97/97]\tTime 0.007 (0.011)\tLoss 0.9567 (0.8851)\tPrec@1 65.476 (68.742)\n",
      "EPOCH: 87 train Results: Prec@1 68.742 Loss: 0.8851\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.2119 (1.2119)\tPrec@1 57.422 (57.422)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4143 (1.2447)\tPrec@1 47.059 (56.900)\n",
      "EPOCH: 87 val Results: Prec@1 56.900 Loss: 1.2447\n",
      "Best Prec@1: 57.000\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/97]\tTime 0.018 (0.018)\tLoss 0.7307 (0.7307)\tPrec@1 76.172 (76.172)\n",
      "Epoch: [88][19/97]\tTime 0.008 (0.011)\tLoss 0.8143 (0.8161)\tPrec@1 70.117 (71.777)\n",
      "Epoch: [88][38/97]\tTime 0.018 (0.011)\tLoss 0.8171 (0.8369)\tPrec@1 71.680 (70.883)\n",
      "Epoch: [88][57/97]\tTime 0.007 (0.011)\tLoss 0.9265 (0.8529)\tPrec@1 68.945 (70.252)\n",
      "Epoch: [88][76/97]\tTime 0.007 (0.010)\tLoss 0.9184 (0.8660)\tPrec@1 66.211 (69.519)\n",
      "Epoch: [88][95/97]\tTime 0.013 (0.010)\tLoss 0.9246 (0.8775)\tPrec@1 68.555 (69.061)\n",
      "Epoch: [88][97/97]\tTime 0.006 (0.010)\tLoss 1.0156 (0.8785)\tPrec@1 64.583 (69.004)\n",
      "EPOCH: 88 train Results: Prec@1 69.004 Loss: 0.8785\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2447 (1.2447)\tPrec@1 56.250 (56.250)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4134 (1.2438)\tPrec@1 50.368 (57.070)\n",
      "EPOCH: 88 val Results: Prec@1 57.070 Loss: 1.2438\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/97]\tTime 0.014 (0.014)\tLoss 0.7827 (0.7827)\tPrec@1 74.023 (74.023)\n",
      "Epoch: [89][19/97]\tTime 0.013 (0.011)\tLoss 0.8313 (0.8234)\tPrec@1 71.484 (71.152)\n",
      "Epoch: [89][38/97]\tTime 0.011 (0.011)\tLoss 0.8713 (0.8330)\tPrec@1 69.141 (70.633)\n",
      "Epoch: [89][57/97]\tTime 0.008 (0.011)\tLoss 0.8261 (0.8492)\tPrec@1 68.750 (69.915)\n",
      "Epoch: [89][76/97]\tTime 0.007 (0.011)\tLoss 0.9350 (0.8665)\tPrec@1 64.453 (69.278)\n",
      "Epoch: [89][95/97]\tTime 0.012 (0.011)\tLoss 0.8808 (0.8801)\tPrec@1 68.750 (68.813)\n",
      "Epoch: [89][97/97]\tTime 0.005 (0.011)\tLoss 1.0118 (0.8814)\tPrec@1 63.690 (68.770)\n",
      "EPOCH: 89 train Results: Prec@1 68.770 Loss: 0.8814\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2417 (1.2417)\tPrec@1 55.664 (55.664)\n",
      "Test: [19/19]\tTime 0.003 (0.002)\tLoss 1.3977 (1.2529)\tPrec@1 51.103 (56.400)\n",
      "EPOCH: 89 val Results: Prec@1 56.400 Loss: 1.2529\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/97]\tTime 0.009 (0.009)\tLoss 0.8241 (0.8241)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [90][19/97]\tTime 0.012 (0.011)\tLoss 0.8376 (0.8254)\tPrec@1 70.312 (70.996)\n",
      "Epoch: [90][38/97]\tTime 0.011 (0.011)\tLoss 0.8739 (0.8434)\tPrec@1 69.531 (70.343)\n",
      "Epoch: [90][57/97]\tTime 0.021 (0.011)\tLoss 0.8029 (0.8549)\tPrec@1 73.438 (69.959)\n",
      "Epoch: [90][76/97]\tTime 0.012 (0.011)\tLoss 0.8728 (0.8623)\tPrec@1 69.727 (69.630)\n",
      "Epoch: [90][95/97]\tTime 0.012 (0.012)\tLoss 0.9503 (0.8744)\tPrec@1 63.281 (69.019)\n",
      "Epoch: [90][97/97]\tTime 0.005 (0.012)\tLoss 0.9048 (0.8750)\tPrec@1 64.286 (68.972)\n",
      "EPOCH: 90 train Results: Prec@1 68.972 Loss: 0.8750\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3012 (1.3012)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.003 (0.003)\tLoss 1.4423 (1.2466)\tPrec@1 46.691 (56.460)\n",
      "EPOCH: 90 val Results: Prec@1 56.460 Loss: 1.2466\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/97]\tTime 0.014 (0.014)\tLoss 0.7858 (0.7858)\tPrec@1 73.047 (73.047)\n",
      "Epoch: [91][19/97]\tTime 0.013 (0.013)\tLoss 0.7969 (0.8201)\tPrec@1 69.531 (71.807)\n",
      "Epoch: [91][38/97]\tTime 0.011 (0.012)\tLoss 0.8614 (0.8289)\tPrec@1 70.703 (71.069)\n",
      "Epoch: [91][57/97]\tTime 0.012 (0.012)\tLoss 0.8754 (0.8425)\tPrec@1 68.555 (70.420)\n",
      "Epoch: [91][76/97]\tTime 0.020 (0.012)\tLoss 0.8483 (0.8592)\tPrec@1 71.094 (69.638)\n",
      "Epoch: [91][95/97]\tTime 0.009 (0.012)\tLoss 0.9394 (0.8745)\tPrec@1 66.992 (69.094)\n",
      "Epoch: [91][97/97]\tTime 0.005 (0.012)\tLoss 0.9068 (0.8752)\tPrec@1 66.369 (69.082)\n",
      "EPOCH: 91 train Results: Prec@1 69.082 Loss: 0.8752\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2665 (1.2665)\tPrec@1 55.664 (55.664)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4469 (1.2534)\tPrec@1 44.485 (56.280)\n",
      "EPOCH: 91 val Results: Prec@1 56.280 Loss: 1.2534\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/97]\tTime 0.016 (0.016)\tLoss 0.8106 (0.8106)\tPrec@1 70.508 (70.508)\n",
      "Epoch: [92][19/97]\tTime 0.009 (0.012)\tLoss 0.8602 (0.8159)\tPrec@1 68.750 (71.260)\n",
      "Epoch: [92][38/97]\tTime 0.012 (0.011)\tLoss 0.8277 (0.8370)\tPrec@1 68.359 (70.488)\n",
      "Epoch: [92][57/97]\tTime 0.007 (0.011)\tLoss 0.9272 (0.8484)\tPrec@1 67.578 (69.989)\n",
      "Epoch: [92][76/97]\tTime 0.009 (0.011)\tLoss 0.9172 (0.8570)\tPrec@1 68.945 (69.554)\n",
      "Epoch: [92][95/97]\tTime 0.009 (0.011)\tLoss 0.8930 (0.8686)\tPrec@1 69.141 (69.177)\n",
      "Epoch: [92][97/97]\tTime 0.006 (0.010)\tLoss 0.9277 (0.8696)\tPrec@1 68.155 (69.166)\n",
      "EPOCH: 92 train Results: Prec@1 69.166 Loss: 0.8696\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3105 (1.3105)\tPrec@1 53.906 (53.906)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4780 (1.2574)\tPrec@1 48.897 (56.480)\n",
      "EPOCH: 92 val Results: Prec@1 56.480 Loss: 1.2574\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/97]\tTime 0.007 (0.007)\tLoss 0.7606 (0.7606)\tPrec@1 76.172 (76.172)\n",
      "Epoch: [93][19/97]\tTime 0.008 (0.012)\tLoss 0.7319 (0.7995)\tPrec@1 74.805 (72.559)\n",
      "Epoch: [93][38/97]\tTime 0.014 (0.011)\tLoss 0.9170 (0.8272)\tPrec@1 67.969 (71.394)\n",
      "Epoch: [93][57/97]\tTime 0.007 (0.011)\tLoss 0.9288 (0.8435)\tPrec@1 67.578 (70.643)\n",
      "Epoch: [93][76/97]\tTime 0.011 (0.011)\tLoss 0.9180 (0.8589)\tPrec@1 68.359 (69.955)\n",
      "Epoch: [93][95/97]\tTime 0.010 (0.011)\tLoss 0.9164 (0.8726)\tPrec@1 68.945 (69.373)\n",
      "Epoch: [93][97/97]\tTime 0.005 (0.011)\tLoss 0.8956 (0.8737)\tPrec@1 68.155 (69.340)\n",
      "EPOCH: 93 train Results: Prec@1 69.340 Loss: 0.8737\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2664 (1.2664)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4549 (1.2461)\tPrec@1 45.588 (56.450)\n",
      "EPOCH: 93 val Results: Prec@1 56.450 Loss: 1.2461\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/97]\tTime 0.013 (0.013)\tLoss 0.8191 (0.8191)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [94][19/97]\tTime 0.008 (0.013)\tLoss 0.7974 (0.8116)\tPrec@1 70.703 (71.133)\n",
      "Epoch: [94][38/97]\tTime 0.009 (0.012)\tLoss 0.8664 (0.8233)\tPrec@1 67.773 (70.813)\n",
      "Epoch: [94][57/97]\tTime 0.007 (0.012)\tLoss 0.9069 (0.8409)\tPrec@1 65.430 (70.299)\n",
      "Epoch: [94][76/97]\tTime 0.006 (0.011)\tLoss 0.8932 (0.8556)\tPrec@1 67.773 (69.762)\n",
      "Epoch: [94][95/97]\tTime 0.026 (0.011)\tLoss 0.8805 (0.8678)\tPrec@1 67.383 (69.356)\n",
      "Epoch: [94][97/97]\tTime 0.027 (0.012)\tLoss 1.0523 (0.8697)\tPrec@1 65.774 (69.300)\n",
      "EPOCH: 94 train Results: Prec@1 69.300 Loss: 0.8697\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2830 (1.2830)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4374 (1.2455)\tPrec@1 50.735 (56.730)\n",
      "EPOCH: 94 val Results: Prec@1 56.730 Loss: 1.2455\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/97]\tTime 0.020 (0.020)\tLoss 0.8316 (0.8316)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [95][19/97]\tTime 0.011 (0.014)\tLoss 0.8149 (0.8087)\tPrec@1 70.312 (71.904)\n",
      "Epoch: [95][38/97]\tTime 0.013 (0.014)\tLoss 0.9119 (0.8332)\tPrec@1 66.797 (71.064)\n",
      "Epoch: [95][57/97]\tTime 0.010 (0.014)\tLoss 0.9340 (0.8467)\tPrec@1 68.359 (70.383)\n",
      "Epoch: [95][76/97]\tTime 0.013 (0.013)\tLoss 0.9395 (0.8587)\tPrec@1 65.625 (69.891)\n",
      "Epoch: [95][95/97]\tTime 0.016 (0.013)\tLoss 0.8838 (0.8707)\tPrec@1 68.555 (69.407)\n",
      "Epoch: [95][97/97]\tTime 0.009 (0.013)\tLoss 0.9019 (0.8716)\tPrec@1 65.476 (69.364)\n",
      "EPOCH: 95 train Results: Prec@1 69.364 Loss: 0.8716\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2910 (1.2910)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4489 (1.2511)\tPrec@1 50.000 (56.440)\n",
      "EPOCH: 95 val Results: Prec@1 56.440 Loss: 1.2511\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/97]\tTime 0.014 (0.014)\tLoss 0.8435 (0.8435)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [96][19/97]\tTime 0.009 (0.012)\tLoss 0.8900 (0.8112)\tPrec@1 70.312 (72.373)\n",
      "Epoch: [96][38/97]\tTime 0.007 (0.011)\tLoss 0.8910 (0.8279)\tPrec@1 69.727 (71.409)\n",
      "Epoch: [96][57/97]\tTime 0.016 (0.011)\tLoss 0.8396 (0.8445)\tPrec@1 69.336 (70.562)\n",
      "Epoch: [96][76/97]\tTime 0.009 (0.011)\tLoss 0.9283 (0.8528)\tPrec@1 66.016 (70.125)\n",
      "Epoch: [96][95/97]\tTime 0.012 (0.011)\tLoss 0.9459 (0.8686)\tPrec@1 66.211 (69.468)\n",
      "Epoch: [96][97/97]\tTime 0.006 (0.011)\tLoss 0.9803 (0.8699)\tPrec@1 67.560 (69.442)\n",
      "EPOCH: 96 train Results: Prec@1 69.442 Loss: 0.8699\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.3248 (1.3248)\tPrec@1 53.320 (53.320)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4380 (1.2587)\tPrec@1 50.735 (56.420)\n",
      "EPOCH: 96 val Results: Prec@1 56.420 Loss: 1.2587\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/97]\tTime 0.009 (0.009)\tLoss 0.8969 (0.8969)\tPrec@1 69.336 (69.336)\n",
      "Epoch: [97][19/97]\tTime 0.008 (0.012)\tLoss 0.8836 (0.8139)\tPrec@1 70.508 (71.934)\n",
      "Epoch: [97][38/97]\tTime 0.011 (0.011)\tLoss 0.8203 (0.8334)\tPrec@1 71.680 (70.984)\n",
      "Epoch: [97][57/97]\tTime 0.011 (0.011)\tLoss 0.9092 (0.8422)\tPrec@1 68.164 (70.501)\n",
      "Epoch: [97][76/97]\tTime 0.008 (0.011)\tLoss 0.9021 (0.8540)\tPrec@1 66.797 (70.036)\n",
      "Epoch: [97][95/97]\tTime 0.012 (0.011)\tLoss 0.8808 (0.8629)\tPrec@1 69.531 (69.682)\n",
      "Epoch: [97][97/97]\tTime 0.012 (0.011)\tLoss 0.8462 (0.8638)\tPrec@1 69.940 (69.644)\n",
      "EPOCH: 97 train Results: Prec@1 69.644 Loss: 0.8638\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3091 (1.3091)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.3791 (1.2483)\tPrec@1 52.574 (56.970)\n",
      "EPOCH: 97 val Results: Prec@1 56.970 Loss: 1.2483\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/97]\tTime 0.009 (0.009)\tLoss 0.8481 (0.8481)\tPrec@1 69.727 (69.727)\n",
      "Epoch: [98][19/97]\tTime 0.014 (0.011)\tLoss 0.7764 (0.8150)\tPrec@1 73.633 (71.602)\n",
      "Epoch: [98][38/97]\tTime 0.008 (0.011)\tLoss 0.8244 (0.8267)\tPrec@1 71.484 (71.209)\n",
      "Epoch: [98][57/97]\tTime 0.007 (0.011)\tLoss 0.8237 (0.8441)\tPrec@1 72.852 (70.390)\n",
      "Epoch: [98][76/97]\tTime 0.015 (0.010)\tLoss 0.9192 (0.8535)\tPrec@1 68.555 (70.069)\n",
      "Epoch: [98][95/97]\tTime 0.007 (0.010)\tLoss 1.0102 (0.8672)\tPrec@1 64.648 (69.570)\n",
      "Epoch: [98][97/97]\tTime 0.005 (0.010)\tLoss 0.9098 (0.8683)\tPrec@1 66.071 (69.514)\n",
      "EPOCH: 98 train Results: Prec@1 69.514 Loss: 0.8683\n",
      "Test: [0/19]\tTime 0.008 (0.008)\tLoss 1.3155 (1.3155)\tPrec@1 54.688 (54.688)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4782 (1.2681)\tPrec@1 48.162 (56.130)\n",
      "EPOCH: 98 val Results: Prec@1 56.130 Loss: 1.2681\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/97]\tTime 0.012 (0.012)\tLoss 0.8728 (0.8728)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [99][19/97]\tTime 0.008 (0.011)\tLoss 0.8292 (0.8129)\tPrec@1 70.117 (71.611)\n",
      "Epoch: [99][38/97]\tTime 0.012 (0.011)\tLoss 0.8750 (0.8297)\tPrec@1 69.727 (70.974)\n",
      "Epoch: [99][57/97]\tTime 0.010 (0.011)\tLoss 0.8526 (0.8377)\tPrec@1 70.898 (70.548)\n",
      "Epoch: [99][76/97]\tTime 0.010 (0.011)\tLoss 0.8386 (0.8484)\tPrec@1 69.922 (70.145)\n",
      "Epoch: [99][95/97]\tTime 0.009 (0.012)\tLoss 0.9044 (0.8620)\tPrec@1 67.578 (69.531)\n",
      "Epoch: [99][97/97]\tTime 0.007 (0.012)\tLoss 0.9927 (0.8631)\tPrec@1 66.964 (69.510)\n",
      "EPOCH: 99 train Results: Prec@1 69.510 Loss: 0.8631\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2646 (1.2646)\tPrec@1 55.664 (55.664)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4062 (1.2456)\tPrec@1 52.574 (56.750)\n",
      "EPOCH: 99 val Results: Prec@1 56.750 Loss: 1.2456\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/97]\tTime 0.008 (0.008)\tLoss 0.7939 (0.7939)\tPrec@1 74.023 (74.023)\n",
      "Epoch: [100][19/97]\tTime 0.010 (0.012)\tLoss 0.8042 (0.8154)\tPrec@1 71.875 (71.650)\n",
      "Epoch: [100][38/97]\tTime 0.012 (0.011)\tLoss 0.8459 (0.8271)\tPrec@1 68.945 (70.783)\n",
      "Epoch: [100][57/97]\tTime 0.008 (0.011)\tLoss 0.8756 (0.8394)\tPrec@1 68.945 (70.201)\n",
      "Epoch: [100][76/97]\tTime 0.007 (0.011)\tLoss 0.9754 (0.8544)\tPrec@1 65.820 (69.638)\n",
      "Epoch: [100][95/97]\tTime 0.007 (0.011)\tLoss 0.8887 (0.8618)\tPrec@1 66.797 (69.371)\n",
      "Epoch: [100][97/97]\tTime 0.006 (0.011)\tLoss 1.0062 (0.8628)\tPrec@1 64.583 (69.340)\n",
      "EPOCH: 100 train Results: Prec@1 69.340 Loss: 0.8628\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.2988 (1.2988)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4782 (1.2610)\tPrec@1 46.691 (56.010)\n",
      "EPOCH: 100 val Results: Prec@1 56.010 Loss: 1.2610\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/97]\tTime 0.011 (0.011)\tLoss 0.7636 (0.7636)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [101][19/97]\tTime 0.009 (0.011)\tLoss 0.8538 (0.8192)\tPrec@1 70.312 (71.494)\n",
      "Epoch: [101][38/97]\tTime 0.007 (0.011)\tLoss 0.8421 (0.8285)\tPrec@1 71.094 (70.913)\n",
      "Epoch: [101][57/97]\tTime 0.012 (0.011)\tLoss 0.8609 (0.8419)\tPrec@1 69.336 (70.383)\n",
      "Epoch: [101][76/97]\tTime 0.007 (0.011)\tLoss 0.8672 (0.8467)\tPrec@1 67.578 (70.211)\n",
      "Epoch: [101][95/97]\tTime 0.011 (0.011)\tLoss 0.9016 (0.8577)\tPrec@1 66.797 (69.820)\n",
      "Epoch: [101][97/97]\tTime 0.005 (0.011)\tLoss 0.8877 (0.8582)\tPrec@1 67.857 (69.784)\n",
      "EPOCH: 101 train Results: Prec@1 69.784 Loss: 0.8582\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.2798 (1.2798)\tPrec@1 55.078 (55.078)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.3968 (1.2628)\tPrec@1 49.265 (56.080)\n",
      "EPOCH: 101 val Results: Prec@1 56.080 Loss: 1.2628\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/97]\tTime 0.009 (0.009)\tLoss 0.7747 (0.7747)\tPrec@1 70.508 (70.508)\n",
      "Epoch: [102][19/97]\tTime 0.014 (0.011)\tLoss 0.8370 (0.7981)\tPrec@1 72.266 (71.943)\n",
      "Epoch: [102][38/97]\tTime 0.011 (0.011)\tLoss 0.8065 (0.8204)\tPrec@1 72.266 (71.109)\n",
      "Epoch: [102][57/97]\tTime 0.009 (0.012)\tLoss 0.8307 (0.8354)\tPrec@1 72.070 (70.488)\n",
      "Epoch: [102][76/97]\tTime 0.007 (0.011)\tLoss 0.9059 (0.8496)\tPrec@1 67.188 (69.858)\n",
      "Epoch: [102][95/97]\tTime 0.008 (0.011)\tLoss 0.9463 (0.8611)\tPrec@1 66.211 (69.560)\n",
      "Epoch: [102][97/97]\tTime 0.010 (0.011)\tLoss 0.8490 (0.8617)\tPrec@1 68.155 (69.530)\n",
      "EPOCH: 102 train Results: Prec@1 69.530 Loss: 0.8617\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3035 (1.3035)\tPrec@1 55.078 (55.078)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4209 (1.2551)\tPrec@1 50.000 (56.620)\n",
      "EPOCH: 102 val Results: Prec@1 56.620 Loss: 1.2551\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/97]\tTime 0.014 (0.014)\tLoss 0.7692 (0.7692)\tPrec@1 74.023 (74.023)\n",
      "Epoch: [103][19/97]\tTime 0.006 (0.010)\tLoss 0.7890 (0.7994)\tPrec@1 71.680 (72.412)\n",
      "Epoch: [103][38/97]\tTime 0.009 (0.010)\tLoss 0.8249 (0.8099)\tPrec@1 69.727 (72.065)\n",
      "Epoch: [103][57/97]\tTime 0.009 (0.010)\tLoss 0.9167 (0.8262)\tPrec@1 66.211 (71.259)\n",
      "Epoch: [103][76/97]\tTime 0.010 (0.011)\tLoss 0.8539 (0.8429)\tPrec@1 71.680 (70.434)\n",
      "Epoch: [103][95/97]\tTime 0.009 (0.012)\tLoss 0.9407 (0.8571)\tPrec@1 66.992 (69.859)\n",
      "Epoch: [103][97/97]\tTime 0.005 (0.012)\tLoss 0.9606 (0.8580)\tPrec@1 67.560 (69.826)\n",
      "EPOCH: 103 train Results: Prec@1 69.826 Loss: 0.8580\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3229 (1.3229)\tPrec@1 51.562 (51.562)\n",
      "Test: [19/19]\tTime 0.006 (0.004)\tLoss 1.4568 (1.2655)\tPrec@1 49.265 (56.530)\n",
      "EPOCH: 103 val Results: Prec@1 56.530 Loss: 1.2655\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/97]\tTime 0.019 (0.019)\tLoss 0.8623 (0.8623)\tPrec@1 70.117 (70.117)\n",
      "Epoch: [104][19/97]\tTime 0.016 (0.012)\tLoss 0.8011 (0.8070)\tPrec@1 74.023 (72.080)\n",
      "Epoch: [104][38/97]\tTime 0.008 (0.012)\tLoss 0.8912 (0.8213)\tPrec@1 68.750 (71.414)\n",
      "Epoch: [104][57/97]\tTime 0.011 (0.011)\tLoss 0.8474 (0.8318)\tPrec@1 70.898 (71.053)\n",
      "Epoch: [104][76/97]\tTime 0.016 (0.011)\tLoss 0.8987 (0.8477)\tPrec@1 67.969 (70.302)\n",
      "Epoch: [104][95/97]\tTime 0.011 (0.010)\tLoss 0.9197 (0.8571)\tPrec@1 65.430 (69.926)\n",
      "Epoch: [104][97/97]\tTime 0.007 (0.010)\tLoss 0.9366 (0.8577)\tPrec@1 66.071 (69.898)\n",
      "EPOCH: 104 train Results: Prec@1 69.898 Loss: 0.8577\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3040 (1.3040)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.002 (0.004)\tLoss 1.4417 (1.2634)\tPrec@1 50.000 (56.170)\n",
      "EPOCH: 104 val Results: Prec@1 56.170 Loss: 1.2634\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/97]\tTime 0.014 (0.014)\tLoss 0.8538 (0.8538)\tPrec@1 69.727 (69.727)\n",
      "Epoch: [105][19/97]\tTime 0.012 (0.010)\tLoss 0.8272 (0.8046)\tPrec@1 69.336 (71.982)\n",
      "Epoch: [105][38/97]\tTime 0.023 (0.011)\tLoss 0.8453 (0.8154)\tPrec@1 70.898 (71.519)\n",
      "Epoch: [105][57/97]\tTime 0.010 (0.011)\tLoss 0.8914 (0.8272)\tPrec@1 70.508 (71.074)\n",
      "Epoch: [105][76/97]\tTime 0.011 (0.011)\tLoss 0.9269 (0.8367)\tPrec@1 69.336 (70.739)\n",
      "Epoch: [105][95/97]\tTime 0.010 (0.011)\tLoss 0.8873 (0.8521)\tPrec@1 68.164 (70.168)\n",
      "Epoch: [105][97/97]\tTime 0.010 (0.011)\tLoss 0.8642 (0.8528)\tPrec@1 70.833 (70.124)\n",
      "EPOCH: 105 train Results: Prec@1 70.124 Loss: 0.8528\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3403 (1.3403)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4364 (1.2733)\tPrec@1 49.632 (55.840)\n",
      "EPOCH: 105 val Results: Prec@1 55.840 Loss: 1.2733\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/97]\tTime 0.009 (0.009)\tLoss 0.8061 (0.8061)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [106][19/97]\tTime 0.017 (0.011)\tLoss 0.7587 (0.7982)\tPrec@1 73.242 (72.451)\n",
      "Epoch: [106][38/97]\tTime 0.008 (0.010)\tLoss 0.8398 (0.8147)\tPrec@1 68.164 (71.434)\n",
      "Epoch: [106][57/97]\tTime 0.011 (0.010)\tLoss 0.8678 (0.8333)\tPrec@1 69.922 (70.669)\n",
      "Epoch: [106][76/97]\tTime 0.007 (0.010)\tLoss 0.9432 (0.8454)\tPrec@1 65.430 (70.028)\n",
      "Epoch: [106][95/97]\tTime 0.009 (0.011)\tLoss 0.9563 (0.8576)\tPrec@1 66.211 (69.586)\n",
      "Epoch: [106][97/97]\tTime 0.006 (0.011)\tLoss 0.8688 (0.8577)\tPrec@1 71.131 (69.590)\n",
      "EPOCH: 106 train Results: Prec@1 69.590 Loss: 0.8577\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3200 (1.3200)\tPrec@1 54.297 (54.297)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4086 (1.2581)\tPrec@1 50.000 (56.530)\n",
      "EPOCH: 106 val Results: Prec@1 56.530 Loss: 1.2581\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/97]\tTime 0.010 (0.010)\tLoss 0.8308 (0.8308)\tPrec@1 70.898 (70.898)\n",
      "Epoch: [107][19/97]\tTime 0.007 (0.011)\tLoss 0.8792 (0.8003)\tPrec@1 69.922 (72.227)\n",
      "Epoch: [107][38/97]\tTime 0.008 (0.011)\tLoss 0.8139 (0.8077)\tPrec@1 74.219 (71.910)\n",
      "Epoch: [107][57/97]\tTime 0.010 (0.011)\tLoss 0.8917 (0.8192)\tPrec@1 67.578 (71.323)\n",
      "Epoch: [107][76/97]\tTime 0.012 (0.011)\tLoss 0.9611 (0.8404)\tPrec@1 67.383 (70.546)\n",
      "Epoch: [107][95/97]\tTime 0.009 (0.011)\tLoss 1.0387 (0.8535)\tPrec@1 63.281 (69.930)\n",
      "Epoch: [107][97/97]\tTime 0.008 (0.011)\tLoss 0.9430 (0.8548)\tPrec@1 67.857 (69.918)\n",
      "EPOCH: 107 train Results: Prec@1 69.918 Loss: 0.8548\n",
      "Test: [0/19]\tTime 0.005 (0.005)\tLoss 1.3343 (1.3343)\tPrec@1 53.320 (53.320)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4464 (1.2617)\tPrec@1 48.529 (56.140)\n",
      "EPOCH: 107 val Results: Prec@1 56.140 Loss: 1.2617\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/97]\tTime 0.008 (0.008)\tLoss 0.7817 (0.7817)\tPrec@1 73.047 (73.047)\n",
      "Epoch: [108][19/97]\tTime 0.016 (0.015)\tLoss 0.8079 (0.7903)\tPrec@1 71.289 (72.568)\n",
      "Epoch: [108][38/97]\tTime 0.011 (0.012)\tLoss 0.8020 (0.8110)\tPrec@1 71.875 (71.424)\n",
      "Epoch: [108][57/97]\tTime 0.012 (0.012)\tLoss 0.8417 (0.8296)\tPrec@1 71.289 (70.636)\n",
      "Epoch: [108][76/97]\tTime 0.014 (0.012)\tLoss 0.9385 (0.8418)\tPrec@1 64.258 (70.165)\n",
      "Epoch: [108][95/97]\tTime 0.012 (0.012)\tLoss 0.8469 (0.8539)\tPrec@1 67.578 (69.767)\n",
      "Epoch: [108][97/97]\tTime 0.017 (0.012)\tLoss 0.9183 (0.8557)\tPrec@1 67.857 (69.704)\n",
      "EPOCH: 108 train Results: Prec@1 69.704 Loss: 0.8557\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3379 (1.3379)\tPrec@1 52.344 (52.344)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3865 (1.2631)\tPrec@1 50.000 (55.860)\n",
      "EPOCH: 108 val Results: Prec@1 55.860 Loss: 1.2631\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/97]\tTime 0.010 (0.010)\tLoss 0.8010 (0.8010)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [109][19/97]\tTime 0.010 (0.012)\tLoss 0.8132 (0.8066)\tPrec@1 70.117 (71.484)\n",
      "Epoch: [109][38/97]\tTime 0.010 (0.011)\tLoss 0.7540 (0.8216)\tPrec@1 75.000 (70.964)\n",
      "Epoch: [109][57/97]\tTime 0.011 (0.011)\tLoss 0.8880 (0.8345)\tPrec@1 69.141 (70.636)\n",
      "Epoch: [109][76/97]\tTime 0.008 (0.011)\tLoss 0.9147 (0.8440)\tPrec@1 66.797 (70.254)\n",
      "Epoch: [109][95/97]\tTime 0.010 (0.011)\tLoss 0.9033 (0.8541)\tPrec@1 67.578 (69.873)\n",
      "Epoch: [109][97/97]\tTime 0.005 (0.011)\tLoss 0.8458 (0.8534)\tPrec@1 71.131 (69.896)\n",
      "EPOCH: 109 train Results: Prec@1 69.896 Loss: 0.8534\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3119 (1.3119)\tPrec@1 55.273 (55.273)\n",
      "Test: [19/19]\tTime 0.004 (0.003)\tLoss 1.3874 (1.2591)\tPrec@1 53.309 (56.600)\n",
      "EPOCH: 109 val Results: Prec@1 56.600 Loss: 1.2591\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/97]\tTime 0.016 (0.016)\tLoss 0.8216 (0.8216)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [110][19/97]\tTime 0.009 (0.012)\tLoss 0.7661 (0.7966)\tPrec@1 73.047 (72.363)\n",
      "Epoch: [110][38/97]\tTime 0.008 (0.011)\tLoss 0.7912 (0.8173)\tPrec@1 73.438 (71.439)\n",
      "Epoch: [110][57/97]\tTime 0.009 (0.011)\tLoss 0.8759 (0.8249)\tPrec@1 67.969 (70.959)\n",
      "Epoch: [110][76/97]\tTime 0.012 (0.011)\tLoss 0.9306 (0.8389)\tPrec@1 64.844 (70.348)\n",
      "Epoch: [110][95/97]\tTime 0.012 (0.011)\tLoss 0.9237 (0.8472)\tPrec@1 68.750 (70.076)\n",
      "Epoch: [110][97/97]\tTime 0.005 (0.011)\tLoss 0.8687 (0.8481)\tPrec@1 67.560 (70.030)\n",
      "EPOCH: 110 train Results: Prec@1 70.030 Loss: 0.8481\n",
      "Test: [0/19]\tTime 0.010 (0.010)\tLoss 1.3291 (1.3291)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4394 (1.2644)\tPrec@1 47.794 (56.140)\n",
      "EPOCH: 110 val Results: Prec@1 56.140 Loss: 1.2644\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/97]\tTime 0.013 (0.013)\tLoss 0.8257 (0.8257)\tPrec@1 71.289 (71.289)\n",
      "Epoch: [111][19/97]\tTime 0.012 (0.011)\tLoss 0.8078 (0.7896)\tPrec@1 70.117 (72.256)\n",
      "Epoch: [111][38/97]\tTime 0.010 (0.011)\tLoss 0.8296 (0.8058)\tPrec@1 70.508 (71.850)\n",
      "Epoch: [111][57/97]\tTime 0.011 (0.011)\tLoss 0.8369 (0.8253)\tPrec@1 71.289 (71.107)\n",
      "Epoch: [111][76/97]\tTime 0.011 (0.011)\tLoss 0.9015 (0.8414)\tPrec@1 68.359 (70.345)\n",
      "Epoch: [111][95/97]\tTime 0.010 (0.011)\tLoss 0.8805 (0.8515)\tPrec@1 69.336 (69.942)\n",
      "Epoch: [111][97/97]\tTime 0.006 (0.011)\tLoss 0.8867 (0.8531)\tPrec@1 70.833 (69.906)\n",
      "EPOCH: 111 train Results: Prec@1 69.906 Loss: 0.8531\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3404 (1.3404)\tPrec@1 52.344 (52.344)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4371 (1.2704)\tPrec@1 48.162 (55.590)\n",
      "EPOCH: 111 val Results: Prec@1 55.590 Loss: 1.2704\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/97]\tTime 0.008 (0.008)\tLoss 0.7350 (0.7350)\tPrec@1 72.852 (72.852)\n",
      "Epoch: [112][19/97]\tTime 0.011 (0.012)\tLoss 0.8109 (0.7909)\tPrec@1 71.094 (72.119)\n",
      "Epoch: [112][38/97]\tTime 0.011 (0.012)\tLoss 0.7982 (0.8081)\tPrec@1 71.875 (71.289)\n",
      "Epoch: [112][57/97]\tTime 0.009 (0.012)\tLoss 0.8562 (0.8217)\tPrec@1 69.922 (70.848)\n",
      "Epoch: [112][76/97]\tTime 0.009 (0.012)\tLoss 0.8844 (0.8374)\tPrec@1 68.750 (70.335)\n",
      "Epoch: [112][95/97]\tTime 0.008 (0.012)\tLoss 0.8549 (0.8461)\tPrec@1 70.312 (69.985)\n",
      "Epoch: [112][97/97]\tTime 0.006 (0.013)\tLoss 0.9162 (0.8474)\tPrec@1 69.940 (69.948)\n",
      "EPOCH: 112 train Results: Prec@1 69.948 Loss: 0.8474\n",
      "Test: [0/19]\tTime 0.008 (0.008)\tLoss 1.3077 (1.3077)\tPrec@1 55.078 (55.078)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4195 (1.2702)\tPrec@1 50.368 (56.010)\n",
      "EPOCH: 112 val Results: Prec@1 56.010 Loss: 1.2702\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/97]\tTime 0.037 (0.037)\tLoss 0.7415 (0.7415)\tPrec@1 71.289 (71.289)\n",
      "Epoch: [113][19/97]\tTime 0.018 (0.013)\tLoss 0.8182 (0.7918)\tPrec@1 71.484 (72.461)\n",
      "Epoch: [113][38/97]\tTime 0.024 (0.012)\tLoss 0.8572 (0.8079)\tPrec@1 68.555 (71.865)\n",
      "Epoch: [113][57/97]\tTime 0.020 (0.012)\tLoss 0.8118 (0.8191)\tPrec@1 70.312 (71.232)\n",
      "Epoch: [113][76/97]\tTime 0.014 (0.012)\tLoss 0.8779 (0.8340)\tPrec@1 68.555 (70.675)\n",
      "Epoch: [113][95/97]\tTime 0.009 (0.012)\tLoss 0.9260 (0.8489)\tPrec@1 66.992 (70.101)\n",
      "Epoch: [113][97/97]\tTime 0.007 (0.012)\tLoss 0.9333 (0.8494)\tPrec@1 67.262 (70.088)\n",
      "EPOCH: 113 train Results: Prec@1 70.088 Loss: 0.8494\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3310 (1.3310)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4104 (1.2695)\tPrec@1 50.368 (55.920)\n",
      "EPOCH: 113 val Results: Prec@1 55.920 Loss: 1.2695\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/97]\tTime 0.009 (0.009)\tLoss 0.7276 (0.7276)\tPrec@1 77.930 (77.930)\n",
      "Epoch: [114][19/97]\tTime 0.007 (0.011)\tLoss 0.8034 (0.7857)\tPrec@1 71.484 (72.725)\n",
      "Epoch: [114][38/97]\tTime 0.013 (0.011)\tLoss 0.7702 (0.8091)\tPrec@1 74.805 (71.740)\n",
      "Epoch: [114][57/97]\tTime 0.009 (0.012)\tLoss 0.7758 (0.8227)\tPrec@1 71.484 (71.171)\n",
      "Epoch: [114][76/97]\tTime 0.009 (0.012)\tLoss 0.9393 (0.8334)\tPrec@1 64.648 (70.665)\n",
      "Epoch: [114][95/97]\tTime 0.011 (0.012)\tLoss 0.9150 (0.8433)\tPrec@1 67.188 (70.213)\n",
      "Epoch: [114][97/97]\tTime 0.006 (0.012)\tLoss 0.8178 (0.8447)\tPrec@1 71.429 (70.164)\n",
      "EPOCH: 114 train Results: Prec@1 70.164 Loss: 0.8447\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3211 (1.3211)\tPrec@1 52.539 (52.539)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4275 (1.2602)\tPrec@1 48.529 (56.250)\n",
      "EPOCH: 114 val Results: Prec@1 56.250 Loss: 1.2602\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/97]\tTime 0.012 (0.012)\tLoss 0.7705 (0.7705)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [115][19/97]\tTime 0.012 (0.011)\tLoss 0.7973 (0.7775)\tPrec@1 71.094 (73.027)\n",
      "Epoch: [115][38/97]\tTime 0.015 (0.012)\tLoss 0.7946 (0.7998)\tPrec@1 72.266 (71.910)\n",
      "Epoch: [115][57/97]\tTime 0.011 (0.012)\tLoss 0.8330 (0.8132)\tPrec@1 67.969 (71.350)\n",
      "Epoch: [115][76/97]\tTime 0.010 (0.013)\tLoss 1.0107 (0.8307)\tPrec@1 67.578 (70.594)\n",
      "Epoch: [115][95/97]\tTime 0.014 (0.014)\tLoss 0.8973 (0.8429)\tPrec@1 66.992 (70.125)\n",
      "Epoch: [115][97/97]\tTime 0.015 (0.014)\tLoss 0.9866 (0.8439)\tPrec@1 65.774 (70.106)\n",
      "EPOCH: 115 train Results: Prec@1 70.106 Loss: 0.8439\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3383 (1.3383)\tPrec@1 52.930 (52.930)\n",
      "Test: [19/19]\tTime 0.002 (0.004)\tLoss 1.4644 (1.2647)\tPrec@1 47.426 (56.190)\n",
      "EPOCH: 115 val Results: Prec@1 56.190 Loss: 1.2647\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/97]\tTime 0.015 (0.015)\tLoss 0.7872 (0.7872)\tPrec@1 75.586 (75.586)\n",
      "Epoch: [116][19/97]\tTime 0.009 (0.021)\tLoss 0.8299 (0.7947)\tPrec@1 71.484 (72.246)\n",
      "Epoch: [116][38/97]\tTime 0.020 (0.017)\tLoss 0.8199 (0.8059)\tPrec@1 70.703 (71.625)\n",
      "Epoch: [116][57/97]\tTime 0.009 (0.017)\tLoss 0.8723 (0.8229)\tPrec@1 66.602 (70.932)\n",
      "Epoch: [116][76/97]\tTime 0.008 (0.018)\tLoss 0.9087 (0.8366)\tPrec@1 66.992 (70.437)\n",
      "Epoch: [116][95/97]\tTime 0.022 (0.017)\tLoss 0.8603 (0.8441)\tPrec@1 69.922 (70.115)\n",
      "Epoch: [116][97/97]\tTime 0.006 (0.017)\tLoss 0.8642 (0.8447)\tPrec@1 67.262 (70.082)\n",
      "EPOCH: 116 train Results: Prec@1 70.082 Loss: 0.8447\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3512 (1.3512)\tPrec@1 53.906 (53.906)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4513 (1.2581)\tPrec@1 49.632 (56.470)\n",
      "EPOCH: 116 val Results: Prec@1 56.470 Loss: 1.2581\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/97]\tTime 0.009 (0.009)\tLoss 0.8124 (0.8124)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [117][19/97]\tTime 0.011 (0.012)\tLoss 0.8584 (0.7919)\tPrec@1 69.336 (72.021)\n",
      "Epoch: [117][38/97]\tTime 0.015 (0.012)\tLoss 0.8250 (0.8054)\tPrec@1 70.898 (71.635)\n",
      "Epoch: [117][57/97]\tTime 0.021 (0.012)\tLoss 0.8499 (0.8182)\tPrec@1 71.875 (71.077)\n",
      "Epoch: [117][76/97]\tTime 0.016 (0.012)\tLoss 0.8767 (0.8341)\tPrec@1 67.969 (70.358)\n",
      "Epoch: [117][95/97]\tTime 0.015 (0.012)\tLoss 0.8407 (0.8437)\tPrec@1 69.922 (70.083)\n",
      "Epoch: [117][97/97]\tTime 0.007 (0.012)\tLoss 0.8905 (0.8445)\tPrec@1 66.964 (70.050)\n",
      "EPOCH: 117 train Results: Prec@1 70.050 Loss: 0.8445\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.3295 (1.3295)\tPrec@1 53.711 (53.711)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4307 (1.2669)\tPrec@1 45.956 (56.130)\n",
      "EPOCH: 117 val Results: Prec@1 56.130 Loss: 1.2669\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/97]\tTime 0.008 (0.008)\tLoss 0.8148 (0.8148)\tPrec@1 72.852 (72.852)\n",
      "Epoch: [118][19/97]\tTime 0.008 (0.012)\tLoss 0.7447 (0.7917)\tPrec@1 74.609 (72.490)\n",
      "Epoch: [118][38/97]\tTime 0.011 (0.011)\tLoss 0.8013 (0.8000)\tPrec@1 71.094 (71.895)\n",
      "Epoch: [118][57/97]\tTime 0.007 (0.012)\tLoss 0.7689 (0.8099)\tPrec@1 71.484 (71.427)\n",
      "Epoch: [118][76/97]\tTime 0.008 (0.011)\tLoss 0.9258 (0.8252)\tPrec@1 67.578 (70.893)\n",
      "Epoch: [118][95/97]\tTime 0.007 (0.011)\tLoss 0.9026 (0.8402)\tPrec@1 66.406 (70.298)\n",
      "Epoch: [118][97/97]\tTime 0.005 (0.011)\tLoss 0.8505 (0.8408)\tPrec@1 70.238 (70.300)\n",
      "EPOCH: 118 train Results: Prec@1 70.300 Loss: 0.8408\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3354 (1.3354)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.4171 (1.2697)\tPrec@1 47.794 (56.250)\n",
      "EPOCH: 118 val Results: Prec@1 56.250 Loss: 1.2697\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/97]\tTime 0.010 (0.010)\tLoss 0.7859 (0.7859)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [119][19/97]\tTime 0.009 (0.011)\tLoss 0.8648 (0.7838)\tPrec@1 66.992 (72.910)\n",
      "Epoch: [119][38/97]\tTime 0.009 (0.011)\tLoss 0.7843 (0.7979)\tPrec@1 71.875 (72.170)\n",
      "Epoch: [119][57/97]\tTime 0.009 (0.011)\tLoss 0.8334 (0.8143)\tPrec@1 71.289 (71.373)\n",
      "Epoch: [119][76/97]\tTime 0.007 (0.011)\tLoss 0.8327 (0.8262)\tPrec@1 69.531 (70.825)\n",
      "Epoch: [119][95/97]\tTime 0.014 (0.011)\tLoss 0.9362 (0.8395)\tPrec@1 66.602 (70.398)\n",
      "Epoch: [119][97/97]\tTime 0.019 (0.011)\tLoss 0.8419 (0.8403)\tPrec@1 70.833 (70.362)\n",
      "EPOCH: 119 train Results: Prec@1 70.362 Loss: 0.8403\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3392 (1.3392)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.005 (0.004)\tLoss 1.4806 (1.2686)\tPrec@1 46.691 (56.270)\n",
      "EPOCH: 119 val Results: Prec@1 56.270 Loss: 1.2686\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/97]\tTime 0.021 (0.021)\tLoss 0.6916 (0.6916)\tPrec@1 75.586 (75.586)\n",
      "Epoch: [120][19/97]\tTime 0.014 (0.014)\tLoss 0.8911 (0.7843)\tPrec@1 68.555 (72.461)\n",
      "Epoch: [120][38/97]\tTime 0.009 (0.013)\tLoss 0.7854 (0.8039)\tPrec@1 74.609 (71.800)\n",
      "Epoch: [120][57/97]\tTime 0.016 (0.012)\tLoss 0.8796 (0.8166)\tPrec@1 67.969 (71.208)\n",
      "Epoch: [120][76/97]\tTime 0.008 (0.013)\tLoss 0.9142 (0.8233)\tPrec@1 66.602 (70.871)\n",
      "Epoch: [120][95/97]\tTime 0.007 (0.013)\tLoss 0.8433 (0.8317)\tPrec@1 68.359 (70.463)\n",
      "Epoch: [120][97/97]\tTime 0.016 (0.013)\tLoss 0.8784 (0.8324)\tPrec@1 66.667 (70.440)\n",
      "EPOCH: 120 train Results: Prec@1 70.440 Loss: 0.8324\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3346 (1.3346)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4109 (1.2729)\tPrec@1 48.897 (56.200)\n",
      "EPOCH: 120 val Results: Prec@1 56.200 Loss: 1.2729\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/97]\tTime 0.013 (0.013)\tLoss 0.7807 (0.7807)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [121][19/97]\tTime 0.008 (0.012)\tLoss 0.7551 (0.7785)\tPrec@1 71.094 (72.627)\n",
      "Epoch: [121][38/97]\tTime 0.009 (0.011)\tLoss 0.8045 (0.8013)\tPrec@1 71.484 (71.755)\n",
      "Epoch: [121][57/97]\tTime 0.016 (0.011)\tLoss 0.9126 (0.8125)\tPrec@1 67.969 (71.474)\n",
      "Epoch: [121][76/97]\tTime 0.011 (0.012)\tLoss 0.8515 (0.8250)\tPrec@1 68.750 (70.909)\n",
      "Epoch: [121][95/97]\tTime 0.012 (0.013)\tLoss 0.9226 (0.8379)\tPrec@1 64.453 (70.363)\n",
      "Epoch: [121][97/97]\tTime 0.005 (0.013)\tLoss 0.7916 (0.8383)\tPrec@1 71.131 (70.340)\n",
      "EPOCH: 121 train Results: Prec@1 70.340 Loss: 0.8383\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3133 (1.3133)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4517 (1.2627)\tPrec@1 45.221 (56.510)\n",
      "EPOCH: 121 val Results: Prec@1 56.510 Loss: 1.2627\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/97]\tTime 0.012 (0.012)\tLoss 0.7772 (0.7772)\tPrec@1 73.242 (73.242)\n",
      "Epoch: [122][19/97]\tTime 0.009 (0.012)\tLoss 0.7921 (0.7731)\tPrec@1 71.484 (73.018)\n",
      "Epoch: [122][38/97]\tTime 0.009 (0.011)\tLoss 0.8676 (0.7989)\tPrec@1 69.141 (71.960)\n",
      "Epoch: [122][57/97]\tTime 0.011 (0.011)\tLoss 0.8537 (0.8181)\tPrec@1 69.531 (71.185)\n",
      "Epoch: [122][76/97]\tTime 0.007 (0.011)\tLoss 0.8113 (0.8319)\tPrec@1 72.266 (70.706)\n",
      "Epoch: [122][95/97]\tTime 0.012 (0.011)\tLoss 0.8787 (0.8408)\tPrec@1 69.141 (70.376)\n",
      "Epoch: [122][97/97]\tTime 0.011 (0.011)\tLoss 0.8391 (0.8409)\tPrec@1 72.619 (70.384)\n",
      "EPOCH: 122 train Results: Prec@1 70.384 Loss: 0.8409\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3355 (1.3355)\tPrec@1 52.148 (52.148)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4708 (1.2776)\tPrec@1 47.794 (56.190)\n",
      "EPOCH: 122 val Results: Prec@1 56.190 Loss: 1.2776\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/97]\tTime 0.013 (0.013)\tLoss 0.7017 (0.7017)\tPrec@1 76.172 (76.172)\n",
      "Epoch: [123][19/97]\tTime 0.006 (0.011)\tLoss 0.7829 (0.7742)\tPrec@1 71.289 (72.900)\n",
      "Epoch: [123][38/97]\tTime 0.022 (0.011)\tLoss 0.8822 (0.7903)\tPrec@1 70.703 (72.246)\n",
      "Epoch: [123][57/97]\tTime 0.009 (0.011)\tLoss 0.8379 (0.8031)\tPrec@1 69.531 (71.828)\n",
      "Epoch: [123][76/97]\tTime 0.009 (0.011)\tLoss 0.8223 (0.8214)\tPrec@1 69.922 (71.018)\n",
      "Epoch: [123][95/97]\tTime 0.010 (0.011)\tLoss 0.9715 (0.8367)\tPrec@1 62.500 (70.304)\n",
      "Epoch: [123][97/97]\tTime 0.014 (0.011)\tLoss 0.8224 (0.8378)\tPrec@1 71.726 (70.306)\n",
      "EPOCH: 123 train Results: Prec@1 70.306 Loss: 0.8378\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3173 (1.3173)\tPrec@1 55.664 (55.664)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4109 (1.2681)\tPrec@1 51.838 (56.440)\n",
      "EPOCH: 123 val Results: Prec@1 56.440 Loss: 1.2681\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/97]\tTime 0.016 (0.016)\tLoss 0.8123 (0.8123)\tPrec@1 70.508 (70.508)\n",
      "Epoch: [124][19/97]\tTime 0.008 (0.012)\tLoss 0.7844 (0.7827)\tPrec@1 73.438 (72.490)\n",
      "Epoch: [124][38/97]\tTime 0.015 (0.011)\tLoss 0.8604 (0.8090)\tPrec@1 71.289 (71.650)\n",
      "Epoch: [124][57/97]\tTime 0.007 (0.013)\tLoss 0.8565 (0.8163)\tPrec@1 69.922 (71.353)\n",
      "Epoch: [124][76/97]\tTime 0.009 (0.013)\tLoss 0.9131 (0.8257)\tPrec@1 67.969 (70.863)\n",
      "Epoch: [124][95/97]\tTime 0.008 (0.012)\tLoss 0.8941 (0.8392)\tPrec@1 69.336 (70.327)\n",
      "Epoch: [124][97/97]\tTime 0.006 (0.012)\tLoss 0.9445 (0.8406)\tPrec@1 63.690 (70.290)\n",
      "EPOCH: 124 train Results: Prec@1 70.290 Loss: 0.8406\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3387 (1.3387)\tPrec@1 54.688 (54.688)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.4104 (1.2794)\tPrec@1 50.000 (56.460)\n",
      "EPOCH: 124 val Results: Prec@1 56.460 Loss: 1.2794\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/97]\tTime 0.011 (0.011)\tLoss 0.7674 (0.7674)\tPrec@1 72.852 (72.852)\n",
      "Epoch: [125][19/97]\tTime 0.008 (0.011)\tLoss 0.7424 (0.7843)\tPrec@1 73.828 (72.510)\n",
      "Epoch: [125][38/97]\tTime 0.010 (0.011)\tLoss 0.8531 (0.8004)\tPrec@1 69.336 (71.780)\n",
      "Epoch: [125][57/97]\tTime 0.008 (0.011)\tLoss 0.8293 (0.8117)\tPrec@1 70.508 (71.286)\n",
      "Epoch: [125][76/97]\tTime 0.010 (0.011)\tLoss 0.9034 (0.8222)\tPrec@1 68.555 (70.825)\n",
      "Epoch: [125][95/97]\tTime 0.011 (0.011)\tLoss 0.9209 (0.8328)\tPrec@1 66.797 (70.483)\n",
      "Epoch: [125][97/97]\tTime 0.012 (0.011)\tLoss 0.8504 (0.8332)\tPrec@1 66.667 (70.444)\n",
      "EPOCH: 125 train Results: Prec@1 70.444 Loss: 0.8332\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3467 (1.3467)\tPrec@1 51.367 (51.367)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.3932 (1.2679)\tPrec@1 53.676 (56.300)\n",
      "EPOCH: 125 val Results: Prec@1 56.300 Loss: 1.2679\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/97]\tTime 0.011 (0.011)\tLoss 0.7479 (0.7479)\tPrec@1 73.242 (73.242)\n",
      "Epoch: [126][19/97]\tTime 0.015 (0.012)\tLoss 0.8280 (0.7644)\tPrec@1 72.656 (73.398)\n",
      "Epoch: [126][38/97]\tTime 0.011 (0.011)\tLoss 0.8286 (0.7911)\tPrec@1 69.727 (72.291)\n",
      "Epoch: [126][57/97]\tTime 0.010 (0.011)\tLoss 0.8686 (0.8017)\tPrec@1 66.992 (71.828)\n",
      "Epoch: [126][76/97]\tTime 0.012 (0.011)\tLoss 0.9317 (0.8162)\tPrec@1 66.992 (71.297)\n",
      "Epoch: [126][95/97]\tTime 0.030 (0.011)\tLoss 0.9315 (0.8307)\tPrec@1 66.016 (70.689)\n",
      "Epoch: [126][97/97]\tTime 0.009 (0.011)\tLoss 0.8169 (0.8306)\tPrec@1 71.429 (70.672)\n",
      "EPOCH: 126 train Results: Prec@1 70.672 Loss: 0.8306\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3446 (1.3446)\tPrec@1 51.953 (51.953)\n",
      "Test: [19/19]\tTime 0.009 (0.004)\tLoss 1.4166 (1.2680)\tPrec@1 48.529 (56.040)\n",
      "EPOCH: 126 val Results: Prec@1 56.040 Loss: 1.2680\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/97]\tTime 0.138 (0.138)\tLoss 0.7730 (0.7730)\tPrec@1 71.289 (71.289)\n",
      "Epoch: [127][19/97]\tTime 0.007 (0.021)\tLoss 0.8042 (0.7695)\tPrec@1 71.680 (73.438)\n",
      "Epoch: [127][38/97]\tTime 0.019 (0.018)\tLoss 0.8170 (0.7882)\tPrec@1 71.094 (72.606)\n",
      "Epoch: [127][57/97]\tTime 0.009 (0.015)\tLoss 0.8340 (0.8019)\tPrec@1 70.312 (71.838)\n",
      "Epoch: [127][76/97]\tTime 0.010 (0.014)\tLoss 0.9279 (0.8196)\tPrec@1 66.211 (70.972)\n",
      "Epoch: [127][95/97]\tTime 0.021 (0.014)\tLoss 0.9340 (0.8309)\tPrec@1 66.992 (70.549)\n",
      "Epoch: [127][97/97]\tTime 0.005 (0.014)\tLoss 0.8877 (0.8310)\tPrec@1 67.857 (70.550)\n",
      "EPOCH: 127 train Results: Prec@1 70.550 Loss: 0.8310\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3391 (1.3391)\tPrec@1 51.172 (51.172)\n",
      "Test: [19/19]\tTime 0.003 (0.003)\tLoss 1.4454 (1.2712)\tPrec@1 49.632 (56.150)\n",
      "EPOCH: 127 val Results: Prec@1 56.150 Loss: 1.2712\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/97]\tTime 0.013 (0.013)\tLoss 0.7719 (0.7719)\tPrec@1 72.852 (72.852)\n",
      "Epoch: [128][19/97]\tTime 0.013 (0.011)\tLoss 0.8502 (0.7798)\tPrec@1 70.703 (72.588)\n",
      "Epoch: [128][38/97]\tTime 0.009 (0.011)\tLoss 0.7975 (0.7959)\tPrec@1 69.531 (71.830)\n",
      "Epoch: [128][57/97]\tTime 0.016 (0.011)\tLoss 0.7944 (0.8030)\tPrec@1 70.703 (71.579)\n",
      "Epoch: [128][76/97]\tTime 0.048 (0.012)\tLoss 0.8530 (0.8152)\tPrec@1 68.945 (71.134)\n",
      "Epoch: [128][95/97]\tTime 0.012 (0.012)\tLoss 0.9336 (0.8288)\tPrec@1 66.992 (70.752)\n",
      "Epoch: [128][97/97]\tTime 0.005 (0.011)\tLoss 0.9493 (0.8295)\tPrec@1 64.286 (70.710)\n",
      "EPOCH: 128 train Results: Prec@1 70.710 Loss: 0.8295\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.3487 (1.3487)\tPrec@1 53.711 (53.711)\n",
      "Test: [19/19]\tTime 0.004 (0.004)\tLoss 1.4568 (1.2800)\tPrec@1 49.265 (55.980)\n",
      "EPOCH: 128 val Results: Prec@1 55.980 Loss: 1.2800\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/97]\tTime 0.008 (0.008)\tLoss 0.7090 (0.7090)\tPrec@1 77.344 (77.344)\n",
      "Epoch: [129][19/97]\tTime 0.009 (0.013)\tLoss 0.7943 (0.7759)\tPrec@1 71.484 (73.457)\n",
      "Epoch: [129][38/97]\tTime 0.007 (0.012)\tLoss 0.7923 (0.7906)\tPrec@1 74.023 (72.616)\n",
      "Epoch: [129][57/97]\tTime 0.007 (0.012)\tLoss 0.8871 (0.8042)\tPrec@1 69.141 (71.959)\n",
      "Epoch: [129][76/97]\tTime 0.022 (0.012)\tLoss 0.8836 (0.8202)\tPrec@1 67.773 (71.246)\n",
      "Epoch: [129][95/97]\tTime 0.008 (0.011)\tLoss 0.8743 (0.8334)\tPrec@1 68.359 (70.862)\n",
      "Epoch: [129][97/97]\tTime 0.008 (0.011)\tLoss 0.8918 (0.8345)\tPrec@1 69.345 (70.812)\n",
      "EPOCH: 129 train Results: Prec@1 70.812 Loss: 0.8345\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3185 (1.3185)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4433 (1.2759)\tPrec@1 50.735 (56.300)\n",
      "EPOCH: 129 val Results: Prec@1 56.300 Loss: 1.2759\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/97]\tTime 0.015 (0.015)\tLoss 0.7529 (0.7529)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [130][19/97]\tTime 0.012 (0.011)\tLoss 0.7825 (0.7690)\tPrec@1 73.633 (73.359)\n",
      "Epoch: [130][38/97]\tTime 0.033 (0.012)\tLoss 0.7951 (0.7856)\tPrec@1 69.922 (72.576)\n",
      "Epoch: [130][57/97]\tTime 0.018 (0.011)\tLoss 0.8001 (0.8031)\tPrec@1 71.289 (71.804)\n",
      "Epoch: [130][76/97]\tTime 0.060 (0.012)\tLoss 0.8371 (0.8188)\tPrec@1 70.117 (71.155)\n",
      "Epoch: [130][95/97]\tTime 0.014 (0.012)\tLoss 0.8501 (0.8288)\tPrec@1 70.898 (70.780)\n",
      "Epoch: [130][97/97]\tTime 0.005 (0.012)\tLoss 0.8151 (0.8291)\tPrec@1 72.321 (70.792)\n",
      "EPOCH: 130 train Results: Prec@1 70.792 Loss: 0.8291\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3120 (1.3120)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4449 (1.2754)\tPrec@1 47.794 (55.750)\n",
      "EPOCH: 130 val Results: Prec@1 55.750 Loss: 1.2754\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/97]\tTime 0.008 (0.008)\tLoss 0.7656 (0.7656)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [131][19/97]\tTime 0.007 (0.010)\tLoss 0.7335 (0.7608)\tPrec@1 73.047 (73.027)\n",
      "Epoch: [131][38/97]\tTime 0.009 (0.010)\tLoss 0.7972 (0.7863)\tPrec@1 73.828 (71.960)\n",
      "Epoch: [131][57/97]\tTime 0.014 (0.010)\tLoss 0.8273 (0.8046)\tPrec@1 72.070 (71.457)\n",
      "Epoch: [131][76/97]\tTime 0.007 (0.010)\tLoss 0.8745 (0.8204)\tPrec@1 69.531 (70.871)\n",
      "Epoch: [131][95/97]\tTime 0.009 (0.011)\tLoss 0.7774 (0.8304)\tPrec@1 70.703 (70.571)\n",
      "Epoch: [131][97/97]\tTime 0.008 (0.011)\tLoss 1.0029 (0.8319)\tPrec@1 64.286 (70.518)\n",
      "EPOCH: 131 train Results: Prec@1 70.518 Loss: 0.8319\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3252 (1.3252)\tPrec@1 54.688 (54.688)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4651 (1.2804)\tPrec@1 48.162 (56.210)\n",
      "EPOCH: 131 val Results: Prec@1 56.210 Loss: 1.2804\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/97]\tTime 0.016 (0.016)\tLoss 0.7738 (0.7738)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [132][19/97]\tTime 0.011 (0.012)\tLoss 0.8354 (0.7826)\tPrec@1 69.141 (72.744)\n",
      "Epoch: [132][38/97]\tTime 0.015 (0.011)\tLoss 0.8680 (0.8013)\tPrec@1 69.336 (72.080)\n",
      "Epoch: [132][57/97]\tTime 0.017 (0.012)\tLoss 0.7809 (0.8100)\tPrec@1 72.852 (71.609)\n",
      "Epoch: [132][76/97]\tTime 0.015 (0.013)\tLoss 0.9650 (0.8221)\tPrec@1 65.234 (71.170)\n",
      "Epoch: [132][95/97]\tTime 0.009 (0.013)\tLoss 0.8819 (0.8338)\tPrec@1 68.359 (70.520)\n",
      "Epoch: [132][97/97]\tTime 0.014 (0.013)\tLoss 0.8625 (0.8340)\tPrec@1 69.345 (70.506)\n",
      "EPOCH: 132 train Results: Prec@1 70.506 Loss: 0.8340\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3124 (1.3124)\tPrec@1 51.953 (51.953)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.3986 (1.2787)\tPrec@1 51.838 (55.980)\n",
      "EPOCH: 132 val Results: Prec@1 55.980 Loss: 1.2787\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/97]\tTime 0.007 (0.007)\tLoss 0.7156 (0.7156)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [133][19/97]\tTime 0.010 (0.011)\tLoss 0.7796 (0.7735)\tPrec@1 70.312 (72.764)\n",
      "Epoch: [133][38/97]\tTime 0.011 (0.011)\tLoss 0.7978 (0.7842)\tPrec@1 72.461 (72.526)\n",
      "Epoch: [133][57/97]\tTime 0.008 (0.012)\tLoss 0.8728 (0.8008)\tPrec@1 69.141 (71.821)\n",
      "Epoch: [133][76/97]\tTime 0.009 (0.012)\tLoss 0.8294 (0.8158)\tPrec@1 69.531 (71.350)\n",
      "Epoch: [133][95/97]\tTime 0.008 (0.011)\tLoss 0.8482 (0.8292)\tPrec@1 72.070 (70.813)\n",
      "Epoch: [133][97/97]\tTime 0.005 (0.011)\tLoss 0.8932 (0.8291)\tPrec@1 64.286 (70.798)\n",
      "EPOCH: 133 train Results: Prec@1 70.798 Loss: 0.8291\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3267 (1.3267)\tPrec@1 52.344 (52.344)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.3780 (1.2717)\tPrec@1 52.941 (56.100)\n",
      "EPOCH: 133 val Results: Prec@1 56.100 Loss: 1.2717\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/97]\tTime 0.010 (0.010)\tLoss 0.7249 (0.7249)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [134][19/97]\tTime 0.007 (0.010)\tLoss 0.8132 (0.7660)\tPrec@1 72.070 (73.057)\n",
      "Epoch: [134][38/97]\tTime 0.009 (0.010)\tLoss 0.8015 (0.7820)\tPrec@1 73.633 (72.551)\n",
      "Epoch: [134][57/97]\tTime 0.009 (0.010)\tLoss 0.8574 (0.7942)\tPrec@1 68.750 (71.993)\n",
      "Epoch: [134][76/97]\tTime 0.016 (0.010)\tLoss 0.9266 (0.8109)\tPrec@1 67.773 (71.340)\n",
      "Epoch: [134][95/97]\tTime 0.026 (0.012)\tLoss 0.8924 (0.8240)\tPrec@1 68.164 (70.896)\n",
      "Epoch: [134][97/97]\tTime 0.010 (0.012)\tLoss 0.9214 (0.8255)\tPrec@1 68.452 (70.854)\n",
      "EPOCH: 134 train Results: Prec@1 70.854 Loss: 0.8255\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3544 (1.3544)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.003 (0.003)\tLoss 1.4321 (1.2857)\tPrec@1 51.471 (56.070)\n",
      "EPOCH: 134 val Results: Prec@1 56.070 Loss: 1.2857\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/97]\tTime 0.025 (0.025)\tLoss 0.7495 (0.7495)\tPrec@1 73.633 (73.633)\n",
      "Epoch: [135][19/97]\tTime 0.021 (0.012)\tLoss 0.7381 (0.7616)\tPrec@1 73.828 (73.027)\n",
      "Epoch: [135][38/97]\tTime 0.014 (0.012)\tLoss 0.7632 (0.7779)\tPrec@1 72.852 (72.451)\n",
      "Epoch: [135][57/97]\tTime 0.009 (0.012)\tLoss 0.8358 (0.7922)\tPrec@1 70.312 (72.000)\n",
      "Epoch: [135][76/97]\tTime 0.008 (0.011)\tLoss 0.8313 (0.8093)\tPrec@1 70.703 (71.469)\n",
      "Epoch: [135][95/97]\tTime 0.010 (0.011)\tLoss 0.8548 (0.8222)\tPrec@1 68.945 (70.833)\n",
      "Epoch: [135][97/97]\tTime 0.012 (0.011)\tLoss 0.9277 (0.8239)\tPrec@1 66.667 (70.768)\n",
      "EPOCH: 135 train Results: Prec@1 70.768 Loss: 0.8239\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3332 (1.3332)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4616 (1.2752)\tPrec@1 48.529 (56.330)\n",
      "EPOCH: 135 val Results: Prec@1 56.330 Loss: 1.2752\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/97]\tTime 0.010 (0.010)\tLoss 0.7343 (0.7343)\tPrec@1 74.414 (74.414)\n",
      "Epoch: [136][19/97]\tTime 0.010 (0.011)\tLoss 0.7541 (0.7791)\tPrec@1 73.633 (72.607)\n",
      "Epoch: [136][38/97]\tTime 0.010 (0.011)\tLoss 0.8141 (0.7859)\tPrec@1 70.898 (72.456)\n",
      "Epoch: [136][57/97]\tTime 0.014 (0.012)\tLoss 0.8308 (0.8031)\tPrec@1 69.727 (71.747)\n",
      "Epoch: [136][76/97]\tTime 0.021 (0.012)\tLoss 0.8799 (0.8142)\tPrec@1 68.555 (71.172)\n",
      "Epoch: [136][95/97]\tTime 0.015 (0.012)\tLoss 0.9317 (0.8271)\tPrec@1 66.406 (70.695)\n",
      "Epoch: [136][97/97]\tTime 0.006 (0.012)\tLoss 0.8759 (0.8273)\tPrec@1 70.536 (70.682)\n",
      "EPOCH: 136 train Results: Prec@1 70.682 Loss: 0.8273\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3241 (1.3241)\tPrec@1 52.344 (52.344)\n",
      "Test: [19/19]\tTime 0.003 (0.003)\tLoss 1.4715 (1.2873)\tPrec@1 51.838 (55.750)\n",
      "EPOCH: 136 val Results: Prec@1 55.750 Loss: 1.2873\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/97]\tTime 0.016 (0.016)\tLoss 0.7056 (0.7056)\tPrec@1 78.906 (78.906)\n",
      "Epoch: [137][19/97]\tTime 0.009 (0.013)\tLoss 0.8162 (0.7823)\tPrec@1 70.117 (73.076)\n",
      "Epoch: [137][38/97]\tTime 0.007 (0.011)\tLoss 0.8198 (0.7933)\tPrec@1 71.094 (72.281)\n",
      "Epoch: [137][57/97]\tTime 0.008 (0.011)\tLoss 0.8309 (0.8019)\tPrec@1 72.461 (71.761)\n",
      "Epoch: [137][76/97]\tTime 0.007 (0.011)\tLoss 0.7732 (0.8130)\tPrec@1 73.828 (71.342)\n",
      "Epoch: [137][95/97]\tTime 0.013 (0.011)\tLoss 0.8794 (0.8274)\tPrec@1 68.164 (70.791)\n",
      "Epoch: [137][97/97]\tTime 0.005 (0.011)\tLoss 0.8828 (0.8291)\tPrec@1 67.560 (70.716)\n",
      "EPOCH: 137 train Results: Prec@1 70.716 Loss: 0.8291\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3118 (1.3118)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4562 (1.2747)\tPrec@1 51.838 (56.060)\n",
      "EPOCH: 137 val Results: Prec@1 56.060 Loss: 1.2747\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/97]\tTime 0.007 (0.007)\tLoss 0.7616 (0.7616)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [138][19/97]\tTime 0.010 (0.012)\tLoss 0.7651 (0.7719)\tPrec@1 74.023 (72.705)\n",
      "Epoch: [138][38/97]\tTime 0.009 (0.011)\tLoss 0.8116 (0.7906)\tPrec@1 69.336 (72.206)\n",
      "Epoch: [138][57/97]\tTime 0.011 (0.011)\tLoss 0.9573 (0.8004)\tPrec@1 68.750 (71.811)\n",
      "Epoch: [138][76/97]\tTime 0.025 (0.011)\tLoss 0.8707 (0.8111)\tPrec@1 67.969 (71.426)\n",
      "Epoch: [138][95/97]\tTime 0.013 (0.011)\tLoss 0.8709 (0.8220)\tPrec@1 68.945 (70.955)\n",
      "Epoch: [138][97/97]\tTime 0.009 (0.011)\tLoss 1.0541 (0.8256)\tPrec@1 61.012 (70.816)\n",
      "EPOCH: 138 train Results: Prec@1 70.816 Loss: 0.8256\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.2937 (1.2937)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4666 (1.2875)\tPrec@1 52.941 (56.490)\n",
      "EPOCH: 138 val Results: Prec@1 56.490 Loss: 1.2875\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/97]\tTime 0.008 (0.008)\tLoss 0.7372 (0.7372)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [139][19/97]\tTime 0.009 (0.012)\tLoss 0.7149 (0.7752)\tPrec@1 75.195 (73.076)\n",
      "Epoch: [139][38/97]\tTime 0.006 (0.011)\tLoss 0.7801 (0.7906)\tPrec@1 74.414 (72.331)\n",
      "Epoch: [139][57/97]\tTime 0.009 (0.013)\tLoss 0.8862 (0.8011)\tPrec@1 67.773 (71.855)\n",
      "Epoch: [139][76/97]\tTime 0.014 (0.012)\tLoss 0.8133 (0.8146)\tPrec@1 70.508 (71.233)\n",
      "Epoch: [139][95/97]\tTime 0.009 (0.013)\tLoss 0.8796 (0.8251)\tPrec@1 68.750 (70.829)\n",
      "Epoch: [139][97/97]\tTime 0.009 (0.013)\tLoss 0.9187 (0.8259)\tPrec@1 62.500 (70.768)\n",
      "EPOCH: 139 train Results: Prec@1 70.768 Loss: 0.8259\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3300 (1.3300)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4717 (1.2817)\tPrec@1 50.735 (56.410)\n",
      "EPOCH: 139 val Results: Prec@1 56.410 Loss: 1.2817\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/97]\tTime 0.014 (0.014)\tLoss 0.7823 (0.7823)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [140][19/97]\tTime 0.016 (0.010)\tLoss 0.7621 (0.7768)\tPrec@1 74.805 (73.203)\n",
      "Epoch: [140][38/97]\tTime 0.007 (0.010)\tLoss 0.8060 (0.7873)\tPrec@1 71.094 (72.531)\n",
      "Epoch: [140][57/97]\tTime 0.007 (0.010)\tLoss 0.7657 (0.7996)\tPrec@1 73.047 (71.979)\n",
      "Epoch: [140][76/97]\tTime 0.007 (0.010)\tLoss 0.8818 (0.8101)\tPrec@1 66.992 (71.436)\n",
      "Epoch: [140][95/97]\tTime 0.014 (0.010)\tLoss 0.9590 (0.8235)\tPrec@1 66.211 (70.903)\n",
      "Epoch: [140][97/97]\tTime 0.007 (0.010)\tLoss 0.8230 (0.8240)\tPrec@1 69.940 (70.878)\n",
      "EPOCH: 140 train Results: Prec@1 70.878 Loss: 0.8240\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3317 (1.3317)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4375 (1.2812)\tPrec@1 49.632 (56.250)\n",
      "EPOCH: 140 val Results: Prec@1 56.250 Loss: 1.2812\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/97]\tTime 0.017 (0.017)\tLoss 0.6960 (0.6960)\tPrec@1 75.977 (75.977)\n",
      "Epoch: [141][19/97]\tTime 0.014 (0.013)\tLoss 0.8601 (0.7640)\tPrec@1 70.117 (73.555)\n",
      "Epoch: [141][38/97]\tTime 0.011 (0.012)\tLoss 0.8118 (0.7803)\tPrec@1 70.898 (72.676)\n",
      "Epoch: [141][57/97]\tTime 0.013 (0.012)\tLoss 0.8221 (0.7934)\tPrec@1 71.484 (72.202)\n",
      "Epoch: [141][76/97]\tTime 0.009 (0.012)\tLoss 0.8904 (0.8049)\tPrec@1 67.773 (71.753)\n",
      "Epoch: [141][95/97]\tTime 0.011 (0.012)\tLoss 0.8689 (0.8165)\tPrec@1 69.727 (71.267)\n",
      "Epoch: [141][97/97]\tTime 0.006 (0.012)\tLoss 0.8458 (0.8169)\tPrec@1 70.833 (71.236)\n",
      "EPOCH: 141 train Results: Prec@1 71.236 Loss: 0.8169\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3365 (1.3365)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4030 (1.2769)\tPrec@1 51.103 (56.200)\n",
      "EPOCH: 141 val Results: Prec@1 56.200 Loss: 1.2769\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/97]\tTime 0.010 (0.010)\tLoss 0.7161 (0.7161)\tPrec@1 77.148 (77.148)\n",
      "Epoch: [142][19/97]\tTime 0.007 (0.011)\tLoss 0.8016 (0.7589)\tPrec@1 74.023 (74.229)\n",
      "Epoch: [142][38/97]\tTime 0.007 (0.011)\tLoss 0.7882 (0.7698)\tPrec@1 72.070 (73.438)\n",
      "Epoch: [142][57/97]\tTime 0.011 (0.011)\tLoss 0.7881 (0.7844)\tPrec@1 72.852 (72.703)\n",
      "Epoch: [142][76/97]\tTime 0.007 (0.011)\tLoss 0.8240 (0.8008)\tPrec@1 69.336 (72.027)\n",
      "Epoch: [142][95/97]\tTime 0.014 (0.010)\tLoss 0.9058 (0.8140)\tPrec@1 66.992 (71.259)\n",
      "Epoch: [142][97/97]\tTime 0.008 (0.010)\tLoss 0.8837 (0.8157)\tPrec@1 71.429 (71.216)\n",
      "EPOCH: 142 train Results: Prec@1 71.216 Loss: 0.8157\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3310 (1.3310)\tPrec@1 53.906 (53.906)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.5031 (1.2848)\tPrec@1 48.162 (55.930)\n",
      "EPOCH: 142 val Results: Prec@1 55.930 Loss: 1.2848\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/97]\tTime 0.010 (0.010)\tLoss 0.6918 (0.6918)\tPrec@1 76.758 (76.758)\n",
      "Epoch: [143][19/97]\tTime 0.013 (0.011)\tLoss 0.8430 (0.7535)\tPrec@1 70.703 (74.268)\n",
      "Epoch: [143][38/97]\tTime 0.014 (0.011)\tLoss 0.8155 (0.7724)\tPrec@1 70.117 (73.187)\n",
      "Epoch: [143][57/97]\tTime 0.009 (0.011)\tLoss 0.9066 (0.7897)\tPrec@1 66.016 (72.249)\n",
      "Epoch: [143][76/97]\tTime 0.015 (0.011)\tLoss 0.8717 (0.8028)\tPrec@1 68.750 (71.758)\n",
      "Epoch: [143][95/97]\tTime 0.017 (0.011)\tLoss 0.9069 (0.8176)\tPrec@1 66.602 (71.171)\n",
      "Epoch: [143][97/97]\tTime 0.007 (0.011)\tLoss 0.9152 (0.8194)\tPrec@1 67.262 (71.110)\n",
      "EPOCH: 143 train Results: Prec@1 71.110 Loss: 0.8194\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3085 (1.3085)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4901 (1.2877)\tPrec@1 48.529 (56.190)\n",
      "EPOCH: 143 val Results: Prec@1 56.190 Loss: 1.2877\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/97]\tTime 0.009 (0.009)\tLoss 0.7706 (0.7706)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [144][19/97]\tTime 0.007 (0.013)\tLoss 0.7691 (0.7788)\tPrec@1 74.023 (73.076)\n",
      "Epoch: [144][38/97]\tTime 0.012 (0.014)\tLoss 0.7925 (0.7907)\tPrec@1 73.438 (72.591)\n",
      "Epoch: [144][57/97]\tTime 0.016 (0.014)\tLoss 0.7992 (0.8009)\tPrec@1 72.266 (72.208)\n",
      "Epoch: [144][76/97]\tTime 0.023 (0.015)\tLoss 0.8325 (0.8110)\tPrec@1 70.312 (71.690)\n",
      "Epoch: [144][95/97]\tTime 0.009 (0.015)\tLoss 0.9065 (0.8215)\tPrec@1 68.164 (71.254)\n",
      "Epoch: [144][97/97]\tTime 0.012 (0.015)\tLoss 0.8870 (0.8221)\tPrec@1 67.857 (71.220)\n",
      "EPOCH: 144 train Results: Prec@1 71.220 Loss: 0.8221\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3283 (1.3283)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4330 (1.2769)\tPrec@1 47.426 (56.510)\n",
      "EPOCH: 144 val Results: Prec@1 56.510 Loss: 1.2769\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/97]\tTime 0.009 (0.009)\tLoss 0.7885 (0.7885)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [145][19/97]\tTime 0.020 (0.014)\tLoss 0.7780 (0.7668)\tPrec@1 71.680 (73.242)\n",
      "Epoch: [145][38/97]\tTime 0.018 (0.013)\tLoss 0.8005 (0.7711)\tPrec@1 72.656 (73.002)\n",
      "Epoch: [145][57/97]\tTime 0.022 (0.013)\tLoss 0.8145 (0.7899)\tPrec@1 69.531 (72.165)\n",
      "Epoch: [145][76/97]\tTime 0.008 (0.012)\tLoss 0.8937 (0.8058)\tPrec@1 68.945 (71.583)\n",
      "Epoch: [145][95/97]\tTime 0.008 (0.012)\tLoss 0.8625 (0.8201)\tPrec@1 70.312 (71.124)\n",
      "Epoch: [145][97/97]\tTime 0.005 (0.012)\tLoss 0.8502 (0.8211)\tPrec@1 67.857 (71.072)\n",
      "EPOCH: 145 train Results: Prec@1 71.072 Loss: 0.8211\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3234 (1.3234)\tPrec@1 51.172 (51.172)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4763 (1.2955)\tPrec@1 51.103 (56.130)\n",
      "EPOCH: 145 val Results: Prec@1 56.130 Loss: 1.2955\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/97]\tTime 0.008 (0.008)\tLoss 0.7561 (0.7561)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [146][19/97]\tTime 0.011 (0.011)\tLoss 0.7552 (0.7591)\tPrec@1 75.195 (73.154)\n",
      "Epoch: [146][38/97]\tTime 0.013 (0.011)\tLoss 0.8462 (0.7794)\tPrec@1 71.484 (72.396)\n",
      "Epoch: [146][57/97]\tTime 0.015 (0.011)\tLoss 0.8311 (0.7976)\tPrec@1 70.898 (71.747)\n",
      "Epoch: [146][76/97]\tTime 0.015 (0.011)\tLoss 0.8229 (0.8096)\tPrec@1 69.141 (71.122)\n",
      "Epoch: [146][95/97]\tTime 0.012 (0.012)\tLoss 0.9278 (0.8203)\tPrec@1 69.336 (70.748)\n",
      "Epoch: [146][97/97]\tTime 0.011 (0.012)\tLoss 0.8757 (0.8215)\tPrec@1 69.048 (70.716)\n",
      "EPOCH: 146 train Results: Prec@1 70.716 Loss: 0.8215\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3055 (1.3055)\tPrec@1 52.539 (52.539)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4627 (1.2872)\tPrec@1 47.426 (55.730)\n",
      "EPOCH: 146 val Results: Prec@1 55.730 Loss: 1.2872\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/97]\tTime 0.010 (0.010)\tLoss 0.6989 (0.6989)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [147][19/97]\tTime 0.011 (0.012)\tLoss 0.7316 (0.7589)\tPrec@1 74.414 (73.662)\n",
      "Epoch: [147][38/97]\tTime 0.006 (0.011)\tLoss 0.8076 (0.7756)\tPrec@1 71.875 (72.927)\n",
      "Epoch: [147][57/97]\tTime 0.010 (0.012)\tLoss 0.7583 (0.7845)\tPrec@1 73.633 (72.633)\n",
      "Epoch: [147][76/97]\tTime 0.008 (0.011)\tLoss 0.7873 (0.7972)\tPrec@1 72.656 (72.088)\n",
      "Epoch: [147][95/97]\tTime 0.011 (0.011)\tLoss 0.9282 (0.8108)\tPrec@1 64.844 (71.560)\n",
      "Epoch: [147][97/97]\tTime 0.012 (0.011)\tLoss 0.8836 (0.8130)\tPrec@1 68.750 (71.486)\n",
      "EPOCH: 147 train Results: Prec@1 71.486 Loss: 0.8130\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3259 (1.3259)\tPrec@1 55.273 (55.273)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4233 (1.2933)\tPrec@1 49.265 (55.870)\n",
      "EPOCH: 147 val Results: Prec@1 55.870 Loss: 1.2933\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/97]\tTime 0.014 (0.014)\tLoss 0.6941 (0.6941)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [148][19/97]\tTime 0.012 (0.012)\tLoss 0.7945 (0.7586)\tPrec@1 73.047 (73.701)\n",
      "Epoch: [148][38/97]\tTime 0.009 (0.012)\tLoss 0.8665 (0.7749)\tPrec@1 66.992 (72.837)\n",
      "Epoch: [148][57/97]\tTime 0.008 (0.012)\tLoss 0.8165 (0.7860)\tPrec@1 71.680 (72.434)\n",
      "Epoch: [148][76/97]\tTime 0.009 (0.012)\tLoss 0.9037 (0.7997)\tPrec@1 68.164 (71.926)\n",
      "Epoch: [148][95/97]\tTime 0.012 (0.012)\tLoss 0.8520 (0.8136)\tPrec@1 66.406 (71.285)\n",
      "Epoch: [148][97/97]\tTime 0.006 (0.012)\tLoss 0.8599 (0.8140)\tPrec@1 68.452 (71.248)\n",
      "EPOCH: 148 train Results: Prec@1 71.248 Loss: 0.8140\n",
      "Test: [0/19]\tTime 0.005 (0.005)\tLoss 1.3215 (1.3215)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.5417 (1.2876)\tPrec@1 47.059 (55.610)\n",
      "EPOCH: 148 val Results: Prec@1 55.610 Loss: 1.2876\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/97]\tTime 0.008 (0.008)\tLoss 0.7136 (0.7136)\tPrec@1 76.172 (76.172)\n",
      "Epoch: [149][19/97]\tTime 0.011 (0.011)\tLoss 0.7354 (0.7602)\tPrec@1 74.609 (73.457)\n",
      "Epoch: [149][38/97]\tTime 0.019 (0.011)\tLoss 0.8924 (0.7745)\tPrec@1 66.406 (72.646)\n",
      "Epoch: [149][57/97]\tTime 0.013 (0.013)\tLoss 0.9056 (0.7877)\tPrec@1 68.359 (72.171)\n",
      "Epoch: [149][76/97]\tTime 0.017 (0.013)\tLoss 0.8029 (0.8007)\tPrec@1 70.117 (71.634)\n",
      "Epoch: [149][95/97]\tTime 0.007 (0.013)\tLoss 0.9304 (0.8186)\tPrec@1 67.969 (70.982)\n",
      "Epoch: [149][97/97]\tTime 0.007 (0.013)\tLoss 0.8275 (0.8188)\tPrec@1 66.964 (70.958)\n",
      "EPOCH: 149 train Results: Prec@1 70.958 Loss: 0.8188\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3005 (1.3005)\tPrec@1 53.711 (53.711)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.4923 (1.2894)\tPrec@1 46.691 (56.010)\n",
      "EPOCH: 149 val Results: Prec@1 56.010 Loss: 1.2894\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/97]\tTime 0.026 (0.026)\tLoss 0.7051 (0.7051)\tPrec@1 77.734 (77.734)\n",
      "Epoch: [150][19/97]\tTime 0.010 (0.012)\tLoss 0.7982 (0.7443)\tPrec@1 69.922 (73.740)\n",
      "Epoch: [150][38/97]\tTime 0.016 (0.012)\tLoss 0.7628 (0.7678)\tPrec@1 70.508 (72.806)\n",
      "Epoch: [150][57/97]\tTime 0.009 (0.011)\tLoss 0.7998 (0.7918)\tPrec@1 70.703 (71.794)\n",
      "Epoch: [150][76/97]\tTime 0.008 (0.011)\tLoss 0.8357 (0.8082)\tPrec@1 70.703 (71.284)\n",
      "Epoch: [150][95/97]\tTime 0.010 (0.012)\tLoss 0.8839 (0.8156)\tPrec@1 67.188 (71.039)\n",
      "Epoch: [150][97/97]\tTime 0.007 (0.012)\tLoss 0.9060 (0.8168)\tPrec@1 67.560 (70.986)\n",
      "EPOCH: 150 train Results: Prec@1 70.986 Loss: 0.8168\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3416 (1.3416)\tPrec@1 55.273 (55.273)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4991 (1.2875)\tPrec@1 50.000 (56.370)\n",
      "EPOCH: 150 val Results: Prec@1 56.370 Loss: 1.2875\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/97]\tTime 0.012 (0.012)\tLoss 0.7490 (0.7490)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [151][19/97]\tTime 0.011 (0.012)\tLoss 0.8180 (0.7606)\tPrec@1 72.461 (73.691)\n",
      "Epoch: [151][38/97]\tTime 0.008 (0.011)\tLoss 0.8160 (0.7686)\tPrec@1 68.555 (73.277)\n",
      "Epoch: [151][57/97]\tTime 0.009 (0.011)\tLoss 0.8263 (0.7804)\tPrec@1 69.336 (72.771)\n",
      "Epoch: [151][76/97]\tTime 0.007 (0.011)\tLoss 0.8695 (0.7966)\tPrec@1 68.750 (72.169)\n",
      "Epoch: [151][95/97]\tTime 0.007 (0.011)\tLoss 0.9546 (0.8111)\tPrec@1 69.336 (71.501)\n",
      "Epoch: [151][97/97]\tTime 0.008 (0.011)\tLoss 0.8064 (0.8122)\tPrec@1 74.702 (71.474)\n",
      "EPOCH: 151 train Results: Prec@1 71.474 Loss: 0.8122\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3100 (1.3100)\tPrec@1 52.930 (52.930)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.4541 (1.2989)\tPrec@1 45.956 (56.040)\n",
      "EPOCH: 151 val Results: Prec@1 56.040 Loss: 1.2989\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/97]\tTime 0.019 (0.019)\tLoss 0.7249 (0.7249)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [152][19/97]\tTime 0.010 (0.011)\tLoss 0.7290 (0.7578)\tPrec@1 75.781 (73.672)\n",
      "Epoch: [152][38/97]\tTime 0.016 (0.011)\tLoss 0.7856 (0.7752)\tPrec@1 73.242 (72.892)\n",
      "Epoch: [152][57/97]\tTime 0.007 (0.011)\tLoss 0.8387 (0.7885)\tPrec@1 66.992 (72.380)\n",
      "Epoch: [152][76/97]\tTime 0.008 (0.011)\tLoss 0.8629 (0.8013)\tPrec@1 67.383 (71.857)\n",
      "Epoch: [152][95/97]\tTime 0.011 (0.012)\tLoss 0.7998 (0.8154)\tPrec@1 72.266 (71.246)\n",
      "Epoch: [152][97/97]\tTime 0.005 (0.012)\tLoss 0.8124 (0.8164)\tPrec@1 70.833 (71.204)\n",
      "EPOCH: 152 train Results: Prec@1 71.204 Loss: 0.8164\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3256 (1.3256)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4570 (1.2914)\tPrec@1 47.059 (55.990)\n",
      "EPOCH: 152 val Results: Prec@1 55.990 Loss: 1.2914\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/97]\tTime 0.010 (0.010)\tLoss 0.6443 (0.6443)\tPrec@1 78.516 (78.516)\n",
      "Epoch: [153][19/97]\tTime 0.011 (0.011)\tLoss 0.7098 (0.7555)\tPrec@1 75.586 (73.721)\n",
      "Epoch: [153][38/97]\tTime 0.008 (0.011)\tLoss 0.8765 (0.7744)\tPrec@1 69.727 (73.117)\n",
      "Epoch: [153][57/97]\tTime 0.006 (0.011)\tLoss 0.8254 (0.7888)\tPrec@1 69.336 (72.296)\n",
      "Epoch: [153][76/97]\tTime 0.008 (0.011)\tLoss 0.8884 (0.8001)\tPrec@1 68.164 (71.807)\n",
      "Epoch: [153][95/97]\tTime 0.010 (0.011)\tLoss 0.8905 (0.8134)\tPrec@1 69.531 (71.322)\n",
      "Epoch: [153][97/97]\tTime 0.005 (0.011)\tLoss 0.7649 (0.8139)\tPrec@1 75.000 (71.316)\n",
      "EPOCH: 153 train Results: Prec@1 71.316 Loss: 0.8139\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3064 (1.3064)\tPrec@1 54.883 (54.883)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.5014 (1.2861)\tPrec@1 49.265 (56.060)\n",
      "EPOCH: 153 val Results: Prec@1 56.060 Loss: 1.2861\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/97]\tTime 0.014 (0.014)\tLoss 0.7316 (0.7316)\tPrec@1 72.852 (72.852)\n",
      "Epoch: [154][19/97]\tTime 0.014 (0.011)\tLoss 0.7762 (0.7589)\tPrec@1 74.219 (73.809)\n",
      "Epoch: [154][38/97]\tTime 0.017 (0.011)\tLoss 0.8040 (0.7711)\tPrec@1 72.461 (73.277)\n",
      "Epoch: [154][57/97]\tTime 0.019 (0.013)\tLoss 0.8424 (0.7867)\tPrec@1 68.555 (72.495)\n",
      "Epoch: [154][76/97]\tTime 0.026 (0.013)\tLoss 0.8865 (0.8076)\tPrec@1 69.727 (71.690)\n",
      "Epoch: [154][95/97]\tTime 0.016 (0.012)\tLoss 0.8550 (0.8143)\tPrec@1 68.750 (71.362)\n",
      "Epoch: [154][97/97]\tTime 0.005 (0.012)\tLoss 0.9281 (0.8159)\tPrec@1 67.560 (71.290)\n",
      "EPOCH: 154 train Results: Prec@1 71.290 Loss: 0.8159\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3536 (1.3536)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4473 (1.2951)\tPrec@1 51.103 (55.760)\n",
      "EPOCH: 154 val Results: Prec@1 55.760 Loss: 1.2951\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/97]\tTime 0.012 (0.012)\tLoss 0.7661 (0.7661)\tPrec@1 72.070 (72.070)\n",
      "Epoch: [155][19/97]\tTime 0.006 (0.011)\tLoss 0.7510 (0.7496)\tPrec@1 73.828 (73.984)\n",
      "Epoch: [155][38/97]\tTime 0.010 (0.011)\tLoss 0.7867 (0.7611)\tPrec@1 72.461 (73.508)\n",
      "Epoch: [155][57/97]\tTime 0.016 (0.011)\tLoss 0.7929 (0.7805)\tPrec@1 75.781 (72.771)\n",
      "Epoch: [155][76/97]\tTime 0.007 (0.011)\tLoss 0.8942 (0.7982)\tPrec@1 68.945 (72.091)\n",
      "Epoch: [155][95/97]\tTime 0.014 (0.011)\tLoss 0.8567 (0.8097)\tPrec@1 69.531 (71.535)\n",
      "Epoch: [155][97/97]\tTime 0.011 (0.011)\tLoss 0.9114 (0.8109)\tPrec@1 67.560 (71.494)\n",
      "EPOCH: 155 train Results: Prec@1 71.494 Loss: 0.8109\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3169 (1.3169)\tPrec@1 55.078 (55.078)\n",
      "Test: [19/19]\tTime 0.007 (0.003)\tLoss 1.5137 (1.3038)\tPrec@1 47.426 (55.780)\n",
      "EPOCH: 155 val Results: Prec@1 55.780 Loss: 1.3038\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/97]\tTime 0.016 (0.016)\tLoss 0.7281 (0.7281)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [156][19/97]\tTime 0.009 (0.011)\tLoss 0.7015 (0.7354)\tPrec@1 76.562 (74.248)\n",
      "Epoch: [156][38/97]\tTime 0.010 (0.011)\tLoss 0.7554 (0.7603)\tPrec@1 71.875 (73.337)\n",
      "Epoch: [156][57/97]\tTime 0.009 (0.011)\tLoss 0.8380 (0.7811)\tPrec@1 68.359 (72.340)\n",
      "Epoch: [156][76/97]\tTime 0.015 (0.011)\tLoss 0.9105 (0.7963)\tPrec@1 68.945 (71.751)\n",
      "Epoch: [156][95/97]\tTime 0.007 (0.011)\tLoss 0.8734 (0.8104)\tPrec@1 70.312 (71.307)\n",
      "Epoch: [156][97/97]\tTime 0.011 (0.011)\tLoss 0.9050 (0.8122)\tPrec@1 64.881 (71.198)\n",
      "EPOCH: 156 train Results: Prec@1 71.198 Loss: 0.8122\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3225 (1.3225)\tPrec@1 54.688 (54.688)\n",
      "Test: [19/19]\tTime 0.005 (0.002)\tLoss 1.4497 (1.2993)\tPrec@1 50.000 (55.940)\n",
      "EPOCH: 156 val Results: Prec@1 55.940 Loss: 1.2993\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/97]\tTime 0.008 (0.008)\tLoss 0.8124 (0.8124)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [157][19/97]\tTime 0.013 (0.013)\tLoss 0.8209 (0.7528)\tPrec@1 70.508 (73.809)\n",
      "Epoch: [157][38/97]\tTime 0.008 (0.013)\tLoss 0.7996 (0.7635)\tPrec@1 73.047 (73.392)\n",
      "Epoch: [157][57/97]\tTime 0.011 (0.012)\tLoss 0.9040 (0.7877)\tPrec@1 68.164 (72.323)\n",
      "Epoch: [157][76/97]\tTime 0.009 (0.012)\tLoss 0.8691 (0.8006)\tPrec@1 68.555 (71.822)\n",
      "Epoch: [157][95/97]\tTime 0.012 (0.012)\tLoss 0.9147 (0.8111)\tPrec@1 67.188 (71.442)\n",
      "Epoch: [157][97/97]\tTime 0.009 (0.013)\tLoss 0.9132 (0.8131)\tPrec@1 64.881 (71.334)\n",
      "EPOCH: 157 train Results: Prec@1 71.334 Loss: 0.8131\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3571 (1.3571)\tPrec@1 53.320 (53.320)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4522 (1.3027)\tPrec@1 52.941 (56.130)\n",
      "EPOCH: 157 val Results: Prec@1 56.130 Loss: 1.3027\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/97]\tTime 0.015 (0.015)\tLoss 0.6750 (0.6750)\tPrec@1 76.562 (76.562)\n",
      "Epoch: [158][19/97]\tTime 0.007 (0.011)\tLoss 0.8011 (0.7602)\tPrec@1 71.680 (73.125)\n",
      "Epoch: [158][38/97]\tTime 0.012 (0.011)\tLoss 0.7694 (0.7728)\tPrec@1 75.781 (72.641)\n",
      "Epoch: [158][57/97]\tTime 0.008 (0.011)\tLoss 0.8717 (0.7868)\tPrec@1 70.117 (72.097)\n",
      "Epoch: [158][76/97]\tTime 0.009 (0.011)\tLoss 0.8229 (0.7995)\tPrec@1 70.898 (71.566)\n",
      "Epoch: [158][95/97]\tTime 0.009 (0.011)\tLoss 0.8172 (0.8105)\tPrec@1 72.852 (71.291)\n",
      "Epoch: [158][97/97]\tTime 0.005 (0.011)\tLoss 0.9099 (0.8119)\tPrec@1 67.857 (71.238)\n",
      "EPOCH: 158 train Results: Prec@1 71.238 Loss: 0.8119\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3359 (1.3359)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4442 (1.2985)\tPrec@1 52.206 (55.480)\n",
      "EPOCH: 158 val Results: Prec@1 55.480 Loss: 1.2985\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/97]\tTime 0.008 (0.008)\tLoss 0.7433 (0.7433)\tPrec@1 76.367 (76.367)\n",
      "Epoch: [159][19/97]\tTime 0.012 (0.012)\tLoss 0.7845 (0.7589)\tPrec@1 71.875 (73.906)\n",
      "Epoch: [159][38/97]\tTime 0.012 (0.011)\tLoss 0.7926 (0.7736)\tPrec@1 72.266 (73.252)\n",
      "Epoch: [159][57/97]\tTime 0.007 (0.011)\tLoss 0.8898 (0.7835)\tPrec@1 66.602 (72.505)\n",
      "Epoch: [159][76/97]\tTime 0.007 (0.011)\tLoss 0.9172 (0.7960)\tPrec@1 67.383 (71.916)\n",
      "Epoch: [159][95/97]\tTime 0.008 (0.010)\tLoss 0.9025 (0.8091)\tPrec@1 69.727 (71.324)\n",
      "Epoch: [159][97/97]\tTime 0.008 (0.010)\tLoss 0.9273 (0.8099)\tPrec@1 68.155 (71.308)\n",
      "EPOCH: 159 train Results: Prec@1 71.308 Loss: 0.8099\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3429 (1.3429)\tPrec@1 52.539 (52.539)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4572 (1.2969)\tPrec@1 50.735 (55.900)\n",
      "EPOCH: 159 val Results: Prec@1 55.900 Loss: 1.2969\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/97]\tTime 0.008 (0.008)\tLoss 0.7974 (0.7974)\tPrec@1 70.508 (70.508)\n",
      "Epoch: [160][19/97]\tTime 0.007 (0.010)\tLoss 0.7811 (0.7731)\tPrec@1 74.609 (72.939)\n",
      "Epoch: [160][38/97]\tTime 0.008 (0.010)\tLoss 0.7281 (0.7756)\tPrec@1 72.461 (72.636)\n",
      "Epoch: [160][57/97]\tTime 0.008 (0.010)\tLoss 0.8170 (0.7882)\tPrec@1 70.117 (72.323)\n",
      "Epoch: [160][76/97]\tTime 0.012 (0.010)\tLoss 0.8422 (0.7979)\tPrec@1 67.773 (71.741)\n",
      "Epoch: [160][95/97]\tTime 0.016 (0.010)\tLoss 0.8121 (0.8053)\tPrec@1 69.141 (71.391)\n",
      "Epoch: [160][97/97]\tTime 0.005 (0.010)\tLoss 0.8529 (0.8057)\tPrec@1 68.750 (71.380)\n",
      "EPOCH: 160 train Results: Prec@1 71.380 Loss: 0.8057\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3458 (1.3458)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.003 (0.002)\tLoss 1.4649 (1.2926)\tPrec@1 46.324 (56.220)\n",
      "EPOCH: 160 val Results: Prec@1 56.220 Loss: 1.2926\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/97]\tTime 0.011 (0.011)\tLoss 0.7348 (0.7348)\tPrec@1 75.195 (75.195)\n",
      "Epoch: [161][19/97]\tTime 0.008 (0.011)\tLoss 0.7015 (0.7531)\tPrec@1 75.391 (73.525)\n",
      "Epoch: [161][38/97]\tTime 0.010 (0.011)\tLoss 0.7427 (0.7764)\tPrec@1 73.828 (72.676)\n",
      "Epoch: [161][57/97]\tTime 0.031 (0.012)\tLoss 0.7834 (0.7860)\tPrec@1 73.242 (72.313)\n",
      "Epoch: [161][76/97]\tTime 0.014 (0.013)\tLoss 0.8301 (0.7946)\tPrec@1 71.094 (72.022)\n",
      "Epoch: [161][95/97]\tTime 0.012 (0.012)\tLoss 0.9000 (0.8101)\tPrec@1 68.555 (71.346)\n",
      "Epoch: [161][97/97]\tTime 0.005 (0.012)\tLoss 0.9020 (0.8117)\tPrec@1 65.476 (71.280)\n",
      "EPOCH: 161 train Results: Prec@1 71.280 Loss: 0.8117\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3447 (1.3447)\tPrec@1 54.688 (54.688)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4186 (1.2891)\tPrec@1 49.632 (56.210)\n",
      "EPOCH: 161 val Results: Prec@1 56.210 Loss: 1.2891\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/97]\tTime 0.008 (0.008)\tLoss 0.6449 (0.6449)\tPrec@1 79.297 (79.297)\n",
      "Epoch: [162][19/97]\tTime 0.009 (0.013)\tLoss 0.8246 (0.7499)\tPrec@1 70.312 (73.721)\n",
      "Epoch: [162][38/97]\tTime 0.009 (0.011)\tLoss 0.7935 (0.7732)\tPrec@1 72.070 (72.842)\n",
      "Epoch: [162][57/97]\tTime 0.007 (0.011)\tLoss 0.8198 (0.7813)\tPrec@1 72.461 (72.491)\n",
      "Epoch: [162][76/97]\tTime 0.010 (0.011)\tLoss 0.8199 (0.7927)\tPrec@1 70.703 (71.997)\n",
      "Epoch: [162][95/97]\tTime 0.009 (0.011)\tLoss 0.8327 (0.8036)\tPrec@1 70.508 (71.509)\n",
      "Epoch: [162][97/97]\tTime 0.005 (0.011)\tLoss 0.8580 (0.8051)\tPrec@1 69.048 (71.462)\n",
      "EPOCH: 162 train Results: Prec@1 71.462 Loss: 0.8051\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.3224 (1.3224)\tPrec@1 52.539 (52.539)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4429 (1.2948)\tPrec@1 51.471 (55.870)\n",
      "EPOCH: 162 val Results: Prec@1 55.870 Loss: 1.2948\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/97]\tTime 0.019 (0.019)\tLoss 0.7591 (0.7591)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [163][19/97]\tTime 0.010 (0.011)\tLoss 0.7456 (0.7514)\tPrec@1 73.242 (73.506)\n",
      "Epoch: [163][38/97]\tTime 0.009 (0.011)\tLoss 0.7647 (0.7538)\tPrec@1 73.828 (73.427)\n",
      "Epoch: [163][57/97]\tTime 0.009 (0.012)\tLoss 0.8696 (0.7748)\tPrec@1 70.312 (72.690)\n",
      "Epoch: [163][76/97]\tTime 0.012 (0.012)\tLoss 0.8697 (0.7913)\tPrec@1 67.969 (72.035)\n",
      "Epoch: [163][95/97]\tTime 0.013 (0.012)\tLoss 0.8711 (0.8046)\tPrec@1 69.922 (71.566)\n",
      "Epoch: [163][97/97]\tTime 0.007 (0.012)\tLoss 0.8866 (0.8057)\tPrec@1 67.262 (71.502)\n",
      "EPOCH: 163 train Results: Prec@1 71.502 Loss: 0.8057\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3340 (1.3340)\tPrec@1 53.320 (53.320)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.4460 (1.2936)\tPrec@1 50.368 (56.320)\n",
      "EPOCH: 163 val Results: Prec@1 56.320 Loss: 1.2936\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/97]\tTime 0.011 (0.011)\tLoss 0.7965 (0.7965)\tPrec@1 74.023 (74.023)\n",
      "Epoch: [164][19/97]\tTime 0.014 (0.011)\tLoss 0.8237 (0.7484)\tPrec@1 71.094 (74.082)\n",
      "Epoch: [164][38/97]\tTime 0.014 (0.011)\tLoss 0.7812 (0.7645)\tPrec@1 72.070 (73.302)\n",
      "Epoch: [164][57/97]\tTime 0.016 (0.012)\tLoss 0.7766 (0.7773)\tPrec@1 73.438 (72.693)\n",
      "Epoch: [164][76/97]\tTime 0.007 (0.013)\tLoss 0.8541 (0.7972)\tPrec@1 68.555 (71.888)\n",
      "Epoch: [164][95/97]\tTime 0.008 (0.013)\tLoss 0.9029 (0.8079)\tPrec@1 66.797 (71.505)\n",
      "Epoch: [164][97/97]\tTime 0.012 (0.013)\tLoss 0.8721 (0.8086)\tPrec@1 65.179 (71.442)\n",
      "EPOCH: 164 train Results: Prec@1 71.442 Loss: 0.8086\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3318 (1.3318)\tPrec@1 52.930 (52.930)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4194 (1.3008)\tPrec@1 50.368 (55.940)\n",
      "EPOCH: 164 val Results: Prec@1 55.940 Loss: 1.3008\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/97]\tTime 0.007 (0.007)\tLoss 0.7412 (0.7412)\tPrec@1 74.805 (74.805)\n",
      "Epoch: [165][19/97]\tTime 0.012 (0.011)\tLoss 0.6907 (0.7241)\tPrec@1 73.633 (74.688)\n",
      "Epoch: [165][38/97]\tTime 0.016 (0.011)\tLoss 0.7372 (0.7566)\tPrec@1 74.414 (73.352)\n",
      "Epoch: [165][57/97]\tTime 0.026 (0.012)\tLoss 0.8221 (0.7690)\tPrec@1 70.508 (72.902)\n",
      "Epoch: [165][76/97]\tTime 0.011 (0.014)\tLoss 0.7647 (0.7845)\tPrec@1 72.656 (72.304)\n",
      "Epoch: [165][95/97]\tTime 0.024 (0.013)\tLoss 0.8559 (0.8002)\tPrec@1 68.945 (71.714)\n",
      "Epoch: [165][97/97]\tTime 0.006 (0.013)\tLoss 0.8618 (0.8014)\tPrec@1 70.833 (71.680)\n",
      "EPOCH: 165 train Results: Prec@1 71.680 Loss: 0.8014\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3362 (1.3362)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4948 (1.3055)\tPrec@1 49.265 (55.730)\n",
      "EPOCH: 165 val Results: Prec@1 55.730 Loss: 1.3055\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/97]\tTime 0.011 (0.011)\tLoss 0.7638 (0.7638)\tPrec@1 74.023 (74.023)\n",
      "Epoch: [166][19/97]\tTime 0.010 (0.012)\tLoss 0.7263 (0.7476)\tPrec@1 77.148 (74.268)\n",
      "Epoch: [166][38/97]\tTime 0.007 (0.011)\tLoss 0.8584 (0.7606)\tPrec@1 66.406 (73.272)\n",
      "Epoch: [166][57/97]\tTime 0.014 (0.011)\tLoss 0.7387 (0.7773)\tPrec@1 74.219 (72.666)\n",
      "Epoch: [166][76/97]\tTime 0.008 (0.010)\tLoss 0.8132 (0.7905)\tPrec@1 72.656 (72.230)\n",
      "Epoch: [166][95/97]\tTime 0.008 (0.010)\tLoss 0.8628 (0.8009)\tPrec@1 67.383 (71.755)\n",
      "Epoch: [166][97/97]\tTime 0.005 (0.010)\tLoss 0.8575 (0.8030)\tPrec@1 69.345 (71.670)\n",
      "EPOCH: 166 train Results: Prec@1 71.670 Loss: 0.8030\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3458 (1.3458)\tPrec@1 54.297 (54.297)\n",
      "Test: [19/19]\tTime 0.007 (0.003)\tLoss 1.4950 (1.3009)\tPrec@1 48.162 (55.880)\n",
      "EPOCH: 166 val Results: Prec@1 55.880 Loss: 1.3009\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/97]\tTime 0.017 (0.017)\tLoss 0.7686 (0.7686)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [167][19/97]\tTime 0.008 (0.010)\tLoss 0.7754 (0.7506)\tPrec@1 72.461 (74.072)\n",
      "Epoch: [167][38/97]\tTime 0.015 (0.011)\tLoss 0.7784 (0.7601)\tPrec@1 73.828 (73.197)\n",
      "Epoch: [167][57/97]\tTime 0.012 (0.012)\tLoss 0.8866 (0.7784)\tPrec@1 70.117 (72.498)\n",
      "Epoch: [167][76/97]\tTime 0.012 (0.012)\tLoss 0.8589 (0.7904)\tPrec@1 67.773 (71.890)\n",
      "Epoch: [167][95/97]\tTime 0.011 (0.012)\tLoss 0.8615 (0.8054)\tPrec@1 69.336 (71.307)\n",
      "Epoch: [167][97/97]\tTime 0.008 (0.012)\tLoss 0.9279 (0.8059)\tPrec@1 67.857 (71.296)\n",
      "EPOCH: 167 train Results: Prec@1 71.296 Loss: 0.8059\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3351 (1.3351)\tPrec@1 52.344 (52.344)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.4492 (1.2987)\tPrec@1 51.471 (55.300)\n",
      "EPOCH: 167 val Results: Prec@1 55.300 Loss: 1.2987\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/97]\tTime 0.013 (0.013)\tLoss 0.6922 (0.6922)\tPrec@1 76.758 (76.758)\n",
      "Epoch: [168][19/97]\tTime 0.008 (0.011)\tLoss 0.7295 (0.7426)\tPrec@1 72.461 (73.828)\n",
      "Epoch: [168][38/97]\tTime 0.007 (0.011)\tLoss 0.8042 (0.7644)\tPrec@1 70.898 (72.877)\n",
      "Epoch: [168][57/97]\tTime 0.008 (0.011)\tLoss 0.8110 (0.7750)\tPrec@1 71.680 (72.353)\n",
      "Epoch: [168][76/97]\tTime 0.009 (0.011)\tLoss 0.8818 (0.7920)\tPrec@1 68.164 (71.829)\n",
      "Epoch: [168][95/97]\tTime 0.009 (0.011)\tLoss 0.8473 (0.8044)\tPrec@1 71.094 (71.411)\n",
      "Epoch: [168][97/97]\tTime 0.009 (0.011)\tLoss 0.8333 (0.8050)\tPrec@1 71.131 (71.394)\n",
      "EPOCH: 168 train Results: Prec@1 71.394 Loss: 0.8050\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3594 (1.3594)\tPrec@1 55.469 (55.469)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4505 (1.3153)\tPrec@1 49.265 (55.840)\n",
      "EPOCH: 168 val Results: Prec@1 55.840 Loss: 1.3153\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/97]\tTime 0.007 (0.007)\tLoss 0.7562 (0.7562)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [169][19/97]\tTime 0.019 (0.010)\tLoss 0.7729 (0.7525)\tPrec@1 73.828 (73.574)\n",
      "Epoch: [169][38/97]\tTime 0.011 (0.010)\tLoss 0.7513 (0.7635)\tPrec@1 73.047 (73.147)\n",
      "Epoch: [169][57/97]\tTime 0.007 (0.010)\tLoss 0.8534 (0.7797)\tPrec@1 67.188 (72.501)\n",
      "Epoch: [169][76/97]\tTime 0.007 (0.010)\tLoss 0.7514 (0.7914)\tPrec@1 72.461 (72.088)\n",
      "Epoch: [169][95/97]\tTime 0.033 (0.011)\tLoss 0.8341 (0.8030)\tPrec@1 71.289 (71.613)\n",
      "Epoch: [169][97/97]\tTime 0.005 (0.011)\tLoss 0.9179 (0.8038)\tPrec@1 67.262 (71.586)\n",
      "EPOCH: 169 train Results: Prec@1 71.586 Loss: 0.8038\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3258 (1.3258)\tPrec@1 54.297 (54.297)\n",
      "Test: [19/19]\tTime 0.002 (0.005)\tLoss 1.4190 (1.3139)\tPrec@1 48.162 (55.590)\n",
      "EPOCH: 169 val Results: Prec@1 55.590 Loss: 1.3139\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/97]\tTime 0.009 (0.009)\tLoss 0.7025 (0.7025)\tPrec@1 75.977 (75.977)\n",
      "Epoch: [170][19/97]\tTime 0.009 (0.012)\tLoss 0.7513 (0.7467)\tPrec@1 72.266 (73.740)\n",
      "Epoch: [170][38/97]\tTime 0.028 (0.012)\tLoss 0.7655 (0.7636)\tPrec@1 73.047 (73.202)\n",
      "Epoch: [170][57/97]\tTime 0.072 (0.013)\tLoss 0.7978 (0.7752)\tPrec@1 72.656 (72.700)\n",
      "Epoch: [170][76/97]\tTime 0.020 (0.013)\tLoss 0.8643 (0.7943)\tPrec@1 66.992 (71.850)\n",
      "Epoch: [170][95/97]\tTime 0.011 (0.013)\tLoss 0.8769 (0.8054)\tPrec@1 69.141 (71.415)\n",
      "Epoch: [170][97/97]\tTime 0.006 (0.013)\tLoss 0.7714 (0.8055)\tPrec@1 78.274 (71.450)\n",
      "EPOCH: 170 train Results: Prec@1 71.450 Loss: 0.8055\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3574 (1.3574)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4581 (1.3110)\tPrec@1 50.368 (56.230)\n",
      "EPOCH: 170 val Results: Prec@1 56.230 Loss: 1.3110\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/97]\tTime 0.012 (0.012)\tLoss 0.7084 (0.7084)\tPrec@1 74.414 (74.414)\n",
      "Epoch: [171][19/97]\tTime 0.014 (0.012)\tLoss 0.7837 (0.7394)\tPrec@1 72.852 (74.199)\n",
      "Epoch: [171][38/97]\tTime 0.011 (0.011)\tLoss 0.7492 (0.7546)\tPrec@1 75.391 (73.638)\n",
      "Epoch: [171][57/97]\tTime 0.009 (0.011)\tLoss 0.8729 (0.7727)\tPrec@1 68.750 (72.882)\n",
      "Epoch: [171][76/97]\tTime 0.007 (0.011)\tLoss 0.8401 (0.7854)\tPrec@1 67.578 (72.144)\n",
      "Epoch: [171][95/97]\tTime 0.009 (0.011)\tLoss 0.8852 (0.7987)\tPrec@1 69.336 (71.659)\n",
      "Epoch: [171][97/97]\tTime 0.006 (0.011)\tLoss 0.8333 (0.8000)\tPrec@1 69.940 (71.640)\n",
      "EPOCH: 171 train Results: Prec@1 71.640 Loss: 0.8000\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3495 (1.3495)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.5123 (1.3149)\tPrec@1 48.897 (55.870)\n",
      "EPOCH: 171 val Results: Prec@1 55.870 Loss: 1.3149\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/97]\tTime 0.008 (0.008)\tLoss 0.7797 (0.7797)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [172][19/97]\tTime 0.007 (0.012)\tLoss 0.7890 (0.7553)\tPrec@1 71.680 (73.389)\n",
      "Epoch: [172][38/97]\tTime 0.008 (0.011)\tLoss 0.8341 (0.7636)\tPrec@1 73.242 (73.167)\n",
      "Epoch: [172][57/97]\tTime 0.007 (0.011)\tLoss 0.8722 (0.7792)\tPrec@1 68.164 (72.417)\n",
      "Epoch: [172][76/97]\tTime 0.012 (0.011)\tLoss 0.8214 (0.7930)\tPrec@1 68.555 (71.837)\n",
      "Epoch: [172][95/97]\tTime 0.019 (0.011)\tLoss 0.8049 (0.8043)\tPrec@1 71.484 (71.446)\n",
      "Epoch: [172][97/97]\tTime 0.005 (0.011)\tLoss 0.9156 (0.8060)\tPrec@1 66.667 (71.374)\n",
      "EPOCH: 172 train Results: Prec@1 71.374 Loss: 0.8060\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3049 (1.3049)\tPrec@1 54.883 (54.883)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4876 (1.2966)\tPrec@1 48.162 (56.450)\n",
      "EPOCH: 172 val Results: Prec@1 56.450 Loss: 1.2966\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/97]\tTime 0.009 (0.009)\tLoss 0.7640 (0.7640)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [173][19/97]\tTime 0.009 (0.011)\tLoss 0.8024 (0.7563)\tPrec@1 71.875 (73.545)\n",
      "Epoch: [173][38/97]\tTime 0.008 (0.011)\tLoss 0.7908 (0.7718)\tPrec@1 74.219 (72.967)\n",
      "Epoch: [173][57/97]\tTime 0.018 (0.011)\tLoss 0.8179 (0.7744)\tPrec@1 69.922 (72.747)\n",
      "Epoch: [173][76/97]\tTime 0.006 (0.011)\tLoss 0.8320 (0.7847)\tPrec@1 70.312 (72.370)\n",
      "Epoch: [173][95/97]\tTime 0.012 (0.012)\tLoss 0.8577 (0.8000)\tPrec@1 66.602 (71.641)\n",
      "Epoch: [173][97/97]\tTime 0.005 (0.012)\tLoss 0.8267 (0.8013)\tPrec@1 70.238 (71.586)\n",
      "EPOCH: 173 train Results: Prec@1 71.586 Loss: 0.8013\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3723 (1.3723)\tPrec@1 53.125 (53.125)\n",
      "Test: [19/19]\tTime 0.002 (0.002)\tLoss 1.4639 (1.2979)\tPrec@1 48.897 (56.160)\n",
      "EPOCH: 173 val Results: Prec@1 56.160 Loss: 1.2979\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/97]\tTime 0.009 (0.009)\tLoss 0.7547 (0.7547)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [174][19/97]\tTime 0.011 (0.009)\tLoss 0.7382 (0.7304)\tPrec@1 72.461 (73.809)\n",
      "Epoch: [174][38/97]\tTime 0.008 (0.009)\tLoss 0.7918 (0.7520)\tPrec@1 72.266 (73.222)\n",
      "Epoch: [174][57/97]\tTime 0.007 (0.011)\tLoss 0.8119 (0.7667)\tPrec@1 71.484 (72.720)\n",
      "Epoch: [174][76/97]\tTime 0.006 (0.011)\tLoss 0.8532 (0.7824)\tPrec@1 71.289 (72.299)\n",
      "Epoch: [174][95/97]\tTime 0.010 (0.011)\tLoss 0.8805 (0.7986)\tPrec@1 70.312 (71.692)\n",
      "Epoch: [174][97/97]\tTime 0.005 (0.011)\tLoss 0.8001 (0.7990)\tPrec@1 71.726 (71.690)\n",
      "EPOCH: 174 train Results: Prec@1 71.690 Loss: 0.7990\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3430 (1.3430)\tPrec@1 56.641 (56.641)\n",
      "Test: [19/19]\tTime 0.002 (0.005)\tLoss 1.4219 (1.2968)\tPrec@1 51.103 (56.130)\n",
      "EPOCH: 174 val Results: Prec@1 56.130 Loss: 1.2968\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/97]\tTime 0.017 (0.017)\tLoss 0.7220 (0.7220)\tPrec@1 75.195 (75.195)\n",
      "Epoch: [175][19/97]\tTime 0.007 (0.011)\tLoss 0.7546 (0.7408)\tPrec@1 75.000 (74.365)\n",
      "Epoch: [175][38/97]\tTime 0.008 (0.011)\tLoss 0.8059 (0.7545)\tPrec@1 70.312 (73.578)\n",
      "Epoch: [175][57/97]\tTime 0.007 (0.011)\tLoss 0.7845 (0.7725)\tPrec@1 70.703 (72.926)\n",
      "Epoch: [175][76/97]\tTime 0.028 (0.011)\tLoss 0.8702 (0.7892)\tPrec@1 68.555 (72.202)\n",
      "Epoch: [175][95/97]\tTime 0.012 (0.011)\tLoss 0.8521 (0.8015)\tPrec@1 69.727 (71.720)\n",
      "Epoch: [175][97/97]\tTime 0.008 (0.011)\tLoss 0.9346 (0.8032)\tPrec@1 69.345 (71.686)\n",
      "EPOCH: 175 train Results: Prec@1 71.686 Loss: 0.8032\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3554 (1.3554)\tPrec@1 51.953 (51.953)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4737 (1.3085)\tPrec@1 48.162 (55.910)\n",
      "EPOCH: 175 val Results: Prec@1 55.910 Loss: 1.3085\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/97]\tTime 0.012 (0.012)\tLoss 0.8003 (0.8003)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [176][19/97]\tTime 0.010 (0.012)\tLoss 0.8048 (0.7527)\tPrec@1 74.023 (74.092)\n",
      "Epoch: [176][38/97]\tTime 0.008 (0.011)\tLoss 0.7660 (0.7546)\tPrec@1 73.242 (73.618)\n",
      "Epoch: [176][57/97]\tTime 0.009 (0.011)\tLoss 0.8180 (0.7675)\tPrec@1 70.312 (72.973)\n",
      "Epoch: [176][76/97]\tTime 0.015 (0.011)\tLoss 0.8915 (0.7795)\tPrec@1 67.578 (72.496)\n",
      "Epoch: [176][95/97]\tTime 0.007 (0.011)\tLoss 0.8028 (0.7914)\tPrec@1 72.266 (72.017)\n",
      "Epoch: [176][97/97]\tTime 0.005 (0.011)\tLoss 0.8116 (0.7916)\tPrec@1 69.345 (71.988)\n",
      "EPOCH: 176 train Results: Prec@1 71.988 Loss: 0.7916\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3199 (1.3199)\tPrec@1 53.906 (53.906)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4309 (1.2940)\tPrec@1 52.206 (56.320)\n",
      "EPOCH: 176 val Results: Prec@1 56.320 Loss: 1.2940\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/97]\tTime 0.011 (0.011)\tLoss 0.7199 (0.7199)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [177][19/97]\tTime 0.007 (0.013)\tLoss 0.7436 (0.7307)\tPrec@1 74.609 (74.248)\n",
      "Epoch: [177][38/97]\tTime 0.013 (0.011)\tLoss 0.7958 (0.7537)\tPrec@1 74.023 (73.503)\n",
      "Epoch: [177][57/97]\tTime 0.015 (0.011)\tLoss 0.7564 (0.7660)\tPrec@1 74.219 (73.138)\n",
      "Epoch: [177][76/97]\tTime 0.013 (0.011)\tLoss 0.8546 (0.7826)\tPrec@1 68.555 (72.532)\n",
      "Epoch: [177][95/97]\tTime 0.014 (0.011)\tLoss 0.8766 (0.7912)\tPrec@1 68.750 (72.162)\n",
      "Epoch: [177][97/97]\tTime 0.010 (0.011)\tLoss 0.9430 (0.7929)\tPrec@1 66.667 (72.102)\n",
      "EPOCH: 177 train Results: Prec@1 72.102 Loss: 0.7929\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.3396 (1.3396)\tPrec@1 52.344 (52.344)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4495 (1.2913)\tPrec@1 50.735 (56.190)\n",
      "EPOCH: 177 val Results: Prec@1 56.190 Loss: 1.2913\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/97]\tTime 0.011 (0.011)\tLoss 0.7384 (0.7384)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [178][19/97]\tTime 0.009 (0.012)\tLoss 0.7092 (0.7322)\tPrec@1 74.219 (74.268)\n",
      "Epoch: [178][38/97]\tTime 0.009 (0.011)\tLoss 0.7850 (0.7548)\tPrec@1 70.703 (73.332)\n",
      "Epoch: [178][57/97]\tTime 0.012 (0.011)\tLoss 0.7777 (0.7773)\tPrec@1 71.289 (72.589)\n",
      "Epoch: [178][76/97]\tTime 0.008 (0.011)\tLoss 0.7945 (0.7897)\tPrec@1 70.898 (72.121)\n",
      "Epoch: [178][95/97]\tTime 0.016 (0.011)\tLoss 0.8448 (0.7996)\tPrec@1 69.922 (71.635)\n",
      "Epoch: [178][97/97]\tTime 0.012 (0.011)\tLoss 0.9324 (0.8013)\tPrec@1 64.286 (71.558)\n",
      "EPOCH: 178 train Results: Prec@1 71.558 Loss: 0.8013\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.3562 (1.3562)\tPrec@1 53.516 (53.516)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4513 (1.3018)\tPrec@1 50.000 (55.710)\n",
      "EPOCH: 178 val Results: Prec@1 55.710 Loss: 1.3018\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/97]\tTime 0.011 (0.011)\tLoss 0.7648 (0.7648)\tPrec@1 73.047 (73.047)\n",
      "Epoch: [179][19/97]\tTime 0.009 (0.010)\tLoss 0.8113 (0.7357)\tPrec@1 71.875 (74.395)\n",
      "Epoch: [179][38/97]\tTime 0.007 (0.010)\tLoss 0.7952 (0.7578)\tPrec@1 71.289 (73.267)\n",
      "Epoch: [179][57/97]\tTime 0.012 (0.010)\tLoss 0.8058 (0.7751)\tPrec@1 71.289 (72.639)\n",
      "Epoch: [179][76/97]\tTime 0.008 (0.010)\tLoss 0.9109 (0.7875)\tPrec@1 68.359 (72.047)\n",
      "Epoch: [179][95/97]\tTime 0.016 (0.011)\tLoss 0.8266 (0.7990)\tPrec@1 71.289 (71.657)\n",
      "Epoch: [179][97/97]\tTime 0.006 (0.010)\tLoss 0.8475 (0.8000)\tPrec@1 71.726 (71.626)\n",
      "EPOCH: 179 train Results: Prec@1 71.626 Loss: 0.8000\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3914 (1.3914)\tPrec@1 52.539 (52.539)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4142 (1.3060)\tPrec@1 51.471 (55.590)\n",
      "EPOCH: 179 val Results: Prec@1 55.590 Loss: 1.3060\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/97]\tTime 0.010 (0.010)\tLoss 0.7062 (0.7062)\tPrec@1 75.195 (75.195)\n",
      "Epoch: [180][19/97]\tTime 0.007 (0.011)\tLoss 0.7770 (0.7443)\tPrec@1 74.219 (74.395)\n",
      "Epoch: [180][38/97]\tTime 0.014 (0.011)\tLoss 0.7718 (0.7570)\tPrec@1 73.047 (73.628)\n",
      "Epoch: [180][57/97]\tTime 0.069 (0.012)\tLoss 0.7654 (0.7719)\tPrec@1 73.047 (73.030)\n",
      "Epoch: [180][76/97]\tTime 0.010 (0.012)\tLoss 0.7928 (0.7813)\tPrec@1 70.508 (72.618)\n",
      "Epoch: [180][95/97]\tTime 0.011 (0.012)\tLoss 0.9657 (0.7917)\tPrec@1 65.234 (72.198)\n",
      "Epoch: [180][97/97]\tTime 0.006 (0.012)\tLoss 0.9036 (0.7928)\tPrec@1 66.667 (72.138)\n",
      "EPOCH: 180 train Results: Prec@1 72.138 Loss: 0.7928\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3715 (1.3715)\tPrec@1 50.781 (50.781)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4824 (1.3146)\tPrec@1 47.059 (55.210)\n",
      "EPOCH: 180 val Results: Prec@1 55.210 Loss: 1.3146\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/97]\tTime 0.013 (0.013)\tLoss 0.7036 (0.7036)\tPrec@1 76.367 (76.367)\n",
      "Epoch: [181][19/97]\tTime 0.013 (0.012)\tLoss 0.7689 (0.7464)\tPrec@1 73.242 (74.014)\n",
      "Epoch: [181][38/97]\tTime 0.015 (0.011)\tLoss 0.7924 (0.7570)\tPrec@1 69.727 (73.448)\n",
      "Epoch: [181][57/97]\tTime 0.007 (0.011)\tLoss 0.7693 (0.7731)\tPrec@1 73.633 (72.828)\n",
      "Epoch: [181][76/97]\tTime 0.016 (0.011)\tLoss 0.8704 (0.7820)\tPrec@1 68.555 (72.387)\n",
      "Epoch: [181][95/97]\tTime 0.015 (0.011)\tLoss 0.8797 (0.7931)\tPrec@1 69.141 (71.985)\n",
      "Epoch: [181][97/97]\tTime 0.005 (0.011)\tLoss 0.9321 (0.7949)\tPrec@1 69.345 (71.918)\n",
      "EPOCH: 181 train Results: Prec@1 71.918 Loss: 0.7949\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3656 (1.3656)\tPrec@1 52.734 (52.734)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4396 (1.3137)\tPrec@1 50.735 (56.110)\n",
      "EPOCH: 181 val Results: Prec@1 56.110 Loss: 1.3137\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/97]\tTime 0.010 (0.010)\tLoss 0.7271 (0.7271)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [182][19/97]\tTime 0.010 (0.014)\tLoss 0.8060 (0.7333)\tPrec@1 70.898 (74.453)\n",
      "Epoch: [182][38/97]\tTime 0.020 (0.012)\tLoss 0.7690 (0.7542)\tPrec@1 73.633 (73.448)\n",
      "Epoch: [182][57/97]\tTime 0.016 (0.013)\tLoss 0.8493 (0.7756)\tPrec@1 71.289 (72.629)\n",
      "Epoch: [182][76/97]\tTime 0.020 (0.015)\tLoss 0.8431 (0.7894)\tPrec@1 68.555 (71.956)\n",
      "Epoch: [182][95/97]\tTime 0.010 (0.015)\tLoss 0.8260 (0.7962)\tPrec@1 71.289 (71.747)\n",
      "Epoch: [182][97/97]\tTime 0.019 (0.015)\tLoss 0.8911 (0.7972)\tPrec@1 65.476 (71.692)\n",
      "EPOCH: 182 train Results: Prec@1 71.692 Loss: 0.7972\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3494 (1.3494)\tPrec@1 51.953 (51.953)\n",
      "Test: [19/19]\tTime 0.003 (0.004)\tLoss 1.4401 (1.3101)\tPrec@1 51.471 (55.920)\n",
      "EPOCH: 182 val Results: Prec@1 55.920 Loss: 1.3101\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/97]\tTime 0.022 (0.022)\tLoss 0.7325 (0.7325)\tPrec@1 74.414 (74.414)\n",
      "Epoch: [183][19/97]\tTime 0.010 (0.016)\tLoss 0.7570 (0.7366)\tPrec@1 74.219 (74.512)\n",
      "Epoch: [183][38/97]\tTime 0.033 (0.017)\tLoss 0.7240 (0.7456)\tPrec@1 75.781 (73.983)\n",
      "Epoch: [183][57/97]\tTime 0.012 (0.015)\tLoss 0.8832 (0.7638)\tPrec@1 66.992 (73.131)\n",
      "Epoch: [183][76/97]\tTime 0.018 (0.014)\tLoss 0.8331 (0.7804)\tPrec@1 70.703 (72.448)\n",
      "Epoch: [183][95/97]\tTime 0.010 (0.014)\tLoss 0.7883 (0.7937)\tPrec@1 71.875 (71.844)\n",
      "Epoch: [183][97/97]\tTime 0.005 (0.014)\tLoss 0.8404 (0.7946)\tPrec@1 66.667 (71.792)\n",
      "EPOCH: 183 train Results: Prec@1 71.792 Loss: 0.7946\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3431 (1.3431)\tPrec@1 52.930 (52.930)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4779 (1.3086)\tPrec@1 49.265 (55.950)\n",
      "EPOCH: 183 val Results: Prec@1 55.950 Loss: 1.3086\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/97]\tTime 0.013 (0.013)\tLoss 0.6952 (0.6952)\tPrec@1 77.148 (77.148)\n",
      "Epoch: [184][19/97]\tTime 0.009 (0.012)\tLoss 0.7969 (0.7335)\tPrec@1 72.070 (74.668)\n",
      "Epoch: [184][38/97]\tTime 0.013 (0.011)\tLoss 0.7759 (0.7464)\tPrec@1 71.680 (73.893)\n",
      "Epoch: [184][57/97]\tTime 0.007 (0.011)\tLoss 0.7934 (0.7687)\tPrec@1 71.875 (72.929)\n",
      "Epoch: [184][76/97]\tTime 0.008 (0.011)\tLoss 0.8255 (0.7843)\tPrec@1 70.117 (72.263)\n",
      "Epoch: [184][95/97]\tTime 0.012 (0.011)\tLoss 0.8213 (0.7954)\tPrec@1 70.703 (71.832)\n",
      "Epoch: [184][97/97]\tTime 0.008 (0.010)\tLoss 0.8978 (0.7965)\tPrec@1 69.345 (71.790)\n",
      "EPOCH: 184 train Results: Prec@1 71.790 Loss: 0.7965\n",
      "Test: [0/19]\tTime 0.009 (0.009)\tLoss 1.3931 (1.3931)\tPrec@1 54.492 (54.492)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4383 (1.3093)\tPrec@1 51.471 (56.250)\n",
      "EPOCH: 184 val Results: Prec@1 56.250 Loss: 1.3093\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/97]\tTime 0.008 (0.008)\tLoss 0.7170 (0.7170)\tPrec@1 76.953 (76.953)\n",
      "Epoch: [185][19/97]\tTime 0.013 (0.011)\tLoss 0.7964 (0.7392)\tPrec@1 72.461 (74.844)\n",
      "Epoch: [185][38/97]\tTime 0.015 (0.011)\tLoss 0.8162 (0.7511)\tPrec@1 68.750 (73.908)\n",
      "Epoch: [185][57/97]\tTime 0.012 (0.012)\tLoss 0.7714 (0.7584)\tPrec@1 74.219 (73.666)\n",
      "Epoch: [185][76/97]\tTime 0.009 (0.012)\tLoss 0.8637 (0.7758)\tPrec@1 69.141 (72.834)\n",
      "Epoch: [185][95/97]\tTime 0.010 (0.012)\tLoss 0.7952 (0.7947)\tPrec@1 73.242 (72.038)\n",
      "Epoch: [185][97/97]\tTime 0.007 (0.012)\tLoss 0.9568 (0.7974)\tPrec@1 65.774 (71.950)\n",
      "EPOCH: 185 train Results: Prec@1 71.950 Loss: 0.7974\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3953 (1.3953)\tPrec@1 52.344 (52.344)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4343 (1.3215)\tPrec@1 52.206 (55.520)\n",
      "EPOCH: 185 val Results: Prec@1 55.520 Loss: 1.3215\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/97]\tTime 0.013 (0.013)\tLoss 0.8000 (0.8000)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [186][19/97]\tTime 0.006 (0.011)\tLoss 0.7848 (0.7473)\tPrec@1 70.508 (74.053)\n",
      "Epoch: [186][38/97]\tTime 0.011 (0.011)\tLoss 0.7432 (0.7611)\tPrec@1 72.266 (73.317)\n",
      "Epoch: [186][57/97]\tTime 0.010 (0.011)\tLoss 0.7893 (0.7711)\tPrec@1 68.750 (72.737)\n",
      "Epoch: [186][76/97]\tTime 0.008 (0.011)\tLoss 0.8107 (0.7843)\tPrec@1 70.703 (72.311)\n",
      "Epoch: [186][95/97]\tTime 0.014 (0.011)\tLoss 0.7981 (0.7951)\tPrec@1 71.875 (71.942)\n",
      "Epoch: [186][97/97]\tTime 0.011 (0.011)\tLoss 0.7844 (0.7961)\tPrec@1 71.429 (71.904)\n",
      "EPOCH: 186 train Results: Prec@1 71.904 Loss: 0.7961\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3417 (1.3417)\tPrec@1 56.641 (56.641)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4406 (1.3173)\tPrec@1 51.103 (56.130)\n",
      "EPOCH: 186 val Results: Prec@1 56.130 Loss: 1.3173\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/97]\tTime 0.010 (0.010)\tLoss 0.6441 (0.6441)\tPrec@1 78.125 (78.125)\n",
      "Epoch: [187][19/97]\tTime 0.006 (0.011)\tLoss 0.7420 (0.7276)\tPrec@1 72.461 (74.150)\n",
      "Epoch: [187][38/97]\tTime 0.029 (0.012)\tLoss 0.8087 (0.7521)\tPrec@1 68.750 (73.212)\n",
      "Epoch: [187][57/97]\tTime 0.012 (0.011)\tLoss 0.8311 (0.7640)\tPrec@1 70.117 (72.754)\n",
      "Epoch: [187][76/97]\tTime 0.009 (0.011)\tLoss 0.8148 (0.7811)\tPrec@1 68.555 (72.207)\n",
      "Epoch: [187][95/97]\tTime 0.008 (0.011)\tLoss 0.8182 (0.7958)\tPrec@1 70.703 (71.739)\n",
      "Epoch: [187][97/97]\tTime 0.009 (0.011)\tLoss 0.8348 (0.7966)\tPrec@1 69.940 (71.708)\n",
      "EPOCH: 187 train Results: Prec@1 71.708 Loss: 0.7966\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3181 (1.3181)\tPrec@1 55.664 (55.664)\n",
      "Test: [19/19]\tTime 0.002 (0.004)\tLoss 1.4018 (1.3089)\tPrec@1 51.103 (55.870)\n",
      "EPOCH: 187 val Results: Prec@1 55.870 Loss: 1.3089\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/97]\tTime 0.018 (0.018)\tLoss 0.7543 (0.7543)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [188][19/97]\tTime 0.007 (0.010)\tLoss 0.8630 (0.7466)\tPrec@1 69.531 (73.740)\n",
      "Epoch: [188][38/97]\tTime 0.008 (0.010)\tLoss 0.7977 (0.7572)\tPrec@1 72.852 (73.297)\n",
      "Epoch: [188][57/97]\tTime 0.012 (0.011)\tLoss 0.7908 (0.7675)\tPrec@1 71.094 (72.808)\n",
      "Epoch: [188][76/97]\tTime 0.011 (0.012)\tLoss 0.7960 (0.7781)\tPrec@1 71.875 (72.362)\n",
      "Epoch: [188][95/97]\tTime 0.017 (0.012)\tLoss 0.8631 (0.7935)\tPrec@1 71.094 (71.796)\n",
      "Epoch: [188][97/97]\tTime 0.012 (0.012)\tLoss 0.8855 (0.7954)\tPrec@1 69.345 (71.724)\n",
      "EPOCH: 188 train Results: Prec@1 71.724 Loss: 0.7954\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3746 (1.3746)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4054 (1.3143)\tPrec@1 50.368 (55.740)\n",
      "EPOCH: 188 val Results: Prec@1 55.740 Loss: 1.3143\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/97]\tTime 0.011 (0.011)\tLoss 0.7207 (0.7207)\tPrec@1 75.195 (75.195)\n",
      "Epoch: [189][19/97]\tTime 0.009 (0.011)\tLoss 0.7037 (0.7135)\tPrec@1 73.438 (75.078)\n",
      "Epoch: [189][38/97]\tTime 0.009 (0.013)\tLoss 0.8305 (0.7385)\tPrec@1 70.898 (74.154)\n",
      "Epoch: [189][57/97]\tTime 0.015 (0.012)\tLoss 0.7783 (0.7566)\tPrec@1 70.703 (73.326)\n",
      "Epoch: [189][76/97]\tTime 0.011 (0.012)\tLoss 0.8719 (0.7714)\tPrec@1 67.578 (72.570)\n",
      "Epoch: [189][95/97]\tTime 0.009 (0.012)\tLoss 0.8033 (0.7856)\tPrec@1 71.680 (72.050)\n",
      "Epoch: [189][97/97]\tTime 0.019 (0.012)\tLoss 0.8684 (0.7874)\tPrec@1 68.750 (72.004)\n",
      "EPOCH: 189 train Results: Prec@1 72.004 Loss: 0.7874\n",
      "Test: [0/19]\tTime 0.004 (0.004)\tLoss 1.3541 (1.3541)\tPrec@1 53.711 (53.711)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4749 (1.3090)\tPrec@1 51.838 (55.930)\n",
      "EPOCH: 189 val Results: Prec@1 55.930 Loss: 1.3090\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/97]\tTime 0.008 (0.008)\tLoss 0.7070 (0.7070)\tPrec@1 76.172 (76.172)\n",
      "Epoch: [190][19/97]\tTime 0.013 (0.011)\tLoss 0.8122 (0.7329)\tPrec@1 71.875 (74.863)\n",
      "Epoch: [190][38/97]\tTime 0.011 (0.011)\tLoss 0.7505 (0.7461)\tPrec@1 73.438 (74.028)\n",
      "Epoch: [190][57/97]\tTime 0.014 (0.011)\tLoss 0.8472 (0.7613)\tPrec@1 70.117 (73.205)\n",
      "Epoch: [190][76/97]\tTime 0.008 (0.011)\tLoss 0.8181 (0.7723)\tPrec@1 71.680 (72.702)\n",
      "Epoch: [190][95/97]\tTime 0.007 (0.011)\tLoss 0.8677 (0.7866)\tPrec@1 67.578 (72.172)\n",
      "Epoch: [190][97/97]\tTime 0.005 (0.011)\tLoss 0.9322 (0.7880)\tPrec@1 66.667 (72.108)\n",
      "EPOCH: 190 train Results: Prec@1 72.108 Loss: 0.7880\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3392 (1.3392)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4589 (1.3111)\tPrec@1 48.162 (55.780)\n",
      "EPOCH: 190 val Results: Prec@1 55.780 Loss: 1.3111\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/97]\tTime 0.011 (0.011)\tLoss 0.7597 (0.7597)\tPrec@1 75.977 (75.977)\n",
      "Epoch: [191][19/97]\tTime 0.012 (0.011)\tLoss 0.7254 (0.7190)\tPrec@1 72.266 (75.010)\n",
      "Epoch: [191][38/97]\tTime 0.014 (0.011)\tLoss 0.7787 (0.7436)\tPrec@1 74.609 (74.008)\n",
      "Epoch: [191][57/97]\tTime 0.015 (0.013)\tLoss 0.8720 (0.7618)\tPrec@1 70.312 (73.272)\n",
      "Epoch: [191][76/97]\tTime 0.019 (0.012)\tLoss 0.8584 (0.7774)\tPrec@1 69.922 (72.562)\n",
      "Epoch: [191][95/97]\tTime 0.016 (0.013)\tLoss 0.8712 (0.7924)\tPrec@1 67.773 (71.944)\n",
      "Epoch: [191][97/97]\tTime 0.005 (0.013)\tLoss 0.9213 (0.7941)\tPrec@1 66.369 (71.870)\n",
      "EPOCH: 191 train Results: Prec@1 71.870 Loss: 0.7941\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3842 (1.3842)\tPrec@1 53.320 (53.320)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4875 (1.3162)\tPrec@1 48.162 (55.290)\n",
      "EPOCH: 191 val Results: Prec@1 55.290 Loss: 1.3162\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/97]\tTime 0.014 (0.014)\tLoss 0.6746 (0.6746)\tPrec@1 75.781 (75.781)\n",
      "Epoch: [192][19/97]\tTime 0.011 (0.011)\tLoss 0.7418 (0.7301)\tPrec@1 73.828 (74.473)\n",
      "Epoch: [192][38/97]\tTime 0.018 (0.011)\tLoss 0.7732 (0.7487)\tPrec@1 71.289 (73.733)\n",
      "Epoch: [192][57/97]\tTime 0.011 (0.011)\tLoss 0.8912 (0.7650)\tPrec@1 67.969 (73.064)\n",
      "Epoch: [192][76/97]\tTime 0.009 (0.011)\tLoss 0.8795 (0.7786)\tPrec@1 66.992 (72.524)\n",
      "Epoch: [192][95/97]\tTime 0.012 (0.012)\tLoss 0.8432 (0.7924)\tPrec@1 68.359 (72.017)\n",
      "Epoch: [192][97/97]\tTime 0.005 (0.012)\tLoss 0.8277 (0.7939)\tPrec@1 71.429 (71.980)\n",
      "EPOCH: 192 train Results: Prec@1 71.980 Loss: 0.7939\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3481 (1.3481)\tPrec@1 51.953 (51.953)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4844 (1.3163)\tPrec@1 48.162 (55.870)\n",
      "EPOCH: 192 val Results: Prec@1 55.870 Loss: 1.3163\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/97]\tTime 0.016 (0.016)\tLoss 0.6934 (0.6934)\tPrec@1 76.172 (76.172)\n",
      "Epoch: [193][19/97]\tTime 0.012 (0.011)\tLoss 0.7740 (0.7361)\tPrec@1 73.047 (74.385)\n",
      "Epoch: [193][38/97]\tTime 0.008 (0.010)\tLoss 0.7049 (0.7540)\tPrec@1 74.805 (73.838)\n",
      "Epoch: [193][57/97]\tTime 0.011 (0.010)\tLoss 0.7992 (0.7694)\tPrec@1 69.336 (72.929)\n",
      "Epoch: [193][76/97]\tTime 0.019 (0.011)\tLoss 0.8279 (0.7819)\tPrec@1 70.703 (72.423)\n",
      "Epoch: [193][95/97]\tTime 0.009 (0.011)\tLoss 0.8452 (0.7923)\tPrec@1 70.898 (72.052)\n",
      "Epoch: [193][97/97]\tTime 0.006 (0.011)\tLoss 0.7649 (0.7921)\tPrec@1 71.726 (72.050)\n",
      "EPOCH: 193 train Results: Prec@1 72.050 Loss: 0.7921\n",
      "Test: [0/19]\tTime 0.008 (0.008)\tLoss 1.3541 (1.3541)\tPrec@1 53.906 (53.906)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.4748 (1.3080)\tPrec@1 46.324 (55.690)\n",
      "EPOCH: 193 val Results: Prec@1 55.690 Loss: 1.3080\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/97]\tTime 0.011 (0.011)\tLoss 0.7779 (0.7779)\tPrec@1 73.242 (73.242)\n",
      "Epoch: [194][19/97]\tTime 0.012 (0.013)\tLoss 0.7293 (0.7349)\tPrec@1 73.633 (74.297)\n",
      "Epoch: [194][38/97]\tTime 0.009 (0.011)\tLoss 0.7273 (0.7482)\tPrec@1 74.805 (73.783)\n",
      "Epoch: [194][57/97]\tTime 0.012 (0.011)\tLoss 0.7892 (0.7633)\tPrec@1 72.266 (73.131)\n",
      "Epoch: [194][76/97]\tTime 0.020 (0.011)\tLoss 0.8181 (0.7771)\tPrec@1 72.266 (72.588)\n",
      "Epoch: [194][95/97]\tTime 0.014 (0.011)\tLoss 0.8426 (0.7884)\tPrec@1 66.992 (72.107)\n",
      "Epoch: [194][97/97]\tTime 0.017 (0.011)\tLoss 0.8780 (0.7890)\tPrec@1 70.833 (72.074)\n",
      "EPOCH: 194 train Results: Prec@1 72.074 Loss: 0.7890\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3810 (1.3810)\tPrec@1 53.711 (53.711)\n",
      "Test: [19/19]\tTime 0.003 (0.003)\tLoss 1.4687 (1.3197)\tPrec@1 46.691 (55.720)\n",
      "EPOCH: 194 val Results: Prec@1 55.720 Loss: 1.3197\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/97]\tTime 0.020 (0.020)\tLoss 0.7266 (0.7266)\tPrec@1 73.633 (73.633)\n",
      "Epoch: [195][19/97]\tTime 0.011 (0.014)\tLoss 0.7587 (0.7362)\tPrec@1 72.266 (74.395)\n",
      "Epoch: [195][38/97]\tTime 0.014 (0.013)\tLoss 0.7607 (0.7506)\tPrec@1 73.047 (73.427)\n",
      "Epoch: [195][57/97]\tTime 0.010 (0.013)\tLoss 0.7721 (0.7606)\tPrec@1 73.633 (73.060)\n",
      "Epoch: [195][76/97]\tTime 0.012 (0.013)\tLoss 0.8436 (0.7738)\tPrec@1 69.531 (72.550)\n",
      "Epoch: [195][95/97]\tTime 0.017 (0.013)\tLoss 0.8493 (0.7860)\tPrec@1 69.141 (72.107)\n",
      "Epoch: [195][97/97]\tTime 0.009 (0.012)\tLoss 0.9470 (0.7871)\tPrec@1 63.690 (72.046)\n",
      "EPOCH: 195 train Results: Prec@1 72.046 Loss: 0.7871\n",
      "Test: [0/19]\tTime 0.003 (0.003)\tLoss 1.3488 (1.3488)\tPrec@1 52.539 (52.539)\n",
      "Test: [19/19]\tTime 0.001 (0.003)\tLoss 1.4901 (1.3198)\tPrec@1 49.632 (55.020)\n",
      "EPOCH: 195 val Results: Prec@1 55.020 Loss: 1.3198\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/97]\tTime 0.008 (0.008)\tLoss 0.7184 (0.7184)\tPrec@1 74.414 (74.414)\n",
      "Epoch: [196][19/97]\tTime 0.007 (0.012)\tLoss 0.7199 (0.7233)\tPrec@1 74.805 (74.727)\n",
      "Epoch: [196][38/97]\tTime 0.008 (0.011)\tLoss 0.7614 (0.7455)\tPrec@1 73.438 (73.918)\n",
      "Epoch: [196][57/97]\tTime 0.012 (0.011)\tLoss 0.8486 (0.7692)\tPrec@1 69.336 (72.831)\n",
      "Epoch: [196][76/97]\tTime 0.013 (0.011)\tLoss 0.8987 (0.7828)\tPrec@1 70.312 (72.245)\n",
      "Epoch: [196][95/97]\tTime 0.012 (0.011)\tLoss 0.7599 (0.7925)\tPrec@1 72.461 (71.855)\n",
      "Epoch: [196][97/97]\tTime 0.005 (0.011)\tLoss 0.7492 (0.7928)\tPrec@1 74.405 (71.850)\n",
      "EPOCH: 196 train Results: Prec@1 71.850 Loss: 0.7928\n",
      "Test: [0/19]\tTime 0.002 (0.002)\tLoss 1.3556 (1.3556)\tPrec@1 52.734 (52.734)\n",
      "Test: [19/19]\tTime 0.001 (0.002)\tLoss 1.4518 (1.3143)\tPrec@1 50.000 (54.980)\n",
      "EPOCH: 196 val Results: Prec@1 54.980 Loss: 1.3143\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/97]\tTime 0.008 (0.008)\tLoss 0.6982 (0.6982)\tPrec@1 78.320 (78.320)\n",
      "Epoch: [197][19/97]\tTime 0.013 (0.010)\tLoss 0.7810 (0.7164)\tPrec@1 72.852 (74.678)\n",
      "Epoch: [197][38/97]\tTime 0.014 (0.010)\tLoss 0.7388 (0.7268)\tPrec@1 74.023 (74.494)\n",
      "Epoch: [197][57/97]\tTime 0.012 (0.012)\tLoss 0.8129 (0.7429)\tPrec@1 70.898 (73.852)\n",
      "Epoch: [197][76/97]\tTime 0.015 (0.012)\tLoss 0.8134 (0.7648)\tPrec@1 71.680 (72.882)\n",
      "Epoch: [197][95/97]\tTime 0.009 (0.012)\tLoss 0.8042 (0.7803)\tPrec@1 71.484 (72.319)\n",
      "Epoch: [197][97/97]\tTime 0.005 (0.012)\tLoss 0.8961 (0.7815)\tPrec@1 65.476 (72.244)\n",
      "EPOCH: 197 train Results: Prec@1 72.244 Loss: 0.7815\n",
      "Test: [0/19]\tTime 0.005 (0.005)\tLoss 1.3502 (1.3502)\tPrec@1 53.320 (53.320)\n",
      "Test: [19/19]\tTime 0.004 (0.004)\tLoss 1.4702 (1.3247)\tPrec@1 50.000 (55.950)\n",
      "EPOCH: 197 val Results: Prec@1 55.950 Loss: 1.3247\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/97]\tTime 0.014 (0.014)\tLoss 0.7225 (0.7225)\tPrec@1 75.195 (75.195)\n",
      "Epoch: [198][19/97]\tTime 0.013 (0.011)\tLoss 0.7394 (0.7346)\tPrec@1 73.828 (74.238)\n",
      "Epoch: [198][38/97]\tTime 0.018 (0.012)\tLoss 0.8918 (0.7533)\tPrec@1 66.797 (73.478)\n",
      "Epoch: [198][57/97]\tTime 0.011 (0.011)\tLoss 0.8301 (0.7663)\tPrec@1 72.656 (72.936)\n",
      "Epoch: [198][76/97]\tTime 0.010 (0.011)\tLoss 0.8299 (0.7794)\tPrec@1 71.484 (72.451)\n",
      "Epoch: [198][95/97]\tTime 0.014 (0.011)\tLoss 0.8322 (0.7894)\tPrec@1 70.703 (72.054)\n",
      "Epoch: [198][97/97]\tTime 0.016 (0.012)\tLoss 0.7903 (0.7893)\tPrec@1 70.238 (72.044)\n",
      "EPOCH: 198 train Results: Prec@1 72.044 Loss: 0.7893\n",
      "Test: [0/19]\tTime 0.006 (0.006)\tLoss 1.3439 (1.3439)\tPrec@1 54.102 (54.102)\n",
      "Test: [19/19]\tTime 0.001 (0.004)\tLoss 1.4849 (1.3116)\tPrec@1 47.426 (55.610)\n",
      "EPOCH: 198 val Results: Prec@1 55.610 Loss: 1.3116\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/97]\tTime 0.027 (0.027)\tLoss 0.7524 (0.7524)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [199][19/97]\tTime 0.007 (0.013)\tLoss 0.7501 (0.7240)\tPrec@1 73.047 (74.678)\n",
      "Epoch: [199][38/97]\tTime 0.013 (0.015)\tLoss 0.7987 (0.7336)\tPrec@1 70.703 (74.329)\n",
      "Epoch: [199][57/97]\tTime 0.021 (0.014)\tLoss 0.7693 (0.7519)\tPrec@1 71.484 (73.384)\n",
      "Epoch: [199][76/97]\tTime 0.010 (0.013)\tLoss 0.8770 (0.7717)\tPrec@1 70.898 (72.514)\n",
      "Epoch: [199][95/97]\tTime 0.007 (0.013)\tLoss 0.9473 (0.7875)\tPrec@1 67.578 (71.934)\n",
      "Epoch: [199][97/97]\tTime 0.011 (0.013)\tLoss 0.7774 (0.7882)\tPrec@1 69.345 (71.882)\n",
      "EPOCH: 199 train Results: Prec@1 71.882 Loss: 0.7882\n",
      "Test: [0/19]\tTime 0.010 (0.010)\tLoss 1.3733 (1.3733)\tPrec@1 52.539 (52.539)\n",
      "Test: [19/19]\tTime 0.002 (0.003)\tLoss 1.5355 (1.3284)\tPrec@1 48.897 (55.430)\n",
      "EPOCH: 199 val Results: Prec@1 55.430 Loss: 1.3284\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/97]\tTime 0.014 (0.014)\tLoss 0.6944 (0.6944)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [200][19/97]\tTime 0.008 (0.011)\tLoss 0.7420 (0.7303)\tPrec@1 76.172 (74.385)\n",
      "Epoch: [200][38/97]\tTime 0.013 (0.011)\tLoss 0.7714 (0.7572)\tPrec@1 76.562 (73.302)\n",
      "Epoch: [200][57/97]\tTime 0.021 (0.012)\tLoss 0.7846 (0.7697)\tPrec@1 74.805 (72.868)\n",
      "Epoch: [200][76/97]\tTime 0.011 (0.011)\tLoss 0.8347 (0.7789)\tPrec@1 70.898 (72.474)\n",
      "Epoch: [200][95/97]\tTime 0.008 (0.011)\tLoss 0.8219 (0.7915)\tPrec@1 71.875 (71.942)\n",
      "Epoch: [200][97/97]\tTime 0.005 (0.011)\tLoss 0.9015 (0.7938)\tPrec@1 68.750 (71.860)\n",
      "EPOCH: 200 train Results: Prec@1 71.860 Loss: 0.7938\n",
      "Test: [0/19]\tTime 0.005 (0.005)\tLoss 1.3474 (1.3474)\tPrec@1 52.930 (52.930)\n",
      "Test: [19/19]\tTime 0.004 (0.003)\tLoss 1.4958 (1.3041)\tPrec@1 51.103 (55.990)\n",
      "EPOCH: 200 val Results: Prec@1 55.990 Loss: 1.3041\n",
      "Best Prec@1: 57.070\n",
      "\n",
      "End time:  Fri Apr  5 00:13:36 2024\n",
      "train executed in 265.4591 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 512\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'scheduler': None, \n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer5 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  Fri Apr  5 00:13:37 2024\n",
      "current lr 1.00000e-02\n",
      "Epoch: [1][0/48]\tTime 0.070 (0.070)\tLoss 5.6750 (5.6750)\tPrec@1 11.816 (11.816)\n",
      "Epoch: [1][9/48]\tTime 0.020 (0.026)\tLoss 4.7818 (5.2282)\tPrec@1 12.305 (11.035)\n",
      "Epoch: [1][18/48]\tTime 0.017 (0.024)\tLoss 3.7836 (4.7092)\tPrec@1 17.578 (12.911)\n",
      "Epoch: [1][27/48]\tTime 0.023 (0.025)\tLoss 3.2810 (4.3349)\tPrec@1 22.168 (15.008)\n",
      "Epoch: [1][36/48]\tTime 0.014 (0.024)\tLoss 2.8565 (4.0358)\tPrec@1 24.512 (16.918)\n",
      "Epoch: [1][45/48]\tTime 0.029 (0.024)\tLoss 2.7543 (3.8056)\tPrec@1 27.051 (18.663)\n",
      "Epoch: [1][48/48]\tTime 0.031 (0.025)\tLoss 2.8402 (3.7480)\tPrec@1 25.590 (19.118)\n",
      "EPOCH: 1 train Results: Prec@1 19.118 Loss: 3.7480\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 2.4123 (2.4123)\tPrec@1 29.004 (29.004)\n",
      "Test: [9/9]\tTime 0.005 (0.008)\tLoss 2.3182 (2.3181)\tPrec@1 29.592 (30.160)\n",
      "EPOCH: 1 val Results: Prec@1 30.160 Loss: 2.3181\n",
      "Best Prec@1: 30.160\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [2][0/48]\tTime 0.025 (0.025)\tLoss 2.6158 (2.6158)\tPrec@1 28.516 (28.516)\n",
      "Epoch: [2][9/48]\tTime 0.028 (0.024)\tLoss 2.5049 (2.5588)\tPrec@1 29.004 (28.896)\n",
      "Epoch: [2][18/48]\tTime 0.064 (0.033)\tLoss 2.3470 (2.5132)\tPrec@1 31.836 (29.482)\n",
      "Epoch: [2][27/48]\tTime 0.017 (0.033)\tLoss 2.3618 (2.4788)\tPrec@1 29.980 (29.653)\n",
      "Epoch: [2][36/48]\tTime 0.049 (0.034)\tLoss 2.3139 (2.4438)\tPrec@1 31.152 (30.097)\n",
      "Epoch: [2][45/48]\tTime 0.031 (0.033)\tLoss 2.2217 (2.4160)\tPrec@1 31.250 (30.354)\n",
      "Epoch: [2][48/48]\tTime 0.019 (0.033)\tLoss 2.2919 (2.4060)\tPrec@1 31.014 (30.484)\n",
      "EPOCH: 2 train Results: Prec@1 30.484 Loss: 2.4060\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 2.0816 (2.0816)\tPrec@1 33.789 (33.789)\n",
      "Test: [9/9]\tTime 0.008 (0.007)\tLoss 1.9832 (1.9871)\tPrec@1 34.694 (35.000)\n",
      "EPOCH: 2 val Results: Prec@1 35.000 Loss: 1.9871\n",
      "Best Prec@1: 35.000\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [3][0/48]\tTime 0.021 (0.021)\tLoss 2.2062 (2.2062)\tPrec@1 33.203 (33.203)\n",
      "Epoch: [3][9/48]\tTime 0.021 (0.026)\tLoss 2.2599 (2.1912)\tPrec@1 30.566 (33.135)\n",
      "Epoch: [3][18/48]\tTime 0.020 (0.024)\tLoss 2.1053 (2.1638)\tPrec@1 34.375 (33.260)\n",
      "Epoch: [3][27/48]\tTime 0.021 (0.024)\tLoss 2.1402 (2.1498)\tPrec@1 31.836 (33.339)\n",
      "Epoch: [3][36/48]\tTime 0.035 (0.026)\tLoss 2.0574 (2.1303)\tPrec@1 34.277 (33.620)\n",
      "Epoch: [3][45/48]\tTime 0.021 (0.027)\tLoss 1.9656 (2.1131)\tPrec@1 37.793 (33.849)\n",
      "Epoch: [3][48/48]\tTime 0.013 (0.027)\tLoss 1.9856 (2.1074)\tPrec@1 37.146 (33.954)\n",
      "EPOCH: 3 train Results: Prec@1 33.954 Loss: 2.1074\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.9281 (1.9281)\tPrec@1 35.840 (35.840)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.8404 (1.8461)\tPrec@1 38.393 (36.990)\n",
      "EPOCH: 3 val Results: Prec@1 36.990 Loss: 1.8461\n",
      "Best Prec@1: 36.990\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [4][0/48]\tTime 0.021 (0.021)\tLoss 1.9778 (1.9778)\tPrec@1 35.059 (35.059)\n",
      "Epoch: [4][9/48]\tTime 0.045 (0.028)\tLoss 1.9403 (1.9753)\tPrec@1 36.133 (36.006)\n",
      "Epoch: [4][18/48]\tTime 0.037 (0.027)\tLoss 1.9005 (1.9530)\tPrec@1 36.230 (36.112)\n",
      "Epoch: [4][27/48]\tTime 0.024 (0.028)\tLoss 1.8909 (1.9388)\tPrec@1 37.695 (36.408)\n",
      "Epoch: [4][36/48]\tTime 0.017 (0.028)\tLoss 1.9360 (1.9344)\tPrec@1 36.523 (36.468)\n",
      "Epoch: [4][45/48]\tTime 0.030 (0.029)\tLoss 1.9042 (1.9271)\tPrec@1 36.133 (36.689)\n",
      "Epoch: [4][48/48]\tTime 0.015 (0.028)\tLoss 1.8136 (1.9238)\tPrec@1 38.797 (36.710)\n",
      "EPOCH: 4 train Results: Prec@1 36.710 Loss: 1.9238\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.8287 (1.8287)\tPrec@1 37.598 (37.598)\n",
      "Test: [9/9]\tTime 0.007 (0.005)\tLoss 1.7575 (1.7607)\tPrec@1 39.668 (39.040)\n",
      "EPOCH: 4 val Results: Prec@1 39.040 Loss: 1.7607\n",
      "Best Prec@1: 39.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [5][0/48]\tTime 0.028 (0.028)\tLoss 1.8891 (1.8891)\tPrec@1 36.328 (36.328)\n",
      "Epoch: [5][9/48]\tTime 0.019 (0.021)\tLoss 1.8361 (1.8191)\tPrec@1 36.230 (37.881)\n",
      "Epoch: [5][18/48]\tTime 0.030 (0.023)\tLoss 1.8612 (1.8361)\tPrec@1 37.402 (37.834)\n",
      "Epoch: [5][27/48]\tTime 0.026 (0.022)\tLoss 1.8042 (1.8244)\tPrec@1 39.258 (38.299)\n",
      "Epoch: [5][36/48]\tTime 0.030 (0.023)\tLoss 1.8456 (1.8220)\tPrec@1 39.844 (38.521)\n",
      "Epoch: [5][45/48]\tTime 0.024 (0.024)\tLoss 1.8458 (1.8126)\tPrec@1 39.746 (38.757)\n",
      "Epoch: [5][48/48]\tTime 0.014 (0.024)\tLoss 1.8165 (1.8121)\tPrec@1 39.269 (38.786)\n",
      "EPOCH: 5 train Results: Prec@1 38.786 Loss: 1.8121\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.7581 (1.7581)\tPrec@1 38.770 (38.770)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.7018 (1.7033)\tPrec@1 40.561 (40.140)\n",
      "EPOCH: 5 val Results: Prec@1 40.140 Loss: 1.7033\n",
      "Best Prec@1: 40.140\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [6][0/48]\tTime 0.025 (0.025)\tLoss 1.7240 (1.7240)\tPrec@1 41.797 (41.797)\n",
      "Epoch: [6][9/48]\tTime 0.014 (0.031)\tLoss 1.7294 (1.7430)\tPrec@1 41.211 (40.596)\n",
      "Epoch: [6][18/48]\tTime 0.033 (0.027)\tLoss 1.6884 (1.7378)\tPrec@1 43.457 (40.789)\n",
      "Epoch: [6][27/48]\tTime 0.025 (0.026)\tLoss 1.7236 (1.7399)\tPrec@1 40.723 (40.660)\n",
      "Epoch: [6][36/48]\tTime 0.021 (0.025)\tLoss 1.6735 (1.7305)\tPrec@1 41.309 (40.818)\n",
      "Epoch: [6][45/48]\tTime 0.029 (0.026)\tLoss 1.6594 (1.7266)\tPrec@1 42.871 (40.818)\n",
      "Epoch: [6][48/48]\tTime 0.020 (0.026)\tLoss 1.7437 (1.7260)\tPrec@1 38.679 (40.754)\n",
      "EPOCH: 6 train Results: Prec@1 40.754 Loss: 1.7260\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.7040 (1.7040)\tPrec@1 40.527 (40.527)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.6599 (1.6595)\tPrec@1 41.582 (41.440)\n",
      "EPOCH: 6 val Results: Prec@1 41.440 Loss: 1.6595\n",
      "Best Prec@1: 41.440\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [7][0/48]\tTime 0.025 (0.025)\tLoss 1.6807 (1.6807)\tPrec@1 42.871 (42.871)\n",
      "Epoch: [7][9/48]\tTime 0.020 (0.022)\tLoss 1.6591 (1.6927)\tPrec@1 42.285 (41.670)\n",
      "Epoch: [7][18/48]\tTime 0.013 (0.021)\tLoss 1.6357 (1.6816)\tPrec@1 42.285 (41.951)\n",
      "Epoch: [7][27/48]\tTime 0.038 (0.023)\tLoss 1.6720 (1.6758)\tPrec@1 42.090 (42.090)\n",
      "Epoch: [7][36/48]\tTime 0.020 (0.022)\tLoss 1.6462 (1.6689)\tPrec@1 42.285 (42.230)\n",
      "Epoch: [7][45/48]\tTime 0.018 (0.022)\tLoss 1.6020 (1.6663)\tPrec@1 45.898 (42.292)\n",
      "Epoch: [7][48/48]\tTime 0.017 (0.022)\tLoss 1.6544 (1.6663)\tPrec@1 40.448 (42.262)\n",
      "EPOCH: 7 train Results: Prec@1 42.262 Loss: 1.6663\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.6653 (1.6653)\tPrec@1 42.188 (42.188)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.6255 (1.6255)\tPrec@1 42.985 (42.270)\n",
      "EPOCH: 7 val Results: Prec@1 42.270 Loss: 1.6255\n",
      "Best Prec@1: 42.270\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [8][0/48]\tTime 0.026 (0.026)\tLoss 1.6096 (1.6096)\tPrec@1 42.969 (42.969)\n",
      "Epoch: [8][9/48]\tTime 0.019 (0.022)\tLoss 1.6158 (1.6409)\tPrec@1 46.094 (42.822)\n",
      "Epoch: [8][18/48]\tTime 0.019 (0.021)\tLoss 1.6053 (1.6372)\tPrec@1 42.871 (42.630)\n",
      "Epoch: [8][27/48]\tTime 0.015 (0.022)\tLoss 1.6338 (1.6290)\tPrec@1 44.141 (42.913)\n",
      "Epoch: [8][36/48]\tTime 0.019 (0.021)\tLoss 1.5777 (1.6207)\tPrec@1 45.215 (43.154)\n",
      "Epoch: [8][45/48]\tTime 0.016 (0.021)\tLoss 1.6450 (1.6234)\tPrec@1 42.969 (43.058)\n",
      "Epoch: [8][48/48]\tTime 0.018 (0.021)\tLoss 1.5730 (1.6212)\tPrec@1 43.868 (43.068)\n",
      "EPOCH: 8 train Results: Prec@1 43.068 Loss: 1.6212\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.6333 (1.6333)\tPrec@1 42.871 (42.871)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.5983 (1.5992)\tPrec@1 42.602 (43.260)\n",
      "EPOCH: 8 val Results: Prec@1 43.260 Loss: 1.5992\n",
      "Best Prec@1: 43.260\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [9][0/48]\tTime 0.016 (0.016)\tLoss 1.5548 (1.5548)\tPrec@1 47.266 (47.266)\n",
      "Epoch: [9][9/48]\tTime 0.019 (0.021)\tLoss 1.5555 (1.5746)\tPrec@1 44.629 (44.707)\n",
      "Epoch: [9][18/48]\tTime 0.015 (0.020)\tLoss 1.5993 (1.5723)\tPrec@1 44.336 (44.562)\n",
      "Epoch: [9][27/48]\tTime 0.023 (0.021)\tLoss 1.5823 (1.5784)\tPrec@1 43.359 (44.448)\n",
      "Epoch: [9][36/48]\tTime 0.025 (0.021)\tLoss 1.5855 (1.5776)\tPrec@1 44.238 (44.510)\n",
      "Epoch: [9][45/48]\tTime 0.021 (0.022)\tLoss 1.6050 (1.5776)\tPrec@1 42.773 (44.332)\n",
      "Epoch: [9][48/48]\tTime 0.016 (0.022)\tLoss 1.5786 (1.5764)\tPrec@1 43.986 (44.442)\n",
      "EPOCH: 9 train Results: Prec@1 44.442 Loss: 1.5764\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.6062 (1.6062)\tPrec@1 43.945 (43.945)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.5794 (1.5761)\tPrec@1 43.112 (44.120)\n",
      "EPOCH: 9 val Results: Prec@1 44.120 Loss: 1.5761\n",
      "Best Prec@1: 44.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [10][0/48]\tTime 0.021 (0.021)\tLoss 1.5197 (1.5197)\tPrec@1 48.828 (48.828)\n",
      "Epoch: [10][9/48]\tTime 0.020 (0.021)\tLoss 1.5410 (1.5418)\tPrec@1 47.754 (46.172)\n",
      "Epoch: [10][18/48]\tTime 0.016 (0.021)\tLoss 1.5487 (1.5536)\tPrec@1 45.605 (45.467)\n",
      "Epoch: [10][27/48]\tTime 0.019 (0.022)\tLoss 1.5780 (1.5527)\tPrec@1 42.480 (45.354)\n",
      "Epoch: [10][36/48]\tTime 0.022 (0.021)\tLoss 1.5726 (1.5510)\tPrec@1 45.801 (45.315)\n",
      "Epoch: [10][45/48]\tTime 0.022 (0.022)\tLoss 1.5977 (1.5482)\tPrec@1 44.531 (45.514)\n",
      "Epoch: [10][48/48]\tTime 0.022 (0.022)\tLoss 1.5003 (1.5471)\tPrec@1 47.524 (45.550)\n",
      "EPOCH: 10 train Results: Prec@1 45.550 Loss: 1.5471\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.5822 (1.5822)\tPrec@1 44.727 (44.727)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.5606 (1.5570)\tPrec@1 43.622 (44.630)\n",
      "EPOCH: 10 val Results: Prec@1 44.630 Loss: 1.5570\n",
      "Best Prec@1: 44.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [11][0/48]\tTime 0.031 (0.031)\tLoss 1.4963 (1.4963)\tPrec@1 46.191 (46.191)\n",
      "Epoch: [11][9/48]\tTime 0.028 (0.028)\tLoss 1.4808 (1.5121)\tPrec@1 48.438 (47.090)\n",
      "Epoch: [11][18/48]\tTime 0.045 (0.028)\tLoss 1.4956 (1.5203)\tPrec@1 48.438 (46.695)\n",
      "Epoch: [11][27/48]\tTime 0.044 (0.028)\tLoss 1.5511 (1.5234)\tPrec@1 44.824 (46.446)\n",
      "Epoch: [11][36/48]\tTime 0.020 (0.026)\tLoss 1.4852 (1.5200)\tPrec@1 47.559 (46.355)\n",
      "Epoch: [11][45/48]\tTime 0.018 (0.026)\tLoss 1.5468 (1.5186)\tPrec@1 46.289 (46.431)\n",
      "Epoch: [11][48/48]\tTime 0.025 (0.026)\tLoss 1.5334 (1.5205)\tPrec@1 45.283 (46.398)\n",
      "EPOCH: 11 train Results: Prec@1 46.398 Loss: 1.5205\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.5641 (1.5641)\tPrec@1 44.922 (44.922)\n",
      "Test: [9/9]\tTime 0.005 (0.005)\tLoss 1.5475 (1.5392)\tPrec@1 43.622 (45.390)\n",
      "EPOCH: 11 val Results: Prec@1 45.390 Loss: 1.5392\n",
      "Best Prec@1: 45.390\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [12][0/48]\tTime 0.022 (0.022)\tLoss 1.4953 (1.4953)\tPrec@1 45.898 (45.898)\n",
      "Epoch: [12][9/48]\tTime 0.030 (0.026)\tLoss 1.4890 (1.4922)\tPrec@1 46.777 (46.631)\n",
      "Epoch: [12][18/48]\tTime 0.029 (0.024)\tLoss 1.4710 (1.4912)\tPrec@1 48.047 (46.803)\n",
      "Epoch: [12][27/48]\tTime 0.021 (0.024)\tLoss 1.4373 (1.4914)\tPrec@1 50.781 (46.903)\n",
      "Epoch: [12][36/48]\tTime 0.017 (0.022)\tLoss 1.4702 (1.4928)\tPrec@1 48.242 (46.991)\n",
      "Epoch: [12][45/48]\tTime 0.021 (0.022)\tLoss 1.5224 (1.4917)\tPrec@1 46.777 (47.185)\n",
      "Epoch: [12][48/48]\tTime 0.018 (0.022)\tLoss 1.4705 (1.4930)\tPrec@1 46.816 (47.078)\n",
      "EPOCH: 12 train Results: Prec@1 47.078 Loss: 1.4930\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.5467 (1.5467)\tPrec@1 45.898 (45.898)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.5302 (1.5238)\tPrec@1 44.770 (45.940)\n",
      "EPOCH: 12 val Results: Prec@1 45.940 Loss: 1.5238\n",
      "Best Prec@1: 45.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [13][0/48]\tTime 0.016 (0.016)\tLoss 1.4790 (1.4790)\tPrec@1 45.996 (45.996)\n",
      "Epoch: [13][9/48]\tTime 0.020 (0.019)\tLoss 1.4488 (1.4781)\tPrec@1 49.512 (46.914)\n",
      "Epoch: [13][18/48]\tTime 0.022 (0.021)\tLoss 1.4714 (1.4707)\tPrec@1 48.828 (47.749)\n",
      "Epoch: [13][27/48]\tTime 0.018 (0.021)\tLoss 1.4885 (1.4727)\tPrec@1 47.852 (47.729)\n",
      "Epoch: [13][36/48]\tTime 0.025 (0.021)\tLoss 1.4646 (1.4718)\tPrec@1 47.559 (47.688)\n",
      "Epoch: [13][45/48]\tTime 0.021 (0.021)\tLoss 1.5247 (1.4718)\tPrec@1 46.582 (47.745)\n",
      "Epoch: [13][48/48]\tTime 0.021 (0.021)\tLoss 1.4551 (1.4717)\tPrec@1 49.528 (47.782)\n",
      "EPOCH: 13 train Results: Prec@1 47.782 Loss: 1.4717\n",
      "Test: [0/9]\tTime 0.015 (0.015)\tLoss 1.5257 (1.5257)\tPrec@1 46.680 (46.680)\n",
      "Test: [9/9]\tTime 0.004 (0.007)\tLoss 1.5166 (1.5096)\tPrec@1 44.898 (46.530)\n",
      "EPOCH: 13 val Results: Prec@1 46.530 Loss: 1.5096\n",
      "Best Prec@1: 46.530\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [14][0/48]\tTime 0.021 (0.021)\tLoss 1.4145 (1.4145)\tPrec@1 52.441 (52.441)\n",
      "Epoch: [14][9/48]\tTime 0.024 (0.019)\tLoss 1.4847 (1.4609)\tPrec@1 48.047 (48.379)\n",
      "Epoch: [14][18/48]\tTime 0.031 (0.020)\tLoss 1.5110 (1.4574)\tPrec@1 46.094 (48.247)\n",
      "Epoch: [14][27/48]\tTime 0.020 (0.021)\tLoss 1.4221 (1.4576)\tPrec@1 48.145 (48.214)\n",
      "Epoch: [14][36/48]\tTime 0.022 (0.021)\tLoss 1.4724 (1.4538)\tPrec@1 47.852 (48.569)\n",
      "Epoch: [14][45/48]\tTime 0.017 (0.021)\tLoss 1.4810 (1.4539)\tPrec@1 47.949 (48.595)\n",
      "Epoch: [14][48/48]\tTime 0.012 (0.021)\tLoss 1.4362 (1.4518)\tPrec@1 48.467 (48.634)\n",
      "EPOCH: 14 train Results: Prec@1 48.634 Loss: 1.4518\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.5140 (1.5140)\tPrec@1 46.777 (46.777)\n",
      "Test: [9/9]\tTime 0.007 (0.007)\tLoss 1.5049 (1.4970)\tPrec@1 44.388 (46.650)\n",
      "EPOCH: 14 val Results: Prec@1 46.650 Loss: 1.4970\n",
      "Best Prec@1: 46.650\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [15][0/48]\tTime 0.039 (0.039)\tLoss 1.4190 (1.4190)\tPrec@1 48.633 (48.633)\n",
      "Epoch: [15][9/48]\tTime 0.022 (0.025)\tLoss 1.4344 (1.4346)\tPrec@1 50.000 (49.102)\n",
      "Epoch: [15][18/48]\tTime 0.019 (0.023)\tLoss 1.4429 (1.4383)\tPrec@1 45.996 (48.720)\n",
      "Epoch: [15][27/48]\tTime 0.020 (0.023)\tLoss 1.4685 (1.4362)\tPrec@1 49.121 (48.915)\n",
      "Epoch: [15][36/48]\tTime 0.025 (0.023)\tLoss 1.4267 (1.4367)\tPrec@1 49.121 (48.989)\n",
      "Epoch: [15][45/48]\tTime 0.029 (0.024)\tLoss 1.4461 (1.4325)\tPrec@1 49.121 (49.219)\n",
      "Epoch: [15][48/48]\tTime 0.017 (0.024)\tLoss 1.4358 (1.4325)\tPrec@1 46.698 (49.164)\n",
      "EPOCH: 15 train Results: Prec@1 49.164 Loss: 1.4325\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.4981 (1.4981)\tPrec@1 47.070 (47.070)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.4947 (1.4837)\tPrec@1 44.005 (46.990)\n",
      "EPOCH: 15 val Results: Prec@1 46.990 Loss: 1.4837\n",
      "Best Prec@1: 46.990\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [16][0/48]\tTime 0.025 (0.025)\tLoss 1.4466 (1.4466)\tPrec@1 49.805 (49.805)\n",
      "Epoch: [16][9/48]\tTime 0.021 (0.024)\tLoss 1.4102 (1.4017)\tPrec@1 49.805 (50.762)\n",
      "Epoch: [16][18/48]\tTime 0.027 (0.026)\tLoss 1.4374 (1.4077)\tPrec@1 50.000 (50.288)\n",
      "Epoch: [16][27/48]\tTime 0.036 (0.027)\tLoss 1.3874 (1.4083)\tPrec@1 51.758 (50.471)\n",
      "Epoch: [16][36/48]\tTime 0.036 (0.027)\tLoss 1.4280 (1.4129)\tPrec@1 50.781 (50.340)\n",
      "Epoch: [16][45/48]\tTime 0.100 (0.028)\tLoss 1.4319 (1.4135)\tPrec@1 50.488 (50.244)\n",
      "Epoch: [16][48/48]\tTime 0.014 (0.028)\tLoss 1.4038 (1.4142)\tPrec@1 51.887 (50.232)\n",
      "EPOCH: 16 train Results: Prec@1 50.232 Loss: 1.4142\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.4889 (1.4889)\tPrec@1 47.754 (47.754)\n",
      "Test: [9/9]\tTime 0.008 (0.005)\tLoss 1.4850 (1.4721)\tPrec@1 44.005 (47.440)\n",
      "EPOCH: 16 val Results: Prec@1 47.440 Loss: 1.4721\n",
      "Best Prec@1: 47.440\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [17][0/48]\tTime 0.025 (0.025)\tLoss 1.3789 (1.3789)\tPrec@1 51.367 (51.367)\n",
      "Epoch: [17][9/48]\tTime 0.015 (0.022)\tLoss 1.3513 (1.3984)\tPrec@1 53.320 (50.938)\n",
      "Epoch: [17][18/48]\tTime 0.024 (0.021)\tLoss 1.4042 (1.4028)\tPrec@1 51.270 (50.673)\n",
      "Epoch: [17][27/48]\tTime 0.019 (0.022)\tLoss 1.4214 (1.4027)\tPrec@1 49.121 (50.579)\n",
      "Epoch: [17][36/48]\tTime 0.018 (0.022)\tLoss 1.4367 (1.4004)\tPrec@1 50.488 (50.567)\n",
      "Epoch: [17][45/48]\tTime 0.025 (0.022)\tLoss 1.4237 (1.4009)\tPrec@1 49.023 (50.510)\n",
      "Epoch: [17][48/48]\tTime 0.016 (0.022)\tLoss 1.4146 (1.4014)\tPrec@1 50.472 (50.538)\n",
      "EPOCH: 17 train Results: Prec@1 50.538 Loss: 1.4014\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.4764 (1.4764)\tPrec@1 48.145 (48.145)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.4769 (1.4609)\tPrec@1 44.515 (48.080)\n",
      "EPOCH: 17 val Results: Prec@1 48.080 Loss: 1.4609\n",
      "Best Prec@1: 48.080\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [18][0/48]\tTime 0.020 (0.020)\tLoss 1.3794 (1.3794)\tPrec@1 51.855 (51.855)\n",
      "Epoch: [18][9/48]\tTime 0.020 (0.026)\tLoss 1.4254 (1.3916)\tPrec@1 49.609 (51.191)\n",
      "Epoch: [18][18/48]\tTime 0.055 (0.026)\tLoss 1.4250 (1.3852)\tPrec@1 49.121 (51.280)\n",
      "Epoch: [18][27/48]\tTime 0.015 (0.025)\tLoss 1.3983 (1.3893)\tPrec@1 49.707 (51.081)\n",
      "Epoch: [18][36/48]\tTime 0.031 (0.024)\tLoss 1.3109 (1.3860)\tPrec@1 55.176 (51.148)\n",
      "Epoch: [18][45/48]\tTime 0.021 (0.024)\tLoss 1.3621 (1.3841)\tPrec@1 52.344 (51.259)\n",
      "Epoch: [18][48/48]\tTime 0.014 (0.024)\tLoss 1.4028 (1.3840)\tPrec@1 51.061 (51.246)\n",
      "EPOCH: 18 train Results: Prec@1 51.246 Loss: 1.3840\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.4637 (1.4637)\tPrec@1 47.852 (47.852)\n",
      "Test: [9/9]\tTime 0.012 (0.007)\tLoss 1.4647 (1.4499)\tPrec@1 45.408 (48.320)\n",
      "EPOCH: 18 val Results: Prec@1 48.320 Loss: 1.4499\n",
      "Best Prec@1: 48.320\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [19][0/48]\tTime 0.027 (0.027)\tLoss 1.3472 (1.3472)\tPrec@1 51.562 (51.562)\n",
      "Epoch: [19][9/48]\tTime 0.034 (0.028)\tLoss 1.3567 (1.3516)\tPrec@1 52.734 (52.373)\n",
      "Epoch: [19][18/48]\tTime 0.027 (0.026)\tLoss 1.4023 (1.3589)\tPrec@1 50.879 (52.148)\n",
      "Epoch: [19][27/48]\tTime 0.021 (0.026)\tLoss 1.3829 (1.3635)\tPrec@1 50.879 (51.953)\n",
      "Epoch: [19][36/48]\tTime 0.029 (0.025)\tLoss 1.3817 (1.3694)\tPrec@1 50.684 (51.797)\n",
      "Epoch: [19][45/48]\tTime 0.016 (0.025)\tLoss 1.3850 (1.3698)\tPrec@1 50.977 (51.762)\n",
      "Epoch: [19][48/48]\tTime 0.022 (0.024)\tLoss 1.4148 (1.3695)\tPrec@1 51.533 (51.764)\n",
      "EPOCH: 19 train Results: Prec@1 51.764 Loss: 1.3695\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.4507 (1.4507)\tPrec@1 48.535 (48.535)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.4549 (1.4395)\tPrec@1 46.301 (48.880)\n",
      "EPOCH: 19 val Results: Prec@1 48.880 Loss: 1.4395\n",
      "Best Prec@1: 48.880\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [20][0/48]\tTime 0.024 (0.024)\tLoss 1.3651 (1.3651)\tPrec@1 52.344 (52.344)\n",
      "Epoch: [20][9/48]\tTime 0.019 (0.023)\tLoss 1.3116 (1.3673)\tPrec@1 56.055 (51.523)\n",
      "Epoch: [20][18/48]\tTime 0.016 (0.022)\tLoss 1.3139 (1.3577)\tPrec@1 55.469 (52.251)\n",
      "Epoch: [20][27/48]\tTime 0.023 (0.022)\tLoss 1.3627 (1.3566)\tPrec@1 50.000 (52.271)\n",
      "Epoch: [20][36/48]\tTime 0.029 (0.023)\tLoss 1.3279 (1.3538)\tPrec@1 52.148 (52.431)\n",
      "Epoch: [20][45/48]\tTime 0.020 (0.023)\tLoss 1.2957 (1.3537)\tPrec@1 53.125 (52.494)\n",
      "Epoch: [20][48/48]\tTime 0.021 (0.023)\tLoss 1.4004 (1.3547)\tPrec@1 49.057 (52.458)\n",
      "EPOCH: 20 train Results: Prec@1 52.458 Loss: 1.3547\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.4392 (1.4392)\tPrec@1 48.828 (48.828)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.4496 (1.4302)\tPrec@1 47.194 (49.280)\n",
      "EPOCH: 20 val Results: Prec@1 49.280 Loss: 1.4302\n",
      "Best Prec@1: 49.280\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [21][0/48]\tTime 0.023 (0.023)\tLoss 1.3323 (1.3323)\tPrec@1 54.102 (54.102)\n",
      "Epoch: [21][9/48]\tTime 0.022 (0.024)\tLoss 1.3538 (1.3303)\tPrec@1 53.516 (53.096)\n",
      "Epoch: [21][18/48]\tTime 0.020 (0.023)\tLoss 1.3431 (1.3394)\tPrec@1 53.027 (52.683)\n",
      "Epoch: [21][27/48]\tTime 0.014 (0.022)\tLoss 1.3153 (1.3376)\tPrec@1 54.102 (52.801)\n",
      "Epoch: [21][36/48]\tTime 0.026 (0.022)\tLoss 1.3557 (1.3386)\tPrec@1 53.125 (52.726)\n",
      "Epoch: [21][45/48]\tTime 0.022 (0.021)\tLoss 1.3381 (1.3411)\tPrec@1 51.855 (52.556)\n",
      "Epoch: [21][48/48]\tTime 0.016 (0.021)\tLoss 1.3401 (1.3412)\tPrec@1 53.302 (52.632)\n",
      "EPOCH: 21 train Results: Prec@1 52.632 Loss: 1.3412\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.4281 (1.4281)\tPrec@1 49.316 (49.316)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.4398 (1.4209)\tPrec@1 47.449 (49.570)\n",
      "EPOCH: 21 val Results: Prec@1 49.570 Loss: 1.4209\n",
      "Best Prec@1: 49.570\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [22][0/48]\tTime 0.025 (0.025)\tLoss 1.3290 (1.3290)\tPrec@1 52.148 (52.148)\n",
      "Epoch: [22][9/48]\tTime 0.020 (0.021)\tLoss 1.3497 (1.3348)\tPrec@1 50.879 (52.373)\n",
      "Epoch: [22][18/48]\tTime 0.021 (0.021)\tLoss 1.3667 (1.3298)\tPrec@1 51.562 (52.894)\n",
      "Epoch: [22][27/48]\tTime 0.020 (0.021)\tLoss 1.3658 (1.3291)\tPrec@1 49.902 (52.804)\n",
      "Epoch: [22][36/48]\tTime 0.014 (0.021)\tLoss 1.3399 (1.3268)\tPrec@1 51.953 (52.927)\n",
      "Epoch: [22][45/48]\tTime 0.022 (0.021)\tLoss 1.3681 (1.3286)\tPrec@1 51.855 (52.928)\n",
      "Epoch: [22][48/48]\tTime 0.020 (0.021)\tLoss 1.3788 (1.3295)\tPrec@1 50.590 (52.904)\n",
      "EPOCH: 22 train Results: Prec@1 52.904 Loss: 1.3295\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.4183 (1.4183)\tPrec@1 49.707 (49.707)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.4334 (1.4110)\tPrec@1 48.087 (49.940)\n",
      "EPOCH: 22 val Results: Prec@1 49.940 Loss: 1.4110\n",
      "Best Prec@1: 49.940\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [23][0/48]\tTime 0.027 (0.027)\tLoss 1.3101 (1.3101)\tPrec@1 52.637 (52.637)\n",
      "Epoch: [23][9/48]\tTime 0.022 (0.025)\tLoss 1.2979 (1.3128)\tPrec@1 54.102 (54.023)\n",
      "Epoch: [23][18/48]\tTime 0.018 (0.023)\tLoss 1.3106 (1.3218)\tPrec@1 54.004 (53.906)\n",
      "Epoch: [23][27/48]\tTime 0.028 (0.023)\tLoss 1.2601 (1.3160)\tPrec@1 55.957 (53.934)\n",
      "Epoch: [23][36/48]\tTime 0.019 (0.022)\tLoss 1.2839 (1.3159)\tPrec@1 55.566 (53.846)\n",
      "Epoch: [23][45/48]\tTime 0.024 (0.025)\tLoss 1.3061 (1.3165)\tPrec@1 53.125 (53.724)\n",
      "Epoch: [23][48/48]\tTime 0.015 (0.025)\tLoss 1.3181 (1.3175)\tPrec@1 53.656 (53.700)\n",
      "EPOCH: 23 train Results: Prec@1 53.700 Loss: 1.3175\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.4076 (1.4076)\tPrec@1 49.414 (49.414)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.4267 (1.4028)\tPrec@1 48.724 (50.120)\n",
      "EPOCH: 23 val Results: Prec@1 50.120 Loss: 1.4028\n",
      "Best Prec@1: 50.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [24][0/48]\tTime 0.019 (0.019)\tLoss 1.3076 (1.3076)\tPrec@1 54.980 (54.980)\n",
      "Epoch: [24][9/48]\tTime 0.017 (0.022)\tLoss 1.3192 (1.2932)\tPrec@1 52.930 (54.570)\n",
      "Epoch: [24][18/48]\tTime 0.013 (0.021)\tLoss 1.3196 (1.2960)\tPrec@1 52.441 (54.461)\n",
      "Epoch: [24][27/48]\tTime 0.026 (0.022)\tLoss 1.3028 (1.3045)\tPrec@1 54.102 (54.077)\n",
      "Epoch: [24][36/48]\tTime 0.030 (0.021)\tLoss 1.2906 (1.3019)\tPrec@1 56.055 (54.170)\n",
      "Epoch: [24][45/48]\tTime 0.024 (0.022)\tLoss 1.2644 (1.2989)\tPrec@1 56.250 (54.310)\n",
      "Epoch: [24][48/48]\tTime 0.018 (0.022)\tLoss 1.3396 (1.3021)\tPrec@1 52.830 (54.150)\n",
      "EPOCH: 24 train Results: Prec@1 54.150 Loss: 1.3021\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.4006 (1.4006)\tPrec@1 49.805 (49.805)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.4181 (1.3950)\tPrec@1 48.597 (50.090)\n",
      "EPOCH: 24 val Results: Prec@1 50.090 Loss: 1.3950\n",
      "Best Prec@1: 50.120\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [25][0/48]\tTime 0.028 (0.028)\tLoss 1.2944 (1.2944)\tPrec@1 55.469 (55.469)\n",
      "Epoch: [25][9/48]\tTime 0.017 (0.024)\tLoss 1.2325 (1.2833)\tPrec@1 55.469 (54.795)\n",
      "Epoch: [25][18/48]\tTime 0.018 (0.023)\tLoss 1.2760 (1.2863)\tPrec@1 56.250 (54.862)\n",
      "Epoch: [25][27/48]\tTime 0.020 (0.023)\tLoss 1.2615 (1.2841)\tPrec@1 55.859 (55.040)\n",
      "Epoch: [25][36/48]\tTime 0.018 (0.022)\tLoss 1.2913 (1.2866)\tPrec@1 53.809 (54.893)\n",
      "Epoch: [25][45/48]\tTime 0.022 (0.022)\tLoss 1.2803 (1.2877)\tPrec@1 55.176 (54.772)\n",
      "Epoch: [25][48/48]\tTime 0.017 (0.022)\tLoss 1.2837 (1.2878)\tPrec@1 54.481 (54.782)\n",
      "EPOCH: 25 train Results: Prec@1 54.782 Loss: 1.2878\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.3911 (1.3911)\tPrec@1 50.293 (50.293)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.4136 (1.3874)\tPrec@1 48.469 (50.390)\n",
      "EPOCH: 25 val Results: Prec@1 50.390 Loss: 1.3874\n",
      "Best Prec@1: 50.390\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [26][0/48]\tTime 0.016 (0.016)\tLoss 1.2620 (1.2620)\tPrec@1 55.957 (55.957)\n",
      "Epoch: [26][9/48]\tTime 0.021 (0.030)\tLoss 1.2753 (1.2662)\tPrec@1 54.688 (55.840)\n",
      "Epoch: [26][18/48]\tTime 0.024 (0.031)\tLoss 1.2466 (1.2735)\tPrec@1 56.348 (55.592)\n",
      "Epoch: [26][27/48]\tTime 0.022 (0.028)\tLoss 1.2473 (1.2749)\tPrec@1 56.152 (55.315)\n",
      "Epoch: [26][36/48]\tTime 0.015 (0.026)\tLoss 1.2023 (1.2750)\tPrec@1 57.227 (55.118)\n",
      "Epoch: [26][45/48]\tTime 0.014 (0.025)\tLoss 1.2260 (1.2734)\tPrec@1 56.543 (55.269)\n",
      "Epoch: [26][48/48]\tTime 0.019 (0.025)\tLoss 1.3133 (1.2745)\tPrec@1 52.476 (55.154)\n",
      "EPOCH: 26 train Results: Prec@1 55.154 Loss: 1.2745\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.3809 (1.3809)\tPrec@1 50.195 (50.195)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.4022 (1.3786)\tPrec@1 48.597 (50.600)\n",
      "EPOCH: 26 val Results: Prec@1 50.600 Loss: 1.3786\n",
      "Best Prec@1: 50.600\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [27][0/48]\tTime 0.021 (0.021)\tLoss 1.2612 (1.2612)\tPrec@1 56.543 (56.543)\n",
      "Epoch: [27][9/48]\tTime 0.032 (0.032)\tLoss 1.2708 (1.2507)\tPrec@1 55.762 (56.240)\n",
      "Epoch: [27][18/48]\tTime 0.023 (0.028)\tLoss 1.2884 (1.2579)\tPrec@1 54.297 (55.911)\n",
      "Epoch: [27][27/48]\tTime 0.028 (0.029)\tLoss 1.2372 (1.2563)\tPrec@1 56.543 (55.947)\n",
      "Epoch: [27][36/48]\tTime 0.028 (0.030)\tLoss 1.2835 (1.2627)\tPrec@1 53.125 (55.696)\n",
      "Epoch: [27][45/48]\tTime 0.034 (0.029)\tLoss 1.3088 (1.2654)\tPrec@1 54.297 (55.545)\n",
      "Epoch: [27][48/48]\tTime 0.019 (0.029)\tLoss 1.2560 (1.2665)\tPrec@1 56.368 (55.502)\n",
      "EPOCH: 27 train Results: Prec@1 55.502 Loss: 1.2665\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.3695 (1.3695)\tPrec@1 50.098 (50.098)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.3958 (1.3708)\tPrec@1 48.597 (50.750)\n",
      "EPOCH: 27 val Results: Prec@1 50.750 Loss: 1.3708\n",
      "Best Prec@1: 50.750\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [28][0/48]\tTime 0.021 (0.021)\tLoss 1.2365 (1.2365)\tPrec@1 55.664 (55.664)\n",
      "Epoch: [28][9/48]\tTime 0.015 (0.021)\tLoss 1.2363 (1.2614)\tPrec@1 57.324 (55.527)\n",
      "Epoch: [28][18/48]\tTime 0.021 (0.022)\tLoss 1.2783 (1.2576)\tPrec@1 53.613 (55.638)\n",
      "Epoch: [28][27/48]\tTime 0.022 (0.021)\tLoss 1.2385 (1.2573)\tPrec@1 56.152 (55.790)\n",
      "Epoch: [28][36/48]\tTime 0.022 (0.023)\tLoss 1.2781 (1.2591)\tPrec@1 55.273 (55.815)\n",
      "Epoch: [28][45/48]\tTime 0.027 (0.022)\tLoss 1.2417 (1.2587)\tPrec@1 57.422 (55.845)\n",
      "Epoch: [28][48/48]\tTime 0.024 (0.022)\tLoss 1.3208 (1.2593)\tPrec@1 52.241 (55.830)\n",
      "EPOCH: 28 train Results: Prec@1 55.830 Loss: 1.2593\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.3620 (1.3620)\tPrec@1 50.488 (50.488)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.3879 (1.3638)\tPrec@1 49.235 (50.900)\n",
      "EPOCH: 28 val Results: Prec@1 50.900 Loss: 1.3638\n",
      "Best Prec@1: 50.900\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [29][0/48]\tTime 0.022 (0.022)\tLoss 1.2254 (1.2254)\tPrec@1 58.594 (58.594)\n",
      "Epoch: [29][9/48]\tTime 0.020 (0.020)\tLoss 1.2267 (1.2409)\tPrec@1 57.129 (56.787)\n",
      "Epoch: [29][18/48]\tTime 0.028 (0.021)\tLoss 1.2241 (1.2406)\tPrec@1 56.738 (56.913)\n",
      "Epoch: [29][27/48]\tTime 0.018 (0.023)\tLoss 1.2384 (1.2408)\tPrec@1 56.836 (56.815)\n",
      "Epoch: [29][36/48]\tTime 0.019 (0.022)\tLoss 1.2477 (1.2456)\tPrec@1 55.566 (56.559)\n",
      "Epoch: [29][45/48]\tTime 0.021 (0.022)\tLoss 1.2683 (1.2453)\tPrec@1 55.371 (56.443)\n",
      "Epoch: [29][48/48]\tTime 0.019 (0.022)\tLoss 1.2717 (1.2462)\tPrec@1 54.363 (56.342)\n",
      "EPOCH: 29 train Results: Prec@1 56.342 Loss: 1.2462\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.3518 (1.3518)\tPrec@1 50.781 (50.781)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.3824 (1.3564)\tPrec@1 49.490 (51.040)\n",
      "EPOCH: 29 val Results: Prec@1 51.040 Loss: 1.3564\n",
      "Best Prec@1: 51.040\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [30][0/48]\tTime 0.014 (0.014)\tLoss 1.2809 (1.2809)\tPrec@1 54.004 (54.004)\n",
      "Epoch: [30][9/48]\tTime 0.020 (0.021)\tLoss 1.2670 (1.2379)\tPrec@1 55.859 (56.875)\n",
      "Epoch: [30][18/48]\tTime 0.028 (0.023)\tLoss 1.2293 (1.2300)\tPrec@1 55.957 (57.149)\n",
      "Epoch: [30][27/48]\tTime 0.037 (0.024)\tLoss 1.1954 (1.2271)\tPrec@1 58.301 (57.181)\n",
      "Epoch: [30][36/48]\tTime 0.032 (0.025)\tLoss 1.2604 (1.2285)\tPrec@1 55.176 (56.976)\n",
      "Epoch: [30][45/48]\tTime 0.024 (0.025)\tLoss 1.2471 (1.2295)\tPrec@1 57.227 (57.004)\n",
      "Epoch: [30][48/48]\tTime 0.026 (0.025)\tLoss 1.2509 (1.2315)\tPrec@1 56.250 (56.968)\n",
      "EPOCH: 30 train Results: Prec@1 56.968 Loss: 1.2315\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.3410 (1.3410)\tPrec@1 50.879 (50.879)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.3757 (1.3478)\tPrec@1 48.980 (51.380)\n",
      "EPOCH: 30 val Results: Prec@1 51.380 Loss: 1.3478\n",
      "Best Prec@1: 51.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [31][0/48]\tTime 0.026 (0.026)\tLoss 1.1989 (1.1989)\tPrec@1 57.910 (57.910)\n",
      "Epoch: [31][9/48]\tTime 0.028 (0.030)\tLoss 1.2177 (1.2115)\tPrec@1 55.469 (57.754)\n",
      "Epoch: [31][18/48]\tTime 0.025 (0.037)\tLoss 1.2068 (1.2144)\tPrec@1 58.398 (57.561)\n",
      "Epoch: [31][27/48]\tTime 0.066 (0.037)\tLoss 1.2218 (1.2151)\tPrec@1 57.422 (57.513)\n",
      "Epoch: [31][36/48]\tTime 0.036 (0.036)\tLoss 1.2153 (1.2152)\tPrec@1 57.422 (57.504)\n",
      "Epoch: [31][45/48]\tTime 0.031 (0.037)\tLoss 1.2936 (1.2203)\tPrec@1 54.102 (57.280)\n",
      "Epoch: [31][48/48]\tTime 0.023 (0.038)\tLoss 1.2893 (1.2209)\tPrec@1 54.599 (57.204)\n",
      "EPOCH: 31 train Results: Prec@1 57.204 Loss: 1.2209\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.3362 (1.3362)\tPrec@1 51.367 (51.367)\n",
      "Test: [9/9]\tTime 0.012 (0.011)\tLoss 1.3675 (1.3413)\tPrec@1 49.362 (51.500)\n",
      "EPOCH: 31 val Results: Prec@1 51.500 Loss: 1.3413\n",
      "Best Prec@1: 51.500\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [32][0/48]\tTime 0.042 (0.042)\tLoss 1.2163 (1.2163)\tPrec@1 57.324 (57.324)\n",
      "Epoch: [32][9/48]\tTime 0.027 (0.046)\tLoss 1.1714 (1.2200)\tPrec@1 59.180 (56.943)\n",
      "Epoch: [32][18/48]\tTime 0.030 (0.040)\tLoss 1.1740 (1.2148)\tPrec@1 59.180 (57.350)\n",
      "Epoch: [32][27/48]\tTime 0.029 (0.036)\tLoss 1.2423 (1.2096)\tPrec@1 57.031 (57.607)\n",
      "Epoch: [32][36/48]\tTime 0.051 (0.036)\tLoss 1.1621 (1.2062)\tPrec@1 59.082 (57.667)\n",
      "Epoch: [32][45/48]\tTime 0.029 (0.034)\tLoss 1.2385 (1.2086)\tPrec@1 55.273 (57.530)\n",
      "Epoch: [32][48/48]\tTime 0.019 (0.034)\tLoss 1.2356 (1.2096)\tPrec@1 57.429 (57.574)\n",
      "EPOCH: 32 train Results: Prec@1 57.574 Loss: 1.2096\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.3283 (1.3283)\tPrec@1 51.367 (51.367)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.3623 (1.3360)\tPrec@1 50.510 (51.860)\n",
      "EPOCH: 32 val Results: Prec@1 51.860 Loss: 1.3360\n",
      "Best Prec@1: 51.860\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [33][0/48]\tTime 0.020 (0.020)\tLoss 1.2623 (1.2623)\tPrec@1 54.688 (54.688)\n",
      "Epoch: [33][9/48]\tTime 0.018 (0.020)\tLoss 1.2156 (1.2021)\tPrec@1 57.520 (58.125)\n",
      "Epoch: [33][18/48]\tTime 0.024 (0.028)\tLoss 1.1615 (1.1990)\tPrec@1 59.570 (58.311)\n",
      "Epoch: [33][27/48]\tTime 0.021 (0.031)\tLoss 1.1631 (1.1938)\tPrec@1 60.645 (58.200)\n",
      "Epoch: [33][36/48]\tTime 0.032 (0.030)\tLoss 1.2143 (1.2022)\tPrec@1 57.324 (57.844)\n",
      "Epoch: [33][45/48]\tTime 0.024 (0.031)\tLoss 1.2436 (1.2026)\tPrec@1 54.883 (57.887)\n",
      "Epoch: [33][48/48]\tTime 0.032 (0.031)\tLoss 1.2233 (1.2033)\tPrec@1 56.014 (57.822)\n",
      "EPOCH: 33 train Results: Prec@1 57.822 Loss: 1.2033\n",
      "Test: [0/9]\tTime 0.029 (0.029)\tLoss 1.3202 (1.3202)\tPrec@1 52.637 (52.637)\n",
      "Test: [9/9]\tTime 0.014 (0.012)\tLoss 1.3593 (1.3299)\tPrec@1 51.148 (52.380)\n",
      "EPOCH: 33 val Results: Prec@1 52.380 Loss: 1.3299\n",
      "Best Prec@1: 52.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [34][0/48]\tTime 0.027 (0.027)\tLoss 1.1782 (1.1782)\tPrec@1 59.863 (59.863)\n",
      "Epoch: [34][9/48]\tTime 0.029 (0.033)\tLoss 1.1959 (1.1753)\tPrec@1 57.324 (59.219)\n",
      "Epoch: [34][18/48]\tTime 0.052 (0.041)\tLoss 1.2064 (1.1816)\tPrec@1 56.055 (58.501)\n",
      "Epoch: [34][27/48]\tTime 0.047 (0.043)\tLoss 1.1788 (1.1819)\tPrec@1 58.887 (58.611)\n",
      "Epoch: [34][36/48]\tTime 0.051 (0.042)\tLoss 1.2142 (1.1853)\tPrec@1 59.961 (58.422)\n",
      "Epoch: [34][45/48]\tTime 0.029 (0.040)\tLoss 1.1621 (1.1906)\tPrec@1 59.766 (58.190)\n",
      "Epoch: [34][48/48]\tTime 0.025 (0.039)\tLoss 1.2191 (1.1903)\tPrec@1 57.665 (58.186)\n",
      "EPOCH: 34 train Results: Prec@1 58.186 Loss: 1.1903\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.3181 (1.3181)\tPrec@1 51.953 (51.953)\n",
      "Test: [9/9]\tTime 0.005 (0.008)\tLoss 1.3523 (1.3239)\tPrec@1 51.020 (52.270)\n",
      "EPOCH: 34 val Results: Prec@1 52.270 Loss: 1.3239\n",
      "Best Prec@1: 52.380\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [35][0/48]\tTime 0.017 (0.017)\tLoss 1.1747 (1.1747)\tPrec@1 59.961 (59.961)\n",
      "Epoch: [35][9/48]\tTime 0.017 (0.027)\tLoss 1.1650 (1.1815)\tPrec@1 58.691 (58.848)\n",
      "Epoch: [35][18/48]\tTime 0.033 (0.026)\tLoss 1.1503 (1.1738)\tPrec@1 60.645 (58.907)\n",
      "Epoch: [35][27/48]\tTime 0.025 (0.026)\tLoss 1.1858 (1.1748)\tPrec@1 57.910 (58.897)\n",
      "Epoch: [35][36/48]\tTime 0.045 (0.026)\tLoss 1.2415 (1.1761)\tPrec@1 56.543 (58.752)\n",
      "Epoch: [35][45/48]\tTime 0.024 (0.026)\tLoss 1.2447 (1.1793)\tPrec@1 56.738 (58.645)\n",
      "Epoch: [35][48/48]\tTime 0.014 (0.025)\tLoss 1.1347 (1.1792)\tPrec@1 60.495 (58.646)\n",
      "EPOCH: 35 train Results: Prec@1 58.646 Loss: 1.1792\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.3047 (1.3047)\tPrec@1 52.832 (52.832)\n",
      "Test: [9/9]\tTime 0.009 (0.007)\tLoss 1.3460 (1.3155)\tPrec@1 51.786 (52.800)\n",
      "EPOCH: 35 val Results: Prec@1 52.800 Loss: 1.3155\n",
      "Best Prec@1: 52.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [36][0/48]\tTime 0.038 (0.038)\tLoss 1.1842 (1.1842)\tPrec@1 60.840 (60.840)\n",
      "Epoch: [36][9/48]\tTime 0.020 (0.026)\tLoss 1.1872 (1.1667)\tPrec@1 57.031 (59.639)\n",
      "Epoch: [36][18/48]\tTime 0.029 (0.025)\tLoss 1.1457 (1.1679)\tPrec@1 59.375 (59.570)\n",
      "Epoch: [36][27/48]\tTime 0.015 (0.023)\tLoss 1.1717 (1.1677)\tPrec@1 59.277 (59.528)\n",
      "Epoch: [36][36/48]\tTime 0.021 (0.023)\tLoss 1.2237 (1.1675)\tPrec@1 55.859 (59.441)\n",
      "Epoch: [36][45/48]\tTime 0.022 (0.023)\tLoss 1.1925 (1.1737)\tPrec@1 60.449 (59.207)\n",
      "Epoch: [36][48/48]\tTime 0.016 (0.023)\tLoss 1.2085 (1.1741)\tPrec@1 56.014 (59.178)\n",
      "EPOCH: 36 train Results: Prec@1 59.178 Loss: 1.1741\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.3001 (1.3001)\tPrec@1 53.320 (53.320)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.3420 (1.3121)\tPrec@1 51.786 (53.140)\n",
      "EPOCH: 36 val Results: Prec@1 53.140 Loss: 1.3121\n",
      "Best Prec@1: 53.140\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [37][0/48]\tTime 0.032 (0.032)\tLoss 1.1054 (1.1054)\tPrec@1 62.988 (62.988)\n",
      "Epoch: [37][9/48]\tTime 0.018 (0.023)\tLoss 1.1247 (1.1467)\tPrec@1 59.375 (60.039)\n",
      "Epoch: [37][18/48]\tTime 0.021 (0.022)\tLoss 1.1765 (1.1547)\tPrec@1 57.715 (59.478)\n",
      "Epoch: [37][27/48]\tTime 0.027 (0.023)\tLoss 1.1877 (1.1559)\tPrec@1 57.129 (59.424)\n",
      "Epoch: [37][36/48]\tTime 0.027 (0.023)\tLoss 1.1306 (1.1608)\tPrec@1 59.863 (59.206)\n",
      "Epoch: [37][45/48]\tTime 0.021 (0.022)\tLoss 1.1390 (1.1610)\tPrec@1 59.082 (59.116)\n",
      "Epoch: [37][48/48]\tTime 0.016 (0.022)\tLoss 1.1799 (1.1620)\tPrec@1 57.901 (59.102)\n",
      "EPOCH: 37 train Results: Prec@1 59.102 Loss: 1.1620\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2956 (1.2956)\tPrec@1 53.223 (53.223)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.3377 (1.3082)\tPrec@1 51.658 (53.100)\n",
      "EPOCH: 37 val Results: Prec@1 53.100 Loss: 1.3082\n",
      "Best Prec@1: 53.140\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [38][0/48]\tTime 0.028 (0.028)\tLoss 1.1385 (1.1385)\tPrec@1 60.156 (60.156)\n",
      "Epoch: [38][9/48]\tTime 0.018 (0.024)\tLoss 1.1590 (1.1554)\tPrec@1 57.129 (59.092)\n",
      "Epoch: [38][18/48]\tTime 0.017 (0.023)\tLoss 1.0803 (1.1526)\tPrec@1 60.547 (59.365)\n",
      "Epoch: [38][27/48]\tTime 0.022 (0.023)\tLoss 1.1264 (1.1521)\tPrec@1 60.352 (59.539)\n",
      "Epoch: [38][36/48]\tTime 0.028 (0.023)\tLoss 1.1397 (1.1484)\tPrec@1 60.156 (59.810)\n",
      "Epoch: [38][45/48]\tTime 0.030 (0.024)\tLoss 1.1885 (1.1539)\tPrec@1 59.863 (59.609)\n",
      "Epoch: [38][48/48]\tTime 0.020 (0.024)\tLoss 1.1407 (1.1543)\tPrec@1 59.552 (59.602)\n",
      "EPOCH: 38 train Results: Prec@1 59.602 Loss: 1.1543\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2841 (1.2841)\tPrec@1 52.930 (52.930)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.3263 (1.3029)\tPrec@1 52.806 (53.280)\n",
      "EPOCH: 38 val Results: Prec@1 53.280 Loss: 1.3029\n",
      "Best Prec@1: 53.280\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [39][0/48]\tTime 0.023 (0.023)\tLoss 1.1484 (1.1484)\tPrec@1 58.203 (58.203)\n",
      "Epoch: [39][9/48]\tTime 0.021 (0.025)\tLoss 1.1834 (1.1380)\tPrec@1 58.203 (60.137)\n",
      "Epoch: [39][18/48]\tTime 0.130 (0.030)\tLoss 1.1715 (1.1413)\tPrec@1 58.105 (60.125)\n",
      "Epoch: [39][27/48]\tTime 0.030 (0.027)\tLoss 1.1808 (1.1475)\tPrec@1 58.008 (60.010)\n",
      "Epoch: [39][36/48]\tTime 0.022 (0.030)\tLoss 1.1040 (1.1430)\tPrec@1 61.914 (60.196)\n",
      "Epoch: [39][45/48]\tTime 0.028 (0.029)\tLoss 1.1690 (1.1447)\tPrec@1 58.398 (60.027)\n",
      "Epoch: [39][48/48]\tTime 0.018 (0.029)\tLoss 1.1604 (1.1473)\tPrec@1 60.613 (59.938)\n",
      "EPOCH: 39 train Results: Prec@1 59.938 Loss: 1.1473\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2854 (1.2854)\tPrec@1 54.199 (54.199)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.3249 (1.2987)\tPrec@1 52.041 (53.460)\n",
      "EPOCH: 39 val Results: Prec@1 53.460 Loss: 1.2987\n",
      "Best Prec@1: 53.460\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [40][0/48]\tTime 0.024 (0.024)\tLoss 1.1365 (1.1365)\tPrec@1 61.426 (61.426)\n",
      "Epoch: [40][9/48]\tTime 0.023 (0.023)\tLoss 1.1045 (1.1175)\tPrec@1 59.961 (61.025)\n",
      "Epoch: [40][18/48]\tTime 0.017 (0.022)\tLoss 1.1344 (1.1187)\tPrec@1 58.496 (60.747)\n",
      "Epoch: [40][27/48]\tTime 0.022 (0.023)\tLoss 1.1205 (1.1220)\tPrec@1 62.109 (60.693)\n",
      "Epoch: [40][36/48]\tTime 0.068 (0.026)\tLoss 1.0871 (1.1247)\tPrec@1 63.281 (60.600)\n",
      "Epoch: [40][45/48]\tTime 0.015 (0.027)\tLoss 1.1557 (1.1327)\tPrec@1 59.570 (60.364)\n",
      "Epoch: [40][48/48]\tTime 0.018 (0.026)\tLoss 1.1591 (1.1346)\tPrec@1 60.377 (60.316)\n",
      "EPOCH: 40 train Results: Prec@1 60.316 Loss: 1.1346\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2746 (1.2746)\tPrec@1 53.613 (53.613)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.3223 (1.2909)\tPrec@1 52.423 (53.490)\n",
      "EPOCH: 40 val Results: Prec@1 53.490 Loss: 1.2909\n",
      "Best Prec@1: 53.490\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [41][0/48]\tTime 0.021 (0.021)\tLoss 1.1455 (1.1455)\tPrec@1 58.789 (58.789)\n",
      "Epoch: [41][9/48]\tTime 0.029 (0.023)\tLoss 1.0767 (1.1112)\tPrec@1 62.793 (60.957)\n",
      "Epoch: [41][18/48]\tTime 0.022 (0.027)\tLoss 1.1728 (1.1143)\tPrec@1 59.570 (60.835)\n",
      "Epoch: [41][27/48]\tTime 0.033 (0.028)\tLoss 1.0784 (1.1182)\tPrec@1 63.477 (60.676)\n",
      "Epoch: [41][36/48]\tTime 0.020 (0.027)\tLoss 1.1318 (1.1225)\tPrec@1 58.984 (60.552)\n",
      "Epoch: [41][45/48]\tTime 0.018 (0.026)\tLoss 1.1151 (1.1243)\tPrec@1 62.207 (60.557)\n",
      "Epoch: [41][48/48]\tTime 0.018 (0.026)\tLoss 1.1611 (1.1261)\tPrec@1 58.255 (60.496)\n",
      "EPOCH: 41 train Results: Prec@1 60.496 Loss: 1.1261\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2664 (1.2664)\tPrec@1 54.004 (54.004)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.3269 (1.2891)\tPrec@1 52.423 (54.050)\n",
      "EPOCH: 41 val Results: Prec@1 54.050 Loss: 1.2891\n",
      "Best Prec@1: 54.050\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [42][0/48]\tTime 0.028 (0.028)\tLoss 1.0636 (1.0636)\tPrec@1 65.039 (65.039)\n",
      "Epoch: [42][9/48]\tTime 0.027 (0.021)\tLoss 1.1372 (1.1026)\tPrec@1 58.887 (61.553)\n",
      "Epoch: [42][18/48]\tTime 0.031 (0.021)\tLoss 1.0818 (1.1054)\tPrec@1 61.719 (61.559)\n",
      "Epoch: [42][27/48]\tTime 0.020 (0.023)\tLoss 1.1304 (1.1148)\tPrec@1 59.277 (60.934)\n",
      "Epoch: [42][36/48]\tTime 0.021 (0.024)\tLoss 1.1282 (1.1137)\tPrec@1 59.863 (60.911)\n",
      "Epoch: [42][45/48]\tTime 0.024 (0.024)\tLoss 1.1042 (1.1187)\tPrec@1 61.328 (60.763)\n",
      "Epoch: [42][48/48]\tTime 0.026 (0.024)\tLoss 1.1116 (1.1190)\tPrec@1 61.203 (60.726)\n",
      "EPOCH: 42 train Results: Prec@1 60.726 Loss: 1.1190\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2629 (1.2629)\tPrec@1 54.199 (54.199)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.3137 (1.2830)\tPrec@1 54.082 (54.030)\n",
      "EPOCH: 42 val Results: Prec@1 54.030 Loss: 1.2830\n",
      "Best Prec@1: 54.050\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [43][0/48]\tTime 0.016 (0.016)\tLoss 1.0682 (1.0682)\tPrec@1 64.160 (64.160)\n",
      "Epoch: [43][9/48]\tTime 0.022 (0.023)\tLoss 1.0926 (1.1044)\tPrec@1 62.012 (60.967)\n",
      "Epoch: [43][18/48]\tTime 0.032 (0.025)\tLoss 1.0783 (1.1066)\tPrec@1 62.012 (61.256)\n",
      "Epoch: [43][27/48]\tTime 0.019 (0.023)\tLoss 1.0719 (1.1044)\tPrec@1 63.867 (61.279)\n",
      "Epoch: [43][36/48]\tTime 0.020 (0.023)\tLoss 1.1521 (1.1084)\tPrec@1 58.594 (61.252)\n",
      "Epoch: [43][45/48]\tTime 0.022 (0.023)\tLoss 1.1603 (1.1125)\tPrec@1 58.398 (61.044)\n",
      "Epoch: [43][48/48]\tTime 0.015 (0.023)\tLoss 1.0451 (1.1119)\tPrec@1 62.972 (61.044)\n",
      "EPOCH: 43 train Results: Prec@1 61.044 Loss: 1.1119\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2625 (1.2625)\tPrec@1 54.004 (54.004)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.3209 (1.2808)\tPrec@1 53.954 (53.720)\n",
      "EPOCH: 43 val Results: Prec@1 53.720 Loss: 1.2808\n",
      "Best Prec@1: 54.050\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [44][0/48]\tTime 0.021 (0.021)\tLoss 1.1095 (1.1095)\tPrec@1 59.668 (59.668)\n",
      "Epoch: [44][9/48]\tTime 0.022 (0.023)\tLoss 1.0461 (1.0903)\tPrec@1 63.184 (61.904)\n",
      "Epoch: [44][18/48]\tTime 0.022 (0.023)\tLoss 1.1274 (1.0894)\tPrec@1 62.109 (61.904)\n",
      "Epoch: [44][27/48]\tTime 0.021 (0.022)\tLoss 1.1193 (1.0986)\tPrec@1 58.301 (61.408)\n",
      "Epoch: [44][36/48]\tTime 0.028 (0.022)\tLoss 1.0452 (1.1022)\tPrec@1 62.500 (61.157)\n",
      "Epoch: [44][45/48]\tTime 0.014 (0.022)\tLoss 1.1343 (1.1055)\tPrec@1 58.984 (61.022)\n",
      "Epoch: [44][48/48]\tTime 0.020 (0.024)\tLoss 1.0759 (1.1037)\tPrec@1 62.146 (61.128)\n",
      "EPOCH: 44 train Results: Prec@1 61.128 Loss: 1.1037\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.2566 (1.2566)\tPrec@1 53.906 (53.906)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.3221 (1.2792)\tPrec@1 53.061 (53.640)\n",
      "EPOCH: 44 val Results: Prec@1 53.640 Loss: 1.2792\n",
      "Best Prec@1: 54.050\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [45][0/48]\tTime 0.021 (0.021)\tLoss 1.0853 (1.0853)\tPrec@1 62.402 (62.402)\n",
      "Epoch: [45][9/48]\tTime 0.027 (0.024)\tLoss 1.0801 (1.0854)\tPrec@1 63.574 (62.119)\n",
      "Epoch: [45][18/48]\tTime 0.018 (0.023)\tLoss 1.1281 (1.0795)\tPrec@1 58.496 (62.274)\n",
      "Epoch: [45][27/48]\tTime 0.020 (0.023)\tLoss 1.1076 (1.0872)\tPrec@1 60.254 (62.001)\n",
      "Epoch: [45][36/48]\tTime 0.021 (0.022)\tLoss 1.1602 (1.0928)\tPrec@1 59.668 (61.753)\n",
      "Epoch: [45][45/48]\tTime 0.021 (0.022)\tLoss 1.1308 (1.0942)\tPrec@1 61.230 (61.738)\n",
      "Epoch: [45][48/48]\tTime 0.025 (0.022)\tLoss 1.0734 (1.0933)\tPrec@1 62.264 (61.776)\n",
      "EPOCH: 45 train Results: Prec@1 61.776 Loss: 1.0933\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2565 (1.2565)\tPrec@1 54.297 (54.297)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.3074 (1.2747)\tPrec@1 52.679 (54.200)\n",
      "EPOCH: 45 val Results: Prec@1 54.200 Loss: 1.2747\n",
      "Best Prec@1: 54.200\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [46][0/48]\tTime 0.025 (0.025)\tLoss 1.0714 (1.0714)\tPrec@1 63.672 (63.672)\n",
      "Epoch: [46][9/48]\tTime 0.026 (0.027)\tLoss 1.1284 (1.0764)\tPrec@1 60.645 (62.793)\n",
      "Epoch: [46][18/48]\tTime 0.028 (0.026)\tLoss 1.1211 (1.0746)\tPrec@1 60.156 (62.433)\n",
      "Epoch: [46][27/48]\tTime 0.019 (0.025)\tLoss 1.0810 (1.0790)\tPrec@1 62.402 (62.106)\n",
      "Epoch: [46][36/48]\tTime 0.020 (0.025)\tLoss 1.0653 (1.0836)\tPrec@1 62.012 (61.756)\n",
      "Epoch: [46][45/48]\tTime 0.021 (0.024)\tLoss 1.0559 (1.0833)\tPrec@1 61.133 (61.725)\n",
      "Epoch: [46][48/48]\tTime 0.018 (0.024)\tLoss 1.1574 (1.0861)\tPrec@1 58.844 (61.632)\n",
      "EPOCH: 46 train Results: Prec@1 61.632 Loss: 1.0861\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2568 (1.2568)\tPrec@1 55.176 (55.176)\n",
      "Test: [9/9]\tTime 0.012 (0.008)\tLoss 1.3006 (1.2729)\tPrec@1 52.296 (54.200)\n",
      "EPOCH: 46 val Results: Prec@1 54.200 Loss: 1.2729\n",
      "Best Prec@1: 54.200\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [47][0/48]\tTime 0.026 (0.026)\tLoss 1.0766 (1.0766)\tPrec@1 63.184 (63.184)\n",
      "Epoch: [47][9/48]\tTime 0.020 (0.024)\tLoss 1.0509 (1.0619)\tPrec@1 64.160 (63.164)\n",
      "Epoch: [47][18/48]\tTime 0.023 (0.024)\tLoss 1.0533 (1.0713)\tPrec@1 62.109 (62.711)\n",
      "Epoch: [47][27/48]\tTime 0.020 (0.024)\tLoss 1.0817 (1.0750)\tPrec@1 62.012 (62.444)\n",
      "Epoch: [47][36/48]\tTime 0.026 (0.023)\tLoss 1.1085 (1.0752)\tPrec@1 61.523 (62.355)\n",
      "Epoch: [47][45/48]\tTime 0.034 (0.024)\tLoss 1.0825 (1.0779)\tPrec@1 60.742 (62.182)\n",
      "Epoch: [47][48/48]\tTime 0.019 (0.024)\tLoss 1.1106 (1.0792)\tPrec@1 58.962 (62.094)\n",
      "EPOCH: 47 train Results: Prec@1 62.094 Loss: 1.0792\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.2523 (1.2523)\tPrec@1 55.664 (55.664)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.3050 (1.2715)\tPrec@1 53.316 (54.220)\n",
      "EPOCH: 47 val Results: Prec@1 54.220 Loss: 1.2715\n",
      "Best Prec@1: 54.220\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [48][0/48]\tTime 0.021 (0.021)\tLoss 1.0533 (1.0533)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [48][9/48]\tTime 0.025 (0.025)\tLoss 1.0848 (1.0588)\tPrec@1 61.523 (63.184)\n",
      "Epoch: [48][18/48]\tTime 0.022 (0.024)\tLoss 1.0386 (1.0636)\tPrec@1 65.820 (63.071)\n",
      "Epoch: [48][27/48]\tTime 0.022 (0.025)\tLoss 1.0872 (1.0610)\tPrec@1 61.035 (63.191)\n",
      "Epoch: [48][36/48]\tTime 0.015 (0.024)\tLoss 1.0856 (1.0666)\tPrec@1 60.840 (62.735)\n",
      "Epoch: [48][45/48]\tTime 0.022 (0.023)\tLoss 1.0921 (1.0687)\tPrec@1 61.230 (62.636)\n",
      "Epoch: [48][48/48]\tTime 0.014 (0.023)\tLoss 1.1425 (1.0712)\tPrec@1 61.792 (62.536)\n",
      "EPOCH: 48 train Results: Prec@1 62.536 Loss: 1.0712\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2472 (1.2472)\tPrec@1 54.785 (54.785)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.3025 (1.2665)\tPrec@1 53.316 (54.470)\n",
      "EPOCH: 48 val Results: Prec@1 54.470 Loss: 1.2665\n",
      "Best Prec@1: 54.470\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [49][0/48]\tTime 0.024 (0.024)\tLoss 1.0601 (1.0601)\tPrec@1 63.477 (63.477)\n",
      "Epoch: [49][9/48]\tTime 0.026 (0.023)\tLoss 1.0142 (1.0311)\tPrec@1 64.648 (63.740)\n",
      "Epoch: [49][18/48]\tTime 0.018 (0.022)\tLoss 1.0516 (1.0479)\tPrec@1 62.109 (63.245)\n",
      "Epoch: [49][27/48]\tTime 0.026 (0.024)\tLoss 1.1184 (1.0567)\tPrec@1 59.082 (63.062)\n",
      "Epoch: [49][36/48]\tTime 0.016 (0.023)\tLoss 1.0350 (1.0615)\tPrec@1 63.770 (62.833)\n",
      "Epoch: [49][45/48]\tTime 0.023 (0.023)\tLoss 1.0941 (1.0640)\tPrec@1 62.500 (62.678)\n",
      "Epoch: [49][48/48]\tTime 0.012 (0.023)\tLoss 1.0749 (1.0657)\tPrec@1 62.264 (62.624)\n",
      "EPOCH: 49 train Results: Prec@1 62.624 Loss: 1.0657\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2373 (1.2373)\tPrec@1 55.078 (55.078)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.2962 (1.2632)\tPrec@1 53.061 (54.410)\n",
      "EPOCH: 49 val Results: Prec@1 54.410 Loss: 1.2632\n",
      "Best Prec@1: 54.470\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [50][0/48]\tTime 0.023 (0.023)\tLoss 1.0283 (1.0283)\tPrec@1 65.332 (65.332)\n",
      "Epoch: [50][9/48]\tTime 0.021 (0.026)\tLoss 1.0837 (1.0309)\tPrec@1 60.938 (63.984)\n",
      "Epoch: [50][18/48]\tTime 0.025 (0.025)\tLoss 1.0922 (1.0434)\tPrec@1 60.156 (63.384)\n",
      "Epoch: [50][27/48]\tTime 0.028 (0.025)\tLoss 1.0339 (1.0462)\tPrec@1 63.965 (63.341)\n",
      "Epoch: [50][36/48]\tTime 0.021 (0.026)\tLoss 1.0635 (1.0503)\tPrec@1 62.402 (63.223)\n",
      "Epoch: [50][45/48]\tTime 0.031 (0.026)\tLoss 1.0807 (1.0547)\tPrec@1 62.207 (63.039)\n",
      "Epoch: [50][48/48]\tTime 0.013 (0.025)\tLoss 1.1067 (1.0577)\tPrec@1 63.090 (62.940)\n",
      "EPOCH: 50 train Results: Prec@1 62.940 Loss: 1.0577\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2411 (1.2411)\tPrec@1 55.273 (55.273)\n",
      "Test: [9/9]\tTime 0.010 (0.005)\tLoss 1.3048 (1.2601)\tPrec@1 53.189 (54.470)\n",
      "EPOCH: 50 val Results: Prec@1 54.470 Loss: 1.2601\n",
      "Best Prec@1: 54.470\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [51][0/48]\tTime 0.020 (0.020)\tLoss 1.0461 (1.0461)\tPrec@1 63.379 (63.379)\n",
      "Epoch: [51][9/48]\tTime 0.024 (0.021)\tLoss 1.0628 (1.0313)\tPrec@1 64.062 (63.877)\n",
      "Epoch: [51][18/48]\tTime 0.029 (0.023)\tLoss 1.0212 (1.0393)\tPrec@1 65.137 (63.862)\n",
      "Epoch: [51][27/48]\tTime 0.016 (0.023)\tLoss 1.0495 (1.0369)\tPrec@1 64.062 (63.808)\n",
      "Epoch: [51][36/48]\tTime 0.021 (0.023)\tLoss 1.0611 (1.0442)\tPrec@1 63.379 (63.508)\n",
      "Epoch: [51][45/48]\tTime 0.018 (0.023)\tLoss 1.1121 (1.0511)\tPrec@1 61.523 (63.326)\n",
      "Epoch: [51][48/48]\tTime 0.013 (0.023)\tLoss 1.0611 (1.0524)\tPrec@1 63.443 (63.186)\n",
      "EPOCH: 51 train Results: Prec@1 63.186 Loss: 1.0524\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2236 (1.2236)\tPrec@1 55.664 (55.664)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.3000 (1.2570)\tPrec@1 52.168 (54.640)\n",
      "EPOCH: 51 val Results: Prec@1 54.640 Loss: 1.2570\n",
      "Best Prec@1: 54.640\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [52][0/48]\tTime 0.027 (0.027)\tLoss 1.0226 (1.0226)\tPrec@1 63.574 (63.574)\n",
      "Epoch: [52][9/48]\tTime 0.028 (0.021)\tLoss 1.0024 (1.0137)\tPrec@1 66.113 (64.717)\n",
      "Epoch: [52][18/48]\tTime 0.022 (0.021)\tLoss 1.0024 (1.0199)\tPrec@1 65.332 (64.541)\n",
      "Epoch: [52][27/48]\tTime 0.027 (0.022)\tLoss 1.0387 (1.0285)\tPrec@1 61.621 (64.122)\n",
      "Epoch: [52][36/48]\tTime 0.019 (0.022)\tLoss 1.0363 (1.0329)\tPrec@1 62.793 (63.722)\n",
      "Epoch: [52][45/48]\tTime 0.029 (0.022)\tLoss 1.0732 (1.0422)\tPrec@1 61.230 (63.379)\n",
      "Epoch: [52][48/48]\tTime 0.013 (0.022)\tLoss 1.0836 (1.0437)\tPrec@1 61.792 (63.304)\n",
      "EPOCH: 52 train Results: Prec@1 63.304 Loss: 1.0437\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2214 (1.2214)\tPrec@1 55.762 (55.762)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.2856 (1.2579)\tPrec@1 53.699 (54.950)\n",
      "EPOCH: 52 val Results: Prec@1 54.950 Loss: 1.2579\n",
      "Best Prec@1: 54.950\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [53][0/48]\tTime 0.029 (0.029)\tLoss 0.9915 (0.9915)\tPrec@1 65.527 (65.527)\n",
      "Epoch: [53][9/48]\tTime 0.027 (0.024)\tLoss 1.0230 (1.0066)\tPrec@1 64.062 (64.727)\n",
      "Epoch: [53][18/48]\tTime 0.025 (0.022)\tLoss 1.0309 (1.0220)\tPrec@1 63.379 (64.032)\n",
      "Epoch: [53][27/48]\tTime 0.018 (0.023)\tLoss 1.1254 (1.0271)\tPrec@1 60.449 (63.811)\n",
      "Epoch: [53][36/48]\tTime 0.022 (0.022)\tLoss 1.0914 (1.0313)\tPrec@1 61.621 (63.675)\n",
      "Epoch: [53][45/48]\tTime 0.019 (0.022)\tLoss 1.0441 (1.0350)\tPrec@1 63.867 (63.532)\n",
      "Epoch: [53][48/48]\tTime 0.026 (0.023)\tLoss 1.0875 (1.0371)\tPrec@1 62.028 (63.462)\n",
      "EPOCH: 53 train Results: Prec@1 63.462 Loss: 1.0371\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2311 (1.2311)\tPrec@1 56.934 (56.934)\n",
      "Test: [9/9]\tTime 0.008 (0.006)\tLoss 1.2977 (1.2569)\tPrec@1 54.719 (55.100)\n",
      "EPOCH: 53 val Results: Prec@1 55.100 Loss: 1.2569\n",
      "Best Prec@1: 55.100\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [54][0/48]\tTime 0.017 (0.017)\tLoss 0.9985 (0.9985)\tPrec@1 64.551 (64.551)\n",
      "Epoch: [54][9/48]\tTime 0.019 (0.023)\tLoss 1.0376 (1.0197)\tPrec@1 64.355 (64.004)\n",
      "Epoch: [54][18/48]\tTime 0.022 (0.024)\tLoss 1.0497 (1.0233)\tPrec@1 62.012 (63.826)\n",
      "Epoch: [54][27/48]\tTime 0.021 (0.023)\tLoss 1.0340 (1.0275)\tPrec@1 62.402 (63.707)\n",
      "Epoch: [54][36/48]\tTime 0.024 (0.024)\tLoss 1.0258 (1.0309)\tPrec@1 64.844 (63.738)\n",
      "Epoch: [54][45/48]\tTime 0.042 (0.024)\tLoss 1.0895 (1.0395)\tPrec@1 61.230 (63.451)\n",
      "Epoch: [54][48/48]\tTime 0.012 (0.024)\tLoss 1.0977 (1.0399)\tPrec@1 61.439 (63.416)\n",
      "EPOCH: 54 train Results: Prec@1 63.416 Loss: 1.0399\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2308 (1.2308)\tPrec@1 56.152 (56.152)\n",
      "Test: [9/9]\tTime 0.005 (0.005)\tLoss 1.2809 (1.2572)\tPrec@1 53.699 (54.840)\n",
      "EPOCH: 54 val Results: Prec@1 54.840 Loss: 1.2572\n",
      "Best Prec@1: 55.100\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [55][0/48]\tTime 0.023 (0.023)\tLoss 1.0318 (1.0318)\tPrec@1 64.160 (64.160)\n",
      "Epoch: [55][9/48]\tTime 0.019 (0.022)\tLoss 1.0124 (1.0089)\tPrec@1 64.844 (64.854)\n",
      "Epoch: [55][18/48]\tTime 0.022 (0.022)\tLoss 1.0073 (1.0134)\tPrec@1 64.551 (64.618)\n",
      "Epoch: [55][27/48]\tTime 0.027 (0.023)\tLoss 1.0600 (1.0174)\tPrec@1 63.379 (64.411)\n",
      "Epoch: [55][36/48]\tTime 0.027 (0.023)\tLoss 1.0004 (1.0188)\tPrec@1 66.309 (64.474)\n",
      "Epoch: [55][45/48]\tTime 0.037 (0.023)\tLoss 1.0165 (1.0244)\tPrec@1 64.453 (64.154)\n",
      "Epoch: [55][48/48]\tTime 0.019 (0.024)\tLoss 1.0312 (1.0260)\tPrec@1 65.094 (64.056)\n",
      "EPOCH: 55 train Results: Prec@1 64.056 Loss: 1.0260\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2363 (1.2363)\tPrec@1 55.859 (55.859)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2844 (1.2556)\tPrec@1 54.209 (55.240)\n",
      "EPOCH: 55 val Results: Prec@1 55.240 Loss: 1.2556\n",
      "Best Prec@1: 55.240\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [56][0/48]\tTime 0.022 (0.022)\tLoss 1.0308 (1.0308)\tPrec@1 65.039 (65.039)\n",
      "Epoch: [56][9/48]\tTime 0.017 (0.022)\tLoss 1.0014 (1.0118)\tPrec@1 61.914 (64.453)\n",
      "Epoch: [56][18/48]\tTime 0.019 (0.023)\tLoss 1.0124 (1.0100)\tPrec@1 65.430 (64.839)\n",
      "Epoch: [56][27/48]\tTime 0.021 (0.022)\tLoss 1.0171 (1.0054)\tPrec@1 63.574 (64.795)\n",
      "Epoch: [56][36/48]\tTime 0.018 (0.023)\tLoss 1.0313 (1.0111)\tPrec@1 64.941 (64.630)\n",
      "Epoch: [56][45/48]\tTime 0.017 (0.023)\tLoss 1.0760 (1.0167)\tPrec@1 63.184 (64.434)\n",
      "Epoch: [56][48/48]\tTime 0.018 (0.023)\tLoss 1.0608 (1.0199)\tPrec@1 62.618 (64.272)\n",
      "EPOCH: 56 train Results: Prec@1 64.272 Loss: 1.0199\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2173 (1.2173)\tPrec@1 56.836 (56.836)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2894 (1.2543)\tPrec@1 53.444 (55.170)\n",
      "EPOCH: 56 val Results: Prec@1 55.170 Loss: 1.2543\n",
      "Best Prec@1: 55.240\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [57][0/48]\tTime 0.025 (0.025)\tLoss 1.0252 (1.0252)\tPrec@1 63.281 (63.281)\n",
      "Epoch: [57][9/48]\tTime 0.022 (0.032)\tLoss 0.9544 (0.9812)\tPrec@1 67.188 (65.459)\n",
      "Epoch: [57][18/48]\tTime 0.017 (0.026)\tLoss 1.0067 (0.9916)\tPrec@1 65.039 (65.394)\n",
      "Epoch: [57][27/48]\tTime 0.016 (0.024)\tLoss 0.9562 (0.9943)\tPrec@1 68.750 (65.395)\n",
      "Epoch: [57][36/48]\tTime 0.015 (0.023)\tLoss 1.0654 (1.0008)\tPrec@1 61.230 (64.844)\n",
      "Epoch: [57][45/48]\tTime 0.018 (0.023)\tLoss 1.0757 (1.0130)\tPrec@1 63.281 (64.419)\n",
      "Epoch: [57][48/48]\tTime 0.011 (0.022)\tLoss 1.0443 (1.0148)\tPrec@1 62.854 (64.358)\n",
      "EPOCH: 57 train Results: Prec@1 64.358 Loss: 1.0148\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2096 (1.2096)\tPrec@1 56.055 (56.055)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2861 (1.2530)\tPrec@1 53.827 (54.860)\n",
      "EPOCH: 57 val Results: Prec@1 54.860 Loss: 1.2530\n",
      "Best Prec@1: 55.240\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [58][0/48]\tTime 0.015 (0.015)\tLoss 1.0007 (1.0007)\tPrec@1 65.332 (65.332)\n",
      "Epoch: [58][9/48]\tTime 0.025 (0.021)\tLoss 1.0295 (0.9914)\tPrec@1 63.184 (65.527)\n",
      "Epoch: [58][18/48]\tTime 0.025 (0.021)\tLoss 0.9790 (0.9899)\tPrec@1 64.844 (65.383)\n",
      "Epoch: [58][27/48]\tTime 0.034 (0.026)\tLoss 1.0341 (0.9983)\tPrec@1 63.867 (65.112)\n",
      "Epoch: [58][36/48]\tTime 0.027 (0.030)\tLoss 1.0077 (1.0020)\tPrec@1 63.477 (64.941)\n",
      "Epoch: [58][45/48]\tTime 0.021 (0.029)\tLoss 1.0793 (1.0095)\tPrec@1 63.281 (64.689)\n",
      "Epoch: [58][48/48]\tTime 0.017 (0.029)\tLoss 1.0536 (1.0118)\tPrec@1 61.792 (64.584)\n",
      "EPOCH: 58 train Results: Prec@1 64.584 Loss: 1.0118\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2166 (1.2166)\tPrec@1 56.152 (56.152)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2991 (1.2536)\tPrec@1 52.679 (55.100)\n",
      "EPOCH: 58 val Results: Prec@1 55.100 Loss: 1.2536\n",
      "Best Prec@1: 55.240\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [59][0/48]\tTime 0.029 (0.029)\tLoss 1.0214 (1.0214)\tPrec@1 63.867 (63.867)\n",
      "Epoch: [59][9/48]\tTime 0.019 (0.022)\tLoss 1.0186 (0.9801)\tPrec@1 63.086 (66.123)\n",
      "Epoch: [59][18/48]\tTime 0.020 (0.023)\tLoss 1.0510 (0.9869)\tPrec@1 64.062 (65.630)\n",
      "Epoch: [59][27/48]\tTime 0.022 (0.023)\tLoss 0.9748 (0.9898)\tPrec@1 67.090 (65.472)\n",
      "Epoch: [59][36/48]\tTime 0.026 (0.022)\tLoss 0.9938 (0.9974)\tPrec@1 65.527 (65.163)\n",
      "Epoch: [59][45/48]\tTime 0.023 (0.022)\tLoss 1.0322 (1.0048)\tPrec@1 62.988 (64.803)\n",
      "Epoch: [59][48/48]\tTime 0.019 (0.022)\tLoss 1.0476 (1.0060)\tPrec@1 62.500 (64.766)\n",
      "EPOCH: 59 train Results: Prec@1 64.766 Loss: 1.0060\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2174 (1.2174)\tPrec@1 56.641 (56.641)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2737 (1.2504)\tPrec@1 54.974 (55.130)\n",
      "EPOCH: 59 val Results: Prec@1 55.130 Loss: 1.2504\n",
      "Best Prec@1: 55.240\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [60][0/48]\tTime 0.015 (0.015)\tLoss 0.9575 (0.9575)\tPrec@1 66.406 (66.406)\n",
      "Epoch: [60][9/48]\tTime 0.027 (0.020)\tLoss 1.0047 (0.9876)\tPrec@1 64.062 (65.410)\n",
      "Epoch: [60][18/48]\tTime 0.027 (0.022)\tLoss 1.0287 (0.9932)\tPrec@1 64.648 (65.425)\n",
      "Epoch: [60][27/48]\tTime 0.034 (0.023)\tLoss 0.9991 (0.9940)\tPrec@1 66.113 (65.395)\n",
      "Epoch: [60][36/48]\tTime 0.025 (0.023)\tLoss 1.0285 (0.9994)\tPrec@1 62.891 (65.044)\n",
      "Epoch: [60][45/48]\tTime 0.021 (0.023)\tLoss 0.9940 (1.0052)\tPrec@1 64.551 (64.672)\n",
      "Epoch: [60][48/48]\tTime 0.025 (0.023)\tLoss 1.0273 (1.0055)\tPrec@1 62.028 (64.666)\n",
      "EPOCH: 60 train Results: Prec@1 64.666 Loss: 1.0055\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2067 (1.2067)\tPrec@1 55.957 (55.957)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.2847 (1.2455)\tPrec@1 54.082 (55.280)\n",
      "EPOCH: 60 val Results: Prec@1 55.280 Loss: 1.2455\n",
      "Best Prec@1: 55.280\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [61][0/48]\tTime 0.016 (0.016)\tLoss 0.9302 (0.9302)\tPrec@1 67.676 (67.676)\n",
      "Epoch: [61][9/48]\tTime 0.025 (0.020)\tLoss 1.0081 (0.9649)\tPrec@1 64.453 (66.406)\n",
      "Epoch: [61][18/48]\tTime 0.022 (0.021)\tLoss 0.9961 (0.9703)\tPrec@1 62.988 (66.062)\n",
      "Epoch: [61][27/48]\tTime 0.022 (0.022)\tLoss 0.9553 (0.9822)\tPrec@1 67.188 (65.597)\n",
      "Epoch: [61][36/48]\tTime 0.016 (0.022)\tLoss 0.9980 (0.9880)\tPrec@1 67.871 (65.496)\n",
      "Epoch: [61][45/48]\tTime 0.022 (0.022)\tLoss 1.0439 (0.9958)\tPrec@1 62.500 (65.094)\n",
      "Epoch: [61][48/48]\tTime 0.021 (0.023)\tLoss 1.0327 (0.9978)\tPrec@1 63.090 (65.016)\n",
      "EPOCH: 61 train Results: Prec@1 65.016 Loss: 0.9978\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2213 (1.2213)\tPrec@1 56.348 (56.348)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2862 (1.2458)\tPrec@1 55.357 (55.280)\n",
      "EPOCH: 61 val Results: Prec@1 55.280 Loss: 1.2458\n",
      "Best Prec@1: 55.280\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [62][0/48]\tTime 0.026 (0.026)\tLoss 0.9795 (0.9795)\tPrec@1 65.430 (65.430)\n",
      "Epoch: [62][9/48]\tTime 0.022 (0.022)\tLoss 0.9536 (0.9679)\tPrec@1 66.504 (66.553)\n",
      "Epoch: [62][18/48]\tTime 0.026 (0.022)\tLoss 0.9681 (0.9707)\tPrec@1 66.895 (66.309)\n",
      "Epoch: [62][27/48]\tTime 0.026 (0.023)\tLoss 0.9776 (0.9753)\tPrec@1 65.723 (66.085)\n",
      "Epoch: [62][36/48]\tTime 0.018 (0.022)\tLoss 0.9871 (0.9825)\tPrec@1 65.820 (65.662)\n",
      "Epoch: [62][45/48]\tTime 0.021 (0.022)\tLoss 1.0283 (0.9893)\tPrec@1 61.328 (65.328)\n",
      "Epoch: [62][48/48]\tTime 0.016 (0.022)\tLoss 1.0208 (0.9900)\tPrec@1 63.090 (65.308)\n",
      "EPOCH: 62 train Results: Prec@1 65.308 Loss: 0.9900\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2243 (1.2243)\tPrec@1 54.980 (54.980)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2747 (1.2466)\tPrec@1 53.189 (55.360)\n",
      "EPOCH: 62 val Results: Prec@1 55.360 Loss: 1.2466\n",
      "Best Prec@1: 55.360\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [63][0/48]\tTime 0.016 (0.016)\tLoss 0.9798 (0.9798)\tPrec@1 65.039 (65.039)\n",
      "Epoch: [63][9/48]\tTime 0.016 (0.024)\tLoss 0.9921 (0.9519)\tPrec@1 64.551 (66.758)\n",
      "Epoch: [63][18/48]\tTime 0.018 (0.023)\tLoss 1.0430 (0.9673)\tPrec@1 62.891 (65.944)\n",
      "Epoch: [63][27/48]\tTime 0.019 (0.022)\tLoss 1.0354 (0.9724)\tPrec@1 65.137 (65.789)\n",
      "Epoch: [63][36/48]\tTime 0.015 (0.021)\tLoss 1.0178 (0.9831)\tPrec@1 63.965 (65.361)\n",
      "Epoch: [63][45/48]\tTime 0.014 (0.021)\tLoss 0.9986 (0.9892)\tPrec@1 64.941 (65.050)\n",
      "Epoch: [63][48/48]\tTime 0.022 (0.021)\tLoss 1.0215 (0.9903)\tPrec@1 64.505 (65.016)\n",
      "EPOCH: 63 train Results: Prec@1 65.016 Loss: 0.9903\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2246 (1.2246)\tPrec@1 56.641 (56.641)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.2842 (1.2429)\tPrec@1 53.444 (56.030)\n",
      "EPOCH: 63 val Results: Prec@1 56.030 Loss: 1.2429\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [64][0/48]\tTime 0.020 (0.020)\tLoss 0.9639 (0.9639)\tPrec@1 66.504 (66.504)\n",
      "Epoch: [64][9/48]\tTime 0.020 (0.022)\tLoss 0.9740 (0.9474)\tPrec@1 66.211 (66.904)\n",
      "Epoch: [64][18/48]\tTime 0.022 (0.021)\tLoss 0.9731 (0.9514)\tPrec@1 65.234 (66.689)\n",
      "Epoch: [64][27/48]\tTime 0.022 (0.021)\tLoss 0.9077 (0.9602)\tPrec@1 67.871 (66.263)\n",
      "Epoch: [64][36/48]\tTime 0.026 (0.022)\tLoss 1.0554 (0.9689)\tPrec@1 62.500 (65.987)\n",
      "Epoch: [64][45/48]\tTime 0.020 (0.022)\tLoss 0.9984 (0.9766)\tPrec@1 67.383 (65.780)\n",
      "Epoch: [64][48/48]\tTime 0.015 (0.022)\tLoss 1.0112 (0.9798)\tPrec@1 64.741 (65.656)\n",
      "EPOCH: 64 train Results: Prec@1 65.656 Loss: 0.9798\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.2116 (1.2116)\tPrec@1 57.812 (57.812)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.2778 (1.2448)\tPrec@1 53.189 (55.740)\n",
      "EPOCH: 64 val Results: Prec@1 55.740 Loss: 1.2448\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [65][0/48]\tTime 0.022 (0.022)\tLoss 0.9627 (0.9627)\tPrec@1 65.527 (65.527)\n",
      "Epoch: [65][9/48]\tTime 0.019 (0.023)\tLoss 0.9701 (0.9416)\tPrec@1 65.039 (67.207)\n",
      "Epoch: [65][18/48]\tTime 0.021 (0.022)\tLoss 0.9735 (0.9558)\tPrec@1 66.602 (66.586)\n",
      "Epoch: [65][27/48]\tTime 0.027 (0.022)\tLoss 1.0211 (0.9669)\tPrec@1 62.598 (66.127)\n",
      "Epoch: [65][36/48]\tTime 0.027 (0.022)\tLoss 1.0112 (0.9754)\tPrec@1 64.941 (65.752)\n",
      "Epoch: [65][45/48]\tTime 0.026 (0.022)\tLoss 1.0123 (0.9787)\tPrec@1 63.477 (65.629)\n",
      "Epoch: [65][48/48]\tTime 0.020 (0.022)\tLoss 1.0708 (0.9815)\tPrec@1 63.090 (65.540)\n",
      "EPOCH: 65 train Results: Prec@1 65.540 Loss: 0.9815\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2190 (1.2190)\tPrec@1 57.520 (57.520)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.2624 (1.2437)\tPrec@1 54.847 (55.720)\n",
      "EPOCH: 65 val Results: Prec@1 55.720 Loss: 1.2437\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [66][0/48]\tTime 0.024 (0.024)\tLoss 0.9690 (0.9690)\tPrec@1 65.918 (65.918)\n",
      "Epoch: [66][9/48]\tTime 0.021 (0.024)\tLoss 0.9543 (0.9474)\tPrec@1 66.406 (66.299)\n",
      "Epoch: [66][18/48]\tTime 0.024 (0.023)\tLoss 0.9664 (0.9482)\tPrec@1 65.820 (66.468)\n",
      "Epoch: [66][27/48]\tTime 0.026 (0.025)\tLoss 0.9929 (0.9543)\tPrec@1 65.234 (66.326)\n",
      "Epoch: [66][36/48]\tTime 0.086 (0.027)\tLoss 0.9858 (0.9642)\tPrec@1 64.844 (66.016)\n",
      "Epoch: [66][45/48]\tTime 0.045 (0.027)\tLoss 0.9630 (0.9705)\tPrec@1 66.211 (65.880)\n",
      "Epoch: [66][48/48]\tTime 0.018 (0.027)\tLoss 1.0444 (0.9731)\tPrec@1 62.382 (65.800)\n",
      "EPOCH: 66 train Results: Prec@1 65.800 Loss: 0.9731\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2290 (1.2290)\tPrec@1 56.543 (56.543)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.2724 (1.2404)\tPrec@1 52.934 (55.440)\n",
      "EPOCH: 66 val Results: Prec@1 55.440 Loss: 1.2404\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [67][0/48]\tTime 0.020 (0.020)\tLoss 0.8893 (0.8893)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [67][9/48]\tTime 0.023 (0.020)\tLoss 0.9786 (0.9350)\tPrec@1 65.332 (67.227)\n",
      "Epoch: [67][18/48]\tTime 0.023 (0.021)\tLoss 0.9824 (0.9410)\tPrec@1 64.453 (66.869)\n",
      "Epoch: [67][27/48]\tTime 0.025 (0.022)\tLoss 0.9778 (0.9541)\tPrec@1 66.895 (66.382)\n",
      "Epoch: [67][36/48]\tTime 0.029 (0.022)\tLoss 0.9581 (0.9605)\tPrec@1 64.551 (65.995)\n",
      "Epoch: [67][45/48]\tTime 0.022 (0.022)\tLoss 1.0469 (0.9685)\tPrec@1 62.793 (65.659)\n",
      "Epoch: [67][48/48]\tTime 0.017 (0.022)\tLoss 0.9950 (0.9706)\tPrec@1 65.212 (65.602)\n",
      "EPOCH: 67 train Results: Prec@1 65.602 Loss: 0.9706\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2090 (1.2090)\tPrec@1 57.520 (57.520)\n",
      "Test: [9/9]\tTime 0.008 (0.006)\tLoss 1.2834 (1.2408)\tPrec@1 53.699 (55.510)\n",
      "EPOCH: 67 val Results: Prec@1 55.510 Loss: 1.2408\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [68][0/48]\tTime 0.027 (0.027)\tLoss 0.8824 (0.8824)\tPrec@1 69.629 (69.629)\n",
      "Epoch: [68][9/48]\tTime 0.025 (0.021)\tLoss 0.9103 (0.9156)\tPrec@1 68.555 (68.057)\n",
      "Epoch: [68][18/48]\tTime 0.029 (0.022)\tLoss 0.9966 (0.9396)\tPrec@1 63.379 (66.792)\n",
      "Epoch: [68][27/48]\tTime 0.024 (0.022)\tLoss 0.9370 (0.9468)\tPrec@1 66.504 (66.727)\n",
      "Epoch: [68][36/48]\tTime 0.025 (0.022)\tLoss 1.0014 (0.9523)\tPrec@1 65.918 (66.522)\n",
      "Epoch: [68][45/48]\tTime 0.025 (0.023)\tLoss 1.0139 (0.9589)\tPrec@1 64.355 (66.296)\n",
      "Epoch: [68][48/48]\tTime 0.013 (0.022)\tLoss 1.0295 (0.9617)\tPrec@1 63.090 (66.134)\n",
      "EPOCH: 68 train Results: Prec@1 66.134 Loss: 0.9617\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2198 (1.2198)\tPrec@1 56.738 (56.738)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2822 (1.2428)\tPrec@1 54.209 (55.710)\n",
      "EPOCH: 68 val Results: Prec@1 55.710 Loss: 1.2428\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [69][0/48]\tTime 0.022 (0.022)\tLoss 0.9066 (0.9066)\tPrec@1 68.457 (68.457)\n",
      "Epoch: [69][9/48]\tTime 0.016 (0.030)\tLoss 0.9703 (0.9185)\tPrec@1 65.723 (68.174)\n",
      "Epoch: [69][18/48]\tTime 0.017 (0.024)\tLoss 0.9602 (0.9306)\tPrec@1 66.113 (67.866)\n",
      "Epoch: [69][27/48]\tTime 0.028 (0.024)\tLoss 0.9951 (0.9413)\tPrec@1 65.527 (67.240)\n",
      "Epoch: [69][36/48]\tTime 0.021 (0.024)\tLoss 0.9776 (0.9518)\tPrec@1 65.039 (66.720)\n",
      "Epoch: [69][45/48]\tTime 0.021 (0.024)\tLoss 0.9450 (0.9594)\tPrec@1 66.113 (66.343)\n",
      "Epoch: [69][48/48]\tTime 0.018 (0.024)\tLoss 1.0084 (0.9620)\tPrec@1 63.208 (66.270)\n",
      "EPOCH: 69 train Results: Prec@1 66.270 Loss: 0.9620\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2214 (1.2214)\tPrec@1 57.129 (57.129)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2859 (1.2424)\tPrec@1 53.954 (55.930)\n",
      "EPOCH: 69 val Results: Prec@1 55.930 Loss: 1.2424\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [70][0/48]\tTime 0.021 (0.021)\tLoss 0.8891 (0.8891)\tPrec@1 67.773 (67.773)\n",
      "Epoch: [70][9/48]\tTime 0.017 (0.021)\tLoss 0.9748 (0.9183)\tPrec@1 66.699 (68.359)\n",
      "Epoch: [70][18/48]\tTime 0.018 (0.022)\tLoss 0.9853 (0.9327)\tPrec@1 65.430 (67.779)\n",
      "Epoch: [70][27/48]\tTime 0.021 (0.022)\tLoss 1.0006 (0.9422)\tPrec@1 64.062 (67.275)\n",
      "Epoch: [70][36/48]\tTime 0.015 (0.022)\tLoss 1.0196 (0.9516)\tPrec@1 63.477 (66.831)\n",
      "Epoch: [70][45/48]\tTime 0.035 (0.022)\tLoss 0.9783 (0.9541)\tPrec@1 65.332 (66.744)\n",
      "Epoch: [70][48/48]\tTime 0.023 (0.022)\tLoss 0.9804 (0.9568)\tPrec@1 65.920 (66.646)\n",
      "EPOCH: 70 train Results: Prec@1 66.646 Loss: 0.9568\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2203 (1.2203)\tPrec@1 57.227 (57.227)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.2741 (1.2466)\tPrec@1 53.571 (55.220)\n",
      "EPOCH: 70 val Results: Prec@1 55.220 Loss: 1.2466\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [71][0/48]\tTime 0.028 (0.028)\tLoss 0.8969 (0.8969)\tPrec@1 68.750 (68.750)\n",
      "Epoch: [71][9/48]\tTime 0.026 (0.023)\tLoss 0.9268 (0.9230)\tPrec@1 68.652 (68.438)\n",
      "Epoch: [71][18/48]\tTime 0.020 (0.022)\tLoss 0.9363 (0.9307)\tPrec@1 65.039 (67.917)\n",
      "Epoch: [71][27/48]\tTime 0.029 (0.023)\tLoss 0.9718 (0.9405)\tPrec@1 66.016 (67.369)\n",
      "Epoch: [71][36/48]\tTime 0.033 (0.023)\tLoss 0.9430 (0.9439)\tPrec@1 68.359 (67.224)\n",
      "Epoch: [71][45/48]\tTime 0.023 (0.023)\tLoss 0.9768 (0.9491)\tPrec@1 66.699 (66.950)\n",
      "Epoch: [71][48/48]\tTime 0.054 (0.024)\tLoss 1.0782 (0.9526)\tPrec@1 61.675 (66.756)\n",
      "EPOCH: 71 train Results: Prec@1 66.756 Loss: 0.9526\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2060 (1.2060)\tPrec@1 56.934 (56.934)\n",
      "Test: [9/9]\tTime 0.004 (0.011)\tLoss 1.2841 (1.2370)\tPrec@1 54.464 (55.920)\n",
      "EPOCH: 71 val Results: Prec@1 55.920 Loss: 1.2370\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [72][0/48]\tTime 0.023 (0.023)\tLoss 0.8988 (0.8988)\tPrec@1 68.848 (68.848)\n",
      "Epoch: [72][9/48]\tTime 0.022 (0.024)\tLoss 0.9247 (0.9181)\tPrec@1 66.309 (67.773)\n",
      "Epoch: [72][18/48]\tTime 0.020 (0.022)\tLoss 0.8932 (0.9264)\tPrec@1 69.434 (67.666)\n",
      "Epoch: [72][27/48]\tTime 0.017 (0.022)\tLoss 0.9353 (0.9328)\tPrec@1 67.480 (67.442)\n",
      "Epoch: [72][36/48]\tTime 0.026 (0.021)\tLoss 0.9629 (0.9439)\tPrec@1 65.430 (66.773)\n",
      "Epoch: [72][45/48]\tTime 0.018 (0.021)\tLoss 0.9334 (0.9503)\tPrec@1 67.188 (66.487)\n",
      "Epoch: [72][48/48]\tTime 0.020 (0.021)\tLoss 1.0030 (0.9532)\tPrec@1 63.561 (66.384)\n",
      "EPOCH: 72 train Results: Prec@1 66.384 Loss: 0.9532\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2126 (1.2126)\tPrec@1 56.543 (56.543)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2632 (1.2347)\tPrec@1 54.209 (55.930)\n",
      "EPOCH: 72 val Results: Prec@1 55.930 Loss: 1.2347\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [73][0/48]\tTime 0.022 (0.022)\tLoss 0.9680 (0.9680)\tPrec@1 66.211 (66.211)\n",
      "Epoch: [73][9/48]\tTime 0.021 (0.025)\tLoss 0.9306 (0.9373)\tPrec@1 67.871 (67.451)\n",
      "Epoch: [73][18/48]\tTime 0.019 (0.026)\tLoss 0.9456 (0.9299)\tPrec@1 66.016 (67.403)\n",
      "Epoch: [73][27/48]\tTime 0.032 (0.030)\tLoss 0.9093 (0.9359)\tPrec@1 67.676 (67.222)\n",
      "Epoch: [73][36/48]\tTime 0.026 (0.033)\tLoss 1.0132 (0.9445)\tPrec@1 62.891 (66.815)\n",
      "Epoch: [73][45/48]\tTime 0.054 (0.035)\tLoss 1.0155 (0.9492)\tPrec@1 61.328 (66.580)\n",
      "Epoch: [73][48/48]\tTime 0.018 (0.034)\tLoss 1.0387 (0.9509)\tPrec@1 63.679 (66.556)\n",
      "EPOCH: 73 train Results: Prec@1 66.556 Loss: 0.9509\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2095 (1.2095)\tPrec@1 56.055 (56.055)\n",
      "Test: [9/9]\tTime 0.005 (0.009)\tLoss 1.2766 (1.2343)\tPrec@1 54.209 (56.010)\n",
      "EPOCH: 73 val Results: Prec@1 56.010 Loss: 1.2343\n",
      "Best Prec@1: 56.030\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [74][0/48]\tTime 0.027 (0.027)\tLoss 0.9155 (0.9155)\tPrec@1 68.164 (68.164)\n",
      "Epoch: [74][9/48]\tTime 0.026 (0.038)\tLoss 0.9092 (0.9292)\tPrec@1 67.188 (67.480)\n",
      "Epoch: [74][18/48]\tTime 0.028 (0.035)\tLoss 0.9422 (0.9299)\tPrec@1 66.211 (67.419)\n",
      "Epoch: [74][27/48]\tTime 0.032 (0.031)\tLoss 1.0234 (0.9375)\tPrec@1 64.355 (67.146)\n",
      "Epoch: [74][36/48]\tTime 0.042 (0.032)\tLoss 1.0390 (0.9398)\tPrec@1 62.207 (66.974)\n",
      "Epoch: [74][45/48]\tTime 0.057 (0.035)\tLoss 0.9796 (0.9462)\tPrec@1 66.211 (66.731)\n",
      "Epoch: [74][48/48]\tTime 0.023 (0.035)\tLoss 0.9910 (0.9483)\tPrec@1 65.684 (66.610)\n",
      "EPOCH: 74 train Results: Prec@1 66.610 Loss: 0.9483\n",
      "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1968 (1.1968)\tPrec@1 57.227 (57.227)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.2774 (1.2335)\tPrec@1 54.847 (56.230)\n",
      "EPOCH: 74 val Results: Prec@1 56.230 Loss: 1.2335\n",
      "Best Prec@1: 56.230\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [75][0/48]\tTime 0.029 (0.029)\tLoss 0.8541 (0.8541)\tPrec@1 69.727 (69.727)\n",
      "Epoch: [75][9/48]\tTime 0.029 (0.031)\tLoss 0.9268 (0.8969)\tPrec@1 68.262 (68.623)\n",
      "Epoch: [75][18/48]\tTime 0.023 (0.032)\tLoss 0.9248 (0.9161)\tPrec@1 67.383 (67.979)\n",
      "Epoch: [75][27/48]\tTime 0.022 (0.029)\tLoss 0.9648 (0.9224)\tPrec@1 65.625 (67.728)\n",
      "Epoch: [75][36/48]\tTime 0.023 (0.027)\tLoss 0.9119 (0.9284)\tPrec@1 68.555 (67.544)\n",
      "Epoch: [75][45/48]\tTime 0.029 (0.026)\tLoss 0.9841 (0.9381)\tPrec@1 66.113 (67.145)\n",
      "Epoch: [75][48/48]\tTime 0.022 (0.026)\tLoss 0.9526 (0.9406)\tPrec@1 66.745 (67.034)\n",
      "EPOCH: 75 train Results: Prec@1 67.034 Loss: 0.9406\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2113 (1.2113)\tPrec@1 57.031 (57.031)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2612 (1.2349)\tPrec@1 54.847 (55.940)\n",
      "EPOCH: 75 val Results: Prec@1 55.940 Loss: 1.2349\n",
      "Best Prec@1: 56.230\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [76][0/48]\tTime 0.025 (0.025)\tLoss 0.9704 (0.9704)\tPrec@1 65.820 (65.820)\n",
      "Epoch: [76][9/48]\tTime 0.022 (0.024)\tLoss 0.9644 (0.9198)\tPrec@1 66.895 (67.979)\n",
      "Epoch: [76][18/48]\tTime 0.020 (0.022)\tLoss 0.9653 (0.9247)\tPrec@1 65.430 (67.527)\n",
      "Epoch: [76][27/48]\tTime 0.014 (0.022)\tLoss 0.9235 (0.9297)\tPrec@1 68.164 (67.400)\n",
      "Epoch: [76][36/48]\tTime 0.019 (0.022)\tLoss 0.9400 (0.9312)\tPrec@1 65.820 (67.246)\n",
      "Epoch: [76][45/48]\tTime 0.016 (0.022)\tLoss 0.9509 (0.9392)\tPrec@1 65.723 (66.850)\n",
      "Epoch: [76][48/48]\tTime 0.014 (0.022)\tLoss 0.9086 (0.9398)\tPrec@1 67.571 (66.812)\n",
      "EPOCH: 76 train Results: Prec@1 66.812 Loss: 0.9398\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2137 (1.2137)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.009 (0.006)\tLoss 1.2637 (1.2431)\tPrec@1 55.102 (55.910)\n",
      "EPOCH: 76 val Results: Prec@1 55.910 Loss: 1.2431\n",
      "Best Prec@1: 56.230\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [77][0/48]\tTime 0.027 (0.027)\tLoss 0.8900 (0.8900)\tPrec@1 69.238 (69.238)\n",
      "Epoch: [77][9/48]\tTime 0.024 (0.022)\tLoss 0.9126 (0.9037)\tPrec@1 68.457 (68.457)\n",
      "Epoch: [77][18/48]\tTime 0.014 (0.021)\tLoss 0.8571 (0.9083)\tPrec@1 69.434 (68.036)\n",
      "Epoch: [77][27/48]\tTime 0.017 (0.022)\tLoss 1.0051 (0.9163)\tPrec@1 64.160 (67.749)\n",
      "Epoch: [77][36/48]\tTime 0.017 (0.021)\tLoss 0.9393 (0.9229)\tPrec@1 67.480 (67.647)\n",
      "Epoch: [77][45/48]\tTime 0.019 (0.022)\tLoss 0.9229 (0.9308)\tPrec@1 68.164 (67.287)\n",
      "Epoch: [77][48/48]\tTime 0.016 (0.022)\tLoss 0.9839 (0.9330)\tPrec@1 68.042 (67.310)\n",
      "EPOCH: 77 train Results: Prec@1 67.310 Loss: 0.9330\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2042 (1.2042)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.2530 (1.2334)\tPrec@1 54.974 (56.210)\n",
      "EPOCH: 77 val Results: Prec@1 56.210 Loss: 1.2334\n",
      "Best Prec@1: 56.230\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [78][0/48]\tTime 0.025 (0.025)\tLoss 0.9135 (0.9135)\tPrec@1 68.555 (68.555)\n",
      "Epoch: [78][9/48]\tTime 0.016 (0.019)\tLoss 0.9039 (0.8975)\tPrec@1 68.652 (68.281)\n",
      "Epoch: [78][18/48]\tTime 0.021 (0.020)\tLoss 0.9086 (0.9070)\tPrec@1 67.188 (68.092)\n",
      "Epoch: [78][27/48]\tTime 0.021 (0.021)\tLoss 0.9411 (0.9196)\tPrec@1 66.992 (67.833)\n",
      "Epoch: [78][36/48]\tTime 0.014 (0.020)\tLoss 0.9609 (0.9283)\tPrec@1 65.527 (67.322)\n",
      "Epoch: [78][45/48]\tTime 0.022 (0.021)\tLoss 0.9736 (0.9344)\tPrec@1 66.309 (67.103)\n",
      "Epoch: [78][48/48]\tTime 0.013 (0.021)\tLoss 0.9667 (0.9345)\tPrec@1 65.920 (67.064)\n",
      "EPOCH: 78 train Results: Prec@1 67.064 Loss: 0.9345\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2094 (1.2094)\tPrec@1 58.594 (58.594)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2578 (1.2317)\tPrec@1 54.209 (56.250)\n",
      "EPOCH: 78 val Results: Prec@1 56.250 Loss: 1.2317\n",
      "Best Prec@1: 56.250\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [79][0/48]\tTime 0.021 (0.021)\tLoss 0.8213 (0.8213)\tPrec@1 72.363 (72.363)\n",
      "Epoch: [79][9/48]\tTime 0.018 (0.022)\tLoss 0.8908 (0.8718)\tPrec@1 69.434 (70.029)\n",
      "Epoch: [79][18/48]\tTime 0.021 (0.021)\tLoss 0.9103 (0.8872)\tPrec@1 67.871 (69.454)\n",
      "Epoch: [79][27/48]\tTime 0.020 (0.022)\tLoss 0.9393 (0.9017)\tPrec@1 66.797 (68.572)\n",
      "Epoch: [79][36/48]\tTime 0.025 (0.023)\tLoss 0.9353 (0.9160)\tPrec@1 66.504 (68.040)\n",
      "Epoch: [79][45/48]\tTime 0.015 (0.022)\tLoss 0.9730 (0.9264)\tPrec@1 65.039 (67.608)\n",
      "Epoch: [79][48/48]\tTime 0.026 (0.022)\tLoss 0.9960 (0.9318)\tPrec@1 62.972 (67.328)\n",
      "EPOCH: 79 train Results: Prec@1 67.328 Loss: 0.9318\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2072 (1.2072)\tPrec@1 57.812 (57.812)\n",
      "Test: [9/9]\tTime 0.006 (0.007)\tLoss 1.2772 (1.2325)\tPrec@1 53.571 (56.090)\n",
      "EPOCH: 79 val Results: Prec@1 56.090 Loss: 1.2325\n",
      "Best Prec@1: 56.250\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [80][0/48]\tTime 0.027 (0.027)\tLoss 0.9217 (0.9217)\tPrec@1 68.262 (68.262)\n",
      "Epoch: [80][9/48]\tTime 0.021 (0.024)\tLoss 0.9195 (0.8856)\tPrec@1 68.359 (69.307)\n",
      "Epoch: [80][18/48]\tTime 0.030 (0.024)\tLoss 0.9226 (0.9002)\tPrec@1 66.113 (68.524)\n",
      "Epoch: [80][27/48]\tTime 0.028 (0.023)\tLoss 0.9640 (0.9116)\tPrec@1 66.309 (67.948)\n",
      "Epoch: [80][36/48]\tTime 0.021 (0.023)\tLoss 0.9376 (0.9224)\tPrec@1 65.918 (67.557)\n",
      "Epoch: [80][45/48]\tTime 0.017 (0.023)\tLoss 0.9462 (0.9249)\tPrec@1 67.480 (67.523)\n",
      "Epoch: [80][48/48]\tTime 0.023 (0.023)\tLoss 0.9765 (0.9267)\tPrec@1 65.802 (67.460)\n",
      "EPOCH: 80 train Results: Prec@1 67.460 Loss: 0.9267\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2082 (1.2082)\tPrec@1 57.422 (57.422)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2648 (1.2390)\tPrec@1 55.357 (56.070)\n",
      "EPOCH: 80 val Results: Prec@1 56.070 Loss: 1.2390\n",
      "Best Prec@1: 56.250\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [81][0/48]\tTime 0.023 (0.023)\tLoss 0.9063 (0.9063)\tPrec@1 67.773 (67.773)\n",
      "Epoch: [81][9/48]\tTime 0.039 (0.037)\tLoss 0.8994 (0.8892)\tPrec@1 68.066 (68.574)\n",
      "Epoch: [81][18/48]\tTime 0.041 (0.032)\tLoss 0.9142 (0.9016)\tPrec@1 67.871 (68.138)\n",
      "Epoch: [81][27/48]\tTime 0.029 (0.029)\tLoss 0.9829 (0.9050)\tPrec@1 64.453 (68.161)\n",
      "Epoch: [81][36/48]\tTime 0.020 (0.030)\tLoss 0.8993 (0.9118)\tPrec@1 67.773 (67.998)\n",
      "Epoch: [81][45/48]\tTime 0.019 (0.029)\tLoss 0.9732 (0.9229)\tPrec@1 64.648 (67.504)\n",
      "Epoch: [81][48/48]\tTime 0.018 (0.028)\tLoss 0.9738 (0.9254)\tPrec@1 63.915 (67.388)\n",
      "EPOCH: 81 train Results: Prec@1 67.388 Loss: 0.9254\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2225 (1.2225)\tPrec@1 57.129 (57.129)\n",
      "Test: [9/9]\tTime 0.008 (0.007)\tLoss 1.2560 (1.2296)\tPrec@1 55.867 (56.630)\n",
      "EPOCH: 81 val Results: Prec@1 56.630 Loss: 1.2296\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [82][0/48]\tTime 0.022 (0.022)\tLoss 0.8731 (0.8731)\tPrec@1 69.922 (69.922)\n",
      "Epoch: [82][9/48]\tTime 0.021 (0.021)\tLoss 0.9166 (0.8946)\tPrec@1 67.090 (69.219)\n",
      "Epoch: [82][18/48]\tTime 0.018 (0.020)\tLoss 0.9498 (0.8947)\tPrec@1 65.234 (68.986)\n",
      "Epoch: [82][27/48]\tTime 0.016 (0.020)\tLoss 0.9288 (0.9033)\tPrec@1 68.457 (68.597)\n",
      "Epoch: [82][36/48]\tTime 0.023 (0.021)\tLoss 0.8998 (0.9101)\tPrec@1 67.188 (68.399)\n",
      "Epoch: [82][45/48]\tTime 0.019 (0.021)\tLoss 0.9736 (0.9189)\tPrec@1 64.160 (67.950)\n",
      "Epoch: [82][48/48]\tTime 0.015 (0.022)\tLoss 0.9731 (0.9237)\tPrec@1 67.099 (67.780)\n",
      "EPOCH: 82 train Results: Prec@1 67.780 Loss: 0.9237\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2046 (1.2046)\tPrec@1 57.324 (57.324)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2580 (1.2387)\tPrec@1 55.230 (56.230)\n",
      "EPOCH: 82 val Results: Prec@1 56.230 Loss: 1.2387\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [83][0/48]\tTime 0.017 (0.017)\tLoss 0.8970 (0.8970)\tPrec@1 68.555 (68.555)\n",
      "Epoch: [83][9/48]\tTime 0.019 (0.023)\tLoss 0.9482 (0.8871)\tPrec@1 66.309 (69.189)\n",
      "Epoch: [83][18/48]\tTime 0.025 (0.024)\tLoss 0.8944 (0.8985)\tPrec@1 68.945 (68.493)\n",
      "Epoch: [83][27/48]\tTime 0.018 (0.023)\tLoss 0.9548 (0.9053)\tPrec@1 65.625 (68.244)\n",
      "Epoch: [83][36/48]\tTime 0.029 (0.025)\tLoss 0.9436 (0.9132)\tPrec@1 66.406 (67.927)\n",
      "Epoch: [83][45/48]\tTime 0.022 (0.027)\tLoss 0.8418 (0.9163)\tPrec@1 69.824 (67.765)\n",
      "Epoch: [83][48/48]\tTime 0.019 (0.027)\tLoss 0.9267 (0.9195)\tPrec@1 66.509 (67.612)\n",
      "EPOCH: 83 train Results: Prec@1 67.612 Loss: 0.9195\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1977 (1.1977)\tPrec@1 57.910 (57.910)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2656 (1.2353)\tPrec@1 53.827 (56.150)\n",
      "EPOCH: 83 val Results: Prec@1 56.150 Loss: 1.2353\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [84][0/48]\tTime 0.024 (0.024)\tLoss 0.8333 (0.8333)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [84][9/48]\tTime 0.030 (0.024)\tLoss 0.9174 (0.8891)\tPrec@1 66.602 (68.779)\n",
      "Epoch: [84][18/48]\tTime 0.027 (0.027)\tLoss 0.8943 (0.8910)\tPrec@1 68.457 (68.796)\n",
      "Epoch: [84][27/48]\tTime 0.030 (0.026)\tLoss 0.9288 (0.8997)\tPrec@1 67.090 (68.380)\n",
      "Epoch: [84][36/48]\tTime 0.016 (0.026)\tLoss 0.9220 (0.9057)\tPrec@1 67.480 (68.032)\n",
      "Epoch: [84][45/48]\tTime 0.024 (0.025)\tLoss 0.9704 (0.9111)\tPrec@1 66.309 (67.914)\n",
      "Epoch: [84][48/48]\tTime 0.019 (0.025)\tLoss 0.9885 (0.9130)\tPrec@1 64.623 (67.876)\n",
      "EPOCH: 84 train Results: Prec@1 67.876 Loss: 0.9130\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2031 (1.2031)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2822 (1.2372)\tPrec@1 54.209 (56.430)\n",
      "EPOCH: 84 val Results: Prec@1 56.430 Loss: 1.2372\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [85][0/48]\tTime 0.023 (0.023)\tLoss 0.8378 (0.8378)\tPrec@1 70.215 (70.215)\n",
      "Epoch: [85][9/48]\tTime 0.026 (0.024)\tLoss 0.8856 (0.8696)\tPrec@1 69.336 (69.590)\n",
      "Epoch: [85][18/48]\tTime 0.022 (0.023)\tLoss 0.8568 (0.8830)\tPrec@1 69.531 (69.079)\n",
      "Epoch: [85][27/48]\tTime 0.025 (0.023)\tLoss 0.8812 (0.8909)\tPrec@1 68.066 (68.659)\n",
      "Epoch: [85][36/48]\tTime 0.021 (0.023)\tLoss 0.9571 (0.9011)\tPrec@1 66.309 (68.336)\n",
      "Epoch: [85][45/48]\tTime 0.023 (0.024)\tLoss 0.9037 (0.9083)\tPrec@1 67.285 (68.013)\n",
      "Epoch: [85][48/48]\tTime 0.021 (0.024)\tLoss 0.9557 (0.9125)\tPrec@1 67.335 (67.872)\n",
      "EPOCH: 85 train Results: Prec@1 67.872 Loss: 0.9125\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.1993 (1.1993)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.2686 (1.2284)\tPrec@1 53.954 (56.520)\n",
      "EPOCH: 85 val Results: Prec@1 56.520 Loss: 1.2284\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [86][0/48]\tTime 0.024 (0.024)\tLoss 0.8886 (0.8886)\tPrec@1 70.312 (70.312)\n",
      "Epoch: [86][9/48]\tTime 0.026 (0.024)\tLoss 0.8605 (0.8901)\tPrec@1 70.020 (69.043)\n",
      "Epoch: [86][18/48]\tTime 0.021 (0.023)\tLoss 0.8969 (0.8850)\tPrec@1 68.066 (69.213)\n",
      "Epoch: [86][27/48]\tTime 0.020 (0.024)\tLoss 0.8987 (0.8912)\tPrec@1 69.922 (68.987)\n",
      "Epoch: [86][36/48]\tTime 0.024 (0.026)\tLoss 0.8813 (0.9018)\tPrec@1 68.750 (68.433)\n",
      "Epoch: [86][45/48]\tTime 0.027 (0.027)\tLoss 1.0171 (0.9086)\tPrec@1 64.746 (68.107)\n",
      "Epoch: [86][48/48]\tTime 0.020 (0.027)\tLoss 0.9378 (0.9106)\tPrec@1 67.453 (68.032)\n",
      "EPOCH: 86 train Results: Prec@1 68.032 Loss: 0.9106\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.1979 (1.1979)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.011 (0.006)\tLoss 1.2778 (1.2348)\tPrec@1 53.954 (56.220)\n",
      "EPOCH: 86 val Results: Prec@1 56.220 Loss: 1.2348\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [87][0/48]\tTime 0.024 (0.024)\tLoss 0.8486 (0.8486)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [87][9/48]\tTime 0.024 (0.023)\tLoss 0.9000 (0.8684)\tPrec@1 68.848 (70.068)\n",
      "Epoch: [87][18/48]\tTime 0.022 (0.022)\tLoss 0.8851 (0.8851)\tPrec@1 69.141 (69.418)\n",
      "Epoch: [87][27/48]\tTime 0.030 (0.022)\tLoss 0.9210 (0.8902)\tPrec@1 65.820 (68.966)\n",
      "Epoch: [87][36/48]\tTime 0.021 (0.023)\tLoss 0.9500 (0.8966)\tPrec@1 67.480 (68.811)\n",
      "Epoch: [87][45/48]\tTime 0.027 (0.023)\tLoss 1.0004 (0.9066)\tPrec@1 64.160 (68.385)\n",
      "Epoch: [87][48/48]\tTime 0.027 (0.023)\tLoss 0.9108 (0.9065)\tPrec@1 67.925 (68.376)\n",
      "EPOCH: 87 train Results: Prec@1 68.376 Loss: 0.9065\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2046 (1.2046)\tPrec@1 59.668 (59.668)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2625 (1.2342)\tPrec@1 54.464 (56.580)\n",
      "EPOCH: 87 val Results: Prec@1 56.580 Loss: 1.2342\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [88][0/48]\tTime 0.016 (0.016)\tLoss 0.7979 (0.7979)\tPrec@1 73.340 (73.340)\n",
      "Epoch: [88][9/48]\tTime 0.019 (0.023)\tLoss 0.8947 (0.8474)\tPrec@1 68.555 (70.703)\n",
      "Epoch: [88][18/48]\tTime 0.036 (0.026)\tLoss 0.9184 (0.8703)\tPrec@1 67.871 (69.516)\n",
      "Epoch: [88][27/48]\tTime 0.027 (0.028)\tLoss 0.9120 (0.8832)\tPrec@1 67.480 (69.060)\n",
      "Epoch: [88][36/48]\tTime 0.021 (0.031)\tLoss 0.9971 (0.8966)\tPrec@1 64.258 (68.454)\n",
      "Epoch: [88][45/48]\tTime 0.045 (0.033)\tLoss 0.9612 (0.9048)\tPrec@1 66.797 (68.151)\n",
      "Epoch: [88][48/48]\tTime 0.040 (0.033)\tLoss 0.9656 (0.9082)\tPrec@1 65.566 (68.004)\n",
      "EPOCH: 88 train Results: Prec@1 68.004 Loss: 0.9082\n",
      "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.1948 (1.1948)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.013 (0.009)\tLoss 1.2725 (1.2331)\tPrec@1 53.954 (56.430)\n",
      "EPOCH: 88 val Results: Prec@1 56.430 Loss: 1.2331\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [89][0/48]\tTime 0.038 (0.038)\tLoss 0.8536 (0.8536)\tPrec@1 70.996 (70.996)\n",
      "Epoch: [89][9/48]\tTime 0.031 (0.028)\tLoss 0.8744 (0.8712)\tPrec@1 68.164 (69.854)\n",
      "Epoch: [89][18/48]\tTime 0.031 (0.028)\tLoss 0.9605 (0.8738)\tPrec@1 66.602 (69.454)\n",
      "Epoch: [89][27/48]\tTime 0.027 (0.029)\tLoss 0.8923 (0.8819)\tPrec@1 68.066 (68.938)\n",
      "Epoch: [89][36/48]\tTime 0.029 (0.030)\tLoss 0.9627 (0.8898)\tPrec@1 65.625 (68.631)\n",
      "Epoch: [89][45/48]\tTime 0.027 (0.030)\tLoss 0.9857 (0.9017)\tPrec@1 65.234 (68.187)\n",
      "Epoch: [89][48/48]\tTime 0.372 (0.037)\tLoss 0.9469 (0.9039)\tPrec@1 68.160 (68.152)\n",
      "EPOCH: 89 train Results: Prec@1 68.152 Loss: 0.9039\n",
      "Test: [0/9]\tTime 0.090 (0.090)\tLoss 1.2010 (1.2010)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.022 (0.023)\tLoss 1.2756 (1.2324)\tPrec@1 54.337 (55.900)\n",
      "EPOCH: 89 val Results: Prec@1 55.900 Loss: 1.2324\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [90][0/48]\tTime 0.111 (0.111)\tLoss 0.8307 (0.8307)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [90][9/48]\tTime 0.020 (0.089)\tLoss 0.9101 (0.8669)\tPrec@1 68.652 (69.766)\n",
      "Epoch: [90][18/48]\tTime 0.024 (0.066)\tLoss 0.9074 (0.8751)\tPrec@1 68.555 (69.372)\n",
      "Epoch: [90][27/48]\tTime 0.098 (0.057)\tLoss 0.9266 (0.8854)\tPrec@1 67.578 (68.966)\n",
      "Epoch: [90][36/48]\tTime 0.109 (0.055)\tLoss 0.9335 (0.8921)\tPrec@1 67.578 (68.660)\n",
      "Epoch: [90][45/48]\tTime 0.100 (0.056)\tLoss 0.9436 (0.8974)\tPrec@1 66.895 (68.495)\n",
      "Epoch: [90][48/48]\tTime 0.034 (0.054)\tLoss 0.9323 (0.9002)\tPrec@1 65.330 (68.366)\n",
      "EPOCH: 90 train Results: Prec@1 68.366 Loss: 0.9002\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2138 (1.2138)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.2656 (1.2271)\tPrec@1 55.995 (56.430)\n",
      "EPOCH: 90 val Results: Prec@1 56.430 Loss: 1.2271\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [91][0/48]\tTime 0.031 (0.031)\tLoss 0.8465 (0.8465)\tPrec@1 71.191 (71.191)\n",
      "Epoch: [91][9/48]\tTime 0.028 (0.034)\tLoss 0.8763 (0.8582)\tPrec@1 67.969 (69.902)\n",
      "Epoch: [91][18/48]\tTime 0.066 (0.038)\tLoss 0.8850 (0.8683)\tPrec@1 68.750 (69.387)\n",
      "Epoch: [91][27/48]\tTime 0.032 (0.046)\tLoss 0.9430 (0.8730)\tPrec@1 66.309 (69.196)\n",
      "Epoch: [91][36/48]\tTime 0.032 (0.043)\tLoss 0.9135 (0.8849)\tPrec@1 67.090 (68.753)\n",
      "Epoch: [91][45/48]\tTime 0.028 (0.039)\tLoss 0.9244 (0.8959)\tPrec@1 66.699 (68.202)\n",
      "Epoch: [91][48/48]\tTime 0.015 (0.038)\tLoss 0.9050 (0.8978)\tPrec@1 68.632 (68.170)\n",
      "EPOCH: 91 train Results: Prec@1 68.170 Loss: 0.8978\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2241 (1.2241)\tPrec@1 57.715 (57.715)\n",
      "Test: [9/9]\tTime 0.004 (0.007)\tLoss 1.2760 (1.2314)\tPrec@1 54.464 (56.430)\n",
      "EPOCH: 91 val Results: Prec@1 56.430 Loss: 1.2314\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [92][0/48]\tTime 0.017 (0.017)\tLoss 0.8979 (0.8979)\tPrec@1 66.992 (66.992)\n",
      "Epoch: [92][9/48]\tTime 0.026 (0.072)\tLoss 0.9060 (0.8606)\tPrec@1 68.848 (69.961)\n",
      "Epoch: [92][18/48]\tTime 0.047 (0.054)\tLoss 0.8688 (0.8714)\tPrec@1 69.336 (69.459)\n",
      "Epoch: [92][27/48]\tTime 0.015 (0.048)\tLoss 0.8504 (0.8755)\tPrec@1 69.824 (69.189)\n",
      "Epoch: [92][36/48]\tTime 0.032 (0.045)\tLoss 0.8816 (0.8792)\tPrec@1 68.164 (69.077)\n",
      "Epoch: [92][45/48]\tTime 0.042 (0.042)\tLoss 0.9034 (0.8896)\tPrec@1 66.602 (68.625)\n",
      "Epoch: [92][48/48]\tTime 0.020 (0.041)\tLoss 0.9540 (0.8929)\tPrec@1 65.212 (68.488)\n",
      "EPOCH: 92 train Results: Prec@1 68.488 Loss: 0.8929\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2223 (1.2223)\tPrec@1 57.227 (57.227)\n",
      "Test: [9/9]\tTime 0.005 (0.009)\tLoss 1.2909 (1.2438)\tPrec@1 54.719 (56.510)\n",
      "EPOCH: 92 val Results: Prec@1 56.510 Loss: 1.2438\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [93][0/48]\tTime 0.027 (0.027)\tLoss 0.8702 (0.8702)\tPrec@1 69.043 (69.043)\n",
      "Epoch: [93][9/48]\tTime 0.020 (0.027)\tLoss 0.8088 (0.8351)\tPrec@1 71.094 (70.908)\n",
      "Epoch: [93][18/48]\tTime 0.033 (0.029)\tLoss 0.8766 (0.8567)\tPrec@1 69.727 (70.122)\n",
      "Epoch: [93][27/48]\tTime 0.035 (0.031)\tLoss 0.9228 (0.8675)\tPrec@1 67.676 (69.754)\n",
      "Epoch: [93][36/48]\tTime 0.040 (0.031)\tLoss 0.8888 (0.8785)\tPrec@1 68.164 (69.241)\n",
      "Epoch: [93][45/48]\tTime 0.044 (0.031)\tLoss 0.9156 (0.8895)\tPrec@1 68.359 (68.784)\n",
      "Epoch: [93][48/48]\tTime 0.015 (0.030)\tLoss 0.9584 (0.8914)\tPrec@1 67.453 (68.702)\n",
      "EPOCH: 93 train Results: Prec@1 68.702 Loss: 0.8914\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2113 (1.2113)\tPrec@1 57.227 (57.227)\n",
      "Test: [9/9]\tTime 0.011 (0.007)\tLoss 1.2656 (1.2336)\tPrec@1 55.357 (56.170)\n",
      "EPOCH: 93 val Results: Prec@1 56.170 Loss: 1.2336\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [94][0/48]\tTime 0.023 (0.023)\tLoss 0.8340 (0.8340)\tPrec@1 73.145 (73.145)\n",
      "Epoch: [94][9/48]\tTime 0.019 (0.027)\tLoss 0.8572 (0.8341)\tPrec@1 70.117 (71.602)\n",
      "Epoch: [94][18/48]\tTime 0.020 (0.029)\tLoss 0.8795 (0.8490)\tPrec@1 69.727 (70.826)\n",
      "Epoch: [94][27/48]\tTime 0.040 (0.030)\tLoss 0.8870 (0.8622)\tPrec@1 68.652 (70.082)\n",
      "Epoch: [94][36/48]\tTime 0.020 (0.029)\tLoss 0.9561 (0.8744)\tPrec@1 65.332 (69.555)\n",
      "Epoch: [94][45/48]\tTime 0.035 (0.029)\tLoss 0.9227 (0.8847)\tPrec@1 67.578 (69.124)\n",
      "Epoch: [94][48/48]\tTime 0.021 (0.028)\tLoss 0.9688 (0.8874)\tPrec@1 64.976 (69.036)\n",
      "EPOCH: 94 train Results: Prec@1 69.036 Loss: 0.8874\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2320 (1.2320)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.003 (0.009)\tLoss 1.2814 (1.2395)\tPrec@1 54.592 (56.170)\n",
      "EPOCH: 94 val Results: Prec@1 56.170 Loss: 1.2395\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [95][0/48]\tTime 0.030 (0.030)\tLoss 0.8690 (0.8690)\tPrec@1 68.652 (68.652)\n",
      "Epoch: [95][9/48]\tTime 0.027 (0.036)\tLoss 0.8475 (0.8404)\tPrec@1 71.582 (70.605)\n",
      "Epoch: [95][18/48]\tTime 0.019 (0.030)\tLoss 0.8803 (0.8584)\tPrec@1 70.801 (70.210)\n",
      "Epoch: [95][27/48]\tTime 0.021 (0.029)\tLoss 0.8318 (0.8641)\tPrec@1 71.289 (69.922)\n",
      "Epoch: [95][36/48]\tTime 0.020 (0.029)\tLoss 0.9119 (0.8731)\tPrec@1 66.895 (69.420)\n",
      "Epoch: [95][45/48]\tTime 0.040 (0.029)\tLoss 0.9909 (0.8869)\tPrec@1 65.234 (68.924)\n",
      "Epoch: [95][48/48]\tTime 0.017 (0.029)\tLoss 0.9492 (0.8878)\tPrec@1 67.335 (68.930)\n",
      "EPOCH: 95 train Results: Prec@1 68.930 Loss: 0.8878\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2132 (1.2132)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.023 (0.008)\tLoss 1.2896 (1.2430)\tPrec@1 54.719 (56.000)\n",
      "EPOCH: 95 val Results: Prec@1 56.000 Loss: 1.2430\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [96][0/48]\tTime 0.019 (0.019)\tLoss 0.8192 (0.8192)\tPrec@1 72.559 (72.559)\n",
      "Epoch: [96][9/48]\tTime 0.024 (0.021)\tLoss 0.8805 (0.8351)\tPrec@1 69.531 (71.514)\n",
      "Epoch: [96][18/48]\tTime 0.033 (0.023)\tLoss 0.9133 (0.8535)\tPrec@1 67.188 (70.374)\n",
      "Epoch: [96][27/48]\tTime 0.029 (0.030)\tLoss 0.9128 (0.8649)\tPrec@1 68.164 (69.772)\n",
      "Epoch: [96][36/48]\tTime 0.021 (0.028)\tLoss 0.8921 (0.8711)\tPrec@1 69.238 (69.571)\n",
      "Epoch: [96][45/48]\tTime 0.025 (0.027)\tLoss 0.9188 (0.8796)\tPrec@1 68.750 (69.200)\n",
      "Epoch: [96][48/48]\tTime 0.012 (0.026)\tLoss 0.9370 (0.8836)\tPrec@1 66.627 (69.008)\n",
      "EPOCH: 96 train Results: Prec@1 69.008 Loss: 0.8836\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2129 (1.2129)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.006 (0.005)\tLoss 1.2736 (1.2399)\tPrec@1 53.189 (56.050)\n",
      "EPOCH: 96 val Results: Prec@1 56.050 Loss: 1.2399\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [97][0/48]\tTime 0.026 (0.026)\tLoss 0.8588 (0.8588)\tPrec@1 70.801 (70.801)\n",
      "Epoch: [97][9/48]\tTime 0.022 (0.023)\tLoss 0.8350 (0.8462)\tPrec@1 71.387 (70.479)\n",
      "Epoch: [97][18/48]\tTime 0.019 (0.022)\tLoss 0.8501 (0.8515)\tPrec@1 72.363 (70.143)\n",
      "Epoch: [97][27/48]\tTime 0.020 (0.022)\tLoss 0.9088 (0.8623)\tPrec@1 67.871 (69.800)\n",
      "Epoch: [97][36/48]\tTime 0.027 (0.022)\tLoss 0.8765 (0.8699)\tPrec@1 68.457 (69.457)\n",
      "Epoch: [97][45/48]\tTime 0.027 (0.022)\tLoss 0.9237 (0.8796)\tPrec@1 67.285 (69.128)\n",
      "Epoch: [97][48/48]\tTime 0.020 (0.022)\tLoss 0.9300 (0.8820)\tPrec@1 65.566 (69.020)\n",
      "EPOCH: 97 train Results: Prec@1 69.020 Loss: 0.8820\n",
      "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.2144 (1.2144)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.004 (0.007)\tLoss 1.2513 (1.2275)\tPrec@1 56.378 (56.510)\n",
      "EPOCH: 97 val Results: Prec@1 56.510 Loss: 1.2275\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [98][0/48]\tTime 0.017 (0.017)\tLoss 0.8746 (0.8746)\tPrec@1 69.043 (69.043)\n",
      "Epoch: [98][9/48]\tTime 0.018 (0.017)\tLoss 0.8090 (0.8481)\tPrec@1 73.047 (70.811)\n",
      "Epoch: [98][18/48]\tTime 0.037 (0.022)\tLoss 0.8436 (0.8567)\tPrec@1 71.094 (70.194)\n",
      "Epoch: [98][27/48]\tTime 0.033 (0.023)\tLoss 0.9101 (0.8662)\tPrec@1 67.480 (69.730)\n",
      "Epoch: [98][36/48]\tTime 0.020 (0.022)\tLoss 0.9607 (0.8736)\tPrec@1 66.016 (69.346)\n",
      "Epoch: [98][45/48]\tTime 0.020 (0.023)\tLoss 0.9085 (0.8825)\tPrec@1 67.676 (68.964)\n",
      "Epoch: [98][48/48]\tTime 0.014 (0.023)\tLoss 0.9090 (0.8847)\tPrec@1 68.750 (68.836)\n",
      "EPOCH: 98 train Results: Prec@1 68.836 Loss: 0.8847\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2248 (1.2248)\tPrec@1 57.520 (57.520)\n",
      "Test: [9/9]\tTime 0.003 (0.008)\tLoss 1.2832 (1.2505)\tPrec@1 54.464 (55.800)\n",
      "EPOCH: 98 val Results: Prec@1 55.800 Loss: 1.2505\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [99][0/48]\tTime 0.028 (0.028)\tLoss 0.8983 (0.8983)\tPrec@1 67.773 (67.773)\n",
      "Epoch: [99][9/48]\tTime 0.024 (0.031)\tLoss 0.8459 (0.8506)\tPrec@1 71.094 (70.752)\n",
      "Epoch: [99][18/48]\tTime 0.028 (0.029)\tLoss 0.8485 (0.8635)\tPrec@1 71.094 (69.886)\n",
      "Epoch: [99][27/48]\tTime 0.023 (0.031)\tLoss 0.8629 (0.8664)\tPrec@1 69.238 (69.629)\n",
      "Epoch: [99][36/48]\tTime 0.041 (0.030)\tLoss 0.9060 (0.8741)\tPrec@1 67.871 (69.257)\n",
      "Epoch: [99][45/48]\tTime 0.121 (0.031)\tLoss 0.8993 (0.8809)\tPrec@1 67.969 (68.896)\n",
      "Epoch: [99][48/48]\tTime 0.029 (0.031)\tLoss 0.9315 (0.8830)\tPrec@1 67.217 (68.790)\n",
      "EPOCH: 99 train Results: Prec@1 68.790 Loss: 0.8830\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.2022 (1.2022)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2701 (1.2412)\tPrec@1 54.337 (56.420)\n",
      "EPOCH: 99 val Results: Prec@1 56.420 Loss: 1.2412\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [100][0/48]\tTime 0.028 (0.028)\tLoss 0.8793 (0.8793)\tPrec@1 69.141 (69.141)\n",
      "Epoch: [100][9/48]\tTime 0.033 (0.034)\tLoss 0.8731 (0.8471)\tPrec@1 68.066 (70.479)\n",
      "Epoch: [100][18/48]\tTime 0.037 (0.030)\tLoss 0.8755 (0.8539)\tPrec@1 67.383 (70.148)\n",
      "Epoch: [100][27/48]\tTime 0.032 (0.030)\tLoss 0.8717 (0.8632)\tPrec@1 67.773 (69.859)\n",
      "Epoch: [100][36/48]\tTime 0.048 (0.029)\tLoss 0.8955 (0.8744)\tPrec@1 68.945 (69.486)\n",
      "Epoch: [100][45/48]\tTime 0.021 (0.028)\tLoss 0.8860 (0.8813)\tPrec@1 68.164 (69.151)\n",
      "Epoch: [100][48/48]\tTime 0.015 (0.027)\tLoss 0.9748 (0.8829)\tPrec@1 66.274 (69.106)\n",
      "EPOCH: 100 train Results: Prec@1 69.106 Loss: 0.8829\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2326 (1.2326)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2738 (1.2426)\tPrec@1 54.719 (56.140)\n",
      "EPOCH: 100 val Results: Prec@1 56.140 Loss: 1.2426\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [101][0/48]\tTime 0.021 (0.021)\tLoss 0.8366 (0.8366)\tPrec@1 69.434 (69.434)\n",
      "Epoch: [101][9/48]\tTime 0.024 (0.023)\tLoss 0.8433 (0.8445)\tPrec@1 70.996 (70.273)\n",
      "Epoch: [101][18/48]\tTime 0.018 (0.023)\tLoss 0.8440 (0.8458)\tPrec@1 71.680 (70.302)\n",
      "Epoch: [101][27/48]\tTime 0.021 (0.023)\tLoss 0.8666 (0.8574)\tPrec@1 69.922 (69.950)\n",
      "Epoch: [101][36/48]\tTime 0.030 (0.023)\tLoss 0.8550 (0.8582)\tPrec@1 69.922 (69.840)\n",
      "Epoch: [101][45/48]\tTime 0.018 (0.023)\tLoss 0.9127 (0.8679)\tPrec@1 68.164 (69.480)\n",
      "Epoch: [101][48/48]\tTime 0.018 (0.023)\tLoss 0.9004 (0.8695)\tPrec@1 68.986 (69.370)\n",
      "EPOCH: 101 train Results: Prec@1 69.370 Loss: 0.8695\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2277 (1.2277)\tPrec@1 56.641 (56.641)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.2901 (1.2458)\tPrec@1 55.485 (55.750)\n",
      "EPOCH: 101 val Results: Prec@1 55.750 Loss: 1.2458\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [102][0/48]\tTime 0.032 (0.032)\tLoss 0.8263 (0.8263)\tPrec@1 70.996 (70.996)\n",
      "Epoch: [102][9/48]\tTime 0.022 (0.029)\tLoss 0.8425 (0.8219)\tPrec@1 70.215 (71.699)\n",
      "Epoch: [102][18/48]\tTime 0.032 (0.029)\tLoss 0.8909 (0.8408)\tPrec@1 70.410 (70.780)\n",
      "Epoch: [102][27/48]\tTime 0.027 (0.026)\tLoss 0.9048 (0.8513)\tPrec@1 67.676 (70.295)\n",
      "Epoch: [102][36/48]\tTime 0.020 (0.025)\tLoss 0.8990 (0.8603)\tPrec@1 68.262 (69.914)\n",
      "Epoch: [102][45/48]\tTime 0.020 (0.024)\tLoss 0.8736 (0.8674)\tPrec@1 68.262 (69.580)\n",
      "Epoch: [102][48/48]\tTime 0.022 (0.024)\tLoss 0.9380 (0.8713)\tPrec@1 66.392 (69.448)\n",
      "EPOCH: 102 train Results: Prec@1 69.448 Loss: 0.8713\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.2222 (1.2222)\tPrec@1 59.082 (59.082)\n",
      "Test: [9/9]\tTime 0.010 (0.008)\tLoss 1.2732 (1.2376)\tPrec@1 54.337 (56.500)\n",
      "EPOCH: 102 val Results: Prec@1 56.500 Loss: 1.2376\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [103][0/48]\tTime 0.024 (0.024)\tLoss 0.8187 (0.8187)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [103][9/48]\tTime 0.020 (0.025)\tLoss 0.8465 (0.8340)\tPrec@1 71.094 (71.328)\n",
      "Epoch: [103][18/48]\tTime 0.030 (0.024)\tLoss 0.8673 (0.8358)\tPrec@1 68.262 (71.012)\n",
      "Epoch: [103][27/48]\tTime 0.023 (0.022)\tLoss 0.9031 (0.8466)\tPrec@1 66.895 (70.358)\n",
      "Epoch: [103][36/48]\tTime 0.027 (0.024)\tLoss 0.9088 (0.8605)\tPrec@1 66.406 (69.882)\n",
      "Epoch: [103][45/48]\tTime 0.023 (0.023)\tLoss 0.9170 (0.8700)\tPrec@1 67.285 (69.593)\n",
      "Epoch: [103][48/48]\tTime 0.014 (0.023)\tLoss 0.8891 (0.8721)\tPrec@1 67.925 (69.468)\n",
      "EPOCH: 103 train Results: Prec@1 69.468 Loss: 0.8721\n",
      "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.2301 (1.2301)\tPrec@1 55.957 (55.957)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.2742 (1.2415)\tPrec@1 54.082 (56.130)\n",
      "EPOCH: 103 val Results: Prec@1 56.130 Loss: 1.2415\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [104][0/48]\tTime 0.031 (0.031)\tLoss 0.8752 (0.8752)\tPrec@1 68.164 (68.164)\n",
      "Epoch: [104][9/48]\tTime 0.024 (0.025)\tLoss 0.8588 (0.8281)\tPrec@1 71.484 (71.084)\n",
      "Epoch: [104][18/48]\tTime 0.018 (0.028)\tLoss 0.8558 (0.8427)\tPrec@1 70.020 (70.585)\n",
      "Epoch: [104][27/48]\tTime 0.025 (0.027)\tLoss 0.9033 (0.8506)\tPrec@1 67.676 (70.204)\n",
      "Epoch: [104][36/48]\tTime 0.085 (0.027)\tLoss 0.9282 (0.8620)\tPrec@1 67.090 (69.687)\n",
      "Epoch: [104][45/48]\tTime 0.016 (0.026)\tLoss 0.9104 (0.8667)\tPrec@1 67.090 (69.453)\n",
      "Epoch: [104][48/48]\tTime 0.020 (0.026)\tLoss 0.9159 (0.8695)\tPrec@1 67.807 (69.358)\n",
      "EPOCH: 104 train Results: Prec@1 69.358 Loss: 0.8695\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2097 (1.2097)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.2761 (1.2367)\tPrec@1 54.464 (56.290)\n",
      "EPOCH: 104 val Results: Prec@1 56.290 Loss: 1.2367\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [105][0/48]\tTime 0.022 (0.022)\tLoss 0.8605 (0.8605)\tPrec@1 70.117 (70.117)\n",
      "Epoch: [105][9/48]\tTime 0.031 (0.023)\tLoss 0.8708 (0.8359)\tPrec@1 70.117 (71.055)\n",
      "Epoch: [105][18/48]\tTime 0.023 (0.023)\tLoss 0.8908 (0.8422)\tPrec@1 68.945 (70.842)\n",
      "Epoch: [105][27/48]\tTime 0.029 (0.023)\tLoss 0.8666 (0.8476)\tPrec@1 69.043 (70.546)\n",
      "Epoch: [105][36/48]\tTime 0.028 (0.023)\tLoss 0.8793 (0.8558)\tPrec@1 69.629 (70.149)\n",
      "Epoch: [105][45/48]\tTime 0.022 (0.024)\tLoss 0.9280 (0.8661)\tPrec@1 66.504 (69.667)\n",
      "Epoch: [105][48/48]\tTime 0.020 (0.024)\tLoss 0.8593 (0.8688)\tPrec@1 70.637 (69.592)\n",
      "EPOCH: 105 train Results: Prec@1 69.592 Loss: 0.8688\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2148 (1.2148)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2618 (1.2359)\tPrec@1 55.230 (56.550)\n",
      "EPOCH: 105 val Results: Prec@1 56.550 Loss: 1.2359\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [106][0/48]\tTime 0.023 (0.023)\tLoss 0.8177 (0.8177)\tPrec@1 73.145 (73.145)\n",
      "Epoch: [106][9/48]\tTime 0.022 (0.027)\tLoss 0.8238 (0.8115)\tPrec@1 69.922 (72.090)\n",
      "Epoch: [106][18/48]\tTime 0.038 (0.027)\tLoss 0.8387 (0.8322)\tPrec@1 71.289 (71.063)\n",
      "Epoch: [106][27/48]\tTime 0.014 (0.025)\tLoss 0.9189 (0.8455)\tPrec@1 66.309 (70.529)\n",
      "Epoch: [106][36/48]\tTime 0.027 (0.025)\tLoss 0.9099 (0.8550)\tPrec@1 67.773 (70.051)\n",
      "Epoch: [106][45/48]\tTime 0.023 (0.024)\tLoss 0.8823 (0.8630)\tPrec@1 68.652 (69.631)\n",
      "Epoch: [106][48/48]\tTime 0.025 (0.024)\tLoss 0.8743 (0.8661)\tPrec@1 68.514 (69.526)\n",
      "EPOCH: 106 train Results: Prec@1 69.526 Loss: 0.8661\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.2073 (1.2073)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.005 (0.009)\tLoss 1.2727 (1.2366)\tPrec@1 55.485 (56.620)\n",
      "EPOCH: 106 val Results: Prec@1 56.620 Loss: 1.2366\n",
      "Best Prec@1: 56.630\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [107][0/48]\tTime 0.020 (0.020)\tLoss 0.8095 (0.8095)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [107][9/48]\tTime 0.014 (0.024)\tLoss 0.8367 (0.8256)\tPrec@1 70.117 (71.338)\n",
      "Epoch: [107][18/48]\tTime 0.021 (0.022)\tLoss 0.8083 (0.8328)\tPrec@1 71.777 (71.032)\n",
      "Epoch: [107][27/48]\tTime 0.016 (0.022)\tLoss 0.8402 (0.8363)\tPrec@1 69.434 (70.773)\n",
      "Epoch: [107][36/48]\tTime 0.019 (0.022)\tLoss 0.9357 (0.8503)\tPrec@1 67.090 (70.252)\n",
      "Epoch: [107][45/48]\tTime 0.019 (0.023)\tLoss 0.8971 (0.8610)\tPrec@1 67.188 (69.801)\n",
      "Epoch: [107][48/48]\tTime 0.020 (0.023)\tLoss 0.9023 (0.8648)\tPrec@1 69.811 (69.664)\n",
      "EPOCH: 107 train Results: Prec@1 69.664 Loss: 0.8648\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1930 (1.1930)\tPrec@1 60.742 (60.742)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2686 (1.2400)\tPrec@1 54.974 (56.800)\n",
      "EPOCH: 107 val Results: Prec@1 56.800 Loss: 1.2400\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [108][0/48]\tTime 0.030 (0.030)\tLoss 0.7871 (0.7871)\tPrec@1 73.438 (73.438)\n",
      "Epoch: [108][9/48]\tTime 0.023 (0.025)\tLoss 0.8411 (0.8147)\tPrec@1 71.094 (71.797)\n",
      "Epoch: [108][18/48]\tTime 0.020 (0.023)\tLoss 0.8665 (0.8308)\tPrec@1 69.922 (71.371)\n",
      "Epoch: [108][27/48]\tTime 0.021 (0.023)\tLoss 0.8411 (0.8425)\tPrec@1 69.727 (70.839)\n",
      "Epoch: [108][36/48]\tTime 0.019 (0.022)\tLoss 0.8108 (0.8510)\tPrec@1 72.266 (70.334)\n",
      "Epoch: [108][45/48]\tTime 0.015 (0.022)\tLoss 0.8776 (0.8601)\tPrec@1 70.215 (70.013)\n",
      "Epoch: [108][48/48]\tTime 0.022 (0.022)\tLoss 0.9529 (0.8624)\tPrec@1 67.217 (69.940)\n",
      "EPOCH: 108 train Results: Prec@1 69.940 Loss: 0.8624\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2159 (1.2159)\tPrec@1 58.594 (58.594)\n",
      "Test: [9/9]\tTime 0.019 (0.011)\tLoss 1.2595 (1.2372)\tPrec@1 54.464 (56.730)\n",
      "EPOCH: 108 val Results: Prec@1 56.730 Loss: 1.2372\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [109][0/48]\tTime 0.092 (0.092)\tLoss 0.8410 (0.8410)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [109][9/48]\tTime 0.027 (0.033)\tLoss 0.8216 (0.8301)\tPrec@1 71.387 (71.289)\n",
      "Epoch: [109][18/48]\tTime 0.019 (0.028)\tLoss 0.8771 (0.8421)\tPrec@1 68.457 (70.765)\n",
      "Epoch: [109][27/48]\tTime 0.025 (0.028)\tLoss 0.8766 (0.8449)\tPrec@1 70.312 (70.428)\n",
      "Epoch: [109][36/48]\tTime 0.048 (0.028)\tLoss 0.8803 (0.8513)\tPrec@1 69.922 (70.289)\n",
      "Epoch: [109][45/48]\tTime 0.045 (0.032)\tLoss 0.8954 (0.8589)\tPrec@1 69.141 (69.928)\n",
      "Epoch: [109][48/48]\tTime 0.021 (0.032)\tLoss 0.8857 (0.8618)\tPrec@1 67.807 (69.726)\n",
      "EPOCH: 109 train Results: Prec@1 69.726 Loss: 0.8618\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.2136 (1.2136)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.004 (0.007)\tLoss 1.2782 (1.2435)\tPrec@1 54.974 (56.340)\n",
      "EPOCH: 109 val Results: Prec@1 56.340 Loss: 1.2435\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [110][0/48]\tTime 0.027 (0.027)\tLoss 0.8066 (0.8066)\tPrec@1 71.387 (71.387)\n",
      "Epoch: [110][9/48]\tTime 0.014 (0.021)\tLoss 0.8083 (0.8227)\tPrec@1 70.215 (70.938)\n",
      "Epoch: [110][18/48]\tTime 0.022 (0.021)\tLoss 0.8174 (0.8359)\tPrec@1 72.852 (70.719)\n",
      "Epoch: [110][27/48]\tTime 0.026 (0.022)\tLoss 0.8842 (0.8365)\tPrec@1 70.508 (70.741)\n",
      "Epoch: [110][36/48]\tTime 0.017 (0.022)\tLoss 0.9097 (0.8497)\tPrec@1 68.066 (70.331)\n",
      "Epoch: [110][45/48]\tTime 0.017 (0.022)\tLoss 0.8672 (0.8548)\tPrec@1 69.434 (69.998)\n",
      "Epoch: [110][48/48]\tTime 0.017 (0.022)\tLoss 0.9261 (0.8567)\tPrec@1 67.099 (69.946)\n",
      "EPOCH: 110 train Results: Prec@1 69.946 Loss: 0.8567\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2009 (1.2009)\tPrec@1 60.156 (60.156)\n",
      "Test: [9/9]\tTime 0.012 (0.007)\tLoss 1.2787 (1.2464)\tPrec@1 52.679 (56.490)\n",
      "EPOCH: 110 val Results: Prec@1 56.490 Loss: 1.2464\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [111][0/48]\tTime 0.027 (0.027)\tLoss 0.8104 (0.8104)\tPrec@1 73.242 (73.242)\n",
      "Epoch: [111][9/48]\tTime 0.020 (0.024)\tLoss 0.8129 (0.8138)\tPrec@1 72.266 (71.709)\n",
      "Epoch: [111][18/48]\tTime 0.033 (0.025)\tLoss 0.8345 (0.8300)\tPrec@1 71.875 (71.089)\n",
      "Epoch: [111][27/48]\tTime 0.033 (0.025)\tLoss 0.8264 (0.8412)\tPrec@1 72.461 (70.710)\n",
      "Epoch: [111][36/48]\tTime 0.025 (0.025)\tLoss 0.9541 (0.8531)\tPrec@1 66.016 (70.165)\n",
      "Epoch: [111][45/48]\tTime 0.022 (0.024)\tLoss 0.9389 (0.8562)\tPrec@1 66.309 (70.005)\n",
      "Epoch: [111][48/48]\tTime 0.014 (0.024)\tLoss 0.9460 (0.8581)\tPrec@1 64.976 (69.930)\n",
      "EPOCH: 111 train Results: Prec@1 69.930 Loss: 0.8581\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2280 (1.2280)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.2923 (1.2561)\tPrec@1 55.357 (56.490)\n",
      "EPOCH: 111 val Results: Prec@1 56.490 Loss: 1.2561\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [112][0/48]\tTime 0.027 (0.027)\tLoss 0.8042 (0.8042)\tPrec@1 72.168 (72.168)\n",
      "Epoch: [112][9/48]\tTime 0.023 (0.023)\tLoss 0.7782 (0.8227)\tPrec@1 73.047 (71.045)\n",
      "Epoch: [112][18/48]\tTime 0.034 (0.024)\tLoss 0.8414 (0.8287)\tPrec@1 70.801 (70.698)\n",
      "Epoch: [112][27/48]\tTime 0.030 (0.025)\tLoss 0.8319 (0.8357)\tPrec@1 70.508 (70.414)\n",
      "Epoch: [112][36/48]\tTime 0.030 (0.025)\tLoss 0.8989 (0.8479)\tPrec@1 67.871 (70.038)\n",
      "Epoch: [112][45/48]\tTime 0.024 (0.026)\tLoss 0.8860 (0.8544)\tPrec@1 69.141 (69.818)\n",
      "Epoch: [112][48/48]\tTime 0.029 (0.026)\tLoss 0.8970 (0.8558)\tPrec@1 68.042 (69.726)\n",
      "EPOCH: 112 train Results: Prec@1 69.726 Loss: 0.8558\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1969 (1.1969)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.009 (0.009)\tLoss 1.2699 (1.2417)\tPrec@1 56.378 (56.320)\n",
      "EPOCH: 112 val Results: Prec@1 56.320 Loss: 1.2417\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [113][0/48]\tTime 0.045 (0.045)\tLoss 0.8270 (0.8270)\tPrec@1 71.484 (71.484)\n",
      "Epoch: [113][9/48]\tTime 0.022 (0.028)\tLoss 0.8639 (0.8094)\tPrec@1 67.773 (71.553)\n",
      "Epoch: [113][18/48]\tTime 0.027 (0.024)\tLoss 0.8439 (0.8219)\tPrec@1 69.727 (71.119)\n",
      "Epoch: [113][27/48]\tTime 0.015 (0.023)\tLoss 0.8533 (0.8344)\tPrec@1 68.652 (70.609)\n",
      "Epoch: [113][36/48]\tTime 0.020 (0.022)\tLoss 0.9038 (0.8422)\tPrec@1 66.309 (70.305)\n",
      "Epoch: [113][45/48]\tTime 0.026 (0.023)\tLoss 0.9016 (0.8500)\tPrec@1 68.164 (69.975)\n",
      "Epoch: [113][48/48]\tTime 0.015 (0.023)\tLoss 0.9107 (0.8534)\tPrec@1 67.099 (69.830)\n",
      "EPOCH: 113 train Results: Prec@1 69.830 Loss: 0.8534\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2155 (1.2155)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.3117 (1.2472)\tPrec@1 54.209 (56.100)\n",
      "EPOCH: 113 val Results: Prec@1 56.100 Loss: 1.2472\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [114][0/48]\tTime 0.016 (0.016)\tLoss 0.7681 (0.7681)\tPrec@1 73.926 (73.926)\n",
      "Epoch: [114][9/48]\tTime 0.019 (0.025)\tLoss 0.8126 (0.8078)\tPrec@1 71.680 (72.041)\n",
      "Epoch: [114][18/48]\tTime 0.023 (0.024)\tLoss 0.9004 (0.8269)\tPrec@1 65.723 (71.150)\n",
      "Epoch: [114][27/48]\tTime 0.020 (0.024)\tLoss 0.8518 (0.8340)\tPrec@1 69.629 (70.794)\n",
      "Epoch: [114][36/48]\tTime 0.038 (0.025)\tLoss 0.8710 (0.8391)\tPrec@1 70.117 (70.532)\n",
      "Epoch: [114][45/48]\tTime 0.023 (0.025)\tLoss 0.8567 (0.8494)\tPrec@1 70.703 (70.230)\n",
      "Epoch: [114][48/48]\tTime 0.018 (0.025)\tLoss 0.9445 (0.8532)\tPrec@1 66.627 (70.032)\n",
      "EPOCH: 114 train Results: Prec@1 70.032 Loss: 0.8532\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.2142 (1.2142)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2736 (1.2354)\tPrec@1 54.847 (56.790)\n",
      "EPOCH: 114 val Results: Prec@1 56.790 Loss: 1.2354\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [115][0/48]\tTime 0.026 (0.026)\tLoss 0.8002 (0.8002)\tPrec@1 70.996 (70.996)\n",
      "Epoch: [115][9/48]\tTime 0.016 (0.024)\tLoss 0.8078 (0.7977)\tPrec@1 73.242 (72.305)\n",
      "Epoch: [115][18/48]\tTime 0.038 (0.025)\tLoss 0.8339 (0.8165)\tPrec@1 70.410 (71.633)\n",
      "Epoch: [115][27/48]\tTime 0.028 (0.024)\tLoss 0.8881 (0.8235)\tPrec@1 69.727 (71.317)\n",
      "Epoch: [115][36/48]\tTime 0.039 (0.024)\tLoss 0.8396 (0.8355)\tPrec@1 70.215 (70.743)\n",
      "Epoch: [115][45/48]\tTime 0.026 (0.025)\tLoss 0.8919 (0.8452)\tPrec@1 67.578 (70.325)\n",
      "Epoch: [115][48/48]\tTime 0.019 (0.025)\tLoss 0.9035 (0.8483)\tPrec@1 67.807 (70.212)\n",
      "EPOCH: 115 train Results: Prec@1 70.212 Loss: 0.8483\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2074 (1.2074)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2810 (1.2379)\tPrec@1 55.230 (56.490)\n",
      "EPOCH: 115 val Results: Prec@1 56.490 Loss: 1.2379\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [116][0/48]\tTime 0.027 (0.027)\tLoss 0.8394 (0.8394)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [116][9/48]\tTime 0.020 (0.023)\tLoss 0.8201 (0.8071)\tPrec@1 72.461 (72.461)\n",
      "Epoch: [116][18/48]\tTime 0.024 (0.024)\tLoss 0.8809 (0.8240)\tPrec@1 70.703 (71.500)\n",
      "Epoch: [116][27/48]\tTime 0.040 (0.026)\tLoss 0.8260 (0.8292)\tPrec@1 71.289 (71.184)\n",
      "Epoch: [116][36/48]\tTime 0.040 (0.026)\tLoss 0.8510 (0.8391)\tPrec@1 72.070 (70.687)\n",
      "Epoch: [116][45/48]\tTime 0.041 (0.028)\tLoss 0.8973 (0.8452)\tPrec@1 68.359 (70.446)\n",
      "Epoch: [116][48/48]\tTime 0.032 (0.029)\tLoss 0.8900 (0.8470)\tPrec@1 67.689 (70.366)\n",
      "EPOCH: 116 train Results: Prec@1 70.366 Loss: 0.8470\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2202 (1.2202)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.005 (0.008)\tLoss 1.2706 (1.2416)\tPrec@1 54.974 (56.700)\n",
      "EPOCH: 116 val Results: Prec@1 56.700 Loss: 1.2416\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [117][0/48]\tTime 0.021 (0.021)\tLoss 0.8019 (0.8019)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [117][9/48]\tTime 0.048 (0.031)\tLoss 0.8247 (0.8099)\tPrec@1 70.703 (71.787)\n",
      "Epoch: [117][18/48]\tTime 0.029 (0.037)\tLoss 0.8051 (0.8195)\tPrec@1 70.801 (71.197)\n",
      "Epoch: [117][27/48]\tTime 0.017 (0.036)\tLoss 0.8482 (0.8273)\tPrec@1 70.117 (70.958)\n",
      "Epoch: [117][36/48]\tTime 0.025 (0.034)\tLoss 0.8348 (0.8371)\tPrec@1 71.680 (70.603)\n",
      "Epoch: [117][45/48]\tTime 0.021 (0.033)\tLoss 0.9105 (0.8452)\tPrec@1 68.848 (70.334)\n",
      "Epoch: [117][48/48]\tTime 0.022 (0.033)\tLoss 0.8857 (0.8467)\tPrec@1 68.042 (70.266)\n",
      "EPOCH: 117 train Results: Prec@1 70.266 Loss: 0.8467\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2052 (1.2052)\tPrec@1 59.766 (59.766)\n",
      "Test: [9/9]\tTime 0.003 (0.008)\tLoss 1.2791 (1.2430)\tPrec@1 54.082 (56.300)\n",
      "EPOCH: 117 val Results: Prec@1 56.300 Loss: 1.2430\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [118][0/48]\tTime 0.037 (0.037)\tLoss 0.8555 (0.8555)\tPrec@1 70.996 (70.996)\n",
      "Epoch: [118][9/48]\tTime 0.040 (0.029)\tLoss 0.7723 (0.8158)\tPrec@1 74.512 (71.592)\n",
      "Epoch: [118][18/48]\tTime 0.020 (0.031)\tLoss 0.8179 (0.8178)\tPrec@1 71.484 (71.644)\n",
      "Epoch: [118][27/48]\tTime 0.022 (0.029)\tLoss 0.8402 (0.8237)\tPrec@1 70.996 (71.247)\n",
      "Epoch: [118][36/48]\tTime 0.037 (0.029)\tLoss 0.9064 (0.8339)\tPrec@1 71.094 (70.803)\n",
      "Epoch: [118][45/48]\tTime 0.024 (0.028)\tLoss 0.8730 (0.8427)\tPrec@1 68.359 (70.480)\n",
      "Epoch: [118][48/48]\tTime 0.034 (0.030)\tLoss 0.8457 (0.8442)\tPrec@1 72.288 (70.468)\n",
      "EPOCH: 118 train Results: Prec@1 70.468 Loss: 0.8442\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.2042 (1.2042)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.005 (0.011)\tLoss 1.2924 (1.2422)\tPrec@1 54.337 (56.770)\n",
      "EPOCH: 118 val Results: Prec@1 56.770 Loss: 1.2422\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [119][0/48]\tTime 0.035 (0.035)\tLoss 0.7571 (0.7571)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [119][9/48]\tTime 0.025 (0.032)\tLoss 0.8154 (0.7940)\tPrec@1 72.168 (72.715)\n",
      "Epoch: [119][18/48]\tTime 0.032 (0.029)\tLoss 0.8258 (0.8041)\tPrec@1 71.387 (72.081)\n",
      "Epoch: [119][27/48]\tTime 0.033 (0.029)\tLoss 0.8305 (0.8183)\tPrec@1 72.461 (71.355)\n",
      "Epoch: [119][36/48]\tTime 0.029 (0.029)\tLoss 0.8390 (0.8248)\tPrec@1 69.238 (71.038)\n",
      "Epoch: [119][45/48]\tTime 0.037 (0.031)\tLoss 0.9358 (0.8377)\tPrec@1 65.625 (70.535)\n",
      "Epoch: [119][48/48]\tTime 0.023 (0.031)\tLoss 0.8157 (0.8386)\tPrec@1 72.524 (70.508)\n",
      "EPOCH: 119 train Results: Prec@1 70.508 Loss: 0.8386\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2311 (1.2311)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.008 (0.006)\tLoss 1.2821 (1.2459)\tPrec@1 54.592 (56.430)\n",
      "EPOCH: 119 val Results: Prec@1 56.430 Loss: 1.2459\n",
      "Best Prec@1: 56.800\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [120][0/48]\tTime 0.064 (0.064)\tLoss 0.7229 (0.7229)\tPrec@1 74.219 (74.219)\n",
      "Epoch: [120][9/48]\tTime 0.027 (0.030)\tLoss 0.8275 (0.8030)\tPrec@1 72.656 (72.002)\n",
      "Epoch: [120][18/48]\tTime 0.031 (0.035)\tLoss 0.8183 (0.8224)\tPrec@1 71.777 (71.279)\n",
      "Epoch: [120][27/48]\tTime 0.044 (0.035)\tLoss 0.8609 (0.8268)\tPrec@1 70.703 (71.076)\n",
      "Epoch: [120][36/48]\tTime 0.019 (0.034)\tLoss 0.8368 (0.8300)\tPrec@1 71.680 (70.838)\n",
      "Epoch: [120][45/48]\tTime 0.028 (0.034)\tLoss 0.8627 (0.8377)\tPrec@1 70.020 (70.489)\n",
      "Epoch: [120][48/48]\tTime 0.016 (0.034)\tLoss 0.8823 (0.8402)\tPrec@1 70.165 (70.474)\n",
      "EPOCH: 120 train Results: Prec@1 70.474 Loss: 0.8402\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.1950 (1.1950)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.2875 (1.2357)\tPrec@1 53.954 (57.130)\n",
      "EPOCH: 120 val Results: Prec@1 57.130 Loss: 1.2357\n",
      "Best Prec@1: 57.130\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [121][0/48]\tTime 0.021 (0.021)\tLoss 0.7610 (0.7610)\tPrec@1 73.242 (73.242)\n",
      "Epoch: [121][9/48]\tTime 0.018 (0.028)\tLoss 0.7604 (0.7872)\tPrec@1 73.047 (72.100)\n",
      "Epoch: [121][18/48]\tTime 0.024 (0.029)\tLoss 0.8548 (0.8100)\tPrec@1 70.117 (71.577)\n",
      "Epoch: [121][27/48]\tTime 0.038 (0.028)\tLoss 0.8088 (0.8200)\tPrec@1 72.363 (71.334)\n",
      "Epoch: [121][36/48]\tTime 0.044 (0.028)\tLoss 0.8685 (0.8303)\tPrec@1 68.555 (70.864)\n",
      "Epoch: [121][45/48]\tTime 0.020 (0.028)\tLoss 0.8537 (0.8370)\tPrec@1 69.922 (70.574)\n",
      "Epoch: [121][48/48]\tTime 0.027 (0.029)\tLoss 0.8317 (0.8397)\tPrec@1 70.047 (70.464)\n",
      "EPOCH: 121 train Results: Prec@1 70.464 Loss: 0.8397\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2190 (1.2190)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.010 (0.007)\tLoss 1.2775 (1.2408)\tPrec@1 54.209 (57.410)\n",
      "EPOCH: 121 val Results: Prec@1 57.410 Loss: 1.2408\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [122][0/48]\tTime 0.032 (0.032)\tLoss 0.8156 (0.8156)\tPrec@1 71.191 (71.191)\n",
      "Epoch: [122][9/48]\tTime 0.026 (0.039)\tLoss 0.8093 (0.8002)\tPrec@1 72.168 (72.070)\n",
      "Epoch: [122][18/48]\tTime 0.039 (0.033)\tLoss 0.8288 (0.8151)\tPrec@1 71.680 (71.500)\n",
      "Epoch: [122][27/48]\tTime 0.019 (0.036)\tLoss 0.8612 (0.8243)\tPrec@1 69.336 (71.132)\n",
      "Epoch: [122][36/48]\tTime 0.094 (0.037)\tLoss 0.8168 (0.8316)\tPrec@1 73.535 (70.922)\n",
      "Epoch: [122][45/48]\tTime 0.054 (0.036)\tLoss 0.8639 (0.8392)\tPrec@1 68.457 (70.533)\n",
      "Epoch: [122][48/48]\tTime 0.033 (0.035)\tLoss 0.8229 (0.8412)\tPrec@1 70.047 (70.468)\n",
      "EPOCH: 122 train Results: Prec@1 70.468 Loss: 0.8412\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2196 (1.2196)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.003 (0.009)\tLoss 1.2957 (1.2461)\tPrec@1 51.786 (56.800)\n",
      "EPOCH: 122 val Results: Prec@1 56.800 Loss: 1.2461\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [123][0/48]\tTime 0.078 (0.078)\tLoss 0.7773 (0.7773)\tPrec@1 73.145 (73.145)\n",
      "Epoch: [123][9/48]\tTime 0.027 (0.040)\tLoss 0.8297 (0.7912)\tPrec@1 71.680 (73.105)\n",
      "Epoch: [123][18/48]\tTime 0.036 (0.039)\tLoss 0.8927 (0.8044)\tPrec@1 68.164 (72.404)\n",
      "Epoch: [123][27/48]\tTime 0.030 (0.034)\tLoss 0.8532 (0.8119)\tPrec@1 69.629 (71.931)\n",
      "Epoch: [123][36/48]\tTime 0.036 (0.033)\tLoss 0.8404 (0.8245)\tPrec@1 70.215 (71.408)\n",
      "Epoch: [123][45/48]\tTime 0.043 (0.032)\tLoss 0.8769 (0.8349)\tPrec@1 67.480 (70.913)\n",
      "Epoch: [123][48/48]\tTime 0.017 (0.032)\tLoss 0.9048 (0.8386)\tPrec@1 67.807 (70.758)\n",
      "EPOCH: 123 train Results: Prec@1 70.758 Loss: 0.8386\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2074 (1.2074)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.006 (0.008)\tLoss 1.2774 (1.2438)\tPrec@1 54.847 (56.990)\n",
      "EPOCH: 123 val Results: Prec@1 56.990 Loss: 1.2438\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [124][0/48]\tTime 0.027 (0.027)\tLoss 0.7806 (0.7806)\tPrec@1 72.949 (72.949)\n",
      "Epoch: [124][9/48]\tTime 0.022 (0.034)\tLoss 0.7968 (0.7893)\tPrec@1 72.656 (72.793)\n",
      "Epoch: [124][18/48]\tTime 0.030 (0.032)\tLoss 0.8457 (0.8068)\tPrec@1 70.996 (71.988)\n",
      "Epoch: [124][27/48]\tTime 0.025 (0.030)\tLoss 0.8424 (0.8152)\tPrec@1 70.410 (71.575)\n",
      "Epoch: [124][36/48]\tTime 0.023 (0.030)\tLoss 0.8892 (0.8215)\tPrec@1 68.164 (71.157)\n",
      "Epoch: [124][45/48]\tTime 0.058 (0.030)\tLoss 0.8867 (0.8332)\tPrec@1 67.773 (70.731)\n",
      "Epoch: [124][48/48]\tTime 0.030 (0.031)\tLoss 0.8937 (0.8376)\tPrec@1 68.042 (70.540)\n",
      "EPOCH: 124 train Results: Prec@1 70.540 Loss: 0.8376\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.2278 (1.2278)\tPrec@1 57.910 (57.910)\n",
      "Test: [9/9]\tTime 0.007 (0.012)\tLoss 1.2752 (1.2438)\tPrec@1 55.485 (56.770)\n",
      "EPOCH: 124 val Results: Prec@1 56.770 Loss: 1.2438\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [125][0/48]\tTime 0.025 (0.025)\tLoss 0.8289 (0.8289)\tPrec@1 71.289 (71.289)\n",
      "Epoch: [125][9/48]\tTime 0.041 (0.036)\tLoss 0.8158 (0.7963)\tPrec@1 71.289 (72.646)\n",
      "Epoch: [125][18/48]\tTime 0.028 (0.032)\tLoss 0.8436 (0.8113)\tPrec@1 71.387 (72.029)\n",
      "Epoch: [125][27/48]\tTime 0.019 (0.034)\tLoss 0.8498 (0.8184)\tPrec@1 71.289 (71.634)\n",
      "Epoch: [125][36/48]\tTime 0.057 (0.033)\tLoss 0.8768 (0.8237)\tPrec@1 68.066 (71.395)\n",
      "Epoch: [125][45/48]\tTime 0.033 (0.034)\tLoss 0.8510 (0.8342)\tPrec@1 70.215 (70.911)\n",
      "Epoch: [125][48/48]\tTime 0.019 (0.034)\tLoss 0.8273 (0.8351)\tPrec@1 70.991 (70.870)\n",
      "EPOCH: 125 train Results: Prec@1 70.870 Loss: 0.8351\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2155 (1.2155)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.007 (0.008)\tLoss 1.2845 (1.2411)\tPrec@1 54.464 (57.030)\n",
      "EPOCH: 125 val Results: Prec@1 57.030 Loss: 1.2411\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [126][0/48]\tTime 0.020 (0.020)\tLoss 0.7770 (0.7770)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [126][9/48]\tTime 0.059 (0.036)\tLoss 0.8098 (0.7915)\tPrec@1 70.801 (72.510)\n",
      "Epoch: [126][18/48]\tTime 0.045 (0.034)\tLoss 0.7841 (0.7985)\tPrec@1 72.266 (72.291)\n",
      "Epoch: [126][27/48]\tTime 0.021 (0.031)\tLoss 0.8319 (0.8100)\tPrec@1 72.070 (71.763)\n",
      "Epoch: [126][36/48]\tTime 0.029 (0.030)\tLoss 0.8131 (0.8207)\tPrec@1 71.973 (71.218)\n",
      "Epoch: [126][45/48]\tTime 0.031 (0.030)\tLoss 0.9239 (0.8302)\tPrec@1 67.871 (70.693)\n",
      "Epoch: [126][48/48]\tTime 0.017 (0.030)\tLoss 0.8118 (0.8326)\tPrec@1 71.108 (70.600)\n",
      "EPOCH: 126 train Results: Prec@1 70.600 Loss: 0.8326\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2214 (1.2214)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.006 (0.009)\tLoss 1.2858 (1.2470)\tPrec@1 53.571 (57.020)\n",
      "EPOCH: 126 val Results: Prec@1 57.020 Loss: 1.2470\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [127][0/48]\tTime 0.031 (0.031)\tLoss 0.7728 (0.7728)\tPrec@1 72.754 (72.754)\n",
      "Epoch: [127][9/48]\tTime 0.027 (0.031)\tLoss 0.7977 (0.7876)\tPrec@1 73.438 (72.646)\n",
      "Epoch: [127][18/48]\tTime 0.029 (0.030)\tLoss 0.7885 (0.7969)\tPrec@1 71.973 (72.271)\n",
      "Epoch: [127][27/48]\tTime 0.031 (0.031)\tLoss 0.8336 (0.8037)\tPrec@1 69.727 (71.924)\n",
      "Epoch: [127][36/48]\tTime 0.026 (0.031)\tLoss 0.8390 (0.8181)\tPrec@1 69.531 (71.265)\n",
      "Epoch: [127][45/48]\tTime 0.035 (0.030)\tLoss 0.8648 (0.8253)\tPrec@1 68.750 (70.954)\n",
      "Epoch: [127][48/48]\tTime 0.044 (0.030)\tLoss 0.9197 (0.8293)\tPrec@1 68.750 (70.824)\n",
      "EPOCH: 127 train Results: Prec@1 70.824 Loss: 0.8293\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2298 (1.2298)\tPrec@1 58.887 (58.887)\n",
      "Test: [9/9]\tTime 0.014 (0.008)\tLoss 1.2902 (1.2470)\tPrec@1 53.316 (56.950)\n",
      "EPOCH: 127 val Results: Prec@1 56.950 Loss: 1.2470\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [128][0/48]\tTime 0.024 (0.024)\tLoss 0.8097 (0.8097)\tPrec@1 71.680 (71.680)\n",
      "Epoch: [128][9/48]\tTime 0.019 (0.023)\tLoss 0.7973 (0.7951)\tPrec@1 73.340 (72.100)\n",
      "Epoch: [128][18/48]\tTime 0.032 (0.026)\tLoss 0.8183 (0.7997)\tPrec@1 70.215 (71.983)\n",
      "Epoch: [128][27/48]\tTime 0.027 (0.025)\tLoss 0.8101 (0.8077)\tPrec@1 72.559 (71.715)\n",
      "Epoch: [128][36/48]\tTime 0.026 (0.026)\tLoss 0.8694 (0.8182)\tPrec@1 68.066 (71.255)\n",
      "Epoch: [128][45/48]\tTime 0.032 (0.030)\tLoss 0.8577 (0.8290)\tPrec@1 69.727 (70.835)\n",
      "Epoch: [128][48/48]\tTime 0.023 (0.030)\tLoss 0.8722 (0.8321)\tPrec@1 69.222 (70.734)\n",
      "EPOCH: 128 train Results: Prec@1 70.734 Loss: 0.8321\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2202 (1.2202)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.022 (0.010)\tLoss 1.2756 (1.2533)\tPrec@1 54.464 (56.960)\n",
      "EPOCH: 128 val Results: Prec@1 56.960 Loss: 1.2533\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [129][0/48]\tTime 0.021 (0.021)\tLoss 0.7384 (0.7384)\tPrec@1 73.535 (73.535)\n",
      "Epoch: [129][9/48]\tTime 0.030 (0.024)\tLoss 0.8130 (0.7893)\tPrec@1 72.559 (72.803)\n",
      "Epoch: [129][18/48]\tTime 0.026 (0.029)\tLoss 0.8544 (0.8013)\tPrec@1 68.945 (72.173)\n",
      "Epoch: [129][27/48]\tTime 0.017 (0.028)\tLoss 0.8220 (0.8112)\tPrec@1 70.117 (71.498)\n",
      "Epoch: [129][36/48]\tTime 0.019 (0.028)\tLoss 0.8369 (0.8189)\tPrec@1 70.508 (71.157)\n",
      "Epoch: [129][45/48]\tTime 0.040 (0.029)\tLoss 0.8669 (0.8298)\tPrec@1 70.312 (70.718)\n",
      "Epoch: [129][48/48]\tTime 0.032 (0.029)\tLoss 0.8864 (0.8336)\tPrec@1 69.104 (70.624)\n",
      "EPOCH: 129 train Results: Prec@1 70.624 Loss: 0.8336\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2054 (1.2054)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.005 (0.009)\tLoss 1.2726 (1.2492)\tPrec@1 55.357 (56.410)\n",
      "EPOCH: 129 val Results: Prec@1 56.410 Loss: 1.2492\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [130][0/48]\tTime 0.021 (0.021)\tLoss 0.8116 (0.8116)\tPrec@1 70.996 (70.996)\n",
      "Epoch: [130][9/48]\tTime 0.044 (0.032)\tLoss 0.8115 (0.7833)\tPrec@1 72.559 (72.793)\n",
      "Epoch: [130][18/48]\tTime 0.021 (0.029)\tLoss 0.8198 (0.7939)\tPrec@1 71.289 (72.250)\n",
      "Epoch: [130][27/48]\tTime 0.025 (0.030)\tLoss 0.8119 (0.8077)\tPrec@1 69.824 (71.634)\n",
      "Epoch: [130][36/48]\tTime 0.044 (0.030)\tLoss 0.8536 (0.8187)\tPrec@1 68.750 (71.183)\n",
      "Epoch: [130][45/48]\tTime 0.020 (0.030)\tLoss 0.8722 (0.8275)\tPrec@1 68.848 (70.763)\n",
      "Epoch: [130][48/48]\tTime 0.017 (0.030)\tLoss 0.8941 (0.8308)\tPrec@1 68.868 (70.672)\n",
      "EPOCH: 130 train Results: Prec@1 70.672 Loss: 0.8308\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2232 (1.2232)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.010 (0.009)\tLoss 1.2592 (1.2457)\tPrec@1 54.847 (56.810)\n",
      "EPOCH: 130 val Results: Prec@1 56.810 Loss: 1.2457\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [131][0/48]\tTime 0.023 (0.023)\tLoss 0.7740 (0.7740)\tPrec@1 72.266 (72.266)\n",
      "Epoch: [131][9/48]\tTime 0.034 (0.031)\tLoss 0.7962 (0.7919)\tPrec@1 71.680 (72.568)\n",
      "Epoch: [131][18/48]\tTime 0.031 (0.029)\tLoss 0.8303 (0.7975)\tPrec@1 70.801 (72.039)\n",
      "Epoch: [131][27/48]\tTime 0.039 (0.029)\tLoss 0.8572 (0.8090)\tPrec@1 70.117 (71.648)\n",
      "Epoch: [131][36/48]\tTime 0.020 (0.029)\tLoss 0.8081 (0.8187)\tPrec@1 72.070 (71.176)\n",
      "Epoch: [131][45/48]\tTime 0.043 (0.028)\tLoss 0.8084 (0.8280)\tPrec@1 70.605 (70.775)\n",
      "Epoch: [131][48/48]\tTime 0.013 (0.028)\tLoss 0.8852 (0.8300)\tPrec@1 65.330 (70.652)\n",
      "EPOCH: 131 train Results: Prec@1 70.652 Loss: 0.8300\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.2229 (1.2229)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.013 (0.008)\tLoss 1.3079 (1.2521)\tPrec@1 54.719 (56.600)\n",
      "EPOCH: 131 val Results: Prec@1 56.600 Loss: 1.2521\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [132][0/48]\tTime 0.023 (0.023)\tLoss 0.8405 (0.8405)\tPrec@1 69.336 (69.336)\n",
      "Epoch: [132][9/48]\tTime 0.022 (0.038)\tLoss 0.8358 (0.7869)\tPrec@1 69.727 (72.246)\n",
      "Epoch: [132][18/48]\tTime 0.042 (0.033)\tLoss 0.7906 (0.7967)\tPrec@1 73.535 (72.255)\n",
      "Epoch: [132][27/48]\tTime 0.030 (0.032)\tLoss 0.8586 (0.8098)\tPrec@1 70.801 (71.763)\n",
      "Epoch: [132][36/48]\tTime 0.031 (0.032)\tLoss 0.8804 (0.8181)\tPrec@1 70.215 (71.453)\n",
      "Epoch: [132][45/48]\tTime 0.036 (0.031)\tLoss 0.8374 (0.8232)\tPrec@1 70.508 (71.090)\n",
      "Epoch: [132][48/48]\tTime 0.029 (0.031)\tLoss 0.8109 (0.8249)\tPrec@1 70.991 (71.008)\n",
      "EPOCH: 132 train Results: Prec@1 71.008 Loss: 0.8249\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2519 (1.2519)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.005 (0.007)\tLoss 1.2672 (1.2484)\tPrec@1 55.485 (57.090)\n",
      "EPOCH: 132 val Results: Prec@1 57.090 Loss: 1.2484\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [133][0/48]\tTime 0.038 (0.038)\tLoss 0.7435 (0.7435)\tPrec@1 74.805 (74.805)\n",
      "Epoch: [133][9/48]\tTime 0.028 (0.037)\tLoss 0.8293 (0.7755)\tPrec@1 70.117 (73.047)\n",
      "Epoch: [133][18/48]\tTime 0.017 (0.032)\tLoss 0.8243 (0.7816)\tPrec@1 71.191 (72.589)\n",
      "Epoch: [133][27/48]\tTime 0.034 (0.032)\tLoss 0.8297 (0.7926)\tPrec@1 69.727 (72.109)\n",
      "Epoch: [133][36/48]\tTime 0.026 (0.031)\tLoss 0.8504 (0.8067)\tPrec@1 70.410 (71.783)\n",
      "Epoch: [133][45/48]\tTime 0.042 (0.031)\tLoss 0.9061 (0.8200)\tPrec@1 67.285 (71.266)\n",
      "Epoch: [133][48/48]\tTime 0.026 (0.031)\tLoss 0.8279 (0.8223)\tPrec@1 71.934 (71.218)\n",
      "EPOCH: 133 train Results: Prec@1 71.218 Loss: 0.8223\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2384 (1.2384)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.007 (0.007)\tLoss 1.2630 (1.2525)\tPrec@1 54.592 (57.110)\n",
      "EPOCH: 133 val Results: Prec@1 57.110 Loss: 1.2525\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [134][0/48]\tTime 0.032 (0.032)\tLoss 0.8128 (0.8128)\tPrec@1 71.875 (71.875)\n",
      "Epoch: [134][9/48]\tTime 0.027 (0.032)\tLoss 0.7943 (0.7853)\tPrec@1 72.656 (72.744)\n",
      "Epoch: [134][18/48]\tTime 0.030 (0.030)\tLoss 0.8063 (0.7984)\tPrec@1 71.387 (71.824)\n",
      "Epoch: [134][27/48]\tTime 0.041 (0.030)\tLoss 0.8427 (0.8033)\tPrec@1 69.824 (71.833)\n",
      "Epoch: [134][36/48]\tTime 0.031 (0.030)\tLoss 0.8284 (0.8110)\tPrec@1 70.996 (71.582)\n",
      "Epoch: [134][45/48]\tTime 0.022 (0.030)\tLoss 0.8651 (0.8195)\tPrec@1 68.164 (71.185)\n",
      "Epoch: [134][48/48]\tTime 0.027 (0.030)\tLoss 0.8586 (0.8220)\tPrec@1 69.340 (71.124)\n",
      "EPOCH: 134 train Results: Prec@1 71.124 Loss: 0.8220\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.2192 (1.2192)\tPrec@1 57.422 (57.422)\n",
      "Test: [9/9]\tTime 0.003 (0.012)\tLoss 1.2762 (1.2529)\tPrec@1 54.209 (56.740)\n",
      "EPOCH: 134 val Results: Prec@1 56.740 Loss: 1.2529\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [135][0/48]\tTime 0.031 (0.031)\tLoss 0.7503 (0.7503)\tPrec@1 75.391 (75.391)\n",
      "Epoch: [135][9/48]\tTime 0.046 (0.034)\tLoss 0.7816 (0.7765)\tPrec@1 72.656 (73.086)\n",
      "Epoch: [135][18/48]\tTime 0.055 (0.031)\tLoss 0.8360 (0.7837)\tPrec@1 70.020 (72.785)\n",
      "Epoch: [135][27/48]\tTime 0.040 (0.032)\tLoss 0.8464 (0.7936)\tPrec@1 71.289 (72.213)\n",
      "Epoch: [135][36/48]\tTime 0.034 (0.031)\tLoss 0.8516 (0.8073)\tPrec@1 69.141 (71.717)\n",
      "Epoch: [135][45/48]\tTime 0.038 (0.030)\tLoss 0.8814 (0.8165)\tPrec@1 69.727 (71.368)\n",
      "Epoch: [135][48/48]\tTime 0.029 (0.030)\tLoss 0.8846 (0.8196)\tPrec@1 67.925 (71.200)\n",
      "EPOCH: 135 train Results: Prec@1 71.200 Loss: 0.8196\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2346 (1.2346)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.006 (0.008)\tLoss 1.2675 (1.2500)\tPrec@1 56.250 (56.650)\n",
      "EPOCH: 135 val Results: Prec@1 56.650 Loss: 1.2500\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [136][0/48]\tTime 0.029 (0.029)\tLoss 0.7605 (0.7605)\tPrec@1 72.754 (72.754)\n",
      "Epoch: [136][9/48]\tTime 0.028 (0.030)\tLoss 0.8096 (0.7869)\tPrec@1 70.117 (72.559)\n",
      "Epoch: [136][18/48]\tTime 0.022 (0.029)\tLoss 0.7634 (0.7903)\tPrec@1 75.098 (72.435)\n",
      "Epoch: [136][27/48]\tTime 0.032 (0.028)\tLoss 0.8484 (0.8059)\tPrec@1 69.141 (71.872)\n",
      "Epoch: [136][36/48]\tTime 0.029 (0.031)\tLoss 0.8165 (0.8137)\tPrec@1 71.094 (71.524)\n",
      "Epoch: [136][45/48]\tTime 0.028 (0.030)\tLoss 0.8885 (0.8205)\tPrec@1 69.531 (71.329)\n",
      "Epoch: [136][48/48]\tTime 0.025 (0.031)\tLoss 0.8377 (0.8229)\tPrec@1 69.811 (71.196)\n",
      "EPOCH: 136 train Results: Prec@1 71.196 Loss: 0.8229\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2003 (1.2003)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.018 (0.013)\tLoss 1.2624 (1.2535)\tPrec@1 54.847 (56.620)\n",
      "EPOCH: 136 val Results: Prec@1 56.620 Loss: 1.2535\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [137][0/48]\tTime 0.039 (0.039)\tLoss 0.7573 (0.7573)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [137][9/48]\tTime 0.025 (0.027)\tLoss 0.7731 (0.7985)\tPrec@1 72.656 (72.188)\n",
      "Epoch: [137][18/48]\tTime 0.022 (0.030)\tLoss 0.7935 (0.7969)\tPrec@1 73.535 (72.209)\n",
      "Epoch: [137][27/48]\tTime 0.034 (0.030)\tLoss 0.8099 (0.7990)\tPrec@1 70.996 (71.983)\n",
      "Epoch: [137][36/48]\tTime 0.053 (0.032)\tLoss 0.8947 (0.8074)\tPrec@1 68.457 (71.624)\n",
      "Epoch: [137][45/48]\tTime 0.027 (0.031)\tLoss 0.7978 (0.8159)\tPrec@1 72.656 (71.412)\n",
      "Epoch: [137][48/48]\tTime 0.020 (0.031)\tLoss 0.9281 (0.8202)\tPrec@1 67.217 (71.214)\n",
      "EPOCH: 137 train Results: Prec@1 71.214 Loss: 0.8202\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2206 (1.2206)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.006 (0.010)\tLoss 1.2760 (1.2486)\tPrec@1 55.612 (56.630)\n",
      "EPOCH: 137 val Results: Prec@1 56.630 Loss: 1.2486\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [138][0/48]\tTime 0.019 (0.019)\tLoss 0.7783 (0.7783)\tPrec@1 73.047 (73.047)\n",
      "Epoch: [138][9/48]\tTime 0.021 (0.027)\tLoss 0.7932 (0.7749)\tPrec@1 71.680 (73.555)\n",
      "Epoch: [138][18/48]\tTime 0.037 (0.026)\tLoss 0.7896 (0.7894)\tPrec@1 71.973 (72.749)\n",
      "Epoch: [138][27/48]\tTime 0.022 (0.027)\tLoss 0.8308 (0.7941)\tPrec@1 71.191 (72.576)\n",
      "Epoch: [138][36/48]\tTime 0.031 (0.027)\tLoss 0.7750 (0.8027)\tPrec@1 74.707 (72.316)\n",
      "Epoch: [138][45/48]\tTime 0.038 (0.028)\tLoss 0.9314 (0.8100)\tPrec@1 65.039 (71.756)\n",
      "Epoch: [138][48/48]\tTime 0.019 (0.028)\tLoss 0.9602 (0.8153)\tPrec@1 64.976 (71.530)\n",
      "EPOCH: 138 train Results: Prec@1 71.530 Loss: 0.8153\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2285 (1.2285)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.006 (0.006)\tLoss 1.2818 (1.2543)\tPrec@1 54.592 (56.960)\n",
      "EPOCH: 138 val Results: Prec@1 56.960 Loss: 1.2543\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [139][0/48]\tTime 0.037 (0.037)\tLoss 0.7155 (0.7155)\tPrec@1 75.684 (75.684)\n",
      "Epoch: [139][9/48]\tTime 0.025 (0.031)\tLoss 0.7695 (0.7683)\tPrec@1 74.219 (73.574)\n",
      "Epoch: [139][18/48]\tTime 0.042 (0.030)\tLoss 0.8454 (0.7869)\tPrec@1 68.750 (72.646)\n",
      "Epoch: [139][27/48]\tTime 0.016 (0.030)\tLoss 0.8080 (0.7912)\tPrec@1 72.363 (72.398)\n",
      "Epoch: [139][36/48]\tTime 0.030 (0.030)\tLoss 0.8247 (0.8019)\tPrec@1 70.996 (71.941)\n",
      "Epoch: [139][45/48]\tTime 0.051 (0.033)\tLoss 0.8991 (0.8132)\tPrec@1 70.410 (71.618)\n",
      "Epoch: [139][48/48]\tTime 0.023 (0.033)\tLoss 0.8520 (0.8149)\tPrec@1 70.047 (71.544)\n",
      "EPOCH: 139 train Results: Prec@1 71.544 Loss: 0.8149\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.2421 (1.2421)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.003 (0.009)\tLoss 1.2738 (1.2523)\tPrec@1 54.592 (56.370)\n",
      "EPOCH: 139 val Results: Prec@1 56.370 Loss: 1.2523\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [140][0/48]\tTime 0.017 (0.017)\tLoss 0.7593 (0.7593)\tPrec@1 74.512 (74.512)\n",
      "Epoch: [140][9/48]\tTime 0.019 (0.026)\tLoss 0.7871 (0.7778)\tPrec@1 73.926 (73.057)\n",
      "Epoch: [140][18/48]\tTime 0.046 (0.027)\tLoss 0.8016 (0.7837)\tPrec@1 70.703 (72.600)\n",
      "Epoch: [140][27/48]\tTime 0.020 (0.028)\tLoss 0.8312 (0.7943)\tPrec@1 71.484 (72.259)\n",
      "Epoch: [140][36/48]\tTime 0.022 (0.029)\tLoss 0.8945 (0.8060)\tPrec@1 68.750 (71.812)\n",
      "Epoch: [140][45/48]\tTime 0.082 (0.029)\tLoss 0.7845 (0.8130)\tPrec@1 73.145 (71.535)\n",
      "Epoch: [140][48/48]\tTime 0.023 (0.030)\tLoss 0.8546 (0.8177)\tPrec@1 69.222 (71.366)\n",
      "EPOCH: 140 train Results: Prec@1 71.366 Loss: 0.8177\n",
      "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.2358 (1.2358)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.009 (0.009)\tLoss 1.2769 (1.2580)\tPrec@1 54.847 (56.070)\n",
      "EPOCH: 140 val Results: Prec@1 56.070 Loss: 1.2580\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [141][0/48]\tTime 0.038 (0.038)\tLoss 0.7185 (0.7185)\tPrec@1 74.805 (74.805)\n",
      "Epoch: [141][9/48]\tTime 0.029 (0.030)\tLoss 0.7821 (0.7629)\tPrec@1 72.754 (73.545)\n",
      "Epoch: [141][18/48]\tTime 0.040 (0.029)\tLoss 0.7922 (0.7809)\tPrec@1 71.973 (72.749)\n",
      "Epoch: [141][27/48]\tTime 0.018 (0.027)\tLoss 0.8400 (0.7953)\tPrec@1 70.801 (72.182)\n",
      "Epoch: [141][36/48]\tTime 0.039 (0.028)\tLoss 0.7937 (0.8044)\tPrec@1 72.363 (71.783)\n",
      "Epoch: [141][45/48]\tTime 0.029 (0.029)\tLoss 0.8829 (0.8131)\tPrec@1 69.141 (71.533)\n",
      "Epoch: [141][48/48]\tTime 0.018 (0.028)\tLoss 0.8140 (0.8143)\tPrec@1 70.047 (71.488)\n",
      "EPOCH: 141 train Results: Prec@1 71.488 Loss: 0.8143\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2400 (1.2400)\tPrec@1 57.617 (57.617)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.2707 (1.2445)\tPrec@1 55.357 (56.890)\n",
      "EPOCH: 141 val Results: Prec@1 56.890 Loss: 1.2445\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [142][0/48]\tTime 0.022 (0.022)\tLoss 0.7725 (0.7725)\tPrec@1 72.949 (72.949)\n",
      "Epoch: [142][9/48]\tTime 0.020 (0.021)\tLoss 0.7972 (0.7752)\tPrec@1 73.926 (73.086)\n",
      "Epoch: [142][18/48]\tTime 0.022 (0.021)\tLoss 0.7507 (0.7872)\tPrec@1 74.707 (72.651)\n",
      "Epoch: [142][27/48]\tTime 0.017 (0.021)\tLoss 0.8628 (0.7962)\tPrec@1 68.164 (72.196)\n",
      "Epoch: [142][36/48]\tTime 0.018 (0.023)\tLoss 0.8211 (0.8032)\tPrec@1 71.484 (71.888)\n",
      "Epoch: [142][45/48]\tTime 0.021 (0.023)\tLoss 0.8479 (0.8096)\tPrec@1 69.727 (71.591)\n",
      "Epoch: [142][48/48]\tTime 0.021 (0.023)\tLoss 0.8909 (0.8139)\tPrec@1 69.693 (71.412)\n",
      "EPOCH: 142 train Results: Prec@1 71.412 Loss: 0.8139\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2247 (1.2247)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.007 (0.008)\tLoss 1.2753 (1.2496)\tPrec@1 55.740 (56.490)\n",
      "EPOCH: 142 val Results: Prec@1 56.490 Loss: 1.2496\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [143][0/48]\tTime 0.020 (0.020)\tLoss 0.7581 (0.7581)\tPrec@1 74.023 (74.023)\n",
      "Epoch: [143][9/48]\tTime 0.020 (0.022)\tLoss 0.8049 (0.7717)\tPrec@1 72.461 (73.379)\n",
      "Epoch: [143][18/48]\tTime 0.014 (0.022)\tLoss 0.7859 (0.7773)\tPrec@1 72.559 (73.098)\n",
      "Epoch: [143][27/48]\tTime 0.022 (0.022)\tLoss 0.7718 (0.7863)\tPrec@1 72.852 (72.496)\n",
      "Epoch: [143][36/48]\tTime 0.022 (0.021)\tLoss 0.8437 (0.7941)\tPrec@1 70.020 (72.123)\n",
      "Epoch: [143][45/48]\tTime 0.021 (0.022)\tLoss 0.8360 (0.8039)\tPrec@1 70.605 (71.701)\n",
      "Epoch: [143][48/48]\tTime 0.016 (0.021)\tLoss 0.8794 (0.8070)\tPrec@1 67.689 (71.584)\n",
      "EPOCH: 143 train Results: Prec@1 71.584 Loss: 0.8070\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2112 (1.2112)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.005 (0.005)\tLoss 1.2756 (1.2581)\tPrec@1 53.954 (56.590)\n",
      "EPOCH: 143 val Results: Prec@1 56.590 Loss: 1.2581\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [144][0/48]\tTime 0.022 (0.022)\tLoss 0.7825 (0.7825)\tPrec@1 72.852 (72.852)\n",
      "Epoch: [144][9/48]\tTime 0.020 (0.023)\tLoss 0.7887 (0.7750)\tPrec@1 72.559 (72.900)\n",
      "Epoch: [144][18/48]\tTime 0.027 (0.022)\tLoss 0.7748 (0.7840)\tPrec@1 72.656 (72.523)\n",
      "Epoch: [144][27/48]\tTime 0.014 (0.023)\tLoss 0.8542 (0.7959)\tPrec@1 69.727 (72.102)\n",
      "Epoch: [144][36/48]\tTime 0.021 (0.023)\tLoss 0.8456 (0.8012)\tPrec@1 70.508 (71.843)\n",
      "Epoch: [144][45/48]\tTime 0.025 (0.023)\tLoss 0.8109 (0.8085)\tPrec@1 71.875 (71.516)\n",
      "Epoch: [144][48/48]\tTime 0.022 (0.023)\tLoss 0.9103 (0.8113)\tPrec@1 68.042 (71.394)\n",
      "EPOCH: 144 train Results: Prec@1 71.394 Loss: 0.8113\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2123 (1.2123)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2851 (1.2495)\tPrec@1 55.612 (56.730)\n",
      "EPOCH: 144 val Results: Prec@1 56.730 Loss: 1.2495\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [145][0/48]\tTime 0.029 (0.029)\tLoss 0.7820 (0.7820)\tPrec@1 73.340 (73.340)\n",
      "Epoch: [145][9/48]\tTime 0.023 (0.022)\tLoss 0.8014 (0.7776)\tPrec@1 73.535 (72.900)\n",
      "Epoch: [145][18/48]\tTime 0.018 (0.020)\tLoss 0.7514 (0.7760)\tPrec@1 73.340 (72.913)\n",
      "Epoch: [145][27/48]\tTime 0.018 (0.020)\tLoss 0.8484 (0.7885)\tPrec@1 69.531 (72.531)\n",
      "Epoch: [145][36/48]\tTime 0.021 (0.021)\tLoss 0.8156 (0.7970)\tPrec@1 70.020 (72.107)\n",
      "Epoch: [145][45/48]\tTime 0.024 (0.021)\tLoss 0.8485 (0.8055)\tPrec@1 67.285 (71.690)\n",
      "Epoch: [145][48/48]\tTime 0.011 (0.020)\tLoss 0.8681 (0.8088)\tPrec@1 68.868 (71.570)\n",
      "EPOCH: 145 train Results: Prec@1 71.570 Loss: 0.8088\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2179 (1.2179)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.2893 (1.2580)\tPrec@1 55.102 (56.290)\n",
      "EPOCH: 145 val Results: Prec@1 56.290 Loss: 1.2580\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [146][0/48]\tTime 0.023 (0.023)\tLoss 0.7538 (0.7538)\tPrec@1 73.145 (73.145)\n",
      "Epoch: [146][9/48]\tTime 0.023 (0.023)\tLoss 0.7898 (0.7636)\tPrec@1 73.145 (73.604)\n",
      "Epoch: [146][18/48]\tTime 0.028 (0.022)\tLoss 0.8020 (0.7788)\tPrec@1 71.484 (72.697)\n",
      "Epoch: [146][27/48]\tTime 0.024 (0.021)\tLoss 0.7998 (0.7856)\tPrec@1 72.461 (72.388)\n",
      "Epoch: [146][36/48]\tTime 0.040 (0.022)\tLoss 0.8497 (0.7964)\tPrec@1 70.215 (72.081)\n",
      "Epoch: [146][45/48]\tTime 0.045 (0.023)\tLoss 0.8343 (0.8053)\tPrec@1 70.996 (71.748)\n",
      "Epoch: [146][48/48]\tTime 0.015 (0.023)\tLoss 0.9125 (0.8090)\tPrec@1 66.745 (71.574)\n",
      "EPOCH: 146 train Results: Prec@1 71.574 Loss: 0.8090\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2272 (1.2272)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.3013 (1.2535)\tPrec@1 52.806 (56.610)\n",
      "EPOCH: 146 val Results: Prec@1 56.610 Loss: 1.2535\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [147][0/48]\tTime 0.030 (0.030)\tLoss 0.7507 (0.7507)\tPrec@1 72.852 (72.852)\n",
      "Epoch: [147][9/48]\tTime 0.037 (0.023)\tLoss 0.8113 (0.7597)\tPrec@1 71.289 (73.301)\n",
      "Epoch: [147][18/48]\tTime 0.033 (0.025)\tLoss 0.8453 (0.7766)\tPrec@1 68.750 (72.584)\n",
      "Epoch: [147][27/48]\tTime 0.021 (0.024)\tLoss 0.8510 (0.7825)\tPrec@1 68.750 (72.499)\n",
      "Epoch: [147][36/48]\tTime 0.020 (0.023)\tLoss 0.8420 (0.7920)\tPrec@1 70.117 (72.120)\n",
      "Epoch: [147][45/48]\tTime 0.021 (0.023)\tLoss 0.8831 (0.8014)\tPrec@1 69.824 (71.841)\n",
      "Epoch: [147][48/48]\tTime 0.024 (0.023)\tLoss 0.8993 (0.8064)\tPrec@1 69.104 (71.698)\n",
      "EPOCH: 147 train Results: Prec@1 71.698 Loss: 0.8064\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2457 (1.2457)\tPrec@1 57.324 (57.324)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.3245 (1.2739)\tPrec@1 53.699 (56.370)\n",
      "EPOCH: 147 val Results: Prec@1 56.370 Loss: 1.2739\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [148][0/48]\tTime 0.022 (0.022)\tLoss 0.7446 (0.7446)\tPrec@1 72.754 (72.754)\n",
      "Epoch: [148][9/48]\tTime 0.017 (0.027)\tLoss 0.8185 (0.7699)\tPrec@1 71.191 (73.125)\n",
      "Epoch: [148][18/48]\tTime 0.018 (0.024)\tLoss 0.8212 (0.7794)\tPrec@1 72.656 (72.965)\n",
      "Epoch: [148][27/48]\tTime 0.016 (0.024)\tLoss 0.7915 (0.7879)\tPrec@1 74.023 (72.621)\n",
      "Epoch: [148][36/48]\tTime 0.021 (0.024)\tLoss 0.8358 (0.7957)\tPrec@1 70.703 (72.250)\n",
      "Epoch: [148][45/48]\tTime 0.031 (0.025)\tLoss 0.8427 (0.8051)\tPrec@1 69.141 (71.905)\n",
      "Epoch: [148][48/48]\tTime 0.018 (0.025)\tLoss 0.7938 (0.8058)\tPrec@1 73.349 (71.842)\n",
      "EPOCH: 148 train Results: Prec@1 71.842 Loss: 0.8058\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2176 (1.2176)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2915 (1.2541)\tPrec@1 54.592 (56.610)\n",
      "EPOCH: 148 val Results: Prec@1 56.610 Loss: 1.2541\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [149][0/48]\tTime 0.022 (0.022)\tLoss 0.7549 (0.7549)\tPrec@1 74.805 (74.805)\n",
      "Epoch: [149][9/48]\tTime 0.021 (0.023)\tLoss 0.7706 (0.7619)\tPrec@1 72.656 (73.496)\n",
      "Epoch: [149][18/48]\tTime 0.015 (0.022)\tLoss 0.7875 (0.7719)\tPrec@1 72.363 (72.985)\n",
      "Epoch: [149][27/48]\tTime 0.026 (0.021)\tLoss 0.7700 (0.7816)\tPrec@1 73.926 (72.552)\n",
      "Epoch: [149][36/48]\tTime 0.022 (0.021)\tLoss 0.8179 (0.7918)\tPrec@1 70.801 (72.186)\n",
      "Epoch: [149][45/48]\tTime 0.022 (0.021)\tLoss 0.8086 (0.8037)\tPrec@1 70.898 (71.599)\n",
      "Epoch: [149][48/48]\tTime 0.028 (0.022)\tLoss 0.7974 (0.8058)\tPrec@1 69.693 (71.452)\n",
      "EPOCH: 149 train Results: Prec@1 71.452 Loss: 0.8058\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2309 (1.2309)\tPrec@1 58.594 (58.594)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.2921 (1.2689)\tPrec@1 55.357 (56.710)\n",
      "EPOCH: 149 val Results: Prec@1 56.710 Loss: 1.2689\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [150][0/48]\tTime 0.023 (0.023)\tLoss 0.7842 (0.7842)\tPrec@1 73.926 (73.926)\n",
      "Epoch: [150][9/48]\tTime 0.024 (0.023)\tLoss 0.7362 (0.7508)\tPrec@1 76.855 (74.297)\n",
      "Epoch: [150][18/48]\tTime 0.036 (0.024)\tLoss 0.7804 (0.7642)\tPrec@1 72.754 (73.617)\n",
      "Epoch: [150][27/48]\tTime 0.022 (0.023)\tLoss 0.8251 (0.7826)\tPrec@1 70.801 (72.691)\n",
      "Epoch: [150][36/48]\tTime 0.017 (0.022)\tLoss 0.8349 (0.7901)\tPrec@1 70.801 (72.376)\n",
      "Epoch: [150][45/48]\tTime 0.014 (0.021)\tLoss 0.8499 (0.7992)\tPrec@1 69.434 (72.066)\n",
      "Epoch: [150][48/48]\tTime 0.021 (0.021)\tLoss 0.8809 (0.8039)\tPrec@1 69.222 (71.888)\n",
      "EPOCH: 150 train Results: Prec@1 71.888 Loss: 0.8039\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2342 (1.2342)\tPrec@1 56.641 (56.641)\n",
      "Test: [9/9]\tTime 0.007 (0.008)\tLoss 1.2783 (1.2593)\tPrec@1 54.974 (55.900)\n",
      "EPOCH: 150 val Results: Prec@1 55.900 Loss: 1.2593\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [151][0/48]\tTime 0.023 (0.023)\tLoss 0.7667 (0.7667)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [151][9/48]\tTime 0.021 (0.028)\tLoss 0.7548 (0.7680)\tPrec@1 72.852 (73.096)\n",
      "Epoch: [151][18/48]\tTime 0.028 (0.027)\tLoss 0.7643 (0.7705)\tPrec@1 73.730 (73.109)\n",
      "Epoch: [151][27/48]\tTime 0.037 (0.026)\tLoss 0.8543 (0.7831)\tPrec@1 69.727 (72.548)\n",
      "Epoch: [151][36/48]\tTime 0.034 (0.029)\tLoss 0.8302 (0.7926)\tPrec@1 69.824 (72.136)\n",
      "Epoch: [151][45/48]\tTime 0.017 (0.028)\tLoss 0.8374 (0.8036)\tPrec@1 70.312 (71.667)\n",
      "Epoch: [151][48/48]\tTime 0.017 (0.028)\tLoss 0.8505 (0.8068)\tPrec@1 70.637 (71.546)\n",
      "EPOCH: 151 train Results: Prec@1 71.546 Loss: 0.8068\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2420 (1.2420)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.007 (0.007)\tLoss 1.2612 (1.2572)\tPrec@1 55.102 (56.800)\n",
      "EPOCH: 151 val Results: Prec@1 56.800 Loss: 1.2572\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [152][0/48]\tTime 0.020 (0.020)\tLoss 0.7622 (0.7622)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [152][9/48]\tTime 0.016 (0.023)\tLoss 0.7744 (0.7604)\tPrec@1 71.289 (73.291)\n",
      "Epoch: [152][18/48]\tTime 0.016 (0.022)\tLoss 0.7697 (0.7708)\tPrec@1 73.145 (73.252)\n",
      "Epoch: [152][27/48]\tTime 0.022 (0.021)\tLoss 0.8097 (0.7769)\tPrec@1 72.070 (72.942)\n",
      "Epoch: [152][36/48]\tTime 0.015 (0.021)\tLoss 0.8328 (0.7900)\tPrec@1 69.922 (72.281)\n",
      "Epoch: [152][45/48]\tTime 0.029 (0.022)\tLoss 0.8113 (0.8001)\tPrec@1 71.289 (71.847)\n",
      "Epoch: [152][48/48]\tTime 0.018 (0.022)\tLoss 0.8343 (0.8027)\tPrec@1 72.288 (71.768)\n",
      "EPOCH: 152 train Results: Prec@1 71.768 Loss: 0.8027\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.2200 (1.2200)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.2753 (1.2616)\tPrec@1 54.082 (56.210)\n",
      "EPOCH: 152 val Results: Prec@1 56.210 Loss: 1.2616\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [153][0/48]\tTime 0.026 (0.026)\tLoss 0.6724 (0.6724)\tPrec@1 76.660 (76.660)\n",
      "Epoch: [153][9/48]\tTime 0.022 (0.024)\tLoss 0.7667 (0.7545)\tPrec@1 72.852 (73.623)\n",
      "Epoch: [153][18/48]\tTime 0.028 (0.023)\tLoss 0.7698 (0.7645)\tPrec@1 72.949 (73.438)\n",
      "Epoch: [153][27/48]\tTime 0.021 (0.023)\tLoss 0.7762 (0.7759)\tPrec@1 71.582 (73.029)\n",
      "Epoch: [153][36/48]\tTime 0.020 (0.023)\tLoss 0.8183 (0.7878)\tPrec@1 70.215 (72.519)\n",
      "Epoch: [153][45/48]\tTime 0.020 (0.023)\tLoss 0.8821 (0.7990)\tPrec@1 69.531 (72.015)\n",
      "Epoch: [153][48/48]\tTime 0.026 (0.023)\tLoss 0.8366 (0.8031)\tPrec@1 70.637 (71.838)\n",
      "EPOCH: 153 train Results: Prec@1 71.838 Loss: 0.8031\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2221 (1.2221)\tPrec@1 58.398 (58.398)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2763 (1.2608)\tPrec@1 55.740 (56.350)\n",
      "EPOCH: 153 val Results: Prec@1 56.350 Loss: 1.2608\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [154][0/48]\tTime 0.016 (0.016)\tLoss 0.7244 (0.7244)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [154][9/48]\tTime 0.161 (0.035)\tLoss 0.7565 (0.7549)\tPrec@1 74.023 (73.799)\n",
      "Epoch: [154][18/48]\tTime 0.017 (0.028)\tLoss 0.7835 (0.7617)\tPrec@1 70.605 (73.324)\n",
      "Epoch: [154][27/48]\tTime 0.016 (0.026)\tLoss 0.7944 (0.7669)\tPrec@1 73.438 (73.148)\n",
      "Epoch: [154][36/48]\tTime 0.016 (0.025)\tLoss 0.9027 (0.7863)\tPrec@1 67.773 (72.337)\n",
      "Epoch: [154][45/48]\tTime 0.015 (0.024)\tLoss 0.8341 (0.7967)\tPrec@1 70.996 (71.900)\n",
      "Epoch: [154][48/48]\tTime 0.013 (0.023)\tLoss 0.8827 (0.7991)\tPrec@1 68.986 (71.824)\n",
      "EPOCH: 154 train Results: Prec@1 71.824 Loss: 0.7991\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2069 (1.2069)\tPrec@1 58.691 (58.691)\n",
      "Test: [9/9]\tTime 0.004 (0.005)\tLoss 1.2637 (1.2677)\tPrec@1 54.082 (56.300)\n",
      "EPOCH: 154 val Results: Prec@1 56.300 Loss: 1.2677\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [155][0/48]\tTime 0.020 (0.020)\tLoss 0.7247 (0.7247)\tPrec@1 74.121 (74.121)\n",
      "Epoch: [155][9/48]\tTime 0.022 (0.022)\tLoss 0.7657 (0.7614)\tPrec@1 72.070 (73.066)\n",
      "Epoch: [155][18/48]\tTime 0.020 (0.022)\tLoss 0.7463 (0.7650)\tPrec@1 75.098 (73.073)\n",
      "Epoch: [155][27/48]\tTime 0.015 (0.021)\tLoss 0.8306 (0.7778)\tPrec@1 71.094 (72.590)\n",
      "Epoch: [155][36/48]\tTime 0.020 (0.021)\tLoss 0.8331 (0.7898)\tPrec@1 70.020 (72.126)\n",
      "Epoch: [155][45/48]\tTime 0.015 (0.021)\tLoss 0.8278 (0.7977)\tPrec@1 70.703 (71.860)\n",
      "Epoch: [155][48/48]\tTime 0.020 (0.021)\tLoss 0.8274 (0.8007)\tPrec@1 70.873 (71.754)\n",
      "EPOCH: 155 train Results: Prec@1 71.754 Loss: 0.8007\n",
      "Test: [0/9]\tTime 0.068 (0.068)\tLoss 1.2145 (1.2145)\tPrec@1 59.375 (59.375)\n",
      "Test: [9/9]\tTime 0.005 (0.014)\tLoss 1.2826 (1.2625)\tPrec@1 55.230 (56.190)\n",
      "EPOCH: 155 val Results: Prec@1 56.190 Loss: 1.2625\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [156][0/48]\tTime 0.015 (0.015)\tLoss 0.7335 (0.7335)\tPrec@1 72.754 (72.754)\n",
      "Epoch: [156][9/48]\tTime 0.017 (0.021)\tLoss 0.7161 (0.7484)\tPrec@1 74.902 (74.004)\n",
      "Epoch: [156][18/48]\tTime 0.020 (0.021)\tLoss 0.7825 (0.7584)\tPrec@1 71.777 (73.520)\n",
      "Epoch: [156][27/48]\tTime 0.014 (0.020)\tLoss 0.7701 (0.7680)\tPrec@1 73.047 (73.298)\n",
      "Epoch: [156][36/48]\tTime 0.016 (0.020)\tLoss 0.8023 (0.7813)\tPrec@1 71.680 (72.698)\n",
      "Epoch: [156][45/48]\tTime 0.019 (0.020)\tLoss 0.8400 (0.7919)\tPrec@1 70.605 (72.232)\n",
      "Epoch: [156][48/48]\tTime 0.019 (0.020)\tLoss 0.8386 (0.7950)\tPrec@1 69.575 (72.100)\n",
      "EPOCH: 156 train Results: Prec@1 72.100 Loss: 0.7950\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2232 (1.2232)\tPrec@1 58.594 (58.594)\n",
      "Test: [9/9]\tTime 0.006 (0.004)\tLoss 1.2669 (1.2678)\tPrec@1 55.230 (56.510)\n",
      "EPOCH: 156 val Results: Prec@1 56.510 Loss: 1.2678\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [157][0/48]\tTime 0.020 (0.020)\tLoss 0.8091 (0.8091)\tPrec@1 70.703 (70.703)\n",
      "Epoch: [157][9/48]\tTime 0.022 (0.024)\tLoss 0.7611 (0.7607)\tPrec@1 73.340 (74.033)\n",
      "Epoch: [157][18/48]\tTime 0.025 (0.023)\tLoss 0.7774 (0.7674)\tPrec@1 71.289 (73.504)\n",
      "Epoch: [157][27/48]\tTime 0.017 (0.022)\tLoss 0.8560 (0.7777)\tPrec@1 70.508 (72.806)\n",
      "Epoch: [157][36/48]\tTime 0.022 (0.022)\tLoss 0.8560 (0.7870)\tPrec@1 69.434 (72.498)\n",
      "Epoch: [157][45/48]\tTime 0.020 (0.022)\tLoss 0.8305 (0.7928)\tPrec@1 71.191 (72.215)\n",
      "Epoch: [157][48/48]\tTime 0.017 (0.022)\tLoss 0.8345 (0.7964)\tPrec@1 69.104 (72.040)\n",
      "EPOCH: 157 train Results: Prec@1 72.040 Loss: 0.7964\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2220 (1.2220)\tPrec@1 59.570 (59.570)\n",
      "Test: [9/9]\tTime 0.005 (0.005)\tLoss 1.2750 (1.2697)\tPrec@1 53.571 (56.410)\n",
      "EPOCH: 157 val Results: Prec@1 56.410 Loss: 1.2697\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [158][0/48]\tTime 0.023 (0.023)\tLoss 0.7449 (0.7449)\tPrec@1 76.562 (76.562)\n",
      "Epoch: [158][9/48]\tTime 0.026 (0.024)\tLoss 0.7997 (0.7527)\tPrec@1 73.828 (74.307)\n",
      "Epoch: [158][18/48]\tTime 0.037 (0.022)\tLoss 0.8116 (0.7590)\tPrec@1 70.605 (73.710)\n",
      "Epoch: [158][27/48]\tTime 0.020 (0.023)\tLoss 0.8153 (0.7741)\tPrec@1 69.824 (72.935)\n",
      "Epoch: [158][36/48]\tTime 0.020 (0.022)\tLoss 0.8586 (0.7863)\tPrec@1 70.410 (72.421)\n",
      "Epoch: [158][45/48]\tTime 0.025 (0.024)\tLoss 0.8056 (0.7943)\tPrec@1 70.703 (72.151)\n",
      "Epoch: [158][48/48]\tTime 0.020 (0.024)\tLoss 0.8777 (0.7965)\tPrec@1 68.514 (72.080)\n",
      "EPOCH: 158 train Results: Prec@1 72.080 Loss: 0.7965\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.1942 (1.1942)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.2894 (1.2669)\tPrec@1 53.827 (56.730)\n",
      "EPOCH: 158 val Results: Prec@1 56.730 Loss: 1.2669\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [159][0/48]\tTime 0.015 (0.015)\tLoss 0.7571 (0.7571)\tPrec@1 72.559 (72.559)\n",
      "Epoch: [159][9/48]\tTime 0.022 (0.025)\tLoss 0.7492 (0.7589)\tPrec@1 73.633 (73.184)\n",
      "Epoch: [159][18/48]\tTime 0.024 (0.022)\tLoss 0.7863 (0.7679)\tPrec@1 71.777 (72.903)\n",
      "Epoch: [159][27/48]\tTime 0.018 (0.023)\tLoss 0.8742 (0.7773)\tPrec@1 69.434 (72.754)\n",
      "Epoch: [159][36/48]\tTime 0.020 (0.022)\tLoss 0.8710 (0.7843)\tPrec@1 70.312 (72.609)\n",
      "Epoch: [159][45/48]\tTime 0.025 (0.024)\tLoss 0.8408 (0.7935)\tPrec@1 70.215 (72.259)\n",
      "Epoch: [159][48/48]\tTime 0.023 (0.024)\tLoss 0.8397 (0.7980)\tPrec@1 70.401 (72.062)\n",
      "EPOCH: 159 train Results: Prec@1 72.062 Loss: 0.7980\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2186 (1.2186)\tPrec@1 57.812 (57.812)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.2859 (1.2718)\tPrec@1 54.719 (55.810)\n",
      "EPOCH: 159 val Results: Prec@1 55.810 Loss: 1.2718\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [160][0/48]\tTime 0.022 (0.022)\tLoss 0.7745 (0.7745)\tPrec@1 73.340 (73.340)\n",
      "Epoch: [160][9/48]\tTime 0.021 (0.029)\tLoss 0.7434 (0.7791)\tPrec@1 72.949 (72.520)\n",
      "Epoch: [160][18/48]\tTime 0.024 (0.025)\tLoss 0.7485 (0.7717)\tPrec@1 74.609 (72.888)\n",
      "Epoch: [160][27/48]\tTime 0.045 (0.034)\tLoss 0.7957 (0.7804)\tPrec@1 73.535 (72.695)\n",
      "Epoch: [160][36/48]\tTime 0.027 (0.033)\tLoss 0.8013 (0.7879)\tPrec@1 72.656 (72.461)\n",
      "Epoch: [160][45/48]\tTime 0.030 (0.034)\tLoss 0.7953 (0.7952)\tPrec@1 71.973 (72.193)\n",
      "Epoch: [160][48/48]\tTime 0.026 (0.034)\tLoss 0.8203 (0.7970)\tPrec@1 71.934 (72.148)\n",
      "EPOCH: 160 train Results: Prec@1 72.148 Loss: 0.7970\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2065 (1.2065)\tPrec@1 58.008 (58.008)\n",
      "Test: [9/9]\tTime 0.013 (0.012)\tLoss 1.2786 (1.2628)\tPrec@1 54.592 (56.470)\n",
      "EPOCH: 160 val Results: Prec@1 56.470 Loss: 1.2628\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [161][0/48]\tTime 0.036 (0.036)\tLoss 0.7546 (0.7546)\tPrec@1 73.633 (73.633)\n",
      "Epoch: [161][9/48]\tTime 0.029 (0.025)\tLoss 0.7087 (0.7539)\tPrec@1 77.051 (73.633)\n",
      "Epoch: [161][18/48]\tTime 0.023 (0.022)\tLoss 0.7499 (0.7637)\tPrec@1 72.656 (73.299)\n",
      "Epoch: [161][27/48]\tTime 0.030 (0.023)\tLoss 0.8373 (0.7711)\tPrec@1 70.020 (72.953)\n",
      "Epoch: [161][36/48]\tTime 0.073 (0.046)\tLoss 0.8229 (0.7799)\tPrec@1 69.336 (72.540)\n",
      "Epoch: [161][45/48]\tTime 0.042 (0.061)\tLoss 0.8454 (0.7902)\tPrec@1 70.605 (72.053)\n",
      "Epoch: [161][48/48]\tTime 0.110 (0.063)\tLoss 0.8268 (0.7929)\tPrec@1 70.047 (71.922)\n",
      "EPOCH: 161 train Results: Prec@1 71.922 Loss: 0.7929\n",
      "Test: [0/9]\tTime 0.027 (0.027)\tLoss 1.2090 (1.2090)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.024 (0.020)\tLoss 1.2841 (1.2608)\tPrec@1 53.571 (56.780)\n",
      "EPOCH: 161 val Results: Prec@1 56.780 Loss: 1.2608\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [162][0/48]\tTime 0.147 (0.147)\tLoss 0.7199 (0.7199)\tPrec@1 73.730 (73.730)\n",
      "Epoch: [162][9/48]\tTime 0.099 (0.098)\tLoss 0.7628 (0.7410)\tPrec@1 74.512 (74.785)\n",
      "Epoch: [162][18/48]\tTime 0.117 (0.104)\tLoss 0.7398 (0.7614)\tPrec@1 74.316 (73.746)\n",
      "Epoch: [162][27/48]\tTime 0.056 (0.100)\tLoss 0.7861 (0.7705)\tPrec@1 72.461 (73.207)\n",
      "Epoch: [162][36/48]\tTime 0.102 (0.099)\tLoss 0.7481 (0.7816)\tPrec@1 74.219 (72.793)\n",
      "Epoch: [162][45/48]\tTime 0.120 (0.101)\tLoss 0.8214 (0.7890)\tPrec@1 71.875 (72.497)\n",
      "Epoch: [162][48/48]\tTime 0.094 (0.101)\tLoss 0.8461 (0.7917)\tPrec@1 70.519 (72.358)\n",
      "EPOCH: 162 train Results: Prec@1 72.358 Loss: 0.7917\n",
      "Test: [0/9]\tTime 0.036 (0.036)\tLoss 1.2045 (1.2045)\tPrec@1 59.766 (59.766)\n",
      "Test: [9/9]\tTime 0.031 (0.035)\tLoss 1.2695 (1.2559)\tPrec@1 54.592 (56.830)\n",
      "EPOCH: 162 val Results: Prec@1 56.830 Loss: 1.2559\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [163][0/48]\tTime 0.123 (0.123)\tLoss 0.7358 (0.7358)\tPrec@1 73.145 (73.145)\n",
      "Epoch: [163][9/48]\tTime 0.119 (0.091)\tLoss 0.7777 (0.7520)\tPrec@1 73.242 (73.857)\n",
      "Epoch: [163][18/48]\tTime 0.189 (0.095)\tLoss 0.7246 (0.7514)\tPrec@1 73.633 (73.880)\n",
      "Epoch: [163][27/48]\tTime 0.020 (0.083)\tLoss 0.7677 (0.7593)\tPrec@1 73.535 (73.605)\n",
      "Epoch: [163][36/48]\tTime 0.020 (0.072)\tLoss 0.8525 (0.7747)\tPrec@1 71.289 (73.092)\n",
      "Epoch: [163][45/48]\tTime 0.033 (0.063)\tLoss 0.8248 (0.7869)\tPrec@1 70.312 (72.573)\n",
      "Epoch: [163][48/48]\tTime 0.019 (0.061)\tLoss 0.8218 (0.7899)\tPrec@1 70.165 (72.420)\n",
      "EPOCH: 163 train Results: Prec@1 72.420 Loss: 0.7899\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2305 (1.2305)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2792 (1.2676)\tPrec@1 56.250 (56.830)\n",
      "EPOCH: 163 val Results: Prec@1 56.830 Loss: 1.2676\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [164][0/48]\tTime 0.022 (0.022)\tLoss 0.8000 (0.8000)\tPrec@1 71.777 (71.777)\n",
      "Epoch: [164][9/48]\tTime 0.017 (0.023)\tLoss 0.7435 (0.7407)\tPrec@1 73.730 (73.867)\n",
      "Epoch: [164][18/48]\tTime 0.017 (0.021)\tLoss 0.7704 (0.7566)\tPrec@1 72.852 (73.458)\n",
      "Epoch: [164][27/48]\tTime 0.026 (0.022)\tLoss 0.8450 (0.7689)\tPrec@1 71.191 (72.883)\n",
      "Epoch: [164][36/48]\tTime 0.031 (0.022)\tLoss 0.8703 (0.7802)\tPrec@1 68.359 (72.392)\n",
      "Epoch: [164][45/48]\tTime 0.021 (0.025)\tLoss 0.8664 (0.7898)\tPrec@1 69.727 (72.015)\n",
      "Epoch: [164][48/48]\tTime 0.013 (0.024)\tLoss 0.8435 (0.7911)\tPrec@1 70.283 (72.008)\n",
      "EPOCH: 164 train Results: Prec@1 72.008 Loss: 0.7911\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2172 (1.2172)\tPrec@1 58.594 (58.594)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.2647 (1.2682)\tPrec@1 54.209 (56.570)\n",
      "EPOCH: 164 val Results: Prec@1 56.570 Loss: 1.2682\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [165][0/48]\tTime 0.024 (0.024)\tLoss 0.7327 (0.7327)\tPrec@1 74.609 (74.609)\n",
      "Epoch: [165][9/48]\tTime 0.019 (0.025)\tLoss 0.7145 (0.7380)\tPrec@1 75.586 (73.740)\n",
      "Epoch: [165][18/48]\tTime 0.015 (0.023)\tLoss 0.8413 (0.7665)\tPrec@1 69.238 (72.790)\n",
      "Epoch: [165][27/48]\tTime 0.025 (0.023)\tLoss 0.7776 (0.7662)\tPrec@1 71.094 (72.970)\n",
      "Epoch: [165][36/48]\tTime 0.016 (0.022)\tLoss 0.8081 (0.7767)\tPrec@1 70.117 (72.585)\n",
      "Epoch: [165][45/48]\tTime 0.022 (0.022)\tLoss 0.8377 (0.7862)\tPrec@1 71.094 (72.244)\n",
      "Epoch: [165][48/48]\tTime 0.022 (0.022)\tLoss 0.8736 (0.7903)\tPrec@1 69.340 (72.102)\n",
      "EPOCH: 165 train Results: Prec@1 72.102 Loss: 0.7903\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2327 (1.2327)\tPrec@1 59.277 (59.277)\n",
      "Test: [9/9]\tTime 0.008 (0.006)\tLoss 1.3029 (1.2769)\tPrec@1 55.102 (56.520)\n",
      "EPOCH: 165 val Results: Prec@1 56.520 Loss: 1.2769\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [166][0/48]\tTime 0.025 (0.025)\tLoss 0.7816 (0.7816)\tPrec@1 73.340 (73.340)\n",
      "Epoch: [166][9/48]\tTime 0.020 (0.023)\tLoss 0.6780 (0.7489)\tPrec@1 77.148 (73.926)\n",
      "Epoch: [166][18/48]\tTime 0.032 (0.024)\tLoss 0.8220 (0.7574)\tPrec@1 70.801 (73.571)\n",
      "Epoch: [166][27/48]\tTime 0.048 (0.025)\tLoss 0.8204 (0.7686)\tPrec@1 70.215 (73.103)\n",
      "Epoch: [166][36/48]\tTime 0.018 (0.024)\tLoss 0.8047 (0.7740)\tPrec@1 71.387 (72.896)\n",
      "Epoch: [166][45/48]\tTime 0.021 (0.024)\tLoss 0.8132 (0.7829)\tPrec@1 72.266 (72.563)\n",
      "Epoch: [166][48/48]\tTime 0.020 (0.024)\tLoss 0.8652 (0.7861)\tPrec@1 70.401 (72.452)\n",
      "EPOCH: 166 train Results: Prec@1 72.452 Loss: 0.7861\n",
      "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2495 (1.2495)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.005 (0.008)\tLoss 1.2823 (1.2709)\tPrec@1 54.974 (56.780)\n",
      "EPOCH: 166 val Results: Prec@1 56.780 Loss: 1.2709\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [167][0/48]\tTime 0.030 (0.030)\tLoss 0.7557 (0.7557)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [167][9/48]\tTime 0.034 (0.025)\tLoss 0.7947 (0.7551)\tPrec@1 71.973 (73.086)\n",
      "Epoch: [167][18/48]\tTime 0.022 (0.023)\tLoss 0.7627 (0.7619)\tPrec@1 73.926 (73.242)\n",
      "Epoch: [167][27/48]\tTime 0.024 (0.024)\tLoss 0.8031 (0.7709)\tPrec@1 70.996 (72.967)\n",
      "Epoch: [167][36/48]\tTime 0.032 (0.025)\tLoss 0.7764 (0.7751)\tPrec@1 71.387 (72.656)\n",
      "Epoch: [167][45/48]\tTime 0.033 (0.025)\tLoss 0.7852 (0.7847)\tPrec@1 71.191 (72.321)\n",
      "Epoch: [167][48/48]\tTime 0.017 (0.024)\tLoss 0.8339 (0.7867)\tPrec@1 68.514 (72.220)\n",
      "EPOCH: 167 train Results: Prec@1 72.220 Loss: 0.7867\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2392 (1.2392)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.004 (0.010)\tLoss 1.2949 (1.2728)\tPrec@1 53.827 (56.130)\n",
      "EPOCH: 167 val Results: Prec@1 56.130 Loss: 1.2728\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [168][0/48]\tTime 0.027 (0.027)\tLoss 0.7770 (0.7770)\tPrec@1 70.410 (70.410)\n",
      "Epoch: [168][9/48]\tTime 0.023 (0.032)\tLoss 0.7512 (0.7530)\tPrec@1 74.609 (73.262)\n",
      "Epoch: [168][18/48]\tTime 0.019 (0.027)\tLoss 0.8105 (0.7611)\tPrec@1 71.680 (73.258)\n",
      "Epoch: [168][27/48]\tTime 0.017 (0.025)\tLoss 0.8202 (0.7681)\tPrec@1 70.898 (72.907)\n",
      "Epoch: [168][36/48]\tTime 0.026 (0.025)\tLoss 0.8140 (0.7799)\tPrec@1 72.461 (72.567)\n",
      "Epoch: [168][45/48]\tTime 0.043 (0.026)\tLoss 0.8334 (0.7894)\tPrec@1 70.020 (72.210)\n",
      "Epoch: [168][48/48]\tTime 0.041 (0.027)\tLoss 0.8124 (0.7916)\tPrec@1 70.873 (72.062)\n",
      "EPOCH: 168 train Results: Prec@1 72.062 Loss: 0.7916\n",
      "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.2324 (1.2324)\tPrec@1 60.059 (60.059)\n",
      "Test: [9/9]\tTime 0.003 (0.009)\tLoss 1.2877 (1.2719)\tPrec@1 53.699 (56.390)\n",
      "EPOCH: 168 val Results: Prec@1 56.390 Loss: 1.2719\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [169][0/48]\tTime 0.047 (0.047)\tLoss 0.7028 (0.7028)\tPrec@1 76.562 (76.562)\n",
      "Epoch: [169][9/48]\tTime 0.037 (0.029)\tLoss 0.7559 (0.7393)\tPrec@1 71.973 (74.570)\n",
      "Epoch: [169][18/48]\tTime 0.022 (0.025)\tLoss 0.7448 (0.7462)\tPrec@1 73.047 (74.234)\n",
      "Epoch: [169][27/48]\tTime 0.029 (0.025)\tLoss 0.7728 (0.7572)\tPrec@1 71.387 (73.591)\n",
      "Epoch: [169][36/48]\tTime 0.027 (0.024)\tLoss 0.8297 (0.7707)\tPrec@1 72.266 (72.986)\n",
      "Epoch: [169][45/48]\tTime 0.017 (0.024)\tLoss 0.8448 (0.7833)\tPrec@1 69.629 (72.484)\n",
      "Epoch: [169][48/48]\tTime 0.019 (0.024)\tLoss 0.8435 (0.7860)\tPrec@1 69.929 (72.388)\n",
      "EPOCH: 169 train Results: Prec@1 72.388 Loss: 0.7860\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2079 (1.2079)\tPrec@1 60.449 (60.449)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2919 (1.2719)\tPrec@1 53.699 (56.420)\n",
      "EPOCH: 169 val Results: Prec@1 56.420 Loss: 1.2719\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [170][0/48]\tTime 0.021 (0.021)\tLoss 0.7971 (0.7971)\tPrec@1 71.973 (71.973)\n",
      "Epoch: [170][9/48]\tTime 0.020 (0.024)\tLoss 0.7661 (0.7479)\tPrec@1 74.219 (74.004)\n",
      "Epoch: [170][18/48]\tTime 0.023 (0.023)\tLoss 0.7380 (0.7562)\tPrec@1 74.023 (73.674)\n",
      "Epoch: [170][27/48]\tTime 0.032 (0.024)\tLoss 0.7997 (0.7668)\tPrec@1 71.875 (73.228)\n",
      "Epoch: [170][36/48]\tTime 0.028 (0.024)\tLoss 0.8142 (0.7756)\tPrec@1 71.094 (72.852)\n",
      "Epoch: [170][45/48]\tTime 0.046 (0.025)\tLoss 0.8563 (0.7870)\tPrec@1 69.531 (72.382)\n",
      "Epoch: [170][48/48]\tTime 0.022 (0.025)\tLoss 0.7767 (0.7875)\tPrec@1 72.524 (72.358)\n",
      "EPOCH: 170 train Results: Prec@1 72.358 Loss: 0.7875\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2458 (1.2458)\tPrec@1 59.766 (59.766)\n",
      "Test: [9/9]\tTime 0.005 (0.009)\tLoss 1.2668 (1.2748)\tPrec@1 55.740 (56.140)\n",
      "EPOCH: 170 val Results: Prec@1 56.140 Loss: 1.2748\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [171][0/48]\tTime 0.025 (0.025)\tLoss 0.7380 (0.7380)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [171][9/48]\tTime 0.021 (0.025)\tLoss 0.7695 (0.7416)\tPrec@1 72.852 (74.072)\n",
      "Epoch: [171][18/48]\tTime 0.060 (0.027)\tLoss 0.7670 (0.7567)\tPrec@1 73.926 (73.766)\n",
      "Epoch: [171][27/48]\tTime 0.040 (0.027)\tLoss 0.7592 (0.7642)\tPrec@1 74.707 (73.542)\n",
      "Epoch: [171][36/48]\tTime 0.037 (0.026)\tLoss 0.8320 (0.7716)\tPrec@1 69.629 (73.115)\n",
      "Epoch: [171][45/48]\tTime 0.030 (0.026)\tLoss 0.8422 (0.7805)\tPrec@1 69.922 (72.665)\n",
      "Epoch: [171][48/48]\tTime 0.028 (0.026)\tLoss 0.8420 (0.7847)\tPrec@1 71.108 (72.448)\n",
      "EPOCH: 171 train Results: Prec@1 72.448 Loss: 0.7847\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2268 (1.2268)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.2880 (1.2688)\tPrec@1 53.444 (56.300)\n",
      "EPOCH: 171 val Results: Prec@1 56.300 Loss: 1.2688\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [172][0/48]\tTime 0.022 (0.022)\tLoss 0.6949 (0.6949)\tPrec@1 76.660 (76.660)\n",
      "Epoch: [172][9/48]\tTime 0.017 (0.023)\tLoss 0.7664 (0.7402)\tPrec@1 72.754 (74.131)\n",
      "Epoch: [172][18/48]\tTime 0.020 (0.023)\tLoss 0.7593 (0.7518)\tPrec@1 71.875 (73.689)\n",
      "Epoch: [172][27/48]\tTime 0.019 (0.023)\tLoss 0.8061 (0.7613)\tPrec@1 70.215 (73.340)\n",
      "Epoch: [172][36/48]\tTime 0.020 (0.024)\tLoss 0.7840 (0.7713)\tPrec@1 72.754 (72.933)\n",
      "Epoch: [172][45/48]\tTime 0.024 (0.025)\tLoss 0.7900 (0.7787)\tPrec@1 73.047 (72.590)\n",
      "Epoch: [172][48/48]\tTime 0.021 (0.025)\tLoss 0.8591 (0.7809)\tPrec@1 69.575 (72.514)\n",
      "EPOCH: 172 train Results: Prec@1 72.514 Loss: 0.7809\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2307 (1.2307)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.008 (0.005)\tLoss 1.2630 (1.2701)\tPrec@1 54.337 (56.490)\n",
      "EPOCH: 172 val Results: Prec@1 56.490 Loss: 1.2701\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [173][0/48]\tTime 0.027 (0.027)\tLoss 0.7729 (0.7729)\tPrec@1 72.656 (72.656)\n",
      "Epoch: [173][9/48]\tTime 0.022 (0.032)\tLoss 0.7897 (0.7519)\tPrec@1 72.363 (73.975)\n",
      "Epoch: [173][18/48]\tTime 0.035 (0.031)\tLoss 0.7739 (0.7620)\tPrec@1 73.535 (73.792)\n",
      "Epoch: [173][27/48]\tTime 0.016 (0.031)\tLoss 0.7466 (0.7604)\tPrec@1 73.828 (73.741)\n",
      "Epoch: [173][36/48]\tTime 0.021 (0.029)\tLoss 0.7626 (0.7678)\tPrec@1 73.047 (73.440)\n",
      "Epoch: [173][45/48]\tTime 0.023 (0.028)\tLoss 0.7845 (0.7813)\tPrec@1 71.680 (72.703)\n",
      "Epoch: [173][48/48]\tTime 0.016 (0.030)\tLoss 0.8313 (0.7826)\tPrec@1 70.401 (72.634)\n",
      "EPOCH: 173 train Results: Prec@1 72.634 Loss: 0.7826\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2404 (1.2404)\tPrec@1 60.059 (60.059)\n",
      "Test: [9/9]\tTime 0.015 (0.007)\tLoss 1.2700 (1.2690)\tPrec@1 53.699 (56.620)\n",
      "EPOCH: 173 val Results: Prec@1 56.620 Loss: 1.2690\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [174][0/48]\tTime 0.026 (0.026)\tLoss 0.7171 (0.7171)\tPrec@1 74.512 (74.512)\n",
      "Epoch: [174][9/48]\tTime 0.028 (0.027)\tLoss 0.7368 (0.7371)\tPrec@1 73.926 (73.652)\n",
      "Epoch: [174][18/48]\tTime 0.038 (0.032)\tLoss 0.7737 (0.7469)\tPrec@1 73.438 (73.638)\n",
      "Epoch: [174][27/48]\tTime 0.042 (0.031)\tLoss 0.8242 (0.7538)\tPrec@1 71.680 (73.420)\n",
      "Epoch: [174][36/48]\tTime 0.025 (0.031)\tLoss 0.8262 (0.7639)\tPrec@1 71.289 (73.044)\n",
      "Epoch: [174][45/48]\tTime 0.029 (0.029)\tLoss 0.8957 (0.7786)\tPrec@1 68.164 (72.510)\n",
      "Epoch: [174][48/48]\tTime 0.015 (0.029)\tLoss 0.8376 (0.7820)\tPrec@1 70.873 (72.414)\n",
      "EPOCH: 174 train Results: Prec@1 72.414 Loss: 0.7820\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2039 (1.2039)\tPrec@1 59.766 (59.766)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.2703 (1.2679)\tPrec@1 55.102 (56.520)\n",
      "EPOCH: 174 val Results: Prec@1 56.520 Loss: 1.2679\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [175][0/48]\tTime 0.030 (0.030)\tLoss 0.7107 (0.7107)\tPrec@1 76.953 (76.953)\n",
      "Epoch: [175][9/48]\tTime 0.024 (0.028)\tLoss 0.7611 (0.7403)\tPrec@1 74.805 (74.014)\n",
      "Epoch: [175][18/48]\tTime 0.034 (0.027)\tLoss 0.7712 (0.7484)\tPrec@1 71.680 (73.674)\n",
      "Epoch: [175][27/48]\tTime 0.018 (0.025)\tLoss 0.8276 (0.7600)\tPrec@1 72.168 (73.357)\n",
      "Epoch: [175][36/48]\tTime 0.032 (0.026)\tLoss 0.8598 (0.7685)\tPrec@1 68.652 (72.912)\n",
      "Epoch: [175][45/48]\tTime 0.123 (0.026)\tLoss 0.8367 (0.7778)\tPrec@1 69.629 (72.535)\n",
      "Epoch: [175][48/48]\tTime 0.031 (0.027)\tLoss 0.8699 (0.7804)\tPrec@1 67.099 (72.392)\n",
      "EPOCH: 175 train Results: Prec@1 72.392 Loss: 0.7804\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.1939 (1.1939)\tPrec@1 59.473 (59.473)\n",
      "Test: [9/9]\tTime 0.007 (0.014)\tLoss 1.2728 (1.2718)\tPrec@1 53.444 (56.310)\n",
      "EPOCH: 175 val Results: Prec@1 56.310 Loss: 1.2718\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [176][0/48]\tTime 0.037 (0.037)\tLoss 0.7511 (0.7511)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [176][9/48]\tTime 0.051 (0.028)\tLoss 0.7531 (0.7416)\tPrec@1 74.023 (74.531)\n",
      "Epoch: [176][18/48]\tTime 0.032 (0.032)\tLoss 0.7348 (0.7455)\tPrec@1 74.902 (74.250)\n",
      "Epoch: [176][27/48]\tTime 0.031 (0.033)\tLoss 0.7533 (0.7540)\tPrec@1 73.926 (73.661)\n",
      "Epoch: [176][36/48]\tTime 0.025 (0.033)\tLoss 0.7951 (0.7651)\tPrec@1 72.461 (73.163)\n",
      "Epoch: [176][45/48]\tTime 0.036 (0.032)\tLoss 0.7960 (0.7753)\tPrec@1 72.461 (72.843)\n",
      "Epoch: [176][48/48]\tTime 0.033 (0.034)\tLoss 0.7708 (0.7774)\tPrec@1 72.995 (72.750)\n",
      "EPOCH: 176 train Results: Prec@1 72.750 Loss: 0.7774\n",
      "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.2239 (1.2239)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.005 (0.015)\tLoss 1.2802 (1.2755)\tPrec@1 53.827 (56.490)\n",
      "EPOCH: 176 val Results: Prec@1 56.490 Loss: 1.2755\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [177][0/48]\tTime 0.035 (0.035)\tLoss 0.7358 (0.7358)\tPrec@1 74.023 (74.023)\n",
      "Epoch: [177][9/48]\tTime 0.031 (0.045)\tLoss 0.7477 (0.7367)\tPrec@1 73.242 (74.141)\n",
      "Epoch: [177][18/48]\tTime 0.031 (0.043)\tLoss 0.7977 (0.7510)\tPrec@1 71.777 (73.797)\n",
      "Epoch: [177][27/48]\tTime 0.035 (0.042)\tLoss 0.7500 (0.7552)\tPrec@1 73.242 (73.549)\n",
      "Epoch: [177][36/48]\tTime 0.106 (0.041)\tLoss 0.7985 (0.7653)\tPrec@1 72.266 (73.100)\n",
      "Epoch: [177][45/48]\tTime 0.029 (0.039)\tLoss 0.8139 (0.7740)\tPrec@1 71.289 (72.673)\n",
      "Epoch: [177][48/48]\tTime 0.058 (0.039)\tLoss 0.8015 (0.7752)\tPrec@1 71.462 (72.644)\n",
      "EPOCH: 177 train Results: Prec@1 72.644 Loss: 0.7752\n",
      "Test: [0/9]\tTime 0.007 (0.007)\tLoss 1.2285 (1.2285)\tPrec@1 59.473 (59.473)\n",
      "Test: [9/9]\tTime 0.006 (0.014)\tLoss 1.3013 (1.2840)\tPrec@1 54.209 (56.300)\n",
      "EPOCH: 177 val Results: Prec@1 56.300 Loss: 1.2840\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [178][0/48]\tTime 0.030 (0.030)\tLoss 0.7448 (0.7448)\tPrec@1 74.121 (74.121)\n",
      "Epoch: [178][9/48]\tTime 0.027 (0.040)\tLoss 0.7443 (0.7325)\tPrec@1 73.535 (73.877)\n",
      "Epoch: [178][18/48]\tTime 0.024 (0.034)\tLoss 0.7458 (0.7453)\tPrec@1 73.145 (73.417)\n",
      "Epoch: [178][27/48]\tTime 0.046 (0.039)\tLoss 0.7462 (0.7589)\tPrec@1 72.754 (72.998)\n",
      "Epoch: [178][36/48]\tTime 0.042 (0.038)\tLoss 0.8190 (0.7704)\tPrec@1 72.363 (72.675)\n",
      "Epoch: [178][45/48]\tTime 0.040 (0.039)\tLoss 0.7827 (0.7758)\tPrec@1 72.266 (72.431)\n",
      "Epoch: [178][48/48]\tTime 0.050 (0.039)\tLoss 0.7995 (0.7784)\tPrec@1 72.170 (72.396)\n",
      "EPOCH: 178 train Results: Prec@1 72.396 Loss: 0.7784\n",
      "Test: [0/9]\tTime 0.032 (0.032)\tLoss 1.2454 (1.2454)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.028 (0.023)\tLoss 1.3120 (1.2831)\tPrec@1 51.148 (55.860)\n",
      "EPOCH: 178 val Results: Prec@1 55.860 Loss: 1.2831\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [179][0/48]\tTime 0.042 (0.042)\tLoss 0.7449 (0.7449)\tPrec@1 74.023 (74.023)\n",
      "Epoch: [179][9/48]\tTime 0.028 (0.034)\tLoss 0.7759 (0.7346)\tPrec@1 73.535 (74.912)\n",
      "Epoch: [179][18/48]\tTime 0.038 (0.038)\tLoss 0.7365 (0.7486)\tPrec@1 74.414 (74.280)\n",
      "Epoch: [179][27/48]\tTime 0.027 (0.037)\tLoss 0.8330 (0.7630)\tPrec@1 70.605 (73.580)\n",
      "Epoch: [179][36/48]\tTime 0.041 (0.035)\tLoss 0.8131 (0.7677)\tPrec@1 69.336 (73.295)\n",
      "Epoch: [179][45/48]\tTime 0.035 (0.037)\tLoss 0.8160 (0.7774)\tPrec@1 69.922 (72.699)\n",
      "Epoch: [179][48/48]\tTime 0.017 (0.036)\tLoss 0.8320 (0.7787)\tPrec@1 72.052 (72.648)\n",
      "EPOCH: 179 train Results: Prec@1 72.648 Loss: 0.7787\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2165 (1.2165)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.008 (0.007)\tLoss 1.2916 (1.2827)\tPrec@1 53.827 (55.970)\n",
      "EPOCH: 179 val Results: Prec@1 55.970 Loss: 1.2827\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [180][0/48]\tTime 0.039 (0.039)\tLoss 0.7008 (0.7008)\tPrec@1 77.148 (77.148)\n",
      "Epoch: [180][9/48]\tTime 0.043 (0.038)\tLoss 0.7857 (0.7334)\tPrec@1 73.340 (74.678)\n",
      "Epoch: [180][18/48]\tTime 0.022 (0.036)\tLoss 0.7222 (0.7418)\tPrec@1 74.023 (74.378)\n",
      "Epoch: [180][27/48]\tTime 0.080 (0.040)\tLoss 0.7579 (0.7562)\tPrec@1 72.852 (73.748)\n",
      "Epoch: [180][36/48]\tTime 0.024 (0.039)\tLoss 0.8223 (0.7692)\tPrec@1 70.312 (73.123)\n",
      "Epoch: [180][45/48]\tTime 0.024 (0.039)\tLoss 0.8623 (0.7761)\tPrec@1 68.164 (72.822)\n",
      "Epoch: [180][48/48]\tTime 0.024 (0.039)\tLoss 0.7856 (0.7785)\tPrec@1 72.877 (72.742)\n",
      "EPOCH: 180 train Results: Prec@1 72.742 Loss: 0.7785\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2183 (1.2183)\tPrec@1 58.301 (58.301)\n",
      "Test: [9/9]\tTime 0.005 (0.006)\tLoss 1.2845 (1.2775)\tPrec@1 56.505 (56.540)\n",
      "EPOCH: 180 val Results: Prec@1 56.540 Loss: 1.2775\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [181][0/48]\tTime 0.019 (0.019)\tLoss 0.6933 (0.6933)\tPrec@1 75.000 (75.000)\n",
      "Epoch: [181][9/48]\tTime 0.036 (0.035)\tLoss 0.7798 (0.7344)\tPrec@1 72.168 (74.150)\n",
      "Epoch: [181][18/48]\tTime 0.023 (0.037)\tLoss 0.8172 (0.7454)\tPrec@1 71.387 (73.823)\n",
      "Epoch: [181][27/48]\tTime 0.028 (0.033)\tLoss 0.7801 (0.7568)\tPrec@1 72.656 (73.336)\n",
      "Epoch: [181][36/48]\tTime 0.033 (0.036)\tLoss 0.8338 (0.7663)\tPrec@1 70.801 (73.028)\n",
      "Epoch: [181][45/48]\tTime 0.061 (0.038)\tLoss 0.8205 (0.7754)\tPrec@1 71.973 (72.718)\n",
      "Epoch: [181][48/48]\tTime 0.018 (0.037)\tLoss 0.8619 (0.7776)\tPrec@1 69.340 (72.592)\n",
      "EPOCH: 181 train Results: Prec@1 72.592 Loss: 0.7776\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2367 (1.2367)\tPrec@1 58.203 (58.203)\n",
      "Test: [9/9]\tTime 0.022 (0.011)\tLoss 1.2708 (1.2809)\tPrec@1 54.847 (56.350)\n",
      "EPOCH: 181 val Results: Prec@1 56.350 Loss: 1.2809\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [182][0/48]\tTime 0.031 (0.031)\tLoss 0.7197 (0.7197)\tPrec@1 75.586 (75.586)\n",
      "Epoch: [182][9/48]\tTime 0.025 (0.025)\tLoss 0.7403 (0.7294)\tPrec@1 73.730 (74.795)\n",
      "Epoch: [182][18/48]\tTime 0.031 (0.030)\tLoss 0.7585 (0.7420)\tPrec@1 73.633 (73.890)\n",
      "Epoch: [182][27/48]\tTime 0.035 (0.029)\tLoss 0.8012 (0.7569)\tPrec@1 73.633 (73.368)\n",
      "Epoch: [182][36/48]\tTime 0.025 (0.027)\tLoss 0.8022 (0.7631)\tPrec@1 70.508 (73.055)\n",
      "Epoch: [182][45/48]\tTime 0.020 (0.027)\tLoss 0.7847 (0.7732)\tPrec@1 72.559 (72.771)\n",
      "Epoch: [182][48/48]\tTime 0.021 (0.027)\tLoss 0.8489 (0.7758)\tPrec@1 70.755 (72.682)\n",
      "EPOCH: 182 train Results: Prec@1 72.682 Loss: 0.7758\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2229 (1.2229)\tPrec@1 59.766 (59.766)\n",
      "Test: [9/9]\tTime 0.011 (0.008)\tLoss 1.2889 (1.2812)\tPrec@1 53.827 (56.340)\n",
      "EPOCH: 182 val Results: Prec@1 56.340 Loss: 1.2812\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [183][0/48]\tTime 0.019 (0.019)\tLoss 0.7028 (0.7028)\tPrec@1 74.414 (74.414)\n",
      "Epoch: [183][9/48]\tTime 0.019 (0.025)\tLoss 0.7435 (0.7292)\tPrec@1 75.391 (74.746)\n",
      "Epoch: [183][18/48]\tTime 0.019 (0.024)\tLoss 0.7692 (0.7377)\tPrec@1 73.828 (74.306)\n",
      "Epoch: [183][27/48]\tTime 0.017 (0.023)\tLoss 0.7683 (0.7502)\tPrec@1 72.266 (73.640)\n",
      "Epoch: [183][36/48]\tTime 0.033 (0.030)\tLoss 0.7686 (0.7606)\tPrec@1 73.438 (73.224)\n",
      "Epoch: [183][45/48]\tTime 0.021 (0.027)\tLoss 0.8609 (0.7730)\tPrec@1 68.848 (72.854)\n",
      "Epoch: [183][48/48]\tTime 0.028 (0.027)\tLoss 0.8282 (0.7760)\tPrec@1 70.755 (72.722)\n",
      "EPOCH: 183 train Results: Prec@1 72.722 Loss: 0.7760\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2376 (1.2376)\tPrec@1 58.984 (58.984)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2730 (1.2849)\tPrec@1 55.102 (56.470)\n",
      "EPOCH: 183 val Results: Prec@1 56.470 Loss: 1.2849\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [184][0/48]\tTime 0.023 (0.023)\tLoss 0.6650 (0.6650)\tPrec@1 77.441 (77.441)\n",
      "Epoch: [184][9/48]\tTime 0.023 (0.027)\tLoss 0.6995 (0.7268)\tPrec@1 75.098 (74.648)\n",
      "Epoch: [184][18/48]\tTime 0.037 (0.029)\tLoss 0.7730 (0.7369)\tPrec@1 71.582 (74.137)\n",
      "Epoch: [184][27/48]\tTime 0.019 (0.028)\tLoss 0.8039 (0.7477)\tPrec@1 69.824 (73.779)\n",
      "Epoch: [184][36/48]\tTime 0.024 (0.027)\tLoss 0.7954 (0.7586)\tPrec@1 72.559 (73.350)\n",
      "Epoch: [184][45/48]\tTime 0.039 (0.027)\tLoss 0.7860 (0.7690)\tPrec@1 73.145 (72.911)\n",
      "Epoch: [184][48/48]\tTime 0.015 (0.027)\tLoss 0.8413 (0.7706)\tPrec@1 69.458 (72.840)\n",
      "EPOCH: 184 train Results: Prec@1 72.840 Loss: 0.7706\n",
      "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.2493 (1.2493)\tPrec@1 58.789 (58.789)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.2758 (1.2810)\tPrec@1 53.954 (56.110)\n",
      "EPOCH: 184 val Results: Prec@1 56.110 Loss: 1.2810\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [185][0/48]\tTime 0.048 (0.048)\tLoss 0.7268 (0.7268)\tPrec@1 75.293 (75.293)\n",
      "Epoch: [185][9/48]\tTime 0.017 (0.042)\tLoss 0.7842 (0.7358)\tPrec@1 71.680 (74.199)\n",
      "Epoch: [185][18/48]\tTime 0.020 (0.034)\tLoss 0.7357 (0.7390)\tPrec@1 74.707 (74.070)\n",
      "Epoch: [185][27/48]\tTime 0.019 (0.030)\tLoss 0.7825 (0.7458)\tPrec@1 71.973 (73.769)\n",
      "Epoch: [185][36/48]\tTime 0.025 (0.029)\tLoss 0.8111 (0.7533)\tPrec@1 70.703 (73.414)\n",
      "Epoch: [185][45/48]\tTime 0.031 (0.031)\tLoss 0.8609 (0.7663)\tPrec@1 67.969 (72.790)\n",
      "Epoch: [185][48/48]\tTime 0.015 (0.031)\tLoss 0.9153 (0.7705)\tPrec@1 69.575 (72.646)\n",
      "EPOCH: 185 train Results: Prec@1 72.646 Loss: 0.7705\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2535 (1.2535)\tPrec@1 57.422 (57.422)\n",
      "Test: [9/9]\tTime 0.008 (0.006)\tLoss 1.2735 (1.2855)\tPrec@1 53.954 (55.970)\n",
      "EPOCH: 185 val Results: Prec@1 55.970 Loss: 1.2855\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [186][0/48]\tTime 0.025 (0.025)\tLoss 0.7123 (0.7123)\tPrec@1 74.805 (74.805)\n",
      "Epoch: [186][9/48]\tTime 0.027 (0.024)\tLoss 0.7598 (0.7292)\tPrec@1 74.707 (74.814)\n",
      "Epoch: [186][18/48]\tTime 0.020 (0.022)\tLoss 0.7253 (0.7442)\tPrec@1 75.098 (74.018)\n",
      "Epoch: [186][27/48]\tTime 0.015 (0.023)\tLoss 0.7995 (0.7524)\tPrec@1 70.898 (73.580)\n",
      "Epoch: [186][36/48]\tTime 0.022 (0.022)\tLoss 0.8294 (0.7636)\tPrec@1 71.289 (73.150)\n",
      "Epoch: [186][45/48]\tTime 0.018 (0.022)\tLoss 0.8006 (0.7719)\tPrec@1 69.531 (72.726)\n",
      "Epoch: [186][48/48]\tTime 0.017 (0.022)\tLoss 0.7886 (0.7731)\tPrec@1 72.406 (72.666)\n",
      "EPOCH: 186 train Results: Prec@1 72.666 Loss: 0.7731\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2497 (1.2497)\tPrec@1 58.496 (58.496)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.2882 (1.2851)\tPrec@1 53.699 (56.230)\n",
      "EPOCH: 186 val Results: Prec@1 56.230 Loss: 1.2851\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [187][0/48]\tTime 0.031 (0.031)\tLoss 0.6970 (0.6970)\tPrec@1 75.684 (75.684)\n",
      "Epoch: [187][9/48]\tTime 0.023 (0.025)\tLoss 0.6696 (0.7183)\tPrec@1 77.734 (75.146)\n",
      "Epoch: [187][18/48]\tTime 0.024 (0.024)\tLoss 0.7398 (0.7302)\tPrec@1 73.633 (74.645)\n",
      "Epoch: [187][27/48]\tTime 0.025 (0.022)\tLoss 0.7299 (0.7389)\tPrec@1 75.488 (74.282)\n",
      "Epoch: [187][36/48]\tTime 0.026 (0.023)\tLoss 0.8226 (0.7524)\tPrec@1 70.117 (73.672)\n",
      "Epoch: [187][45/48]\tTime 0.023 (0.025)\tLoss 0.8365 (0.7642)\tPrec@1 69.824 (73.164)\n",
      "Epoch: [187][48/48]\tTime 0.018 (0.024)\tLoss 0.8144 (0.7664)\tPrec@1 71.108 (73.036)\n",
      "EPOCH: 187 train Results: Prec@1 73.036 Loss: 0.7664\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2133 (1.2133)\tPrec@1 59.180 (59.180)\n",
      "Test: [9/9]\tTime 0.003 (0.007)\tLoss 1.3106 (1.2816)\tPrec@1 54.719 (56.340)\n",
      "EPOCH: 187 val Results: Prec@1 56.340 Loss: 1.2816\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [188][0/48]\tTime 0.022 (0.022)\tLoss 0.7619 (0.7619)\tPrec@1 73.242 (73.242)\n",
      "Epoch: [188][9/48]\tTime 0.022 (0.026)\tLoss 0.8026 (0.7324)\tPrec@1 72.656 (74.238)\n",
      "Epoch: [188][18/48]\tTime 0.040 (0.028)\tLoss 0.7653 (0.7430)\tPrec@1 75.000 (74.070)\n",
      "Epoch: [188][27/48]\tTime 0.039 (0.028)\tLoss 0.7514 (0.7506)\tPrec@1 72.852 (73.615)\n",
      "Epoch: [188][36/48]\tTime 0.018 (0.026)\tLoss 0.7785 (0.7586)\tPrec@1 73.340 (73.263)\n",
      "Epoch: [188][45/48]\tTime 0.021 (0.025)\tLoss 0.8751 (0.7707)\tPrec@1 68.555 (72.733)\n",
      "Epoch: [188][48/48]\tTime 0.017 (0.025)\tLoss 0.8442 (0.7723)\tPrec@1 68.278 (72.658)\n",
      "EPOCH: 188 train Results: Prec@1 72.658 Loss: 0.7723\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2184 (1.2184)\tPrec@1 57.812 (57.812)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2927 (1.2912)\tPrec@1 54.082 (55.880)\n",
      "EPOCH: 188 val Results: Prec@1 55.880 Loss: 1.2912\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [189][0/48]\tTime 0.017 (0.017)\tLoss 0.7510 (0.7510)\tPrec@1 73.242 (73.242)\n",
      "Epoch: [189][9/48]\tTime 0.024 (0.024)\tLoss 0.6978 (0.7197)\tPrec@1 76.172 (75.098)\n",
      "Epoch: [189][18/48]\tTime 0.022 (0.026)\tLoss 0.7534 (0.7369)\tPrec@1 74.512 (74.419)\n",
      "Epoch: [189][27/48]\tTime 0.021 (0.025)\tLoss 0.7711 (0.7461)\tPrec@1 73.438 (73.915)\n",
      "Epoch: [189][36/48]\tTime 0.021 (0.025)\tLoss 0.8213 (0.7570)\tPrec@1 71.484 (73.395)\n",
      "Epoch: [189][45/48]\tTime 0.021 (0.025)\tLoss 0.8000 (0.7648)\tPrec@1 71.094 (73.193)\n",
      "Epoch: [189][48/48]\tTime 0.017 (0.025)\tLoss 0.8988 (0.7687)\tPrec@1 69.458 (73.062)\n",
      "EPOCH: 189 train Results: Prec@1 73.062 Loss: 0.7687\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2217 (1.2217)\tPrec@1 60.059 (60.059)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.3128 (1.2886)\tPrec@1 53.189 (56.130)\n",
      "EPOCH: 189 val Results: Prec@1 56.130 Loss: 1.2886\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [190][0/48]\tTime 0.037 (0.037)\tLoss 0.7023 (0.7023)\tPrec@1 75.879 (75.879)\n",
      "Epoch: [190][9/48]\tTime 0.020 (0.027)\tLoss 0.8175 (0.7212)\tPrec@1 71.973 (75.117)\n",
      "Epoch: [190][18/48]\tTime 0.039 (0.027)\tLoss 0.7966 (0.7361)\tPrec@1 72.656 (74.465)\n",
      "Epoch: [190][27/48]\tTime 0.024 (0.028)\tLoss 0.7923 (0.7457)\tPrec@1 71.191 (73.978)\n",
      "Epoch: [190][36/48]\tTime 0.021 (0.027)\tLoss 0.7782 (0.7557)\tPrec@1 71.387 (73.485)\n",
      "Epoch: [190][45/48]\tTime 0.022 (0.026)\tLoss 0.8133 (0.7650)\tPrec@1 70.020 (73.087)\n",
      "Epoch: [190][48/48]\tTime 0.015 (0.026)\tLoss 0.7925 (0.7686)\tPrec@1 71.462 (72.928)\n",
      "EPOCH: 190 train Results: Prec@1 72.928 Loss: 0.7686\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2297 (1.2297)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.003 (0.004)\tLoss 1.3080 (1.2861)\tPrec@1 54.209 (56.380)\n",
      "EPOCH: 190 val Results: Prec@1 56.380 Loss: 1.2861\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [191][0/48]\tTime 0.023 (0.023)\tLoss 0.7183 (0.7183)\tPrec@1 75.977 (75.977)\n",
      "Epoch: [191][9/48]\tTime 0.020 (0.022)\tLoss 0.7092 (0.7286)\tPrec@1 74.609 (74.307)\n",
      "Epoch: [191][18/48]\tTime 0.015 (0.021)\tLoss 0.7788 (0.7384)\tPrec@1 71.094 (73.967)\n",
      "Epoch: [191][27/48]\tTime 0.028 (0.023)\tLoss 0.7789 (0.7447)\tPrec@1 73.535 (73.755)\n",
      "Epoch: [191][36/48]\tTime 0.021 (0.022)\tLoss 0.7583 (0.7513)\tPrec@1 72.754 (73.522)\n",
      "Epoch: [191][45/48]\tTime 0.021 (0.023)\tLoss 0.8771 (0.7642)\tPrec@1 69.531 (73.087)\n",
      "Epoch: [191][48/48]\tTime 0.019 (0.023)\tLoss 0.9110 (0.7701)\tPrec@1 66.392 (72.820)\n",
      "EPOCH: 191 train Results: Prec@1 72.820 Loss: 0.7701\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2259 (1.2259)\tPrec@1 59.961 (59.961)\n",
      "Test: [9/9]\tTime 0.004 (0.007)\tLoss 1.2955 (1.2890)\tPrec@1 54.464 (56.040)\n",
      "EPOCH: 191 val Results: Prec@1 56.040 Loss: 1.2890\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [192][0/48]\tTime 0.021 (0.021)\tLoss 0.7014 (0.7014)\tPrec@1 76.660 (76.660)\n",
      "Epoch: [192][9/48]\tTime 0.019 (0.024)\tLoss 0.7239 (0.7176)\tPrec@1 74.707 (74.941)\n",
      "Epoch: [192][18/48]\tTime 0.015 (0.023)\tLoss 0.7738 (0.7318)\tPrec@1 75.195 (74.604)\n",
      "Epoch: [192][27/48]\tTime 0.018 (0.024)\tLoss 0.7726 (0.7422)\tPrec@1 70.898 (74.187)\n",
      "Epoch: [192][36/48]\tTime 0.021 (0.023)\tLoss 0.7617 (0.7502)\tPrec@1 74.512 (73.802)\n",
      "Epoch: [192][45/48]\tTime 0.021 (0.023)\tLoss 0.8146 (0.7602)\tPrec@1 71.191 (73.263)\n",
      "Epoch: [192][48/48]\tTime 0.019 (0.024)\tLoss 0.8021 (0.7627)\tPrec@1 70.991 (73.170)\n",
      "EPOCH: 192 train Results: Prec@1 73.170 Loss: 0.7627\n",
      "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.2236 (1.2236)\tPrec@1 59.473 (59.473)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.3081 (1.2914)\tPrec@1 54.337 (56.290)\n",
      "EPOCH: 192 val Results: Prec@1 56.290 Loss: 1.2914\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [193][0/48]\tTime 0.019 (0.019)\tLoss 0.7045 (0.7045)\tPrec@1 76.270 (76.270)\n",
      "Epoch: [193][9/48]\tTime 0.028 (0.023)\tLoss 0.7373 (0.7246)\tPrec@1 73.535 (74.287)\n",
      "Epoch: [193][18/48]\tTime 0.020 (0.022)\tLoss 0.7851 (0.7423)\tPrec@1 71.777 (73.509)\n",
      "Epoch: [193][27/48]\tTime 0.014 (0.022)\tLoss 0.7457 (0.7430)\tPrec@1 75.098 (73.685)\n",
      "Epoch: [193][36/48]\tTime 0.022 (0.022)\tLoss 0.8462 (0.7560)\tPrec@1 70.312 (73.271)\n",
      "Epoch: [193][45/48]\tTime 0.018 (0.022)\tLoss 0.8211 (0.7621)\tPrec@1 70.801 (73.017)\n",
      "Epoch: [193][48/48]\tTime 0.016 (0.022)\tLoss 0.7685 (0.7637)\tPrec@1 73.821 (72.984)\n",
      "EPOCH: 193 train Results: Prec@1 72.984 Loss: 0.7637\n",
      "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2149 (1.2149)\tPrec@1 59.863 (59.863)\n",
      "Test: [9/9]\tTime 0.003 (0.006)\tLoss 1.3008 (1.2763)\tPrec@1 53.571 (56.820)\n",
      "EPOCH: 193 val Results: Prec@1 56.820 Loss: 1.2763\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [194][0/48]\tTime 0.025 (0.025)\tLoss 0.7423 (0.7423)\tPrec@1 72.070 (72.070)\n",
      "Epoch: [194][9/48]\tTime 0.020 (0.027)\tLoss 0.7087 (0.7198)\tPrec@1 75.977 (74.951)\n",
      "Epoch: [194][18/48]\tTime 0.022 (0.024)\tLoss 0.7613 (0.7304)\tPrec@1 72.949 (74.522)\n",
      "Epoch: [194][27/48]\tTime 0.019 (0.024)\tLoss 0.7546 (0.7344)\tPrec@1 74.023 (74.250)\n",
      "Epoch: [194][36/48]\tTime 0.024 (0.023)\tLoss 0.7537 (0.7444)\tPrec@1 73.535 (73.860)\n",
      "Epoch: [194][45/48]\tTime 0.020 (0.023)\tLoss 0.8603 (0.7575)\tPrec@1 68.750 (73.329)\n",
      "Epoch: [194][48/48]\tTime 0.023 (0.023)\tLoss 0.8284 (0.7606)\tPrec@1 71.816 (73.264)\n",
      "EPOCH: 194 train Results: Prec@1 73.264 Loss: 0.7606\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2357 (1.2357)\tPrec@1 59.766 (59.766)\n",
      "Test: [9/9]\tTime 0.011 (0.006)\tLoss 1.2921 (1.2928)\tPrec@1 55.995 (56.970)\n",
      "EPOCH: 194 val Results: Prec@1 56.970 Loss: 1.2928\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [195][0/48]\tTime 0.018 (0.018)\tLoss 0.7025 (0.7025)\tPrec@1 75.293 (75.293)\n",
      "Epoch: [195][9/48]\tTime 0.019 (0.021)\tLoss 0.7177 (0.7182)\tPrec@1 73.730 (74.629)\n",
      "Epoch: [195][18/48]\tTime 0.018 (0.021)\tLoss 0.7814 (0.7329)\tPrec@1 71.777 (74.054)\n",
      "Epoch: [195][27/48]\tTime 0.023 (0.022)\tLoss 0.7483 (0.7398)\tPrec@1 74.512 (73.863)\n",
      "Epoch: [195][36/48]\tTime 0.042 (0.023)\tLoss 0.7832 (0.7505)\tPrec@1 70.996 (73.533)\n",
      "Epoch: [195][45/48]\tTime 0.045 (0.027)\tLoss 0.8291 (0.7603)\tPrec@1 70.312 (73.176)\n",
      "Epoch: [195][48/48]\tTime 0.019 (0.027)\tLoss 0.7836 (0.7634)\tPrec@1 72.642 (73.022)\n",
      "EPOCH: 195 train Results: Prec@1 73.022 Loss: 0.7634\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2416 (1.2416)\tPrec@1 57.812 (57.812)\n",
      "Test: [9/9]\tTime 0.004 (0.006)\tLoss 1.2844 (1.2825)\tPrec@1 53.061 (56.050)\n",
      "EPOCH: 195 val Results: Prec@1 56.050 Loss: 1.2825\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [196][0/48]\tTime 0.028 (0.028)\tLoss 0.6700 (0.6700)\tPrec@1 77.148 (77.148)\n",
      "Epoch: [196][9/48]\tTime 0.031 (0.025)\tLoss 0.7478 (0.7017)\tPrec@1 75.000 (75.957)\n",
      "Epoch: [196][18/48]\tTime 0.025 (0.026)\tLoss 0.6965 (0.7258)\tPrec@1 76.953 (74.800)\n",
      "Epoch: [196][27/48]\tTime 0.019 (0.025)\tLoss 0.8048 (0.7402)\tPrec@1 71.582 (74.271)\n",
      "Epoch: [196][36/48]\tTime 0.031 (0.025)\tLoss 0.8214 (0.7540)\tPrec@1 69.727 (73.564)\n",
      "Epoch: [196][45/48]\tTime 0.015 (0.027)\tLoss 0.8025 (0.7619)\tPrec@1 70.801 (73.202)\n",
      "Epoch: [196][48/48]\tTime 0.011 (0.026)\tLoss 0.7476 (0.7639)\tPrec@1 72.642 (73.090)\n",
      "EPOCH: 196 train Results: Prec@1 73.090 Loss: 0.7639\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2287 (1.2287)\tPrec@1 60.156 (60.156)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2784 (1.2885)\tPrec@1 56.122 (56.320)\n",
      "EPOCH: 196 val Results: Prec@1 56.320 Loss: 1.2885\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [197][0/48]\tTime 0.028 (0.028)\tLoss 0.7180 (0.7180)\tPrec@1 73.828 (73.828)\n",
      "Epoch: [197][9/48]\tTime 0.015 (0.023)\tLoss 0.7686 (0.7056)\tPrec@1 73.047 (74.893)\n",
      "Epoch: [197][18/48]\tTime 0.023 (0.023)\tLoss 0.7381 (0.7201)\tPrec@1 74.414 (74.579)\n",
      "Epoch: [197][27/48]\tTime 0.020 (0.026)\tLoss 0.7624 (0.7312)\tPrec@1 72.949 (74.107)\n",
      "Epoch: [197][36/48]\tTime 0.020 (0.024)\tLoss 0.7646 (0.7433)\tPrec@1 73.340 (73.614)\n",
      "Epoch: [197][45/48]\tTime 0.018 (0.024)\tLoss 0.8264 (0.7531)\tPrec@1 70.020 (73.251)\n",
      "Epoch: [197][48/48]\tTime 0.013 (0.023)\tLoss 0.8308 (0.7565)\tPrec@1 69.104 (73.108)\n",
      "EPOCH: 197 train Results: Prec@1 73.108 Loss: 0.7565\n",
      "Test: [0/9]\tTime 0.005 (0.005)\tLoss 1.2135 (1.2135)\tPrec@1 60.938 (60.938)\n",
      "Test: [9/9]\tTime 0.003 (0.005)\tLoss 1.2858 (1.2913)\tPrec@1 54.719 (56.560)\n",
      "EPOCH: 197 val Results: Prec@1 56.560 Loss: 1.2913\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [198][0/48]\tTime 0.018 (0.018)\tLoss 0.6948 (0.6948)\tPrec@1 75.293 (75.293)\n",
      "Epoch: [198][9/48]\tTime 0.016 (0.022)\tLoss 0.7907 (0.7244)\tPrec@1 71.875 (74.902)\n",
      "Epoch: [198][18/48]\tTime 0.041 (0.024)\tLoss 0.7413 (0.7314)\tPrec@1 73.535 (74.296)\n",
      "Epoch: [198][27/48]\tTime 0.023 (0.023)\tLoss 0.8560 (0.7422)\tPrec@1 68.848 (73.737)\n",
      "Epoch: [198][36/48]\tTime 0.027 (0.023)\tLoss 0.7947 (0.7526)\tPrec@1 70.312 (73.287)\n",
      "Epoch: [198][45/48]\tTime 0.034 (0.023)\tLoss 0.7988 (0.7626)\tPrec@1 71.289 (72.905)\n",
      "Epoch: [198][48/48]\tTime 0.022 (0.024)\tLoss 0.7984 (0.7660)\tPrec@1 70.165 (72.748)\n",
      "EPOCH: 198 train Results: Prec@1 72.748 Loss: 0.7660\n",
      "Test: [0/9]\tTime 0.006 (0.006)\tLoss 1.2316 (1.2316)\tPrec@1 60.156 (60.156)\n",
      "Test: [9/9]\tTime 0.007 (0.006)\tLoss 1.2865 (1.2913)\tPrec@1 56.122 (56.230)\n",
      "EPOCH: 198 val Results: Prec@1 56.230 Loss: 1.2913\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [199][0/48]\tTime 0.019 (0.019)\tLoss 0.7668 (0.7668)\tPrec@1 74.316 (74.316)\n",
      "Epoch: [199][9/48]\tTime 0.026 (0.024)\tLoss 0.7563 (0.7194)\tPrec@1 72.461 (74.658)\n",
      "Epoch: [199][18/48]\tTime 0.013 (0.022)\tLoss 0.7377 (0.7184)\tPrec@1 75.000 (74.918)\n",
      "Epoch: [199][27/48]\tTime 0.024 (0.022)\tLoss 0.7699 (0.7378)\tPrec@1 73.145 (74.191)\n",
      "Epoch: [199][36/48]\tTime 0.021 (0.022)\tLoss 0.7892 (0.7468)\tPrec@1 71.680 (73.825)\n",
      "Epoch: [199][45/48]\tTime 0.017 (0.021)\tLoss 0.8507 (0.7584)\tPrec@1 69.531 (73.389)\n",
      "Epoch: [199][48/48]\tTime 0.011 (0.021)\tLoss 0.8123 (0.7613)\tPrec@1 69.458 (73.260)\n",
      "EPOCH: 199 train Results: Prec@1 73.260 Loss: 0.7613\n",
      "Test: [0/9]\tTime 0.004 (0.004)\tLoss 1.2623 (1.2623)\tPrec@1 58.105 (58.105)\n",
      "Test: [9/9]\tTime 0.003 (0.009)\tLoss 1.2811 (1.3063)\tPrec@1 53.827 (55.810)\n",
      "EPOCH: 199 val Results: Prec@1 55.810 Loss: 1.3063\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "current lr 1.00000e-02\n",
      "Epoch: [200][0/48]\tTime 0.025 (0.025)\tLoss 0.6997 (0.6997)\tPrec@1 75.586 (75.586)\n",
      "Epoch: [200][9/48]\tTime 0.021 (0.020)\tLoss 0.7129 (0.7114)\tPrec@1 76.270 (75.498)\n",
      "Epoch: [200][18/48]\tTime 0.018 (0.020)\tLoss 0.7186 (0.7260)\tPrec@1 75.000 (74.933)\n",
      "Epoch: [200][27/48]\tTime 0.040 (0.022)\tLoss 0.7696 (0.7374)\tPrec@1 73.828 (74.393)\n",
      "Epoch: [200][36/48]\tTime 0.022 (0.025)\tLoss 0.7011 (0.7423)\tPrec@1 75.000 (74.105)\n",
      "Epoch: [200][45/48]\tTime 0.024 (0.025)\tLoss 0.7662 (0.7529)\tPrec@1 73.242 (73.599)\n",
      "Epoch: [200][48/48]\tTime 0.019 (0.025)\tLoss 0.8774 (0.7569)\tPrec@1 68.160 (73.408)\n",
      "EPOCH: 200 train Results: Prec@1 73.408 Loss: 0.7569\n",
      "Test: [0/9]\tTime 0.008 (0.008)\tLoss 1.2529 (1.2529)\tPrec@1 59.277 (59.277)\n",
      "Test: [9/9]\tTime 0.003 (0.008)\tLoss 1.3128 (1.2966)\tPrec@1 53.316 (56.060)\n",
      "EPOCH: 200 val Results: Prec@1 56.060 Loss: 1.2966\n",
      "Best Prec@1: 57.410\n",
      "\n",
      "End time:  Fri Apr  5 00:18:15 2024\n",
      "train executed in 278.6192 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_model(layers):\n",
    "    model = MLP()\n",
    "    str2obj = {\n",
    "        'linear': HiddenLayer, \n",
    "        'relu': relu, \n",
    "        'leaky_relu': leaky_relu,\n",
    "        'sigmoid': sigmoid, \n",
    "        'tanh': tanh,\n",
    "        'batchnorm': batchnorm,\n",
    "        'dropout': dropout\n",
    "    }\n",
    "    for i in layers:\n",
    "        model.add_layer(str2obj[i['type']](**i['params']))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "layers = [\n",
    "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.1}},\n",
    "    # {'type': 'sigmoid', 'params': {'name': 'sigmoid'}},  \n",
    "    {'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},  \n",
    "    #{'type': 'relu', 'params': {'name': 'relu1'}},  \n",
    "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},  \n",
    "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
    "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}}, \n",
    "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.1}},\n",
    "    {'type': 'relu', 'params': {'name': 'relu2'}}, \n",
    "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
    "]\n",
    "\n",
    "bs = 1024\n",
    "config = {\n",
    "    'layers': layers,\n",
    "    'lr': 0.01, \n",
    "    'bs': bs,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,   # 5e-4, 2e-4, 1e-4, 5e-3, 0\n",
    "    'seed': 0,\n",
    "    'epoch': 200,\n",
    "    'optimizer': 'sgd',  # adam, sgd\n",
    "    'scheduler': None, \n",
    "    'pre-process': 'norm',      # min-max, norm, None\n",
    "    'print_freq': 50000 // bs // 5\n",
    "}\n",
    "np.random.seed(config['seed'])\n",
    "\n",
    "# pre process\n",
    "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
    "\n",
    "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
    "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
    "model = get_model(config['layers'])\n",
    "trainer6 = Trainer(config, model, train_dataloader, test_dataloader)\n",
    "trainer6.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainers):\n",
    "    train_accu = [trainer.train_precs for trainer in trainers]\n",
    "    test_accu = [trainer.test_precs for trainer in trainers]\n",
    "    lrs = [16, 64, 128, 256, 512, 1024]\n",
    "    c = ['r-', 'b-', 'g-', 'y-', 'k-', 'm-']\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "\n",
    "    for i, y1 in enumerate(train_accu):\n",
    "        ax1.plot(x, y1, c[i], label='batch size ' + str(lrs[i])) \n",
    "    \n",
    "    for i, y2 in enumerate(test_accu):\n",
    "        ax2.plot(x, y2, c[i], label='batch size ' + str(lrs[i])) \n",
    "\n",
    "    ax1.set_title('Training Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "\n",
    "    ax2.set_title('Testing Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax2.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy([trainer2, trainer3, trainer, trainer4, trainer5, trainer6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_accuracy(trainers):\n",
    "    train_losses = [trainer.train_losses for trainer in trainers]\n",
    "    test_losses = [trainer.test_losses for trainer in trainers]\n",
    "    lrs = [16, 64, 128, 256, 512, 1024]\n",
    "    c = ['r-', 'b-', 'g-', 'y-', 'k-', 'm-']\n",
    "    x = [i + 1 for i in range(trainer.epochs)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 4))\n",
    "\n",
    "    for i, y1 in enumerate(train_losses):\n",
    "        ax1.plot(x, y1, c[i], label='batch size ' + str(lrs[i])) \n",
    "    \n",
    "    for i, y2 in enumerate(test_losses):\n",
    "        ax2.plot(x, y2, c[i], label='batch size ' + str(lrs[i])) \n",
    "\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "\n",
    "    ax2.set_title('Testing Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy([trainer2, trainer3, trainer, trainer4, trainer5, trainer6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
