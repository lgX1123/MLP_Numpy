{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRG7wGHIRFzl"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeK35Nb8RJnU"
      },
      "source": [
        "## How to run our code?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfzlLLKGX0J_"
      },
      "source": [
        "\"Run all\" is all you need. Around 500s to run the best model to get reproducible result in \"run!\" cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PwOrPw6YVG7"
      },
      "source": [
        "## What include?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4H3q2dXYcKi"
      },
      "source": [
        "1. A more than one hidden layer MLP.\n",
        "\n",
        "2. Activations(Relu, Leaky Relu, Sigmoid, Tanh).\n",
        "\n",
        "3. Weight decay.\n",
        "\n",
        "4. Momentum in SGD.\n",
        "\n",
        "5. Dropout.\n",
        "\n",
        "6. Softmax and cross-entropy loss.\n",
        "\n",
        "7. Mini-batch training.\n",
        "\n",
        "8. Batch Normalization.\n",
        "\n",
        "9. Other advanced operations including Kaiming init, Adam, Cosine Annealing Learning Rate Scheduler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pb_LiK5XxLa"
      },
      "source": [
        "## Best model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl8EVauDX42Q"
      },
      "source": [
        "Input ->\n",
        "\n",
        "-> Hidden layer(Fully connected layer-> BatchNorm -> dropout -> Relu activation) ->\n",
        "\n",
        "-> Hidden layer(Fully connected layer-> BatchNorm -> dropout -> Relu activation) ->\n",
        "\n",
        "-> Output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIzytcrPaqUa"
      },
      "source": [
        "# Hardware information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkGJhR_MawbS"
      },
      "source": [
        "## OS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztNmRx_aaidU",
        "outputId": "a48e9b7d-ef13-44de-a7b4-193185c9c2e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n"
          ]
        }
      ],
      "source": [
        "!cat /etc/os-release | grep PRETTY_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX68m6dJayVx"
      },
      "source": [
        "## CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI42T4QCa1uA",
        "outputId": "a37f9b00-d929-4ef1-b98e-f6bb97143988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/cpuinfo | grep \"model name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcPIG-yqa0dv"
      },
      "source": [
        "## RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROYmeJeobChi",
        "outputId": "4c9eff35-39a4-45a9-9fae-78593d6f7fca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MemTotal:       13290480 kB\n"
          ]
        }
      ],
      "source": [
        "!cat /proc/meminfo | grep MemTotal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGoRBFRatIYm"
      },
      "source": [
        "# Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d7l1lD2tIYn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrFEzGraUC0k"
      },
      "source": [
        "Version of python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjI1D0u8UCgT",
        "outputId": "d82a1051-5ea2-41f7-ab5b-fe84d510b36f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsiZrMXrT0GJ"
      },
      "source": [
        "Version of packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S9Bzn4rSFJY",
        "outputId": "d5570ac0-7637-4537-f9f5-89495a18e42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "matplotlib                       3.7.1\n",
            "numpy                            1.25.2\n",
            "scikit-learn                     1.2.2\n",
            "seaborn                          0.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep -E 'numpy |matplotlib |scikit-learn |seaborn '"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Wf949ztIYo"
      },
      "source": [
        "# Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2XewK-Kt6sB",
        "outputId": "dd20233f-2d17-4877-84a3-b0c76e9336bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-12 00:40:49--  https://drive.usercontent.google.com/download?id=1--R9BajB93t5bNaK4VMEBcVQQJGPqSYv&export=download&authuser=0&confirm=t&uuid=7d82a59a-e8bc-4f4a-9498-9d86a6e63924&at=APZUnTULQyXo28wH5AmkTCXafwHk%3A1710286096144\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 209.85.146.132, 2607:f8b0:4001:c1f::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|209.85.146.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested range not satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header='Host: drive.usercontent.google.com' --header='User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36' --header='Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' --header='Accept-Language: en-US,en;q=0.9,vi;q=0.8' --header='Cookie: SID=g.a000hAiDNMGLmlmI8WP2uNc4fkAw9NIT3CBgwOoVfDTLBxXx0VmrU6XWLNUpFLHu3JIEc4BhMwACgYKAVISAQASFQHGX2Mi2hSQ1ylGC0zPDlmcJBzUxxoVAUF8yKqjWqqmbvMg-hENYxu3t_f40076; __Secure-1PSID=g.a000hAiDNMGLmlmI8WP2uNc4fkAw9NIT3CBgwOoVfDTLBxXx0VmrqQa_qA1cQjo6BW-BYvpQwQACgYKAWQSAQASFQHGX2MixrLgKSSMs3o4Q59YBTUDMRoVAUF8yKruVNxSm4tykyFUM08OjtXQ0076; __Secure-3PSID=g.a000hAiDNMGLmlmI8WP2uNc4fkAw9NIT3CBgwOoVfDTLBxXx0Vmr85URed2zxoX9Y2j0N7eEWgACgYKAcYSAQASFQHGX2MiVw8t_A_Cgz1A2WNIJ4xmUxoVAUF8yKosoPrNVWjaKHJpTY72CPi30076; HSID=ACzx1pw3ShKANVjOn; SSID=APFHJjkBX2Ardga4C; APISID=dxNw8ece1kCo5fAO/AXxCaUXF-K7mjXEus; SAPISID=CnQJa7gSq9zp2_u0/A9PUllhZ4wSjix75d; __Secure-1PAPISID=CnQJa7gSq9zp2_u0/A9PUllhZ4wSjix75d; __Secure-3PAPISID=CnQJa7gSq9zp2_u0/A9PUllhZ4wSjix75d; AEC=Ae3NU9P31jMLEqdaxTyRIYabDTkaH3z-0TFndxK6jNbqvrG5onjGEKpAAg; NID=512=RCQW39mQpEN5eL7qADv5XDGHRBIfDjig6TwJD0w-A8gQ8YdqQJEZ-sXc2D7-0LOcwUZYnn3nLYEr104Gh37rgn1R6_Z_qfBE-HRRgG94j4TXvi48nydZibrQaejXzu-y-kuN16zUsEzs25XwV7QgWSRYvYs20Sn45IQI78iRwzWFtWO0nyCTUyISif82pAimT8ChvS58bVNMwGdL7bRawtrdbfDjcXXsihMSdGje70rQsVNqSNFkY4f9BvGUay90YTQ72aHE-DYdiiMbWGwpvzSIxueS4uWJz-yxS_x-IK2R7xq13ChfuEqik-xvSK6CQQU0bKt3oY1HZYkcEdpJ5gSJLPuXgG0tY04YMph3kYlXzzJSUxeaBMsOyU7YPHL-XjfzpkNYR297-ipjYbNDB10C6GH1w8L5Xbp2qOn6FSLySP5WUxJp1B0xaIpm3cbpLS9Ok8Me7wYeYxoRH_mA145eX-YTE1PipMY78gMAgiKDjYqo3osQ2XjF5wUPgxbs66oO3VFzwUJaqM-m8NUWu1OirP_9ufD--S7x_5nt5oauYlWiBFM0ZBS6zqPy-go7DXfMq_2A52b2PsC2xweayFP7FSXd6qZskGzf6SRkKmzCwnLVQt7QHuYgnO9s42HZYV_Na3j579mmxXcd9RXzha-ONjnyl87MdmbvC48iNHXFjuTBidu4ak-Be7ktV7hzaNc5EkRevbuEBE4sZLbCaXnq25qciTumEDR-PkJ6BkcWmSHhmt1ncpxzflDcBqnPn9-AKAKkr3-6pXF6qlqzwWUHOJnH3NBF7UonA27oz-x6p83ZgAI_HBQut8ObTikULyEN7AZl7AGysZdetlzVRGWJMxeE5sLasKc2b-YMiEu5lLrgHzCrnePW9T15JrhXHxtnApgxuqngR0jzTdPnYAwUzosBukdxKd2MM87ixRgUhx3hwAL-X2hY-8mcaLywiduBgobC7DnPuuYJp5I; 1P_JAR=2024-03-12-23; __Secure-1PSIDTS=sidts-CjIBYfD7Z-RV7pAeo2n3jdeML5aQ-RWhxljnj4V2m60vIvD8TpxpccqdF_7po3xAznp2IhAA; __Secure-3PSIDTS=sidts-CjIBYfD7Z-RV7pAeo2n3jdeML5aQ-RWhxljnj4V2m60vIvD8TpxpccqdF_7po3xAznp2IhAA; SIDCC=AKEyXzWgMDAURtbhYZZaukOmlkB0yP-68WeDLD0w9sP-YXDQSisq6R4VGwiObhKlCUyZYQrhr2_h; __Secure-1PSIDCC=AKEyXzXMSMZTuNARscTVdkkSv8kvEcLts8crm5ZvDXCcZb4bcNdLCmv7ptgFbKJT6ZMAPcrVYhc; __Secure-3PSIDCC=AKEyXzXOPe8GuBWvIMs9VJQK7vEMpnOHw1xc7HBylZEKqDUOPQPFi4i8Lgd4i5eylN2Kp0rgmGU8' --header='Connection: keep-alive' 'https://drive.usercontent.google.com/download?id=1--R9BajB93t5bNaK4VMEBcVQQJGPqSYv&export=download&authuser=0&confirm=t&uuid=7d82a59a-e8bc-4f4a-9498-9d86a6e63924&at=APZUnTULQyXo28wH5AmkTCXafwHk%3A1710286096144' -c -O 'Assignment1-Dataset.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6kmAp5buFD6",
        "outputId": "6c6df09d-f4ca-4a65-ea57-65e9baf26e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  Assignment1-Dataset.zip\n",
            "  inflating: __MACOSX/._Assignment1-Dataset  \n",
            "  inflating: Assignment1-Dataset/test_label.npy  \n",
            "  inflating: __MACOSX/Assignment1-Dataset/._test_label.npy  \n",
            "  inflating: Assignment1-Dataset/train_data.npy  \n",
            "  inflating: __MACOSX/Assignment1-Dataset/._train_data.npy  \n",
            "  inflating: Assignment1-Dataset/train_label.npy  \n",
            "  inflating: __MACOSX/Assignment1-Dataset/._train_label.npy  \n",
            "  inflating: Assignment1-Dataset/test_data.npy  \n",
            "  inflating: __MACOSX/Assignment1-Dataset/._test_data.npy  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o Assignment1-Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmSeIcCttIYo"
      },
      "outputs": [],
      "source": [
        "file_path = './Assignment1-Dataset/'\n",
        "\n",
        "train_X = np.load(file_path + 'train_data.npy')\n",
        "train_y = np.load(file_path + 'train_label.npy')\n",
        "test_X = np.load(file_path + 'test_data.npy')\n",
        "test_y = np.load(file_path + 'test_label.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj6_8NFwtIYo",
        "outputId": "eda9ba4b-52b7-4ceb-ffa3-79dace26abf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples of train set: 50000.\n",
            "Number of features: 128.\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of samples of train set: {train_X.shape[0]}.')\n",
        "print(f'Number of features: {train_X.shape[1]}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IYHrqV1tIYp",
        "outputId": "30145409-bffb-4850-e59b-62f71bc3bfd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of class: 10.\n",
            "All classes: [0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ],
      "source": [
        "print(f'Number of class: {np.unique(train_y).shape[0]}.')\n",
        "print(f'All classes: {np.unique(train_y)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGKgwN9ltIYp"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59lycXm5tIYq"
      },
      "source": [
        "## Timer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCDipE1_tIYq"
      },
      "source": [
        "A decorator for recording training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTkUZ-vHtIYq",
        "outputId": "efd84945-5c64-4694-bde1-4fe51625ce59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start time:  Fri Apr 12 00:40:51 2024\n",
            "End time:  Fri Apr 12 00:40:52 2024\n",
            "test_fun executed in 1.0006 seconds\n"
          ]
        }
      ],
      "source": [
        "def timer(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print('Start time: ', time.ctime())\n",
        "        start_time = time.time()  # start time\n",
        "\n",
        "        result = func(*args, **kwargs)  # run\n",
        "\n",
        "        end_time = time.time()  # end time\n",
        "        print('End time: ', time.ctime())\n",
        "        print(f\"{func.__name__} executed in {(end_time - start_time):.4f} seconds\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@timer\n",
        "def test_fun(x):\n",
        "    time.sleep(x)\n",
        "\n",
        "test_fun(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08-BfFoDtIYq"
      },
      "source": [
        "## Kaiming Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "illi833atIYr"
      },
      "source": [
        "Refer from https://github.com/pytorch/pytorch/blob/main/torch/nn/init.py.\n",
        "\n",
        "Modify tensor to np.array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HoB_B1KtIYr"
      },
      "outputs": [],
      "source": [
        "def calculate_gain(nonlinearity, param=None):\n",
        "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
        "    The values are as follows:\n",
        "\n",
        "    ================= ====================================================\n",
        "    nonlinearity      gain\n",
        "    ================= ====================================================\n",
        "    Linear / Identity :math:`1`\n",
        "    Conv{1,2,3}D      :math:`1`\n",
        "    Sigmoid           :math:`1`\n",
        "    Tanh              :math:`\\frac{5}{3}`\n",
        "    ReLU              :math:`\\sqrt{2}`\n",
        "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
        "    SELU              :math:`\\frac{3}{4}`\n",
        "    ================= ====================================================\n",
        "    \"\"\"\n",
        "\n",
        "    if nonlinearity == 'sigmoid':\n",
        "        return 1\n",
        "    elif nonlinearity == 'tanh':\n",
        "        return 5.0 / 3\n",
        "    elif nonlinearity == 'relu':\n",
        "        return math.sqrt(2.0)\n",
        "    elif nonlinearity == 'leaky_relu':\n",
        "        if param is None:\n",
        "            negative_slope = 0.01\n",
        "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
        "            # True/False are instances of int, hence check above\n",
        "            negative_slope = param\n",
        "        else:\n",
        "            raise ValueError(f\"negative_slope {param} not a valid number\")\n",
        "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
        "    elif nonlinearity == 'selu':\n",
        "        return 3.0 / 4  # Value found empirically (https://github.com/pytorch/pytorch/pull/50664)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported nonlinearity {nonlinearity}\")\n",
        "\n",
        "def _calculate_fan_in_and_fan_out(array):\n",
        "    dimensions = len(array.shape)\n",
        "    if dimensions < 2:\n",
        "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
        "\n",
        "    num_input_fmaps = array.shape[1]\n",
        "    num_output_fmaps = array.shape[0]\n",
        "    receptive_field_size = 1\n",
        "    if dimensions > 2:\n",
        "        # math.prod is not always available, accumulate the product manually\n",
        "        # we could use functools.reduce but that is not supported by TorchScript\n",
        "        for s in array.shape[2:]:\n",
        "            receptive_field_size *= s\n",
        "    fan_in = num_input_fmaps * receptive_field_size\n",
        "    fan_out = num_output_fmaps * receptive_field_size\n",
        "\n",
        "    return fan_in, fan_out\n",
        "\n",
        "def _calculate_correct_fan(array, mode):\n",
        "    mode = mode.lower()\n",
        "    valid_modes = ['fan_in', 'fan_out']\n",
        "    if mode not in valid_modes:\n",
        "        raise ValueError(f\"Mode {mode} not supported, please use one of {valid_modes}\")\n",
        "\n",
        "    fan_in, fan_out = _calculate_fan_in_and_fan_out(array)\n",
        "    return fan_in if mode == 'fan_in' else fan_out\n",
        "\n",
        "def kaiming_normal_(array: np.array, a: float = 0, mode: str = 'fan_in', nonlinearity: str = 'relu'):\n",
        "    fan = _calculate_correct_fan(array, mode)\n",
        "    gain = calculate_gain(nonlinearity, a)\n",
        "    std = gain / math.sqrt(fan)\n",
        "    return np.random.normal(0, std, array.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNbjDd89tIYr"
      },
      "source": [
        "## Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYsGkGdItIYs"
      },
      "outputs": [],
      "source": [
        "class Parameter(object):\n",
        "    \"\"\"Parameter class for saving data and gradients\"\"\"\n",
        "    def __init__(self, data, requires_grad, skip_decay=False):\n",
        "        self.data = data\n",
        "        self.grad = None\n",
        "        self.skip_decay = skip_decay\n",
        "        self.requires_grad = requires_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KEjTajltIYs"
      },
      "source": [
        "## AverageMeter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwGqNz0ctIYs"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-L1nsyB_tIYs"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkDB9H6ytIYt"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target):\n",
        "    \"\"\"\n",
        "        output: [batch_size, num_of_class]\n",
        "        target: [batch_size, 1]\n",
        "    \"\"\"\n",
        "    preds = output.argmax(axis=-1, keepdims=True)\n",
        "    return np.mean(preds == target) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDVi7qXFtIYt"
      },
      "source": [
        "## Pre-process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_dx8sCvtIYt"
      },
      "source": [
        "Min-max normalization:\n",
        "\n",
        "$$x_{min-max} = {{x-min(x)}\\over{max(x)-min(x)}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTERFSrgtIYt"
      },
      "source": [
        "Standardization:\n",
        "\n",
        "$$x_{norm} = {{x-\\mu}\\over{\\sigma}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yCRrmLRtIYt"
      },
      "outputs": [],
      "source": [
        "def get_transform(train_X, test_X, mode=None):\n",
        "    if mode == 'min-max':\n",
        "        print('Pre-process: min-max normalization')\n",
        "        min_each_feature = np.min(train_X, axis=0)\n",
        "        max_each_feature = np.max(train_X, axis=0)\n",
        "        scale = max_each_feature - min_each_feature\n",
        "        scale[scale == 0] = 1   # To avoid divided by 0\n",
        "        scaled_train = (train_X - min_each_feature) / scale\n",
        "        scaled_test = (test_X - min_each_feature) / scale\n",
        "        return scaled_train, scaled_test\n",
        "\n",
        "    if mode == 'standardization':\n",
        "        print('Pre-process: standardization')\n",
        "        std_each_feature = np.std(train_X, axis=0)\n",
        "        mean_each_feature = np.mean(train_X, axis=0)\n",
        "        std_each_feature[std_each_feature == 0] = 1     # To avoid divided by 0\n",
        "        norm_train = (train_X - mean_each_feature) / std_each_feature\n",
        "        norm_test = (test_X - mean_each_feature) / std_each_feature\n",
        "        return norm_train, norm_test\n",
        "\n",
        "    print('No pre-process')\n",
        "\n",
        "    return train_X, test_X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jciEZbz_tIYt"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS3w5UeKtIYu"
      },
      "source": [
        "## Base layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LilMK7rKtIYu"
      },
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        self.name = name\n",
        "        self.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, *args):\n",
        "        pass\n",
        "\n",
        "    def backward(self, *args):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAgBfLgMtIYu"
      },
      "source": [
        "## Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV7jZB22tIYu"
      },
      "source": [
        "### Relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTy4wVH6tIYu"
      },
      "source": [
        "Forward:\n",
        "$$f(x) = \\max(0, x)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x0y_XQxtIYu"
      },
      "source": [
        "Backward:\n",
        "\n",
        "$$f'(x) = \\begin{cases}\n",
        "1 & \\text{if } x > 0 \\\\\n",
        "0 & \\text{if } x \\leq 0\n",
        "\\end{cases}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU25qT50tIYv"
      },
      "outputs": [],
      "source": [
        "class relu(Layer):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.maximum(0, input)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_output[self.input <= 0] = 0\n",
        "        return grad_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF03uZTOtIYv"
      },
      "source": [
        "### Leaky Relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRFtcv9WtIYv"
      },
      "source": [
        "Forward:\n",
        "\n",
        "$$f(x) = \\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "\\alpha x & \\text{if } x \\leq 0\n",
        "\\end{cases}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OctWv0OytIYv"
      },
      "source": [
        "Backward:\n",
        "\n",
        "$$f'(x) = \\begin{cases}\n",
        "1 & \\text{if } x > 0 \\\\\n",
        "\\alpha & \\text{if } x \\leq 0\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8TVi9iGtIYw"
      },
      "outputs": [],
      "source": [
        "class leaky_relu(Layer):\n",
        "    def __init__(self, name, alpha, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        return np.where(input > 0, input, self.alpha * input)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        tmp = np.where(self.input > 0, 1, self.alpha)\n",
        "        return tmp * grad_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO8X_NSMtIYw"
      },
      "source": [
        "### Sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJGPk4cStIYw"
      },
      "source": [
        "Forward:\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rPXpw-8tIYw"
      },
      "source": [
        "Backward:\n",
        "\n",
        "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tiysl0OJtIYx"
      },
      "outputs": [],
      "source": [
        "class sigmoid(Layer):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.y = 1. / (1. + np.exp(-input))\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        return self.y * (1 - self.y) * grad_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSp1QEmZtIYx"
      },
      "source": [
        "### Tanh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoYlRvT7tIYx"
      },
      "source": [
        "Forward:\n",
        "\n",
        "$$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0tRp8v9tIYy"
      },
      "source": [
        "Backward:\n",
        "\n",
        "$$\\tanh'(x) = 1 - \\tanh^2(x)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2Z9VmWZtIYy"
      },
      "outputs": [],
      "source": [
        "class tanh(Layer):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.y = np.tanh(input)\n",
        "        return np.tanh(input)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        return (1 - self.y ** 2) * grad_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37mBJi7BtIYy"
      },
      "source": [
        "### Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_nvaO8etIYz"
      },
      "source": [
        "Forward:\n",
        "\n",
        "$$softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjc62yNjtIYz"
      },
      "outputs": [],
      "source": [
        "class softmax(Layer):\n",
        "    def __init__(self, name, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x_max = input.max(axis=-1, keepdims=True)       # to avoid overflow\n",
        "        x_exp = np.exp(input - x_max)\n",
        "        return x_exp / x_exp.sum(axis=-1, keepdims=True)\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        # packaged in CrossEntropyLoss for more convenient grad computation\n",
        "        return grad_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9kpLPAhtIYz"
      },
      "source": [
        "## Hidden layer(FC only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPdzP-k7tIYz"
      },
      "source": [
        "Forward:\n",
        "\n",
        "$$\\mathbf{y} = \\mathbf{xW} + \\mathbf{b}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t54PHENtIYz"
      },
      "source": [
        "Backward:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{W}^T$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldoSny7HtIYz"
      },
      "source": [
        "Gradient of W:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{y}} \\mathbf{x}^T$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71Zvf0otIY0"
      },
      "source": [
        "Gradient of b:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\mathbf{b}} = \\frac{\\partial L}{\\partial \\mathbf{y}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH-06hAgtIY0"
      },
      "outputs": [],
      "source": [
        "class HiddenLayer(Layer):\n",
        "    def __init__(self, name, in_num, out_num):\n",
        "        super().__init__(name, requires_grad=True)\n",
        "        self.in_num = in_num\n",
        "        self.out_num = out_num\n",
        "\n",
        "        W = kaiming_normal_(np.array([0] * in_num * out_num).reshape(in_num, out_num), a=math.sqrt(5))\n",
        "        self.W = Parameter(W, self.requires_grad)\n",
        "        self.b = Parameter(np.zeros(out_num), self.requires_grad)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "            input: [batch size, in_num]\n",
        "            W: [in_num, out_num]\n",
        "            b: [out_num]\n",
        "        \"\"\"\n",
        "        self.input = input\n",
        "        return input @ self.W.data + self.b.data      # [batch size, in_num] @ [in_num, out_num] + [out_num] => [batch size, out_num]\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "            grad_output: [batch size, out_num]\n",
        "        \"\"\"\n",
        "        batch_size = grad_output.shape[0]\n",
        "        self.W.grad = self.input.T @ grad_output / batch_size   # [in_num, batch size] @ [batch size, out_num] => [in_num, out_num],\n",
        "        self.b.grad = grad_output.sum(axis=0) / batch_size      # here divided by batch size to compute avg of gradient\n",
        "        return grad_output @ self.W.data.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSjJgcStIY0"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCPtRXqHtIY0"
      },
      "source": [
        "Cross Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQFye79_tIY0"
      },
      "source": [
        "Formula:\n",
        "\n",
        "$$CrossEntropy = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKs7s6YvtIY0"
      },
      "source": [
        "Gradient of softmax:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial z_k} = \\sum_{i}^{c} \\left( \\frac{\\partial L}{\\partial \\hat{y}_i} \\frac{\\partial \\hat{y}_i}{\\partial z_k} \\right)$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\hat{y}_i} = - \\frac{y_i}{\\hat{y}_i}, \\qquad \\frac{\\partial \\hat{y}_i}{\\partial z_k} = \\begin{cases}\n",
        "\\hat{y}_i(1 - \\hat{y}_i) & \\text{if } i = k \\\\\n",
        "-\\hat{y}_k\\hat{y}_i & \\text{if } i \\neq k\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial z_k} = - \\left( (y_k(1 - \\hat{y}_k)) - \\sum_{i \\neq k}^{c} y_i \\hat{y}_k \\right) = -(y_k - \\hat{y}_k \\sum_{i}^{c} y_i) = \\hat{y}_k - y_k\n",
        "$$\n",
        "\n",
        "$$=> \\frac{\\partial L}{\\partial z} = \\hat{y} - y$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AfkVeRbtIY0"
      },
      "outputs": [],
      "source": [
        "class CrossEntropyLoss(object):\n",
        "    def __init__(self):\n",
        "        self.softmax = softmax('softmax')\n",
        "\n",
        "    def __call__(self, input, ground_truth):\n",
        "        self.bacth_size = input.shape[0]\n",
        "        self.class_num = input.shape[1]\n",
        "\n",
        "        preds = self.softmax.forward(input)\n",
        "        ground_truth = self.one_hot_encoding(ground_truth)\n",
        "\n",
        "        self.grad = preds - ground_truth\n",
        "\n",
        "        loss = -1 * (ground_truth * np.log(preds + 1e-8)).sum() / self.bacth_size   # to avoid divided by 0\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def one_hot_encoding(self, x):\n",
        "        one_hot_encoded = np.zeros((self.bacth_size, self.class_num))\n",
        "        one_hot_encoded[np.arange(x.size), x.flatten()] = 1\n",
        "        return one_hot_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r229RxftIY0"
      },
      "source": [
        "# BatchNorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMwsv9jftIY0"
      },
      "source": [
        "Forward:\n",
        "\n",
        "$$y = \\frac{x - \\text{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\cdot \\gamma + \\beta$$\n",
        "\n",
        "At the stage of training, $\\text{E}[x]$ is the mean of a mini-batch, $\\text{Var}[x]$ is the variance of a mini-batch.\n",
        "\n",
        "At the stage of evaluation(after training on the whole train dataset), $\\text{E}[x]$ is the mean of the train dataset, and $\\text{Var}[x]$ is the variance of the train dataset. And here, we introduce running mean and running variance to compute $\\text{E}[x]$ and $\\text{Var}[x]$ of the whole training dataset for evaluation.\n",
        "\n",
        "$$\\mu_{\\text{running}} = \\alpha \\mu_{\\text{running}} + (1 - \\alpha) \\mu_B$$\n",
        "\n",
        "$$\\sigma^2_{\\text{running}} = \\alpha \\sigma^2_{\\text{running}} + (1 - \\alpha) \\sigma^2_B$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXKtN_JtIY1"
      },
      "source": [
        "Gradient of $\\gamma$:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\gamma} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\gamma} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\hat{x}_i\n",
        ", \\qquad where \\ \\ \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{(\\sigma^2)}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv-ZvSgAtIY1"
      },
      "source": [
        "Gradient of $\\beta$:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial \\beta} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\beta} = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{\\partial L}{\\partial y_i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2DFk_67tIY1"
      },
      "source": [
        "Backward:\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial x_i} + \\frac{\\partial L}{\\partial \\mu} \\frac{\\partial \\mu}{\\partial x_i} + \\frac{\\partial L}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{\\partial x_i}\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gc4hCKJtIY1"
      },
      "source": [
        "$$= \\frac{\\partial L}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial x_i} + \\frac{\\partial L}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial \\mu} \\frac{\\partial \\mu}{\\partial x_i} + \\frac{\\partial L}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{\\partial x_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1temr2FMtIY1"
      },
      "source": [
        "$$ = \\frac{\\partial L}{\\partial y_i} \\cdot \\gamma \\cdot \\frac{1}{\\sigma} -\\frac{\\gamma}{m\\sigma} \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} - \\hat{x}_i \\sum_{j=1}^{m} \\frac{\\partial f}{\\partial \\hat{x}_j} \\hat{x}_j\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4jm9mGttIY1"
      },
      "source": [
        "$$\n",
        "= \\frac{\\gamma}{\\sigma} \\left( \\frac{\\partial L}{\\partial y_i} - \\frac{\\partial L}{\\partial \\beta} - \\hat{x}_i \\frac{\\partial L}{\\partial \\gamma} \\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxU2oFt6tIY1"
      },
      "outputs": [],
      "source": [
        "class batchnorm(Layer):\n",
        "    def __init__(self, name, shape, requires_grad=True):\n",
        "        super().__init__(name)\n",
        "        self.gamma = Parameter(np.random.uniform(0.9, 1.1, shape), requires_grad, skip_decay=True)  # no weight decay\n",
        "        self.beta = Parameter(np.random.uniform(-0.1, 0.1, shape), requires_grad, skip_decay=True)  # no weight decay\n",
        "        self.requires_grad = requires_grad\n",
        "\n",
        "        self.running_mean = Parameter(np.zeros(shape), False)\n",
        "        self.running_var = Parameter(np.zeros(shape), False)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "            input: [batch size, feature shape]\n",
        "            self.gamma: [feature shape]\n",
        "            self.beta: [feature shape]\n",
        "        \"\"\"\n",
        "        if self.train:\n",
        "            batch_mean = input.mean(axis=0)\n",
        "            batch_var = input.var(axis=0)\n",
        "            batch_std = np.sqrt(batch_var + 1e-8)    # To avoid divided by 0\n",
        "\n",
        "            momentum = 0.9\n",
        "            self.running_mean.data = momentum * self.running_mean.data + (1 - momentum) * batch_mean\n",
        "            self.running_var.data = momentum * self.running_var.data + (1 - momentum) * batch_var\n",
        "\n",
        "\n",
        "        else:\n",
        "            batch_mean = self.running_mean.data\n",
        "            batch_std = np.sqrt(self.running_var.data)\n",
        "\n",
        "        self.norm = (input - batch_mean) / batch_std\n",
        "        self.gamma_norm = self.gamma.data / batch_std\n",
        "\n",
        "        return self.gamma.data * self.norm + self.beta.data\n",
        "\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        batch_size = grad_output.shape[0]\n",
        "        self.gamma.grad = (grad_output * self.norm).sum(axis=0) / batch_size\n",
        "        self.beta.grad = grad_output.sum(axis=0) / batch_size\n",
        "        return self.gamma_norm * (grad_output - self.norm * self.gamma.grad - self.beta.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIrmUdyltIY1"
      },
      "source": [
        "# Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZSFQPJWtIY1"
      },
      "source": [
        "Forward:\n",
        "\n",
        "$$mask = \\begin{cases}\n",
        "0 & \\text{if } r < p\\\\\n",
        "\\frac{1}{1 - p} & \\text{if } r \\ge p\n",
        "\\end{cases}\n",
        ", \\qquad where \\ \\ r\\sim \\text{Uniform}(0, 1)\n",
        "$$\n",
        "\n",
        "$$y = mask \\cdot x$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLXEbh_jtIY1"
      },
      "source": [
        "Backward:\n",
        "\n",
        "At the stage of training, the randomly dropouted neurals deactivate. However, at the stage of evaluation, all neurals are activated.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\mathbf{x}} = \\begin{cases}\n",
        "\\frac{\\partial L}{\\partial \\mathbf{y}} \\cdot mask & \\text{if } training\\\\\n",
        "\\frac{\\partial L}{\\partial \\mathbf{y}}  & \\text{if } evaluation\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfFrmLuetIY1"
      },
      "outputs": [],
      "source": [
        "class dropout(Layer):\n",
        "    def __init__(self, name, drop_rate, requires_grad=False):\n",
        "        super().__init__(name, requires_grad)\n",
        "        self.drop_rate = drop_rate\n",
        "        self.fix_value = 1 / (1 - self.drop_rate)   # to keep average fixed\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.train:\n",
        "            self.mask = np.random.uniform(0, 1, input.shape) > self.drop_rate\n",
        "            return input * self.mask * self.fix_value\n",
        "        else:\n",
        "            return input\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        if self.train:\n",
        "            return grad_output * self.mask\n",
        "        else:\n",
        "            return grad_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KzafIXStIY2"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b872P7FbtIY2"
      },
      "outputs": [],
      "source": [
        "class MLP(object):\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.params = []\n",
        "        self.num_layers = 0\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        if layer.requires_grad:\n",
        "            if hasattr(layer, 'W'):\n",
        "                self.params.append(layer.W)\n",
        "            if hasattr(layer, 'b'):\n",
        "                self.params.append(layer.b)\n",
        "            if hasattr(layer, 'gamma'):\n",
        "                self.params.append(layer.gamma)\n",
        "            if hasattr(layer, 'beta'):\n",
        "                self.params.append(layer.beta)\n",
        "        self.num_layers += 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, x):\n",
        "        for layer in self.layers[::-1]:\n",
        "            x = layer.backward(x)\n",
        "        return x\n",
        "\n",
        "    def train(self):\n",
        "        for layer in self.layers:\n",
        "            layer.train = True\n",
        "\n",
        "    def test(self):\n",
        "        for layer in self.layers:\n",
        "            layer.train = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGof0UZwtIY2"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHHK-VVntIY2"
      },
      "source": [
        "## SGD with Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykwCoMpitIY2"
      },
      "source": [
        "Formula:\n",
        "\n",
        "$$v_t = \\beta v_{t-1} + \\alpha g_t$$\n",
        "\n",
        "$$\\theta_t = \\theta_{t-1} - v_t$$\n",
        "\n",
        "where $\\alpha$ is the learning rate, $\\beta$ is the momentum term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kb4ODaEtIY2"
      },
      "outputs": [],
      "source": [
        "class SGD(object):\n",
        "    def __init__(self, parameters, momentum, lr, weight_decay):\n",
        "        self.parameters = parameters\n",
        "        self.momentum = momentum\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
        "\n",
        "    def step(self):\n",
        "        for i, (v, p) in enumerate(zip(self.v, self.parameters)):\n",
        "            if not p.skip_decay:\n",
        "                p.data -= self.weight_decay * p.data\n",
        "            v = self.momentum * v + self.lr * p.grad\n",
        "            self.v[i] = v\n",
        "            p.data -= self.v[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCC2k2HBtIY2"
      },
      "source": [
        "## Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsCqz1mutIY2"
      },
      "source": [
        "Formula:\n",
        "\n",
        "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
        "\n",
        "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
        "\n",
        "$$ \\text{bias correction: } \\ \\   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
        "\n",
        "$$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAI4EX7wtIY2"
      },
      "outputs": [],
      "source": [
        "class Adam(object):\n",
        "    def __init__(self, parameters, lr, weight_decay=0, beta=(0.9, 0.999), eps=1e-8):\n",
        "        self.beta1 = beta[0]\n",
        "        self.beta2 = beta[1]\n",
        "        self.lr = lr\n",
        "        self.weight_decay = weight_decay\n",
        "        self.eps = eps\n",
        "        self.parameters = parameters\n",
        "        self.m = [np.zeros(p.data.shape) for p in self.parameters]\n",
        "        self.v = [np.zeros(p.data.shape) for p in self.parameters]\n",
        "        self.iterations = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.iterations += 1\n",
        "        for i, (p, m, v) in enumerate(zip(self.parameters, self.m, self.v)):\n",
        "            if not p.skip_decay:\n",
        "                p.data -= self.weight_decay * p.data\n",
        "            m = self.beta1 * m + (1 - self.beta1) * p.grad\n",
        "            v = self.beta2 * v + (1 - self.beta2) * np.power(p.grad, 2)\n",
        "\n",
        "            self.m[i] = m\n",
        "            self.v[i] = v\n",
        "\n",
        "            # bias correction\n",
        "            m = m / (1 - np.power(self.beta1, self.iterations))\n",
        "            v = v / (1 - np.power(self.beta2, self.iterations))\n",
        "\n",
        "            p.data -= self.lr * m / (np.sqrt(v + self.eps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4o7oiv4tIY2"
      },
      "source": [
        "# Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZj9GU0WtIY3"
      },
      "source": [
        "## Cosine Annealing Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQoCT8D6tIY3"
      },
      "source": [
        "Formula:\n",
        "\n",
        "$$\\eta_t = \\frac{1}{2}\\eta_{\\text{base}} (1 + \\cos(\\frac{T_{\\text{cur}}}{T_{\\max}}\\pi))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_oej-9rtIY3"
      },
      "outputs": [],
      "source": [
        "class CosineLR(object):\n",
        "    def __init__(self, optimizer, T_max):\n",
        "        self.optimizer = optimizer\n",
        "        self.T_max = T_max\n",
        "        self.n = -1\n",
        "        self.base_lr = optimizer.lr\n",
        "        self.step()\n",
        "\n",
        "    def step(self):\n",
        "        self.n += 1\n",
        "        lr = self.get_lr()\n",
        "        self.optimizer.lr = lr\n",
        "\n",
        "    def get_lr(self):\n",
        "        cos = np.cos(np.pi * self.n / self.T_max)\n",
        "        return self.base_lr * (1 + cos) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKLpJcgitIY3"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGotxnWOtIY3"
      },
      "outputs": [],
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, config, model=None, train_loader=None, val_loader=None):\n",
        "        self.config = config\n",
        "        self.epochs = self.config['epoch']\n",
        "        self.lr = self.config['lr']\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.print_freq = self.config['print_freq']\n",
        "        self.scheduler = self.config['scheduler']\n",
        "        self.train_precs = []\n",
        "        self.test_precs = []\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "\n",
        "        self.criterion = CrossEntropyLoss()\n",
        "        if self.config['optimizer'] == 'sgd':\n",
        "            self.optimizer = SGD(self.model.params, self.config['momentum'], self.lr, self.config['weight_decay'])\n",
        "        elif self.config['optimizer'] == 'adam':\n",
        "            self.optimizer = Adam(self.model.params, self.lr, self.config['weight_decay'])\n",
        "        if self.scheduler == 'cos':\n",
        "            self.train_scheduler = CosineLR(self.optimizer, T_max=self.epochs)\n",
        "\n",
        "    @timer\n",
        "    def train(self):\n",
        "        best_acc1 = 0\n",
        "        for epoch in range(self.epochs):\n",
        "            print('current lr {:.5e}'.format(self.optimizer.lr))\n",
        "            self.train_per_epoch(epoch)\n",
        "            if self.scheduler == 'cos':\n",
        "                self.train_scheduler.step()\n",
        "\n",
        "            # evaluate on validation set\n",
        "            acc1 = self.validate(epoch)\n",
        "\n",
        "            # remember best prec@1\n",
        "            best_acc1 = max(acc1, best_acc1)\n",
        "            output_best = 'Best Prec@1: %.3f\\n' % (best_acc1)\n",
        "            print(output_best)\n",
        "\n",
        "\n",
        "    def train_per_epoch(self, epoch):\n",
        "        batch_time = AverageMeter()\n",
        "        losses = AverageMeter()\n",
        "        top1 = AverageMeter()\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        for i, (input, target) in enumerate(self.train_loader):\n",
        "            # compute output\n",
        "            output = self.model.forward(input)\n",
        "            loss = self.criterion(output, target)\n",
        "\n",
        "            # compute gradient and step\n",
        "            self.model.backward(self.criterion.grad)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output, target)\n",
        "            losses.update(loss, input.shape[0])\n",
        "            top1.update(prec1, input.shape[0])\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if (i % self.print_freq == 0) or (i == len(self.train_loader) - 1):\n",
        "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                        epoch + 1, i, len(self.train_loader) - 1, batch_time=batch_time,\n",
        "                        loss=losses, top1=top1))\n",
        "\n",
        "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='train', top1=top1, losses=losses))\n",
        "        print(output)\n",
        "        self.train_losses.append(losses.avg)\n",
        "        self.train_precs.append(top1.avg)\n",
        "\n",
        "    def validate(self, epoch):\n",
        "        batch_time = AverageMeter()\n",
        "        losses = AverageMeter()\n",
        "        top1 = AverageMeter()\n",
        "\n",
        "        self.model.test()\n",
        "\n",
        "        end = time.time()\n",
        "        for i, (input, target) in enumerate(self.val_loader):\n",
        "            # compute output\n",
        "            output = self.model.forward(input)\n",
        "            loss = self.criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output, target)\n",
        "            losses.update(loss, input.shape[0])\n",
        "            top1.update(prec1, input.shape[0])\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if (i % self.print_freq == 0) or (i == len(self.val_loader) - 1):\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                    'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                    'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                        i, len(self.val_loader) - 1, batch_time=batch_time, loss=losses,\n",
        "                        top1=top1))\n",
        "\n",
        "        output = ('EPOCH: {epoch} {flag} Results: Prec@1 {top1.avg:.3f} Loss: {losses.avg:.4f}'.format(epoch=epoch + 1 , flag='val', top1=top1, losses=losses))\n",
        "        print(output)\n",
        "        self.test_losses.append(losses.avg)\n",
        "        self.test_precs.append(top1.avg)\n",
        "\n",
        "        return top1.avg\n",
        "\n",
        "    def plot_cm(self, save_path):\n",
        "        self.model.test()\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        for i, (input, target) in enumerate(self.val_loader):\n",
        "            # compute output\n",
        "            output = self.model.forward(input)\n",
        "            output = np.argmax(output, axis=1)\n",
        "            y_pred += list(output)\n",
        "            y_true += list(target.flatten())\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure()\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.ylabel(\"Ground Truth\")\n",
        "        plt.xlabel(\"Prediction\")\n",
        "        plt.savefig(save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_qnaqvctIY3"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkU_KrrDtIY3"
      },
      "outputs": [],
      "source": [
        "class Dataloader(object):\n",
        "    def __init__(self, X, y, batch_size, shuffle=True, seed=None):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.index = np.arange(X.shape[0])\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            if self.seed is not None:\n",
        "                np.random.seed(self.seed)\n",
        "            np.random.shuffle(self.index)\n",
        "        self.n = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.n >= len(self.index):\n",
        "            raise StopIteration\n",
        "\n",
        "        index = self.index[self.n:self.n + self.batch_size]\n",
        "        batch_X = self.X[index]\n",
        "        batch_y = self.y[index]\n",
        "        self.n += self.batch_size\n",
        "\n",
        "        return batch_X, batch_y\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "            num of batch\n",
        "        \"\"\"\n",
        "        return (len(self.index) + self.batch_size - 1) // self.batch_size  # ceiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQVd2JL5tIY3"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM3bYw8DtIY3"
      },
      "source": [
        "Configs of Baseline vs Best Model\n",
        "\n",
        "| Modules                | Baseline          | Best Model        |\n",
        "| ---------------------- | ----------------- | ----------------- |\n",
        "| Batch size             | 128               | 1024              |\n",
        "| Learning rate          | 0.1               | 0.01              |\n",
        "| Scheduler              | CosineAnnealingLR | None              |\n",
        "| Epoch                  | 200               | 200               |\n",
        "| Pre-processing         | Standardization   | Standardization   |\n",
        "| Number of Hidden layer | 2                 | 2                 |\n",
        "| Hidden units           | [64, 32]          | [256, 128]        |\n",
        "| Activations            | [Relu, Relu]      | [Relu, Relu]      |\n",
        "| Weight initialisation  | Kaiming           | Kaiming           |\n",
        "| Weight decay           | 5e-4              | 5e-4              |\n",
        "| Optimizer              | SGD with Momentum | SGD with Momentum |\n",
        "| Momentum               | 0.9               | 0.9               |\n",
        "| Batch Normalisation    | Yes               | Yes               |\n",
        "| Dropout rate           | 0.1               | 0.3               |\n",
        "| Accuracy               | 53.06%            | 58.71%            |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAJiQDrIdWTm"
      },
      "source": [
        "Ablation Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccKG7O9Hdfpa"
      },
      "source": [
        "| Modules                | Best Model        | Model 1           | Model 2           | Model 3           | Model 4           |\n",
        "| ---------------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- |\n",
        "| Batch size             | 1024              | 1024             | 1024              | 1024              | 1024              |\n",
        "| Learning rate          | 0.01              | 0.01             | 0.01              | 0.01              | 0.01              |\n",
        "| Scheduler              | None              | None              |   None          | None                |  None             |\n",
        "| Epoch                  | 200               | 200               | 200               | 200               | 200               |\n",
        "| Pre-processing         | Standardization   | Standardization   | Standardization   | Standardization  |Standardization    |\n",
        "| Number of Hidden layer | 2                 | 2                 | 2                 | 2                 | 2                 |\n",
        "| Hidden units           | [256, 128]        | [256, 128]       |  [256, 128]       |  [256, 128]       |  [256, 128]       |\n",
        "| Activations            | [Relu, Relu]      |  [Relu, Relu]     |  [Relu, Relu]     |  [Relu, Relu]     |  [Relu, Relu]     |\n",
        "| Weight initialisation  | Kaiming           | Kaiming           | Kaiming           | Kaiming           | Kaiming           |\n",
        "| Weight decay           | 5e-4              | **None**          | 5e-4              |  5e-4             | 5e-4              |\n",
        "| Optimizer              | SGD with Momentum | SGD with Momentum | SGD               | SGD with Momentum | SGD with Momentum |\n",
        "| Momentum               | 0.9               | 0.9               | **None**          | 0.9               | 0.9               |\n",
        "| Batch Normalisation    | Yes               | Yes               | Yes               | **None**          | Yes               |\n",
        "| Dropout rate           | 0.3               | 0.3               |  0.3              | 0.3               | **None**          |\n",
        "| Accuracy               | 58.71%            | 53.47%            | 57.23%            | 54.79%            | 50.05%              |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj1T9YIJtIY4"
      },
      "source": [
        "## run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN18waewtIY4",
        "outputId": "2b0ee401-3873-460f-fe1a-8e5eb0397785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-process: standardization\n",
            "Start time:  Fri Apr 12 00:40:52 2024\n",
            "current lr 1.00000e-02\n",
            "Epoch: [1][0/48]\tTime 0.060 (0.060)\tLoss 6.3604 (6.3604)\tPrec@1 10.059 (10.059)\n",
            "Epoch: [1][9/48]\tTime 0.045 (0.048)\tLoss 5.7673 (6.0833)\tPrec@1 10.254 (10.771)\n",
            "Epoch: [1][18/48]\tTime 0.045 (0.048)\tLoss 4.9681 (5.7364)\tPrec@1 13.867 (11.570)\n",
            "Epoch: [1][27/48]\tTime 0.079 (0.050)\tLoss 4.5470 (5.4580)\tPrec@1 15.723 (12.266)\n",
            "Epoch: [1][36/48]\tTime 0.040 (0.053)\tLoss 4.0316 (5.2018)\tPrec@1 16.016 (12.962)\n",
            "Epoch: [1][45/48]\tTime 0.043 (0.052)\tLoss 3.9700 (4.9673)\tPrec@1 17.578 (13.827)\n",
            "Epoch: [1][48/48]\tTime 0.036 (0.051)\tLoss 3.9148 (4.9051)\tPrec@1 18.986 (14.072)\n",
            "EPOCH: 1 train Results: Prec@1 14.072 Loss: 4.9051\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 2.5207 (2.5207)\tPrec@1 23.438 (23.438)\n",
            "Test: [9/9]\tTime 0.008 (0.012)\tLoss 2.4671 (2.4695)\tPrec@1 24.235 (24.160)\n",
            "EPOCH: 1 val Results: Prec@1 24.160 Loss: 2.4695\n",
            "Best Prec@1: 24.160\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [2][0/48]\tTime 0.051 (0.051)\tLoss 3.6886 (3.6886)\tPrec@1 18.750 (18.750)\n",
            "Epoch: [2][9/48]\tTime 0.041 (0.047)\tLoss 3.5065 (3.6394)\tPrec@1 18.945 (18.467)\n",
            "Epoch: [2][18/48]\tTime 0.041 (0.045)\tLoss 3.3405 (3.5276)\tPrec@1 21.387 (18.930)\n",
            "Epoch: [2][27/48]\tTime 0.043 (0.045)\tLoss 3.1686 (3.4438)\tPrec@1 21.484 (19.413)\n",
            "Epoch: [2][36/48]\tTime 0.041 (0.045)\tLoss 3.1179 (3.3656)\tPrec@1 20.996 (19.811)\n",
            "Epoch: [2][45/48]\tTime 0.124 (0.057)\tLoss 2.9688 (3.2950)\tPrec@1 21.484 (20.119)\n",
            "Epoch: [2][48/48]\tTime 0.070 (0.059)\tLoss 2.9902 (3.2750)\tPrec@1 23.821 (20.266)\n",
            "EPOCH: 2 train Results: Prec@1 20.266 Loss: 3.2750\n",
            "Test: [0/9]\tTime 0.020 (0.020)\tLoss 2.1158 (2.1158)\tPrec@1 29.492 (29.492)\n",
            "Test: [9/9]\tTime 0.015 (0.020)\tLoss 2.0517 (2.0555)\tPrec@1 30.740 (30.110)\n",
            "EPOCH: 2 val Results: Prec@1 30.110 Loss: 2.0555\n",
            "Best Prec@1: 30.110\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [3][0/48]\tTime 0.096 (0.096)\tLoss 2.7495 (2.7495)\tPrec@1 24.023 (24.023)\n",
            "Epoch: [3][9/48]\tTime 0.139 (0.101)\tLoss 2.8650 (2.8634)\tPrec@1 22.656 (23.057)\n",
            "Epoch: [3][18/48]\tTime 0.097 (0.100)\tLoss 2.7034 (2.8145)\tPrec@1 24.121 (23.047)\n",
            "Epoch: [3][27/48]\tTime 0.086 (0.094)\tLoss 2.7192 (2.7798)\tPrec@1 22.363 (23.200)\n",
            "Epoch: [3][36/48]\tTime 0.136 (0.105)\tLoss 2.6361 (2.7456)\tPrec@1 24.219 (23.321)\n",
            "Epoch: [3][45/48]\tTime 0.131 (0.110)\tLoss 2.5153 (2.7097)\tPrec@1 24.805 (23.654)\n",
            "Epoch: [3][48/48]\tTime 0.107 (0.113)\tLoss 2.5097 (2.6996)\tPrec@1 25.590 (23.714)\n",
            "EPOCH: 3 train Results: Prec@1 23.714 Loss: 2.6996\n",
            "Test: [0/9]\tTime 0.035 (0.035)\tLoss 1.9692 (1.9692)\tPrec@1 33.008 (33.008)\n",
            "Test: [9/9]\tTime 0.045 (0.030)\tLoss 1.9056 (1.9110)\tPrec@1 33.036 (32.960)\n",
            "EPOCH: 3 val Results: Prec@1 32.960 Loss: 1.9110\n",
            "Best Prec@1: 32.960\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [4][0/48]\tTime 0.147 (0.147)\tLoss 2.4991 (2.4991)\tPrec@1 25.195 (25.195)\n",
            "Epoch: [4][9/48]\tTime 0.092 (0.130)\tLoss 2.4951 (2.4789)\tPrec@1 25.586 (25.283)\n",
            "Epoch: [4][18/48]\tTime 0.091 (0.114)\tLoss 2.4547 (2.4551)\tPrec@1 25.195 (25.601)\n",
            "Epoch: [4][27/48]\tTime 0.080 (0.108)\tLoss 2.2913 (2.4244)\tPrec@1 26.953 (25.820)\n",
            "Epoch: [4][36/48]\tTime 0.076 (0.103)\tLoss 2.3667 (2.4048)\tPrec@1 24.707 (25.757)\n",
            "Epoch: [4][45/48]\tTime 0.140 (0.101)\tLoss 2.3214 (2.3866)\tPrec@1 27.051 (25.860)\n",
            "Epoch: [4][48/48]\tTime 0.083 (0.102)\tLoss 2.1949 (2.3794)\tPrec@1 27.712 (25.922)\n",
            "EPOCH: 4 train Results: Prec@1 25.922 Loss: 2.3794\n",
            "Test: [0/9]\tTime 0.043 (0.043)\tLoss 1.8910 (1.8910)\tPrec@1 34.277 (34.277)\n",
            "Test: [9/9]\tTime 0.010 (0.022)\tLoss 1.8292 (1.8369)\tPrec@1 35.459 (35.020)\n",
            "EPOCH: 4 val Results: Prec@1 35.020 Loss: 1.8369\n",
            "Best Prec@1: 35.020\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [5][0/48]\tTime 0.115 (0.115)\tLoss 2.3123 (2.3123)\tPrec@1 24.023 (24.023)\n",
            "Epoch: [5][9/48]\tTime 0.111 (0.113)\tLoss 2.2414 (2.2464)\tPrec@1 27.344 (26.855)\n",
            "Epoch: [5][18/48]\tTime 0.103 (0.107)\tLoss 2.2352 (2.2446)\tPrec@1 25.293 (26.886)\n",
            "Epoch: [5][27/48]\tTime 0.088 (0.108)\tLoss 2.2065 (2.2325)\tPrec@1 25.977 (27.138)\n",
            "Epoch: [5][36/48]\tTime 0.090 (0.103)\tLoss 2.1471 (2.2232)\tPrec@1 26.758 (27.122)\n",
            "Epoch: [5][45/48]\tTime 0.063 (0.101)\tLoss 2.2061 (2.2101)\tPrec@1 26.758 (27.312)\n",
            "Epoch: [5][48/48]\tTime 0.073 (0.100)\tLoss 2.1916 (2.2068)\tPrec@1 29.835 (27.422)\n",
            "EPOCH: 5 train Results: Prec@1 27.422 Loss: 2.2068\n",
            "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.8424 (1.8424)\tPrec@1 35.156 (35.156)\n",
            "Test: [9/9]\tTime 0.015 (0.019)\tLoss 1.7869 (1.7938)\tPrec@1 36.352 (36.330)\n",
            "EPOCH: 5 val Results: Prec@1 36.330 Loss: 1.7938\n",
            "Best Prec@1: 36.330\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [6][0/48]\tTime 0.086 (0.086)\tLoss 2.1191 (2.1191)\tPrec@1 29.883 (29.883)\n",
            "Epoch: [6][9/48]\tTime 0.163 (0.094)\tLoss 2.0729 (2.1022)\tPrec@1 29.492 (29.160)\n",
            "Epoch: [6][18/48]\tTime 0.092 (0.113)\tLoss 2.0343 (2.0906)\tPrec@1 29.980 (29.323)\n",
            "Epoch: [6][27/48]\tTime 0.125 (0.120)\tLoss 2.0500 (2.0858)\tPrec@1 29.492 (29.360)\n",
            "Epoch: [6][36/48]\tTime 0.110 (0.118)\tLoss 2.0118 (2.0752)\tPrec@1 31.152 (29.426)\n",
            "Epoch: [6][45/48]\tTime 0.050 (0.112)\tLoss 2.0316 (2.0680)\tPrec@1 31.055 (29.445)\n",
            "Epoch: [6][48/48]\tTime 0.068 (0.109)\tLoss 2.0699 (2.0657)\tPrec@1 26.887 (29.396)\n",
            "EPOCH: 6 train Results: Prec@1 29.396 Loss: 2.0657\n",
            "Test: [0/9]\tTime 0.036 (0.036)\tLoss 1.8076 (1.8076)\tPrec@1 35.645 (35.645)\n",
            "Test: [9/9]\tTime 0.017 (0.024)\tLoss 1.7571 (1.7651)\tPrec@1 36.607 (37.220)\n",
            "EPOCH: 6 val Results: Prec@1 37.220 Loss: 1.7651\n",
            "Best Prec@1: 37.220\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [7][0/48]\tTime 0.115 (0.115)\tLoss 1.9746 (1.9746)\tPrec@1 31.836 (31.836)\n",
            "Epoch: [7][9/48]\tTime 0.108 (0.105)\tLoss 1.9601 (2.0173)\tPrec@1 30.273 (30.146)\n",
            "Epoch: [7][18/48]\tTime 0.063 (0.100)\tLoss 1.9781 (2.0149)\tPrec@1 29.004 (29.893)\n",
            "Epoch: [7][27/48]\tTime 0.071 (0.102)\tLoss 1.9554 (2.0029)\tPrec@1 32.715 (30.315)\n",
            "Epoch: [7][36/48]\tTime 0.077 (0.101)\tLoss 1.9343 (1.9905)\tPrec@1 31.250 (30.566)\n",
            "Epoch: [7][45/48]\tTime 0.172 (0.100)\tLoss 1.9163 (1.9861)\tPrec@1 33.008 (30.692)\n",
            "Epoch: [7][48/48]\tTime 0.094 (0.100)\tLoss 2.0358 (1.9875)\tPrec@1 28.892 (30.666)\n",
            "EPOCH: 7 train Results: Prec@1 30.666 Loss: 1.9875\n",
            "Test: [0/9]\tTime 0.036 (0.036)\tLoss 1.7816 (1.7816)\tPrec@1 36.523 (36.523)\n",
            "Test: [9/9]\tTime 0.047 (0.028)\tLoss 1.7362 (1.7439)\tPrec@1 37.500 (38.320)\n",
            "EPOCH: 7 val Results: Prec@1 38.320 Loss: 1.7439\n",
            "Best Prec@1: 38.320\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [8][0/48]\tTime 0.145 (0.145)\tLoss 1.9263 (1.9263)\tPrec@1 31.738 (31.738)\n",
            "Epoch: [8][9/48]\tTime 0.109 (0.094)\tLoss 1.9718 (1.9574)\tPrec@1 32.520 (31.807)\n",
            "Epoch: [8][18/48]\tTime 0.110 (0.099)\tLoss 1.9030 (1.9548)\tPrec@1 34.375 (31.363)\n",
            "Epoch: [8][27/48]\tTime 0.107 (0.098)\tLoss 1.9738 (1.9464)\tPrec@1 30.469 (31.522)\n",
            "Epoch: [8][36/48]\tTime 0.144 (0.107)\tLoss 1.9328 (1.9365)\tPrec@1 31.445 (31.617)\n",
            "Epoch: [8][45/48]\tTime 0.155 (0.117)\tLoss 1.9202 (1.9346)\tPrec@1 32.129 (31.660)\n",
            "Epoch: [8][48/48]\tTime 0.128 (0.119)\tLoss 1.9040 (1.9320)\tPrec@1 33.491 (31.666)\n",
            "EPOCH: 8 train Results: Prec@1 31.666 Loss: 1.9320\n",
            "Test: [0/9]\tTime 0.060 (0.060)\tLoss 1.7614 (1.7614)\tPrec@1 37.598 (37.598)\n",
            "Test: [9/9]\tTime 0.043 (0.042)\tLoss 1.7181 (1.7259)\tPrec@1 39.158 (39.110)\n",
            "EPOCH: 8 val Results: Prec@1 39.110 Loss: 1.7259\n",
            "Best Prec@1: 39.110\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [9][0/48]\tTime 0.098 (0.098)\tLoss 1.8445 (1.8445)\tPrec@1 34.375 (34.375)\n",
            "Epoch: [9][9/48]\tTime 0.044 (0.051)\tLoss 1.8723 (1.8867)\tPrec@1 32.129 (32.090)\n",
            "Epoch: [9][18/48]\tTime 0.050 (0.049)\tLoss 1.9008 (1.8869)\tPrec@1 33.398 (32.874)\n",
            "Epoch: [9][27/48]\tTime 0.040 (0.046)\tLoss 1.8607 (1.8887)\tPrec@1 34.668 (32.840)\n",
            "Epoch: [9][36/48]\tTime 0.041 (0.046)\tLoss 1.8798 (1.8858)\tPrec@1 32.520 (32.921)\n",
            "Epoch: [9][45/48]\tTime 0.043 (0.045)\tLoss 1.9070 (1.8830)\tPrec@1 31.445 (32.863)\n",
            "Epoch: [9][48/48]\tTime 0.038 (0.045)\tLoss 1.8762 (1.8815)\tPrec@1 32.429 (32.888)\n",
            "EPOCH: 9 train Results: Prec@1 32.888 Loss: 1.8815\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.7434 (1.7434)\tPrec@1 38.867 (38.867)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.7035 (1.7107)\tPrec@1 40.051 (39.630)\n",
            "EPOCH: 9 val Results: Prec@1 39.630 Loss: 1.7107\n",
            "Best Prec@1: 39.630\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [10][0/48]\tTime 0.044 (0.044)\tLoss 1.8433 (1.8433)\tPrec@1 33.984 (33.984)\n",
            "Epoch: [10][9/48]\tTime 0.040 (0.044)\tLoss 1.8394 (1.8316)\tPrec@1 34.766 (34.434)\n",
            "Epoch: [10][18/48]\tTime 0.041 (0.043)\tLoss 1.8200 (1.8439)\tPrec@1 35.449 (34.005)\n",
            "Epoch: [10][27/48]\tTime 0.043 (0.043)\tLoss 1.8408 (1.8427)\tPrec@1 33.398 (33.970)\n",
            "Epoch: [10][36/48]\tTime 0.049 (0.043)\tLoss 1.8563 (1.8431)\tPrec@1 33.789 (33.728)\n",
            "Epoch: [10][45/48]\tTime 0.041 (0.043)\tLoss 1.8445 (1.8388)\tPrec@1 33.789 (33.944)\n",
            "Epoch: [10][48/48]\tTime 0.035 (0.043)\tLoss 1.8260 (1.8372)\tPrec@1 34.316 (33.970)\n",
            "EPOCH: 10 train Results: Prec@1 33.970 Loss: 1.8372\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.7251 (1.7251)\tPrec@1 39.453 (39.453)\n",
            "Test: [9/9]\tTime 0.008 (0.012)\tLoss 1.6891 (1.6956)\tPrec@1 39.923 (40.410)\n",
            "EPOCH: 10 val Results: Prec@1 40.410 Loss: 1.6956\n",
            "Best Prec@1: 40.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [11][0/48]\tTime 0.049 (0.049)\tLoss 1.8199 (1.8199)\tPrec@1 32.910 (32.910)\n",
            "Epoch: [11][9/48]\tTime 0.040 (0.044)\tLoss 1.7935 (1.8183)\tPrec@1 36.133 (34.531)\n",
            "Epoch: [11][18/48]\tTime 0.045 (0.043)\tLoss 1.8194 (1.8212)\tPrec@1 34.082 (34.457)\n",
            "Epoch: [11][27/48]\tTime 0.041 (0.043)\tLoss 1.8373 (1.8203)\tPrec@1 34.082 (34.755)\n",
            "Epoch: [11][36/48]\tTime 0.040 (0.043)\tLoss 1.7940 (1.8154)\tPrec@1 35.645 (34.826)\n",
            "Epoch: [11][45/48]\tTime 0.046 (0.043)\tLoss 1.8089 (1.8121)\tPrec@1 34.375 (34.925)\n",
            "Epoch: [11][48/48]\tTime 0.039 (0.043)\tLoss 1.7940 (1.8119)\tPrec@1 33.373 (34.930)\n",
            "EPOCH: 11 train Results: Prec@1 34.930 Loss: 1.8119\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.7109 (1.7109)\tPrec@1 39.746 (39.746)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.6781 (1.6828)\tPrec@1 40.179 (40.690)\n",
            "EPOCH: 11 val Results: Prec@1 40.690 Loss: 1.6828\n",
            "Best Prec@1: 40.690\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [12][0/48]\tTime 0.044 (0.044)\tLoss 1.8174 (1.8174)\tPrec@1 35.645 (35.645)\n",
            "Epoch: [12][9/48]\tTime 0.043 (0.042)\tLoss 1.7607 (1.7959)\tPrec@1 36.328 (35.430)\n",
            "Epoch: [12][18/48]\tTime 0.041 (0.043)\tLoss 1.7962 (1.7944)\tPrec@1 35.449 (35.639)\n",
            "Epoch: [12][27/48]\tTime 0.041 (0.044)\tLoss 1.7359 (1.7922)\tPrec@1 35.938 (35.557)\n",
            "Epoch: [12][36/48]\tTime 0.044 (0.044)\tLoss 1.7838 (1.7915)\tPrec@1 34.375 (35.647)\n",
            "Epoch: [12][45/48]\tTime 0.046 (0.044)\tLoss 1.7908 (1.7870)\tPrec@1 33.789 (35.723)\n",
            "Epoch: [12][48/48]\tTime 0.037 (0.044)\tLoss 1.7448 (1.7882)\tPrec@1 36.675 (35.692)\n",
            "EPOCH: 12 train Results: Prec@1 35.692 Loss: 1.7882\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.6961 (1.6961)\tPrec@1 40.625 (40.625)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.6641 (1.6700)\tPrec@1 41.454 (41.190)\n",
            "EPOCH: 12 val Results: Prec@1 41.190 Loss: 1.6700\n",
            "Best Prec@1: 41.190\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [13][0/48]\tTime 0.046 (0.046)\tLoss 1.7837 (1.7837)\tPrec@1 34.766 (34.766)\n",
            "Epoch: [13][9/48]\tTime 0.041 (0.043)\tLoss 1.7496 (1.7679)\tPrec@1 37.793 (35.898)\n",
            "Epoch: [13][18/48]\tTime 0.044 (0.044)\tLoss 1.7659 (1.7643)\tPrec@1 36.230 (36.292)\n",
            "Epoch: [13][27/48]\tTime 0.063 (0.048)\tLoss 1.7808 (1.7650)\tPrec@1 35.449 (36.391)\n",
            "Epoch: [13][36/48]\tTime 0.082 (0.055)\tLoss 1.7611 (1.7629)\tPrec@1 34.766 (36.405)\n",
            "Epoch: [13][45/48]\tTime 0.134 (0.061)\tLoss 1.7676 (1.7615)\tPrec@1 36.230 (36.419)\n",
            "Epoch: [13][48/48]\tTime 0.068 (0.062)\tLoss 1.7595 (1.7616)\tPrec@1 36.910 (36.412)\n",
            "EPOCH: 13 train Results: Prec@1 36.412 Loss: 1.7616\n",
            "Test: [0/9]\tTime 0.019 (0.019)\tLoss 1.6823 (1.6823)\tPrec@1 40.723 (40.723)\n",
            "Test: [9/9]\tTime 0.009 (0.019)\tLoss 1.6532 (1.6584)\tPrec@1 41.071 (41.450)\n",
            "EPOCH: 13 val Results: Prec@1 41.450 Loss: 1.6584\n",
            "Best Prec@1: 41.450\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [14][0/48]\tTime 0.109 (0.109)\tLoss 1.7151 (1.7151)\tPrec@1 39.551 (39.551)\n",
            "Epoch: [14][9/48]\tTime 0.071 (0.082)\tLoss 1.7546 (1.7552)\tPrec@1 37.012 (36.523)\n",
            "Epoch: [14][18/48]\tTime 0.041 (0.068)\tLoss 1.7634 (1.7520)\tPrec@1 36.719 (36.744)\n",
            "Epoch: [14][27/48]\tTime 0.058 (0.061)\tLoss 1.7035 (1.7532)\tPrec@1 36.816 (36.621)\n",
            "Epoch: [14][36/48]\tTime 0.041 (0.056)\tLoss 1.7307 (1.7473)\tPrec@1 38.086 (36.896)\n",
            "Epoch: [14][45/48]\tTime 0.041 (0.054)\tLoss 1.7835 (1.7485)\tPrec@1 36.914 (36.957)\n",
            "Epoch: [14][48/48]\tTime 0.038 (0.053)\tLoss 1.7455 (1.7472)\tPrec@1 36.675 (36.958)\n",
            "EPOCH: 14 train Results: Prec@1 36.958 Loss: 1.7472\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.6718 (1.6718)\tPrec@1 40.234 (40.234)\n",
            "Test: [9/9]\tTime 0.010 (0.011)\tLoss 1.6443 (1.6486)\tPrec@1 40.434 (41.770)\n",
            "EPOCH: 14 val Results: Prec@1 41.770 Loss: 1.6486\n",
            "Best Prec@1: 41.770\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [15][0/48]\tTime 0.064 (0.064)\tLoss 1.6789 (1.6789)\tPrec@1 38.184 (38.184)\n",
            "Epoch: [15][9/48]\tTime 0.041 (0.045)\tLoss 1.7363 (1.7337)\tPrec@1 39.648 (37.432)\n",
            "Epoch: [15][18/48]\tTime 0.040 (0.044)\tLoss 1.7394 (1.7426)\tPrec@1 36.230 (37.125)\n",
            "Epoch: [15][27/48]\tTime 0.041 (0.044)\tLoss 1.7415 (1.7354)\tPrec@1 37.891 (37.263)\n",
            "Epoch: [15][36/48]\tTime 0.041 (0.044)\tLoss 1.7040 (1.7338)\tPrec@1 37.695 (37.249)\n",
            "Epoch: [15][45/48]\tTime 0.047 (0.044)\tLoss 1.6844 (1.7281)\tPrec@1 38.477 (37.489)\n",
            "Epoch: [15][48/48]\tTime 0.035 (0.044)\tLoss 1.7049 (1.7277)\tPrec@1 36.675 (37.438)\n",
            "EPOCH: 15 train Results: Prec@1 37.438 Loss: 1.7277\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.6588 (1.6588)\tPrec@1 41.406 (41.406)\n",
            "Test: [9/9]\tTime 0.010 (0.010)\tLoss 1.6324 (1.6374)\tPrec@1 41.327 (42.220)\n",
            "EPOCH: 15 val Results: Prec@1 42.220 Loss: 1.6374\n",
            "Best Prec@1: 42.220\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [16][0/48]\tTime 0.058 (0.058)\tLoss 1.7441 (1.7441)\tPrec@1 38.184 (38.184)\n",
            "Epoch: [16][9/48]\tTime 0.044 (0.044)\tLoss 1.6901 (1.7057)\tPrec@1 37.695 (38.594)\n",
            "Epoch: [16][18/48]\tTime 0.041 (0.044)\tLoss 1.7289 (1.7148)\tPrec@1 37.793 (38.040)\n",
            "Epoch: [16][27/48]\tTime 0.041 (0.044)\tLoss 1.6604 (1.7125)\tPrec@1 40.430 (38.372)\n",
            "Epoch: [16][36/48]\tTime 0.041 (0.044)\tLoss 1.6798 (1.7124)\tPrec@1 38.867 (38.247)\n",
            "Epoch: [16][45/48]\tTime 0.044 (0.044)\tLoss 1.7062 (1.7107)\tPrec@1 38.379 (38.311)\n",
            "Epoch: [16][48/48]\tTime 0.036 (0.044)\tLoss 1.6987 (1.7103)\tPrec@1 41.156 (38.362)\n",
            "EPOCH: 16 train Results: Prec@1 38.362 Loss: 1.7103\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.6494 (1.6494)\tPrec@1 41.699 (41.699)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.6231 (1.6274)\tPrec@1 41.454 (42.540)\n",
            "EPOCH: 16 val Results: Prec@1 42.540 Loss: 1.6274\n",
            "Best Prec@1: 42.540\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [17][0/48]\tTime 0.046 (0.046)\tLoss 1.6684 (1.6684)\tPrec@1 38.672 (38.672)\n",
            "Epoch: [17][9/48]\tTime 0.041 (0.044)\tLoss 1.6615 (1.6930)\tPrec@1 41.699 (38.828)\n",
            "Epoch: [17][18/48]\tTime 0.044 (0.044)\tLoss 1.6797 (1.7001)\tPrec@1 39.453 (38.775)\n",
            "Epoch: [17][27/48]\tTime 0.042 (0.044)\tLoss 1.7054 (1.7010)\tPrec@1 36.914 (38.595)\n",
            "Epoch: [17][36/48]\tTime 0.041 (0.043)\tLoss 1.7358 (1.6999)\tPrec@1 36.914 (38.564)\n",
            "Epoch: [17][45/48]\tTime 0.041 (0.044)\tLoss 1.7226 (1.6979)\tPrec@1 38.184 (38.697)\n",
            "Epoch: [17][48/48]\tTime 0.037 (0.043)\tLoss 1.6820 (1.6970)\tPrec@1 39.151 (38.734)\n",
            "EPOCH: 17 train Results: Prec@1 38.734 Loss: 1.6970\n",
            "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.6373 (1.6373)\tPrec@1 42.578 (42.578)\n",
            "Test: [9/9]\tTime 0.008 (0.012)\tLoss 1.6143 (1.6168)\tPrec@1 42.219 (43.120)\n",
            "EPOCH: 17 val Results: Prec@1 43.120 Loss: 1.6168\n",
            "Best Prec@1: 43.120\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [18][0/48]\tTime 0.047 (0.047)\tLoss 1.6867 (1.6867)\tPrec@1 40.918 (40.918)\n",
            "Epoch: [18][9/48]\tTime 0.041 (0.043)\tLoss 1.7477 (1.6989)\tPrec@1 38.867 (39.160)\n",
            "Epoch: [18][18/48]\tTime 0.042 (0.045)\tLoss 1.7198 (1.6930)\tPrec@1 37.988 (39.278)\n",
            "Epoch: [18][27/48]\tTime 0.041 (0.044)\tLoss 1.7071 (1.6943)\tPrec@1 37.695 (39.108)\n",
            "Epoch: [18][36/48]\tTime 0.070 (0.047)\tLoss 1.6380 (1.6890)\tPrec@1 41.699 (39.345)\n",
            "Epoch: [18][45/48]\tTime 0.079 (0.053)\tLoss 1.6619 (1.6869)\tPrec@1 40.039 (39.483)\n",
            "Epoch: [18][48/48]\tTime 0.073 (0.054)\tLoss 1.7159 (1.6860)\tPrec@1 40.802 (39.518)\n",
            "EPOCH: 18 train Results: Prec@1 39.518 Loss: 1.6860\n",
            "Test: [0/9]\tTime 0.019 (0.019)\tLoss 1.6287 (1.6287)\tPrec@1 42.285 (42.285)\n",
            "Test: [9/9]\tTime 0.020 (0.018)\tLoss 1.6047 (1.6079)\tPrec@1 42.219 (43.120)\n",
            "EPOCH: 18 val Results: Prec@1 43.120 Loss: 1.6079\n",
            "Best Prec@1: 43.120\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [19][0/48]\tTime 0.110 (0.110)\tLoss 1.6750 (1.6750)\tPrec@1 37.695 (37.695)\n",
            "Epoch: [19][9/48]\tTime 0.068 (0.079)\tLoss 1.6846 (1.6622)\tPrec@1 40.039 (40.410)\n",
            "Epoch: [19][18/48]\tTime 0.095 (0.078)\tLoss 1.7076 (1.6631)\tPrec@1 40.039 (40.188)\n",
            "Epoch: [19][27/48]\tTime 0.050 (0.076)\tLoss 1.6702 (1.6665)\tPrec@1 38.770 (39.980)\n",
            "Epoch: [19][36/48]\tTime 0.041 (0.068)\tLoss 1.7090 (1.6732)\tPrec@1 37.695 (39.844)\n",
            "Epoch: [19][45/48]\tTime 0.041 (0.063)\tLoss 1.6989 (1.6725)\tPrec@1 39.941 (39.950)\n",
            "Epoch: [19][48/48]\tTime 0.034 (0.062)\tLoss 1.7095 (1.6717)\tPrec@1 37.618 (39.964)\n",
            "EPOCH: 19 train Results: Prec@1 39.964 Loss: 1.6717\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.6182 (1.6182)\tPrec@1 42.969 (42.969)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.5961 (1.5983)\tPrec@1 42.219 (43.610)\n",
            "EPOCH: 19 val Results: Prec@1 43.610 Loss: 1.5983\n",
            "Best Prec@1: 43.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [20][0/48]\tTime 0.043 (0.043)\tLoss 1.6385 (1.6385)\tPrec@1 42.383 (42.383)\n",
            "Epoch: [20][9/48]\tTime 0.046 (0.044)\tLoss 1.6292 (1.6663)\tPrec@1 41.699 (40.801)\n",
            "Epoch: [20][18/48]\tTime 0.041 (0.045)\tLoss 1.6337 (1.6639)\tPrec@1 42.676 (40.682)\n",
            "Epoch: [20][27/48]\tTime 0.041 (0.044)\tLoss 1.6445 (1.6631)\tPrec@1 39.746 (40.395)\n",
            "Epoch: [20][36/48]\tTime 0.042 (0.044)\tLoss 1.6459 (1.6597)\tPrec@1 39.453 (40.625)\n",
            "Epoch: [20][45/48]\tTime 0.043 (0.044)\tLoss 1.6127 (1.6567)\tPrec@1 42.773 (40.778)\n",
            "Epoch: [20][48/48]\tTime 0.034 (0.044)\tLoss 1.6845 (1.6573)\tPrec@1 39.151 (40.758)\n",
            "EPOCH: 20 train Results: Prec@1 40.758 Loss: 1.6573\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.6081 (1.6081)\tPrec@1 42.871 (42.871)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.5873 (1.5889)\tPrec@1 41.964 (43.740)\n",
            "EPOCH: 20 val Results: Prec@1 43.740 Loss: 1.5889\n",
            "Best Prec@1: 43.740\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [21][0/48]\tTime 0.042 (0.042)\tLoss 1.6679 (1.6679)\tPrec@1 39.941 (39.941)\n",
            "Epoch: [21][9/48]\tTime 0.041 (0.042)\tLoss 1.6457 (1.6423)\tPrec@1 40.430 (40.684)\n",
            "Epoch: [21][18/48]\tTime 0.040 (0.042)\tLoss 1.6324 (1.6510)\tPrec@1 41.016 (40.389)\n",
            "Epoch: [21][27/48]\tTime 0.041 (0.043)\tLoss 1.6568 (1.6504)\tPrec@1 41.895 (40.625)\n",
            "Epoch: [21][36/48]\tTime 0.040 (0.043)\tLoss 1.6811 (1.6490)\tPrec@1 40.820 (40.691)\n",
            "Epoch: [21][45/48]\tTime 0.040 (0.043)\tLoss 1.6401 (1.6472)\tPrec@1 41.797 (40.795)\n",
            "Epoch: [21][48/48]\tTime 0.036 (0.043)\tLoss 1.6785 (1.6468)\tPrec@1 39.387 (40.818)\n",
            "EPOCH: 21 train Results: Prec@1 40.818 Loss: 1.6468\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.5984 (1.5984)\tPrec@1 43.652 (43.652)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.5789 (1.5804)\tPrec@1 42.474 (43.980)\n",
            "EPOCH: 21 val Results: Prec@1 43.980 Loss: 1.5804\n",
            "Best Prec@1: 43.980\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [22][0/48]\tTime 0.044 (0.044)\tLoss 1.6114 (1.6114)\tPrec@1 43.555 (43.555)\n",
            "Epoch: [22][9/48]\tTime 0.055 (0.044)\tLoss 1.6100 (1.6443)\tPrec@1 41.602 (41.074)\n",
            "Epoch: [22][18/48]\tTime 0.043 (0.044)\tLoss 1.6500 (1.6346)\tPrec@1 41.895 (41.350)\n",
            "Epoch: [22][27/48]\tTime 0.041 (0.043)\tLoss 1.6731 (1.6364)\tPrec@1 38.770 (41.354)\n",
            "Epoch: [22][36/48]\tTime 0.041 (0.044)\tLoss 1.6468 (1.6328)\tPrec@1 39.551 (41.309)\n",
            "Epoch: [22][45/48]\tTime 0.044 (0.044)\tLoss 1.6779 (1.6324)\tPrec@1 38.965 (41.338)\n",
            "Epoch: [22][48/48]\tTime 0.034 (0.043)\tLoss 1.6884 (1.6341)\tPrec@1 38.561 (41.310)\n",
            "EPOCH: 22 train Results: Prec@1 41.310 Loss: 1.6341\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.5880 (1.5880)\tPrec@1 43.750 (43.750)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.5700 (1.5705)\tPrec@1 42.602 (44.200)\n",
            "EPOCH: 22 val Results: Prec@1 44.200 Loss: 1.5705\n",
            "Best Prec@1: 44.200\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [23][0/48]\tTime 0.042 (0.042)\tLoss 1.5979 (1.5979)\tPrec@1 43.164 (43.164)\n",
            "Epoch: [23][9/48]\tTime 0.044 (0.045)\tLoss 1.6254 (1.6260)\tPrec@1 40.918 (41.816)\n",
            "Epoch: [23][18/48]\tTime 0.042 (0.044)\tLoss 1.6171 (1.6304)\tPrec@1 40.234 (41.740)\n",
            "Epoch: [23][27/48]\tTime 0.045 (0.043)\tLoss 1.5525 (1.6204)\tPrec@1 45.117 (42.010)\n",
            "Epoch: [23][36/48]\tTime 0.041 (0.043)\tLoss 1.5902 (1.6189)\tPrec@1 44.043 (42.100)\n",
            "Epoch: [23][45/48]\tTime 0.044 (0.043)\tLoss 1.6133 (1.6179)\tPrec@1 41.113 (42.009)\n",
            "Epoch: [23][48/48]\tTime 0.035 (0.043)\tLoss 1.6293 (1.6196)\tPrec@1 39.741 (41.878)\n",
            "EPOCH: 23 train Results: Prec@1 41.878 Loss: 1.6196\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.5795 (1.5795)\tPrec@1 43.945 (43.945)\n",
            "Test: [9/9]\tTime 0.013 (0.015)\tLoss 1.5618 (1.5619)\tPrec@1 43.495 (44.410)\n",
            "EPOCH: 23 val Results: Prec@1 44.410 Loss: 1.5619\n",
            "Best Prec@1: 44.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [24][0/48]\tTime 0.060 (0.060)\tLoss 1.6318 (1.6318)\tPrec@1 43.066 (43.066)\n",
            "Epoch: [24][9/48]\tTime 0.079 (0.072)\tLoss 1.6264 (1.6113)\tPrec@1 42.285 (42.764)\n",
            "Epoch: [24][18/48]\tTime 0.079 (0.079)\tLoss 1.6098 (1.6112)\tPrec@1 41.211 (42.444)\n",
            "Epoch: [24][27/48]\tTime 0.055 (0.077)\tLoss 1.6486 (1.6173)\tPrec@1 41.895 (42.250)\n",
            "Epoch: [24][36/48]\tTime 0.080 (0.078)\tLoss 1.5981 (1.6157)\tPrec@1 41.797 (42.227)\n",
            "Epoch: [24][45/48]\tTime 0.042 (0.073)\tLoss 1.5675 (1.6108)\tPrec@1 43.848 (42.427)\n",
            "Epoch: [24][48/48]\tTime 0.035 (0.071)\tLoss 1.6211 (1.6124)\tPrec@1 43.278 (42.400)\n",
            "EPOCH: 24 train Results: Prec@1 42.400 Loss: 1.6124\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.5726 (1.5726)\tPrec@1 44.531 (44.531)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.5539 (1.5535)\tPrec@1 44.005 (44.890)\n",
            "EPOCH: 24 val Results: Prec@1 44.890 Loss: 1.5535\n",
            "Best Prec@1: 44.890\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [25][0/48]\tTime 0.043 (0.043)\tLoss 1.6008 (1.6008)\tPrec@1 41.895 (41.895)\n",
            "Epoch: [25][9/48]\tTime 0.044 (0.043)\tLoss 1.5508 (1.5996)\tPrec@1 42.578 (42.363)\n",
            "Epoch: [25][18/48]\tTime 0.041 (0.043)\tLoss 1.5705 (1.6017)\tPrec@1 43.555 (42.465)\n",
            "Epoch: [25][27/48]\tTime 0.041 (0.043)\tLoss 1.6051 (1.6004)\tPrec@1 42.578 (42.829)\n",
            "Epoch: [25][36/48]\tTime 0.041 (0.043)\tLoss 1.5978 (1.6018)\tPrec@1 42.090 (42.742)\n",
            "Epoch: [25][45/48]\tTime 0.041 (0.043)\tLoss 1.5764 (1.6005)\tPrec@1 43.164 (42.665)\n",
            "Epoch: [25][48/48]\tTime 0.037 (0.043)\tLoss 1.6082 (1.6009)\tPrec@1 40.212 (42.614)\n",
            "EPOCH: 25 train Results: Prec@1 42.614 Loss: 1.6009\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.5624 (1.5624)\tPrec@1 45.508 (45.508)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.5466 (1.5454)\tPrec@1 43.622 (45.060)\n",
            "EPOCH: 25 val Results: Prec@1 45.060 Loss: 1.5454\n",
            "Best Prec@1: 45.060\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [26][0/48]\tTime 0.043 (0.043)\tLoss 1.6042 (1.6042)\tPrec@1 40.820 (40.820)\n",
            "Epoch: [26][9/48]\tTime 0.041 (0.043)\tLoss 1.5798 (1.5908)\tPrec@1 44.238 (43.145)\n",
            "Epoch: [26][18/48]\tTime 0.041 (0.044)\tLoss 1.5678 (1.5953)\tPrec@1 44.043 (43.205)\n",
            "Epoch: [26][27/48]\tTime 0.043 (0.044)\tLoss 1.5587 (1.5917)\tPrec@1 44.531 (43.105)\n",
            "Epoch: [26][36/48]\tTime 0.054 (0.044)\tLoss 1.5154 (1.5906)\tPrec@1 46.484 (43.006)\n",
            "Epoch: [26][45/48]\tTime 0.041 (0.044)\tLoss 1.5341 (1.5878)\tPrec@1 44.238 (43.134)\n",
            "Epoch: [26][48/48]\tTime 0.033 (0.043)\tLoss 1.6174 (1.5885)\tPrec@1 41.627 (43.086)\n",
            "EPOCH: 26 train Results: Prec@1 43.086 Loss: 1.5885\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.5541 (1.5541)\tPrec@1 44.824 (44.824)\n",
            "Test: [9/9]\tTime 0.010 (0.011)\tLoss 1.5369 (1.5356)\tPrec@1 43.878 (45.130)\n",
            "EPOCH: 26 val Results: Prec@1 45.130 Loss: 1.5356\n",
            "Best Prec@1: 45.130\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [27][0/48]\tTime 0.047 (0.047)\tLoss 1.5922 (1.5922)\tPrec@1 43.750 (43.750)\n",
            "Epoch: [27][9/48]\tTime 0.043 (0.043)\tLoss 1.5869 (1.5691)\tPrec@1 41.895 (43.311)\n",
            "Epoch: [27][18/48]\tTime 0.040 (0.043)\tLoss 1.6076 (1.5719)\tPrec@1 40.527 (43.210)\n",
            "Epoch: [27][27/48]\tTime 0.040 (0.043)\tLoss 1.5578 (1.5730)\tPrec@1 43.848 (43.415)\n",
            "Epoch: [27][36/48]\tTime 0.045 (0.043)\tLoss 1.6086 (1.5805)\tPrec@1 41.992 (43.143)\n",
            "Epoch: [27][45/48]\tTime 0.046 (0.043)\tLoss 1.6416 (1.5825)\tPrec@1 40.918 (43.066)\n",
            "Epoch: [27][48/48]\tTime 0.035 (0.043)\tLoss 1.5535 (1.5828)\tPrec@1 44.340 (43.102)\n",
            "EPOCH: 27 train Results: Prec@1 43.102 Loss: 1.5828\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.5454 (1.5454)\tPrec@1 45.215 (45.215)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.5300 (1.5279)\tPrec@1 43.750 (45.300)\n",
            "EPOCH: 27 val Results: Prec@1 45.300 Loss: 1.5279\n",
            "Best Prec@1: 45.300\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [28][0/48]\tTime 0.042 (0.042)\tLoss 1.5906 (1.5906)\tPrec@1 41.895 (41.895)\n",
            "Epoch: [28][9/48]\tTime 0.045 (0.044)\tLoss 1.5365 (1.5842)\tPrec@1 43.164 (43.262)\n",
            "Epoch: [28][18/48]\tTime 0.044 (0.043)\tLoss 1.5851 (1.5765)\tPrec@1 41.895 (43.421)\n",
            "Epoch: [28][27/48]\tTime 0.041 (0.043)\tLoss 1.5644 (1.5735)\tPrec@1 43.262 (43.474)\n",
            "Epoch: [28][36/48]\tTime 0.040 (0.043)\tLoss 1.5867 (1.5739)\tPrec@1 42.871 (43.565)\n",
            "Epoch: [28][45/48]\tTime 0.043 (0.043)\tLoss 1.5159 (1.5725)\tPrec@1 43.066 (43.652)\n",
            "Epoch: [28][48/48]\tTime 0.034 (0.043)\tLoss 1.5910 (1.5721)\tPrec@1 41.392 (43.612)\n",
            "EPOCH: 28 train Results: Prec@1 43.612 Loss: 1.5721\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.5375 (1.5375)\tPrec@1 45.508 (45.508)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.5233 (1.5192)\tPrec@1 44.133 (45.550)\n",
            "EPOCH: 28 val Results: Prec@1 45.550 Loss: 1.5192\n",
            "Best Prec@1: 45.550\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [29][0/48]\tTime 0.061 (0.061)\tLoss 1.5870 (1.5870)\tPrec@1 43.652 (43.652)\n",
            "Epoch: [29][9/48]\tTime 0.041 (0.047)\tLoss 1.5219 (1.5698)\tPrec@1 45.508 (43.799)\n",
            "Epoch: [29][18/48]\tTime 0.093 (0.057)\tLoss 1.5539 (1.5618)\tPrec@1 43.164 (43.899)\n",
            "Epoch: [29][27/48]\tTime 0.065 (0.062)\tLoss 1.5453 (1.5593)\tPrec@1 43.945 (43.879)\n",
            "Epoch: [29][36/48]\tTime 0.104 (0.068)\tLoss 1.5665 (1.5616)\tPrec@1 41.504 (43.869)\n",
            "Epoch: [29][45/48]\tTime 0.077 (0.071)\tLoss 1.5433 (1.5580)\tPrec@1 44.824 (43.988)\n",
            "Epoch: [29][48/48]\tTime 0.069 (0.070)\tLoss 1.6055 (1.5590)\tPrec@1 40.920 (43.914)\n",
            "EPOCH: 29 train Results: Prec@1 43.914 Loss: 1.5590\n",
            "Test: [0/9]\tTime 0.026 (0.026)\tLoss 1.5273 (1.5273)\tPrec@1 46.094 (46.094)\n",
            "Test: [9/9]\tTime 0.008 (0.020)\tLoss 1.5158 (1.5103)\tPrec@1 44.515 (46.020)\n",
            "EPOCH: 29 val Results: Prec@1 46.020 Loss: 1.5103\n",
            "Best Prec@1: 46.020\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [30][0/48]\tTime 0.080 (0.080)\tLoss 1.6109 (1.6109)\tPrec@1 40.137 (40.137)\n",
            "Epoch: [30][9/48]\tTime 0.040 (0.056)\tLoss 1.5808 (1.5572)\tPrec@1 43.262 (44.043)\n",
            "Epoch: [30][18/48]\tTime 0.040 (0.050)\tLoss 1.5475 (1.5511)\tPrec@1 44.043 (44.562)\n",
            "Epoch: [30][27/48]\tTime 0.041 (0.048)\tLoss 1.4970 (1.5472)\tPrec@1 47.656 (44.702)\n",
            "Epoch: [30][36/48]\tTime 0.042 (0.047)\tLoss 1.5658 (1.5490)\tPrec@1 43.750 (44.452)\n",
            "Epoch: [30][45/48]\tTime 0.040 (0.046)\tLoss 1.5706 (1.5505)\tPrec@1 42.188 (44.370)\n",
            "Epoch: [30][48/48]\tTime 0.034 (0.046)\tLoss 1.5385 (1.5511)\tPrec@1 44.458 (44.320)\n",
            "EPOCH: 30 train Results: Prec@1 44.320 Loss: 1.5511\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.5182 (1.5182)\tPrec@1 46.484 (46.484)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.5094 (1.5013)\tPrec@1 44.388 (46.310)\n",
            "EPOCH: 30 val Results: Prec@1 46.310 Loss: 1.5013\n",
            "Best Prec@1: 46.310\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [31][0/48]\tTime 0.047 (0.047)\tLoss 1.5109 (1.5109)\tPrec@1 45.605 (45.605)\n",
            "Epoch: [31][9/48]\tTime 0.041 (0.044)\tLoss 1.5446 (1.5352)\tPrec@1 44.727 (44.854)\n",
            "Epoch: [31][18/48]\tTime 0.040 (0.043)\tLoss 1.5396 (1.5361)\tPrec@1 45.508 (44.896)\n",
            "Epoch: [31][27/48]\tTime 0.041 (0.043)\tLoss 1.5334 (1.5383)\tPrec@1 45.020 (44.817)\n",
            "Epoch: [31][36/48]\tTime 0.042 (0.043)\tLoss 1.5207 (1.5370)\tPrec@1 47.754 (44.837)\n",
            "Epoch: [31][45/48]\tTime 0.041 (0.043)\tLoss 1.6083 (1.5396)\tPrec@1 40.527 (44.727)\n",
            "Epoch: [31][48/48]\tTime 0.034 (0.043)\tLoss 1.5653 (1.5389)\tPrec@1 42.689 (44.726)\n",
            "EPOCH: 31 train Results: Prec@1 44.726 Loss: 1.5389\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.5109 (1.5109)\tPrec@1 46.777 (46.777)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.5010 (1.4935)\tPrec@1 45.536 (46.690)\n",
            "EPOCH: 31 val Results: Prec@1 46.690 Loss: 1.4935\n",
            "Best Prec@1: 46.690\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [32][0/48]\tTime 0.044 (0.044)\tLoss 1.5532 (1.5532)\tPrec@1 46.094 (46.094)\n",
            "Epoch: [32][9/48]\tTime 0.042 (0.046)\tLoss 1.5319 (1.5409)\tPrec@1 46.582 (44.775)\n",
            "Epoch: [32][18/48]\tTime 0.043 (0.045)\tLoss 1.5267 (1.5330)\tPrec@1 46.289 (45.097)\n",
            "Epoch: [32][27/48]\tTime 0.041 (0.044)\tLoss 1.5490 (1.5335)\tPrec@1 44.629 (45.079)\n",
            "Epoch: [32][36/48]\tTime 0.039 (0.044)\tLoss 1.4632 (1.5309)\tPrec@1 46.680 (44.961)\n",
            "Epoch: [32][45/48]\tTime 0.040 (0.044)\tLoss 1.5413 (1.5320)\tPrec@1 45.020 (44.956)\n",
            "Epoch: [32][48/48]\tTime 0.034 (0.043)\tLoss 1.5555 (1.5321)\tPrec@1 47.170 (45.050)\n",
            "EPOCH: 32 train Results: Prec@1 45.050 Loss: 1.5321\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.5024 (1.5024)\tPrec@1 47.168 (47.168)\n",
            "Test: [9/9]\tTime 0.019 (0.011)\tLoss 1.4936 (1.4854)\tPrec@1 45.918 (46.950)\n",
            "EPOCH: 32 val Results: Prec@1 46.950 Loss: 1.4854\n",
            "Best Prec@1: 46.950\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [33][0/48]\tTime 0.044 (0.044)\tLoss 1.5874 (1.5874)\tPrec@1 42.969 (42.969)\n",
            "Epoch: [33][9/48]\tTime 0.040 (0.045)\tLoss 1.5553 (1.5310)\tPrec@1 44.043 (45.420)\n",
            "Epoch: [33][18/48]\tTime 0.040 (0.043)\tLoss 1.5209 (1.5241)\tPrec@1 47.656 (45.400)\n",
            "Epoch: [33][27/48]\tTime 0.040 (0.044)\tLoss 1.4862 (1.5183)\tPrec@1 49.316 (45.682)\n",
            "Epoch: [33][36/48]\tTime 0.043 (0.043)\tLoss 1.5332 (1.5238)\tPrec@1 46.191 (45.537)\n",
            "Epoch: [33][45/48]\tTime 0.040 (0.043)\tLoss 1.5476 (1.5222)\tPrec@1 43.848 (45.629)\n",
            "Epoch: [33][48/48]\tTime 0.034 (0.043)\tLoss 1.5430 (1.5224)\tPrec@1 43.160 (45.572)\n",
            "EPOCH: 33 train Results: Prec@1 45.572 Loss: 1.5224\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.4937 (1.4937)\tPrec@1 47.266 (47.266)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.4876 (1.4775)\tPrec@1 46.301 (47.430)\n",
            "EPOCH: 33 val Results: Prec@1 47.430 Loss: 1.4775\n",
            "Best Prec@1: 47.430\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [34][0/48]\tTime 0.048 (0.048)\tLoss 1.5460 (1.5460)\tPrec@1 45.508 (45.508)\n",
            "Epoch: [34][9/48]\tTime 0.040 (0.043)\tLoss 1.5343 (1.5146)\tPrec@1 44.824 (46.123)\n",
            "Epoch: [34][18/48]\tTime 0.041 (0.042)\tLoss 1.5252 (1.5142)\tPrec@1 44.434 (45.868)\n",
            "Epoch: [34][27/48]\tTime 0.040 (0.043)\tLoss 1.4982 (1.5122)\tPrec@1 47.070 (45.902)\n",
            "Epoch: [34][36/48]\tTime 0.113 (0.051)\tLoss 1.5275 (1.5141)\tPrec@1 45.898 (45.867)\n",
            "Epoch: [34][45/48]\tTime 0.115 (0.059)\tLoss 1.4700 (1.5142)\tPrec@1 48.145 (45.867)\n",
            "Epoch: [34][48/48]\tTime 0.066 (0.060)\tLoss 1.5485 (1.5138)\tPrec@1 44.340 (45.858)\n",
            "EPOCH: 34 train Results: Prec@1 45.858 Loss: 1.5138\n",
            "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.4860 (1.4860)\tPrec@1 47.168 (47.168)\n",
            "Test: [9/9]\tTime 0.016 (0.020)\tLoss 1.4799 (1.4687)\tPrec@1 46.301 (47.460)\n",
            "EPOCH: 34 val Results: Prec@1 47.460 Loss: 1.4687\n",
            "Best Prec@1: 47.460\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [35][0/48]\tTime 0.092 (0.092)\tLoss 1.4884 (1.4884)\tPrec@1 47.559 (47.559)\n",
            "Epoch: [35][9/48]\tTime 0.084 (0.073)\tLoss 1.5132 (1.5199)\tPrec@1 45.508 (46.191)\n",
            "Epoch: [35][18/48]\tTime 0.077 (0.075)\tLoss 1.4685 (1.5034)\tPrec@1 47.559 (46.186)\n",
            "Epoch: [35][27/48]\tTime 0.043 (0.065)\tLoss 1.4896 (1.5010)\tPrec@1 48.438 (46.380)\n",
            "Epoch: [35][36/48]\tTime 0.041 (0.060)\tLoss 1.5575 (1.5022)\tPrec@1 42.969 (46.228)\n",
            "Epoch: [35][45/48]\tTime 0.044 (0.057)\tLoss 1.5684 (1.5043)\tPrec@1 41.113 (46.098)\n",
            "Epoch: [35][48/48]\tTime 0.034 (0.056)\tLoss 1.4228 (1.5023)\tPrec@1 50.825 (46.266)\n",
            "EPOCH: 35 train Results: Prec@1 46.266 Loss: 1.5023\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.4763 (1.4763)\tPrec@1 47.363 (47.363)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.4732 (1.4606)\tPrec@1 47.066 (47.800)\n",
            "EPOCH: 35 val Results: Prec@1 47.800 Loss: 1.4606\n",
            "Best Prec@1: 47.800\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [36][0/48]\tTime 0.042 (0.042)\tLoss 1.5155 (1.5155)\tPrec@1 47.168 (47.168)\n",
            "Epoch: [36][9/48]\tTime 0.042 (0.046)\tLoss 1.5117 (1.4915)\tPrec@1 44.531 (45.830)\n",
            "Epoch: [36][18/48]\tTime 0.041 (0.044)\tLoss 1.4561 (1.4904)\tPrec@1 48.730 (46.279)\n",
            "Epoch: [36][27/48]\tTime 0.040 (0.044)\tLoss 1.4935 (1.4908)\tPrec@1 47.363 (46.387)\n",
            "Epoch: [36][36/48]\tTime 0.041 (0.044)\tLoss 1.5191 (1.4904)\tPrec@1 46.191 (46.360)\n",
            "Epoch: [36][45/48]\tTime 0.044 (0.044)\tLoss 1.5258 (1.4930)\tPrec@1 44.922 (46.300)\n",
            "Epoch: [36][48/48]\tTime 0.049 (0.044)\tLoss 1.5229 (1.4935)\tPrec@1 44.458 (46.310)\n",
            "EPOCH: 36 train Results: Prec@1 46.310 Loss: 1.4935\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.4663 (1.4663)\tPrec@1 48.145 (48.145)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.4663 (1.4517)\tPrec@1 47.449 (48.210)\n",
            "EPOCH: 36 val Results: Prec@1 48.210 Loss: 1.4517\n",
            "Best Prec@1: 48.210\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [37][0/48]\tTime 0.042 (0.042)\tLoss 1.4606 (1.4606)\tPrec@1 47.559 (47.559)\n",
            "Epoch: [37][9/48]\tTime 0.040 (0.045)\tLoss 1.4590 (1.4808)\tPrec@1 46.484 (46.855)\n",
            "Epoch: [37][18/48]\tTime 0.042 (0.045)\tLoss 1.5181 (1.4835)\tPrec@1 45.410 (46.916)\n",
            "Epoch: [37][27/48]\tTime 0.040 (0.045)\tLoss 1.5198 (1.4813)\tPrec@1 44.727 (47.032)\n",
            "Epoch: [37][36/48]\tTime 0.042 (0.044)\tLoss 1.4586 (1.4853)\tPrec@1 48.730 (46.796)\n",
            "Epoch: [37][45/48]\tTime 0.041 (0.044)\tLoss 1.4411 (1.4837)\tPrec@1 48.535 (46.856)\n",
            "Epoch: [37][48/48]\tTime 0.038 (0.044)\tLoss 1.4831 (1.4844)\tPrec@1 46.934 (46.804)\n",
            "EPOCH: 37 train Results: Prec@1 46.804 Loss: 1.4844\n",
            "Test: [0/9]\tTime 0.033 (0.033)\tLoss 1.4595 (1.4595)\tPrec@1 47.461 (47.461)\n",
            "Test: [9/9]\tTime 0.008 (0.013)\tLoss 1.4599 (1.4444)\tPrec@1 47.066 (48.360)\n",
            "EPOCH: 37 val Results: Prec@1 48.360 Loss: 1.4444\n",
            "Best Prec@1: 48.360\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [38][0/48]\tTime 0.043 (0.043)\tLoss 1.4670 (1.4670)\tPrec@1 48.047 (48.047)\n",
            "Epoch: [38][9/48]\tTime 0.043 (0.043)\tLoss 1.5244 (1.4906)\tPrec@1 44.141 (45.938)\n",
            "Epoch: [38][18/48]\tTime 0.041 (0.044)\tLoss 1.4397 (1.4824)\tPrec@1 48.145 (46.361)\n",
            "Epoch: [38][27/48]\tTime 0.041 (0.044)\tLoss 1.4495 (1.4786)\tPrec@1 49.414 (46.791)\n",
            "Epoch: [38][36/48]\tTime 0.048 (0.044)\tLoss 1.4823 (1.4733)\tPrec@1 44.531 (46.909)\n",
            "Epoch: [38][45/48]\tTime 0.049 (0.044)\tLoss 1.5246 (1.4775)\tPrec@1 46.582 (46.756)\n",
            "Epoch: [38][48/48]\tTime 0.033 (0.043)\tLoss 1.4585 (1.4775)\tPrec@1 46.462 (46.796)\n",
            "EPOCH: 38 train Results: Prec@1 46.796 Loss: 1.4775\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.4477 (1.4477)\tPrec@1 47.852 (47.852)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.4504 (1.4357)\tPrec@1 47.321 (48.550)\n",
            "EPOCH: 38 val Results: Prec@1 48.550 Loss: 1.4357\n",
            "Best Prec@1: 48.550\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [39][0/48]\tTime 0.044 (0.044)\tLoss 1.4675 (1.4675)\tPrec@1 45.508 (45.508)\n",
            "Epoch: [39][9/48]\tTime 0.040 (0.043)\tLoss 1.4695 (1.4702)\tPrec@1 48.242 (46.699)\n",
            "Epoch: [39][18/48]\tTime 0.053 (0.044)\tLoss 1.4947 (1.4666)\tPrec@1 44.336 (47.338)\n",
            "Epoch: [39][27/48]\tTime 0.043 (0.043)\tLoss 1.4808 (1.4707)\tPrec@1 47.363 (47.402)\n",
            "Epoch: [39][36/48]\tTime 0.040 (0.043)\tLoss 1.4559 (1.4673)\tPrec@1 46.777 (47.545)\n",
            "Epoch: [39][45/48]\tTime 0.095 (0.045)\tLoss 1.4625 (1.4647)\tPrec@1 47.266 (47.627)\n",
            "Epoch: [39][48/48]\tTime 0.043 (0.046)\tLoss 1.5029 (1.4667)\tPrec@1 45.401 (47.546)\n",
            "EPOCH: 39 train Results: Prec@1 47.546 Loss: 1.4667\n",
            "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.4386 (1.4386)\tPrec@1 48.535 (48.535)\n",
            "Test: [9/9]\tTime 0.010 (0.016)\tLoss 1.4422 (1.4281)\tPrec@1 47.704 (49.070)\n",
            "EPOCH: 39 val Results: Prec@1 49.070 Loss: 1.4281\n",
            "Best Prec@1: 49.070\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [40][0/48]\tTime 0.055 (0.055)\tLoss 1.4831 (1.4831)\tPrec@1 45.898 (45.898)\n",
            "Epoch: [40][9/48]\tTime 0.082 (0.083)\tLoss 1.4315 (1.4574)\tPrec@1 47.559 (46.943)\n",
            "Epoch: [40][18/48]\tTime 0.082 (0.083)\tLoss 1.4612 (1.4558)\tPrec@1 47.852 (47.379)\n",
            "Epoch: [40][27/48]\tTime 0.062 (0.081)\tLoss 1.4503 (1.4544)\tPrec@1 48.535 (47.461)\n",
            "Epoch: [40][36/48]\tTime 0.041 (0.078)\tLoss 1.3902 (1.4555)\tPrec@1 50.098 (47.545)\n",
            "Epoch: [40][45/48]\tTime 0.041 (0.071)\tLoss 1.4373 (1.4580)\tPrec@1 48.633 (47.565)\n",
            "Epoch: [40][48/48]\tTime 0.034 (0.069)\tLoss 1.4627 (1.4583)\tPrec@1 47.524 (47.586)\n",
            "EPOCH: 40 train Results: Prec@1 47.586 Loss: 1.4583\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.4298 (1.4298)\tPrec@1 49.023 (49.023)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.4352 (1.4191)\tPrec@1 47.959 (49.230)\n",
            "EPOCH: 40 val Results: Prec@1 49.230 Loss: 1.4191\n",
            "Best Prec@1: 49.230\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [41][0/48]\tTime 0.044 (0.044)\tLoss 1.4713 (1.4713)\tPrec@1 45.020 (45.020)\n",
            "Epoch: [41][9/48]\tTime 0.044 (0.046)\tLoss 1.4286 (1.4516)\tPrec@1 49.414 (47.842)\n",
            "Epoch: [41][18/48]\tTime 0.160 (0.063)\tLoss 1.4631 (1.4468)\tPrec@1 47.949 (47.954)\n",
            "Epoch: [41][27/48]\tTime 0.043 (0.063)\tLoss 1.3804 (1.4468)\tPrec@1 49.902 (47.984)\n",
            "Epoch: [41][36/48]\tTime 0.044 (0.058)\tLoss 1.4402 (1.4490)\tPrec@1 44.727 (47.796)\n",
            "Epoch: [41][45/48]\tTime 0.045 (0.055)\tLoss 1.4045 (1.4468)\tPrec@1 49.023 (47.939)\n",
            "Epoch: [41][48/48]\tTime 0.034 (0.054)\tLoss 1.4737 (1.4491)\tPrec@1 48.231 (47.882)\n",
            "EPOCH: 41 train Results: Prec@1 47.882 Loss: 1.4491\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.4191 (1.4191)\tPrec@1 48.828 (48.828)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.4277 (1.4109)\tPrec@1 47.959 (49.560)\n",
            "EPOCH: 41 val Results: Prec@1 49.560 Loss: 1.4109\n",
            "Best Prec@1: 49.560\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [42][0/48]\tTime 0.049 (0.049)\tLoss 1.4333 (1.4333)\tPrec@1 48.535 (48.535)\n",
            "Epoch: [42][9/48]\tTime 0.042 (0.043)\tLoss 1.4651 (1.4368)\tPrec@1 45.117 (48.125)\n",
            "Epoch: [42][18/48]\tTime 0.040 (0.045)\tLoss 1.4263 (1.4360)\tPrec@1 49.707 (48.268)\n",
            "Epoch: [42][27/48]\tTime 0.042 (0.044)\tLoss 1.4185 (1.4389)\tPrec@1 48.438 (48.284)\n",
            "Epoch: [42][36/48]\tTime 0.047 (0.044)\tLoss 1.4674 (1.4360)\tPrec@1 43.652 (48.266)\n",
            "Epoch: [42][45/48]\tTime 0.041 (0.044)\tLoss 1.4074 (1.4379)\tPrec@1 51.172 (48.181)\n",
            "Epoch: [42][48/48]\tTime 0.033 (0.044)\tLoss 1.4106 (1.4375)\tPrec@1 49.646 (48.214)\n",
            "EPOCH: 42 train Results: Prec@1 48.214 Loss: 1.4375\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.4106 (1.4106)\tPrec@1 49.023 (49.023)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.4199 (1.4030)\tPrec@1 48.469 (49.830)\n",
            "EPOCH: 42 val Results: Prec@1 49.830 Loss: 1.4030\n",
            "Best Prec@1: 49.830\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [43][0/48]\tTime 0.043 (0.043)\tLoss 1.4300 (1.4300)\tPrec@1 48.926 (48.926)\n",
            "Epoch: [43][9/48]\tTime 0.041 (0.044)\tLoss 1.4123 (1.4406)\tPrec@1 50.488 (48.281)\n",
            "Epoch: [43][18/48]\tTime 0.042 (0.043)\tLoss 1.3878 (1.4352)\tPrec@1 51.270 (48.283)\n",
            "Epoch: [43][27/48]\tTime 0.041 (0.043)\tLoss 1.3856 (1.4295)\tPrec@1 50.977 (48.615)\n",
            "Epoch: [43][36/48]\tTime 0.041 (0.043)\tLoss 1.4670 (1.4314)\tPrec@1 46.875 (48.620)\n",
            "Epoch: [43][45/48]\tTime 0.042 (0.043)\tLoss 1.4513 (1.4309)\tPrec@1 49.219 (48.622)\n",
            "Epoch: [43][48/48]\tTime 0.035 (0.043)\tLoss 1.3520 (1.4295)\tPrec@1 52.005 (48.674)\n",
            "EPOCH: 43 train Results: Prec@1 48.674 Loss: 1.4295\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.4004 (1.4004)\tPrec@1 49.414 (49.414)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.4110 (1.3943)\tPrec@1 48.597 (50.040)\n",
            "EPOCH: 43 val Results: Prec@1 50.040 Loss: 1.3943\n",
            "Best Prec@1: 50.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [44][0/48]\tTime 0.044 (0.044)\tLoss 1.4354 (1.4354)\tPrec@1 48.340 (48.340)\n",
            "Epoch: [44][9/48]\tTime 0.040 (0.045)\tLoss 1.3715 (1.4233)\tPrec@1 49.512 (49.121)\n",
            "Epoch: [44][18/48]\tTime 0.041 (0.044)\tLoss 1.4118 (1.4187)\tPrec@1 49.316 (49.327)\n",
            "Epoch: [44][27/48]\tTime 0.048 (0.044)\tLoss 1.4385 (1.4266)\tPrec@1 47.949 (48.989)\n",
            "Epoch: [44][36/48]\tTime 0.041 (0.043)\tLoss 1.3630 (1.4259)\tPrec@1 50.488 (48.831)\n",
            "Epoch: [44][45/48]\tTime 0.041 (0.043)\tLoss 1.4249 (1.4256)\tPrec@1 49.414 (48.968)\n",
            "Epoch: [44][48/48]\tTime 0.063 (0.044)\tLoss 1.4290 (1.4248)\tPrec@1 47.642 (48.994)\n",
            "EPOCH: 44 train Results: Prec@1 48.994 Loss: 1.4248\n",
            "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.3942 (1.3942)\tPrec@1 49.219 (49.219)\n",
            "Test: [9/9]\tTime 0.013 (0.019)\tLoss 1.4033 (1.3884)\tPrec@1 48.980 (50.490)\n",
            "EPOCH: 44 val Results: Prec@1 50.490 Loss: 1.3884\n",
            "Best Prec@1: 50.490\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [45][0/48]\tTime 0.088 (0.088)\tLoss 1.4118 (1.4118)\tPrec@1 49.121 (49.121)\n",
            "Epoch: [45][9/48]\tTime 0.084 (0.083)\tLoss 1.4409 (1.4132)\tPrec@1 48.145 (48.926)\n",
            "Epoch: [45][18/48]\tTime 0.078 (0.084)\tLoss 1.4519 (1.4088)\tPrec@1 48.145 (49.162)\n",
            "Epoch: [45][27/48]\tTime 0.062 (0.080)\tLoss 1.3984 (1.4097)\tPrec@1 49.414 (49.295)\n",
            "Epoch: [45][36/48]\tTime 0.083 (0.078)\tLoss 1.4570 (1.4108)\tPrec@1 48.145 (49.393)\n",
            "Epoch: [45][45/48]\tTime 0.040 (0.074)\tLoss 1.4674 (1.4128)\tPrec@1 46.387 (49.257)\n",
            "Epoch: [45][48/48]\tTime 0.034 (0.072)\tLoss 1.3814 (1.4116)\tPrec@1 51.061 (49.294)\n",
            "EPOCH: 45 train Results: Prec@1 49.294 Loss: 1.4116\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.3843 (1.3843)\tPrec@1 49.023 (49.023)\n",
            "Test: [9/9]\tTime 0.009 (0.010)\tLoss 1.3985 (1.3787)\tPrec@1 48.342 (50.660)\n",
            "EPOCH: 45 val Results: Prec@1 50.660 Loss: 1.3787\n",
            "Best Prec@1: 50.660\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [46][0/48]\tTime 0.044 (0.044)\tLoss 1.4267 (1.4267)\tPrec@1 50.098 (50.098)\n",
            "Epoch: [46][9/48]\tTime 0.052 (0.043)\tLoss 1.4773 (1.4173)\tPrec@1 47.070 (48.975)\n",
            "Epoch: [46][18/48]\tTime 0.040 (0.043)\tLoss 1.4523 (1.4090)\tPrec@1 47.070 (48.987)\n",
            "Epoch: [46][27/48]\tTime 0.044 (0.043)\tLoss 1.3940 (1.4084)\tPrec@1 50.195 (49.090)\n",
            "Epoch: [46][36/48]\tTime 0.041 (0.043)\tLoss 1.3690 (1.4079)\tPrec@1 51.465 (49.221)\n",
            "Epoch: [46][45/48]\tTime 0.045 (0.043)\tLoss 1.3753 (1.4033)\tPrec@1 51.953 (49.418)\n",
            "Epoch: [46][48/48]\tTime 0.033 (0.043)\tLoss 1.4451 (1.4057)\tPrec@1 49.410 (49.318)\n",
            "EPOCH: 46 train Results: Prec@1 49.318 Loss: 1.4057\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.3774 (1.3774)\tPrec@1 50.586 (50.586)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.3913 (1.3724)\tPrec@1 49.362 (51.100)\n",
            "EPOCH: 46 val Results: Prec@1 51.100 Loss: 1.3724\n",
            "Best Prec@1: 51.100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [47][0/48]\tTime 0.052 (0.052)\tLoss 1.4150 (1.4150)\tPrec@1 49.707 (49.707)\n",
            "Epoch: [47][9/48]\tTime 0.064 (0.046)\tLoss 1.3925 (1.3967)\tPrec@1 50.098 (49.990)\n",
            "Epoch: [47][18/48]\tTime 0.041 (0.044)\tLoss 1.3664 (1.4015)\tPrec@1 50.977 (49.789)\n",
            "Epoch: [47][27/48]\tTime 0.041 (0.044)\tLoss 1.3734 (1.3991)\tPrec@1 49.805 (49.920)\n",
            "Epoch: [47][36/48]\tTime 0.040 (0.044)\tLoss 1.4237 (1.3972)\tPrec@1 49.707 (49.884)\n",
            "Epoch: [47][45/48]\tTime 0.042 (0.044)\tLoss 1.3648 (1.3954)\tPrec@1 49.902 (49.983)\n",
            "Epoch: [47][48/48]\tTime 0.035 (0.044)\tLoss 1.3734 (1.3950)\tPrec@1 48.467 (49.980)\n",
            "EPOCH: 47 train Results: Prec@1 49.980 Loss: 1.3950\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.3665 (1.3665)\tPrec@1 50.879 (50.879)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.3857 (1.3647)\tPrec@1 50.383 (51.530)\n",
            "EPOCH: 47 val Results: Prec@1 51.530 Loss: 1.3647\n",
            "Best Prec@1: 51.530\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [48][0/48]\tTime 0.044 (0.044)\tLoss 1.3843 (1.3843)\tPrec@1 49.902 (49.902)\n",
            "Epoch: [48][9/48]\tTime 0.040 (0.045)\tLoss 1.4212 (1.3886)\tPrec@1 49.121 (49.941)\n",
            "Epoch: [48][18/48]\tTime 0.046 (0.044)\tLoss 1.3745 (1.3910)\tPrec@1 51.465 (50.175)\n",
            "Epoch: [48][27/48]\tTime 0.041 (0.044)\tLoss 1.3931 (1.3868)\tPrec@1 49.609 (50.296)\n",
            "Epoch: [48][36/48]\tTime 0.044 (0.044)\tLoss 1.4075 (1.3894)\tPrec@1 47.949 (50.274)\n",
            "Epoch: [48][45/48]\tTime 0.042 (0.044)\tLoss 1.4169 (1.3888)\tPrec@1 48.242 (50.293)\n",
            "Epoch: [48][48/48]\tTime 0.034 (0.044)\tLoss 1.4192 (1.3888)\tPrec@1 48.821 (50.264)\n",
            "EPOCH: 48 train Results: Prec@1 50.264 Loss: 1.3888\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.3562 (1.3562)\tPrec@1 50.781 (50.781)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.3787 (1.3566)\tPrec@1 50.128 (51.560)\n",
            "EPOCH: 48 val Results: Prec@1 51.560 Loss: 1.3566\n",
            "Best Prec@1: 51.560\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [49][0/48]\tTime 0.042 (0.042)\tLoss 1.4195 (1.4195)\tPrec@1 47.363 (47.363)\n",
            "Epoch: [49][9/48]\tTime 0.043 (0.045)\tLoss 1.3526 (1.3762)\tPrec@1 51.465 (50.508)\n",
            "Epoch: [49][18/48]\tTime 0.053 (0.044)\tLoss 1.3858 (1.3795)\tPrec@1 50.195 (50.899)\n",
            "Epoch: [49][27/48]\tTime 0.042 (0.044)\tLoss 1.4092 (1.3809)\tPrec@1 48.633 (50.764)\n",
            "Epoch: [49][36/48]\tTime 0.052 (0.044)\tLoss 1.3383 (1.3810)\tPrec@1 50.781 (50.713)\n",
            "Epoch: [49][45/48]\tTime 0.041 (0.044)\tLoss 1.3919 (1.3815)\tPrec@1 50.977 (50.590)\n",
            "Epoch: [49][48/48]\tTime 0.034 (0.043)\tLoss 1.3791 (1.3812)\tPrec@1 50.354 (50.602)\n",
            "EPOCH: 49 train Results: Prec@1 50.602 Loss: 1.3812\n",
            "Test: [0/9]\tTime 0.022 (0.022)\tLoss 1.3514 (1.3514)\tPrec@1 51.855 (51.855)\n",
            "Test: [9/9]\tTime 0.008 (0.013)\tLoss 1.3730 (1.3503)\tPrec@1 50.000 (51.970)\n",
            "EPOCH: 49 val Results: Prec@1 51.970 Loss: 1.3503\n",
            "Best Prec@1: 51.970\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [50][0/48]\tTime 0.044 (0.044)\tLoss 1.3879 (1.3879)\tPrec@1 50.391 (50.391)\n",
            "Epoch: [50][9/48]\tTime 0.042 (0.044)\tLoss 1.3902 (1.3660)\tPrec@1 49.414 (51.279)\n",
            "Epoch: [50][18/48]\tTime 0.087 (0.057)\tLoss 1.3634 (1.3693)\tPrec@1 51.074 (50.920)\n",
            "Epoch: [50][27/48]\tTime 0.080 (0.065)\tLoss 1.3265 (1.3685)\tPrec@1 51.172 (50.628)\n",
            "Epoch: [50][36/48]\tTime 0.080 (0.067)\tLoss 1.3773 (1.3681)\tPrec@1 52.539 (50.702)\n",
            "Epoch: [50][45/48]\tTime 0.082 (0.070)\tLoss 1.3707 (1.3701)\tPrec@1 50.586 (50.677)\n",
            "Epoch: [50][48/48]\tTime 0.079 (0.072)\tLoss 1.4253 (1.3717)\tPrec@1 50.118 (50.596)\n",
            "EPOCH: 50 train Results: Prec@1 50.596 Loss: 1.3717\n",
            "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.3437 (1.3437)\tPrec@1 51.953 (51.953)\n",
            "Test: [9/9]\tTime 0.016 (0.019)\tLoss 1.3662 (1.3427)\tPrec@1 50.765 (52.400)\n",
            "EPOCH: 50 val Results: Prec@1 52.400 Loss: 1.3427\n",
            "Best Prec@1: 52.400\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [51][0/48]\tTime 0.070 (0.070)\tLoss 1.3680 (1.3680)\tPrec@1 50.781 (50.781)\n",
            "Epoch: [51][9/48]\tTime 0.043 (0.047)\tLoss 1.3584 (1.3535)\tPrec@1 50.000 (51.191)\n",
            "Epoch: [51][18/48]\tTime 0.050 (0.046)\tLoss 1.3652 (1.3580)\tPrec@1 51.270 (51.244)\n",
            "Epoch: [51][27/48]\tTime 0.041 (0.045)\tLoss 1.4006 (1.3579)\tPrec@1 49.805 (51.259)\n",
            "Epoch: [51][36/48]\tTime 0.045 (0.045)\tLoss 1.3717 (1.3612)\tPrec@1 51.074 (51.101)\n",
            "Epoch: [51][45/48]\tTime 0.043 (0.044)\tLoss 1.3873 (1.3644)\tPrec@1 50.879 (51.098)\n",
            "Epoch: [51][48/48]\tTime 0.033 (0.044)\tLoss 1.3808 (1.3644)\tPrec@1 50.825 (51.108)\n",
            "EPOCH: 51 train Results: Prec@1 51.108 Loss: 1.3644\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.3312 (1.3312)\tPrec@1 52.246 (52.246)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.3605 (1.3352)\tPrec@1 50.255 (52.730)\n",
            "EPOCH: 51 val Results: Prec@1 52.730 Loss: 1.3352\n",
            "Best Prec@1: 52.730\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [52][0/48]\tTime 0.044 (0.044)\tLoss 1.3539 (1.3539)\tPrec@1 52.148 (52.148)\n",
            "Epoch: [52][9/48]\tTime 0.041 (0.044)\tLoss 1.3307 (1.3425)\tPrec@1 52.734 (52.168)\n",
            "Epoch: [52][18/48]\tTime 0.042 (0.044)\tLoss 1.3332 (1.3448)\tPrec@1 53.125 (51.984)\n",
            "Epoch: [52][27/48]\tTime 0.064 (0.044)\tLoss 1.3449 (1.3460)\tPrec@1 49.902 (51.719)\n",
            "Epoch: [52][36/48]\tTime 0.047 (0.044)\tLoss 1.3087 (1.3445)\tPrec@1 52.832 (51.636)\n",
            "Epoch: [52][45/48]\tTime 0.042 (0.044)\tLoss 1.3706 (1.3521)\tPrec@1 52.734 (51.486)\n",
            "Epoch: [52][48/48]\tTime 0.034 (0.044)\tLoss 1.3882 (1.3529)\tPrec@1 50.472 (51.452)\n",
            "EPOCH: 52 train Results: Prec@1 51.452 Loss: 1.3529\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.3252 (1.3252)\tPrec@1 52.441 (52.441)\n",
            "Test: [9/9]\tTime 0.008 (0.012)\tLoss 1.3514 (1.3276)\tPrec@1 51.020 (53.040)\n",
            "EPOCH: 52 val Results: Prec@1 53.040 Loss: 1.3276\n",
            "Best Prec@1: 53.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [53][0/48]\tTime 0.051 (0.051)\tLoss 1.3224 (1.3224)\tPrec@1 52.832 (52.832)\n",
            "Epoch: [53][9/48]\tTime 0.041 (0.043)\tLoss 1.3535 (1.3345)\tPrec@1 51.465 (52.129)\n",
            "Epoch: [53][18/48]\tTime 0.041 (0.043)\tLoss 1.3128 (1.3381)\tPrec@1 52.148 (51.537)\n",
            "Epoch: [53][27/48]\tTime 0.040 (0.044)\tLoss 1.4241 (1.3407)\tPrec@1 48.926 (51.538)\n",
            "Epoch: [53][36/48]\tTime 0.041 (0.044)\tLoss 1.3350 (1.3400)\tPrec@1 52.246 (51.687)\n",
            "Epoch: [53][45/48]\tTime 0.041 (0.043)\tLoss 1.3231 (1.3414)\tPrec@1 51.855 (51.792)\n",
            "Epoch: [53][48/48]\tTime 0.034 (0.044)\tLoss 1.3465 (1.3420)\tPrec@1 50.354 (51.794)\n",
            "EPOCH: 53 train Results: Prec@1 51.794 Loss: 1.3420\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.3200 (1.3200)\tPrec@1 52.246 (52.246)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.3496 (1.3234)\tPrec@1 49.745 (52.910)\n",
            "EPOCH: 53 val Results: Prec@1 52.910 Loss: 1.3234\n",
            "Best Prec@1: 53.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [54][0/48]\tTime 0.049 (0.049)\tLoss 1.3240 (1.3240)\tPrec@1 50.977 (50.977)\n",
            "Epoch: [54][9/48]\tTime 0.041 (0.044)\tLoss 1.3199 (1.3367)\tPrec@1 54.785 (52.021)\n",
            "Epoch: [54][18/48]\tTime 0.041 (0.043)\tLoss 1.3637 (1.3385)\tPrec@1 50.195 (52.164)\n",
            "Epoch: [54][27/48]\tTime 0.042 (0.044)\tLoss 1.3080 (1.3367)\tPrec@1 51.855 (52.194)\n",
            "Epoch: [54][36/48]\tTime 0.040 (0.044)\tLoss 1.3239 (1.3351)\tPrec@1 53.516 (52.251)\n",
            "Epoch: [54][45/48]\tTime 0.044 (0.044)\tLoss 1.3543 (1.3409)\tPrec@1 50.684 (52.010)\n",
            "Epoch: [54][48/48]\tTime 0.034 (0.044)\tLoss 1.3482 (1.3400)\tPrec@1 52.358 (52.080)\n",
            "EPOCH: 54 train Results: Prec@1 52.080 Loss: 1.3400\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.3124 (1.3124)\tPrec@1 53.906 (53.906)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.3393 (1.3165)\tPrec@1 50.128 (53.510)\n",
            "EPOCH: 54 val Results: Prec@1 53.510 Loss: 1.3165\n",
            "Best Prec@1: 53.510\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [55][0/48]\tTime 0.043 (0.043)\tLoss 1.3311 (1.3311)\tPrec@1 52.344 (52.344)\n",
            "Epoch: [55][9/48]\tTime 0.043 (0.043)\tLoss 1.3566 (1.3297)\tPrec@1 51.074 (52.178)\n",
            "Epoch: [55][18/48]\tTime 0.041 (0.044)\tLoss 1.3495 (1.3291)\tPrec@1 50.684 (52.534)\n",
            "Epoch: [55][27/48]\tTime 0.092 (0.049)\tLoss 1.3602 (1.3306)\tPrec@1 50.488 (52.424)\n",
            "Epoch: [55][36/48]\tTime 0.081 (0.057)\tLoss 1.3032 (1.3296)\tPrec@1 54.590 (52.499)\n",
            "Epoch: [55][45/48]\tTime 0.093 (0.061)\tLoss 1.3156 (1.3296)\tPrec@1 54.785 (52.516)\n",
            "Epoch: [55][48/48]\tTime 0.062 (0.062)\tLoss 1.3351 (1.3293)\tPrec@1 51.179 (52.470)\n",
            "EPOCH: 55 train Results: Prec@1 52.470 Loss: 1.3293\n",
            "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.3071 (1.3071)\tPrec@1 54.297 (54.297)\n",
            "Test: [9/9]\tTime 0.023 (0.022)\tLoss 1.3364 (1.3109)\tPrec@1 51.020 (53.600)\n",
            "EPOCH: 55 val Results: Prec@1 53.600 Loss: 1.3109\n",
            "Best Prec@1: 53.600\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [56][0/48]\tTime 0.081 (0.081)\tLoss 1.3276 (1.3276)\tPrec@1 54.004 (54.004)\n",
            "Epoch: [56][9/48]\tTime 0.080 (0.072)\tLoss 1.3213 (1.3437)\tPrec@1 53.027 (51.133)\n",
            "Epoch: [56][18/48]\tTime 0.042 (0.062)\tLoss 1.3090 (1.3261)\tPrec@1 54.297 (52.118)\n",
            "Epoch: [56][27/48]\tTime 0.044 (0.056)\tLoss 1.2766 (1.3169)\tPrec@1 54.785 (52.689)\n",
            "Epoch: [56][36/48]\tTime 0.041 (0.053)\tLoss 1.3417 (1.3215)\tPrec@1 52.148 (52.729)\n",
            "Epoch: [56][45/48]\tTime 0.041 (0.051)\tLoss 1.3401 (1.3224)\tPrec@1 50.977 (52.681)\n",
            "Epoch: [56][48/48]\tTime 0.035 (0.051)\tLoss 1.3213 (1.3225)\tPrec@1 50.825 (52.658)\n",
            "EPOCH: 56 train Results: Prec@1 52.658 Loss: 1.3225\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2972 (1.2972)\tPrec@1 54.883 (54.883)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.3280 (1.3037)\tPrec@1 50.765 (53.780)\n",
            "EPOCH: 56 val Results: Prec@1 53.780 Loss: 1.3037\n",
            "Best Prec@1: 53.780\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [57][0/48]\tTime 0.042 (0.042)\tLoss 1.3690 (1.3690)\tPrec@1 50.098 (50.098)\n",
            "Epoch: [57][9/48]\tTime 0.042 (0.044)\tLoss 1.2644 (1.3044)\tPrec@1 55.859 (52.617)\n",
            "Epoch: [57][18/48]\tTime 0.041 (0.043)\tLoss 1.3412 (1.3127)\tPrec@1 52.246 (52.570)\n",
            "Epoch: [57][27/48]\tTime 0.042 (0.044)\tLoss 1.2790 (1.3126)\tPrec@1 54.785 (52.672)\n",
            "Epoch: [57][36/48]\tTime 0.041 (0.044)\tLoss 1.3430 (1.3104)\tPrec@1 50.391 (52.843)\n",
            "Epoch: [57][45/48]\tTime 0.042 (0.044)\tLoss 1.3594 (1.3183)\tPrec@1 50.684 (52.494)\n",
            "Epoch: [57][48/48]\tTime 0.034 (0.044)\tLoss 1.3061 (1.3190)\tPrec@1 54.835 (52.542)\n",
            "EPOCH: 57 train Results: Prec@1 52.542 Loss: 1.3190\n",
            "Test: [0/9]\tTime 0.022 (0.022)\tLoss 1.2911 (1.2911)\tPrec@1 54.492 (54.492)\n",
            "Test: [9/9]\tTime 0.008 (0.012)\tLoss 1.3190 (1.2990)\tPrec@1 51.403 (53.730)\n",
            "EPOCH: 57 val Results: Prec@1 53.730 Loss: 1.2990\n",
            "Best Prec@1: 53.780\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [58][0/48]\tTime 0.043 (0.043)\tLoss 1.3381 (1.3381)\tPrec@1 50.977 (50.977)\n",
            "Epoch: [58][9/48]\tTime 0.046 (0.043)\tLoss 1.3386 (1.3064)\tPrec@1 53.320 (53.154)\n",
            "Epoch: [58][18/48]\tTime 0.041 (0.044)\tLoss 1.2988 (1.3045)\tPrec@1 53.320 (53.444)\n",
            "Epoch: [58][27/48]\tTime 0.041 (0.044)\tLoss 1.3001 (1.3076)\tPrec@1 52.734 (53.282)\n",
            "Epoch: [58][36/48]\tTime 0.043 (0.044)\tLoss 1.2638 (1.3047)\tPrec@1 54.297 (53.376)\n",
            "Epoch: [58][45/48]\tTime 0.040 (0.044)\tLoss 1.3525 (1.3058)\tPrec@1 51.855 (53.233)\n",
            "Epoch: [58][48/48]\tTime 0.037 (0.044)\tLoss 1.2892 (1.3054)\tPrec@1 53.774 (53.194)\n",
            "EPOCH: 58 train Results: Prec@1 53.194 Loss: 1.3054\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2833 (1.2833)\tPrec@1 55.078 (55.078)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.3191 (1.2916)\tPrec@1 50.893 (53.900)\n",
            "EPOCH: 58 val Results: Prec@1 53.900 Loss: 1.2916\n",
            "Best Prec@1: 53.900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [59][0/48]\tTime 0.042 (0.042)\tLoss 1.3497 (1.3497)\tPrec@1 52.344 (52.344)\n",
            "Epoch: [59][9/48]\tTime 0.049 (0.043)\tLoss 1.3232 (1.2845)\tPrec@1 52.441 (54.014)\n",
            "Epoch: [59][18/48]\tTime 0.041 (0.044)\tLoss 1.3747 (1.2950)\tPrec@1 50.586 (53.510)\n",
            "Epoch: [59][27/48]\tTime 0.042 (0.044)\tLoss 1.2416 (1.2920)\tPrec@1 56.250 (53.903)\n",
            "Epoch: [59][36/48]\tTime 0.043 (0.044)\tLoss 1.2537 (1.2961)\tPrec@1 55.176 (53.560)\n",
            "Epoch: [59][45/48]\tTime 0.040 (0.044)\tLoss 1.3034 (1.2993)\tPrec@1 52.832 (53.382)\n",
            "Epoch: [59][48/48]\tTime 0.036 (0.044)\tLoss 1.3187 (1.3002)\tPrec@1 52.123 (53.368)\n",
            "EPOCH: 59 train Results: Prec@1 53.368 Loss: 1.3002\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2784 (1.2784)\tPrec@1 54.590 (54.590)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.3085 (1.2851)\tPrec@1 51.913 (54.320)\n",
            "EPOCH: 59 val Results: Prec@1 54.320 Loss: 1.2851\n",
            "Best Prec@1: 54.320\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [60][0/48]\tTime 0.042 (0.042)\tLoss 1.2808 (1.2808)\tPrec@1 55.762 (55.762)\n",
            "Epoch: [60][9/48]\tTime 0.041 (0.042)\tLoss 1.2969 (1.2981)\tPrec@1 53.809 (53.867)\n",
            "Epoch: [60][18/48]\tTime 0.044 (0.043)\tLoss 1.3280 (1.3007)\tPrec@1 55.078 (53.881)\n",
            "Epoch: [60][27/48]\tTime 0.041 (0.043)\tLoss 1.2784 (1.2993)\tPrec@1 54.688 (53.938)\n",
            "Epoch: [60][36/48]\tTime 0.080 (0.044)\tLoss 1.2947 (1.3020)\tPrec@1 54.395 (53.645)\n",
            "Epoch: [60][45/48]\tTime 0.097 (0.052)\tLoss 1.2732 (1.3008)\tPrec@1 54.688 (53.550)\n",
            "Epoch: [60][48/48]\tTime 0.061 (0.053)\tLoss 1.3096 (1.2998)\tPrec@1 52.476 (53.510)\n",
            "EPOCH: 60 train Results: Prec@1 53.510 Loss: 1.2998\n",
            "Test: [0/9]\tTime 0.019 (0.019)\tLoss 1.2731 (1.2731)\tPrec@1 55.664 (55.664)\n",
            "Test: [9/9]\tTime 0.015 (0.020)\tLoss 1.3076 (1.2816)\tPrec@1 51.913 (54.320)\n",
            "EPOCH: 60 val Results: Prec@1 54.320 Loss: 1.2816\n",
            "Best Prec@1: 54.320\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [61][0/48]\tTime 0.082 (0.082)\tLoss 1.2762 (1.2762)\tPrec@1 53.320 (53.320)\n",
            "Epoch: [61][9/48]\tTime 0.079 (0.071)\tLoss 1.2818 (1.2726)\tPrec@1 53.906 (54.404)\n",
            "Epoch: [61][18/48]\tTime 0.078 (0.075)\tLoss 1.3309 (1.2805)\tPrec@1 53.125 (54.071)\n",
            "Epoch: [61][27/48]\tTime 0.081 (0.074)\tLoss 1.2632 (1.2871)\tPrec@1 57.520 (53.903)\n",
            "Epoch: [61][36/48]\tTime 0.041 (0.067)\tLoss 1.2649 (1.2895)\tPrec@1 56.836 (53.835)\n",
            "Epoch: [61][45/48]\tTime 0.042 (0.062)\tLoss 1.3294 (1.2920)\tPrec@1 51.953 (53.679)\n",
            "Epoch: [61][48/48]\tTime 0.034 (0.061)\tLoss 1.3054 (1.2928)\tPrec@1 52.594 (53.652)\n",
            "EPOCH: 61 train Results: Prec@1 53.652 Loss: 1.2928\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2661 (1.2661)\tPrec@1 55.762 (55.762)\n",
            "Test: [9/9]\tTime 0.008 (0.012)\tLoss 1.2991 (1.2759)\tPrec@1 51.531 (54.480)\n",
            "EPOCH: 61 val Results: Prec@1 54.480 Loss: 1.2759\n",
            "Best Prec@1: 54.480\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [62][0/48]\tTime 0.054 (0.054)\tLoss 1.2993 (1.2993)\tPrec@1 52.832 (52.832)\n",
            "Epoch: [62][9/48]\tTime 0.040 (0.044)\tLoss 1.2939 (1.2866)\tPrec@1 52.344 (53.633)\n",
            "Epoch: [62][18/48]\tTime 0.040 (0.043)\tLoss 1.2855 (1.2817)\tPrec@1 54.980 (53.988)\n",
            "Epoch: [62][27/48]\tTime 0.040 (0.044)\tLoss 1.2497 (1.2775)\tPrec@1 53.027 (54.189)\n",
            "Epoch: [62][36/48]\tTime 0.041 (0.043)\tLoss 1.2470 (1.2772)\tPrec@1 55.078 (54.173)\n",
            "Epoch: [62][45/48]\tTime 0.046 (0.043)\tLoss 1.3116 (1.2816)\tPrec@1 54.199 (54.108)\n",
            "Epoch: [62][48/48]\tTime 0.036 (0.044)\tLoss 1.3059 (1.2815)\tPrec@1 52.594 (54.064)\n",
            "EPOCH: 62 train Results: Prec@1 54.064 Loss: 1.2815\n",
            "Test: [0/9]\tTime 0.015 (0.015)\tLoss 1.2590 (1.2590)\tPrec@1 56.738 (56.738)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.2915 (1.2712)\tPrec@1 52.168 (55.000)\n",
            "EPOCH: 62 val Results: Prec@1 55.000 Loss: 1.2712\n",
            "Best Prec@1: 55.000\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [63][0/48]\tTime 0.045 (0.045)\tLoss 1.2783 (1.2783)\tPrec@1 54.590 (54.590)\n",
            "Epoch: [63][9/48]\tTime 0.041 (0.043)\tLoss 1.2553 (1.2581)\tPrec@1 54.980 (55.342)\n",
            "Epoch: [63][18/48]\tTime 0.060 (0.044)\tLoss 1.3551 (1.2702)\tPrec@1 51.074 (54.636)\n",
            "Epoch: [63][27/48]\tTime 0.040 (0.044)\tLoss 1.3043 (1.2703)\tPrec@1 52.344 (54.583)\n",
            "Epoch: [63][36/48]\tTime 0.041 (0.043)\tLoss 1.3008 (1.2773)\tPrec@1 55.859 (54.416)\n",
            "Epoch: [63][45/48]\tTime 0.041 (0.044)\tLoss 1.2681 (1.2778)\tPrec@1 55.566 (54.409)\n",
            "Epoch: [63][48/48]\tTime 0.035 (0.043)\tLoss 1.2705 (1.2773)\tPrec@1 54.245 (54.368)\n",
            "EPOCH: 63 train Results: Prec@1 54.368 Loss: 1.2773\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2504 (1.2504)\tPrec@1 56.934 (56.934)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.2876 (1.2653)\tPrec@1 51.913 (55.110)\n",
            "EPOCH: 63 val Results: Prec@1 55.110 Loss: 1.2653\n",
            "Best Prec@1: 55.110\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [64][0/48]\tTime 0.041 (0.041)\tLoss 1.2413 (1.2413)\tPrec@1 55.762 (55.762)\n",
            "Epoch: [64][9/48]\tTime 0.041 (0.042)\tLoss 1.2866 (1.2553)\tPrec@1 53.711 (55.371)\n",
            "Epoch: [64][18/48]\tTime 0.044 (0.043)\tLoss 1.2651 (1.2636)\tPrec@1 54.688 (54.857)\n",
            "Epoch: [64][27/48]\tTime 0.043 (0.043)\tLoss 1.1792 (1.2631)\tPrec@1 58.398 (54.918)\n",
            "Epoch: [64][36/48]\tTime 0.041 (0.043)\tLoss 1.3332 (1.2669)\tPrec@1 52.441 (54.682)\n",
            "Epoch: [64][45/48]\tTime 0.041 (0.043)\tLoss 1.2327 (1.2691)\tPrec@1 57.324 (54.564)\n",
            "Epoch: [64][48/48]\tTime 0.039 (0.043)\tLoss 1.2925 (1.2719)\tPrec@1 53.420 (54.454)\n",
            "EPOCH: 64 train Results: Prec@1 54.454 Loss: 1.2719\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2468 (1.2468)\tPrec@1 56.543 (56.543)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.2843 (1.2629)\tPrec@1 53.061 (55.630)\n",
            "EPOCH: 64 val Results: Prec@1 55.630 Loss: 1.2629\n",
            "Best Prec@1: 55.630\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [65][0/48]\tTime 0.045 (0.045)\tLoss 1.2570 (1.2570)\tPrec@1 54.004 (54.004)\n",
            "Epoch: [65][9/48]\tTime 0.043 (0.043)\tLoss 1.2626 (1.2518)\tPrec@1 54.395 (55.107)\n",
            "Epoch: [65][18/48]\tTime 0.043 (0.044)\tLoss 1.2969 (1.2624)\tPrec@1 54.590 (55.166)\n",
            "Epoch: [65][27/48]\tTime 0.040 (0.044)\tLoss 1.2776 (1.2657)\tPrec@1 52.734 (54.827)\n",
            "Epoch: [65][36/48]\tTime 0.041 (0.044)\tLoss 1.2700 (1.2708)\tPrec@1 56.738 (54.651)\n",
            "Epoch: [65][45/48]\tTime 0.042 (0.044)\tLoss 1.2989 (1.2706)\tPrec@1 52.344 (54.630)\n",
            "Epoch: [65][48/48]\tTime 0.035 (0.043)\tLoss 1.3356 (1.2727)\tPrec@1 53.892 (54.620)\n",
            "EPOCH: 65 train Results: Prec@1 54.620 Loss: 1.2727\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.2389 (1.2389)\tPrec@1 56.445 (56.445)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.2857 (1.2580)\tPrec@1 52.679 (55.630)\n",
            "EPOCH: 65 val Results: Prec@1 55.630 Loss: 1.2580\n",
            "Best Prec@1: 55.630\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [66][0/48]\tTime 0.045 (0.045)\tLoss 1.2281 (1.2281)\tPrec@1 54.492 (54.492)\n",
            "Epoch: [66][9/48]\tTime 0.080 (0.064)\tLoss 1.2544 (1.2575)\tPrec@1 54.395 (54.414)\n",
            "Epoch: [66][18/48]\tTime 0.090 (0.077)\tLoss 1.2746 (1.2520)\tPrec@1 53.223 (54.862)\n",
            "Epoch: [66][27/48]\tTime 0.062 (0.078)\tLoss 1.2707 (1.2568)\tPrec@1 55.371 (54.855)\n",
            "Epoch: [66][36/48]\tTime 0.060 (0.076)\tLoss 1.2801 (1.2622)\tPrec@1 52.246 (54.643)\n",
            "Epoch: [66][45/48]\tTime 0.044 (0.076)\tLoss 1.2523 (1.2647)\tPrec@1 53.320 (54.560)\n",
            "Epoch: [66][48/48]\tTime 0.034 (0.074)\tLoss 1.3009 (1.2659)\tPrec@1 52.358 (54.480)\n",
            "EPOCH: 66 train Results: Prec@1 54.480 Loss: 1.2659\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2389 (1.2389)\tPrec@1 56.738 (56.738)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.2810 (1.2549)\tPrec@1 53.061 (55.490)\n",
            "EPOCH: 66 val Results: Prec@1 55.490 Loss: 1.2549\n",
            "Best Prec@1: 55.630\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [67][0/48]\tTime 0.042 (0.042)\tLoss 1.1758 (1.1758)\tPrec@1 56.836 (56.836)\n",
            "Epoch: [67][9/48]\tTime 0.047 (0.044)\tLoss 1.2903 (1.2423)\tPrec@1 54.102 (55.068)\n",
            "Epoch: [67][18/48]\tTime 0.070 (0.045)\tLoss 1.2596 (1.2434)\tPrec@1 54.688 (55.176)\n",
            "Epoch: [67][27/48]\tTime 0.045 (0.044)\tLoss 1.2597 (1.2504)\tPrec@1 53.906 (55.068)\n",
            "Epoch: [67][36/48]\tTime 0.044 (0.044)\tLoss 1.2305 (1.2540)\tPrec@1 55.664 (54.975)\n",
            "Epoch: [67][45/48]\tTime 0.041 (0.044)\tLoss 1.2691 (1.2545)\tPrec@1 54.395 (55.053)\n",
            "Epoch: [67][48/48]\tTime 0.035 (0.044)\tLoss 1.2206 (1.2554)\tPrec@1 54.009 (54.976)\n",
            "EPOCH: 67 train Results: Prec@1 54.976 Loss: 1.2554\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2336 (1.2336)\tPrec@1 57.031 (57.031)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.2788 (1.2511)\tPrec@1 51.531 (55.650)\n",
            "EPOCH: 67 val Results: Prec@1 55.650 Loss: 1.2511\n",
            "Best Prec@1: 55.650\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [68][0/48]\tTime 0.043 (0.043)\tLoss 1.1721 (1.1721)\tPrec@1 58.301 (58.301)\n",
            "Epoch: [68][9/48]\tTime 0.043 (0.043)\tLoss 1.1771 (1.2237)\tPrec@1 58.789 (56.621)\n",
            "Epoch: [68][18/48]\tTime 0.041 (0.043)\tLoss 1.3005 (1.2401)\tPrec@1 52.441 (55.659)\n",
            "Epoch: [68][27/48]\tTime 0.040 (0.043)\tLoss 1.2415 (1.2396)\tPrec@1 54.688 (55.755)\n",
            "Epoch: [68][36/48]\tTime 0.044 (0.043)\tLoss 1.3087 (1.2436)\tPrec@1 53.320 (55.598)\n",
            "Epoch: [68][45/48]\tTime 0.040 (0.043)\tLoss 1.2351 (1.2460)\tPrec@1 54.980 (55.507)\n",
            "Epoch: [68][48/48]\tTime 0.034 (0.043)\tLoss 1.2855 (1.2460)\tPrec@1 54.363 (55.468)\n",
            "EPOCH: 68 train Results: Prec@1 55.468 Loss: 1.2460\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2256 (1.2256)\tPrec@1 57.910 (57.910)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.2697 (1.2475)\tPrec@1 52.806 (55.900)\n",
            "EPOCH: 68 val Results: Prec@1 55.900 Loss: 1.2475\n",
            "Best Prec@1: 55.900\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [69][0/48]\tTime 0.045 (0.045)\tLoss 1.1982 (1.1982)\tPrec@1 57.324 (57.324)\n",
            "Epoch: [69][9/48]\tTime 0.041 (0.045)\tLoss 1.2780 (1.2199)\tPrec@1 55.176 (56.729)\n",
            "Epoch: [69][18/48]\tTime 0.041 (0.045)\tLoss 1.2737 (1.2301)\tPrec@1 54.395 (56.240)\n",
            "Epoch: [69][27/48]\tTime 0.040 (0.044)\tLoss 1.2814 (1.2368)\tPrec@1 54.980 (55.831)\n",
            "Epoch: [69][36/48]\tTime 0.042 (0.044)\tLoss 1.2434 (1.2399)\tPrec@1 54.883 (55.656)\n",
            "Epoch: [69][45/48]\tTime 0.043 (0.044)\tLoss 1.2406 (1.2413)\tPrec@1 54.785 (55.560)\n",
            "Epoch: [69][48/48]\tTime 0.034 (0.044)\tLoss 1.2275 (1.2409)\tPrec@1 55.071 (55.640)\n",
            "EPOCH: 69 train Results: Prec@1 55.640 Loss: 1.2409\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2256 (1.2256)\tPrec@1 57.617 (57.617)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.2690 (1.2454)\tPrec@1 52.934 (56.010)\n",
            "EPOCH: 69 val Results: Prec@1 56.010 Loss: 1.2454\n",
            "Best Prec@1: 56.010\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [70][0/48]\tTime 0.042 (0.042)\tLoss 1.2186 (1.2186)\tPrec@1 57.520 (57.520)\n",
            "Epoch: [70][9/48]\tTime 0.043 (0.046)\tLoss 1.2615 (1.2297)\tPrec@1 55.664 (56.338)\n",
            "Epoch: [70][18/48]\tTime 0.040 (0.044)\tLoss 1.2421 (1.2320)\tPrec@1 56.543 (56.147)\n",
            "Epoch: [70][27/48]\tTime 0.041 (0.043)\tLoss 1.2919 (1.2361)\tPrec@1 52.832 (55.999)\n",
            "Epoch: [70][36/48]\tTime 0.040 (0.044)\tLoss 1.2433 (1.2402)\tPrec@1 54.102 (55.888)\n",
            "Epoch: [70][45/48]\tTime 0.040 (0.043)\tLoss 1.2353 (1.2408)\tPrec@1 55.664 (55.802)\n",
            "Epoch: [70][48/48]\tTime 0.033 (0.043)\tLoss 1.2274 (1.2413)\tPrec@1 55.307 (55.768)\n",
            "EPOCH: 70 train Results: Prec@1 55.768 Loss: 1.2413\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2246 (1.2246)\tPrec@1 57.520 (57.520)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.2699 (1.2429)\tPrec@1 52.296 (55.910)\n",
            "EPOCH: 70 val Results: Prec@1 55.910 Loss: 1.2429\n",
            "Best Prec@1: 56.010\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [71][0/48]\tTime 0.043 (0.043)\tLoss 1.2489 (1.2489)\tPrec@1 52.539 (52.539)\n",
            "Epoch: [71][9/48]\tTime 0.041 (0.045)\tLoss 1.1985 (1.2294)\tPrec@1 57.324 (55.791)\n",
            "Epoch: [71][18/48]\tTime 0.098 (0.047)\tLoss 1.2059 (1.2238)\tPrec@1 54.883 (56.173)\n",
            "Epoch: [71][27/48]\tTime 0.096 (0.056)\tLoss 1.2199 (1.2292)\tPrec@1 57.324 (55.922)\n",
            "Epoch: [71][36/48]\tTime 0.064 (0.062)\tLoss 1.1981 (1.2306)\tPrec@1 55.469 (55.873)\n",
            "Epoch: [71][45/48]\tTime 0.060 (0.066)\tLoss 1.2523 (1.2337)\tPrec@1 57.324 (55.906)\n",
            "Epoch: [71][48/48]\tTime 0.072 (0.067)\tLoss 1.3090 (1.2361)\tPrec@1 53.302 (55.814)\n",
            "EPOCH: 71 train Results: Prec@1 55.814 Loss: 1.2361\n",
            "Test: [0/9]\tTime 0.020 (0.020)\tLoss 1.2197 (1.2197)\tPrec@1 57.910 (57.910)\n",
            "Test: [9/9]\tTime 0.011 (0.019)\tLoss 1.2655 (1.2385)\tPrec@1 53.189 (55.970)\n",
            "EPOCH: 71 val Results: Prec@1 55.970 Loss: 1.2385\n",
            "Best Prec@1: 56.010\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [72][0/48]\tTime 0.085 (0.085)\tLoss 1.2062 (1.2062)\tPrec@1 55.859 (55.859)\n",
            "Epoch: [72][9/48]\tTime 0.094 (0.067)\tLoss 1.2544 (1.2213)\tPrec@1 53.613 (56.025)\n",
            "Epoch: [72][18/48]\tTime 0.041 (0.061)\tLoss 1.1467 (1.2183)\tPrec@1 60.742 (56.157)\n",
            "Epoch: [72][27/48]\tTime 0.045 (0.055)\tLoss 1.2257 (1.2236)\tPrec@1 56.445 (56.041)\n",
            "Epoch: [72][36/48]\tTime 0.040 (0.052)\tLoss 1.2465 (1.2315)\tPrec@1 55.762 (55.759)\n",
            "Epoch: [72][45/48]\tTime 0.041 (0.050)\tLoss 1.1775 (1.2322)\tPrec@1 57.617 (55.641)\n",
            "Epoch: [72][48/48]\tTime 0.041 (0.050)\tLoss 1.2416 (1.2331)\tPrec@1 56.604 (55.662)\n",
            "EPOCH: 72 train Results: Prec@1 55.662 Loss: 1.2331\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2133 (1.2133)\tPrec@1 57.520 (57.520)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.2575 (1.2330)\tPrec@1 54.592 (56.430)\n",
            "EPOCH: 72 val Results: Prec@1 56.430 Loss: 1.2330\n",
            "Best Prec@1: 56.430\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [73][0/48]\tTime 0.043 (0.043)\tLoss 1.2509 (1.2509)\tPrec@1 55.273 (55.273)\n",
            "Epoch: [73][9/48]\tTime 0.043 (0.043)\tLoss 1.1959 (1.2280)\tPrec@1 55.664 (56.191)\n",
            "Epoch: [73][18/48]\tTime 0.040 (0.043)\tLoss 1.2541 (1.2202)\tPrec@1 55.078 (56.373)\n",
            "Epoch: [73][27/48]\tTime 0.041 (0.043)\tLoss 1.2057 (1.2205)\tPrec@1 57.324 (56.152)\n",
            "Epoch: [73][36/48]\tTime 0.042 (0.044)\tLoss 1.2648 (1.2255)\tPrec@1 53.809 (55.917)\n",
            "Epoch: [73][45/48]\tTime 0.131 (0.049)\tLoss 1.2903 (1.2265)\tPrec@1 54.297 (56.074)\n",
            "Epoch: [73][48/48]\tTime 0.049 (0.054)\tLoss 1.2730 (1.2273)\tPrec@1 54.717 (56.034)\n",
            "EPOCH: 73 train Results: Prec@1 56.034 Loss: 1.2273\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2120 (1.2120)\tPrec@1 56.934 (56.934)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.2581 (1.2317)\tPrec@1 52.679 (56.300)\n",
            "EPOCH: 73 val Results: Prec@1 56.300 Loss: 1.2317\n",
            "Best Prec@1: 56.430\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [74][0/48]\tTime 0.048 (0.048)\tLoss 1.2503 (1.2503)\tPrec@1 54.004 (54.004)\n",
            "Epoch: [74][9/48]\tTime 0.148 (0.073)\tLoss 1.1635 (1.2274)\tPrec@1 59.668 (55.820)\n",
            "Epoch: [74][18/48]\tTime 0.041 (0.068)\tLoss 1.2393 (1.2272)\tPrec@1 56.445 (56.029)\n",
            "Epoch: [74][27/48]\tTime 0.041 (0.060)\tLoss 1.3050 (1.2284)\tPrec@1 52.539 (56.219)\n",
            "Epoch: [74][36/48]\tTime 0.042 (0.056)\tLoss 1.2943 (1.2263)\tPrec@1 54.004 (56.266)\n",
            "Epoch: [74][45/48]\tTime 0.039 (0.053)\tLoss 1.2319 (1.2265)\tPrec@1 56.348 (56.225)\n",
            "Epoch: [74][48/48]\tTime 0.033 (0.052)\tLoss 1.2569 (1.2282)\tPrec@1 54.717 (56.190)\n",
            "EPOCH: 74 train Results: Prec@1 56.190 Loss: 1.2282\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.2008 (1.2008)\tPrec@1 58.008 (58.008)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.2492 (1.2292)\tPrec@1 53.316 (56.540)\n",
            "EPOCH: 74 val Results: Prec@1 56.540 Loss: 1.2292\n",
            "Best Prec@1: 56.540\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [75][0/48]\tTime 0.045 (0.045)\tLoss 1.1368 (1.1368)\tPrec@1 59.082 (59.082)\n",
            "Epoch: [75][9/48]\tTime 0.040 (0.045)\tLoss 1.2334 (1.1925)\tPrec@1 55.957 (57.500)\n",
            "Epoch: [75][18/48]\tTime 0.040 (0.044)\tLoss 1.1885 (1.2128)\tPrec@1 58.203 (56.749)\n",
            "Epoch: [75][27/48]\tTime 0.041 (0.044)\tLoss 1.2596 (1.2140)\tPrec@1 54.395 (56.550)\n",
            "Epoch: [75][36/48]\tTime 0.041 (0.044)\tLoss 1.1905 (1.2158)\tPrec@1 57.520 (56.390)\n",
            "Epoch: [75][45/48]\tTime 0.042 (0.044)\tLoss 1.2513 (1.2190)\tPrec@1 55.762 (56.314)\n",
            "Epoch: [75][48/48]\tTime 0.034 (0.043)\tLoss 1.2015 (1.2206)\tPrec@1 57.783 (56.308)\n",
            "EPOCH: 75 train Results: Prec@1 56.308 Loss: 1.2206\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.2009 (1.2009)\tPrec@1 57.520 (57.520)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.2395 (1.2259)\tPrec@1 53.444 (56.530)\n",
            "EPOCH: 75 val Results: Prec@1 56.530 Loss: 1.2259\n",
            "Best Prec@1: 56.540\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [76][0/48]\tTime 0.046 (0.046)\tLoss 1.2921 (1.2921)\tPrec@1 53.711 (53.711)\n",
            "Epoch: [76][9/48]\tTime 0.041 (0.045)\tLoss 1.2324 (1.2175)\tPrec@1 56.055 (56.455)\n",
            "Epoch: [76][18/48]\tTime 0.067 (0.052)\tLoss 1.2441 (1.2201)\tPrec@1 56.934 (56.363)\n",
            "Epoch: [76][27/48]\tTime 0.081 (0.060)\tLoss 1.1926 (1.2159)\tPrec@1 57.422 (56.428)\n",
            "Epoch: [76][36/48]\tTime 0.086 (0.064)\tLoss 1.2091 (1.2137)\tPrec@1 57.520 (56.464)\n",
            "Epoch: [76][45/48]\tTime 0.055 (0.067)\tLoss 1.1899 (1.2156)\tPrec@1 55.371 (56.307)\n",
            "Epoch: [76][48/48]\tTime 0.068 (0.068)\tLoss 1.1945 (1.2154)\tPrec@1 58.255 (56.376)\n",
            "EPOCH: 76 train Results: Prec@1 56.376 Loss: 1.2154\n",
            "Test: [0/9]\tTime 0.028 (0.028)\tLoss 1.2030 (1.2030)\tPrec@1 57.324 (57.324)\n",
            "Test: [9/9]\tTime 0.024 (0.019)\tLoss 1.2417 (1.2237)\tPrec@1 54.082 (56.490)\n",
            "EPOCH: 76 val Results: Prec@1 56.490 Loss: 1.2237\n",
            "Best Prec@1: 56.540\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [77][0/48]\tTime 0.109 (0.109)\tLoss 1.1682 (1.1682)\tPrec@1 58.594 (58.594)\n",
            "Epoch: [77][9/48]\tTime 0.041 (0.061)\tLoss 1.1758 (1.1891)\tPrec@1 58.105 (57.646)\n",
            "Epoch: [77][18/48]\tTime 0.041 (0.053)\tLoss 1.1680 (1.1994)\tPrec@1 59.766 (57.139)\n",
            "Epoch: [77][27/48]\tTime 0.040 (0.050)\tLoss 1.2729 (1.2059)\tPrec@1 55.762 (56.993)\n",
            "Epoch: [77][36/48]\tTime 0.040 (0.048)\tLoss 1.2339 (1.2056)\tPrec@1 55.762 (57.137)\n",
            "Epoch: [77][45/48]\tTime 0.041 (0.047)\tLoss 1.1850 (1.2094)\tPrec@1 56.445 (56.917)\n",
            "Epoch: [77][48/48]\tTime 0.039 (0.047)\tLoss 1.2268 (1.2106)\tPrec@1 55.189 (56.832)\n",
            "EPOCH: 77 train Results: Prec@1 56.832 Loss: 1.2106\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1972 (1.1972)\tPrec@1 58.691 (58.691)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.2275 (1.2184)\tPrec@1 54.719 (56.530)\n",
            "EPOCH: 77 val Results: Prec@1 56.530 Loss: 1.2184\n",
            "Best Prec@1: 56.540\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [78][0/48]\tTime 0.044 (0.044)\tLoss 1.2262 (1.2262)\tPrec@1 54.980 (54.980)\n",
            "Epoch: [78][9/48]\tTime 0.048 (0.044)\tLoss 1.1642 (1.1858)\tPrec@1 58.301 (57.822)\n",
            "Epoch: [78][18/48]\tTime 0.042 (0.044)\tLoss 1.2265 (1.1984)\tPrec@1 54.590 (57.196)\n",
            "Epoch: [78][27/48]\tTime 0.041 (0.043)\tLoss 1.2134 (1.2067)\tPrec@1 58.691 (57.188)\n",
            "Epoch: [78][36/48]\tTime 0.042 (0.044)\tLoss 1.2208 (1.2122)\tPrec@1 54.980 (56.952)\n",
            "Epoch: [78][45/48]\tTime 0.040 (0.043)\tLoss 1.2100 (1.2116)\tPrec@1 58.594 (56.991)\n",
            "Epoch: [78][48/48]\tTime 0.033 (0.043)\tLoss 1.2143 (1.2108)\tPrec@1 54.245 (56.922)\n",
            "EPOCH: 78 train Results: Prec@1 56.922 Loss: 1.2108\n",
            "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.1890 (1.1890)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.007 (0.012)\tLoss 1.2377 (1.2160)\tPrec@1 53.827 (56.720)\n",
            "EPOCH: 78 val Results: Prec@1 56.720 Loss: 1.2160\n",
            "Best Prec@1: 56.720\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [79][0/48]\tTime 0.042 (0.042)\tLoss 1.1423 (1.1423)\tPrec@1 60.449 (60.449)\n",
            "Epoch: [79][9/48]\tTime 0.042 (0.045)\tLoss 1.1420 (1.1662)\tPrec@1 60.059 (58.848)\n",
            "Epoch: [79][18/48]\tTime 0.043 (0.044)\tLoss 1.2126 (1.1770)\tPrec@1 55.176 (58.244)\n",
            "Epoch: [79][27/48]\tTime 0.041 (0.044)\tLoss 1.2069 (1.1862)\tPrec@1 58.789 (57.962)\n",
            "Epoch: [79][36/48]\tTime 0.047 (0.044)\tLoss 1.2176 (1.1945)\tPrec@1 56.250 (57.646)\n",
            "Epoch: [79][45/48]\tTime 0.041 (0.044)\tLoss 1.2426 (1.2028)\tPrec@1 54.980 (57.106)\n",
            "Epoch: [79][48/48]\tTime 0.034 (0.044)\tLoss 1.2243 (1.2052)\tPrec@1 56.132 (56.976)\n",
            "EPOCH: 79 train Results: Prec@1 56.976 Loss: 1.2052\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1945 (1.1945)\tPrec@1 57.715 (57.715)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.2444 (1.2164)\tPrec@1 54.592 (56.860)\n",
            "EPOCH: 79 val Results: Prec@1 56.860 Loss: 1.2164\n",
            "Best Prec@1: 56.860\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [80][0/48]\tTime 0.045 (0.045)\tLoss 1.1759 (1.1759)\tPrec@1 58.398 (58.398)\n",
            "Epoch: [80][9/48]\tTime 0.043 (0.044)\tLoss 1.2077 (1.1841)\tPrec@1 57.422 (58.369)\n",
            "Epoch: [80][18/48]\tTime 0.041 (0.043)\tLoss 1.1902 (1.1853)\tPrec@1 58.789 (58.003)\n",
            "Epoch: [80][27/48]\tTime 0.063 (0.044)\tLoss 1.2174 (1.1898)\tPrec@1 56.055 (57.610)\n",
            "Epoch: [80][36/48]\tTime 0.043 (0.043)\tLoss 1.2054 (1.1935)\tPrec@1 56.738 (57.483)\n",
            "Epoch: [80][45/48]\tTime 0.042 (0.043)\tLoss 1.2288 (1.1976)\tPrec@1 57.520 (57.449)\n",
            "Epoch: [80][48/48]\tTime 0.034 (0.043)\tLoss 1.2179 (1.1972)\tPrec@1 59.670 (57.510)\n",
            "EPOCH: 80 train Results: Prec@1 57.510 Loss: 1.1972\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1921 (1.1921)\tPrec@1 58.203 (58.203)\n",
            "Test: [9/9]\tTime 0.011 (0.012)\tLoss 1.2352 (1.2131)\tPrec@1 54.592 (56.980)\n",
            "EPOCH: 80 val Results: Prec@1 56.980 Loss: 1.2131\n",
            "Best Prec@1: 56.980\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [81][0/48]\tTime 0.042 (0.042)\tLoss 1.1728 (1.1728)\tPrec@1 58.496 (58.496)\n",
            "Epoch: [81][9/48]\tTime 0.041 (0.042)\tLoss 1.1735 (1.1740)\tPrec@1 59.766 (58.359)\n",
            "Epoch: [81][18/48]\tTime 0.043 (0.042)\tLoss 1.1650 (1.1819)\tPrec@1 58.887 (57.987)\n",
            "Epoch: [81][27/48]\tTime 0.042 (0.043)\tLoss 1.2595 (1.1867)\tPrec@1 55.957 (57.889)\n",
            "Epoch: [81][36/48]\tTime 0.069 (0.049)\tLoss 1.1468 (1.1899)\tPrec@1 58.105 (57.876)\n",
            "Epoch: [81][45/48]\tTime 0.079 (0.055)\tLoss 1.2004 (1.1972)\tPrec@1 56.152 (57.473)\n",
            "Epoch: [81][48/48]\tTime 0.068 (0.055)\tLoss 1.2338 (1.1987)\tPrec@1 56.486 (57.404)\n",
            "EPOCH: 81 train Results: Prec@1 57.404 Loss: 1.1987\n",
            "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1946 (1.1946)\tPrec@1 58.105 (58.105)\n",
            "Test: [9/9]\tTime 0.015 (0.019)\tLoss 1.2336 (1.2119)\tPrec@1 55.102 (56.930)\n",
            "EPOCH: 81 val Results: Prec@1 56.930 Loss: 1.2119\n",
            "Best Prec@1: 56.980\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [82][0/48]\tTime 0.089 (0.089)\tLoss 1.1823 (1.1823)\tPrec@1 57.324 (57.324)\n",
            "Epoch: [82][9/48]\tTime 0.076 (0.080)\tLoss 1.1957 (1.1744)\tPrec@1 57.129 (58.242)\n",
            "Epoch: [82][18/48]\tTime 0.071 (0.080)\tLoss 1.2039 (1.1817)\tPrec@1 56.445 (57.967)\n",
            "Epoch: [82][27/48]\tTime 0.044 (0.068)\tLoss 1.1780 (1.1838)\tPrec@1 57.422 (57.816)\n",
            "Epoch: [82][36/48]\tTime 0.057 (0.063)\tLoss 1.1690 (1.1871)\tPrec@1 57.715 (57.636)\n",
            "Epoch: [82][45/48]\tTime 0.046 (0.059)\tLoss 1.1895 (1.1938)\tPrec@1 56.445 (57.335)\n",
            "Epoch: [82][48/48]\tTime 0.034 (0.058)\tLoss 1.2628 (1.1974)\tPrec@1 54.245 (57.224)\n",
            "EPOCH: 82 train Results: Prec@1 57.224 Loss: 1.1974\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1885 (1.1885)\tPrec@1 58.008 (58.008)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.2329 (1.2094)\tPrec@1 53.571 (56.890)\n",
            "EPOCH: 82 val Results: Prec@1 56.890 Loss: 1.2094\n",
            "Best Prec@1: 56.980\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [83][0/48]\tTime 0.045 (0.045)\tLoss 1.1964 (1.1964)\tPrec@1 57.129 (57.129)\n",
            "Epoch: [83][9/48]\tTime 0.067 (0.045)\tLoss 1.2178 (1.1767)\tPrec@1 58.691 (58.574)\n",
            "Epoch: [83][18/48]\tTime 0.041 (0.044)\tLoss 1.1753 (1.1831)\tPrec@1 57.715 (58.136)\n",
            "Epoch: [83][27/48]\tTime 0.041 (0.044)\tLoss 1.1993 (1.1843)\tPrec@1 58.105 (57.949)\n",
            "Epoch: [83][36/48]\tTime 0.041 (0.044)\tLoss 1.2019 (1.1881)\tPrec@1 57.031 (57.723)\n",
            "Epoch: [83][45/48]\tTime 0.045 (0.044)\tLoss 1.1210 (1.1921)\tPrec@1 59.766 (57.456)\n",
            "Epoch: [83][48/48]\tTime 0.033 (0.044)\tLoss 1.1825 (1.1937)\tPrec@1 54.009 (57.338)\n",
            "EPOCH: 83 train Results: Prec@1 57.338 Loss: 1.1937\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1942 (1.1942)\tPrec@1 58.301 (58.301)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.2258 (1.2060)\tPrec@1 55.357 (57.130)\n",
            "EPOCH: 83 val Results: Prec@1 57.130 Loss: 1.2060\n",
            "Best Prec@1: 57.130\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [84][0/48]\tTime 0.047 (0.047)\tLoss 1.1083 (1.1083)\tPrec@1 60.059 (60.059)\n",
            "Epoch: [84][9/48]\tTime 0.040 (0.044)\tLoss 1.2384 (1.1725)\tPrec@1 54.688 (58.242)\n",
            "Epoch: [84][18/48]\tTime 0.040 (0.043)\tLoss 1.1522 (1.1727)\tPrec@1 59.277 (58.301)\n",
            "Epoch: [84][27/48]\tTime 0.041 (0.043)\tLoss 1.2298 (1.1855)\tPrec@1 56.445 (57.819)\n",
            "Epoch: [84][36/48]\tTime 0.042 (0.043)\tLoss 1.1881 (1.1873)\tPrec@1 56.836 (57.699)\n",
            "Epoch: [84][45/48]\tTime 0.040 (0.043)\tLoss 1.1923 (1.1865)\tPrec@1 59.570 (57.766)\n",
            "Epoch: [84][48/48]\tTime 0.034 (0.043)\tLoss 1.2737 (1.1888)\tPrec@1 55.425 (57.720)\n",
            "EPOCH: 84 train Results: Prec@1 57.720 Loss: 1.1888\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1954 (1.1954)\tPrec@1 57.422 (57.422)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.2241 (1.2030)\tPrec@1 55.102 (57.260)\n",
            "EPOCH: 84 val Results: Prec@1 57.260 Loss: 1.2030\n",
            "Best Prec@1: 57.260\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [85][0/48]\tTime 0.042 (0.042)\tLoss 1.1312 (1.1312)\tPrec@1 58.398 (58.398)\n",
            "Epoch: [85][9/48]\tTime 0.040 (0.044)\tLoss 1.1930 (1.1619)\tPrec@1 58.984 (58.926)\n",
            "Epoch: [85][18/48]\tTime 0.042 (0.043)\tLoss 1.1566 (1.1672)\tPrec@1 60.938 (58.804)\n",
            "Epoch: [85][27/48]\tTime 0.041 (0.043)\tLoss 1.1706 (1.1729)\tPrec@1 57.910 (58.350)\n",
            "Epoch: [85][36/48]\tTime 0.042 (0.043)\tLoss 1.2822 (1.1818)\tPrec@1 54.492 (58.105)\n",
            "Epoch: [85][45/48]\tTime 0.041 (0.043)\tLoss 1.1656 (1.1861)\tPrec@1 59.375 (57.844)\n",
            "Epoch: [85][48/48]\tTime 0.039 (0.043)\tLoss 1.1894 (1.1888)\tPrec@1 56.840 (57.760)\n",
            "EPOCH: 85 train Results: Prec@1 57.760 Loss: 1.1888\n",
            "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.1793 (1.1793)\tPrec@1 58.691 (58.691)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.2139 (1.1986)\tPrec@1 55.867 (57.270)\n",
            "EPOCH: 85 val Results: Prec@1 57.270 Loss: 1.1986\n",
            "Best Prec@1: 57.270\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [86][0/48]\tTime 0.043 (0.043)\tLoss 1.1587 (1.1587)\tPrec@1 58.496 (58.496)\n",
            "Epoch: [86][9/48]\tTime 0.041 (0.043)\tLoss 1.1477 (1.1770)\tPrec@1 59.863 (58.076)\n",
            "Epoch: [86][18/48]\tTime 0.040 (0.042)\tLoss 1.1973 (1.1724)\tPrec@1 58.203 (58.285)\n",
            "Epoch: [86][27/48]\tTime 0.041 (0.043)\tLoss 1.1846 (1.1721)\tPrec@1 59.473 (58.398)\n",
            "Epoch: [86][36/48]\tTime 0.041 (0.043)\tLoss 1.1358 (1.1775)\tPrec@1 59.961 (58.193)\n",
            "Epoch: [86][45/48]\tTime 0.075 (0.043)\tLoss 1.2415 (1.1814)\tPrec@1 56.445 (57.923)\n",
            "Epoch: [86][48/48]\tTime 0.052 (0.044)\tLoss 1.2092 (1.1821)\tPrec@1 59.198 (57.918)\n",
            "EPOCH: 86 train Results: Prec@1 57.918 Loss: 1.1821\n",
            "Test: [0/9]\tTime 0.015 (0.015)\tLoss 1.1731 (1.1731)\tPrec@1 58.496 (58.496)\n",
            "Test: [9/9]\tTime 0.008 (0.019)\tLoss 1.2296 (1.1992)\tPrec@1 56.122 (57.020)\n",
            "EPOCH: 86 val Results: Prec@1 57.020 Loss: 1.1992\n",
            "Best Prec@1: 57.270\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [87][0/48]\tTime 0.062 (0.062)\tLoss 1.1286 (1.1286)\tPrec@1 61.133 (61.133)\n",
            "Epoch: [87][9/48]\tTime 0.092 (0.080)\tLoss 1.1779 (1.1563)\tPrec@1 57.812 (59.355)\n",
            "Epoch: [87][18/48]\tTime 0.079 (0.078)\tLoss 1.1693 (1.1705)\tPrec@1 57.422 (58.671)\n",
            "Epoch: [87][27/48]\tTime 0.059 (0.076)\tLoss 1.1941 (1.1762)\tPrec@1 56.836 (58.343)\n",
            "Epoch: [87][36/48]\tTime 0.040 (0.075)\tLoss 1.2022 (1.1768)\tPrec@1 57.812 (58.319)\n",
            "Epoch: [87][45/48]\tTime 0.042 (0.069)\tLoss 1.2077 (1.1809)\tPrec@1 56.250 (58.105)\n",
            "Epoch: [87][48/48]\tTime 0.033 (0.067)\tLoss 1.1878 (1.1813)\tPrec@1 57.311 (58.054)\n",
            "EPOCH: 87 train Results: Prec@1 58.054 Loss: 1.1813\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1759 (1.1759)\tPrec@1 57.617 (57.617)\n",
            "Test: [9/9]\tTime 0.026 (0.011)\tLoss 1.2177 (1.1983)\tPrec@1 54.847 (57.240)\n",
            "EPOCH: 87 val Results: Prec@1 57.240 Loss: 1.1983\n",
            "Best Prec@1: 57.270\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [88][0/48]\tTime 0.044 (0.044)\tLoss 1.0907 (1.0907)\tPrec@1 62.695 (62.695)\n",
            "Epoch: [88][9/48]\tTime 0.040 (0.042)\tLoss 1.1469 (1.1480)\tPrec@1 57.715 (59.316)\n",
            "Epoch: [88][18/48]\tTime 0.040 (0.043)\tLoss 1.1966 (1.1681)\tPrec@1 56.445 (58.362)\n",
            "Epoch: [88][27/48]\tTime 0.039 (0.043)\tLoss 1.1870 (1.1739)\tPrec@1 57.227 (58.133)\n",
            "Epoch: [88][36/48]\tTime 0.044 (0.043)\tLoss 1.2510 (1.1783)\tPrec@1 56.445 (58.005)\n",
            "Epoch: [88][45/48]\tTime 0.042 (0.044)\tLoss 1.1801 (1.1788)\tPrec@1 57.324 (58.025)\n",
            "Epoch: [88][48/48]\tTime 0.052 (0.044)\tLoss 1.2518 (1.1813)\tPrec@1 57.075 (58.002)\n",
            "EPOCH: 88 train Results: Prec@1 58.002 Loss: 1.1813\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1810 (1.1810)\tPrec@1 58.008 (58.008)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.2118 (1.1998)\tPrec@1 56.505 (57.020)\n",
            "EPOCH: 88 val Results: Prec@1 57.020 Loss: 1.1998\n",
            "Best Prec@1: 57.270\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [89][0/48]\tTime 0.044 (0.044)\tLoss 1.1659 (1.1659)\tPrec@1 56.836 (56.836)\n",
            "Epoch: [89][9/48]\tTime 0.040 (0.045)\tLoss 1.1536 (1.1537)\tPrec@1 59.082 (58.340)\n",
            "Epoch: [89][18/48]\tTime 0.041 (0.044)\tLoss 1.2061 (1.1564)\tPrec@1 57.129 (58.476)\n",
            "Epoch: [89][27/48]\tTime 0.040 (0.044)\tLoss 1.1911 (1.1657)\tPrec@1 56.348 (58.228)\n",
            "Epoch: [89][36/48]\tTime 0.044 (0.044)\tLoss 1.1741 (1.1708)\tPrec@1 58.203 (58.177)\n",
            "Epoch: [89][45/48]\tTime 0.041 (0.044)\tLoss 1.2347 (1.1776)\tPrec@1 57.617 (58.044)\n",
            "Epoch: [89][48/48]\tTime 0.039 (0.044)\tLoss 1.1664 (1.1787)\tPrec@1 58.491 (57.990)\n",
            "EPOCH: 89 train Results: Prec@1 57.990 Loss: 1.1787\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1786 (1.1786)\tPrec@1 58.496 (58.496)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.2224 (1.1981)\tPrec@1 53.827 (57.190)\n",
            "EPOCH: 89 val Results: Prec@1 57.190 Loss: 1.1981\n",
            "Best Prec@1: 57.270\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [90][0/48]\tTime 0.044 (0.044)\tLoss 1.1531 (1.1531)\tPrec@1 58.301 (58.301)\n",
            "Epoch: [90][9/48]\tTime 0.041 (0.045)\tLoss 1.2125 (1.1552)\tPrec@1 57.324 (58.955)\n",
            "Epoch: [90][18/48]\tTime 0.040 (0.044)\tLoss 1.2013 (1.1670)\tPrec@1 57.910 (58.748)\n",
            "Epoch: [90][27/48]\tTime 0.064 (0.044)\tLoss 1.1693 (1.1681)\tPrec@1 60.352 (58.733)\n",
            "Epoch: [90][36/48]\tTime 0.040 (0.044)\tLoss 1.1425 (1.1666)\tPrec@1 59.473 (58.718)\n",
            "Epoch: [90][45/48]\tTime 0.041 (0.043)\tLoss 1.1798 (1.1691)\tPrec@1 56.250 (58.585)\n",
            "Epoch: [90][48/48]\tTime 0.035 (0.043)\tLoss 1.2073 (1.1716)\tPrec@1 56.604 (58.460)\n",
            "EPOCH: 90 train Results: Prec@1 58.460 Loss: 1.1716\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1770 (1.1770)\tPrec@1 58.496 (58.496)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.2207 (1.1953)\tPrec@1 55.230 (57.440)\n",
            "EPOCH: 90 val Results: Prec@1 57.440 Loss: 1.1953\n",
            "Best Prec@1: 57.440\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [91][0/48]\tTime 0.060 (0.060)\tLoss 1.1347 (1.1347)\tPrec@1 59.570 (59.570)\n",
            "Epoch: [91][9/48]\tTime 0.043 (0.044)\tLoss 1.1374 (1.1447)\tPrec@1 59.082 (59.424)\n",
            "Epoch: [91][18/48]\tTime 0.041 (0.043)\tLoss 1.1648 (1.1483)\tPrec@1 58.887 (59.421)\n",
            "Epoch: [91][27/48]\tTime 0.041 (0.044)\tLoss 1.2466 (1.1581)\tPrec@1 56.934 (59.145)\n",
            "Epoch: [91][36/48]\tTime 0.041 (0.043)\tLoss 1.1565 (1.1665)\tPrec@1 59.863 (58.823)\n",
            "Epoch: [91][45/48]\tTime 0.040 (0.043)\tLoss 1.2153 (1.1735)\tPrec@1 55.762 (58.485)\n",
            "Epoch: [91][48/48]\tTime 0.040 (0.043)\tLoss 1.1996 (1.1751)\tPrec@1 58.373 (58.392)\n",
            "EPOCH: 91 train Results: Prec@1 58.392 Loss: 1.1751\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1849 (1.1849)\tPrec@1 59.473 (59.473)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.2358 (1.1989)\tPrec@1 54.337 (57.140)\n",
            "EPOCH: 91 val Results: Prec@1 57.140 Loss: 1.1989\n",
            "Best Prec@1: 57.440\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [92][0/48]\tTime 0.044 (0.044)\tLoss 1.2187 (1.2187)\tPrec@1 54.980 (54.980)\n",
            "Epoch: [92][9/48]\tTime 0.083 (0.048)\tLoss 1.2291 (1.1530)\tPrec@1 56.055 (58.203)\n",
            "Epoch: [92][18/48]\tTime 0.075 (0.063)\tLoss 1.1733 (1.1578)\tPrec@1 58.984 (58.501)\n",
            "Epoch: [92][27/48]\tTime 0.082 (0.070)\tLoss 1.1249 (1.1609)\tPrec@1 60.449 (58.447)\n",
            "Epoch: [92][36/48]\tTime 0.066 (0.072)\tLoss 1.1713 (1.1628)\tPrec@1 60.156 (58.557)\n",
            "Epoch: [92][45/48]\tTime 0.086 (0.074)\tLoss 1.1757 (1.1702)\tPrec@1 58.398 (58.305)\n",
            "Epoch: [92][48/48]\tTime 0.051 (0.073)\tLoss 1.1860 (1.1726)\tPrec@1 57.311 (58.254)\n",
            "EPOCH: 92 train Results: Prec@1 58.254 Loss: 1.1726\n",
            "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.1796 (1.1796)\tPrec@1 58.008 (58.008)\n",
            "Test: [9/9]\tTime 0.014 (0.021)\tLoss 1.2248 (1.1970)\tPrec@1 56.122 (57.170)\n",
            "EPOCH: 92 val Results: Prec@1 57.170 Loss: 1.1970\n",
            "Best Prec@1: 57.440\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [93][0/48]\tTime 0.065 (0.065)\tLoss 1.1210 (1.1210)\tPrec@1 58.496 (58.496)\n",
            "Epoch: [93][9/48]\tTime 0.041 (0.045)\tLoss 1.0813 (1.1329)\tPrec@1 63.867 (59.688)\n",
            "Epoch: [93][18/48]\tTime 0.040 (0.043)\tLoss 1.1763 (1.1443)\tPrec@1 57.715 (59.169)\n",
            "Epoch: [93][27/48]\tTime 0.040 (0.043)\tLoss 1.1981 (1.1539)\tPrec@1 58.301 (58.758)\n",
            "Epoch: [93][36/48]\tTime 0.040 (0.044)\tLoss 1.1318 (1.1578)\tPrec@1 58.887 (58.607)\n",
            "Epoch: [93][45/48]\tTime 0.043 (0.043)\tLoss 1.1918 (1.1689)\tPrec@1 58.398 (58.195)\n",
            "Epoch: [93][48/48]\tTime 0.037 (0.043)\tLoss 1.2228 (1.1697)\tPrec@1 57.193 (58.148)\n",
            "EPOCH: 93 train Results: Prec@1 58.148 Loss: 1.1697\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1694 (1.1694)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.2074 (1.1918)\tPrec@1 56.122 (57.460)\n",
            "EPOCH: 93 val Results: Prec@1 57.460 Loss: 1.1918\n",
            "Best Prec@1: 57.460\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [94][0/48]\tTime 0.043 (0.043)\tLoss 1.1458 (1.1458)\tPrec@1 59.570 (59.570)\n",
            "Epoch: [94][9/48]\tTime 0.040 (0.043)\tLoss 1.1166 (1.1282)\tPrec@1 61.133 (59.746)\n",
            "Epoch: [94][18/48]\tTime 0.045 (0.043)\tLoss 1.1495 (1.1361)\tPrec@1 58.398 (59.524)\n",
            "Epoch: [94][27/48]\tTime 0.060 (0.043)\tLoss 1.1683 (1.1488)\tPrec@1 59.473 (59.086)\n",
            "Epoch: [94][36/48]\tTime 0.046 (0.043)\tLoss 1.2128 (1.1561)\tPrec@1 55.859 (58.847)\n",
            "Epoch: [94][45/48]\tTime 0.043 (0.043)\tLoss 1.1746 (1.1629)\tPrec@1 57.422 (58.626)\n",
            "Epoch: [94][48/48]\tTime 0.040 (0.043)\tLoss 1.2334 (1.1649)\tPrec@1 56.486 (58.560)\n",
            "EPOCH: 94 train Results: Prec@1 58.560 Loss: 1.1649\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1830 (1.1830)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.014 (0.011)\tLoss 1.2217 (1.1990)\tPrec@1 55.740 (57.220)\n",
            "EPOCH: 94 val Results: Prec@1 57.220 Loss: 1.1990\n",
            "Best Prec@1: 57.460\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [95][0/48]\tTime 0.056 (0.056)\tLoss 1.1755 (1.1755)\tPrec@1 58.691 (58.691)\n",
            "Epoch: [95][9/48]\tTime 0.042 (0.043)\tLoss 1.1161 (1.1319)\tPrec@1 58.984 (60.020)\n",
            "Epoch: [95][18/48]\tTime 0.039 (0.043)\tLoss 1.1699 (1.1461)\tPrec@1 59.180 (59.488)\n",
            "Epoch: [95][27/48]\tTime 0.040 (0.043)\tLoss 1.1298 (1.1506)\tPrec@1 59.570 (59.476)\n",
            "Epoch: [95][36/48]\tTime 0.051 (0.043)\tLoss 1.1966 (1.1563)\tPrec@1 55.762 (59.135)\n",
            "Epoch: [95][45/48]\tTime 0.040 (0.042)\tLoss 1.2685 (1.1667)\tPrec@1 55.859 (58.727)\n",
            "Epoch: [95][48/48]\tTime 0.043 (0.042)\tLoss 1.2005 (1.1675)\tPrec@1 57.193 (58.648)\n",
            "EPOCH: 95 train Results: Prec@1 58.648 Loss: 1.1675\n",
            "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.1691 (1.1691)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.2126 (1.1892)\tPrec@1 55.740 (57.540)\n",
            "EPOCH: 95 val Results: Prec@1 57.540 Loss: 1.1892\n",
            "Best Prec@1: 57.540\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [96][0/48]\tTime 0.045 (0.045)\tLoss 1.1415 (1.1415)\tPrec@1 58.691 (58.691)\n",
            "Epoch: [96][9/48]\tTime 0.040 (0.043)\tLoss 1.1680 (1.1351)\tPrec@1 58.887 (59.639)\n",
            "Epoch: [96][18/48]\tTime 0.044 (0.042)\tLoss 1.1663 (1.1416)\tPrec@1 56.055 (59.385)\n",
            "Epoch: [96][27/48]\tTime 0.041 (0.043)\tLoss 1.1729 (1.1537)\tPrec@1 59.766 (59.117)\n",
            "Epoch: [96][36/48]\tTime 0.040 (0.043)\tLoss 1.1347 (1.1543)\tPrec@1 60.938 (59.093)\n",
            "Epoch: [96][45/48]\tTime 0.056 (0.043)\tLoss 1.1444 (1.1571)\tPrec@1 59.180 (58.914)\n",
            "Epoch: [96][48/48]\tTime 0.034 (0.043)\tLoss 1.2133 (1.1608)\tPrec@1 57.075 (58.770)\n",
            "EPOCH: 96 train Results: Prec@1 58.770 Loss: 1.1608\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1617 (1.1617)\tPrec@1 59.375 (59.375)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1992 (1.1882)\tPrec@1 57.908 (57.610)\n",
            "EPOCH: 96 val Results: Prec@1 57.610 Loss: 1.1882\n",
            "Best Prec@1: 57.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [97][0/48]\tTime 0.045 (0.045)\tLoss 1.1736 (1.1736)\tPrec@1 58.496 (58.496)\n",
            "Epoch: [97][9/48]\tTime 0.039 (0.043)\tLoss 1.1841 (1.1343)\tPrec@1 57.422 (60.127)\n",
            "Epoch: [97][18/48]\tTime 0.060 (0.043)\tLoss 1.1454 (1.1490)\tPrec@1 58.984 (59.354)\n",
            "Epoch: [97][27/48]\tTime 0.044 (0.043)\tLoss 1.1879 (1.1519)\tPrec@1 58.887 (59.253)\n",
            "Epoch: [97][36/48]\tTime 0.070 (0.050)\tLoss 1.1509 (1.1551)\tPrec@1 58.691 (59.135)\n",
            "Epoch: [97][45/48]\tTime 0.110 (0.056)\tLoss 1.1811 (1.1615)\tPrec@1 57.812 (58.906)\n",
            "Epoch: [97][48/48]\tTime 0.076 (0.058)\tLoss 1.1706 (1.1620)\tPrec@1 58.019 (58.816)\n",
            "EPOCH: 97 train Results: Prec@1 58.816 Loss: 1.1620\n",
            "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1663 (1.1663)\tPrec@1 59.082 (59.082)\n",
            "Test: [9/9]\tTime 0.015 (0.019)\tLoss 1.1999 (1.1888)\tPrec@1 56.122 (57.160)\n",
            "EPOCH: 97 val Results: Prec@1 57.160 Loss: 1.1888\n",
            "Best Prec@1: 57.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [98][0/48]\tTime 0.090 (0.090)\tLoss 1.1662 (1.1662)\tPrec@1 58.496 (58.496)\n",
            "Epoch: [98][9/48]\tTime 0.077 (0.081)\tLoss 1.0952 (1.1342)\tPrec@1 60.449 (59.482)\n",
            "Epoch: [98][18/48]\tTime 0.040 (0.078)\tLoss 1.1264 (1.1418)\tPrec@1 59.961 (59.370)\n",
            "Epoch: [98][27/48]\tTime 0.040 (0.066)\tLoss 1.1752 (1.1472)\tPrec@1 58.789 (59.204)\n",
            "Epoch: [98][36/48]\tTime 0.042 (0.061)\tLoss 1.1691 (1.1507)\tPrec@1 59.570 (59.314)\n",
            "Epoch: [98][45/48]\tTime 0.040 (0.058)\tLoss 1.1518 (1.1576)\tPrec@1 60.938 (59.025)\n",
            "Epoch: [98][48/48]\tTime 0.034 (0.056)\tLoss 1.2067 (1.1605)\tPrec@1 55.660 (58.890)\n",
            "EPOCH: 98 train Results: Prec@1 58.890 Loss: 1.1605\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1687 (1.1687)\tPrec@1 58.008 (58.008)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.2100 (1.1935)\tPrec@1 55.740 (57.000)\n",
            "EPOCH: 98 val Results: Prec@1 57.000 Loss: 1.1935\n",
            "Best Prec@1: 57.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [99][0/48]\tTime 0.042 (0.042)\tLoss 1.1540 (1.1540)\tPrec@1 57.520 (57.520)\n",
            "Epoch: [99][9/48]\tTime 0.041 (0.046)\tLoss 1.1394 (1.1467)\tPrec@1 59.473 (58.887)\n",
            "Epoch: [99][18/48]\tTime 0.040 (0.044)\tLoss 1.1161 (1.1505)\tPrec@1 59.668 (58.655)\n",
            "Epoch: [99][27/48]\tTime 0.052 (0.044)\tLoss 1.1571 (1.1506)\tPrec@1 60.059 (58.876)\n",
            "Epoch: [99][36/48]\tTime 0.041 (0.044)\tLoss 1.1831 (1.1576)\tPrec@1 58.203 (58.562)\n",
            "Epoch: [99][45/48]\tTime 0.042 (0.044)\tLoss 1.1292 (1.1606)\tPrec@1 60.156 (58.456)\n",
            "Epoch: [99][48/48]\tTime 0.036 (0.043)\tLoss 1.1512 (1.1610)\tPrec@1 59.434 (58.440)\n",
            "EPOCH: 99 train Results: Prec@1 58.440 Loss: 1.1610\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1765 (1.1765)\tPrec@1 58.691 (58.691)\n",
            "Test: [9/9]\tTime 0.011 (0.012)\tLoss 1.2032 (1.1892)\tPrec@1 55.612 (57.530)\n",
            "EPOCH: 99 val Results: Prec@1 57.530 Loss: 1.1892\n",
            "Best Prec@1: 57.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [100][0/48]\tTime 0.046 (0.046)\tLoss 1.1522 (1.1522)\tPrec@1 59.277 (59.277)\n",
            "Epoch: [100][9/48]\tTime 0.043 (0.044)\tLoss 1.1522 (1.1373)\tPrec@1 57.617 (59.590)\n",
            "Epoch: [100][18/48]\tTime 0.041 (0.043)\tLoss 1.1826 (1.1410)\tPrec@1 58.203 (59.565)\n",
            "Epoch: [100][27/48]\tTime 0.040 (0.043)\tLoss 1.1274 (1.1484)\tPrec@1 61.719 (59.406)\n",
            "Epoch: [100][36/48]\tTime 0.043 (0.043)\tLoss 1.1638 (1.1541)\tPrec@1 59.180 (59.172)\n",
            "Epoch: [100][45/48]\tTime 0.043 (0.043)\tLoss 1.1328 (1.1576)\tPrec@1 59.961 (59.012)\n",
            "Epoch: [100][48/48]\tTime 0.036 (0.043)\tLoss 1.1563 (1.1574)\tPrec@1 58.255 (59.012)\n",
            "EPOCH: 100 train Results: Prec@1 59.012 Loss: 1.1574\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1716 (1.1716)\tPrec@1 58.594 (58.594)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.2137 (1.1916)\tPrec@1 53.827 (57.230)\n",
            "EPOCH: 100 val Results: Prec@1 57.230 Loss: 1.1916\n",
            "Best Prec@1: 57.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [101][0/48]\tTime 0.043 (0.043)\tLoss 1.1262 (1.1262)\tPrec@1 61.133 (61.133)\n",
            "Epoch: [101][9/48]\tTime 0.041 (0.043)\tLoss 1.1540 (1.1312)\tPrec@1 59.570 (59.502)\n",
            "Epoch: [101][18/48]\tTime 0.040 (0.043)\tLoss 1.1504 (1.1432)\tPrec@1 58.789 (59.231)\n",
            "Epoch: [101][27/48]\tTime 0.041 (0.043)\tLoss 1.1233 (1.1471)\tPrec@1 60.645 (59.117)\n",
            "Epoch: [101][36/48]\tTime 0.039 (0.043)\tLoss 1.1278 (1.1454)\tPrec@1 57.520 (58.998)\n",
            "Epoch: [101][45/48]\tTime 0.040 (0.043)\tLoss 1.1676 (1.1511)\tPrec@1 58.008 (58.882)\n",
            "Epoch: [101][48/48]\tTime 0.033 (0.043)\tLoss 1.1286 (1.1514)\tPrec@1 61.085 (58.908)\n",
            "EPOCH: 101 train Results: Prec@1 58.908 Loss: 1.1514\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1698 (1.1698)\tPrec@1 58.203 (58.203)\n",
            "Test: [9/9]\tTime 0.012 (0.010)\tLoss 1.2049 (1.1890)\tPrec@1 54.592 (57.470)\n",
            "EPOCH: 101 val Results: Prec@1 57.470 Loss: 1.1890\n",
            "Best Prec@1: 57.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [102][0/48]\tTime 0.045 (0.045)\tLoss 1.0971 (1.0971)\tPrec@1 61.328 (61.328)\n",
            "Epoch: [102][9/48]\tTime 0.041 (0.042)\tLoss 1.1528 (1.1272)\tPrec@1 58.496 (60.010)\n",
            "Epoch: [102][18/48]\tTime 0.041 (0.044)\tLoss 1.1529 (1.1345)\tPrec@1 57.812 (59.987)\n",
            "Epoch: [102][27/48]\tTime 0.040 (0.044)\tLoss 1.1833 (1.1422)\tPrec@1 57.129 (59.462)\n",
            "Epoch: [102][36/48]\tTime 0.045 (0.043)\tLoss 1.1576 (1.1477)\tPrec@1 60.938 (59.248)\n",
            "Epoch: [102][45/48]\tTime 0.083 (0.046)\tLoss 1.1645 (1.1506)\tPrec@1 57.910 (59.226)\n",
            "Epoch: [102][48/48]\tTime 0.075 (0.048)\tLoss 1.1535 (1.1534)\tPrec@1 58.608 (59.062)\n",
            "EPOCH: 102 train Results: Prec@1 59.062 Loss: 1.1534\n",
            "Test: [0/9]\tTime 0.022 (0.022)\tLoss 1.1757 (1.1757)\tPrec@1 57.910 (57.910)\n",
            "Test: [9/9]\tTime 0.015 (0.019)\tLoss 1.2145 (1.1883)\tPrec@1 55.230 (57.530)\n",
            "EPOCH: 102 val Results: Prec@1 57.530 Loss: 1.1883\n",
            "Best Prec@1: 57.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [103][0/48]\tTime 0.083 (0.083)\tLoss 1.1310 (1.1310)\tPrec@1 58.691 (58.691)\n",
            "Epoch: [103][9/48]\tTime 0.080 (0.081)\tLoss 1.1387 (1.1364)\tPrec@1 60.645 (60.059)\n",
            "Epoch: [103][18/48]\tTime 0.086 (0.080)\tLoss 1.1483 (1.1357)\tPrec@1 59.277 (59.699)\n",
            "Epoch: [103][27/48]\tTime 0.066 (0.079)\tLoss 1.1415 (1.1377)\tPrec@1 59.277 (59.734)\n",
            "Epoch: [103][36/48]\tTime 0.040 (0.072)\tLoss 1.1463 (1.1457)\tPrec@1 58.105 (59.254)\n",
            "Epoch: [103][45/48]\tTime 0.040 (0.066)\tLoss 1.1482 (1.1503)\tPrec@1 58.887 (59.086)\n",
            "Epoch: [103][48/48]\tTime 0.040 (0.065)\tLoss 1.1533 (1.1514)\tPrec@1 58.137 (58.982)\n",
            "EPOCH: 103 train Results: Prec@1 58.982 Loss: 1.1514\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1579 (1.1579)\tPrec@1 58.691 (58.691)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.2009 (1.1860)\tPrec@1 55.485 (57.680)\n",
            "EPOCH: 103 val Results: Prec@1 57.680 Loss: 1.1860\n",
            "Best Prec@1: 57.680\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [104][0/48]\tTime 0.044 (0.044)\tLoss 1.1375 (1.1375)\tPrec@1 60.742 (60.742)\n",
            "Epoch: [104][9/48]\tTime 0.040 (0.044)\tLoss 1.1461 (1.1162)\tPrec@1 57.617 (60.361)\n",
            "Epoch: [104][18/48]\tTime 0.040 (0.044)\tLoss 1.1170 (1.1268)\tPrec@1 59.570 (59.719)\n",
            "Epoch: [104][27/48]\tTime 0.044 (0.043)\tLoss 1.1528 (1.1352)\tPrec@1 60.742 (59.598)\n",
            "Epoch: [104][36/48]\tTime 0.040 (0.044)\tLoss 1.1923 (1.1440)\tPrec@1 59.277 (59.354)\n",
            "Epoch: [104][45/48]\tTime 0.045 (0.043)\tLoss 1.1813 (1.1471)\tPrec@1 56.543 (59.120)\n",
            "Epoch: [104][48/48]\tTime 0.033 (0.043)\tLoss 1.1322 (1.1490)\tPrec@1 58.726 (59.048)\n",
            "EPOCH: 104 train Results: Prec@1 59.048 Loss: 1.1490\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1573 (1.1573)\tPrec@1 59.570 (59.570)\n",
            "Test: [9/9]\tTime 0.007 (0.012)\tLoss 1.2155 (1.1848)\tPrec@1 54.847 (57.860)\n",
            "EPOCH: 104 val Results: Prec@1 57.860 Loss: 1.1848\n",
            "Best Prec@1: 57.860\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [105][0/48]\tTime 0.044 (0.044)\tLoss 1.1581 (1.1581)\tPrec@1 59.277 (59.277)\n",
            "Epoch: [105][9/48]\tTime 0.043 (0.042)\tLoss 1.1763 (1.1328)\tPrec@1 59.375 (59.902)\n",
            "Epoch: [105][18/48]\tTime 0.040 (0.043)\tLoss 1.1526 (1.1385)\tPrec@1 59.570 (59.550)\n",
            "Epoch: [105][27/48]\tTime 0.039 (0.043)\tLoss 1.1451 (1.1360)\tPrec@1 60.449 (59.598)\n",
            "Epoch: [105][36/48]\tTime 0.042 (0.043)\tLoss 1.1032 (1.1382)\tPrec@1 59.863 (59.452)\n",
            "Epoch: [105][45/48]\tTime 0.042 (0.043)\tLoss 1.1986 (1.1477)\tPrec@1 56.934 (59.069)\n",
            "Epoch: [105][48/48]\tTime 0.037 (0.043)\tLoss 1.1114 (1.1496)\tPrec@1 61.203 (59.046)\n",
            "EPOCH: 105 train Results: Prec@1 59.046 Loss: 1.1496\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1582 (1.1582)\tPrec@1 60.938 (60.938)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1889 (1.1828)\tPrec@1 55.102 (58.100)\n",
            "EPOCH: 105 val Results: Prec@1 58.100 Loss: 1.1828\n",
            "Best Prec@1: 58.100\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [106][0/48]\tTime 0.043 (0.043)\tLoss 1.0814 (1.0814)\tPrec@1 62.012 (62.012)\n",
            "Epoch: [106][9/48]\tTime 0.040 (0.042)\tLoss 1.1015 (1.1086)\tPrec@1 59.668 (61.084)\n",
            "Epoch: [106][18/48]\tTime 0.041 (0.043)\tLoss 1.1516 (1.1239)\tPrec@1 58.984 (60.238)\n",
            "Epoch: [106][27/48]\tTime 0.043 (0.044)\tLoss 1.1535 (1.1299)\tPrec@1 59.277 (60.118)\n",
            "Epoch: [106][36/48]\tTime 0.040 (0.043)\tLoss 1.1609 (1.1387)\tPrec@1 57.031 (59.700)\n",
            "Epoch: [106][45/48]\tTime 0.041 (0.043)\tLoss 1.1882 (1.1445)\tPrec@1 57.812 (59.477)\n",
            "Epoch: [106][48/48]\tTime 0.035 (0.043)\tLoss 1.1044 (1.1465)\tPrec@1 61.321 (59.446)\n",
            "EPOCH: 106 train Results: Prec@1 59.446 Loss: 1.1465\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1502 (1.1502)\tPrec@1 60.742 (60.742)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1946 (1.1772)\tPrec@1 57.143 (58.410)\n",
            "EPOCH: 106 val Results: Prec@1 58.410 Loss: 1.1772\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [107][0/48]\tTime 0.042 (0.042)\tLoss 1.1039 (1.1039)\tPrec@1 62.305 (62.305)\n",
            "Epoch: [107][9/48]\tTime 0.041 (0.044)\tLoss 1.1496 (1.1125)\tPrec@1 58.203 (60.811)\n",
            "Epoch: [107][18/48]\tTime 0.040 (0.044)\tLoss 1.1180 (1.1185)\tPrec@1 61.426 (60.675)\n",
            "Epoch: [107][27/48]\tTime 0.046 (0.043)\tLoss 1.1065 (1.1179)\tPrec@1 61.719 (60.798)\n",
            "Epoch: [107][36/48]\tTime 0.040 (0.043)\tLoss 1.2000 (1.1297)\tPrec@1 57.031 (60.367)\n",
            "Epoch: [107][45/48]\tTime 0.041 (0.043)\tLoss 1.1536 (1.1381)\tPrec@1 58.691 (59.874)\n",
            "Epoch: [107][48/48]\tTime 0.033 (0.043)\tLoss 1.1846 (1.1407)\tPrec@1 58.019 (59.732)\n",
            "EPOCH: 107 train Results: Prec@1 59.732 Loss: 1.1407\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1521 (1.1521)\tPrec@1 59.082 (59.082)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1908 (1.1835)\tPrec@1 54.847 (57.870)\n",
            "EPOCH: 107 val Results: Prec@1 57.870 Loss: 1.1835\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [108][0/48]\tTime 0.051 (0.051)\tLoss 1.0777 (1.0777)\tPrec@1 63.086 (63.086)\n",
            "Epoch: [108][9/48]\tTime 0.052 (0.052)\tLoss 1.0866 (1.1073)\tPrec@1 61.035 (60.996)\n",
            "Epoch: [108][18/48]\tTime 0.079 (0.060)\tLoss 1.1525 (1.1248)\tPrec@1 58.496 (60.151)\n",
            "Epoch: [108][27/48]\tTime 0.078 (0.067)\tLoss 1.1606 (1.1320)\tPrec@1 59.277 (59.710)\n",
            "Epoch: [108][36/48]\tTime 0.048 (0.069)\tLoss 1.1133 (1.1390)\tPrec@1 60.449 (59.459)\n",
            "Epoch: [108][45/48]\tTime 0.096 (0.070)\tLoss 1.1636 (1.1425)\tPrec@1 58.301 (59.405)\n",
            "Epoch: [108][48/48]\tTime 0.033 (0.070)\tLoss 1.2060 (1.1435)\tPrec@1 56.014 (59.354)\n",
            "EPOCH: 108 train Results: Prec@1 59.354 Loss: 1.1435\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1617 (1.1617)\tPrec@1 58.691 (58.691)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1919 (1.1820)\tPrec@1 57.015 (57.830)\n",
            "EPOCH: 108 val Results: Prec@1 57.830 Loss: 1.1820\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [109][0/48]\tTime 0.043 (0.043)\tLoss 1.1790 (1.1790)\tPrec@1 56.543 (56.543)\n",
            "Epoch: [109][9/48]\tTime 0.041 (0.045)\tLoss 1.1496 (1.1306)\tPrec@1 59.277 (59.795)\n",
            "Epoch: [109][18/48]\tTime 0.040 (0.043)\tLoss 1.1422 (1.1399)\tPrec@1 60.645 (59.642)\n",
            "Epoch: [109][27/48]\tTime 0.067 (0.044)\tLoss 1.1502 (1.1395)\tPrec@1 57.617 (59.584)\n",
            "Epoch: [109][36/48]\tTime 0.044 (0.044)\tLoss 1.1440 (1.1432)\tPrec@1 60.742 (59.560)\n",
            "Epoch: [109][45/48]\tTime 0.041 (0.043)\tLoss 1.1620 (1.1441)\tPrec@1 59.961 (59.468)\n",
            "Epoch: [109][48/48]\tTime 0.034 (0.043)\tLoss 1.1599 (1.1456)\tPrec@1 59.316 (59.438)\n",
            "EPOCH: 109 train Results: Prec@1 59.438 Loss: 1.1456\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1447 (1.1447)\tPrec@1 60.352 (60.352)\n",
            "Test: [9/9]\tTime 0.009 (0.010)\tLoss 1.1990 (1.1764)\tPrec@1 57.653 (58.390)\n",
            "EPOCH: 109 val Results: Prec@1 58.390 Loss: 1.1764\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [110][0/48]\tTime 0.067 (0.067)\tLoss 1.1140 (1.1140)\tPrec@1 61.230 (61.230)\n",
            "Epoch: [110][9/48]\tTime 0.040 (0.045)\tLoss 1.0386 (1.1101)\tPrec@1 63.477 (60.693)\n",
            "Epoch: [110][18/48]\tTime 0.040 (0.043)\tLoss 1.1097 (1.1189)\tPrec@1 62.012 (60.434)\n",
            "Epoch: [110][27/48]\tTime 0.041 (0.044)\tLoss 1.1772 (1.1249)\tPrec@1 58.105 (60.191)\n",
            "Epoch: [110][36/48]\tTime 0.043 (0.044)\tLoss 1.1630 (1.1343)\tPrec@1 59.473 (59.869)\n",
            "Epoch: [110][45/48]\tTime 0.047 (0.044)\tLoss 1.1056 (1.1395)\tPrec@1 58.301 (59.481)\n",
            "Epoch: [110][48/48]\tTime 0.042 (0.044)\tLoss 1.1861 (1.1399)\tPrec@1 57.901 (59.456)\n",
            "EPOCH: 110 train Results: Prec@1 59.456 Loss: 1.1399\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1487 (1.1487)\tPrec@1 59.863 (59.863)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1946 (1.1788)\tPrec@1 57.270 (58.040)\n",
            "EPOCH: 110 val Results: Prec@1 58.040 Loss: 1.1788\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [111][0/48]\tTime 0.047 (0.047)\tLoss 1.1013 (1.1013)\tPrec@1 60.645 (60.645)\n",
            "Epoch: [111][9/48]\tTime 0.040 (0.042)\tLoss 1.1267 (1.1205)\tPrec@1 60.254 (60.527)\n",
            "Epoch: [111][18/48]\tTime 0.040 (0.042)\tLoss 1.1269 (1.1291)\tPrec@1 61.523 (59.951)\n",
            "Epoch: [111][27/48]\tTime 0.040 (0.043)\tLoss 1.1105 (1.1429)\tPrec@1 61.133 (59.434)\n",
            "Epoch: [111][36/48]\tTime 0.040 (0.042)\tLoss 1.2213 (1.1485)\tPrec@1 57.520 (59.298)\n",
            "Epoch: [111][45/48]\tTime 0.044 (0.043)\tLoss 1.1985 (1.1478)\tPrec@1 59.277 (59.350)\n",
            "Epoch: [111][48/48]\tTime 0.048 (0.043)\tLoss 1.2344 (1.1486)\tPrec@1 56.250 (59.316)\n",
            "EPOCH: 111 train Results: Prec@1 59.316 Loss: 1.1486\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1385 (1.1385)\tPrec@1 60.059 (60.059)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1895 (1.1806)\tPrec@1 56.122 (57.940)\n",
            "EPOCH: 111 val Results: Prec@1 57.940 Loss: 1.1806\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [112][0/48]\tTime 0.054 (0.054)\tLoss 1.1121 (1.1121)\tPrec@1 59.375 (59.375)\n",
            "Epoch: [112][9/48]\tTime 0.040 (0.043)\tLoss 1.0982 (1.1146)\tPrec@1 61.523 (60.156)\n",
            "Epoch: [112][18/48]\tTime 0.041 (0.044)\tLoss 1.1158 (1.1205)\tPrec@1 59.961 (59.997)\n",
            "Epoch: [112][27/48]\tTime 0.044 (0.044)\tLoss 1.0742 (1.1236)\tPrec@1 62.402 (59.832)\n",
            "Epoch: [112][36/48]\tTime 0.041 (0.043)\tLoss 1.1204 (1.1352)\tPrec@1 60.156 (59.415)\n",
            "Epoch: [112][45/48]\tTime 0.043 (0.044)\tLoss 1.1918 (1.1410)\tPrec@1 57.715 (59.163)\n",
            "Epoch: [112][48/48]\tTime 0.034 (0.044)\tLoss 1.2008 (1.1419)\tPrec@1 56.250 (59.096)\n",
            "EPOCH: 112 train Results: Prec@1 59.096 Loss: 1.1419\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1359 (1.1359)\tPrec@1 60.742 (60.742)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1858 (1.1771)\tPrec@1 58.036 (58.110)\n",
            "EPOCH: 112 val Results: Prec@1 58.110 Loss: 1.1771\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [113][0/48]\tTime 0.043 (0.043)\tLoss 1.1550 (1.1550)\tPrec@1 57.910 (57.910)\n",
            "Epoch: [113][9/48]\tTime 0.050 (0.044)\tLoss 1.1718 (1.1160)\tPrec@1 59.375 (60.391)\n",
            "Epoch: [113][18/48]\tTime 0.043 (0.045)\tLoss 1.1608 (1.1226)\tPrec@1 59.766 (60.300)\n",
            "Epoch: [113][27/48]\tTime 0.083 (0.052)\tLoss 1.1226 (1.1310)\tPrec@1 59.863 (59.769)\n",
            "Epoch: [113][36/48]\tTime 0.114 (0.061)\tLoss 1.1614 (1.1326)\tPrec@1 58.203 (59.789)\n",
            "Epoch: [113][45/48]\tTime 0.083 (0.065)\tLoss 1.1577 (1.1382)\tPrec@1 58.887 (59.555)\n",
            "Epoch: [113][48/48]\tTime 0.066 (0.065)\tLoss 1.1287 (1.1400)\tPrec@1 59.434 (59.520)\n",
            "EPOCH: 113 train Results: Prec@1 59.520 Loss: 1.1400\n",
            "Test: [0/9]\tTime 0.021 (0.021)\tLoss 1.1450 (1.1450)\tPrec@1 60.059 (60.059)\n",
            "Test: [9/9]\tTime 0.014 (0.019)\tLoss 1.1953 (1.1807)\tPrec@1 57.526 (57.890)\n",
            "EPOCH: 113 val Results: Prec@1 57.890 Loss: 1.1807\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [114][0/48]\tTime 0.090 (0.090)\tLoss 1.0872 (1.0872)\tPrec@1 61.621 (61.621)\n",
            "Epoch: [114][9/48]\tTime 0.073 (0.070)\tLoss 1.1133 (1.1194)\tPrec@1 60.059 (60.342)\n",
            "Epoch: [114][18/48]\tTime 0.041 (0.062)\tLoss 1.1762 (1.1281)\tPrec@1 59.180 (60.531)\n",
            "Epoch: [114][27/48]\tTime 0.041 (0.056)\tLoss 1.0801 (1.1301)\tPrec@1 63.574 (60.310)\n",
            "Epoch: [114][36/48]\tTime 0.041 (0.053)\tLoss 1.0778 (1.1295)\tPrec@1 63.672 (60.407)\n",
            "Epoch: [114][45/48]\tTime 0.044 (0.051)\tLoss 1.1922 (1.1366)\tPrec@1 56.641 (60.071)\n",
            "Epoch: [114][48/48]\tTime 0.042 (0.051)\tLoss 1.2068 (1.1393)\tPrec@1 53.420 (59.898)\n",
            "EPOCH: 114 train Results: Prec@1 59.898 Loss: 1.1393\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1473 (1.1473)\tPrec@1 60.156 (60.156)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1943 (1.1819)\tPrec@1 55.102 (57.890)\n",
            "EPOCH: 114 val Results: Prec@1 57.890 Loss: 1.1819\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [115][0/48]\tTime 0.043 (0.043)\tLoss 1.1224 (1.1224)\tPrec@1 61.035 (61.035)\n",
            "Epoch: [115][9/48]\tTime 0.051 (0.043)\tLoss 1.1240 (1.0993)\tPrec@1 60.059 (60.986)\n",
            "Epoch: [115][18/48]\tTime 0.040 (0.043)\tLoss 1.1159 (1.1148)\tPrec@1 60.254 (60.501)\n",
            "Epoch: [115][27/48]\tTime 0.041 (0.043)\tLoss 1.1866 (1.1193)\tPrec@1 57.617 (60.324)\n",
            "Epoch: [115][36/48]\tTime 0.040 (0.043)\tLoss 1.1326 (1.1273)\tPrec@1 60.156 (60.040)\n",
            "Epoch: [115][45/48]\tTime 0.041 (0.043)\tLoss 1.1470 (1.1330)\tPrec@1 58.008 (59.795)\n",
            "Epoch: [115][48/48]\tTime 0.034 (0.043)\tLoss 1.1298 (1.1327)\tPrec@1 61.439 (59.880)\n",
            "EPOCH: 115 train Results: Prec@1 59.880 Loss: 1.1327\n",
            "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.1408 (1.1408)\tPrec@1 59.180 (59.180)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1865 (1.1778)\tPrec@1 57.015 (57.720)\n",
            "EPOCH: 115 val Results: Prec@1 57.720 Loss: 1.1778\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [116][0/48]\tTime 0.051 (0.051)\tLoss 1.1173 (1.1173)\tPrec@1 59.473 (59.473)\n",
            "Epoch: [116][9/48]\tTime 0.043 (0.043)\tLoss 1.1319 (1.1149)\tPrec@1 59.473 (60.586)\n",
            "Epoch: [116][18/48]\tTime 0.040 (0.044)\tLoss 1.1291 (1.1157)\tPrec@1 58.105 (60.362)\n",
            "Epoch: [116][27/48]\tTime 0.040 (0.044)\tLoss 1.1094 (1.1185)\tPrec@1 61.523 (60.083)\n",
            "Epoch: [116][36/48]\tTime 0.040 (0.044)\tLoss 1.1776 (1.1263)\tPrec@1 57.617 (59.900)\n",
            "Epoch: [116][45/48]\tTime 0.042 (0.044)\tLoss 1.1697 (1.1294)\tPrec@1 59.277 (59.865)\n",
            "Epoch: [116][48/48]\tTime 0.034 (0.044)\tLoss 1.1726 (1.1319)\tPrec@1 57.901 (59.782)\n",
            "EPOCH: 116 train Results: Prec@1 59.782 Loss: 1.1319\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1569 (1.1569)\tPrec@1 59.570 (59.570)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1737 (1.1763)\tPrec@1 58.163 (57.850)\n",
            "EPOCH: 116 val Results: Prec@1 57.850 Loss: 1.1763\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [117][0/48]\tTime 0.043 (0.043)\tLoss 1.1026 (1.1026)\tPrec@1 62.500 (62.500)\n",
            "Epoch: [117][9/48]\tTime 0.043 (0.042)\tLoss 1.1315 (1.1107)\tPrec@1 59.766 (60.674)\n",
            "Epoch: [117][18/48]\tTime 0.041 (0.043)\tLoss 1.0849 (1.1191)\tPrec@1 63.574 (60.362)\n",
            "Epoch: [117][27/48]\tTime 0.049 (0.043)\tLoss 1.1124 (1.1205)\tPrec@1 58.984 (60.296)\n",
            "Epoch: [117][36/48]\tTime 0.059 (0.043)\tLoss 1.1376 (1.1290)\tPrec@1 59.570 (59.869)\n",
            "Epoch: [117][45/48]\tTime 0.041 (0.043)\tLoss 1.1644 (1.1328)\tPrec@1 60.059 (59.819)\n",
            "Epoch: [117][48/48]\tTime 0.037 (0.043)\tLoss 1.1585 (1.1332)\tPrec@1 58.137 (59.834)\n",
            "EPOCH: 117 train Results: Prec@1 59.834 Loss: 1.1332\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1398 (1.1398)\tPrec@1 60.742 (60.742)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1820 (1.1728)\tPrec@1 56.760 (58.020)\n",
            "EPOCH: 117 val Results: Prec@1 58.020 Loss: 1.1728\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [118][0/48]\tTime 0.042 (0.042)\tLoss 1.1160 (1.1160)\tPrec@1 60.938 (60.938)\n",
            "Epoch: [118][9/48]\tTime 0.067 (0.046)\tLoss 1.0750 (1.1101)\tPrec@1 63.672 (60.781)\n",
            "Epoch: [118][18/48]\tTime 0.041 (0.044)\tLoss 1.1563 (1.1136)\tPrec@1 59.082 (60.747)\n",
            "Epoch: [118][27/48]\tTime 0.045 (0.044)\tLoss 1.1427 (1.1190)\tPrec@1 58.594 (60.400)\n",
            "Epoch: [118][36/48]\tTime 0.041 (0.044)\tLoss 1.1794 (1.1258)\tPrec@1 58.984 (60.114)\n",
            "Epoch: [118][45/48]\tTime 0.076 (0.051)\tLoss 1.1472 (1.1322)\tPrec@1 59.180 (59.897)\n",
            "Epoch: [118][48/48]\tTime 0.079 (0.053)\tLoss 1.0963 (1.1321)\tPrec@1 61.321 (59.916)\n",
            "EPOCH: 118 train Results: Prec@1 59.916 Loss: 1.1321\n",
            "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.1379 (1.1379)\tPrec@1 60.938 (60.938)\n",
            "Test: [9/9]\tTime 0.021 (0.019)\tLoss 1.1832 (1.1702)\tPrec@1 55.995 (58.260)\n",
            "EPOCH: 118 val Results: Prec@1 58.260 Loss: 1.1702\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [119][0/48]\tTime 0.094 (0.094)\tLoss 1.0779 (1.0779)\tPrec@1 63.281 (63.281)\n",
            "Epoch: [119][9/48]\tTime 0.076 (0.083)\tLoss 1.1319 (1.0987)\tPrec@1 57.422 (61.475)\n",
            "Epoch: [119][18/48]\tTime 0.106 (0.081)\tLoss 1.1431 (1.1093)\tPrec@1 61.328 (60.860)\n",
            "Epoch: [119][27/48]\tTime 0.040 (0.076)\tLoss 1.1063 (1.1173)\tPrec@1 59.473 (60.470)\n",
            "Epoch: [119][36/48]\tTime 0.040 (0.068)\tLoss 1.1221 (1.1197)\tPrec@1 58.691 (60.349)\n",
            "Epoch: [119][45/48]\tTime 0.042 (0.063)\tLoss 1.1866 (1.1266)\tPrec@1 57.715 (60.020)\n",
            "Epoch: [119][48/48]\tTime 0.047 (0.062)\tLoss 1.0953 (1.1272)\tPrec@1 62.618 (60.040)\n",
            "EPOCH: 119 train Results: Prec@1 60.040 Loss: 1.1272\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1342 (1.1342)\tPrec@1 59.863 (59.863)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1801 (1.1709)\tPrec@1 56.760 (57.990)\n",
            "EPOCH: 119 val Results: Prec@1 57.990 Loss: 1.1709\n",
            "Best Prec@1: 58.410\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [120][0/48]\tTime 0.048 (0.048)\tLoss 1.0422 (1.0422)\tPrec@1 61.523 (61.523)\n",
            "Epoch: [120][9/48]\tTime 0.040 (0.043)\tLoss 1.1678 (1.1054)\tPrec@1 59.180 (60.635)\n",
            "Epoch: [120][18/48]\tTime 0.040 (0.044)\tLoss 1.1044 (1.1184)\tPrec@1 60.254 (60.151)\n",
            "Epoch: [120][27/48]\tTime 0.045 (0.043)\tLoss 1.1945 (1.1222)\tPrec@1 57.324 (60.118)\n",
            "Epoch: [120][36/48]\tTime 0.041 (0.043)\tLoss 1.1285 (1.1231)\tPrec@1 60.645 (60.037)\n",
            "Epoch: [120][45/48]\tTime 0.041 (0.043)\tLoss 1.1683 (1.1275)\tPrec@1 57.422 (59.901)\n",
            "Epoch: [120][48/48]\tTime 0.036 (0.043)\tLoss 1.1211 (1.1291)\tPrec@1 59.316 (59.828)\n",
            "EPOCH: 120 train Results: Prec@1 59.828 Loss: 1.1291\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1384 (1.1384)\tPrec@1 59.766 (59.766)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1750 (1.1653)\tPrec@1 58.418 (58.610)\n",
            "EPOCH: 120 val Results: Prec@1 58.610 Loss: 1.1653\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [121][0/48]\tTime 0.049 (0.049)\tLoss 1.1276 (1.1276)\tPrec@1 58.203 (58.203)\n",
            "Epoch: [121][9/48]\tTime 0.041 (0.043)\tLoss 1.0859 (1.1036)\tPrec@1 61.523 (61.055)\n",
            "Epoch: [121][18/48]\tTime 0.040 (0.043)\tLoss 1.1282 (1.1182)\tPrec@1 60.645 (60.542)\n",
            "Epoch: [121][27/48]\tTime 0.039 (0.043)\tLoss 1.1005 (1.1188)\tPrec@1 60.059 (60.498)\n",
            "Epoch: [121][36/48]\tTime 0.041 (0.043)\tLoss 1.1108 (1.1233)\tPrec@1 58.887 (60.228)\n",
            "Epoch: [121][45/48]\tTime 0.040 (0.043)\tLoss 1.1172 (1.1276)\tPrec@1 58.789 (59.974)\n",
            "Epoch: [121][48/48]\tTime 0.034 (0.043)\tLoss 1.1117 (1.1284)\tPrec@1 59.788 (59.944)\n",
            "EPOCH: 121 train Results: Prec@1 59.944 Loss: 1.1284\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1445 (1.1445)\tPrec@1 58.398 (58.398)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1782 (1.1727)\tPrec@1 55.485 (57.630)\n",
            "EPOCH: 121 val Results: Prec@1 57.630 Loss: 1.1727\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [122][0/48]\tTime 0.044 (0.044)\tLoss 1.1277 (1.1277)\tPrec@1 59.277 (59.277)\n",
            "Epoch: [122][9/48]\tTime 0.041 (0.044)\tLoss 1.0863 (1.0965)\tPrec@1 60.840 (61.348)\n",
            "Epoch: [122][18/48]\tTime 0.045 (0.044)\tLoss 1.1075 (1.1073)\tPrec@1 60.449 (60.927)\n",
            "Epoch: [122][27/48]\tTime 0.041 (0.044)\tLoss 1.1376 (1.1145)\tPrec@1 60.840 (60.547)\n",
            "Epoch: [122][36/48]\tTime 0.040 (0.044)\tLoss 1.1476 (1.1219)\tPrec@1 59.766 (60.320)\n",
            "Epoch: [122][45/48]\tTime 0.044 (0.044)\tLoss 1.1242 (1.1297)\tPrec@1 60.645 (60.127)\n",
            "Epoch: [122][48/48]\tTime 0.037 (0.044)\tLoss 1.1664 (1.1313)\tPrec@1 59.316 (60.036)\n",
            "EPOCH: 122 train Results: Prec@1 60.036 Loss: 1.1313\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1525 (1.1525)\tPrec@1 57.520 (57.520)\n",
            "Test: [9/9]\tTime 0.009 (0.010)\tLoss 1.1772 (1.1742)\tPrec@1 58.036 (58.260)\n",
            "EPOCH: 122 val Results: Prec@1 58.260 Loss: 1.1742\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [123][0/48]\tTime 0.045 (0.045)\tLoss 1.0627 (1.0627)\tPrec@1 61.133 (61.133)\n",
            "Epoch: [123][9/48]\tTime 0.044 (0.046)\tLoss 1.0826 (1.0981)\tPrec@1 62.207 (60.908)\n",
            "Epoch: [123][18/48]\tTime 0.040 (0.044)\tLoss 1.2057 (1.1118)\tPrec@1 57.812 (60.639)\n",
            "Epoch: [123][27/48]\tTime 0.040 (0.043)\tLoss 1.1269 (1.1160)\tPrec@1 62.012 (60.617)\n",
            "Epoch: [123][36/48]\tTime 0.046 (0.044)\tLoss 1.1079 (1.1231)\tPrec@1 59.863 (60.301)\n",
            "Epoch: [123][45/48]\tTime 0.041 (0.043)\tLoss 1.1172 (1.1277)\tPrec@1 60.059 (60.025)\n",
            "Epoch: [123][48/48]\tTime 0.040 (0.043)\tLoss 1.1246 (1.1301)\tPrec@1 60.613 (59.996)\n",
            "EPOCH: 123 train Results: Prec@1 59.996 Loss: 1.1301\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1410 (1.1410)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.009 (0.010)\tLoss 1.1754 (1.1679)\tPrec@1 57.526 (58.550)\n",
            "EPOCH: 123 val Results: Prec@1 58.550 Loss: 1.1679\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [124][0/48]\tTime 0.066 (0.066)\tLoss 1.1064 (1.1064)\tPrec@1 59.082 (59.082)\n",
            "Epoch: [124][9/48]\tTime 0.079 (0.081)\tLoss 1.1042 (1.0978)\tPrec@1 58.984 (60.859)\n",
            "Epoch: [124][18/48]\tTime 0.087 (0.079)\tLoss 1.0662 (1.1105)\tPrec@1 62.305 (60.403)\n",
            "Epoch: [124][27/48]\tTime 0.053 (0.078)\tLoss 1.0909 (1.1165)\tPrec@1 62.109 (60.247)\n",
            "Epoch: [124][36/48]\tTime 0.084 (0.078)\tLoss 1.1351 (1.1167)\tPrec@1 59.668 (60.214)\n",
            "Epoch: [124][45/48]\tTime 0.044 (0.075)\tLoss 1.1408 (1.1229)\tPrec@1 60.254 (60.084)\n",
            "Epoch: [124][48/48]\tTime 0.034 (0.073)\tLoss 1.1533 (1.1237)\tPrec@1 59.788 (60.068)\n",
            "EPOCH: 124 train Results: Prec@1 60.068 Loss: 1.1237\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1381 (1.1381)\tPrec@1 59.570 (59.570)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1667 (1.1694)\tPrec@1 58.036 (58.170)\n",
            "EPOCH: 124 val Results: Prec@1 58.170 Loss: 1.1694\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [125][0/48]\tTime 0.046 (0.046)\tLoss 1.1308 (1.1308)\tPrec@1 58.984 (58.984)\n",
            "Epoch: [125][9/48]\tTime 0.042 (0.042)\tLoss 1.0658 (1.0984)\tPrec@1 62.109 (61.094)\n",
            "Epoch: [125][18/48]\tTime 0.040 (0.043)\tLoss 1.1390 (1.1078)\tPrec@1 60.156 (60.809)\n",
            "Epoch: [125][27/48]\tTime 0.043 (0.043)\tLoss 1.1016 (1.1140)\tPrec@1 63.086 (60.784)\n",
            "Epoch: [125][36/48]\tTime 0.040 (0.043)\tLoss 1.1362 (1.1169)\tPrec@1 58.594 (60.634)\n",
            "Epoch: [125][45/48]\tTime 0.040 (0.043)\tLoss 1.1372 (1.1229)\tPrec@1 60.449 (60.475)\n",
            "Epoch: [125][48/48]\tTime 0.035 (0.043)\tLoss 1.1176 (1.1232)\tPrec@1 59.788 (60.454)\n",
            "EPOCH: 125 train Results: Prec@1 60.454 Loss: 1.1232\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1427 (1.1427)\tPrec@1 58.691 (58.691)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1737 (1.1719)\tPrec@1 56.633 (57.900)\n",
            "EPOCH: 125 val Results: Prec@1 57.900 Loss: 1.1719\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [126][0/48]\tTime 0.042 (0.042)\tLoss 1.1075 (1.1075)\tPrec@1 61.133 (61.133)\n",
            "Epoch: [126][9/48]\tTime 0.040 (0.042)\tLoss 1.1112 (1.0938)\tPrec@1 61.426 (61.084)\n",
            "Epoch: [126][18/48]\tTime 0.041 (0.043)\tLoss 1.0590 (1.1027)\tPrec@1 63.770 (60.943)\n",
            "Epoch: [126][27/48]\tTime 0.043 (0.043)\tLoss 1.0941 (1.1079)\tPrec@1 60.742 (60.795)\n",
            "Epoch: [126][36/48]\tTime 0.054 (0.043)\tLoss 1.0787 (1.1143)\tPrec@1 60.742 (60.510)\n",
            "Epoch: [126][45/48]\tTime 0.041 (0.043)\tLoss 1.1805 (1.1224)\tPrec@1 58.008 (60.184)\n",
            "Epoch: [126][48/48]\tTime 0.033 (0.043)\tLoss 1.0798 (1.1232)\tPrec@1 61.203 (60.126)\n",
            "EPOCH: 126 train Results: Prec@1 60.126 Loss: 1.1232\n",
            "Test: [0/9]\tTime 0.012 (0.012)\tLoss 1.1361 (1.1361)\tPrec@1 59.277 (59.277)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1734 (1.1677)\tPrec@1 57.270 (58.400)\n",
            "EPOCH: 126 val Results: Prec@1 58.400 Loss: 1.1677\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [127][0/48]\tTime 0.044 (0.044)\tLoss 1.0457 (1.0457)\tPrec@1 63.672 (63.672)\n",
            "Epoch: [127][9/48]\tTime 0.043 (0.042)\tLoss 1.0931 (1.0978)\tPrec@1 61.719 (61.338)\n",
            "Epoch: [127][18/48]\tTime 0.046 (0.043)\tLoss 1.1223 (1.0991)\tPrec@1 59.961 (61.230)\n",
            "Epoch: [127][27/48]\tTime 0.040 (0.043)\tLoss 1.1355 (1.1031)\tPrec@1 58.105 (60.965)\n",
            "Epoch: [127][36/48]\tTime 0.042 (0.043)\tLoss 1.1673 (1.1168)\tPrec@1 57.031 (60.323)\n",
            "Epoch: [127][45/48]\tTime 0.043 (0.043)\tLoss 1.1406 (1.1169)\tPrec@1 59.277 (60.305)\n",
            "Epoch: [127][48/48]\tTime 0.033 (0.043)\tLoss 1.1337 (1.1194)\tPrec@1 59.670 (60.234)\n",
            "EPOCH: 127 train Results: Prec@1 60.234 Loss: 1.1194\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1505 (1.1505)\tPrec@1 59.277 (59.277)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1850 (1.1706)\tPrec@1 56.633 (58.200)\n",
            "EPOCH: 127 val Results: Prec@1 58.200 Loss: 1.1706\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [128][0/48]\tTime 0.043 (0.043)\tLoss 1.1322 (1.1322)\tPrec@1 59.766 (59.766)\n",
            "Epoch: [128][9/48]\tTime 0.040 (0.044)\tLoss 1.0980 (1.1017)\tPrec@1 62.207 (61.191)\n",
            "Epoch: [128][18/48]\tTime 0.041 (0.043)\tLoss 1.1768 (1.1074)\tPrec@1 59.180 (60.902)\n",
            "Epoch: [128][27/48]\tTime 0.047 (0.043)\tLoss 1.0963 (1.1123)\tPrec@1 61.914 (60.658)\n",
            "Epoch: [128][36/48]\tTime 0.041 (0.044)\tLoss 1.1598 (1.1182)\tPrec@1 59.277 (60.370)\n",
            "Epoch: [128][45/48]\tTime 0.040 (0.044)\tLoss 1.1304 (1.1216)\tPrec@1 58.691 (60.216)\n",
            "Epoch: [128][48/48]\tTime 0.035 (0.043)\tLoss 1.1527 (1.1231)\tPrec@1 62.854 (60.264)\n",
            "EPOCH: 128 train Results: Prec@1 60.264 Loss: 1.1231\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1278 (1.1278)\tPrec@1 59.082 (59.082)\n",
            "Test: [9/9]\tTime 0.009 (0.011)\tLoss 1.1749 (1.1685)\tPrec@1 57.015 (57.960)\n",
            "EPOCH: 128 val Results: Prec@1 57.960 Loss: 1.1685\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [129][0/48]\tTime 0.042 (0.042)\tLoss 1.0820 (1.0820)\tPrec@1 61.426 (61.426)\n",
            "Epoch: [129][9/48]\tTime 0.040 (0.046)\tLoss 1.1037 (1.0852)\tPrec@1 61.719 (61.699)\n",
            "Epoch: [129][18/48]\tTime 0.056 (0.051)\tLoss 1.1224 (1.0961)\tPrec@1 59.961 (61.369)\n",
            "Epoch: [129][27/48]\tTime 0.078 (0.060)\tLoss 1.1267 (1.1057)\tPrec@1 61.328 (61.101)\n",
            "Epoch: [129][36/48]\tTime 0.078 (0.066)\tLoss 1.1578 (1.1135)\tPrec@1 56.348 (60.576)\n",
            "Epoch: [129][45/48]\tTime 0.061 (0.070)\tLoss 1.1665 (1.1235)\tPrec@1 57.520 (60.192)\n",
            "Epoch: [129][48/48]\tTime 0.067 (0.070)\tLoss 1.1599 (1.1253)\tPrec@1 60.731 (60.142)\n",
            "EPOCH: 129 train Results: Prec@1 60.142 Loss: 1.1253\n",
            "Test: [0/9]\tTime 0.023 (0.023)\tLoss 1.1303 (1.1303)\tPrec@1 59.570 (59.570)\n",
            "Test: [9/9]\tTime 0.016 (0.020)\tLoss 1.1757 (1.1662)\tPrec@1 57.653 (58.510)\n",
            "EPOCH: 129 val Results: Prec@1 58.510 Loss: 1.1662\n",
            "Best Prec@1: 58.610\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [130][0/48]\tTime 0.082 (0.082)\tLoss 1.1378 (1.1378)\tPrec@1 61.426 (61.426)\n",
            "Epoch: [130][9/48]\tTime 0.039 (0.047)\tLoss 1.1069 (1.0912)\tPrec@1 61.328 (61.240)\n",
            "Epoch: [130][18/48]\tTime 0.046 (0.046)\tLoss 1.1445 (1.0916)\tPrec@1 61.035 (61.066)\n",
            "Epoch: [130][27/48]\tTime 0.044 (0.044)\tLoss 1.0986 (1.1034)\tPrec@1 60.645 (60.788)\n",
            "Epoch: [130][36/48]\tTime 0.044 (0.044)\tLoss 1.1187 (1.1123)\tPrec@1 59.961 (60.423)\n",
            "Epoch: [130][45/48]\tTime 0.041 (0.044)\tLoss 1.1323 (1.1172)\tPrec@1 58.594 (60.286)\n",
            "Epoch: [130][48/48]\tTime 0.034 (0.044)\tLoss 1.1610 (1.1201)\tPrec@1 58.019 (60.168)\n",
            "EPOCH: 130 train Results: Prec@1 60.168 Loss: 1.1201\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1471 (1.1471)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1765 (1.1663)\tPrec@1 56.760 (58.640)\n",
            "EPOCH: 130 val Results: Prec@1 58.640 Loss: 1.1663\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [131][0/48]\tTime 0.042 (0.042)\tLoss 1.0708 (1.0708)\tPrec@1 61.621 (61.621)\n",
            "Epoch: [131][9/48]\tTime 0.048 (0.045)\tLoss 1.0643 (1.0979)\tPrec@1 60.645 (61.084)\n",
            "Epoch: [131][18/48]\tTime 0.040 (0.045)\tLoss 1.1146 (1.1057)\tPrec@1 60.449 (60.768)\n",
            "Epoch: [131][27/48]\tTime 0.040 (0.044)\tLoss 1.1417 (1.1106)\tPrec@1 58.887 (60.379)\n",
            "Epoch: [131][36/48]\tTime 0.040 (0.044)\tLoss 1.0721 (1.1145)\tPrec@1 61.426 (60.389)\n",
            "Epoch: [131][45/48]\tTime 0.040 (0.044)\tLoss 1.1044 (1.1207)\tPrec@1 62.109 (60.267)\n",
            "Epoch: [131][48/48]\tTime 0.035 (0.044)\tLoss 1.1539 (1.1229)\tPrec@1 60.613 (60.198)\n",
            "EPOCH: 131 train Results: Prec@1 60.198 Loss: 1.1229\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1285 (1.1285)\tPrec@1 61.328 (61.328)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1759 (1.1668)\tPrec@1 57.526 (58.350)\n",
            "EPOCH: 131 val Results: Prec@1 58.350 Loss: 1.1668\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [132][0/48]\tTime 0.043 (0.043)\tLoss 1.1601 (1.1601)\tPrec@1 58.398 (58.398)\n",
            "Epoch: [132][9/48]\tTime 0.041 (0.044)\tLoss 1.1103 (1.0964)\tPrec@1 60.645 (61.406)\n",
            "Epoch: [132][18/48]\tTime 0.042 (0.044)\tLoss 1.1369 (1.1078)\tPrec@1 60.352 (60.814)\n",
            "Epoch: [132][27/48]\tTime 0.041 (0.043)\tLoss 1.1620 (1.1142)\tPrec@1 58.984 (60.477)\n",
            "Epoch: [132][36/48]\tTime 0.039 (0.043)\tLoss 1.1285 (1.1138)\tPrec@1 58.203 (60.457)\n",
            "Epoch: [132][45/48]\tTime 0.040 (0.043)\tLoss 1.1072 (1.1166)\tPrec@1 61.719 (60.339)\n",
            "Epoch: [132][48/48]\tTime 0.036 (0.043)\tLoss 1.0749 (1.1163)\tPrec@1 61.792 (60.330)\n",
            "EPOCH: 132 train Results: Prec@1 60.330 Loss: 1.1163\n",
            "Test: [0/9]\tTime 0.014 (0.014)\tLoss 1.1426 (1.1426)\tPrec@1 60.156 (60.156)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1656 (1.1647)\tPrec@1 57.015 (58.510)\n",
            "EPOCH: 132 val Results: Prec@1 58.510 Loss: 1.1647\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [133][0/48]\tTime 0.043 (0.043)\tLoss 1.0465 (1.0465)\tPrec@1 64.551 (64.551)\n",
            "Epoch: [133][9/48]\tTime 0.040 (0.044)\tLoss 1.1209 (1.0968)\tPrec@1 59.570 (61.562)\n",
            "Epoch: [133][18/48]\tTime 0.041 (0.043)\tLoss 1.0780 (1.0900)\tPrec@1 62.109 (61.518)\n",
            "Epoch: [133][27/48]\tTime 0.040 (0.043)\tLoss 1.0943 (1.0966)\tPrec@1 60.840 (61.283)\n",
            "Epoch: [133][36/48]\tTime 0.044 (0.043)\tLoss 1.1318 (1.1054)\tPrec@1 61.426 (61.101)\n",
            "Epoch: [133][45/48]\tTime 0.042 (0.043)\tLoss 1.1511 (1.1143)\tPrec@1 58.301 (60.793)\n",
            "Epoch: [133][48/48]\tTime 0.034 (0.043)\tLoss 1.0643 (1.1146)\tPrec@1 62.500 (60.824)\n",
            "EPOCH: 133 train Results: Prec@1 60.824 Loss: 1.1146\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1419 (1.1419)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1662 (1.1641)\tPrec@1 55.995 (58.060)\n",
            "EPOCH: 133 val Results: Prec@1 58.060 Loss: 1.1641\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [134][0/48]\tTime 0.044 (0.044)\tLoss 1.1403 (1.1403)\tPrec@1 59.863 (59.863)\n",
            "Epoch: [134][9/48]\tTime 0.043 (0.044)\tLoss 1.1208 (1.1026)\tPrec@1 60.449 (61.113)\n",
            "Epoch: [134][18/48]\tTime 0.040 (0.043)\tLoss 1.0998 (1.1054)\tPrec@1 60.254 (60.912)\n",
            "Epoch: [134][27/48]\tTime 0.084 (0.045)\tLoss 1.0947 (1.1046)\tPrec@1 60.547 (60.965)\n",
            "Epoch: [134][36/48]\tTime 0.082 (0.055)\tLoss 1.1521 (1.1076)\tPrec@1 58.398 (60.779)\n",
            "Epoch: [134][45/48]\tTime 0.081 (0.060)\tLoss 1.1161 (1.1126)\tPrec@1 60.059 (60.462)\n",
            "Epoch: [134][48/48]\tTime 0.074 (0.061)\tLoss 1.1561 (1.1138)\tPrec@1 59.080 (60.438)\n",
            "EPOCH: 134 train Results: Prec@1 60.438 Loss: 1.1138\n",
            "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1302 (1.1302)\tPrec@1 59.961 (59.961)\n",
            "Test: [9/9]\tTime 0.016 (0.020)\tLoss 1.1740 (1.1663)\tPrec@1 56.505 (58.230)\n",
            "EPOCH: 134 val Results: Prec@1 58.230 Loss: 1.1663\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [135][0/48]\tTime 0.087 (0.087)\tLoss 1.0845 (1.0845)\tPrec@1 61.914 (61.914)\n",
            "Epoch: [135][9/48]\tTime 0.058 (0.074)\tLoss 1.0768 (1.0860)\tPrec@1 62.988 (61.504)\n",
            "Epoch: [135][18/48]\tTime 0.043 (0.071)\tLoss 1.0894 (1.0953)\tPrec@1 62.207 (61.266)\n",
            "Epoch: [135][27/48]\tTime 0.041 (0.062)\tLoss 1.1095 (1.0974)\tPrec@1 61.426 (61.192)\n",
            "Epoch: [135][36/48]\tTime 0.059 (0.058)\tLoss 1.1127 (1.1064)\tPrec@1 61.523 (60.935)\n",
            "Epoch: [135][45/48]\tTime 0.041 (0.055)\tLoss 1.1901 (1.1128)\tPrec@1 57.910 (60.685)\n",
            "Epoch: [135][48/48]\tTime 0.034 (0.054)\tLoss 1.1622 (1.1135)\tPrec@1 58.373 (60.638)\n",
            "EPOCH: 135 train Results: Prec@1 60.638 Loss: 1.1135\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1529 (1.1529)\tPrec@1 58.691 (58.691)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1752 (1.1687)\tPrec@1 55.485 (58.170)\n",
            "EPOCH: 135 val Results: Prec@1 58.170 Loss: 1.1687\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [136][0/48]\tTime 0.044 (0.044)\tLoss 1.0857 (1.0857)\tPrec@1 61.328 (61.328)\n",
            "Epoch: [136][9/48]\tTime 0.044 (0.043)\tLoss 1.1005 (1.0933)\tPrec@1 60.059 (60.928)\n",
            "Epoch: [136][18/48]\tTime 0.041 (0.043)\tLoss 1.0953 (1.0949)\tPrec@1 62.012 (61.236)\n",
            "Epoch: [136][27/48]\tTime 0.040 (0.043)\tLoss 1.1468 (1.1088)\tPrec@1 57.910 (60.613)\n",
            "Epoch: [136][36/48]\tTime 0.040 (0.043)\tLoss 1.1257 (1.1117)\tPrec@1 60.156 (60.610)\n",
            "Epoch: [136][45/48]\tTime 0.041 (0.043)\tLoss 1.1584 (1.1148)\tPrec@1 60.059 (60.509)\n",
            "Epoch: [136][48/48]\tTime 0.033 (0.043)\tLoss 1.1577 (1.1172)\tPrec@1 59.906 (60.496)\n",
            "EPOCH: 136 train Results: Prec@1 60.496 Loss: 1.1172\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1588 (1.1588)\tPrec@1 59.375 (59.375)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1736 (1.1721)\tPrec@1 56.633 (57.970)\n",
            "EPOCH: 136 val Results: Prec@1 57.970 Loss: 1.1721\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [137][0/48]\tTime 0.045 (0.045)\tLoss 1.0587 (1.0587)\tPrec@1 62.207 (62.207)\n",
            "Epoch: [137][9/48]\tTime 0.040 (0.044)\tLoss 1.0708 (1.1046)\tPrec@1 62.402 (60.391)\n",
            "Epoch: [137][18/48]\tTime 0.041 (0.043)\tLoss 1.1214 (1.1015)\tPrec@1 60.156 (61.061)\n",
            "Epoch: [137][27/48]\tTime 0.041 (0.043)\tLoss 1.1127 (1.1036)\tPrec@1 60.938 (60.777)\n",
            "Epoch: [137][36/48]\tTime 0.046 (0.044)\tLoss 1.1574 (1.1086)\tPrec@1 59.277 (60.621)\n",
            "Epoch: [137][45/48]\tTime 0.040 (0.044)\tLoss 1.0969 (1.1140)\tPrec@1 60.938 (60.430)\n",
            "Epoch: [137][48/48]\tTime 0.037 (0.043)\tLoss 1.1578 (1.1156)\tPrec@1 57.901 (60.416)\n",
            "EPOCH: 137 train Results: Prec@1 60.416 Loss: 1.1156\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1443 (1.1443)\tPrec@1 59.863 (59.863)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1639 (1.1704)\tPrec@1 58.036 (58.190)\n",
            "EPOCH: 137 val Results: Prec@1 58.190 Loss: 1.1704\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [138][0/48]\tTime 0.046 (0.046)\tLoss 1.1017 (1.1017)\tPrec@1 62.402 (62.402)\n",
            "Epoch: [138][9/48]\tTime 0.042 (0.046)\tLoss 1.1061 (1.0922)\tPrec@1 60.254 (62.021)\n",
            "Epoch: [138][18/48]\tTime 0.041 (0.044)\tLoss 1.0734 (1.1005)\tPrec@1 64.258 (61.688)\n",
            "Epoch: [138][27/48]\tTime 0.040 (0.044)\tLoss 1.0711 (1.0988)\tPrec@1 60.547 (61.516)\n",
            "Epoch: [138][36/48]\tTime 0.040 (0.044)\tLoss 1.0942 (1.1043)\tPrec@1 62.207 (61.164)\n",
            "Epoch: [138][45/48]\tTime 0.042 (0.043)\tLoss 1.1564 (1.1073)\tPrec@1 58.887 (61.046)\n",
            "Epoch: [138][48/48]\tTime 0.039 (0.043)\tLoss 1.2124 (1.1107)\tPrec@1 55.896 (60.862)\n",
            "EPOCH: 138 train Results: Prec@1 60.862 Loss: 1.1107\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1394 (1.1394)\tPrec@1 60.449 (60.449)\n",
            "Test: [9/9]\tTime 0.007 (0.012)\tLoss 1.1667 (1.1673)\tPrec@1 56.633 (58.470)\n",
            "EPOCH: 138 val Results: Prec@1 58.470 Loss: 1.1673\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [139][0/48]\tTime 0.045 (0.045)\tLoss 1.0576 (1.0576)\tPrec@1 61.621 (61.621)\n",
            "Epoch: [139][9/48]\tTime 0.040 (0.042)\tLoss 1.0815 (1.0852)\tPrec@1 61.133 (60.918)\n",
            "Epoch: [139][18/48]\tTime 0.040 (0.043)\tLoss 1.1184 (1.0919)\tPrec@1 60.449 (60.922)\n",
            "Epoch: [139][27/48]\tTime 0.043 (0.043)\tLoss 1.0859 (1.1000)\tPrec@1 62.012 (60.658)\n",
            "Epoch: [139][36/48]\tTime 0.040 (0.043)\tLoss 1.1048 (1.1061)\tPrec@1 60.156 (60.520)\n",
            "Epoch: [139][45/48]\tTime 0.078 (0.048)\tLoss 1.1766 (1.1116)\tPrec@1 60.352 (60.286)\n",
            "Epoch: [139][48/48]\tTime 0.067 (0.049)\tLoss 1.1761 (1.1137)\tPrec@1 57.429 (60.202)\n",
            "EPOCH: 139 train Results: Prec@1 60.202 Loss: 1.1137\n",
            "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.1480 (1.1480)\tPrec@1 59.766 (59.766)\n",
            "Test: [9/9]\tTime 0.015 (0.018)\tLoss 1.1685 (1.1670)\tPrec@1 57.015 (58.390)\n",
            "EPOCH: 139 val Results: Prec@1 58.390 Loss: 1.1670\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [140][0/48]\tTime 0.086 (0.086)\tLoss 1.0493 (1.0493)\tPrec@1 65.039 (65.039)\n",
            "Epoch: [140][9/48]\tTime 0.083 (0.081)\tLoss 1.0536 (1.0829)\tPrec@1 66.309 (62.412)\n",
            "Epoch: [140][18/48]\tTime 0.073 (0.083)\tLoss 1.0778 (1.0923)\tPrec@1 60.547 (61.436)\n",
            "Epoch: [140][27/48]\tTime 0.040 (0.081)\tLoss 1.1066 (1.0956)\tPrec@1 60.547 (61.283)\n",
            "Epoch: [140][36/48]\tTime 0.040 (0.073)\tLoss 1.1380 (1.1024)\tPrec@1 61.230 (61.019)\n",
            "Epoch: [140][45/48]\tTime 0.043 (0.067)\tLoss 1.0986 (1.1072)\tPrec@1 60.254 (60.842)\n",
            "Epoch: [140][48/48]\tTime 0.035 (0.065)\tLoss 1.1567 (1.1112)\tPrec@1 56.958 (60.584)\n",
            "EPOCH: 140 train Results: Prec@1 60.584 Loss: 1.1112\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1469 (1.1469)\tPrec@1 59.570 (59.570)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1679 (1.1667)\tPrec@1 55.867 (58.370)\n",
            "EPOCH: 140 val Results: Prec@1 58.370 Loss: 1.1667\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [141][0/48]\tTime 0.044 (0.044)\tLoss 1.0351 (1.0351)\tPrec@1 63.379 (63.379)\n",
            "Epoch: [141][9/48]\tTime 0.044 (0.045)\tLoss 1.1080 (1.0675)\tPrec@1 61.914 (62.305)\n",
            "Epoch: [141][18/48]\tTime 0.041 (0.044)\tLoss 1.1074 (1.0865)\tPrec@1 60.059 (61.421)\n",
            "Epoch: [141][27/48]\tTime 0.043 (0.043)\tLoss 1.1328 (1.0923)\tPrec@1 61.133 (61.426)\n",
            "Epoch: [141][36/48]\tTime 0.040 (0.043)\tLoss 1.0916 (1.0996)\tPrec@1 61.816 (60.998)\n",
            "Epoch: [141][45/48]\tTime 0.040 (0.043)\tLoss 1.1529 (1.1063)\tPrec@1 58.691 (60.751)\n",
            "Epoch: [141][48/48]\tTime 0.033 (0.043)\tLoss 1.0991 (1.1065)\tPrec@1 60.613 (60.748)\n",
            "EPOCH: 141 train Results: Prec@1 60.748 Loss: 1.1065\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1353 (1.1353)\tPrec@1 60.645 (60.645)\n",
            "Test: [9/9]\tTime 0.009 (0.011)\tLoss 1.1597 (1.1673)\tPrec@1 57.908 (58.410)\n",
            "EPOCH: 141 val Results: Prec@1 58.410 Loss: 1.1673\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [142][0/48]\tTime 0.045 (0.045)\tLoss 1.0442 (1.0442)\tPrec@1 63.770 (63.770)\n",
            "Epoch: [142][9/48]\tTime 0.039 (0.043)\tLoss 1.1300 (1.0894)\tPrec@1 61.230 (61.982)\n",
            "Epoch: [142][18/48]\tTime 0.043 (0.043)\tLoss 1.0632 (1.0986)\tPrec@1 60.742 (61.354)\n",
            "Epoch: [142][27/48]\tTime 0.041 (0.043)\tLoss 1.1206 (1.1005)\tPrec@1 59.863 (61.196)\n",
            "Epoch: [142][36/48]\tTime 0.041 (0.043)\tLoss 1.0880 (1.1053)\tPrec@1 60.449 (60.969)\n",
            "Epoch: [142][45/48]\tTime 0.042 (0.043)\tLoss 1.1444 (1.1095)\tPrec@1 57.812 (60.674)\n",
            "Epoch: [142][48/48]\tTime 0.033 (0.043)\tLoss 1.1632 (1.1132)\tPrec@1 57.547 (60.494)\n",
            "EPOCH: 142 train Results: Prec@1 60.494 Loss: 1.1132\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1432 (1.1432)\tPrec@1 59.961 (59.961)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1673 (1.1691)\tPrec@1 57.398 (58.420)\n",
            "EPOCH: 142 val Results: Prec@1 58.420 Loss: 1.1691\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [143][0/48]\tTime 0.043 (0.043)\tLoss 1.0232 (1.0232)\tPrec@1 64.355 (64.355)\n",
            "Epoch: [143][9/48]\tTime 0.041 (0.044)\tLoss 1.1231 (1.0919)\tPrec@1 61.426 (61.963)\n",
            "Epoch: [143][18/48]\tTime 0.044 (0.044)\tLoss 1.0979 (1.0917)\tPrec@1 62.891 (61.708)\n",
            "Epoch: [143][27/48]\tTime 0.039 (0.044)\tLoss 1.0984 (1.0951)\tPrec@1 59.863 (61.391)\n",
            "Epoch: [143][36/48]\tTime 0.042 (0.043)\tLoss 1.1351 (1.1026)\tPrec@1 60.254 (61.088)\n",
            "Epoch: [143][45/48]\tTime 0.040 (0.043)\tLoss 1.1312 (1.1092)\tPrec@1 59.277 (60.768)\n",
            "Epoch: [143][48/48]\tTime 0.036 (0.043)\tLoss 1.1451 (1.1099)\tPrec@1 59.670 (60.748)\n",
            "EPOCH: 143 train Results: Prec@1 60.748 Loss: 1.1099\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1453 (1.1453)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.007 (0.013)\tLoss 1.1725 (1.1700)\tPrec@1 56.250 (58.300)\n",
            "EPOCH: 143 val Results: Prec@1 58.300 Loss: 1.1700\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [144][0/48]\tTime 0.042 (0.042)\tLoss 1.1172 (1.1172)\tPrec@1 62.305 (62.305)\n",
            "Epoch: [144][9/48]\tTime 0.041 (0.043)\tLoss 1.0936 (1.0976)\tPrec@1 62.109 (61.045)\n",
            "Epoch: [144][18/48]\tTime 0.041 (0.043)\tLoss 1.1063 (1.1005)\tPrec@1 59.570 (61.009)\n",
            "Epoch: [144][27/48]\tTime 0.041 (0.044)\tLoss 1.0981 (1.1074)\tPrec@1 60.645 (60.983)\n",
            "Epoch: [144][36/48]\tTime 0.049 (0.044)\tLoss 1.1494 (1.1067)\tPrec@1 60.840 (60.980)\n",
            "Epoch: [144][45/48]\tTime 0.059 (0.044)\tLoss 1.0583 (1.1089)\tPrec@1 62.988 (60.927)\n",
            "Epoch: [144][48/48]\tTime 0.034 (0.043)\tLoss 1.1140 (1.1101)\tPrec@1 59.670 (60.826)\n",
            "EPOCH: 144 train Results: Prec@1 60.826 Loss: 1.1101\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1511 (1.1511)\tPrec@1 59.863 (59.863)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1579 (1.1646)\tPrec@1 56.760 (58.520)\n",
            "EPOCH: 144 val Results: Prec@1 58.520 Loss: 1.1646\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [145][0/48]\tTime 0.090 (0.090)\tLoss 1.1241 (1.1241)\tPrec@1 60.352 (60.352)\n",
            "Epoch: [145][9/48]\tTime 0.053 (0.073)\tLoss 1.1263 (1.0998)\tPrec@1 58.984 (60.352)\n",
            "Epoch: [145][18/48]\tTime 0.107 (0.079)\tLoss 1.0303 (1.0913)\tPrec@1 63.184 (61.051)\n",
            "Epoch: [145][27/48]\tTime 0.084 (0.079)\tLoss 1.1434 (1.1041)\tPrec@1 58.984 (60.641)\n",
            "Epoch: [145][36/48]\tTime 0.089 (0.079)\tLoss 1.0788 (1.1080)\tPrec@1 62.500 (60.600)\n",
            "Epoch: [145][45/48]\tTime 0.040 (0.077)\tLoss 1.1301 (1.1115)\tPrec@1 60.156 (60.585)\n",
            "Epoch: [145][48/48]\tTime 0.038 (0.074)\tLoss 1.1801 (1.1144)\tPrec@1 57.429 (60.462)\n",
            "EPOCH: 145 train Results: Prec@1 60.462 Loss: 1.1144\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1564 (1.1564)\tPrec@1 58.594 (58.594)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1697 (1.1713)\tPrec@1 56.633 (58.090)\n",
            "EPOCH: 145 val Results: Prec@1 58.090 Loss: 1.1713\n",
            "Best Prec@1: 58.640\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [146][0/48]\tTime 0.043 (0.043)\tLoss 1.0560 (1.0560)\tPrec@1 62.695 (62.695)\n",
            "Epoch: [146][9/48]\tTime 0.045 (0.044)\tLoss 1.1008 (1.0820)\tPrec@1 61.719 (61.953)\n",
            "Epoch: [146][18/48]\tTime 0.042 (0.044)\tLoss 1.0759 (1.0894)\tPrec@1 62.500 (61.765)\n",
            "Epoch: [146][27/48]\tTime 0.041 (0.043)\tLoss 1.1060 (1.0912)\tPrec@1 60.059 (61.482)\n",
            "Epoch: [146][36/48]\tTime 0.044 (0.044)\tLoss 1.1569 (1.0992)\tPrec@1 57.520 (61.159)\n",
            "Epoch: [146][45/48]\tTime 0.045 (0.044)\tLoss 1.1554 (1.1048)\tPrec@1 58.691 (60.918)\n",
            "Epoch: [146][48/48]\tTime 0.034 (0.043)\tLoss 1.1578 (1.1055)\tPrec@1 58.844 (60.920)\n",
            "EPOCH: 146 train Results: Prec@1 60.920 Loss: 1.1055\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1453 (1.1453)\tPrec@1 59.277 (59.277)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1686 (1.1618)\tPrec@1 57.653 (58.750)\n",
            "EPOCH: 146 val Results: Prec@1 58.750 Loss: 1.1618\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [147][0/48]\tTime 0.063 (0.063)\tLoss 1.0880 (1.0880)\tPrec@1 61.328 (61.328)\n",
            "Epoch: [147][9/48]\tTime 0.046 (0.046)\tLoss 1.1197 (1.0866)\tPrec@1 59.863 (61.562)\n",
            "Epoch: [147][18/48]\tTime 0.041 (0.045)\tLoss 1.1523 (1.0935)\tPrec@1 59.375 (61.426)\n",
            "Epoch: [147][27/48]\tTime 0.041 (0.045)\tLoss 1.1779 (1.0959)\tPrec@1 58.203 (61.419)\n",
            "Epoch: [147][36/48]\tTime 0.041 (0.045)\tLoss 1.1184 (1.1032)\tPrec@1 61.328 (61.183)\n",
            "Epoch: [147][45/48]\tTime 0.041 (0.044)\tLoss 1.1708 (1.1061)\tPrec@1 60.449 (60.980)\n",
            "Epoch: [147][48/48]\tTime 0.034 (0.044)\tLoss 1.1826 (1.1096)\tPrec@1 58.373 (60.866)\n",
            "EPOCH: 147 train Results: Prec@1 60.866 Loss: 1.1096\n",
            "Test: [0/9]\tTime 0.013 (0.013)\tLoss 1.1469 (1.1469)\tPrec@1 58.398 (58.398)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1803 (1.1676)\tPrec@1 56.378 (58.130)\n",
            "EPOCH: 147 val Results: Prec@1 58.130 Loss: 1.1676\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [148][0/48]\tTime 0.043 (0.043)\tLoss 1.0512 (1.0512)\tPrec@1 61.426 (61.426)\n",
            "Epoch: [148][9/48]\tTime 0.041 (0.043)\tLoss 1.1445 (1.0848)\tPrec@1 59.277 (61.445)\n",
            "Epoch: [148][18/48]\tTime 0.048 (0.044)\tLoss 1.1417 (1.0880)\tPrec@1 62.207 (61.642)\n",
            "Epoch: [148][27/48]\tTime 0.041 (0.044)\tLoss 1.0992 (1.0963)\tPrec@1 59.180 (61.244)\n",
            "Epoch: [148][36/48]\tTime 0.040 (0.044)\tLoss 1.1666 (1.1017)\tPrec@1 58.594 (60.901)\n",
            "Epoch: [148][45/48]\tTime 0.041 (0.044)\tLoss 1.1272 (1.1072)\tPrec@1 58.887 (60.642)\n",
            "Epoch: [148][48/48]\tTime 0.034 (0.044)\tLoss 1.1024 (1.1065)\tPrec@1 62.146 (60.670)\n",
            "EPOCH: 148 train Results: Prec@1 60.670 Loss: 1.1065\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1524 (1.1524)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1748 (1.1635)\tPrec@1 57.270 (58.420)\n",
            "EPOCH: 148 val Results: Prec@1 58.420 Loss: 1.1635\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [149][0/48]\tTime 0.043 (0.043)\tLoss 1.0871 (1.0871)\tPrec@1 61.328 (61.328)\n",
            "Epoch: [149][9/48]\tTime 0.041 (0.043)\tLoss 1.0278 (1.0835)\tPrec@1 62.598 (61.836)\n",
            "Epoch: [149][18/48]\tTime 0.044 (0.045)\tLoss 1.0906 (1.0927)\tPrec@1 62.012 (61.565)\n",
            "Epoch: [149][27/48]\tTime 0.043 (0.045)\tLoss 1.0695 (1.0956)\tPrec@1 62.891 (61.401)\n",
            "Epoch: [149][36/48]\tTime 0.041 (0.044)\tLoss 1.0806 (1.0986)\tPrec@1 62.988 (61.236)\n",
            "Epoch: [149][45/48]\tTime 0.040 (0.044)\tLoss 1.1123 (1.1052)\tPrec@1 60.547 (60.914)\n",
            "Epoch: [149][48/48]\tTime 0.034 (0.044)\tLoss 1.1170 (1.1077)\tPrec@1 57.665 (60.788)\n",
            "EPOCH: 149 train Results: Prec@1 60.788 Loss: 1.1077\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1450 (1.1450)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1628 (1.1657)\tPrec@1 58.163 (58.630)\n",
            "EPOCH: 149 val Results: Prec@1 58.630 Loss: 1.1657\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [150][0/48]\tTime 0.042 (0.042)\tLoss 1.0668 (1.0668)\tPrec@1 62.012 (62.012)\n",
            "Epoch: [150][9/48]\tTime 0.046 (0.043)\tLoss 1.0719 (1.0727)\tPrec@1 62.891 (62.490)\n",
            "Epoch: [150][18/48]\tTime 0.082 (0.055)\tLoss 1.0490 (1.0781)\tPrec@1 63.086 (62.079)\n",
            "Epoch: [150][27/48]\tTime 0.082 (0.066)\tLoss 1.1214 (1.0917)\tPrec@1 60.156 (61.408)\n",
            "Epoch: [150][36/48]\tTime 0.093 (0.070)\tLoss 1.1321 (1.1010)\tPrec@1 60.254 (60.977)\n",
            "Epoch: [150][45/48]\tTime 0.065 (0.071)\tLoss 1.0726 (1.1025)\tPrec@1 61.426 (60.846)\n",
            "Epoch: [150][48/48]\tTime 0.051 (0.071)\tLoss 1.1199 (1.1059)\tPrec@1 60.495 (60.680)\n",
            "EPOCH: 150 train Results: Prec@1 60.680 Loss: 1.1059\n",
            "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1462 (1.1462)\tPrec@1 59.570 (59.570)\n",
            "Test: [9/9]\tTime 0.015 (0.020)\tLoss 1.1711 (1.1633)\tPrec@1 56.505 (58.580)\n",
            "EPOCH: 150 val Results: Prec@1 58.580 Loss: 1.1633\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [151][0/48]\tTime 0.089 (0.089)\tLoss 1.0805 (1.0805)\tPrec@1 61.230 (61.230)\n",
            "Epoch: [151][9/48]\tTime 0.041 (0.047)\tLoss 1.0611 (1.0759)\tPrec@1 63.965 (61.992)\n",
            "Epoch: [151][18/48]\tTime 0.041 (0.045)\tLoss 1.0580 (1.0767)\tPrec@1 62.988 (62.186)\n",
            "Epoch: [151][27/48]\tTime 0.042 (0.045)\tLoss 1.1351 (1.0841)\tPrec@1 61.816 (61.907)\n",
            "Epoch: [151][36/48]\tTime 0.040 (0.044)\tLoss 1.1378 (1.0950)\tPrec@1 59.082 (61.320)\n",
            "Epoch: [151][45/48]\tTime 0.044 (0.045)\tLoss 1.1299 (1.1008)\tPrec@1 59.863 (61.046)\n",
            "Epoch: [151][48/48]\tTime 0.040 (0.044)\tLoss 1.1369 (1.1029)\tPrec@1 61.910 (60.982)\n",
            "EPOCH: 151 train Results: Prec@1 60.982 Loss: 1.1029\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1518 (1.1518)\tPrec@1 57.910 (57.910)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1484 (1.1623)\tPrec@1 57.908 (58.700)\n",
            "EPOCH: 151 val Results: Prec@1 58.700 Loss: 1.1623\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [152][0/48]\tTime 0.044 (0.044)\tLoss 1.0828 (1.0828)\tPrec@1 62.402 (62.402)\n",
            "Epoch: [152][9/48]\tTime 0.042 (0.043)\tLoss 1.0748 (1.0778)\tPrec@1 61.816 (62.344)\n",
            "Epoch: [152][18/48]\tTime 0.060 (0.044)\tLoss 1.0664 (1.0826)\tPrec@1 62.793 (62.043)\n",
            "Epoch: [152][27/48]\tTime 0.041 (0.044)\tLoss 1.1291 (1.0854)\tPrec@1 58.398 (61.952)\n",
            "Epoch: [152][36/48]\tTime 0.040 (0.044)\tLoss 1.0964 (1.0926)\tPrec@1 61.133 (61.574)\n",
            "Epoch: [152][45/48]\tTime 0.040 (0.044)\tLoss 1.1076 (1.0990)\tPrec@1 59.766 (61.303)\n",
            "Epoch: [152][48/48]\tTime 0.033 (0.044)\tLoss 1.0863 (1.0999)\tPrec@1 60.967 (61.236)\n",
            "EPOCH: 152 train Results: Prec@1 61.236 Loss: 1.0999\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1480 (1.1480)\tPrec@1 58.984 (58.984)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1634 (1.1645)\tPrec@1 56.250 (58.260)\n",
            "EPOCH: 152 val Results: Prec@1 58.260 Loss: 1.1645\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [153][0/48]\tTime 0.045 (0.045)\tLoss 1.0048 (1.0048)\tPrec@1 63.184 (63.184)\n",
            "Epoch: [153][9/48]\tTime 0.043 (0.043)\tLoss 1.0750 (1.0641)\tPrec@1 62.012 (62.256)\n",
            "Epoch: [153][18/48]\tTime 0.040 (0.044)\tLoss 1.1009 (1.0766)\tPrec@1 60.645 (62.027)\n",
            "Epoch: [153][27/48]\tTime 0.040 (0.043)\tLoss 1.0764 (1.0868)\tPrec@1 62.012 (61.572)\n",
            "Epoch: [153][36/48]\tTime 0.051 (0.043)\tLoss 1.0859 (1.0920)\tPrec@1 60.352 (61.397)\n",
            "Epoch: [153][45/48]\tTime 0.040 (0.043)\tLoss 1.1626 (1.0971)\tPrec@1 58.594 (61.137)\n",
            "Epoch: [153][48/48]\tTime 0.034 (0.043)\tLoss 1.1344 (1.1011)\tPrec@1 58.608 (61.010)\n",
            "EPOCH: 153 train Results: Prec@1 61.010 Loss: 1.1011\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1428 (1.1428)\tPrec@1 60.156 (60.156)\n",
            "Test: [9/9]\tTime 0.011 (0.011)\tLoss 1.1668 (1.1636)\tPrec@1 57.398 (58.660)\n",
            "EPOCH: 153 val Results: Prec@1 58.660 Loss: 1.1636\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [154][0/48]\tTime 0.048 (0.048)\tLoss 1.0593 (1.0593)\tPrec@1 62.012 (62.012)\n",
            "Epoch: [154][9/48]\tTime 0.040 (0.044)\tLoss 1.0432 (1.0784)\tPrec@1 62.793 (61.748)\n",
            "Epoch: [154][18/48]\tTime 0.041 (0.044)\tLoss 1.1488 (1.0852)\tPrec@1 58.691 (61.446)\n",
            "Epoch: [154][27/48]\tTime 0.043 (0.044)\tLoss 1.1121 (1.0856)\tPrec@1 59.082 (61.471)\n",
            "Epoch: [154][36/48]\tTime 0.040 (0.044)\tLoss 1.1503 (1.0943)\tPrec@1 59.375 (61.246)\n",
            "Epoch: [154][45/48]\tTime 0.042 (0.044)\tLoss 1.1265 (1.1003)\tPrec@1 59.082 (60.918)\n",
            "Epoch: [154][48/48]\tTime 0.035 (0.044)\tLoss 1.1594 (1.1008)\tPrec@1 58.491 (60.896)\n",
            "EPOCH: 154 train Results: Prec@1 60.896 Loss: 1.1008\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1439 (1.1439)\tPrec@1 58.398 (58.398)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1583 (1.1653)\tPrec@1 57.398 (58.540)\n",
            "EPOCH: 154 val Results: Prec@1 58.540 Loss: 1.1653\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [155][0/48]\tTime 0.043 (0.043)\tLoss 1.0173 (1.0173)\tPrec@1 64.160 (64.160)\n",
            "Epoch: [155][9/48]\tTime 0.041 (0.045)\tLoss 1.0986 (1.0561)\tPrec@1 59.277 (62.744)\n",
            "Epoch: [155][18/48]\tTime 0.040 (0.043)\tLoss 1.0678 (1.0711)\tPrec@1 62.891 (62.037)\n",
            "Epoch: [155][27/48]\tTime 0.077 (0.047)\tLoss 1.0951 (1.0826)\tPrec@1 61.719 (61.659)\n",
            "Epoch: [155][36/48]\tTime 0.079 (0.056)\tLoss 1.1094 (1.0892)\tPrec@1 60.156 (61.399)\n",
            "Epoch: [155][45/48]\tTime 0.084 (0.059)\tLoss 1.0753 (1.0966)\tPrec@1 60.449 (61.135)\n",
            "Epoch: [155][48/48]\tTime 0.068 (0.060)\tLoss 1.1511 (1.0987)\tPrec@1 59.434 (61.036)\n",
            "EPOCH: 155 train Results: Prec@1 61.036 Loss: 1.0987\n",
            "Test: [0/9]\tTime 0.019 (0.019)\tLoss 1.1541 (1.1541)\tPrec@1 58.203 (58.203)\n",
            "Test: [9/9]\tTime 0.011 (0.019)\tLoss 1.1698 (1.1660)\tPrec@1 57.143 (58.280)\n",
            "EPOCH: 155 val Results: Prec@1 58.280 Loss: 1.1660\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [156][0/48]\tTime 0.074 (0.074)\tLoss 1.0639 (1.0639)\tPrec@1 62.207 (62.207)\n",
            "Epoch: [156][9/48]\tTime 0.066 (0.075)\tLoss 1.0038 (1.0621)\tPrec@1 65.918 (62.588)\n",
            "Epoch: [156][18/48]\tTime 0.044 (0.070)\tLoss 1.1477 (1.0729)\tPrec@1 59.668 (62.202)\n",
            "Epoch: [156][27/48]\tTime 0.044 (0.062)\tLoss 1.0339 (1.0767)\tPrec@1 63.574 (62.074)\n",
            "Epoch: [156][36/48]\tTime 0.039 (0.057)\tLoss 1.0755 (1.0884)\tPrec@1 62.500 (61.632)\n",
            "Epoch: [156][45/48]\tTime 0.040 (0.054)\tLoss 1.1119 (1.0939)\tPrec@1 60.352 (61.526)\n",
            "Epoch: [156][48/48]\tTime 0.033 (0.053)\tLoss 1.1418 (1.0948)\tPrec@1 59.906 (61.518)\n",
            "EPOCH: 156 train Results: Prec@1 61.518 Loss: 1.0948\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1590 (1.1590)\tPrec@1 58.008 (58.008)\n",
            "Test: [9/9]\tTime 0.012 (0.011)\tLoss 1.1686 (1.1670)\tPrec@1 55.740 (58.440)\n",
            "EPOCH: 156 val Results: Prec@1 58.440 Loss: 1.1670\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [157][0/48]\tTime 0.044 (0.044)\tLoss 1.0963 (1.0963)\tPrec@1 59.863 (59.863)\n",
            "Epoch: [157][9/48]\tTime 0.039 (0.041)\tLoss 1.0857 (1.0763)\tPrec@1 59.766 (61.367)\n",
            "Epoch: [157][18/48]\tTime 0.040 (0.043)\tLoss 1.1022 (1.0812)\tPrec@1 60.742 (61.601)\n",
            "Epoch: [157][27/48]\tTime 0.040 (0.043)\tLoss 1.1793 (1.0909)\tPrec@1 58.398 (61.060)\n",
            "Epoch: [157][36/48]\tTime 0.039 (0.043)\tLoss 1.1209 (1.0951)\tPrec@1 61.133 (60.916)\n",
            "Epoch: [157][45/48]\tTime 0.041 (0.043)\tLoss 1.1392 (1.0985)\tPrec@1 59.180 (60.814)\n",
            "Epoch: [157][48/48]\tTime 0.036 (0.043)\tLoss 1.0648 (1.0999)\tPrec@1 60.613 (60.726)\n",
            "EPOCH: 157 train Results: Prec@1 60.726 Loss: 1.0999\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1501 (1.1501)\tPrec@1 58.008 (58.008)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1632 (1.1683)\tPrec@1 57.270 (58.280)\n",
            "EPOCH: 157 val Results: Prec@1 58.280 Loss: 1.1683\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [158][0/48]\tTime 0.042 (0.042)\tLoss 1.0677 (1.0677)\tPrec@1 63.379 (63.379)\n",
            "Epoch: [158][9/48]\tTime 0.041 (0.042)\tLoss 1.1229 (1.0786)\tPrec@1 61.035 (61.846)\n",
            "Epoch: [158][18/48]\tTime 0.042 (0.043)\tLoss 1.1172 (1.0792)\tPrec@1 60.156 (61.724)\n",
            "Epoch: [158][27/48]\tTime 0.040 (0.043)\tLoss 1.0912 (1.0856)\tPrec@1 61.914 (61.433)\n",
            "Epoch: [158][36/48]\tTime 0.056 (0.043)\tLoss 1.1423 (1.0926)\tPrec@1 60.742 (61.273)\n",
            "Epoch: [158][45/48]\tTime 0.042 (0.043)\tLoss 1.1123 (1.0970)\tPrec@1 60.449 (61.207)\n",
            "Epoch: [158][48/48]\tTime 0.039 (0.043)\tLoss 1.1846 (1.0983)\tPrec@1 57.547 (61.168)\n",
            "EPOCH: 158 train Results: Prec@1 61.168 Loss: 1.0983\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1399 (1.1399)\tPrec@1 59.375 (59.375)\n",
            "Test: [9/9]\tTime 0.009 (0.010)\tLoss 1.1517 (1.1615)\tPrec@1 56.633 (58.560)\n",
            "EPOCH: 158 val Results: Prec@1 58.560 Loss: 1.1615\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [159][0/48]\tTime 0.042 (0.042)\tLoss 1.0542 (1.0542)\tPrec@1 62.207 (62.207)\n",
            "Epoch: [159][9/48]\tTime 0.064 (0.045)\tLoss 1.0712 (1.0673)\tPrec@1 62.793 (62.354)\n",
            "Epoch: [159][18/48]\tTime 0.040 (0.044)\tLoss 1.0480 (1.0770)\tPrec@1 62.598 (62.017)\n",
            "Epoch: [159][27/48]\tTime 0.040 (0.043)\tLoss 1.1371 (1.0831)\tPrec@1 58.496 (61.705)\n",
            "Epoch: [159][36/48]\tTime 0.044 (0.044)\tLoss 1.1304 (1.0881)\tPrec@1 59.961 (61.576)\n",
            "Epoch: [159][45/48]\tTime 0.040 (0.043)\tLoss 1.0861 (1.0950)\tPrec@1 60.156 (61.277)\n",
            "Epoch: [159][48/48]\tTime 0.040 (0.043)\tLoss 1.1037 (1.0963)\tPrec@1 61.203 (61.236)\n",
            "EPOCH: 159 train Results: Prec@1 61.236 Loss: 1.0963\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1437 (1.1437)\tPrec@1 58.301 (58.301)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1629 (1.1636)\tPrec@1 58.418 (58.680)\n",
            "EPOCH: 159 val Results: Prec@1 58.680 Loss: 1.1636\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [160][0/48]\tTime 0.044 (0.044)\tLoss 1.0671 (1.0671)\tPrec@1 63.574 (63.574)\n",
            "Epoch: [160][9/48]\tTime 0.040 (0.045)\tLoss 1.0872 (1.0868)\tPrec@1 60.742 (61.807)\n",
            "Epoch: [160][18/48]\tTime 0.040 (0.044)\tLoss 1.0893 (1.0844)\tPrec@1 62.207 (61.580)\n",
            "Epoch: [160][27/48]\tTime 0.041 (0.044)\tLoss 1.0701 (1.0901)\tPrec@1 61.914 (61.370)\n",
            "Epoch: [160][36/48]\tTime 0.040 (0.044)\tLoss 1.1267 (1.0925)\tPrec@1 60.059 (61.386)\n",
            "Epoch: [160][45/48]\tTime 0.069 (0.047)\tLoss 1.0322 (1.0929)\tPrec@1 63.770 (61.426)\n",
            "Epoch: [160][48/48]\tTime 0.067 (0.049)\tLoss 1.0990 (1.0923)\tPrec@1 59.788 (61.372)\n",
            "EPOCH: 160 train Results: Prec@1 61.372 Loss: 1.0923\n",
            "Test: [0/9]\tTime 0.024 (0.024)\tLoss 1.1410 (1.1410)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.024 (0.020)\tLoss 1.1612 (1.1677)\tPrec@1 57.270 (58.110)\n",
            "EPOCH: 160 val Results: Prec@1 58.110 Loss: 1.1677\n",
            "Best Prec@1: 58.750\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [161][0/48]\tTime 0.107 (0.107)\tLoss 1.0381 (1.0381)\tPrec@1 63.086 (63.086)\n",
            "Epoch: [161][9/48]\tTime 0.084 (0.080)\tLoss 1.0213 (1.0525)\tPrec@1 65.820 (62.803)\n",
            "Epoch: [161][18/48]\tTime 0.060 (0.080)\tLoss 1.1079 (1.0697)\tPrec@1 61.328 (62.058)\n",
            "Epoch: [161][27/48]\tTime 0.060 (0.079)\tLoss 1.1471 (1.0810)\tPrec@1 59.473 (61.820)\n",
            "Epoch: [161][36/48]\tTime 0.045 (0.073)\tLoss 1.1612 (1.0886)\tPrec@1 58.105 (61.579)\n",
            "Epoch: [161][45/48]\tTime 0.041 (0.068)\tLoss 1.1711 (1.0958)\tPrec@1 58.203 (61.313)\n",
            "Epoch: [161][48/48]\tTime 0.036 (0.066)\tLoss 1.1258 (1.0968)\tPrec@1 58.608 (61.232)\n",
            "EPOCH: 161 train Results: Prec@1 61.232 Loss: 1.0968\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1352 (1.1352)\tPrec@1 60.059 (60.059)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1658 (1.1608)\tPrec@1 57.015 (59.040)\n",
            "EPOCH: 161 val Results: Prec@1 59.040 Loss: 1.1608\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [162][0/48]\tTime 0.044 (0.044)\tLoss 1.0627 (1.0627)\tPrec@1 62.207 (62.207)\n",
            "Epoch: [162][9/48]\tTime 0.043 (0.043)\tLoss 1.1252 (1.0598)\tPrec@1 59.375 (62.178)\n",
            "Epoch: [162][18/48]\tTime 0.040 (0.043)\tLoss 1.1072 (1.0794)\tPrec@1 60.938 (62.079)\n",
            "Epoch: [162][27/48]\tTime 0.041 (0.043)\tLoss 1.0583 (1.0847)\tPrec@1 62.695 (61.830)\n",
            "Epoch: [162][36/48]\tTime 0.062 (0.044)\tLoss 1.0596 (1.0872)\tPrec@1 61.523 (61.690)\n",
            "Epoch: [162][45/48]\tTime 0.041 (0.044)\tLoss 1.1134 (1.0927)\tPrec@1 61.816 (61.498)\n",
            "Epoch: [162][48/48]\tTime 0.034 (0.044)\tLoss 1.1251 (1.0937)\tPrec@1 61.085 (61.450)\n",
            "EPOCH: 162 train Results: Prec@1 61.450 Loss: 1.0937\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1219 (1.1219)\tPrec@1 59.473 (59.473)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1621 (1.1625)\tPrec@1 55.612 (58.440)\n",
            "EPOCH: 162 val Results: Prec@1 58.440 Loss: 1.1625\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [163][0/48]\tTime 0.042 (0.042)\tLoss 1.0686 (1.0686)\tPrec@1 60.156 (60.156)\n",
            "Epoch: [163][9/48]\tTime 0.042 (0.044)\tLoss 1.1030 (1.0632)\tPrec@1 59.863 (61.777)\n",
            "Epoch: [163][18/48]\tTime 0.043 (0.044)\tLoss 1.0466 (1.0642)\tPrec@1 61.328 (62.212)\n",
            "Epoch: [163][27/48]\tTime 0.039 (0.043)\tLoss 1.1026 (1.0743)\tPrec@1 60.742 (61.879)\n",
            "Epoch: [163][36/48]\tTime 0.040 (0.044)\tLoss 1.1269 (1.0865)\tPrec@1 59.473 (61.521)\n",
            "Epoch: [163][45/48]\tTime 0.039 (0.043)\tLoss 1.0865 (1.0944)\tPrec@1 62.012 (61.292)\n",
            "Epoch: [163][48/48]\tTime 0.038 (0.043)\tLoss 1.1030 (1.0961)\tPrec@1 59.434 (61.208)\n",
            "EPOCH: 163 train Results: Prec@1 61.208 Loss: 1.0961\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1475 (1.1475)\tPrec@1 58.691 (58.691)\n",
            "Test: [9/9]\tTime 0.012 (0.010)\tLoss 1.1581 (1.1646)\tPrec@1 57.015 (58.310)\n",
            "EPOCH: 163 val Results: Prec@1 58.310 Loss: 1.1646\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [164][0/48]\tTime 0.044 (0.044)\tLoss 1.0922 (1.0922)\tPrec@1 61.133 (61.133)\n",
            "Epoch: [164][9/48]\tTime 0.042 (0.044)\tLoss 1.0443 (1.0509)\tPrec@1 62.305 (62.959)\n",
            "Epoch: [164][18/48]\tTime 0.040 (0.043)\tLoss 1.0839 (1.0697)\tPrec@1 61.328 (62.253)\n",
            "Epoch: [164][27/48]\tTime 0.040 (0.043)\tLoss 1.1421 (1.0785)\tPrec@1 59.277 (61.921)\n",
            "Epoch: [164][36/48]\tTime 0.040 (0.043)\tLoss 1.1138 (1.0851)\tPrec@1 59.375 (61.645)\n",
            "Epoch: [164][45/48]\tTime 0.040 (0.043)\tLoss 1.1107 (1.0906)\tPrec@1 60.059 (61.330)\n",
            "Epoch: [164][48/48]\tTime 0.036 (0.043)\tLoss 1.0848 (1.0898)\tPrec@1 60.259 (61.396)\n",
            "EPOCH: 164 train Results: Prec@1 61.396 Loss: 1.0898\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1402 (1.1402)\tPrec@1 58.398 (58.398)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1696 (1.1665)\tPrec@1 57.015 (58.310)\n",
            "EPOCH: 164 val Results: Prec@1 58.310 Loss: 1.1665\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [165][0/48]\tTime 0.042 (0.042)\tLoss 1.0684 (1.0684)\tPrec@1 62.695 (62.695)\n",
            "Epoch: [165][9/48]\tTime 0.041 (0.044)\tLoss 1.0324 (1.0513)\tPrec@1 63.867 (63.154)\n",
            "Epoch: [165][18/48]\tTime 0.039 (0.043)\tLoss 1.1451 (1.0753)\tPrec@1 58.008 (62.058)\n",
            "Epoch: [165][27/48]\tTime 0.043 (0.044)\tLoss 1.0625 (1.0770)\tPrec@1 61.133 (61.991)\n",
            "Epoch: [165][36/48]\tTime 0.043 (0.043)\tLoss 1.0685 (1.0894)\tPrec@1 61.035 (61.381)\n",
            "Epoch: [165][45/48]\tTime 0.041 (0.043)\tLoss 1.1033 (1.0939)\tPrec@1 62.109 (61.247)\n",
            "Epoch: [165][48/48]\tTime 0.034 (0.043)\tLoss 1.1572 (1.0979)\tPrec@1 58.608 (61.112)\n",
            "EPOCH: 165 train Results: Prec@1 61.112 Loss: 1.0979\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1497 (1.1497)\tPrec@1 58.398 (58.398)\n",
            "Test: [9/9]\tTime 0.016 (0.013)\tLoss 1.1697 (1.1719)\tPrec@1 57.270 (57.900)\n",
            "EPOCH: 165 val Results: Prec@1 57.900 Loss: 1.1719\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [166][0/48]\tTime 0.042 (0.042)\tLoss 1.1019 (1.1019)\tPrec@1 61.523 (61.523)\n",
            "Epoch: [166][9/48]\tTime 0.083 (0.061)\tLoss 1.0473 (1.0690)\tPrec@1 62.988 (62.607)\n",
            "Epoch: [166][18/48]\tTime 0.080 (0.068)\tLoss 1.1244 (1.0720)\tPrec@1 59.961 (62.217)\n",
            "Epoch: [166][27/48]\tTime 0.079 (0.072)\tLoss 1.0927 (1.0797)\tPrec@1 61.035 (61.799)\n",
            "Epoch: [166][36/48]\tTime 0.070 (0.074)\tLoss 1.1040 (1.0841)\tPrec@1 59.180 (61.455)\n",
            "Epoch: [166][45/48]\tTime 0.065 (0.076)\tLoss 1.1425 (1.0900)\tPrec@1 60.742 (61.351)\n",
            "Epoch: [166][48/48]\tTime 0.033 (0.074)\tLoss 1.1766 (1.0925)\tPrec@1 57.783 (61.254)\n",
            "EPOCH: 166 train Results: Prec@1 61.254 Loss: 1.0925\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1365 (1.1365)\tPrec@1 59.961 (59.961)\n",
            "Test: [9/9]\tTime 0.010 (0.011)\tLoss 1.1644 (1.1614)\tPrec@1 57.781 (58.670)\n",
            "EPOCH: 166 val Results: Prec@1 58.670 Loss: 1.1614\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [167][0/48]\tTime 0.041 (0.041)\tLoss 1.0833 (1.0833)\tPrec@1 62.207 (62.207)\n",
            "Epoch: [167][9/48]\tTime 0.040 (0.041)\tLoss 1.1351 (1.0726)\tPrec@1 60.547 (62.217)\n",
            "Epoch: [167][18/48]\tTime 0.040 (0.043)\tLoss 1.0794 (1.0749)\tPrec@1 61.328 (61.986)\n",
            "Epoch: [167][27/48]\tTime 0.046 (0.043)\tLoss 1.0909 (1.0855)\tPrec@1 60.840 (61.593)\n",
            "Epoch: [167][36/48]\tTime 0.047 (0.044)\tLoss 1.0863 (1.0884)\tPrec@1 61.523 (61.539)\n",
            "Epoch: [167][45/48]\tTime 0.040 (0.044)\tLoss 1.0252 (1.0926)\tPrec@1 63.770 (61.434)\n",
            "Epoch: [167][48/48]\tTime 0.033 (0.043)\tLoss 1.1202 (1.0942)\tPrec@1 59.906 (61.330)\n",
            "EPOCH: 167 train Results: Prec@1 61.330 Loss: 1.0942\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1353 (1.1353)\tPrec@1 59.570 (59.570)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1716 (1.1659)\tPrec@1 57.398 (58.480)\n",
            "EPOCH: 167 val Results: Prec@1 58.480 Loss: 1.1659\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [168][0/48]\tTime 0.042 (0.042)\tLoss 1.0964 (1.0964)\tPrec@1 60.840 (60.840)\n",
            "Epoch: [168][9/48]\tTime 0.040 (0.043)\tLoss 1.0373 (1.0651)\tPrec@1 63.281 (62.920)\n",
            "Epoch: [168][18/48]\tTime 0.040 (0.043)\tLoss 1.1265 (1.0696)\tPrec@1 61.523 (62.464)\n",
            "Epoch: [168][27/48]\tTime 0.040 (0.043)\tLoss 1.1331 (1.0708)\tPrec@1 60.254 (62.336)\n",
            "Epoch: [168][36/48]\tTime 0.040 (0.043)\tLoss 1.1373 (1.0800)\tPrec@1 59.668 (62.046)\n",
            "Epoch: [168][45/48]\tTime 0.040 (0.043)\tLoss 1.1034 (1.0878)\tPrec@1 61.621 (61.795)\n",
            "Epoch: [168][48/48]\tTime 0.038 (0.043)\tLoss 1.0878 (1.0888)\tPrec@1 61.910 (61.712)\n",
            "EPOCH: 168 train Results: Prec@1 61.712 Loss: 1.0888\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1380 (1.1380)\tPrec@1 60.547 (60.547)\n",
            "Test: [9/9]\tTime 0.008 (0.010)\tLoss 1.1668 (1.1663)\tPrec@1 58.418 (58.830)\n",
            "EPOCH: 168 val Results: Prec@1 58.830 Loss: 1.1663\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [169][0/48]\tTime 0.044 (0.044)\tLoss 1.0346 (1.0346)\tPrec@1 62.598 (62.598)\n",
            "Epoch: [169][9/48]\tTime 0.043 (0.044)\tLoss 1.0518 (1.0596)\tPrec@1 61.328 (62.305)\n",
            "Epoch: [169][18/48]\tTime 0.039 (0.043)\tLoss 1.0679 (1.0703)\tPrec@1 61.816 (62.166)\n",
            "Epoch: [169][27/48]\tTime 0.059 (0.043)\tLoss 1.1078 (1.0765)\tPrec@1 58.008 (62.001)\n",
            "Epoch: [169][36/48]\tTime 0.042 (0.043)\tLoss 1.1150 (1.0845)\tPrec@1 59.668 (61.727)\n",
            "Epoch: [169][45/48]\tTime 0.045 (0.043)\tLoss 1.0868 (1.0897)\tPrec@1 64.453 (61.492)\n",
            "Epoch: [169][48/48]\tTime 0.033 (0.043)\tLoss 1.1512 (1.0916)\tPrec@1 58.373 (61.400)\n",
            "EPOCH: 169 train Results: Prec@1 61.400 Loss: 1.0916\n",
            "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1313 (1.1313)\tPrec@1 60.547 (60.547)\n",
            "Test: [9/9]\tTime 0.011 (0.012)\tLoss 1.1693 (1.1661)\tPrec@1 58.291 (58.620)\n",
            "EPOCH: 169 val Results: Prec@1 58.620 Loss: 1.1661\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [170][0/48]\tTime 0.054 (0.054)\tLoss 1.0995 (1.0995)\tPrec@1 62.109 (62.109)\n",
            "Epoch: [170][9/48]\tTime 0.039 (0.043)\tLoss 1.0619 (1.0663)\tPrec@1 63.770 (62.510)\n",
            "Epoch: [170][18/48]\tTime 0.039 (0.042)\tLoss 1.0271 (1.0700)\tPrec@1 62.695 (62.402)\n",
            "Epoch: [170][27/48]\tTime 0.042 (0.042)\tLoss 1.0601 (1.0761)\tPrec@1 62.793 (62.081)\n",
            "Epoch: [170][36/48]\tTime 0.040 (0.043)\tLoss 1.0807 (1.0817)\tPrec@1 62.109 (61.954)\n",
            "Epoch: [170][45/48]\tTime 0.040 (0.042)\tLoss 1.0957 (1.0870)\tPrec@1 62.598 (61.797)\n",
            "Epoch: [170][48/48]\tTime 0.059 (0.043)\tLoss 1.0954 (1.0873)\tPrec@1 62.736 (61.768)\n",
            "EPOCH: 170 train Results: Prec@1 61.768 Loss: 1.0873\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1374 (1.1374)\tPrec@1 59.180 (59.180)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1719 (1.1671)\tPrec@1 58.673 (58.100)\n",
            "EPOCH: 170 val Results: Prec@1 58.100 Loss: 1.1671\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [171][0/48]\tTime 0.043 (0.043)\tLoss 1.0849 (1.0849)\tPrec@1 61.133 (61.133)\n",
            "Epoch: [171][9/48]\tTime 0.043 (0.044)\tLoss 1.1160 (1.0622)\tPrec@1 60.352 (62.588)\n",
            "Epoch: [171][18/48]\tTime 0.041 (0.043)\tLoss 1.0602 (1.0700)\tPrec@1 62.500 (62.336)\n",
            "Epoch: [171][27/48]\tTime 0.084 (0.054)\tLoss 1.0711 (1.0765)\tPrec@1 63.477 (62.078)\n",
            "Epoch: [171][36/48]\tTime 0.095 (0.062)\tLoss 1.1304 (1.0805)\tPrec@1 59.570 (61.822)\n",
            "Epoch: [171][45/48]\tTime 0.076 (0.066)\tLoss 1.1432 (1.0894)\tPrec@1 59.375 (61.470)\n",
            "Epoch: [171][48/48]\tTime 0.074 (0.067)\tLoss 1.1184 (1.0909)\tPrec@1 59.906 (61.404)\n",
            "EPOCH: 171 train Results: Prec@1 61.404 Loss: 1.0909\n",
            "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1536 (1.1536)\tPrec@1 59.473 (59.473)\n",
            "Test: [9/9]\tTime 0.013 (0.018)\tLoss 1.1640 (1.1651)\tPrec@1 57.781 (58.270)\n",
            "EPOCH: 171 val Results: Prec@1 58.270 Loss: 1.1651\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [172][0/48]\tTime 0.068 (0.068)\tLoss 1.0320 (1.0320)\tPrec@1 64.746 (64.746)\n",
            "Epoch: [172][9/48]\tTime 0.041 (0.070)\tLoss 1.0752 (1.0606)\tPrec@1 61.035 (62.627)\n",
            "Epoch: [172][18/48]\tTime 0.040 (0.056)\tLoss 1.0499 (1.0656)\tPrec@1 63.086 (62.449)\n",
            "Epoch: [172][27/48]\tTime 0.040 (0.053)\tLoss 1.1248 (1.0731)\tPrec@1 60.645 (62.113)\n",
            "Epoch: [172][36/48]\tTime 0.040 (0.051)\tLoss 1.0846 (1.0826)\tPrec@1 61.426 (61.711)\n",
            "Epoch: [172][45/48]\tTime 0.041 (0.049)\tLoss 1.1369 (1.0872)\tPrec@1 60.449 (61.585)\n",
            "Epoch: [172][48/48]\tTime 0.034 (0.048)\tLoss 1.1132 (1.0888)\tPrec@1 59.434 (61.466)\n",
            "EPOCH: 172 train Results: Prec@1 61.466 Loss: 1.0888\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1477 (1.1477)\tPrec@1 60.156 (60.156)\n",
            "Test: [9/9]\tTime 0.011 (0.011)\tLoss 1.1550 (1.1620)\tPrec@1 57.653 (58.590)\n",
            "EPOCH: 172 val Results: Prec@1 58.590 Loss: 1.1620\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [173][0/48]\tTime 0.042 (0.042)\tLoss 1.0185 (1.0185)\tPrec@1 64.844 (64.844)\n",
            "Epoch: [173][9/48]\tTime 0.043 (0.044)\tLoss 1.0773 (1.0649)\tPrec@1 61.719 (62.490)\n",
            "Epoch: [173][18/48]\tTime 0.041 (0.043)\tLoss 1.0820 (1.0757)\tPrec@1 63.281 (62.063)\n",
            "Epoch: [173][27/48]\tTime 0.043 (0.043)\tLoss 1.0319 (1.0720)\tPrec@1 63.574 (62.068)\n",
            "Epoch: [173][36/48]\tTime 0.040 (0.043)\tLoss 1.0797 (1.0767)\tPrec@1 61.816 (61.896)\n",
            "Epoch: [173][45/48]\tTime 0.041 (0.043)\tLoss 1.0792 (1.0850)\tPrec@1 61.426 (61.492)\n",
            "Epoch: [173][48/48]\tTime 0.033 (0.043)\tLoss 1.0766 (1.0850)\tPrec@1 62.028 (61.498)\n",
            "EPOCH: 173 train Results: Prec@1 61.498 Loss: 1.0850\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1515 (1.1515)\tPrec@1 59.375 (59.375)\n",
            "Test: [9/9]\tTime 0.008 (0.012)\tLoss 1.1562 (1.1660)\tPrec@1 58.546 (58.530)\n",
            "EPOCH: 173 val Results: Prec@1 58.530 Loss: 1.1660\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [174][0/48]\tTime 0.063 (0.063)\tLoss 1.0418 (1.0418)\tPrec@1 63.086 (63.086)\n",
            "Epoch: [174][9/48]\tTime 0.041 (0.045)\tLoss 1.0772 (1.0532)\tPrec@1 62.500 (62.744)\n",
            "Epoch: [174][18/48]\tTime 0.040 (0.044)\tLoss 1.0969 (1.0600)\tPrec@1 61.719 (62.315)\n",
            "Epoch: [174][27/48]\tTime 0.040 (0.044)\tLoss 1.1321 (1.0672)\tPrec@1 61.035 (62.120)\n",
            "Epoch: [174][36/48]\tTime 0.040 (0.043)\tLoss 1.1362 (1.0737)\tPrec@1 58.984 (61.964)\n",
            "Epoch: [174][45/48]\tTime 0.045 (0.043)\tLoss 1.1673 (1.0854)\tPrec@1 56.738 (61.383)\n",
            "Epoch: [174][48/48]\tTime 0.045 (0.043)\tLoss 1.0748 (1.0870)\tPrec@1 62.854 (61.354)\n",
            "EPOCH: 174 train Results: Prec@1 61.354 Loss: 1.0870\n",
            "Test: [0/9]\tTime 0.019 (0.019)\tLoss 1.1383 (1.1383)\tPrec@1 58.984 (58.984)\n",
            "Test: [9/9]\tTime 0.007 (0.012)\tLoss 1.1595 (1.1613)\tPrec@1 57.526 (58.660)\n",
            "EPOCH: 174 val Results: Prec@1 58.660 Loss: 1.1613\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [175][0/48]\tTime 0.045 (0.045)\tLoss 1.0375 (1.0375)\tPrec@1 63.672 (63.672)\n",
            "Epoch: [175][9/48]\tTime 0.041 (0.044)\tLoss 1.0480 (1.0435)\tPrec@1 64.453 (63.164)\n",
            "Epoch: [175][18/48]\tTime 0.044 (0.044)\tLoss 1.1224 (1.0591)\tPrec@1 58.008 (62.557)\n",
            "Epoch: [175][27/48]\tTime 0.040 (0.044)\tLoss 1.1460 (1.0678)\tPrec@1 59.668 (62.347)\n",
            "Epoch: [175][36/48]\tTime 0.039 (0.043)\tLoss 1.2070 (1.0772)\tPrec@1 56.152 (61.940)\n",
            "Epoch: [175][45/48]\tTime 0.044 (0.044)\tLoss 1.1505 (1.0839)\tPrec@1 57.227 (61.685)\n",
            "Epoch: [175][48/48]\tTime 0.034 (0.044)\tLoss 1.1352 (1.0864)\tPrec@1 58.491 (61.534)\n",
            "EPOCH: 175 train Results: Prec@1 61.534 Loss: 1.0864\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1456 (1.1456)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.017 (0.010)\tLoss 1.1593 (1.1634)\tPrec@1 58.291 (58.300)\n",
            "EPOCH: 175 val Results: Prec@1 58.300 Loss: 1.1634\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [176][0/48]\tTime 0.043 (0.043)\tLoss 1.0801 (1.0801)\tPrec@1 62.012 (62.012)\n",
            "Epoch: [176][9/48]\tTime 0.042 (0.042)\tLoss 1.0776 (1.0659)\tPrec@1 62.012 (62.705)\n",
            "Epoch: [176][18/48]\tTime 0.041 (0.043)\tLoss 1.0407 (1.0706)\tPrec@1 62.891 (62.294)\n",
            "Epoch: [176][27/48]\tTime 0.040 (0.043)\tLoss 1.0710 (1.0738)\tPrec@1 61.719 (62.074)\n",
            "Epoch: [176][36/48]\tTime 0.057 (0.045)\tLoss 1.0989 (1.0776)\tPrec@1 59.961 (61.933)\n",
            "Epoch: [176][45/48]\tTime 0.091 (0.050)\tLoss 1.1023 (1.0815)\tPrec@1 61.133 (61.808)\n",
            "Epoch: [176][48/48]\tTime 0.076 (0.052)\tLoss 1.0897 (1.0830)\tPrec@1 59.198 (61.704)\n",
            "EPOCH: 176 train Results: Prec@1 61.704 Loss: 1.0830\n",
            "Test: [0/9]\tTime 0.029 (0.029)\tLoss 1.1416 (1.1416)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.023 (0.023)\tLoss 1.1598 (1.1647)\tPrec@1 58.801 (58.340)\n",
            "EPOCH: 176 val Results: Prec@1 58.340 Loss: 1.1647\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [177][0/48]\tTime 0.090 (0.090)\tLoss 1.0883 (1.0883)\tPrec@1 62.695 (62.695)\n",
            "Epoch: [177][9/48]\tTime 0.083 (0.080)\tLoss 1.0858 (1.0594)\tPrec@1 60.547 (62.490)\n",
            "Epoch: [177][18/48]\tTime 0.092 (0.076)\tLoss 1.1573 (1.0673)\tPrec@1 58.789 (62.022)\n",
            "Epoch: [177][27/48]\tTime 0.051 (0.077)\tLoss 1.0498 (1.0768)\tPrec@1 62.305 (61.639)\n",
            "Epoch: [177][36/48]\tTime 0.041 (0.069)\tLoss 1.1275 (1.0823)\tPrec@1 61.133 (61.634)\n",
            "Epoch: [177][45/48]\tTime 0.046 (0.064)\tLoss 1.1221 (1.0877)\tPrec@1 60.840 (61.413)\n",
            "Epoch: [177][48/48]\tTime 0.033 (0.062)\tLoss 1.0807 (1.0872)\tPrec@1 62.618 (61.422)\n",
            "EPOCH: 177 train Results: Prec@1 61.422 Loss: 1.0872\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1372 (1.1372)\tPrec@1 59.863 (59.863)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1663 (1.1648)\tPrec@1 57.015 (58.200)\n",
            "EPOCH: 177 val Results: Prec@1 58.200 Loss: 1.1648\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [178][0/48]\tTime 0.062 (0.062)\tLoss 1.0809 (1.0809)\tPrec@1 60.645 (60.645)\n",
            "Epoch: [178][9/48]\tTime 0.040 (0.044)\tLoss 1.0431 (1.0546)\tPrec@1 63.672 (62.910)\n",
            "Epoch: [178][18/48]\tTime 0.040 (0.044)\tLoss 1.0451 (1.0635)\tPrec@1 61.914 (62.515)\n",
            "Epoch: [178][27/48]\tTime 0.040 (0.043)\tLoss 1.0599 (1.0730)\tPrec@1 59.863 (61.876)\n",
            "Epoch: [178][36/48]\tTime 0.040 (0.043)\tLoss 1.1397 (1.0799)\tPrec@1 58.301 (61.647)\n",
            "Epoch: [178][45/48]\tTime 0.040 (0.043)\tLoss 1.0919 (1.0824)\tPrec@1 61.133 (61.534)\n",
            "Epoch: [178][48/48]\tTime 0.056 (0.043)\tLoss 1.1021 (1.0840)\tPrec@1 61.439 (61.504)\n",
            "EPOCH: 178 train Results: Prec@1 61.504 Loss: 1.0840\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1373 (1.1373)\tPrec@1 59.082 (59.082)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1634 (1.1625)\tPrec@1 57.908 (58.500)\n",
            "EPOCH: 178 val Results: Prec@1 58.500 Loss: 1.1625\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [179][0/48]\tTime 0.045 (0.045)\tLoss 1.0606 (1.0606)\tPrec@1 61.914 (61.914)\n",
            "Epoch: [179][9/48]\tTime 0.043 (0.043)\tLoss 1.0960 (1.0581)\tPrec@1 60.742 (62.598)\n",
            "Epoch: [179][18/48]\tTime 0.040 (0.043)\tLoss 1.0094 (1.0666)\tPrec@1 63.672 (62.145)\n",
            "Epoch: [179][27/48]\tTime 0.040 (0.043)\tLoss 1.1257 (1.0757)\tPrec@1 60.840 (61.792)\n",
            "Epoch: [179][36/48]\tTime 0.040 (0.043)\tLoss 1.1065 (1.0777)\tPrec@1 60.449 (61.756)\n",
            "Epoch: [179][45/48]\tTime 0.051 (0.043)\tLoss 1.1123 (1.0842)\tPrec@1 60.059 (61.536)\n",
            "Epoch: [179][48/48]\tTime 0.039 (0.043)\tLoss 1.1063 (1.0842)\tPrec@1 59.434 (61.486)\n",
            "EPOCH: 179 train Results: Prec@1 61.486 Loss: 1.0842\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1302 (1.1302)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1710 (1.1614)\tPrec@1 57.908 (58.540)\n",
            "EPOCH: 179 val Results: Prec@1 58.540 Loss: 1.1614\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [180][0/48]\tTime 0.042 (0.042)\tLoss 1.0517 (1.0517)\tPrec@1 64.941 (64.941)\n",
            "Epoch: [180][9/48]\tTime 0.041 (0.043)\tLoss 1.0941 (1.0620)\tPrec@1 62.012 (62.832)\n",
            "Epoch: [180][18/48]\tTime 0.040 (0.044)\tLoss 1.0367 (1.0658)\tPrec@1 62.207 (62.469)\n",
            "Epoch: [180][27/48]\tTime 0.042 (0.044)\tLoss 1.0907 (1.0717)\tPrec@1 63.770 (62.207)\n",
            "Epoch: [180][36/48]\tTime 0.039 (0.043)\tLoss 1.1255 (1.0780)\tPrec@1 61.621 (62.001)\n",
            "Epoch: [180][45/48]\tTime 0.040 (0.043)\tLoss 1.1701 (1.0829)\tPrec@1 58.984 (61.742)\n",
            "Epoch: [180][48/48]\tTime 0.042 (0.043)\tLoss 1.1435 (1.0857)\tPrec@1 57.783 (61.582)\n",
            "EPOCH: 180 train Results: Prec@1 61.582 Loss: 1.0857\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1535 (1.1535)\tPrec@1 58.105 (58.105)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1627 (1.1654)\tPrec@1 57.398 (58.390)\n",
            "EPOCH: 180 val Results: Prec@1 58.390 Loss: 1.1654\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [181][0/48]\tTime 0.044 (0.044)\tLoss 1.0284 (1.0284)\tPrec@1 62.402 (62.402)\n",
            "Epoch: [181][9/48]\tTime 0.040 (0.042)\tLoss 1.1288 (1.0543)\tPrec@1 60.938 (62.695)\n",
            "Epoch: [181][18/48]\tTime 0.040 (0.043)\tLoss 1.0853 (1.0599)\tPrec@1 61.621 (62.202)\n",
            "Epoch: [181][27/48]\tTime 0.040 (0.043)\tLoss 1.0745 (1.0706)\tPrec@1 62.988 (62.005)\n",
            "Epoch: [181][36/48]\tTime 0.042 (0.043)\tLoss 1.0749 (1.0766)\tPrec@1 62.305 (61.811)\n",
            "Epoch: [181][45/48]\tTime 0.046 (0.043)\tLoss 1.1218 (1.0831)\tPrec@1 59.375 (61.528)\n",
            "Epoch: [181][48/48]\tTime 0.036 (0.043)\tLoss 1.1076 (1.0832)\tPrec@1 59.080 (61.528)\n",
            "EPOCH: 181 train Results: Prec@1 61.528 Loss: 1.0832\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1469 (1.1469)\tPrec@1 59.082 (59.082)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1602 (1.1594)\tPrec@1 57.015 (58.870)\n",
            "EPOCH: 181 val Results: Prec@1 58.870 Loss: 1.1594\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [182][0/48]\tTime 0.056 (0.056)\tLoss 1.0626 (1.0626)\tPrec@1 63.184 (63.184)\n",
            "Epoch: [182][9/48]\tTime 0.084 (0.072)\tLoss 1.0577 (1.0534)\tPrec@1 63.379 (63.213)\n",
            "Epoch: [182][18/48]\tTime 0.113 (0.077)\tLoss 1.0822 (1.0619)\tPrec@1 61.816 (62.721)\n",
            "Epoch: [182][27/48]\tTime 0.086 (0.082)\tLoss 1.0740 (1.0709)\tPrec@1 60.840 (62.315)\n",
            "Epoch: [182][36/48]\tTime 0.074 (0.080)\tLoss 1.1095 (1.0743)\tPrec@1 61.133 (62.178)\n",
            "Epoch: [182][45/48]\tTime 0.041 (0.076)\tLoss 1.0922 (1.0807)\tPrec@1 60.156 (61.816)\n",
            "Epoch: [182][48/48]\tTime 0.036 (0.074)\tLoss 1.1561 (1.0825)\tPrec@1 58.255 (61.750)\n",
            "EPOCH: 182 train Results: Prec@1 61.750 Loss: 1.0825\n",
            "Test: [0/9]\tTime 0.016 (0.016)\tLoss 1.1453 (1.1453)\tPrec@1 59.180 (59.180)\n",
            "Test: [9/9]\tTime 0.007 (0.013)\tLoss 1.1512 (1.1612)\tPrec@1 58.291 (58.230)\n",
            "EPOCH: 182 val Results: Prec@1 58.230 Loss: 1.1612\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [183][0/48]\tTime 0.047 (0.047)\tLoss 1.0223 (1.0223)\tPrec@1 65.234 (65.234)\n",
            "Epoch: [183][9/48]\tTime 0.040 (0.042)\tLoss 1.0875 (1.0560)\tPrec@1 63.965 (62.021)\n",
            "Epoch: [183][18/48]\tTime 0.043 (0.043)\tLoss 1.0884 (1.0655)\tPrec@1 61.426 (61.822)\n",
            "Epoch: [183][27/48]\tTime 0.046 (0.043)\tLoss 1.0764 (1.0703)\tPrec@1 64.648 (61.890)\n",
            "Epoch: [183][36/48]\tTime 0.042 (0.043)\tLoss 1.0504 (1.0725)\tPrec@1 60.840 (61.835)\n",
            "Epoch: [183][45/48]\tTime 0.048 (0.043)\tLoss 1.1327 (1.0826)\tPrec@1 60.059 (61.538)\n",
            "Epoch: [183][48/48]\tTime 0.033 (0.043)\tLoss 1.1834 (1.0850)\tPrec@1 58.962 (61.504)\n",
            "EPOCH: 183 train Results: Prec@1 61.504 Loss: 1.0850\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1541 (1.1541)\tPrec@1 58.398 (58.398)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1623 (1.1599)\tPrec@1 58.673 (58.620)\n",
            "EPOCH: 183 val Results: Prec@1 58.620 Loss: 1.1599\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [184][0/48]\tTime 0.042 (0.042)\tLoss 0.9897 (0.9897)\tPrec@1 66.016 (66.016)\n",
            "Epoch: [184][9/48]\tTime 0.042 (0.042)\tLoss 1.0648 (1.0477)\tPrec@1 63.770 (63.242)\n",
            "Epoch: [184][18/48]\tTime 0.065 (0.044)\tLoss 1.1098 (1.0600)\tPrec@1 60.352 (62.659)\n",
            "Epoch: [184][27/48]\tTime 0.040 (0.043)\tLoss 1.1073 (1.0677)\tPrec@1 61.035 (62.507)\n",
            "Epoch: [184][36/48]\tTime 0.039 (0.043)\tLoss 1.0668 (1.0739)\tPrec@1 62.305 (62.178)\n",
            "Epoch: [184][45/48]\tTime 0.040 (0.043)\tLoss 1.0858 (1.0798)\tPrec@1 60.645 (61.861)\n",
            "Epoch: [184][48/48]\tTime 0.034 (0.043)\tLoss 1.1736 (1.0806)\tPrec@1 58.844 (61.770)\n",
            "EPOCH: 184 train Results: Prec@1 61.770 Loss: 1.0806\n",
            "Test: [0/9]\tTime 0.011 (0.011)\tLoss 1.1560 (1.1560)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.017 (0.012)\tLoss 1.1526 (1.1637)\tPrec@1 57.908 (58.590)\n",
            "EPOCH: 184 val Results: Prec@1 58.590 Loss: 1.1637\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [185][0/48]\tTime 0.045 (0.045)\tLoss 1.0314 (1.0314)\tPrec@1 65.723 (65.723)\n",
            "Epoch: [185][9/48]\tTime 0.041 (0.043)\tLoss 1.0912 (1.0614)\tPrec@1 61.523 (62.656)\n",
            "Epoch: [185][18/48]\tTime 0.040 (0.044)\tLoss 1.0652 (1.0612)\tPrec@1 60.938 (62.433)\n",
            "Epoch: [185][27/48]\tTime 0.041 (0.044)\tLoss 1.0298 (1.0614)\tPrec@1 62.695 (62.441)\n",
            "Epoch: [185][36/48]\tTime 0.044 (0.044)\tLoss 1.1407 (1.0675)\tPrec@1 59.375 (62.170)\n",
            "Epoch: [185][45/48]\tTime 0.049 (0.044)\tLoss 1.1648 (1.0758)\tPrec@1 59.180 (61.876)\n",
            "Epoch: [185][48/48]\tTime 0.034 (0.044)\tLoss 1.2022 (1.0790)\tPrec@1 57.901 (61.774)\n",
            "EPOCH: 185 train Results: Prec@1 61.774 Loss: 1.0790\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1537 (1.1537)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1508 (1.1685)\tPrec@1 57.781 (58.520)\n",
            "EPOCH: 185 val Results: Prec@1 58.520 Loss: 1.1685\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [186][0/48]\tTime 0.043 (0.043)\tLoss 1.0462 (1.0462)\tPrec@1 64.258 (64.258)\n",
            "Epoch: [186][9/48]\tTime 0.046 (0.044)\tLoss 1.0829 (1.0571)\tPrec@1 60.059 (62.666)\n",
            "Epoch: [186][18/48]\tTime 0.041 (0.044)\tLoss 1.0320 (1.0641)\tPrec@1 64.258 (62.423)\n",
            "Epoch: [186][27/48]\tTime 0.050 (0.043)\tLoss 1.0573 (1.0688)\tPrec@1 62.695 (62.092)\n",
            "Epoch: [186][36/48]\tTime 0.040 (0.044)\tLoss 1.1125 (1.0790)\tPrec@1 61.133 (61.798)\n",
            "Epoch: [186][45/48]\tTime 0.039 (0.043)\tLoss 1.0205 (1.0818)\tPrec@1 64.551 (61.753)\n",
            "Epoch: [186][48/48]\tTime 0.034 (0.043)\tLoss 1.0914 (1.0811)\tPrec@1 62.618 (61.792)\n",
            "EPOCH: 186 train Results: Prec@1 61.792 Loss: 1.0811\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1479 (1.1479)\tPrec@1 58.594 (58.594)\n",
            "Test: [9/9]\tTime 0.010 (0.011)\tLoss 1.1486 (1.1661)\tPrec@1 58.546 (58.400)\n",
            "EPOCH: 186 val Results: Prec@1 58.400 Loss: 1.1661\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [187][0/48]\tTime 0.047 (0.047)\tLoss 1.0400 (1.0400)\tPrec@1 64.648 (64.648)\n",
            "Epoch: [187][9/48]\tTime 0.040 (0.044)\tLoss 1.0091 (1.0481)\tPrec@1 63.281 (63.057)\n",
            "Epoch: [187][18/48]\tTime 0.060 (0.051)\tLoss 1.0379 (1.0602)\tPrec@1 63.477 (62.521)\n",
            "Epoch: [187][27/48]\tTime 0.079 (0.060)\tLoss 1.0356 (1.0615)\tPrec@1 61.816 (62.259)\n",
            "Epoch: [187][36/48]\tTime 0.083 (0.065)\tLoss 1.1390 (1.0755)\tPrec@1 59.668 (61.856)\n",
            "Epoch: [187][45/48]\tTime 0.089 (0.068)\tLoss 1.1320 (1.0819)\tPrec@1 59.570 (61.625)\n",
            "Epoch: [187][48/48]\tTime 0.074 (0.069)\tLoss 1.0901 (1.0831)\tPrec@1 63.090 (61.602)\n",
            "EPOCH: 187 train Results: Prec@1 61.602 Loss: 1.0831\n",
            "Test: [0/9]\tTime 0.024 (0.024)\tLoss 1.1499 (1.1499)\tPrec@1 58.398 (58.398)\n",
            "Test: [9/9]\tTime 0.021 (0.019)\tLoss 1.1638 (1.1705)\tPrec@1 56.250 (57.930)\n",
            "EPOCH: 187 val Results: Prec@1 57.930 Loss: 1.1705\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [188][0/48]\tTime 0.098 (0.098)\tLoss 1.0097 (1.0097)\tPrec@1 64.160 (64.160)\n",
            "Epoch: [188][9/48]\tTime 0.053 (0.063)\tLoss 1.0873 (1.0343)\tPrec@1 62.500 (63.672)\n",
            "Epoch: [188][18/48]\tTime 0.040 (0.054)\tLoss 1.0447 (1.0556)\tPrec@1 64.453 (62.844)\n",
            "Epoch: [188][27/48]\tTime 0.041 (0.050)\tLoss 1.0383 (1.0609)\tPrec@1 64.258 (62.608)\n",
            "Epoch: [188][36/48]\tTime 0.040 (0.048)\tLoss 1.0645 (1.0697)\tPrec@1 61.621 (62.307)\n",
            "Epoch: [188][45/48]\tTime 0.042 (0.048)\tLoss 1.1942 (1.0791)\tPrec@1 58.887 (61.931)\n",
            "Epoch: [188][48/48]\tTime 0.033 (0.047)\tLoss 1.1226 (1.0802)\tPrec@1 59.552 (61.846)\n",
            "EPOCH: 188 train Results: Prec@1 61.846 Loss: 1.0802\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1576 (1.1576)\tPrec@1 57.520 (57.520)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1471 (1.1632)\tPrec@1 57.398 (58.560)\n",
            "EPOCH: 188 val Results: Prec@1 58.560 Loss: 1.1632\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [189][0/48]\tTime 0.044 (0.044)\tLoss 1.0780 (1.0780)\tPrec@1 61.426 (61.426)\n",
            "Epoch: [189][9/48]\tTime 0.043 (0.042)\tLoss 1.0220 (1.0469)\tPrec@1 63.965 (63.252)\n",
            "Epoch: [189][18/48]\tTime 0.040 (0.043)\tLoss 1.0900 (1.0611)\tPrec@1 58.789 (62.089)\n",
            "Epoch: [189][27/48]\tTime 0.043 (0.043)\tLoss 1.0848 (1.0706)\tPrec@1 61.816 (61.837)\n",
            "Epoch: [189][36/48]\tTime 0.040 (0.043)\tLoss 1.1144 (1.0741)\tPrec@1 59.180 (61.795)\n",
            "Epoch: [189][45/48]\tTime 0.040 (0.043)\tLoss 1.1130 (1.0789)\tPrec@1 62.109 (61.748)\n",
            "Epoch: [189][48/48]\tTime 0.033 (0.043)\tLoss 1.1689 (1.0832)\tPrec@1 58.962 (61.622)\n",
            "EPOCH: 189 train Results: Prec@1 61.622 Loss: 1.0832\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1487 (1.1487)\tPrec@1 58.203 (58.203)\n",
            "Test: [9/9]\tTime 0.009 (0.011)\tLoss 1.1714 (1.1660)\tPrec@1 57.270 (58.290)\n",
            "EPOCH: 189 val Results: Prec@1 58.290 Loss: 1.1660\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [190][0/48]\tTime 0.045 (0.045)\tLoss 1.0642 (1.0642)\tPrec@1 62.402 (62.402)\n",
            "Epoch: [190][9/48]\tTime 0.040 (0.042)\tLoss 1.0737 (1.0477)\tPrec@1 61.621 (63.535)\n",
            "Epoch: [190][18/48]\tTime 0.041 (0.043)\tLoss 1.0783 (1.0494)\tPrec@1 61.523 (63.214)\n",
            "Epoch: [190][27/48]\tTime 0.040 (0.043)\tLoss 1.1088 (1.0589)\tPrec@1 60.352 (62.681)\n",
            "Epoch: [190][36/48]\tTime 0.040 (0.044)\tLoss 1.1177 (1.0686)\tPrec@1 59.570 (62.128)\n",
            "Epoch: [190][45/48]\tTime 0.042 (0.044)\tLoss 1.1009 (1.0744)\tPrec@1 60.840 (61.990)\n",
            "Epoch: [190][48/48]\tTime 0.034 (0.043)\tLoss 1.1314 (1.0773)\tPrec@1 58.019 (61.872)\n",
            "EPOCH: 190 train Results: Prec@1 61.872 Loss: 1.0773\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1472 (1.1472)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.007 (0.010)\tLoss 1.1591 (1.1597)\tPrec@1 58.673 (58.800)\n",
            "EPOCH: 190 val Results: Prec@1 58.800 Loss: 1.1597\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [191][0/48]\tTime 0.042 (0.042)\tLoss 1.0812 (1.0812)\tPrec@1 61.230 (61.230)\n",
            "Epoch: [191][9/48]\tTime 0.044 (0.045)\tLoss 1.0708 (1.0617)\tPrec@1 62.012 (62.607)\n",
            "Epoch: [191][18/48]\tTime 0.042 (0.044)\tLoss 1.0929 (1.0684)\tPrec@1 60.254 (62.073)\n",
            "Epoch: [191][27/48]\tTime 0.041 (0.044)\tLoss 1.1074 (1.0718)\tPrec@1 62.207 (62.057)\n",
            "Epoch: [191][36/48]\tTime 0.040 (0.044)\tLoss 1.0769 (1.0739)\tPrec@1 60.938 (61.967)\n",
            "Epoch: [191][45/48]\tTime 0.041 (0.043)\tLoss 1.1424 (1.0814)\tPrec@1 61.035 (61.785)\n",
            "Epoch: [191][48/48]\tTime 0.034 (0.043)\tLoss 1.1814 (1.0856)\tPrec@1 58.491 (61.650)\n",
            "EPOCH: 191 train Results: Prec@1 61.650 Loss: 1.0856\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1514 (1.1514)\tPrec@1 58.887 (58.887)\n",
            "Test: [9/9]\tTime 0.009 (0.011)\tLoss 1.1620 (1.1614)\tPrec@1 57.653 (58.520)\n",
            "EPOCH: 191 val Results: Prec@1 58.520 Loss: 1.1614\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [192][0/48]\tTime 0.043 (0.043)\tLoss 1.0180 (1.0180)\tPrec@1 63.379 (63.379)\n",
            "Epoch: [192][9/48]\tTime 0.042 (0.046)\tLoss 1.0564 (1.0348)\tPrec@1 63.086 (63.428)\n",
            "Epoch: [192][18/48]\tTime 0.040 (0.044)\tLoss 1.1083 (1.0519)\tPrec@1 60.352 (62.670)\n",
            "Epoch: [192][27/48]\tTime 0.046 (0.044)\tLoss 1.0475 (1.0587)\tPrec@1 62.598 (62.354)\n",
            "Epoch: [192][36/48]\tTime 0.073 (0.049)\tLoss 1.0766 (1.0660)\tPrec@1 61.328 (62.030)\n",
            "Epoch: [192][45/48]\tTime 0.094 (0.057)\tLoss 1.0895 (1.0719)\tPrec@1 61.914 (61.969)\n",
            "Epoch: [192][48/48]\tTime 0.070 (0.059)\tLoss 1.1194 (1.0739)\tPrec@1 58.491 (61.920)\n",
            "EPOCH: 192 train Results: Prec@1 61.920 Loss: 1.0739\n",
            "Test: [0/9]\tTime 0.018 (0.018)\tLoss 1.1359 (1.1359)\tPrec@1 59.277 (59.277)\n",
            "Test: [9/9]\tTime 0.015 (0.018)\tLoss 1.1693 (1.1574)\tPrec@1 56.760 (58.590)\n",
            "EPOCH: 192 val Results: Prec@1 58.590 Loss: 1.1574\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [193][0/48]\tTime 0.087 (0.087)\tLoss 1.0024 (1.0024)\tPrec@1 63.770 (63.770)\n",
            "Epoch: [193][9/48]\tTime 0.074 (0.075)\tLoss 1.0970 (1.0541)\tPrec@1 61.719 (62.842)\n",
            "Epoch: [193][18/48]\tTime 0.102 (0.078)\tLoss 1.1162 (1.0625)\tPrec@1 59.473 (62.413)\n",
            "Epoch: [193][27/48]\tTime 0.040 (0.067)\tLoss 1.0740 (1.0677)\tPrec@1 62.793 (62.263)\n",
            "Epoch: [193][36/48]\tTime 0.039 (0.061)\tLoss 1.1354 (1.0775)\tPrec@1 60.742 (61.896)\n",
            "Epoch: [193][45/48]\tTime 0.039 (0.057)\tLoss 1.1292 (1.0808)\tPrec@1 58.691 (61.742)\n",
            "Epoch: [193][48/48]\tTime 0.034 (0.056)\tLoss 1.0762 (1.0815)\tPrec@1 61.557 (61.676)\n",
            "EPOCH: 193 train Results: Prec@1 61.676 Loss: 1.0815\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1349 (1.1349)\tPrec@1 59.668 (59.668)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1529 (1.1569)\tPrec@1 59.311 (58.940)\n",
            "EPOCH: 193 val Results: Prec@1 58.940 Loss: 1.1569\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [194][0/48]\tTime 0.044 (0.044)\tLoss 1.0916 (1.0916)\tPrec@1 61.035 (61.035)\n",
            "Epoch: [194][9/48]\tTime 0.043 (0.042)\tLoss 1.0180 (1.0497)\tPrec@1 63.574 (62.930)\n",
            "Epoch: [194][18/48]\tTime 0.041 (0.043)\tLoss 1.0850 (1.0633)\tPrec@1 60.840 (62.222)\n",
            "Epoch: [194][27/48]\tTime 0.040 (0.043)\tLoss 1.0387 (1.0668)\tPrec@1 63.574 (61.991)\n",
            "Epoch: [194][36/48]\tTime 0.042 (0.044)\tLoss 1.0279 (1.0713)\tPrec@1 65.527 (61.803)\n",
            "Epoch: [194][45/48]\tTime 0.044 (0.044)\tLoss 1.1514 (1.0767)\tPrec@1 58.496 (61.661)\n",
            "Epoch: [194][48/48]\tTime 0.033 (0.043)\tLoss 1.0750 (1.0760)\tPrec@1 60.849 (61.704)\n",
            "EPOCH: 194 train Results: Prec@1 61.704 Loss: 1.0760\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1402 (1.1402)\tPrec@1 59.473 (59.473)\n",
            "Test: [9/9]\tTime 0.008 (0.011)\tLoss 1.1520 (1.1598)\tPrec@1 58.291 (58.970)\n",
            "EPOCH: 194 val Results: Prec@1 58.970 Loss: 1.1598\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [195][0/48]\tTime 0.043 (0.043)\tLoss 0.9866 (0.9866)\tPrec@1 66.113 (66.113)\n",
            "Epoch: [195][9/48]\tTime 0.050 (0.046)\tLoss 1.0475 (1.0343)\tPrec@1 65.430 (63.564)\n",
            "Epoch: [195][18/48]\tTime 0.045 (0.044)\tLoss 1.1429 (1.0561)\tPrec@1 59.766 (62.377)\n",
            "Epoch: [195][27/48]\tTime 0.040 (0.044)\tLoss 1.0656 (1.0594)\tPrec@1 62.695 (62.399)\n",
            "Epoch: [195][36/48]\tTime 0.042 (0.044)\tLoss 1.0855 (1.0662)\tPrec@1 60.840 (62.170)\n",
            "Epoch: [195][45/48]\tTime 0.041 (0.044)\tLoss 1.1141 (1.0708)\tPrec@1 61.816 (62.118)\n",
            "Epoch: [195][48/48]\tTime 0.033 (0.043)\tLoss 1.1057 (1.0740)\tPrec@1 60.142 (61.990)\n",
            "EPOCH: 195 train Results: Prec@1 61.990 Loss: 1.0740\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1481 (1.1481)\tPrec@1 59.180 (59.180)\n",
            "Test: [9/9]\tTime 0.011 (0.010)\tLoss 1.1548 (1.1575)\tPrec@1 57.781 (58.450)\n",
            "EPOCH: 195 val Results: Prec@1 58.450 Loss: 1.1575\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [196][0/48]\tTime 0.046 (0.046)\tLoss 1.0230 (1.0230)\tPrec@1 64.453 (64.453)\n",
            "Epoch: [196][9/48]\tTime 0.039 (0.044)\tLoss 1.0130 (1.0324)\tPrec@1 64.355 (63.926)\n",
            "Epoch: [196][18/48]\tTime 0.041 (0.043)\tLoss 1.0335 (1.0477)\tPrec@1 63.086 (63.029)\n",
            "Epoch: [196][27/48]\tTime 0.055 (0.044)\tLoss 1.1155 (1.0570)\tPrec@1 59.473 (62.699)\n",
            "Epoch: [196][36/48]\tTime 0.041 (0.043)\tLoss 1.1188 (1.0659)\tPrec@1 60.352 (62.381)\n",
            "Epoch: [196][45/48]\tTime 0.041 (0.043)\tLoss 1.1091 (1.0699)\tPrec@1 60.352 (62.137)\n",
            "Epoch: [196][48/48]\tTime 0.033 (0.043)\tLoss 1.0122 (1.0708)\tPrec@1 65.094 (62.144)\n",
            "EPOCH: 196 train Results: Prec@1 62.144 Loss: 1.0708\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1444 (1.1444)\tPrec@1 58.984 (58.984)\n",
            "Test: [9/9]\tTime 0.008 (0.012)\tLoss 1.1568 (1.1596)\tPrec@1 56.378 (58.540)\n",
            "EPOCH: 196 val Results: Prec@1 58.540 Loss: 1.1596\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [197][0/48]\tTime 0.069 (0.069)\tLoss 1.0231 (1.0231)\tPrec@1 64.648 (64.648)\n",
            "Epoch: [197][9/48]\tTime 0.040 (0.045)\tLoss 1.0384 (1.0229)\tPrec@1 62.500 (63.838)\n",
            "Epoch: [197][18/48]\tTime 0.042 (0.044)\tLoss 1.0632 (1.0418)\tPrec@1 62.305 (63.333)\n",
            "Epoch: [197][27/48]\tTime 0.043 (0.045)\tLoss 1.0626 (1.0549)\tPrec@1 62.500 (62.898)\n",
            "Epoch: [197][36/48]\tTime 0.040 (0.044)\tLoss 1.0967 (1.0672)\tPrec@1 61.133 (62.545)\n",
            "Epoch: [197][45/48]\tTime 0.060 (0.045)\tLoss 1.0534 (1.0736)\tPrec@1 62.695 (62.292)\n",
            "Epoch: [197][48/48]\tTime 0.070 (0.047)\tLoss 1.1161 (1.0770)\tPrec@1 59.788 (62.112)\n",
            "EPOCH: 197 train Results: Prec@1 62.112 Loss: 1.0770\n",
            "Test: [0/9]\tTime 0.017 (0.017)\tLoss 1.1448 (1.1448)\tPrec@1 58.301 (58.301)\n",
            "Test: [9/9]\tTime 0.020 (0.020)\tLoss 1.1758 (1.1629)\tPrec@1 57.270 (58.220)\n",
            "EPOCH: 197 val Results: Prec@1 58.220 Loss: 1.1629\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [198][0/48]\tTime 0.074 (0.074)\tLoss 1.0041 (1.0041)\tPrec@1 65.723 (65.723)\n",
            "Epoch: [198][9/48]\tTime 0.079 (0.081)\tLoss 1.0856 (1.0438)\tPrec@1 60.449 (63.867)\n",
            "Epoch: [198][18/48]\tTime 0.052 (0.078)\tLoss 1.1056 (1.0580)\tPrec@1 61.426 (63.029)\n",
            "Epoch: [198][27/48]\tTime 0.076 (0.082)\tLoss 1.1478 (1.0638)\tPrec@1 59.082 (62.605)\n",
            "Epoch: [198][36/48]\tTime 0.062 (0.076)\tLoss 1.0795 (1.0671)\tPrec@1 61.426 (62.423)\n",
            "Epoch: [198][45/48]\tTime 0.041 (0.069)\tLoss 1.1070 (1.0717)\tPrec@1 61.328 (62.258)\n",
            "Epoch: [198][48/48]\tTime 0.035 (0.068)\tLoss 1.0933 (1.0731)\tPrec@1 60.259 (62.202)\n",
            "EPOCH: 198 train Results: Prec@1 62.202 Loss: 1.0731\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1449 (1.1449)\tPrec@1 58.203 (58.203)\n",
            "Test: [9/9]\tTime 0.009 (0.011)\tLoss 1.1758 (1.1660)\tPrec@1 57.143 (58.210)\n",
            "EPOCH: 198 val Results: Prec@1 58.210 Loss: 1.1660\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [199][0/48]\tTime 0.043 (0.043)\tLoss 1.0615 (1.0615)\tPrec@1 62.695 (62.695)\n",
            "Epoch: [199][9/48]\tTime 0.041 (0.045)\tLoss 1.1080 (1.0399)\tPrec@1 61.621 (63.906)\n",
            "Epoch: [199][18/48]\tTime 0.039 (0.043)\tLoss 1.0883 (1.0393)\tPrec@1 60.938 (63.749)\n",
            "Epoch: [199][27/48]\tTime 0.040 (0.043)\tLoss 1.0897 (1.0535)\tPrec@1 61.328 (62.957)\n",
            "Epoch: [199][36/48]\tTime 0.042 (0.043)\tLoss 1.0902 (1.0617)\tPrec@1 60.938 (62.574)\n",
            "Epoch: [199][45/48]\tTime 0.039 (0.043)\tLoss 1.1145 (1.0710)\tPrec@1 59.277 (62.120)\n",
            "Epoch: [199][48/48]\tTime 0.033 (0.043)\tLoss 1.0530 (1.0732)\tPrec@1 61.203 (62.024)\n",
            "EPOCH: 199 train Results: Prec@1 62.024 Loss: 1.0732\n",
            "Test: [0/9]\tTime 0.009 (0.009)\tLoss 1.1492 (1.1492)\tPrec@1 58.789 (58.789)\n",
            "Test: [9/9]\tTime 0.009 (0.011)\tLoss 1.1608 (1.1654)\tPrec@1 58.163 (58.770)\n",
            "EPOCH: 199 val Results: Prec@1 58.770 Loss: 1.1654\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "current lr 1.00000e-02\n",
            "Epoch: [200][0/48]\tTime 0.044 (0.044)\tLoss 1.0633 (1.0633)\tPrec@1 63.477 (63.477)\n",
            "Epoch: [200][9/48]\tTime 0.044 (0.044)\tLoss 1.0807 (1.0571)\tPrec@1 62.988 (63.057)\n",
            "Epoch: [200][18/48]\tTime 0.041 (0.044)\tLoss 1.0878 (1.0625)\tPrec@1 62.598 (62.572)\n",
            "Epoch: [200][27/48]\tTime 0.041 (0.043)\tLoss 1.0895 (1.0620)\tPrec@1 60.156 (62.472)\n",
            "Epoch: [200][36/48]\tTime 0.040 (0.044)\tLoss 0.9842 (1.0618)\tPrec@1 64.648 (62.450)\n",
            "Epoch: [200][45/48]\tTime 0.040 (0.044)\tLoss 1.0180 (1.0689)\tPrec@1 64.160 (62.235)\n",
            "Epoch: [200][48/48]\tTime 0.035 (0.043)\tLoss 1.1502 (1.0707)\tPrec@1 59.198 (62.152)\n",
            "EPOCH: 200 train Results: Prec@1 62.152 Loss: 1.0707\n",
            "Test: [0/9]\tTime 0.010 (0.010)\tLoss 1.1494 (1.1494)\tPrec@1 58.301 (58.301)\n",
            "Test: [9/9]\tTime 0.007 (0.011)\tLoss 1.1605 (1.1653)\tPrec@1 57.526 (58.710)\n",
            "EPOCH: 200 val Results: Prec@1 58.710 Loss: 1.1653\n",
            "Best Prec@1: 59.040\n",
            "\n",
            "End time:  Fri Apr 12 00:49:34 2024\n",
            "train executed in 521.5465 seconds\n"
          ]
        }
      ],
      "source": [
        "def get_model(layers):\n",
        "    model = MLP()\n",
        "    str2obj = {\n",
        "        'linear': HiddenLayer,\n",
        "        'relu': relu,\n",
        "        'leaky_relu': leaky_relu,\n",
        "        'sigmoid': sigmoid,\n",
        "        'tanh': tanh,\n",
        "        'batchnorm': batchnorm,\n",
        "        'dropout': dropout\n",
        "    }\n",
        "    for i in layers:\n",
        "        model.add_layer(str2obj[i['type']](**i['params']))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "layers = [\n",
        "    {'type': 'linear', 'params': {'name': 'fc1', 'in_num': 128, 'out_num': 256}},\n",
        "    {'type': 'batchnorm', 'params': {'name': 'bn1', 'shape': 256}},\n",
        "    {'type': 'dropout', 'params': {'name': 'dropout1', 'drop_rate': 0.3}},\n",
        "    #{'type': 'sigmoid', 'params': {'name': 'sigmoid'}},\n",
        "    #{'type': 'leaky_relu', 'params': {'name': 'leaky_relu1', 'alpha': 0.1}},\n",
        "    {'type': 'relu', 'params': {'name': 'relu1'}},\n",
        "    #{'type': 'tanh', 'params': {'name': 'tanh1'}},\n",
        "    {'type': 'linear', 'params': {'name': 'fc2', 'in_num': 256, 'out_num': 128}},\n",
        "    {'type': 'batchnorm', 'params': {'name': 'bn2', 'shape': 128}},\n",
        "    {'type': 'dropout', 'params': {'name': 'dropout2', 'drop_rate': 0.3}},\n",
        "    {'type': 'relu', 'params': {'name': 'relu2'}},\n",
        "    {'type': 'linear', 'params': {'name': 'fc3', 'in_num': 128, 'out_num': 10}},\n",
        "]\n",
        "\n",
        "bs = 1024\n",
        "config = {\n",
        "    'layers': layers,\n",
        "    'lr': 0.01,\n",
        "    'bs': bs,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 5e-4,\n",
        "    'seed': 0,\n",
        "    'epoch': 200,\n",
        "    'optimizer': 'sgd',     # adam, sgd\n",
        "    'scheduler': None,      # cos, None\n",
        "    'pre-process': 'standardization',      # min-max, standardization, None\n",
        "    'print_freq': 50000 // bs // 5\n",
        "}\n",
        "np.random.seed(config['seed'])\n",
        "\n",
        "# pre process\n",
        "train_X, test_X = get_transform(train_X, test_X, config['pre-process'])\n",
        "\n",
        "train_dataloader = Dataloader(train_X, train_y, config['bs'], shuffle=True, seed=config['seed'])\n",
        "test_dataloader = Dataloader(test_X, test_y, config['bs'], shuffle=False)\n",
        "model = get_model(config['layers'])\n",
        "trainer = Trainer(config, model, train_dataloader, test_dataloader)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LzF2S2qtIY4"
      },
      "source": [
        "# Plot results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKj2ik_itIY4"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "o5uP2_uBtIY4",
        "outputId": "fff46e03-42d6-490e-b6c6-23f9a24cfba9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.plot_cm('./cm.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn91K5VXtIY4"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "m6Sh8IMxtIY4",
        "outputId": "30543417-0e8c-4a78-f959-05dca6881348"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8PklEQVR4nO3dd3zN9/cH8NfN3iEhq4jYam+hdtSq2kq1olVajV2/Kh3oV2mVaquqtUJrlqKovUtRmxaxZxKKZglZ9/z+eLv3urKTm9zc5PV8PO7j3vsZ73s+9ya5J++pEREBERERkQWyMncARERERDnFRIaIiIgsFhMZIiIislhMZIiIiMhiMZEhIiIii8VEhoiIiCwWExkiIiKyWExkiIiIyGIxkSEiIiKLxUSGKBcGDBiAsmXL5ujciRMnQqPRmDagAubatWvQaDRYtGiRuUPJ1KJFi6DRaHDt2jVzh0JE2cBEhgoljUaTpduePXvMHWqRV7Zs2Sx9VqZKhqZMmYJ169aZpKy88P7770Oj0eCVV14xdyhEFkHDtZaoMFqyZInR859++gnbt2/Hzz//bLS9bdu28Pb2zvHrJCUlQavVwt7ePtvnJicnIzk5GQ4ODjl+/YLu2rVrCAgIQGhoKAYMGJDmMevWrUNcXJz++aZNm7B8+XLMnDkTJUqU0G9v0qQJypUrl+uYXFxc0LNnz1SJUUpKCpKSkmBvb2+2mjIRQZkyZWBjY4M7d+7gzp07cHV1NUssRJbCxtwBEOWF1157zej5oUOHsH379lTbnxUfHw8nJ6csv46trW2O4gMAGxsb2NjwV7Br165GzyMjI7F8+XJ07do1x812OWFtbQ1ra+t8e7207NmzB7du3cKuXbvQrl07rFmzBsHBwWaNKT3Z/V0hyitsWqIiq2XLlqhevTqOHTuG5s2bw8nJCePHjwcA/Pbbb+jUqRP8/Pxgb2+P8uXL43//+x9SUlKMyni2j4yuT8j06dMxd+5clC9fHvb29mjQoAGOHDlidG5afWQ0Gg2GDh2KdevWoXr16rC3t0e1atWwZcuWVPHv2bMH9evXh4ODA8qXL48ff/wxy/1u/vjjD/Tq1QtlypSBvb09SpcujVGjRuHRo0eprs/FxQW3b99G165d4eLigpIlS2LMmDGp3ouoqCgMGDAA7u7uKFasGIKDgxEVFZVpLFm1ZMkS1KtXD46OjvDw8ECfPn1w8+ZNo2MuXryIHj16wMfHBw4ODihVqhT69OmD6OhoAOr9ffjwIRYvXqxvstLVFKXVR6Zs2bJ46aWXsH//fjRs2BAODg4oV64cfvrpp1TxnT59Gi1atICjoyNKlSqFyZMnIzQ0NFv9bpYuXYrnn38erVq1QlBQEJYuXZrmcbdv38bAgQP1P58BAQEYMmQIEhMT9cdERUVh1KhRKFu2LOzt7VGqVCn0798f9+7dS/d6AfVz9Wyzqyl+VwDg8OHD6NixI4oXLw5nZ2fUrFkT33zzDQDo36sTJ06kOm/KlCmwtrbG7du3s/Q+UtHCfwepSLt//z46dOiAPn364LXXXtM3My1atAguLi4YPXo0XFxcsGvXLnzyySeIiYnBl19+mWm5y5YtQ2xsLN5++21oNBpMmzYN3bt3x5UrVzKtxdm/fz/WrFmDd999F66urvj222/Ro0cP3LhxA56engCAEydOoH379vD19cWkSZOQkpKCTz/9FCVLlszSda9atQrx8fEYMmQIPD098ddff2HWrFm4desWVq1aZXRsSkoK2rVrh0aNGmH69OnYsWMHZsyYgfLly2PIkCEAVJNIly5dsH//frzzzjuoWrUq1q5da7LahM8++wwff/wxevfujbfeegv//vsvZs2ahebNm+PEiRMoVqwYEhMT0a5dOyQkJGDYsGHw8fHB7du3sXHjRkRFRcHd3R0///wz3nrrLTRs2BCDBw8GAJQvXz7D17506RJ69uyJgQMHIjg4GAsXLsSAAQNQr149VKtWDYBKLFq1agWNRoNx48bB2dkZ8+fPz1aTY0JCAn799Ve89957AIC+ffvijTfeQGRkJHx8fPTHhYeHo2HDhoiKisLgwYNRpUoV3L59G6tXr0Z8fDzs7OwQFxeHZs2a4dy5c3jzzTdRt25d3Lt3D+vXr8etW7eMmuyyKre/K9u3b8dLL70EX19fjBgxAj4+Pjh37hw2btyIESNGoGfPnggJCcHSpUtRp04do9deunQpWrZsieeeey7bcVMRIERFQEhIiDz7496iRQsBID/88EOq4+Pj41Nte/vtt8XJyUkeP36s3xYcHCz+/v7651evXhUA4unpKQ8ePNBv/+233wSAbNiwQb9twoQJqWICIHZ2dnLp0iX9tlOnTgkAmTVrln5b586dxcnJSW7fvq3fdvHiRbGxsUlVZlrSur6pU6eKRqOR69evG10fAPn000+Njq1Tp47Uq1dP/3zdunUCQKZNm6bflpycLM2aNRMAEhoammlMOl9++aUAkKtXr4qIyLVr18Ta2lo+++wzo+POnDkjNjY2+u0nTpwQALJq1aoMy3d2dpbg4OBU20NDQ41eV0TE399fAMi+ffv02+7evSv29vby3nvv6bcNGzZMNBqNnDhxQr/t/v374uHhkarM9KxevVoAyMWLF0VEJCYmRhwcHGTmzJlGx/Xv31+srKzkyJEjqcrQarUiIvLJJ58IAFmzZk26x6R1vSIiu3fvFgCye/du/bbc/q4kJydLQECA+Pv7y3///ZdmPCIiffv2FT8/P0lJSdFvO378eLZ/hqhoYdMSFWn29vZ44403Um13dHTUP46NjcW9e/fQrFkzxMfH4/z585mW+8orr6B48eL6582aNQMAXLlyJdNzg4KCjGoJatasCTc3N/25KSkp2LFjB7p27Qo/Pz/9cRUqVECHDh0yLR8wvr6HDx/i3r17aNKkCUQkzar9d955x+h5s2bNjK5l06ZNsLGx0dfQAKrPybBhw7IUT0bWrFkDrVaL3r174969e/qbj48PKlasiN27dwMA3N3dAQBbt25FfHx8rl9X5/nnn9d/fgBQsmRJVK5c2ej6t2zZgsDAQNSuXVu/zcPDA/369cvy6yxduhT169dHhQoVAACurq7o1KmTUfOSVqvFunXr0LlzZ9SvXz9VGbpmxV9//RW1atVCt27d0j0mu3Lzu3LixAlcvXoVI0eORLFixdKNp3///ggPD9d/poB6XxwdHdGjR48cxU2FHxMZKtKee+452NnZpdr+zz//oFu3bnB3d4ebmxtKliyp7yis62+RkTJlyhg91yU1//33X7bP1Z2vO/fu3bt49OiR/gvvaWltS8uNGzcwYMAAeHh46Pu9tGjRAkDq63NwcEjVZPV0PABw/fp1+Pr6wsXFxei4ypUrZymejFy8eBEigooVK6JkyZJGt3PnzuHu3bsAgICAAIwePRrz589HiRIl0K5dO8yePTtLn1dGMvs8AHX9ufk8oqKisGnTJrRo0QKXLl3S35o2bYqjR4/iwoULAIB///0XMTExqF69eoblXb58OdNjsis3vyuXL18GgExjatu2LXx9ffXJm1arxfLly9GlSxeO3qJ0sY8MFWlP/zepExUVhRYtWsDNzQ2ffvopypcvDwcHBxw/fhxjx46FVqvNtNz0Rr9IFmY7yM25WZGSkoK2bdviwYMHGDt2LKpUqQJnZ2fcvn0bAwYMSHV95h7Jo9VqodFosHnz5jRjeTp5mjFjBgYMGIDffvsN27Ztw/DhwzF16lQcOnQIpUqVytHr5/XnAag+SwkJCZgxYwZmzJiRav/SpUsxadIkk70ekH7NTFqddIG8+115mrW1NV599VXMmzcP33//PQ4cOIDw8PBMRxtS0cZEhugZe/bswf3797FmzRo0b95cv/3q1atmjMrAy8sLDg4OuHTpUqp9aW171pkzZ3DhwgUsXrwY/fv312/fvn17jmPy9/fHzp07ERcXZ5RYhIWF5bhMnfLly0NEEBAQgEqVKmV6fI0aNVCjRg189NFH+PPPP9G0aVP88MMPmDx5MoCcN61kxN/fP8efB6ASlerVq2PChAmp9v34449YtmwZJk2ahJIlS8LNzQ1///13huWVL18+02N0tYTPjiy7fv16lmIGsv67omsq/fvvvxEUFJRhmf3798eMGTOwYcMGbN68GSVLlkS7du2yHBMVPWxaInqG7j/wp//jTkxMxPfff2+ukIxYW1sjKCgI69atQ3h4uH77pUuXsHnz5iydDxhfn4joh8HmRMeOHZGcnIw5c+bot6WkpGDWrFk5LlOne/fusLa2xqRJk1LVgogI7t+/DwCIiYlBcnKy0f4aNWrAysoKCQkJ+m3Ozs4mHRYOAO3atcPBgwdx8uRJ/bYHDx6kO3z6aTdv3sS+ffvQu3dv9OzZM9XtjTfewKVLl3D48GFYWVmha9eu2LBhA44ePZqqLN3706NHD5w6dQpr165N9xhdcrFv3z79vpSUFMydOzfL153V35W6desiICAAX3/9dar3/tnPtGbNmqhZsybmz5+PX3/9FX369OF8S5Qh/nQQPaNJkyYoXrw4goODMXz4cGg0Gvz8888mbUrIrYkTJ2Lbtm1o2rQphgwZgpSUFHz33XeoXr260ZdpWqpUqYLy5ctjzJgxuH37Ntzc3PDrr79mqf9Oejp37oymTZvigw8+wLVr1/D8889jzZo1ue6fAqgv3MmTJ2PcuHG4du0aunbtCldXV1y9ehVr167F4MGDMWbMGOzatQtDhw5Fr169UKlSJSQnJ+Pnn3+GtbW1UUfRevXqYceOHfjqq6/g5+eHgIAANGrUKFcxvv/++1iyZAnatm2LYcOG6YdflylTBg8ePMiwFmjZsmUQEbz88stp7u/YsSNsbGywdOlSNGrUCFOmTMG2bdvQokULDB48GFWrVkVERARWrVqF/fv3o1ixYvi///s/rF69Gr169cKbb76JevXq4cGDB1i/fj1++OEH1KpVC9WqVUPjxo0xbtw4PHjwAB4eHlixYkWqZDAjWf1dsbKywpw5c9C5c2fUrl0bb7zxBnx9fXH+/Hn8888/2Lp1q9Hx/fv3x5gxYwCkntySKJX8HiZFZA7pDb+uVq1amscfOHBAGjduLI6OjuLn5yfvv/++bN26NdWw1PSGX3/55ZepygQgEyZM0D9Pb/h1SEhIqnP9/f1TDRneuXOn1KlTR+zs7KR8+fIyf/58ee+998TBwSGdd8Hg7NmzEhQUJC4uLlKiRAkZNGiQfpj308Ncg4ODxdnZOdX5acV+//59ef3118XNzU3c3d3l9ddf1w+Jzs3wa51ff/1VXnjhBXF2dhZnZ2epUqWKhISESFhYmIiIXLlyRd58800pX768ODg4iIeHh7Rq1Up27NhhVM758+elefPm4ujoKAD072t6w687deqUKsYWLVpIixYtjLadOHFCmjVrJvb29lKqVCmZOnWqfPvttwJAIiMj073eGjVqSJkyZTJ8T1q2bCleXl6SlJQkIiLXr1+X/v37S8mSJcXe3l7KlSsnISEhkpCQoD/n/v37MnToUHnuuefEzs5OSpUqJcHBwXLv3j39MZcvX5agoCCxt7cXb29vGT9+vGzfvj3N4de5/V0REdm/f7+0bdtWXF1dxdnZWWrWrGk0rYBORESEWFtbS6VKlTJ8X4hERLjWElEh0rVrV/zzzz+4ePGiuUMhACNHjsSPP/6IuLg4s3eatiT37t2Dr68vPvnkE3z88cfmDocKOPaRIbJQzy4ncPHiRWzatAktW7Y0T0BF3LOfx/379/Hzzz/jhRdeYBKTTYsWLUJKSgpef/11c4dCFoB9ZIgsVLly5TBgwACUK1cO169fx5w5c2BnZ4f333/f3KEVSYGBgWjZsiWqVq2KO3fuYMGCBYiJiWGNQjbs2rULZ8+exWeffZbvi4aS5WLTEpGFeuONN7B7925ERkbC3t4egYGBmDJlCurWrWvu0Iqk8ePHY/Xq1bh16xY0Gg3q1q2LCRMmZDrcmAxatmypHzK/ZMkSrq1EWcJEhoiIiCwW+8gQERGRxWIiQ0RERBar0Hf21Wq1CA8Ph6ura55MTU5ERESmJyKIjY2Fn58frKzSr3cp9IlMeHg4Spcube4wiIiIKAdu3ryZ4aKvhT6R0S39fvPmTbi5uZk5GiIiIsqKmJgYlC5dWv89np5Cn8jompPc3NyYyBAREVmYzLqFsLMvERERWSwmMkRERGSxmMgQERGRxSr0fWSyKiUlBUlJSeYOgyhdtra2XHyQiOgZRT6RERFERkYiKirK3KEQZapYsWLw8fHhnEhERE8U+URGl8R4eXnBycmJXxBUIIkI4uPjcffuXQCAr6+vmSMiIioYinQik5KSok9iPD09zR0OUYYcHR0BAHfv3oWXlxebmYiIUMQ7++r6xDg5OZk5EqKs0f2ssj8XEZFSpBMZHTYnkaXgzyoRkTEmMkRERGSxmMgQAKBs2bL4+uuvs3z8nj17oNFoONqLiIjMiomMhdFoNBneJk6cmKNyjxw5gsGDB2f5+CZNmiAiIgLu7u45er2cqFKlCuzt7REZGZlvr0lERAUbExkLExERob99/fXXcHNzM9o2ZswY/bEiguTk5CyVW7JkyWx1erazs8vX+Uz279+PR48eoWfPnli8eHG+vGZG2NmWiIqk+HhAqzXelpJinlieYCJjYXx8fPQ3d3d3aDQa/fPz58/D1dUVmzdvRr169WBvb4/9+/fj8uXL6NKlC7y9veHi4oIGDRpgx44dRuU+27Sk0Wgwf/58dOvWDU5OTqhYsSLWr1+v3/9s09KiRYtQrFgxbN26FVWrVoWLiwvat2+PiIgI/TnJyckYPnw4ihUrBk9PT4wdOxbBwcHo2rVrpte9YMECvPrqq3j99dexcOHCVPtv3bqFvn37wsPDA87Ozqhfvz4OHz6s379hwwY0aNAADg4OKFGiBLp162Z0revWrTMqr1ixYli0aBEA4Nq1a9BoNFi5ciVatGgBBwcHLF26FPfv30ffvn3x3HPPwcnJCTVq1MDy5cuNytFqtZg2bRoqVKgAe3t7lClTBp999hkAoHXr1hg6dKjR8f/++y/s7Oywc+fOTN8TIqI8ExsLzJgBdO8ODB4MjB4NNGgAuLgA5coBy5YBV64AH38M+PsDf/9ttlCL9DwyqYiobNMcnJwAE9VufPDBB5g+fTrKlSuH4sWL4+bNm+jYsSM+++wz2Nvb46effkLnzp0RFhaGMmXKpFvOpEmTMG3aNHz55ZeYNWsW+vXrh+vXr8PDwyPN4+Pj4zF9+nT8/PPPsLKywmuvvYYxY8Zg6dKlAIAvvvgCS5cuRWhoKKpWrYpvvvkG69atQ6tWrTK8ntjYWKxatQqHDx9GlSpVEB0djT/++APNmjUDAMTFxaFFixZ47rnnsH79evj4+OD48ePQPvmv4ffff0e3bt3w4Ycf4qeffkJiYiI2bdqUo/d1xowZqFOnDhwcHPD48WPUq1cPY8eOhZubG37//Xe8/vrrKF++PBo2bAgAGDduHObNm4eZM2fihRdeQEREBM6fPw8AeOuttzB06FDMmDED9vb2AIAlS5bgueeeQ+vWrbMdHxFRlqWkqGSlWDHj7SLA9OnA1KnAf/+lfe7160C/fsbbfvoJmDYtT0LNlBRy0dHRAkCio6NT7Xv06JGcPXtWHj16pDbExYmojzH/b3Fx2b620NBQcXd31z/fvXu3AJB169Zlem61atVk1qxZ+uf+/v4yc+ZM/XMA8tFHH+mfx8XFCQDZvHmz0Wv9999/+lgAyKVLl/TnzJ49W7y9vfXPvb295csvv9Q/T05OljJlykiXLl0yjHXu3LlSu3Zt/fMRI0ZIcHCw/vmPP/4orq6ucv/+/TTPDwwMlH79+qVbPgBZu3at0TZ3d3cJDQ0VEZGrV68KAPn6668zjFNEpFOnTvLee++JiEhMTIzY29vLvHnz0jz20aNHUrx4cVm5cqV+W82aNWXixInplp/qZ5aI8l9iovleOzlZ5No1kXPnRE6cEPnzT5E//hCJj1f7tVqRJUtEPvpIJCpKbTt6VOTFF0WmTBFJSlLn164tYmMj8sUXIikp6riEBJF+/QzfS5Uqqf2TJomMGCGyaJHI5csikyeLuLqqY9q2FfnlF5HHj01+qRl9fz+NNTKFUP369Y2ex8XFYeLEifj9998RERGB5ORkPHr0CDdu3MiwnJo1a+ofOzs7w83NTT9FflqcnJxQvnx5/XNfX1/98dHR0bhz546+pgIArK2tUa9ePX3NSXoWLlyI1157Tf/8tddeQ4sWLTBr1iy4urri5MmTqFOnTro1RSdPnsSgQYMyfI2sePZ9TUlJwZQpU/DLL7/g9u3bSExMREJCgr6v0blz55CQkIA2bdqkWZ6Dg4O+qax37944fvw4/v77b6MmPCIyo8ePgQkTgMqVgTffVF/vw4cDCxcC338PBAdnrZz//gNmzwZcXYFhwwCrLPTqSEkBZs4EvvkG6N8fmDQJuHhRNfU8qdU1UqaMqkXZuBHQNXEvXqxinDYNSEwEtm0Dfv0VuHkT0P0tHzsW2LULqFkT2LcPOHwYsLEBZs0CBg0C0ppB/MMPgZEjgUePgBIlsvYe5CEmMk9zcgLi4sz32ibi7Oxs9HzMmDHYvn07pk+fjgoVKsDR0RE9e/ZEYmJihuXY2toaPddoNBkmHWkdLyLZjN7Y2bNncejQIfz1118YO3asfntKSgpWrFiBQYMG6afuT09m+9OKM63OvM++r19++SW++eYbfP3116hRowacnZ0xcuRI/fua2esCqnmpdu3auHXrFkJDQ9G6dWv4+/tneh5RkSUC7N+vvnjzctSkCPDGG8CKFer5lSuArS3w3Xfq+cCBgJcX0KGDeh4XB0REAPb2KqkAVCL03XfAlCmGZpqDB4FFi4DkZODMGXW8o6PavmkTEBMDVK0KHD+urhNQ5+/cCZw7p/bb2Ki+Ko6O6hYdDdy4YWjusbYGfH1VwjJ5strWqhVw4gRw7Jh6Xru2Ov6jj4CtW9UNUOX++ivw4osZvz/OzupWADCReZpGU2A+GFM6cOAABgwYoO/gGhcXh2vXruVrDO7u7vD29saRI0fQvHlzACoZOX78OGrXrp3ueQsWLEDz5s0xe/Zso+2hoaFYsGABBg0ahJo1a2L+/Pl48OBBmrUyNWvWxM6dO/HGG2+k+RolS5Y06pR88eJFxGehr9SBAwfQpUsXfW2RVqvFhQsX8PzzzwMAKlasCEdHR+zcuRNvvfVWmmXUqFED9evXx7x587Bs2TJ8p/sjSURpmzYN+OADoFs3YM2atI9JTgb++gsoWxbw81NJxscfq23TpwOBgao2YdkyoG5doE6d1GVMnKiSGCsrNUrnSSd9AOqc48eBnj1VWSdPAvfvG/a3awcEBQHffquSCQCoVEklQytXAn/8Ady5k/5on+3b1b2LCzBkCPDDD6qmBACaNQNWr1ZJlE58vHpfvvhC1ZCsWAHUqgX83/+p1/vgA/U4PBx4/31V7ldfqe+7F18E5s5VCZWXl6rxqVgxCx9EAWLyRq0CJlt9ZCxMen1kdP1WdLp16ya1a9eWEydOyMmTJ6Vz587i6uoqI0aM0B+TVh+ZjPqNpNVH5ulYRETWrl0rT/+ITZ48WTw9PWXdunVy/vx5CQkJETc3N+natWua15eYmCglS5aUOXPmpNp39uxZASB///23JCQkSKVKlaRZs2ayf/9+uXz5sqxevVr+/PNPfaxWVlbyySefyNmzZ+X06dPy+eef68vq06ePVK1aVY4fPy5HjhyR1q1bi62tbao+MidOnDCKYdSoUVK6dGk5cOCAnD17Vt566y1xc3Mz6vMzceJEKV68uCxevFguXbokBw8elPnz5xuVM3fuXLGzs5PixYtn+rNo6T+zRBl6/Fhk40aRBw/S3n/mjIidneqbodGIXL1q2JeQoPqCfPqpyHPPqWNsbER69xYpV87Q78PGRmT4cJEyZdRzOzuR5ctV35K9e0WGDlX9R3THz58v8t13hufjxqnXevHF1H0dnZ1VXE9vK11a9S1JThbZuVPE3d2wz89PxermJlK/vuqLMn++yOjRIsOGiVy5oq7twgWRjh1FxoxRr52e6OjUfVW02tx8ImaV1T4yTGQs+Eshq4nM1atXpVWrVuLo6CilS5eW7777Tlq0aJHviUxSUpIMHTpU3NzcpHjx4jJ27Fjp1auX9OnTJ83rW716tVhZWUlkZGSa+6tWrSqjRo0SEZFr165Jjx49xM3NTZycnKR+/fpy+PBh/bG//vqr1K5dW+zs7KREiRLSvXt3/b7bt2/Liy++KM7OzlKxYkXZtGlTmp19n01k7t+/L126dBEXFxfx8vKSjz76SPr372+UyKSkpMjkyZPF399fbG1tpUyZMjJlyhSjcmJjY8XJyUnefffdNK/zaZb+M0uUrps3RRo1Ul/wvr4iTwYW6CUmitSrZ5wkjBun9o0dK2Jvb7zPxSV1QvHyy8bbnJwMj6tUSZ2YTJhgeP2tW1WSoUsMYmNVR9h581QCFROjtl++rDrGNm4s8uWXIs/+rl6+LLJsmXESRmliIvNEYU5kLF1KSopUqlTJaHRUUXT16lWxsrKSY8eOZXosf2bJYsTEiNy+nbUagb17RUqWTJ1IvPKKyO+/i+zYYagBKV7cUEPi5aUSCd3xxYuLtG+vRu08fqxG9bz9tsioUaq2QqsV+fFHkWrVRCZOVMnIyJGG8x0dRQYOVKNwbt/O87eIMsZE5gkmMgXHtWvXZO7cuRIWFianT5+WwYMHi62trZw9e9bcoZlFYmKiRERESL9+/aRJkyZZOoc/s5RtcXEioaEi//6b+bHx8SKDB4tkYaoBvX//FVm7VmTaNJF799S28HARHx+VHLi7i7RpI7J/v9qn1aqhw3fvque//WaoTalVS+Tvv1WNRlrTVFhZqSQjMVHV2jy9b+LEnDejLF0qMnOmIX4qEJjIPMFEpuC4ceOGNGnSRNzc3MTV1VUCAwNl79695g7LbHTNc5UqVZLTp09n6Rz+zFK2aLUiXbuqL/oaNQzNH//9JxIRkfr4d9819D85d86w/cEDkY8/Vn06hgxRzSX//SfSvbtxMlG/vnqNtPqPACqWypUNSUnTpiLW1up5ly4iDx8aXvPoUdVPxMtL9T15913jmD75xFBuq1aqDwoVKllNZDQiuRwfW8DFxMTA3d0d0dHRcHNzM9r3+PFjXL16FQEBAXBwcDBThERZx59ZypZffgFeecXw/KWXgPbtgfHj1Uiebt3UvCZNm6r5R55augOvvQb8/LMaIfPWW2qIr069ekBUFHD5snr+/PNAZCTw4IGarv76dcDBQQ0ptrJSc5LMn284385OzWuiExys9tukMZBWq1UjSp+d+fz2baBKFTV1xYkTanQSFSoZfX8/zexrLd2+fRuvvfYaPD094ejoiBo1auDo0aP6/SKCTz75BL6+vnB0dERQUBAuXrxoxoiJiPLB4sVqmO/mzTk7/99/Ad1aXn36qMRi40a1LSZGJQi//gq0bAl4eKjEBQBeflndL1sGhIaquUaio4Hq1dUQXw8PNRfJ5csqaTl6FPjnHzUHiqOjSmIAtU5P7dpqvpd581RSM3KkmkPl3j01FHn6dDXPysKFaScxgEqE0lq+5bnngLNn1Ro/TGKKtvyoHkrPgwcPxN/fXwYMGCCHDx+WK1euyNatW42muf/888/F3d1d1q1bJ6dOnZKXX35ZAgICsly1zqYlKkz4M1tEbNmiml50w4M3bVLDbjdvVn05xo4V+fBDkVOnjM9LSlKdVGfPNozCqV5dnbtypWoycnNT+0+fFhk0yHg4cP366tiOHY2bhLp3NzTdXL0q0q6d6oj7bJ+SDRtU+a++atHDfqlgsIimpQ8++AAHDhzAH3/8keZ+EYGfnx/ee+89jBkzBoCa6t7b2xuLFi1Cnz59Mn0NNi1RYcKfWQslkvVFYU+cULUkMTGAj49qstHN/vpktXkjVauqsu/eVZOyPf0n3c1NTT9fr556fvEi4OmpalV0UlKA06eBU6dU01OJEsChQ2qiNwCoXx/Yuzfrs48nJKh4iXLJIpqW1q9fj/r166NXr17w8vJCnTp1MG/ePP3+q1evIjIyEkFBQfpt7u7uaNSoEQ4ePJhmmQkJCYiJiTG6ERGZXHKycdLwLK1WJQRvvw0ULw688IJqCgGAa9fUOjrt2qnZVX19gUaNgNKlVXNSTIyawfXSJaBrV5UcREWp43r3BkaMUDOw2tqqaevPnlXNNbqEqVo14Ouv1evokhhAzdj67OzX1tZqZtsBAwzr5jRuDISEAE2aAOvXZ28JFSYxlM/MukTBlStXMGfOHIwePRrjx4/HkSNHMHz4cNjZ2SE4OBiRkZEAAG9vb6PzvL299fueNXXqVEyaNCnPYyeiIuz779WU7wkJar2fOnWAzp1VInL+vOpDsnev8bT1Bw6o42rVAo4cMS4vPl7VvACqT0irVmqaeWdnNcX8ihVAQIBKLJ5exO/+fbUej6urml7ey0vVuKS10F92cbkMshBmbVqys7ND/fr18eeff+q3DR8+HEeOHMHBgwfx559/omnTpggPD4evr6/+mN69e0Oj0WDlypWpykxISEBCQoL+eUxMDEqXLs2mJSoU+DNrIrt3qxV8Bw9WNRHZsXYt0KNHxrUxOs7OquakVy/gxx+B339X2zUaoEULlfwEBQFJSaqTbPHiQIMGai0coiIuq01LZq2R8fX11S+wp1O1alX8+uuvAAAfHx8AwJ07d4wSmTt37qS70KC9vT3sWbVpMhMnTsS6detw8uRJc4dCZBpLl6pVjZOSVNOPh4dhpE5mjhxRo3hE1GJ+H32kRgft2qVGBMXGqiHB1aoBzZur/iW6VeFfekmN7LlxQyUwpUoZl/10ExARZZlZ+8g0bdoUYWFhRtsuXLgAf39/AEBAQAB8fHywc+dO/f6YmBgcPnwYgbqOaEWMRqPJ8DZx4sRclb1u3TqjbWPGjDF6//ParVu3YGdnh+rVq+fba1IRMnu2GmaclKSGDosAffuqlY27dgUqV1b9ULy9VR+R8+cN5969q4559Ajo0EGtbOznp5qKRo0Cdu5Uqyv/9BMwdqzqLKtLYgBVC9Opk0qAnk1iiCjHzJrIjBo1CocOHcKUKVNw6dIlLFu2DHPnzkVISAgA9cU6cuRITJ48GevXr8eZM2fQv39/+Pn5oWvXruYM3WwiIiL0t6+//hpubm5G23Sju0zFxcUFnp6eJi0zI4sWLULv3r31Cas5paSkQKvVmjUGMqHt24Hhw9Xj0aOBCxeAtm1V/5SPPgJ++01ti4xUScv336sRQa+9pjrSvvoqEB6utq1cmf68J0SUv/J+JHjGNmzYINWrVxd7e3upUqWKzJ0712i/VquVjz/+WLy9vcXe3l7atGkjYWFhWS6/MM8jk9aK0/PmzZMqVaqIvb29VK5cWWbPnq3fl5CQICEhIeLj4yP29vZGKzH7+/sLAP3N399fREQmTJggtWrV0pcRHBwsXbp0kS+//FJ8fHzEw8ND3n33XUlMTNQfEx4eLh07dhQHBwcpW7asLF26NNXq2mnRarVSrlw52bJli4wdO1YGDRqU6pj9+/dLixYtxNHRUYoVKyYvvviiPHjwQETUIpRffPGFlC9fXuzs7KR06dIyefJkEUl7ZfATJ04IALn6ZBVa3fv522+/SdWqVcXa2lquXr0qf/31lwQFBYmnp6e4ublJ8+bNUy3w+N9//8ngwYPFy8tL7O3tpVq1arJhwwaJi4sTV1dXWbVqldHxa9euFScnJ4nRTRmfRZb+M5unHj8WuXJFJCXFsC06Ws2rcvy4WlAQEHnjDcMcJ1FRamr8F18U+fxzkZ071UKD27ap7RqNYTFB3WrJ//xjhosjKnqyOo+M2f+leOmll/DSSy+lu1+j0eDTTz/Fp59+muexiKh/zszBySnr00ykZ+nSpfjkk0/w3XffoU6dOjhx4gQGDRoEZ2dnBAcH49tvv8X69evxyy+/oEyZMrh58yZu3rwJADhy5Ai8vLwQGhqK9u3bwzqDUQ+7d++Gr68vdu/ejUuXLuGVV15B7dq1MWjQIABA//79ce/ePezZswe2trYYPXo07t69m2n8u3fvRnx8PIKCgvDcc8+hSZMmmDlzJpydnQEAJ0+eRJs2bfDmm2/im2++gY2NDXbv3o2UlBQAwLhx4zBv3jzMnDkTL7zwAiIiInD+6aaBLIiPj8cXX3yB+fPnw9PTE15eXrhy5QqCg4Mxa9YsiAhmzJiBjh074uLFi3B1dYVWq0WHDh0QGxuLJUuWoHz58jh79iysra3h7OyMPn36IDQ0FD179tS/ju65q6trtuKjJ/79V9WUuLmpafVv3wa++krVphQvrvqbXLumhi8/rXFjYM4cwy+buzvwTHOqXtu2wOHDavp8XRP4vHlqOn6yCImJavUFjQbYt09NbkzGEhOBDRvUr8v9+6q7WOPG5o4qm/InrzKf7NTIxMWlvc5Zftzi4rJ/bc/WyJQvX16WLVtmdMz//vc/CQwMFBGRYcOGSevWrUWbzoybAGTt2rVG29KqkfH395fkpxZo69Wrl7zyyisiInLu3DkBIEeOHNHvv3jxogDItEbm1VdflZEjR+qf16pVS0JDQ/XP+/btK02bNk3z3JiYGLG3t5d58+aluT+rNTIA5OTJkxnGmZKSIq6urrJhwwYREdm6datYWVmlW1N4+PBhsba2lvDwcBERuXPnjtjY2MiePXsyfJ20FJkamWvX1Gy233wj8vrrIuXLi5QqpVZGFhF56620f5F0NSjPrpis0YjUratqZ7IrPl7V1qTzs0UF14YNhh+DGTOyf35ysvoR/PXX/J2oeO5ckebNRa5fN015Wq3IH3+knoj58WORtm2Nf108PbO+UHpERN6+L1mtkTH7WktkGg8fPsTly5cxcOBAuLi46G+TJ0/G5ScLuw0YMAAnT55E5cqVMXz4cGzbti1Hr1WtWjWjGhtfX199jUtYWBhsbGxQt25d/f4KFSqgePHiGZYZFRWFNWvW4DXdei8AXnvtNSxYsED/XFcjk5Zz584hISEh3f1ZZWdnh5o1axptu3PnDgYNGoSKFSvC3d0dbm5uiIuLw40bN/RxlSpVCpUqVUqzzIYNG6JatWpYvHgxAGDJkiXw9/dH8+bNcxVroRMRoYYoN24MlC0LdOyoJn77+We1rs+tW6pj7qFDgO7n4p131BpAdeqodYEePlS1KD/8AGzdqvq2pKSoyemOHcvZmjyOjqrz7ltvmfRyTUkEOHNGXX5BkJioprxp0kRNtfOse/eAO3dU3HfvAt98o5aAunfPcMy+feqjzuoEIVqtGlU/fbphfcvlyw37P/ss7YmRnxYRoZaYiotTr/v22+pHsEcP1U/7ya98pm7eBNasSfvaM3PiBPDuu+r6P/rIsD0uTl1XTiZMmT1bza9Yo4b6NQDUr8Xrr6uuY87O6lerfHlVK/N//5dxeY8fqwF5vr5AyZJAmzbpV2zmB7M3LRUkTk7qh8Vcr50bcU8CnzdvHho1amS0T5d01K1bF1evXsXmzZuxY8cO9O7dG0FBQVi9enW2Xsv26ZEYUM1/ue0Uu2zZMjx+/NgodhGBVqvFhQsXUKlSJTg6OqZ7fkb7AMDKykpfpk5SUlKa5WieaeMLDg7G/fv38c0338Df3x/29vYIDAxE4pPVezN7bQB46623MHv2bHzwwQcIDQ3FG2+8kep1iqyffwa+/FJ9E+tYW6smnAoVVKJSt676VjlzBmjdWv01f+UV1Uz0rIYN1a2A0WrVRL7h4SrXenqqmMRE9WVevryadiarEhPVW/Djj2qC3woVgC1bVDm3b6upaZo0yV6cSUnq7bfKxr+5Dx4Ae/ao3NPBQa0LqZt8fe5ctcA2oJruP/lETWqs1aqWvbg49aUKqHz199+BJUtUix6grmnwYDUwLK3+1SKqxW/KFMN6lcePq22//aaelyihkqQvvgCmTk3/Onr2BP78U+W7L7ygFg+3slKvu3mzWgPzxAk14E0nMlKtyfnff2oA23//qVH2Wq0acb92bdpxp6So9+zCBfVZ1a2rBsP1768mjQbUTAHjx6tR/W3aqHsHB5XnN2yoPttu3dQ8iE9bs0YNoPvgAzVJ9AcfqO0REWpWgP791c/giRNqYN3ataol9eBB1RS3aJH6LB8+VAnnsGHG31EzZxomqb5/X80+8NT/oPkv7yqFCoai1NnXz89PPv300yyfv2XLFgEg9+/fFxERW1tbWb16tdEx6XX2fdqIESOkRYsWImJoWjp69Kh+f1aalurWrSvvvfeenDlzxujWrFkzGTt2rIiIDBgwIN2mpUePHomjo2O6TUtnz54VAPLPUx01586dm2Zn32e5uLjITz/9pH9+48YNo+vZs2dPhk1LImqBVAcHB/nmm2/EyspKbt68me6xGbGon9nkZJHOnVVddVCQyMcfi1y4YHzMpk2G5iCNRi1aOGOGSGRk6vI2bzbUf9vbq+anbDp4UGT8eJEn/cONHD4ssmiRcV9hU1q61BD+0qWG7TExxtX7v/xifN7DhyLTpomk1RI5cGDqljQvL9Wf2dZWPf/uu6zHuGePSIkSah3Jl14SmThR5NtvRX74QeSjj0T691frR3bsKDJsmHrP1q8X8fZWr9Wtm1pz0t/fOJ64OJEjR0TKlUu7FbBBA0N/6jffVB8vIGJjYzimTRvVMjhnjkirViIjRojcuCEydKjhGDc3Q7nDh6v7gACRdevUYwcHka++St3EIqLiS6u1csECkXPnRGrVUs87dTI0p9y+LVK5ctrnWVsbricpSeTmTZHYWMNn+uy6nIBIsWLqvmRJdY2ASPv2Ir6+6XdLsLZWn5XuT9v162qdUUCtG9qypXrcpInq0/70uTY2Is+MQ5AhQ1K/RsuWhthv3RJxdlbb588XOXpU3efg1zFTWW1aYiJjKV8KaXj2i3fevHni6Ogo33zzjYSFhcnp06dl4cKFMuNJ4/CMGTNk2bJlcu7cOQkLC5OBAweKj4+PpDz5y12xYkUZMmSIRERE6EcCZTeREREJCgqSunXryuHDh+X48ePSqlUrcXR0lK+//jrN69D1VTl37lyqfd9//734+PhIUlKShIWFiZ2dnQwZMkROnTol586dk++//17+fdKgO3HiRClevLgsXrxYLl26JAcPHpT58+eLiEhiYqKULl1aevXqJRcuXJCNGzdK5cqVs5TI1KlTR9q2bStnz56VQ4cOSbNmzcTR0dEoMWvZsqVUr15dtm3bJleuXJFNmzbJ5s2bjcp59dVXxc7OTtq3b5/m+5AVFvUzO2VK2n95O3RQ39anThn+cr/xRtYa5v/v/9TxEyZkO5yNGw1fkP36Ge+7c0d9CQIio0aZpt3/8mW1QPXff4skJqpuPrq3oGtXdcy9e6rrztNf7A4OKuHSefqLpV8/Q4536JBh+9dfi5w/L1K7duq3295eLXSdnKxywU2bRMLDU1/jzz8bkp/c3Fq3Vvfe3iqJAER69jR8+ZUqpT6L+HgV16VL6vXnzjUup3NnNejsxx8N56bVBUp3mzJFlfnmm8bbx49X1xoUZPzl7+KiPvMxY9T+AQPUvl69RD77TMU+a5bh/Tl71vD+rF6tkriKFdXz0qVFli0TmTRJ5JNP1Gfx22+GBcx1SY2jo8jgwSIvvGB4/vLL6sdfN6gOUH1yjh83vo7nn1ef/ZUrIr//rl6nYUPDfn9/9X4NGpT6vbGzU/EnJqr/E4YNU5/3jRupf27/+0+kbFn1XjdoYPi9aNZMfW7du6vngYF532+IicwTRSmRERFZunSp1K5dW+zs7KR48eLSvHlzWbNmjYioGojatWuLs7OzuLm5SZs2beT48eP6c9evXy8VKlQQGxubTIdfP+3ZRCY8PFw6dOgg9vb24u/vL8uWLRMvLy/54Ycf0ryOoUOHyvPPP5/mvoiICLGyspLffvtNRFTtR5MmTcTe3l6KFSsm7dq103fgTUlJkcmTJ4u/v7/Y2toaDS8XUUO3a9SoIQ4ODtKsWTNZtWpVlhKZ48ePS/369cXBwUEqVqwoq1atSjWc/P79+/LGG2+Ip6enODg4SPXq1WXjxo1G5ezcuVMAyC/P/sudDQXyZzYxUf0bmJRk2HbihOGv/uefq2+jjh3T/iZq1Ej1OswKrVZ9S2TzL+jKlcb/3QPGNRzP9h1+MmpfRNSX45w5IkuWZP1lV6wwfAE4O4sEBxse65KLmBhDklKypEpMXnrJ8PzIEZG9ew0x6d46T09VC9KggXoeHGx43ehokb59VRKwd6/hv/5KldQX4dPXWLmyyJYt6j/td981bO/ZU8Uyfbr6UuzdW5U3ZIjI1Kki33+v+j2/+qr6ItZoVH755ZfG5U+frr4sn94WFKRGvKf30b7yijqualV1LTpnzhgSQT8/kf/9z5AMODioxEInPFwlKbrXPHNGbX/4UH2OaSV7H31kSHIPHUr/c/3oI0MC8nQCceVK2sfPm5f689Pd3N1F9u83HBsbq+JbvNiwTZc0eHqqxDgtZ8+qxANQn7cuaVq9Wv1qPfvznBUPHxo+p0OHVKzPvmdPjefIM0xknijMiYyluHnzpgCQHTt2mDsUs/rpp5/E09NTEhISclxGgfqZjYhQbQ9+foZ/+6pWVXXYurrwbt2Mv/0vXhQZO9bwreTtrerc80hCgsjIkYY/vn36GP5jrVZN5WDHjhm+ZJ5uqmnYUOS99wyXB6iqeV3O9tTAPb0LFwxfxoDxf9mAqjnR/Rc/daoh19u9W50fG2uooXFwEHnuOfX4rbfUF8ezX8KurupjSM+dO4ZmH108VaoYagp0X5K6x2PHZq9pLSbG8PFptSoJAlTzVFyceo90TTKvvaY+j4w8eqQSxrt3U++LjhbZulUdo3u9w4fVj9SzdJWBNWum3qfVquaRy5dFvvjC+P2sXz/jZDU+XqRCBXWslZWqxclsINyVK6rmIylJJZc9eqjXOXUq4/NE1Hlvvpl50rBvn3GipKv0TUpSiU5uHTumkqQGDdR7+tlnuS8zK5jIPMFEJv/t3LlTfvvtN7ly5YocOHBAmjZtKmXLljWaNK8oefjwoVy6dEmef/55GT9+fK7KMvvPrFar2gLefdfwL2xa/24CqnNEWt9IunLOnk1/vwn8+achKQBERo9WX6z376svWt0Xna7549VX1Xmffpr6ckqVUonF09tsbVVtyNWr6gs1ONjw37BGo5qVHj0y1HYEBKiKp3HjjN+ytm2N446ONu4/4eenqvtF1Pm6vh9A1oYU79unvjg//NBQTlSUakLTJTSlS4ts35779zw6Wr3P27YZtt2/L7JrV971PUpLUpLq15PZl7hWq5rrdO/nokWZl33unKqZMUWCYEq6Vtf8qi3JD0xknmAik/+2bNki1apVE0dHR/Hy8pKuXbvKtbzoCWYhJkyYIDY2NtK6dWuJ1fWYyyGz/czu3auqA56uUwdEGjdWnQMePVLf6Nu2qQb+pUvTr2/PgqdbqbLrzBnVz0EXooeHaop52pIlxrUSTk7GFUPh4ao17PXXVT+Jx49Vi1aTJqnztWeTnk6dVMva0w4fNvRtebbvQ1pNGcnJqnakTBnjpEBn2zYVV27eJxFVK/Dtt+k39xQFDx+qfj2NGhlqeyzR48eq5i6/akvyQ1YTGY2ISD4Oksp3GS0D/vjxY1y9ehUBAQFw4JSPZAHy5Gf28WNgxw6gQQO1WOKzzp5V4zx1k3NoNGoI9EcfAS1a5HhKahHjUy9eBKZNU0NAz54FBg5UQ3efPiYmBli1Si0erRtympQE/P03cOoUsHq1Gr6rC/PNN4H//U/Nd/GsixfVaO4bN4BGjdQaj1kRFaViP38emDBBzcNhbw/07q3m/8hsVlQRoGJFNdS4c2dg/fqsvS5RUZPR97eRfEmrzIg1MlSY5MnPrK5Th0ajphNdscLQUeDuXUPbywsvqA4JWezjExWl+hC0b5+6H8Hp06qvSOPGql/JmTOqJerZ2o6nRr2LiOp0CqhhvJcuiRw4YOjo+HQNSc+eWeuDYApXrqQ9nDsjK1ao2p1sLBtHVOSwRuaJrNTIlC1bNkuTmhGZ26NHj3Dt2jXT1chs26ZmaXtWw4ZAtWrAmjVIiY7FFp8BWP/id4hJdERSkproKzhY1XqcOwfs368Wh36yLBZOn1azoeqWOgoIUC9VoYKaBKxLF1W7AqiJ4ezt1cRatWsDkyapybqmTFH7TpxQ5/32G/D0ovceHqqSKCVFTaxWp46qVBo0SNV4EJFlY43MExlldMnJyXL27Fm5l9bsSEQF0L179+Ts2bNGa13l2OPHhiE0I0aoGa0mTtSPEdYC8gMGS2mb26lqSgDVSfKzzwwjbypUUPNMjBhhmJCrTBnDACU3NzUKR9c35YUXRFq0MJRXr57qGCqi+ojo9lWpIrJmjSoLEHn7bdVJV3de377GQ3WJqHBgjcwTmWV0ERERiIqKgpeXF5ycnDhtPBVIIoL4+HjcvXsXxYoVg29anT6yVpDqGHLiBLBxI2JWb8Vqt4HY1+kLeJe2Q4UKQPniD+C3ZSE+3B2ENVdqAwA8PdUU5AEBaprz6dMN08oDqibm2XV+OnUCFi9W0623a6f6sOi88oqaBt3WVi1afeGCWqWgWDHDMTdvqkWs//3XsK1sWeCff9QaNv/7n9r/6qu5XzmeiAqerNbIFPlERkQQGRmJqMxWEyMqAIoVKwYfH5/sJ9z//acWPly/HrhzB8dRB99iOH5BbzxC+gt92dqqtWlCQtQaLzr79qn1ZeLj1RpBXboAI0eqxKVBA9Us1KaNIcFISFALADo7q3Vssrp24507Kmn6/nvg0SNgwwaVIBFR4cdE5omsvhEpKSlpLiJIVFDY2toarTqeZSKqc8n69TiGunjPaib2ag0rb1epohYqjIlRI2kuXQKuXgUqVVKJSf36aRebkKBqZZ5eTO7BA6B4cdPXkNy/rxb9q1zZtOUSUcGV1e9vrn79hLW1dc6+JIgKiMePgVmz1ArADg6qmcbfHyjx12bcXl8S+60W4yd5HaLVwMZGDRceOlQNF3428UhJUSsgZ8TePvU2Dw+TXY4RT091IyJ6FhMZIgu1dCkwf75q1gkMBAYPVqOFUuuoblr17NVXgc8/B0qXTr9s5vREZCmYyBBZoFOn1GRviYlqOLNOSc8UvFXrCJLPnMe9f7W4Dn/chyf8PBNRtlcD9HtNg6ZNzRY2EZHJMZEhsjCPHgH9+qkkJjBQPT95EmjtexZL7raD765bhoMrVwaaNwc++wwoyaE9RFT4MJEhKoBSUoDdu4GVK4E//1QTxb38suq3u3KlGoLs7a0miSvx+2JEvPkhfCNuQwOo7KZ/f6BXL3YsIaJCj6OWiAqY69dVR9y//sr4uE1rHqODyx9Ax45qspb27dXiP5kt9kNEZAE4aomogIqJUZ1yRdTIn7p1ARsb9XzjRmDAADWM2c0N6NMrBS/a78XhdeHYGl4DTohHbZxEF/yG9oOOqval5GTVg3fJEs4MR0RFDhMZonyUlKS6rDw9y62/v0peduwADhxQ2+rXE6x6ZTXKfjcGuHEDPQBMs7FRbUwVKgB/XQSu3FcHt2gBLFzIJIaIiiQmMkT56KuvVBLj6AiUKaNmrr1+XS2UCKgamqEhgs8u94H9+7+ojX5+wLBhKtvx8VHbkpOB1avVcgP/939pT+pCRFQEsI8MUT65cgWoXl2NMlq8WPXHffQIWL5c5SRVqgBjxgB+u5YAr78O2Nmp0UYhISrzISIqQrhEwRNMZMjcRFSfmOHD1RpFrVoBO3em0xIUGQk8/7xaG+mzz4Dx4/M9XiKigoCdfYkKgMuXgW7dVAsQoCpZfvghnSRGRNW+/Pef6gH8f/+Xr7ESEVkiK3MHQFRYiQADB6okxsFBLcy4c6dajDFNq1cDa9aoIUwLF6qlp4mIKEOskSHKI4sXA3v3qu4t//wDBAQ8c4BWC9y9q2a2u39f1cYAqjmpVq18j5eIyBKxRoYoD9y7pzruAmpEklESIwKsXauGUvv6AtWqqeqaf/9VvYE//NAcIRMRWSTWyBCZQGKimsRON6ndlCmqkqVGDWDkyCcHbdkC/PKLmjDm5k3DyefOqXsrK9WkZGeX3+ETEVksJjJEuXTuHNC2LXD7tvF2Hx/VvGRrI8BnU4CPPjLsdHEBRowABg0C1q0DVqwA+vYFGjTI19iJiCwdh18T5UJcHNCwoXGlynPPAe+9p3IUJ0cB3n8fmD5dHTBwoFpI6YUXACcn8wVORFTAcfg1UR4TUcnKuXNq8t3jx1W/XSNTphqSmBkzgNGj8z1OIqLCjIkMURZdvgxcvQpUrKgGG330EbBtmxot/csvaSQxW7campO++84wKomIiEyGiQxRJiIigE8+Uf1wtVrjfTY2wOzZQNOmz5z099+qz4uu2oZJDBFRnuDwa6I0JCSoFqFWrdTq1PPnqySmXDk1T51Go9ZKCgsDBg9+cpKImvGuc2egZk01Q2/DhsCsWWa9FiKiwow1MkRpGDUKmDPH8DwwUCU2TZqohacTEgBn5yc7k5OBJUvU0ta6tQgA4KWXgB9/5MrURER5iIkM0TPCwoC5c9Xjzz8HevQAKlQw7LexUTcAakmB8ePVSYDKbgYMUCtEprsWARERmQoTGSKoYdQigKurmlg3JUVVqIwdm8FJCxeq4dQA4OmphlkPGgQUL54vMRMRERMZIpw+Dbz4ourS0r49sH69mg9m6tQMTkpIACZOVI/feQf44guA8xQREeU7JjJUpJ08CQQFqeUEAJXEAEBwsFr2KF2hoWqZAT8/YOZMtbw1ERHlOyYyVGQdP66WFnjwQK0MMG0asGyZWmpgypRnDhZRk8bcuaNO0h3wwQdMYoiIzIiJDBVJR4+qfCQqCmjcWK3n6O4OtGyZxsGnT6t1kfbsMd7u66v6xBARkdlwHhkqco4fV81JUVFqOPXWrSqJSdP27UD9+iqJcXAwbm8aP561MUREZsYaGSpS7t0DunYFoqPVuo2bNqmRSmkKCwN69QKSkoAOHdTEMv7+wD//qLUKOnXKz9CJiCgNTGSoyEhJAfr1U310K1YENm7MIIl58EDN0Bsdrapt1q41TGxXrZq6ERGR2bFpiYqMTz9V/XWdnNQ8duk2J924ATRvDly8CJQpY5zEEBFRgcIaGSoSNm8G/vc/9fjHHzMYWn36tGpGCg9XQ6s3bQK8vPItTiIiyh4mMlToXbsGvPaaGkE9ZIh6nKZ//gFat1aTylSrprKf0qXzM1QiIsomJjJUKMXEqJl5//4bOHHCMFfMzJnpnHD5shqPff++OnDbNqBYsfwMmYiIcoCJDBU6Impm3nXrDNtKlgRWrUqnq8utW2o8dkQEUKOGmlSGSQwRkUVgIkOFzqxZKomxswO+/BIICACaNgU8PNI4+O5dlcRcu6aGMm3bls6BRERUEDGRoULl6FFgzBj1ePp0YNiwDA6OjQXatVPzxZQuDezYAfj45EucRERkGhx+TYVGdDTwyitq/rru3YGhQzM5Yfx4tWqktzewc6caak1ERBaFiQwVCiLAW28BV64AZcsCCxYAGk0GJxw8CMyerR4vW6aalYiIyOKwaYksnlar+sKsXg3Y2AArVqTTV1cEuH5dPR48WD0fMEANuSYiIovERIYs2rZtwPvvA6dOqedffAE0apTGgdHRaqK7gwcN20qUUB1piIjIYpm1aWnixInQaDRGtypVquj3P378GCEhIfD09ISLiwt69OiBO3fumDFiKki2bFF9dU+dAtzcgM8/B0aNSuPAuDigY0eVxFhbqxWrXVzUFL+envkeNxERmY7Za2SqVauGHTt26J/b2BhCGjVqFH7//XesWrUK7u7uGDp0KLp3744DBw6YI1QqQO7cUXPFAEDfvsB336UzajomBujSBfjzT9XetHs3ULt2PkZKRER5yeyJjI2NDXzSGPIaHR2NBQsWYNmyZWj9pA9DaGgoqlatikOHDqFx48b5HSoVEFqt6tpy965aM2nBAsDRMY0Db98GOnVSVTaursDWrUxiiIgKGbOPWrp48SL8/PxQrlw59OvXDzdu3AAAHDt2DElJSQgKCtIfW6VKFZQpUwYHn+7n8IyEhATExMQY3ajwuHVLVbBs2aJaiJYvTyeJuXkTaNxYJTFeXsCuXUDDhvkeLxER5S2zJjKNGjXCokWLsGXLFsyZMwdXr15Fs2bNEBsbi8jISNjZ2aHYM8NPvL29ERkZmW6ZU6dOhbu7u/5Wmov+FRrLlqm1HDduBGxtM1nFetIklfVUqgQcOgTUr5+vsRIRUf4wa9NShw4d9I9r1qyJRo0awd/fH7/88gsc0/w3O3Pjxo3D6NGj9c9jYmKYzFg4rVbNXffFF+p548bA/PkqqUnTtWvA4sXq8aJFao0CIiIqlMzetPS0YsWKoVKlSrh06RJ8fHyQmJiIqKgoo2Pu3LmTZp8aHXt7e7i5uRndyHIlJgK9ehmSmHHjgP37M0hiADV8KTlZraEUGJgvcRIRkXkUqEQmLi4Oly9fhq+vL+rVqwdbW1vs3LlTvz8sLAw3btxAIL+cioSkJKBPH2DNGrVq9c8/A1OmqBHUaYqPB06fBhYuVM8//jjfYiUiIvMwa9PSmDFj0LlzZ/j7+yM8PBwTJkyAtbU1+vbtC3d3dwwcOBCjR4+Gh4cH3NzcMGzYMAQGBnLEUhFw7x7w9tvA2rUqiVm/HnjxxXQOjolRGc/mzYZtLVoAzZvnS6xERGQ+Zk1kbt26hb59++L+/fsoWbIkXnjhBRw6dAglS5YEAMycORNWVlbo0aMHEhIS0K5dO3z//ffmDJnyWHw8MHEi8P33wMOHgJ2dqpFJN4l58EDN2PvXX+q5jQ1QqhQwbVp+hUxERGakERExdxB5KSYmBu7u7oiOjmZ/GQvw5ptAaKh6XK8eMGOGqlxJ0927QNu2qjnJw0PVyDRokMlqkUREZAmy+v1t9gnxiHT27DEkMStXqk6+6eYkt28DbdoAYWGAtzewY0cGY7GJiKiwYiJDBUJCAvDOO+rxO+8AvXtncPCtW6r/y9WrQOnSwM6dQMWK+RInEREVLAVq1BIVTXFxwNChhsqVqVMzODglBXjtNZXElC8P/PEHkxgioiKMiQyZTVKSmreucmU1wR0AfPutWtsxXdOnA3v3qtWrt24F/P3zI1QiIiqgmMhQvtNq1aikChXU4o/h4Wry3XXrMmlSOn7cMDfMt9+qGhkiIirS2EeG8t3//qeGWANqPcfRo4ERI9QikOm6eBF46SVVjdO9u8qAiIioyGMiQ/lq40ZDEjNlCjBqVCYJDABcuQK0bg1ERKiRSfPmcYg1EREBYCJD+ejiRdVPFwBCQtS6SZk6dw5o106NVKpaVY1Q8vDI0ziJiMhysI8M5YsbN9TcddHRQNOmwFdfZeGkQ4eAF14Abt5UPYJ37lRtUURERE8wkaE8I6KWQTp9Wi1Eff06UKkS8OuvaumBNJ06BTRqBBQvrlaufvBAzda7fz/g65uv8RMRUcHHpiXKEzExagmkP/80bPP3VxPwentncOKHHxrWTQKArl3VstcuLnkVKhERWTAmMpQnRo82JDEuLkDNmsBPP6mJeNN18SLw++/q8e7dQP36TGCIiChDTGTI5H7/HViwQA0s2r07g0Ufn/Xdd+q+Y0egZcu8Co+IiAoR9pEhk4qIAAYNUo9HjsxGEhMTY1gxcsSIvAiNiIgKIdbIkMkcOgT06KGSmcqVgc8+y8JJc+eqE8PDgdhYNcS6bds8j5WIiAoHJjJkEqtXA/36AYmJKhdZvx5wdMzkpE2bgLffNt42bBgnuyMioixjIkO59uefaqK7xEQ1yOinnwBX10xOio42JDFdugB16qgh17p2KSIioixgIkO5cuWKykMSEtT96tWAtXUmJyUkAO+9p2brLV8eWLYMcHLKl3iJiKhwYWdfyrFLl1R3lnv3gHr1gKVLM0litmwBKlZUbU4LFqhtCxYwiSEiohxjjQzlyJEjQKdOwL//AuXKqT4xzs4ZnHDuHNCrFxAXp567uAAff5yNYU1ERESpMZGhbAsLU4tRx8UBdeuqPrsZztYbHa06z8TFqflhVqxQayaxUy8REeUSExnKluRkoH9/lZO88IJKYjLs2BsbC/TsCVy4AJQqBaxcyYUfiYjIZJjIULZMnaqWQnJ3V310M0xiIiJU+9OJE6ofzJo1TGKIiMikmMhQlv3+O/Dpp+rx7NmZrJsUHQ00bQpcvQqULKlObtAgX+IkIqKig6OWKFMpKapf7ksvqaalV14BXn01k5M++kglMf7+wMGDTGKIiChPMJGhdImoPjD16wOTJ6tt774LLF6cST/dI0dUlQ0AzJ+v5oohIiLKA2xaIiOJiWpQ0e7dasbeCxfUdldXYM4ctQxBhpKT1Yy9IurgoKA8j5mIiIouJjIEQOUdP/8MTJgAXLtm2O7gAAwdCnzwAeDpmUkhWi0weLDq3Fu8OPDVV3kZMhERERMZUiZNUjcA8PEB3nwTCAwEmjQBPDyyUIBWC7z1FhAaClhZqSYljlAiIqI8xkSGcOSIoQ/MhAnA++/nYNWAkSMNScyyZUD37qYOk4iIKBUmMkXc48dAcLAamfTKK8DEiTkoZNEiYNYs9XjJElUQERFRPuCopSIsJkY1IZ07p5YY0A00ypYjR4B33lGPJ0wA+vY1aYxEREQZYY1MEbVtm+rScvOmGko9b14WOvM+684d1YSUkAC8/DLwySd5EisREVF6WCNTxERHqwSmXTuVxAQEADt3Ap07Z7OgpCSgd2/g1i2gcmXgp59U/xgiIqJ8xG+eImTvXqB6dWDBAvV8+HDgzBmgVascFPbee8C+fWqCmXXr1OJLRERE+YyJTBGQlKTWSGrdWlWgVKigcpBvvgGcnbNZmAgwdqxx594qVUweMxERUVawj0whdveuSlYWLgQiI9W2AQOA777LQQIDqKFN77yj5ogBgJkzVd8YIiIiM2EiU0g9egS0bKlGJAFqbrrp04HXX89FoZ9/rpIYKyvVO/jNN00RKhERUY4xkSmkJk5USYyPj2oFevllwM4uFwWGhan2KYBJDBERFRhMZAqhw4dV7QsAzJ2bgxFJz9Jq1UKQiYlA+/bAG2/kOkYiIiJTYCJTSFy6BHTtCty7pya602qB117LZRKzfLm6/fsvcOiQWrdgzhw18QwREVEBwESmkBg1CvjnH8PzUqVUR98c27EDePVV422ffQaULZuLQomIiEyLiUwhsGsXsHEjYGMDbN6slhvw9wfc3HJYYFSUofmod2+gWzfV2aZFC1OFTEREZBJMZCxcSoqamw4AhgwBgoJyWaAIMGyYYcKZhQtzOFabiIgo73FCPAs3Zw5w8qSaWDfXSx3dugW89JKa5M7KSi07wCSGiIgKMNbIWLCFC9UyA4BaeLpEiVwU9uefQIcOqqewnZ3qYBMYaJI4iYiI8goTGQu1cCEwcKB6/O67wMiRuSgsIUH1iYmJARo2BEJDgeefN0WYREREeSrbTUtly5bFp59+ihs3buRFPJQFR4+qaV0AVSPz3Xe5HBH9+efAhQuqQ++2bUxiiIjIYmQ7kRk5ciTWrFmDcuXKoW3btlixYgUSEhLyIjZKQ1wc0LcvkJwMdO8OfP11LpOYsDBgyhT1+JtvuIo1ERFZlBwlMidPnsRff/2FqlWrYtiwYfD19cXQoUNx/PjxvIiRntANKLp0Sc0TM29eLpMYXYG6GXt79TJZrERERPkhx6OW6tati2+//Rbh4eGYMGEC5s+fjwYNGqB27dpYuHAhRMSUcRZ5IsCHHwKLFqnkZelSwMMjl4Vu3Ahs3646986ezRl7iYjI4uS4s29SUhLWrl2L0NBQbN++HY0bN8bAgQNx69YtjB8/Hjt27MCyZctMGWuRJQJ88AEwbZp6/vXXQPPmuSw0MdEwAc3o0UC5crkskIiIKP9lO5E5fvw4QkNDsXz5clhZWaF///6YOXMmqlSpoj+mW7duaNCggUkDLaoePQLeeUdN6QKolayHDjVBwbNnAxcvqmmAx483QYFERET5L9uJTIMGDdC2bVvMmTMHXbt2ha2tbapjAgIC0KdPH5MEWJTduqUWgjx2DLC2VrmHbrRSrty+DUyapB5PmQK4upqgUCIiovynkWx2Zrl+/Tr8/f3zKh6Ti4mJgbu7O6Kjo+GW48WH8l9yMtCkCXDkiJro7pdfgFatTFCwCPDyy6p/TMOGaiI8a2sTFExERGQ6Wf3+znZn37t37+Lw4cOpth8+fBhHjx7NbnGUjm++UUmMuztw+LCJkhhA9RLeuFF18F24kEkMERFZtGwnMiEhIbh582aq7bdv30ZISIhJgirqLl0CPvpIPZ4xwwT9cGNj1ax5b7wB6D6jTz4BqlXLZcFERETmle0+MmfPnkXdunVTba9Tpw7Onj1rkqCKsqgo4LXXgMePgdatgTffNEGhH36oegnrNGwIvP++CQomIiIyr2zXyNjb2+POnTuptkdERMDGhks35ca1a6pfzOHDqv9trie8A9RUwIsWqcfDhwNr1gB79wJpdNImIiKyNNlOZF588UWMGzcO0dHR+m1RUVEYP3482rZtm+NAPv/8c2g0Gox8avXDx48fIyQkBJ6ennBxcUGPHj3STKIKg3v31GLT584Bzz0H7Ntnoqldli9XTUsVKgAzZwLdugEODiYomIiIyPyynchMnz4dN2/ehL+/P1q1aoVWrVohICAAkZGRmDFjRo6COHLkCH788UfUrFnTaPuoUaOwYcMGrFq1Cnv37kV4eDi6d++eo9co6ObOBSIjgYoVgUOHgNq1TVCoCDBnjnr8zjuAVY4nciYiIiqQsj38GgAePnyIpUuX4tSpU3B0dETNmjXRt2/fNOeUyUxcXBzq1q2L77//HpMnT0bt2rXx9ddfIzo6GiVLlsSyZcvQs2dPAMD58+dRtWpVHDx4EI0bN85S+ZYw/Do5WdW+3LwJLF4M9O9vooIPHwYaNwbs7dXcMZ6eJiqYiIgob2X1+ztHnVqcnZ0xePDgHAf3tJCQEHTq1AlBQUGYPHmyfvuxY8eQlJSEoKAg/bYqVaqgTJkyGSYyCQkJRqtxx8TEmCTOvPT77yqJ8fQEevc2YcG6Dr69ezOJISKiQinHvXPPnj2LGzduIDEx0Wj7yy+/nOUyVqxYgePHj+PIkSOp9kVGRsLOzg7FihUz2u7t7Y3IyMh0y5w6dSom6WattRCzZ6v7gQNN2H1l/Xo1ZwxgojUNiIiICp5sJzJXrlxBt27dcObMGWg0Gv0q15onw2tSUlKyVM7NmzcxYsQIbN++HQ4m7Hw6btw4jB49Wv88JiYGpUuXNln5pnbhglqAWqNR3VhM4vZtNWcMAIwapYZbExERFULZ7v05YsQIBAQE4O7du3BycsI///yDffv2oX79+tizZ0+Wyzl27Bju3r2LunXrwsbGBjY2Nti7dy++/fZb2NjYwNvbG4mJiYiKijI6786dO/Dx8Um3XHt7e7i5uRndCrJPPlH3nToBAQEmKFAEeP114MEDoG5dYOpUExRKRERUMGW7RubgwYPYtWsXSpQoASsrK1hZWeGFF17A1KlTMXz4cJw4cSJL5bRp0wZnzpwx2vbGG2+gSpUqGDt2LEqXLg1bW1vs3LkTPXr0AACEhYXhxo0bCAwMzG7YBdLmzcDKlWow0aefmqjQPXuA3bsBR0dgxQrV0ZeIiKiQynYik5KSAtcnqyWXKFEC4eHhqFy5Mvz9/REWFpblclxdXVG9enWjbc7OzvD09NRvHzhwIEaPHg0PDw+4ublh2LBhCAwMzPKIpYIsPh549131eORIoE4dExX8/ffqPjhYjeUmIiIqxLKdyFSvXh2nTp1CQEAAGjVqhGnTpsHOzg5z585FOZPM4GYwc+ZMWFlZoUePHkhISEC7du3wve6L2sJNnapm8i1dGjBZ3+TwcGDtWvV4yBATFUpERFRwZXsema1bt+Lhw4fo3r07Ll26hJdeegkXLlyAp6cnVq5cidatW+dVrDlSEOeRiY9Xs/dGRQG//gqYbI6/SZOAiROBF14A/vjDRIUSERHlvzybR6Zdu3b6xxUqVMD58+fx4MEDFC9eXD9yiTK2YoVKYsqVA7p2NVGhSUlqemDA0GZFRERUyGVr1FJSUhJsbGzw999/G2338PBgEpMNulUD3n7bhKsGzJihmpa8vExYxUNERFSwZetr1NbWFmXKlMnyXDGU2pEjwNGjgJ2dYaqXXFu5Ehg3Tj2eNIkjlYiIqMjIdn3Ahx9+iPHjx+PBgwd5EU+hp6uN6dULKFnSBAUeOmRYnGnECBPOqkdERFTwZbuzb506dXDp0iUkJSXB398fzs7ORvuPHz9u0gBzqyB19o2PB0qUAB49AvbvB5o2NUGh7doB27apzjarVwPW1iYolIiIyLzyrLNvV5P1Ti16du1SSYy/P9CkiQkKvHMH2LFDPZ4+nUkMEREVOdlOZCZMmJAXcRQJGzeq+06d1NpKufbLL4BWCzRqBJQvb4ICiYiILIupxsxQJkSA339Xj196yUSFLlum7vv2NVGBREREliXbNTJWVlYZDrXmiKa0nT4N3LoFODkBrVqZoMArV1RHXysroHdvExRIRERkebKdyKzVTYH/RFJSEk6cOIHFixdjksnm2i98dLUxbdoADg4mKHD5cnXfujXg62uCAomIiCxPthOZLl26pNrWs2dPVKtWDStXrsTAgQNNElhho+sfY5Jmpfh4YMEC9fjVV01QIBERkWUyWR+Zxo0bY+fOnaYqrlC5d0+1AgFAx44mKHDcOODqVcDPD+jZ0wQFEhERWSaTJDKPHj3Ct99+i+eee84UxRU6a9aozr61awOlSuWysD17gG+/VY8XLABcXXNZIBERkeXKdtPSs4tDighiY2Ph5OSEJUuWmDS4wuLnn9V9v365LOjhQ8O6Bm+9BbRvn8sCiYiILFu2E5mZM2caJTJWVlYoWbIkGjVqhOLFi5s0uMLgyhU1i6+VlQm6s0ydCly7BpQpoxaJJCIiKuKyncgMGDAgD8IovHSVVG3aqC4tOXbpEvDll+rxN98AZl5ugYiIqCDIdh+Z0NBQrFq1KtX2VatWYfHixSYJqrAQAX76ST3WreuY44JGjAASE9XaSmmMHCMiIiqKsp3ITJ06FSVKlEi13cvLC1OmTDFJUIXFoUPA5cuAszPQrVsuCtq6Fdi0CbC1VR19TbK+ARERkeXLdiJz48YNBAQEpNru7++PGzdumCSowkJXcdW9u0pmcuzzz9X90KFApUq5jouIiKiwyHYi4+XlhdOnT6fafurUKXh6epokqMJizx51n6u5Y44eBfbuBWxsgNGjTREWERFRoZHtRKZv374YPnw4du/ejZSUFKSkpGDXrl0YMWIE+vTpkxcxWqSoKODkSfW4RYtcFKQbndSnjwkmoSEiIipcsj1q6X//+x+uXbuGNm3awMZGna7VatG/f3/2kXnKH3+oPrqVKuViKaQbNwztU++9Z7LYiIiICotsJzJ2dnZYuXIlJk+ejJMnT8LR0RE1atSAv79/XsRnsfbuVfe5qo355hsgJUUtDFm7tinCIiIiKlSyncjoVKxYERUrVjRlLIWKrn9MjhOZuDjDwpDsG0NERJSmbPeR6dGjB7744otU26dNm4ZevXqZJChLFx0NnDihHuc4kVmyRBVUoQLQoYPJYiMiIipMsp3I7Nu3Dx3TGIbToUMH7Nu3zyRBWboDBwCtFihfPof9c0WA775Tj0NC1PoGRERElEq2vyHj4uJgZ2eXarutrS1iYmJMEpSly3Wz0u7dwD//qMlndItEEhERUSrZTmRq1KiBlStXptq+YsUKPP/88yYJytLt36/uc5zIzJql7vv3B9zdTRITERFRYZTtzr4ff/wxunfvjsuXL6N169YAgJ07d2LZsmVYvXq1yQO0NCKqMgUA6tbNQQH37gEbNqjHISEmi4uIiKgwynYi07lzZ6xbtw5TpkzB6tWr4ejoiFq1amHXrl3w8PDIixgtSmQkEBOjurXkaFDXmjVqyHWdOkC1aiaPj4iIqDDJ0fDrTp06oVOnTgCAmJgYLF++HGPGjMGxY8eQkpJi0gAtzfnz6r5cOcDePgcF6JrtXnnFZDEREREVVjkeDrNv3z4EBwfDz88PM2bMQOvWrXHo0CFTxmaRzp1T91Wq5ODkO3cMPYU5lJ2IiChT2aqRiYyMxKJFi7BgwQLExMSgd+/eSEhIwLp169jR9wldjUyOEplff1Xjths0UFU6RERElKEs18h07twZlStXxunTp/H1118jPDwcs3Sja0hPl8hUrZqDk3/5Rd2zWYmIiChLslwjs3nzZgwfPhxDhgzh0gQZyHHTUkQEoJtQkM1KREREWZLlGpn9+/cjNjYW9erVQ6NGjfDdd9/h3r17eRmbxYmNBW7dUo+znchs2KDGbjdsCJQpY/LYiIiICqMsJzKNGzfGvHnzEBERgbfffhsrVqyAn58ftFottm/fjtjY2LyM0yJcuKDuvbyAbI9E37hR3b/8skljIiIiKsyyPWrJ2dkZb775Jvbv348zZ87gvffew+effw4vLy+8XMS/hHPc0ffRI2DHDvW4c2eTxkRERFSY5Wo1wsqVK2PatGm4desWli9fbqqYLJauf0y2O/ru3KmSmTJlgBo1TB4XERFRYWWSZZWtra3RtWtXrF+/3hTFWawc18jompVeegnQaEwaExERUWFmkkSGlBwNvRYxJDJsViIiIsoWJjImkpwMXLyoHmerRubECeD2bcDZGWjZMi9CIyIiKrSYyJjI7dtAYiJgZweULp2NEzdtUvdt2wIODnkSGxERUWHFRMZEbt5U96VKqZWvs2z7dnXfvr3JYyIiIirsmMiYiC6RyVZtTFwccPCgehwUZPKYiIiICjsmMiaSo0Tmjz+ApCSgbFkuEklERJQDTGRMJEeJjG4SvKAgDrsmIiLKASYyJnLjhrrPcSJDRERE2cZExkSyXSNz5w5w+rR63Lp1nsRERERU2DGRMRFdIpPlhat37VL3tWsDJUvmRUhERESFHhMZE3j0CLh3Tz3Oco2Mbtg1m5WIiIhyjImMCdy6pe6dnYFixbJwQkoK8Pvv6vGLL+ZVWERERIUeExkTeLqjb5YGH+3fD9y9CxQvzmUJiIiIcoGJjAlku6Pvr7+q+5dfBmxt8yQmIiKiooCJjAlkK5HRaoE1a9Tjnj3zLCYiIqKigImMCWQrkfnrL7XCpKurWiiSiIiIcoyJjAlkK5HRNSu99BJgb59nMRERERUFTGRMIMtzyIgYmpV69MjTmIiIiIoCsyYyc+bMQc2aNeHm5gY3NzcEBgZi8+bN+v2PHz9GSEgIPD094eLigh49euDOnTtmjDhtWV6e4PJl4MoVwM4OaN8+z+MiIiIq7MyayJQqVQqff/45jh07hqNHj6J169bo0qUL/vnnHwDAqFGjsGHDBqxatQp79+5FeHg4unfvbs6QU4mOBmJj1eNMExndbL6NG6tJZ4iIiChXbMz54p07dzZ6/tlnn2HOnDk4dOgQSpUqhQULFmDZsmVo/WQtotDQUFStWhWHDh1C48aNzRFyKrpmpeLFs5Cb7N6t7rm2EhERkUkUmD4yKSkpWLFiBR4+fIjAwEAcO3YMSUlJCHpqCv8qVaqgTJkyOHjwoBkjNZbljr4ihkSmVas8jYmIiKioMGuNDACcOXMGgYGBePz4MVxcXLB27Vo8//zzOHnyJOzs7FDsmTn/vb29ERkZmW55CQkJSEhI0D+PiYnJq9ABAPfvq3svr0wOPHdOrXjt6Ag0apSnMRERERUVZq+RqVy5Mk6ePInDhw9jyJAhCA4OxtmzZ3Nc3tSpU+Hu7q6/lc7ydLs5o8uT3N0zOVBXG9O0KYddExERmYjZExk7OztUqFAB9erVw9SpU1GrVi1888038PHxQWJiIqKiooyOv3PnDnx8fNItb9y4cYiOjtbfburafvKILpFxc8vkQF1HXzYrERERmYzZE5lnabVaJCQkoF69erC1tcXOnTv1+8LCwnDjxg0EBgame769vb1+OLfulpeylMhotcCePeoxO/oSERGZjFn7yIwbNw4dOnRAmTJlEBsbi2XLlmHPnj3YunUr3N3dMXDgQIwePRoeHh5wc3PDsGHDEBgYWGBGLAFZTGTOnAEePABcXIB69fIlLiIioqLArInM3bt30b9/f0RERMDd3R01a9bE1q1b0fbJGkQzZ86ElZUVevTogYSEBLRr1w7ff/+9OUNOJUuJzMmT6r5BA652TUREZEJmTWQWLFiQ4X4HBwfMnj0bs2fPzqeIsi9LiYyu83K1ankeDxERUVFS4PrIWJpsJTLPP5/n8RARERUlTGRyiYkMERGR+TCRyaVME5n4eODqVfWYiQwREZFJMZHJpUwTmbAwtTxBiRJAyZL5FhcREVFRwEQmlzJNZJ6s5M3aGCIiItNjIpMLSUnAo0fqcbqJDPvHEBER5RkmMrkQG2t47OqazkEcek1ERJRnmMjkgq5ZydExg3nuWCNDRESUZ5jI5EKm/WMePwYuX1aPmcgQERGZHBOZXMg0kblwQS0YWbw44O2db3EREREVFUxkciHTRObpZiWNJl9iIiIiKkqYyORCponM33+r+6pV8yUeIiKiooaJTC5kmsgcOaLu69bNl3iIiIiKGiYyuZBhIqPVAocPq8eNG+dbTEREREUJE5lcyDCRuXABiI5WY7Nr1MjXuIiIiIoKJjK5oEtk0pwM79AhdV+/PmBjk28xERERFSVMZHJBN7NvmjUyumalRo3yLR4iIqKiholMLmTYtKSrkWH/GCIiojzDRCYX0k1kHj4EzpxRj1kjQ0RElGeYyORCuonMsWNASgrg5weUKpXvcRERERUVTGRyId1EhsOuiYiI8gUTmVzINJFhsxIREVGeYiKTC+kmMqdOqfv69fM1HiIioqKGiUwOabXpDL9OSgKuXlWPK1fO97iIiIiKEiYyOfTwISCiHhslMlevqo6+Tk6qsy8RERHlGSYyOaRrVrKxARwcntpx4YK6r1gR0GjyPS4iIqKihIlMDj3dP8YoX9ElMpUq5XtMRERERQ0TmRxKt6PvxYvqnokMERFRnmMik0PpJjJPNy0RERFRnmIik0OskSEiIjI/JjI5lGYiEx8P3LypHrNGhoiIKM8xkcmhNBOZS5fUffHigKdnvsdERERU1DCRyaE0E5mnm5U49JqIiCjPMZHJoTQTGXb0JSIiyldMZHIo0xoZIiIiynM25g7AUrVrp5KYJk2e2sgaGSIionzFRCaHundXNyOc1ZeIiChfsWnJVGJigH//VY8rVDBvLEREREUEExlTiYhQ925uacySR0RERHmBiYypREaqex8f88ZBRERUhDCRMRUmMkRERPmOiYypMJEhIiLKd0xkTIWJDBERUb5jImMqTGSIiIjyHRMZU9GNWvL1NW8cRERERQgTGVNhjQwREVG+YyJjKkxkiIiI8h0TGVNISTHM6stEhoiIKN8wkTGFf/8FtFrAygooWdLc0RARERUZTGRMQdesVLIkYG1t3liIiIiKECYypsD+MURERGbBRMYUmMgQERGZBRMZU2AiQ0REZBZMZEyBiQwREZFZMJExBV0iw1l9iYiI8hUTGVNgjQwREZFZMJExBd06S0xkiIiI8hUTGVNgjQwREZFZMJHJrfh4ICZGPWYiQ0RElK/MmshMnToVDRo0gKurK7y8vNC1a1eEhYUZHfP48WOEhITA09MTLi4u6NGjB+7cuWOmiNOgi8XBAXBzM28sRERERYxZE5m9e/ciJCQEhw4dwvbt25GUlIQXX3wRDx8+1B8zatQobNiwAatWrcLevXsRHh6O7t27mzHqZzzdrKTRmDcWIiKiIsbGnC++ZcsWo+eLFi2Cl5cXjh07hubNmyM6OhoLFizAsmXL0Lp1awBAaGgoqlatikOHDqFx48bmCNuYrkbG29u8cRARERVBBaqPTHR0NADAw8MDAHDs2DEkJSUhKChIf0yVKlVQpkwZHDx40CwxphIVpe6LFzdrGEREREWRWWtknqbVajFy5Eg0bdoU1atXBwBERkbCzs4OxYoVMzrW29sbkbomnWckJCQgISFB/zxG1xE3r8TGqnv2jyEiIsp3BaZGJiQkBH///TdWrFiRq3KmTp0Kd3d3/a106dImijAdukTJ1TVvX4eIiIhSKRCJzNChQ7Fx40bs3r0bpUqV0m/38fFBYmIionTNN0/cuXMHPukMdR43bhyio6P1t5s3b+Zl6IYaGSYyRERE+c6siYyIYOjQoVi7di127dqFgIAAo/316tWDra0tdu7cqd8WFhaGGzduIDAwMM0y7e3t4ebmZnTLU7oaGTYtERER5Tuz9pEJCQnBsmXL8Ntvv8HV1VXf78Xd3R2Ojo5wd3fHwIEDMXr0aHh4eMDNzQ3Dhg1DYGBgwRixBLBGhoiIyIzMmsjMmTMHANCyZUuj7aGhoRgwYAAAYObMmbCyskKPHj2QkJCAdu3a4fvvv8/nSDPAzr5ERERmY9ZERkQyPcbBwQGzZ8/G7Nmz8yGiHGBnXyIiIrMpEJ19LRprZIiIiMyGiUxusUaGiIjIbJjI5BY7+xIREZkNE5nc4vBrIiIis2EikxtaLaBbqZs1MkRERPmOiUxuxMUZHrNGhoiIKN8xkckNXbOSjQ1gb2/eWIiIiIogJjK58XRHX43GvLEQEREVQUxkcoMdfYmIiMyKiUxucOg1ERGRWTGRyQ3WyBAREZkVE5ncYI0MERGRWTGRyQ2us0RERGRWTGRyg+ssERERmRUTmdxg0xIREZFZMZHJDXb2JSIiMismMrnBGhkiIiKzYiKTG6yRISIiMismMrnBGhkiIiKzYiKTGxx+TUREZFZMZHKDw6+JiIjMiolMbrBpiYiIyKyYyOQGO/sSERGZFROZnNJqgbg49Zg1MkRERGbBRCandEkMwBoZIiIiM2Eik1O6/jHW1oCDg3ljISIiKqKYyOTU0x19NRrzxkJERFREMZHJKXb0JSIiMjsmMjnFoddERERmx0Qmp1gjQ0REZHZMZHKKNTJERERmx0Qmp1gjQ0REZHZMZHKKNTJERERmx0Qmp5jIEBERmR0TmZxi0xIREZHZMZHJDQcH1sgQERGZkUZExNxB5KWYmBi4u7sjOjoabnlReyLCmX2JiIhMLKvf36yRyS0mMURERGbDRIaIiIgsFhMZIiIislhMZIiIiMhiMZEhIiIii8VEhoiIiCwWExkiIiKyWExkiIiIyGIxkSEiIiKLxUSGiIiILBYTGSIiIrJYTGSIiIjIYjGRISIiIovFRIaIiIgslo25A8hrIgJALQdORERElkH3va37Hk9PoU9kYmNjAQClS5c2cyRERESUXbGxsXB3d093v0YyS3UsnFarRXh4OFxdXaHRaHJdXkxMDEqXLo2bN2/Czc3NBBEWPIX9Ggv79QG8xsKgsF8fwGssDPLy+kQEsbGx8PPzg5VV+j1hCn2NjJWVFUqVKmXyct3c3ArlD+XTCvs1FvbrA3iNhUFhvz6A11gY5NX1ZVQTo8POvkRERGSxmMgQERGRxWIik0329vaYMGEC7O3tzR1Knins11jYrw/gNRYGhf36AF5jYVAQrq/Qd/YlIiKiwos1MkRERGSxmMgQERGRxWIiQ0RERBaLiQwRERFZLCYy2TR79myULVsWDg4OaNSoEf766y9zh5QjU6dORYMGDeDq6govLy907doVYWFhRse0bNkSGo3G6PbOO++YKeLsmzhxYqr4q1Spot//+PFjhISEwNPTEy4uLujRowfu3Lljxoizp2zZsqmuT6PRICQkBIBlfn779u1D586d4efnB41Gg3Xr1hntFxF88skn8PX1haOjI4KCgnDx4kWjYx48eIB+/frBzc0NxYoVw8CBAxEXF5ePV5GxjK4xKSkJY8eORY0aNeDs7Aw/Pz/0798f4eHhRmWk9dl//vnn+XwlacvsMxwwYECq2Nu3b290jCV/hgDS/L3UaDT48ssv9ccU5M8wK98PWfn7eePGDXTq1AlOTk7w8vLC//3f/yE5Odnk8TKRyYaVK1di9OjRmDBhAo4fP45atWqhXbt2uHv3rrlDy7a9e/ciJCQEhw4dwvbt25GUlIQXX3wRDx8+NDpu0KBBiIiI0N+mTZtmpohzplq1akbx79+/X79v1KhR2LBhA1atWoW9e/ciPDwc3bt3N2O02XPkyBGja9u+fTsAoFevXvpjLO3ze/jwIWrVqoXZs2enuX/atGn49ttv8cMPP+Dw4cNwdnZGu3bt8PjxY/0x/fr1wz///IPt27dj48aN2LdvHwYPHpxfl5CpjK4xPj4ex48fx8cff4zjx49jzZo1CAsLw8svv5zq2E8//dTosx02bFh+hJ+pzD5DAGjfvr1R7MuXLzfab8mfIQCja4uIiMDChQuh0WjQo0cPo+MK6meYle+HzP5+pqSkoFOnTkhMTMSff/6JxYsXY9GiRfjkk09MH7BQljVs2FBCQkL0z1NSUsTPz0+mTp1qxqhM4+7duwJA9u7dq9/WokULGTFihPmCyqUJEyZIrVq10twXFRUltra2smrVKv22c+fOCQA5ePBgPkVoWiNGjJDy5cuLVqsVEcv//ADI2rVr9c+1Wq34+PjIl19+qd8WFRUl9vb2snz5chEROXv2rACQI0eO6I/ZvHmzaDQauX37dr7FnlXPXmNa/vrrLwEg169f12/z9/eXmTNn5m1wJpDW9QUHB0uXLl3SPacwfoZdunSR1q1bG22zlM9QJPX3Q1b+fm7atEmsrKwkMjJSf8ycOXPEzc1NEhISTBofa2SyKDExEceOHUNQUJB+m5WVFYKCgnDw4EEzRmYa0dHRAAAPDw+j7UuXLkWJEiVQvXp1jBs3DvHx8eYIL8cuXrwIPz8/lCtXDv369cONGzcAAMeOHUNSUpLR51mlShWUKVPGIj/PxMRELFmyBG+++abR4qiW/vk97erVq4iMjDT6zNzd3dGoUSP9Z3bw4EEUK1YM9evX1x8TFBQEKysrHD58ON9jNoXo6GhoNBoUK1bMaPvnn38OT09P1KlTB19++WWeVNnnlT179sDLywuVK1fGkCFDcP/+ff2+wvYZ3rlzB7///jsGDhyYap+lfIbPfj9k5e/nwYMHUaNGDXh7e+uPadeuHWJiYvDPP/+YNL5Cv2ikqdy7dw8pKSlGHwoAeHt74/z582aKyjS0Wi1GjhyJpk2bonr16vrtr776Kvz9/eHn54fTp09j7NixCAsLw5o1a8wYbdY1atQIixYtQuXKlREREYFJkyahWbNm+PvvvxEZGQk7O7tUXw7e3t6IjIw0T8C5sG7dOkRFRWHAgAH6bZb++T1L97mk9Tuo2xcZGQkvLy+j/TY2NvDw8LDIz/Xx48cYO3Ys+vbta7Qg3/Dhw1G3bl14eHjgzz//xLhx4xAREYGvvvrKjNFmTfv27dG9e3cEBATg8uXLGD9+PDp06ICDBw/C2tq60H2Gixcvhqura6pma0v5DNP6fsjK38/IyMg0f1d1+0yJiQwhJCQEf//9t1H/EQBGbdI1atSAr68v2rRpg8uXL6N8+fL5HWa2dejQQf+4Zs2aaNSoEfz9/fHLL7/A0dHRjJGZ3oIFC9ChQwf4+fnpt1n651fUJSUloXfv3hARzJkzx2jf6NGj9Y9r1qwJOzs7vP3225g6dWqBnwq/T58++sc1atRAzZo1Ub58eezZswdt2rQxY2R5Y+HChejXrx8cHByMtlvKZ5je90NBwqalLCpRogSsra1T9cq+c+cOfHx8zBRV7g0dOhQbN27E7t27UapUqQyPbdSoEQDg0qVL+RGayRUrVgyVKlXCpUuX4OPjg8TERERFRRkdY4mf5/Xr17Fjxw689dZbGR5n6Z+f7nPJ6HfQx8cnVef75ORkPHjwwKI+V10Sc/36dWzfvt2oNiYtjRo1QnJyMq5du5Y/AZpQuXLlUKJECf3PZWH5DAHgjz/+QFhYWKa/m0DB/AzT+37Iyt9PHx+fNH9XdftMiYlMFtnZ2aFevXrYuXOnfptWq8XOnTsRGBhoxshyRkQwdOhQrF27Frt27UJAQECm55w8eRIA4Ovrm8fR5Y24uDhcvnwZvr6+qFevHmxtbY0+z7CwMNy4ccPiPs/Q0FB4eXmhU6dOGR5n6Z9fQEAAfHx8jD6zmJgYHD58WP+ZBQYGIioqCseOHdMfs2vXLmi1Wn0iV9DpkpiLFy9ix44d8PT0zPSckydPwsrKKlWTjCW4desW7t+/r/+5LAyfoc6CBQtQr1491KpVK9NjC9JnmNn3Q1b+fgYGBuLMmTNGSakuKX/++edNHjBl0YoVK8Te3l4WLVokZ8+elcGDB0uxYsWMemVbiiFDhoi7u7vs2bNHIiIi9Lf4+HgREbl06ZJ8+umncvToUbl69ar89ttvUq5cOWnevLmZI8+69957T/bs2SNXr16VAwcOSFBQkJQoUULu3r0rIiLvvPOOlClTRnbt2iVHjx6VwMBACQwMNHPU2ZOSkiJlypSRsWPHGm231M8vNjZWTpw4ISdOnBAA8tVXX8mJEyf0I3Y+//xzKVasmPz2229y+vRp6dKliwQEBMijR4/0ZbRv317q1Kkjhw8flv3790vFihWlb9++5rqkVDK6xsTERHn55ZelVKlScvLkSaPfTd1Ijz///FNmzpwpJ0+elMuXL8uSJUukZMmS0r9/fzNfmZLR9cXGxsqYMWPk4MGDcvXqVdmxY4fUrVtXKlasKI8fP9aXYcmfoU50dLQ4OTnJnDlzUp1f0D/DzL4fRDL/+5mcnCzVq1eXF198UU6ePClbtmyRkiVLyrhx40weLxOZbJo1a5aUKVNG7OzspGHDhnLo0CFzh5QjANK8hYaGiojIjRs3pHnz5uLh4SH29vZSoUIF+b//+z+Jjo42b+DZ8Morr4ivr6/Y2dnJc889J6+88opcunRJv//Ro0fy7rvvSvHixcXJyUm6desmERERZow4+7Zu3SoAJCwszGi7pX5+u3fvTvPnMjg4WETUEOyPP/5YvL29xd7eXtq0aZPq2u/fvy99+/YVFxcXcXNzkzfeeENiY2PNcDVpy+gar169mu7v5u7du0VE5NixY9KoUSNxd3cXBwcHqVq1qkyZMsUoETCnjK4vPj5eXnzxRSlZsqTY2tqKv7+/DBo0KNU/g5b8Ger8+OOP4ujoKFFRUanOL+ifYWbfDyJZ+/t57do16dChgzg6OkqJEiXkvffek6SkJJPHq3kSNBEREZHFYR8ZIiIislhMZIiIiMhiMZEhIiIii8VEhoiIiCwWExkiIiKyWExkiIiIyGIxkSEiIiKLxUSGiIocjUaDdevWmTsMIjIBJjJElK8GDBgAjUaT6ta+fXtzh0ZEFsjG3AEQUdHTvn17hIaGGm2zt7c3UzREZMlYI0NE+c7e3h4+Pj5Gt+LFiwNQzT5z5sxBhw4d4OjoiHLlymH16tVG5585cwatW7eGo6MjPD09MXjwYMTFxRkds3DhQlSrVg329vbw9fXF0KFDjfbfu3cP3bp1g5OTEypWrIj169fn7UUTUZ5gIkNEBc7HH3+MHj164NSpU+jXrx/69OmDc+fOAQAePnyIdu3aoXjx4jhy5AhWrVqFHTt2GCUqc+bMQUhICAYPHowzZ85g/fr1qFChgtFrTJo0Cb1798bp06fRsWNH9OvXDw8ePMjX6yQiEzD5MpRERBkIDg4Wa2trcXZ2Nrp99tlnIqJW3n3nnXeMzmnUqJEMGTJERETmzp0rxYsXl7i4OP3+33//XaysrPSrKPv5+cmHH36YbgwA5KOPPtI/j4uLEwCyefNmk10nEeUP9pEhonzXqlUrzJkzx2ibh4eH/nFgYKDRvsDAQJw8eRIAcO7cOdSqVQvOzs76/U2bNoVWq0VYWBg0Gg3Cw8PRpk2bDGOoWbOm/rGzszPc3Nxw9+7dnF4SEZkJExkiynfOzs6pmnpMxdHRMUvH2draGj3XaDTQarV5ERIR5SH2kSGiAufQoUOpnletWhUAULVqVZw6dQoPHz7U7z9w4ACsrKxQuXJluLq6omzZsti5c2e+xkxE5sEaGSLKdwkJCYiMjDTaZmNjgxIlSgAAVq1ahfr16+OFF17A0qVL8ddff2HBggUAgH79+mHChAkIDg7GxIkT8e+//2LYsGF4/fXX4e3tDQCYOHEi3nnnHXh5eaFDhw6IjY3FgQMHMGzYsPy9UCLKc0xkiCjfbdmyBb6+vkbbKleujPPnzwNQI4pWrFiBd999F76+vli+fDmef/55AICTkxO2bt2KESNGoEGDBnByckKPHj3w1Vdf6csKDg7G48ePMXPmTIwZMwYlSpRAz5498+8CiSjfaEREzB0EEZGORqPB2rVr0bVrV3OHQkQWgH1kiIiIyGIxkSEiIiKLxT4yRFSgsLWbiLKDNTJERERksZjIEBERkcViIkNEREQWi4kMERERWSwmMkRERGSxmMgQERGRxWIiQ0RERBaLiQwRERFZLCYyREREZLH+H6W7lg314LM2AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 58.71\n"
          ]
        }
      ],
      "source": [
        "def plot_accuracy(trainer):\n",
        "    train_accu = trainer.train_precs\n",
        "    test_accu = trainer.test_precs\n",
        "    x = [i + 1 for i in range(trainer.epochs)]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(x, train_accu, 'r-', label='Training Accuracy')\n",
        "    plt.plot(x, test_accu, 'b-', label='Testing Accuracy')\n",
        "    plt.title('Training and Testing Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(f\"Test Accuracy: {test_accu[-1]}\")\n",
        "\n",
        "plot_accuracy(trainer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMYnzRtAtIY4"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "PJh_P6YHtIY4",
        "outputId": "c51b5192-8db0-4f08-8ee1-2bb1fa422686"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_loss(trainer):\n",
        "    train_accu = trainer.train_losses\n",
        "    test_accu = trainer.test_losses\n",
        "    x = [i + 1 for i in range(trainer.epochs)]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(x, train_accu, 'r-', label='Training Loss')\n",
        "    plt.plot(x, test_accu, 'b-', label='Testing Loss')\n",
        "    plt.title('Training and Testing Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(trainer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
